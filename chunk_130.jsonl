{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_files(self, fls):\n\n        for f in fls:\n            # Make sure paths exists\n            if os.path.exists(f):\n                self._parser(f)\n            else:\n                logger.warning(\"File {} does not exist\".format(f))", "response": "Private method for parsing the output files."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef iter_filter(self, filters, databases=None, fields=None,\n                    filter_behavior=\"and\"):\n        \"\"\"General purpose filter iterator.\n\n        This general filter iterator allows the filtering of entries based\n        on one or more custom filters. These filters must contain\n        an entry of the `storage` attribute, a comparison operator, and the\n        test value. For example, to filter out entries with coverage below 80::\n\n            my_filter = [\"coverage\", \">=\", 80]\n\n        Filters should always be provide as a list of lists::\n\n            iter_filter([[\"coverage\", \">=\", 80]])\n            # or\n            my_filters = [[\"coverage\", \">=\", 80],\n                          [\"identity\", \">=\", 50]]\n\n            iter_filter(my_filters)\n\n        As a convenience, a list of the desired databases can be directly\n        specified using the `database` argument, which will only report\n        entries for the specified databases::\n\n            iter_filter(my_filters, databases=[\"plasmidfinder\"])\n\n        By default, this method will yield the complete entry record. However,\n        the returned filters can be specified using the `fields` option::\n\n            iter_filter(my_filters, fields=[\"reference\", \"coverage\"])\n\n        Parameters\n        ----------\n        filters : list\n            List of lists with the custom filter. Each list should have three\n            elements. (1) the key from the entry to be compared; (2) the\n            comparison operator; (3) the test value. Example:\n                ``[[\"identity\", \">\", 80]]``.\n        databases : list\n            List of databases that should be reported.\n        fields : list\n            List of fields from each individual entry that are yielded.\n        filter_behavior : str\n            options: ``'and'`` ``'or'``\n            Sets the behaviour of the filters, if multiple filters have been\n            provided. By default it is set to ``'and'``, which means that an\n            entry has to pass all filters. It can be set to ``'or'``, in which\n            case one one of the filters has to pass.\n\n        yields\n        ------\n        dic : dict\n            Dictionary object containing a :py:attr:`Abricate.storage` entry\n            that passed the filters.\n\n        \"\"\"\n\n        if filter_behavior not in [\"and\", \"or\"]:\n            raise ValueError(\"Filter behavior must be either 'and' or 'or'\")\n\n        for dic in self.storage.values():\n\n            # This attribute will determine whether an entry will be yielded\n            # or not\n            _pass = False\n\n            # Stores the flags with the test results for each filter\n            # The results will be either True or False\n            flag = []\n\n            # Filter for databases\n            if databases:\n                # Skip entry if not in specified database\n                if dic[\"database\"] not in databases:\n                    continue\n\n            # Apply filters\n            for f in filters:\n                # Get value of current filter\n                val = dic[f[0]]\n                if not self._test_truth(val, f[1], f[2]):\n                    flag.append(False)\n                else:\n                    flag.append(True)\n\n            # Test whether the entry will pass based on the test results\n            # and the filter behaviour\n            if filter_behavior == \"and\":\n                if all(flag):\n                    _pass = True\n            elif filter_behavior == \"or\":\n                if any(flag):\n                    _pass = True\n\n            if _pass:\n                if fields:\n                    yield dict((x, y) for x, y in dic.items() if x in fields)\n                else:\n                    yield dic", "response": "This method returns a generator that yields the complete entry record for the specified filters."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_contig_id(contig_str):\n\n        contig_id = contig_str\n\n        try:\n            contig_id = re.search(\".*NODE_([0-9]*)_.*\", contig_str).group(1)\n        except AttributeError:\n            pass\n\n        try:\n            contig_id = re.search(\".*Contig_([0-9]*)_.*\", contig_str).group(1)\n        except AttributeError:\n            pass\n\n        return contig_id", "response": "Tries to retrieve contig id. Returns the original string if it is unable to retrieve the id."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_plot_data(self):\n\n        json_dic = {\"plotData\": []}\n        sample_dic = {}\n        sample_assembly_map = {}\n\n        for entry in self.storage.values():\n\n            sample_id = re.match(\"(.*)_abr\", entry[\"log_file\"]).groups()[0]\n            if sample_id not in sample_dic:\n                sample_dic[sample_id] = {}\n\n            # Get contig ID using the same regex as in `assembly_report.py`\n            # template\n            contig_id = self._get_contig_id(entry[\"reference\"])\n            # Get database\n            database = entry[\"database\"]\n            if database not in sample_dic[sample_id]:\n                sample_dic[sample_id][database] = []\n\n            # Update the sample-assembly correspondence dict\n            if sample_id not in sample_assembly_map:\n                sample_assembly_map[sample_id] = entry[\"infile\"]\n\n            sample_dic[sample_id][database].append(\n                {\"contig\": contig_id,\n                 \"seqRange\": entry[\"seq_range\"],\n                 \"gene\": entry[\"gene\"].replace(\"'\", \"\"),\n                 \"accession\": entry[\"accession\"],\n                 \"coverage\": entry[\"coverage\"],\n                 \"identity\": entry[\"identity\"],\n                 },\n            )\n\n        for sample, data in sample_dic.items():\n            json_dic[\"plotData\"].append(\n                {\n                    \"sample\": sample,\n                    \"data\": {\"abricateXrange\": data},\n                    \"assemblyFile\": sample_assembly_map[sample]\n                }\n            )\n\n        return json_dic", "response": "Generates the JSON report to plot the gene boxes of the current assembly."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_report_data(self):\n\n        json_plot = self.get_plot_data()\n        json_table = self.get_table_data()\n\n        json_dic = {**json_plot, **json_table}\n\n        with open(\".report.json\", \"w\") as json_report:\n            json_report.write(json.dumps(json_dic, separators=(\",\", \":\")))", "response": "Writes the JSON report to a json file"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef main(sample_id, assembly_file, coverage_bp_file=None):\n\n    logger.info(\"Starting assembly report\")\n    assembly_obj = Assembly(assembly_file, sample_id)\n\n    logger.info(\"Retrieving summary statistics for assembly\")\n    assembly_obj.get_summary_stats(\"{}_assembly_report.csv\".format(sample_id))\n\n    size_dist = [len(x) for x in assembly_obj.contigs.values()]\n    json_dic = {\n        \"tableRow\": [{\n            \"sample\": sample_id,\n            \"data\": [\n                {\"header\": \"Contigs\",\n                 \"value\": assembly_obj.summary_info[\"ncontigs\"],\n                 \"table\": \"assembly\",\n                 \"columnBar\": True},\n                {\"header\": \"Assembled BP\",\n                 \"value\": assembly_obj.summary_info[\"total_len\"],\n                 \"table\": \"assembly\",\n                 \"columnBar\": True},\n            ]\n        }],\n        \"plotData\": [{\n            \"sample\": sample_id,\n            \"data\": {\n                \"size_dist\": size_dist\n            }\n        }]\n    }\n\n    if coverage_bp_file:\n        try:\n            window = 2000\n            gc_sliding_data = assembly_obj.get_gc_sliding(window=window)\n            cov_sliding_data = \\\n                assembly_obj.get_coverage_sliding(coverage_bp_file,\n                                                  window=window)\n\n            # Get total basepairs based on the individual coverage of each\n            # contig bpx\n            total_bp = sum(\n                [sum(x) for x in assembly_obj.contig_coverage.values()]\n            )\n\n            # Add data to json report\n            json_dic[\"plotData\"][0][\"data\"][\"genomeSliding\"] = {\n                \"gcData\": gc_sliding_data,\n                \"covData\": cov_sliding_data,\n                \"window\": window,\n                \"xbars\": assembly_obj._get_window_labels(window),\n                \"assemblyFile\": os.path.basename(assembly_file)\n            }\n            json_dic[\"plotData\"][0][\"data\"][\"sparkline\"] = total_bp\n\n        except:\n            logger.error(\"Unexpected error creating sliding window data:\\\\n\"\n                         \"{}\".format(traceback.format_exc()))\n\n    # Write json report\n    with open(\".report.json\", \"w\") as json_report:\n\n        json_report.write(json.dumps(json_dic, separators=(\",\", \":\")))\n\n    with open(\".status\", \"w\") as status_fh:\n        status_fh.write(\"pass\")", "response": "Main function of the assembly_report template."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _parse_assembly(self, assembly_file):\n\n        with open(assembly_file) as fh:\n\n            header = None\n            logger.debug(\"Starting iteration of assembly file: {}\".format(\n                assembly_file))\n\n            for line in fh:\n\n                # Skip empty lines\n                if not line.strip():\n                    continue\n\n                if line.startswith(\">\"):\n                    # Add contig header to contig dictionary\n                    header = line[1:].strip()\n                    self.contigs[header] = []\n\n                else:\n                    # Add sequence string for the current contig\n                    self.contigs[header].append(line.strip())\n\n            # After populating the contigs dictionary, convert the values\n            # list into a string sequence\n            self.contigs = OrderedDict(\n                (header, \"\".join(seq)) for header, seq in self.contigs.items())", "response": "This method parses an assembly file in fasta format and populates the self. contigs attribute with the data for each contig in the assembly."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_summary_stats(self, output_csv=None):\n\n        contig_size_list = []\n\n        self.summary_info[\"ncontigs\"] = len(self.contigs)\n\n        for contig_id, sequence in self.contigs.items():\n\n            logger.debug(\"Processing contig: {}\".format(contig_id))\n\n            # Get contig sequence size\n            contig_len = len(sequence)\n\n            # Add size for average contig size\n            contig_size_list.append(contig_len)\n\n            # Add to total assembly length\n            self.summary_info[\"total_len\"] += contig_len\n\n            # Add to average gc\n            self.summary_info[\"avg_gc\"].append(\n                sum(map(sequence.count, [\"G\", \"C\"])) / contig_len\n            )\n\n            # Add to missing data\n            self.summary_info[\"missing_data\"] += sequence.count(\"N\")\n\n        # Get average contig size\n        logger.debug(\"Getting average contig size\")\n        self.summary_info[\"avg_contig_size\"] = \\\n            sum(contig_size_list) / len(contig_size_list)\n\n        # Get average gc content\n        logger.debug(\"Getting average GC content\")\n        self.summary_info[\"avg_gc\"] = \\\n            sum(self.summary_info[\"avg_gc\"]) / len(self.summary_info[\"avg_gc\"])\n\n        # Get N50\n        logger.debug(\"Getting N50\")\n        cum_size = 0\n        for l in sorted(contig_size_list, reverse=True):\n            cum_size += l\n            if cum_size >= self.summary_info[\"total_len\"] / 2:\n                self.summary_info[\"n50\"] = l\n                break\n\n        if output_csv:\n            logger.debug(\"Writing report to csv\")\n            # Write summary info to CSV\n            with open(output_csv, \"w\") as fh:\n                summary_line = \"{}, {}\\\\n\".format(\n                    self.sample, \",\".join(\n                        [str(x) for x in self.summary_info.values()]))\n                fh.write(summary_line)", "response": "Generates a CSV report with summary statistics about the assembly and the contigs in the contigs list."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the mapping between sliding window points and their contigs and their x - axis positions and the x - axis labels.", "response": "def _get_window_labels(self, window):\n        \"\"\"Returns the mapping between sliding window points and their contigs,\n        and the x-axis position of contig\n\n        Parameters\n        ----------\n        window : int\n            Size of the window.\n\n        Returns\n        -------\n        xbars : list\n            The x-axis position of the ending for each contig.\n        labels : list\n            The x-axis labels for each data point in the sliding window\n\n        \"\"\"\n\n        # Get summary stats, if they have not yet been triggered\n        if not self.summary_info:\n            self.get_summary_stats()\n\n        # Get contig boundary positon\n        c = 0\n        xbars = []\n        for contig, seq in self.contigs.items():\n            contig_id = self._get_contig_id(contig)\n            self.contig_boundaries[contig_id] = [c, c + len(seq)]\n            c += len(seq)\n            xbars.append((contig_id, c, contig))\n\n        return xbars"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _gc_prop(s, length):\n\n        gc = sum(map(s.count, [\"c\", \"g\"]))\n\n        return gc / length", "response": "Get proportion of GC from a string\n            Arbitrary string\n           "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_gc_sliding(self, window=2000):\n\n        gc_res = []\n\n        # Get complete sequence to calculate sliding window values\n        complete_seq = \"\".join(self.contigs.values()).lower()\n\n        for i in range(0, len(complete_seq), window):\n\n            seq_window = complete_seq[i:i + window]\n\n            # Get GC proportion\n            gc_res.append(round(self._gc_prop(seq_window, len(seq_window)), 2))\n\n        return gc_res", "response": "Calculates a sliding window of the GC content for each data point in the assembly\n           "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite the report to a JSON file.", "response": "def write_json_report(sample_id, data1, data2):\n    \"\"\"Writes the report\n\n    Parameters\n    ----------\n    data1\n    data2\n\n    Returns\n    -------\n\n    \"\"\"\n\n    parser_map = {\n        \"base_sequence_quality\": \">>Per base sequence quality\",\n        \"sequence_quality\": \">>Per sequence quality scores\",\n        \"base_gc_content\": \">>Per sequence GC content\",\n        \"base_n_content\": \">>Per base N content\",\n        \"sequence_length_dist\": \">>Sequence Length Distribution\",\n        \"per_base_sequence_content\": \">>Per base sequence content\"\n    }\n\n    json_dic = {\n        \"plotData\": [{\n            \"sample\": sample_id,\n            \"data\": {\n                \"base_sequence_quality\": {\"status\": None, \"data\": []},\n                \"sequence_quality\": {\"status\": None, \"data\": []},\n                \"base_gc_content\": {\"status\": None, \"data\": []},\n                \"base_n_content\": {\"status\": None, \"data\": []},\n                \"sequence_length_dist\": {\"status\": None, \"data\": []},\n                \"per_base_sequence_content\": {\"status\": None, \"data\": []}\n            }\n        }]\n    }\n\n    for cat, start_str in parser_map.items():\n\n        if cat == \"per_base_sequence_content\":\n            fs = 1\n            fe = 5\n        else:\n            fs = 1\n            fe = 2\n\n        report1, status1 = _get_quality_stats(data1, start_str,\n                                              field_start=fs, field_end=fe)\n        report2, status2 = _get_quality_stats(data2, start_str,\n                                              field_start=fs, field_end=fe)\n\n        status = None\n        for i in [\"fail\", \"warn\", \"pass\"]:\n            if i in [status1, status2]:\n                status = i\n\n        json_dic[\"plotData\"][0][\"data\"][cat][\"data\"] = [report1, report2]\n        json_dic[\"plotData\"][0][\"data\"][cat][\"status\"] = status\n\n    return json_dic"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_trim_index(biased_list):\n\n    # Return index 0 if there are no biased positions\n    if set(biased_list) == {False}:\n        return 0\n\n    if set(biased_list[:5]) == {False}:\n        return 0\n\n    # Iterate over the biased_list array. Keep the iteration going until\n    # we find a biased position with the two following positions unbiased\n    # (e.g.: True, False, False).\n    # When this condition is verified, return the last biased position\n    # index for subsequent trimming.\n    for i, val in enumerate(biased_list):\n        if val and set(biased_list[i+1:i+3]) == {False}:\n            return i + 1\n\n    # If the previous iteration could not find and index to trim, it means\n    # that the whole list is basically biased. Return the length of the\n    # biased_list\n    return len(biased_list)", "response": "Returns the index of the optimal trim position from a list of True elements."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nassess the optimal trim range for a given FastQC data file.", "response": "def trim_range(data_file):\n    \"\"\"Assess the optimal trim range for a given FastQC data file.\n\n    This function will parse a single FastQC data file, namely the\n    *'Per base sequence content'* category. It will retrieve the A/T and G/C\n    content for each nucleotide position in the reads, and check whether the\n    G/C and A/T proportions are between 80% and 120%. If they are, that\n    nucleotide position is marked as biased for future removal.\n\n    Parameters\n    ----------\n    data_file: str\n        Path to FastQC data file.\n\n    Returns\n    -------\n    trim_nt: list\n        List containing the range with the best trimming positions for the\n        corresponding FastQ file. The first element is the 5' end trim index\n        and the second element is the 3' end trim index.\n    \"\"\"\n\n    logger.debug(\"Starting trim range assessment\")\n\n    # Target string for nucleotide bias assessment\n    target_nuc_bias = \">>Per base sequence content\"\n    logger.debug(\"Target string to start nucleotide bias assessment set to \"\n                 \"{}\".format(target_nuc_bias))\n    # This flag will become True when gathering base proportion data\n    # from file.\n    gather = False\n\n    # This variable will store a boolean array on the biased/unbiased\n    # positions. Biased position will be True, while unbiased positions\n    # will be False\n    biased = []\n\n    with open(data_file) as fh:\n\n        for line in fh:\n            # Start assessment of nucleotide bias\n            if line.startswith(target_nuc_bias):\n                # Skip comment line\n                logger.debug(\"Found target string at line: {}\".format(line))\n                next(fh)\n                gather = True\n            # Stop assessment when reaching end of target module\n            elif line.startswith(\">>END_MODULE\") and gather:\n                logger.debug(\"Stopping parsing at line: {}\".format(line))\n                break\n            elif gather:\n                # Get proportions of each nucleotide\n                g, a, t, c = [float(x) for x in line.strip().split()[1:]]\n                # Get 'GC' and 'AT content\n                gc = (g + 0.1) / (c + 0.1)\n                at = (a + 0.1) / (t + 0.1)\n                # Assess bias\n                if 0.8 <= gc <= 1.2 and 0.8 <= at <= 1.2:\n                    biased.append(False)\n                else:\n                    biased.append(True)\n\n    logger.debug(\"Finished bias assessment with result: {}\".format(biased))\n\n    # Split biased list in half to get the 5' and 3' ends\n    biased_5end, biased_3end = biased[:int(len(biased)/2)],\\\n        biased[int(len(biased)/2):][::-1]\n\n    logger.debug(\"Getting optimal trim range from biased list\")\n    trim_nt = [0, 0]\n    # Assess number of nucleotides to clip at 5' end\n    trim_nt[0] = get_trim_index(biased_5end)\n    logger.debug(\"Optimal trim range at 5' end set to: {}\".format(trim_nt[0]))\n    # Assess number of nucleotides to clip at 3' end\n    trim_nt[1] = len(biased) - get_trim_index(biased_3end)\n    logger.debug(\"Optimal trim range at 3' end set to: {}\".format(trim_nt[1]))\n\n    return trim_nt"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_sample_trim(p1_data, p2_data):\n\n    sample_ranges = [trim_range(x) for x in [p1_data, p2_data]]\n\n    # Get the optimal trim position for 5' end\n    optimal_5trim = max([x[0] for x in sample_ranges])\n    # Get optimal trim position for 3' end\n    optimal_3trim = min([x[1] for x in sample_ranges])\n\n    return optimal_5trim, optimal_3trim", "response": "This function returns the optimal read trim range from data files of paired - end FastQ reads."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing a FastQC summary report file and returns it as a dictionary.", "response": "def get_summary(summary_file):\n    \"\"\"Parses a FastQC summary report file and returns it as a dictionary.\n\n    This function parses a typical FastQC summary report file, retrieving\n    only the information on the first two columns. For instance, a line could\n    be::\n\n        'PASS\tBasic Statistics\tSH10762A_1.fastq.gz'\n\n    This parser will build a dictionary with the string in the second column\n    as a key and the QC result as the value. In this case, the returned\n    ``dict`` would be something like::\n\n        {\"Basic Statistics\": \"PASS\"}\n\n    Parameters\n    ----------\n    summary_file: str\n        Path to FastQC summary report.\n\n    Returns\n    -------\n    summary_info: :py:data:`OrderedDict`\n        Returns the information of the FastQC summary report as an ordered\n        dictionary, with the categories as strings and the QC result as values.\n\n    \"\"\"\n\n    summary_info = OrderedDict()\n    logger.debug(\"Retrieving summary information from file: {}\".format(\n        summary_file))\n\n    with open(summary_file) as fh:\n        for line in fh:\n            # Skip empty lines\n            if not line.strip():\n                continue\n            # Populate summary info\n            fields = [x.strip() for x in line.split(\"\\t\")]\n            summary_info[fields[1]] = fields[0]\n\n    logger.debug(\"Retrieved summary information from file: {}\".format(\n        summary_info))\n\n    return summary_info"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_summary_health(summary_file, **kwargs):\n\n    # Store the summary categories that cannot fail. If they fail, do not\n    # proceed with this sample\n    fail_sensitive = kwargs.get(\"fail_sensitive\", [\n        \"Per base sequence quality\",\n        \"Overrepresented sequences\",\n        \"Sequence Length Distribution\",\n        \"Per sequence GC content\"\n    ])\n    logger.debug(\"Fail sensitive categories: {}\".format(fail_sensitive))\n\n    # Store summary categories that must pass. If they do not, do not proceed\n    # with that sample\n    must_pass = kwargs.get(\"must_pass\", [\n        \"Per base N content\",\n        \"Adapter Content\"\n    ])\n    logger.debug(\"Must pass categories: {}\".format(must_pass))\n\n    warning_fail_sensitive = kwargs.get(\"warning_fail_sensitive\", [\n        \"Per base sequence quality\",\n        \"Overrepresented sequences\",\n\n    ])\n\n    warning_must_pass = kwargs.get(\"warning_must_pass\", [\n        \"Per base sequence content\"\n    ])\n\n    # Get summary dictionary\n    summary_info = get_summary(summary_file)\n\n    # This flag will change to False if one of the tests fails\n    health = True\n    # List of failing categories\n    failed = []\n    # List of warning categories\n    warning = []\n\n    for cat, test in summary_info.items():\n\n        logger.debug(\"Assessing category {} with result {}\".format(cat, test))\n\n        # FAILURES\n        # Check for fail sensitive\n        if cat in fail_sensitive and test == \"FAIL\":\n            health = False\n            failed.append(\"{}:{}\".format(cat, test))\n            logger.error(\"Category {} failed a fail sensitive \"\n                         \"category\".format(cat))\n\n        # Check for must pass\n        if cat in must_pass and test != \"PASS\":\n            health = False\n            failed.append(\"{}:{}\".format(cat, test))\n            logger.error(\"Category {} failed a must pass category\".format(\n                cat))\n\n        # WARNINGS\n        # Check for fail sensitive\n        if cat in warning_fail_sensitive and test == \"FAIL\":\n            warning.append(\"Failed category: {}\".format(cat))\n            logger.warning(\"Category {} flagged at a fail sensitive \"\n                           \"category\".format(cat))\n\n        if cat in warning_must_pass and test != \"PASS\":\n            warning.append(\"Did not pass category: {}\".format(cat))\n            logger.warning(\"Category {} flagged at a must pass \"\n                           \"category\".format(cat))\n\n    # Passed all tests\n    return health, failed, warning", "response": "Checks the health of a sample from the FastQC summary file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing a bowtie log file and populate the self. n_reads self. align_0x self. align_1x and self. overall_rate attributes with the data from the log file.", "response": "def parse_log(self, bowtie_log):\n        \"\"\"Parse a bowtie log file.\n\n        This is a bowtie log parsing method that populates the\n        :py:attr:`self.n_reads, self.align_0x, self.align_1x, self.align_mt1x and self.overall_rate` attributes with\n        data from the log file.\n\n        Disclamer: THIS METHOD IS HORRIBLE BECAUSE THE BOWTIE LOG IS HORRIBLE.\n\n        The insertion of data on the attribytes is done by the\n        :py:meth:`set_attribute method.\n\n        Parameters\n        ----------\n        bowtie_log : str\n            Path to the boetie log file.\n\n       \"\"\"\n\n        print(\"is here!\")\n\n        # Regexes - thanks to https://github.com/ewels/MultiQC/blob/master/multiqc/modules/bowtie2/bowtie2.py\n        regexes = {\n            'unpaired': {\n                'unpaired_aligned_none': r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned 0 times\",\n                'unpaired_aligned_one': r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned exactly 1 time\",\n                'unpaired_aligned_multi': r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned >1 times\"\n            },\n            'paired': {\n                'paired_aligned_none': r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned concordantly 0 times\",\n                'paired_aligned_one': r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned concordantly exactly 1 time\",\n                'paired_aligned_multi': r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned concordantly >1 times\",\n                'paired_aligned_discord_one': r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned discordantly 1 time\",\n                'paired_aligned_discord_multi': r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned discordantly >1 times\",\n                'paired_aligned_mate_one': r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned exactly 1 time\",\n                'paired_aligned_mate_multi': r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned >1 times\",\n                'paired_aligned_mate_none': r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) aligned 0 times\"\n            }\n        }\n\n        #Missing parser for unpaired (not implemented in flowcraft yet)\n\n        with open(bowtie_log, \"r\") as f:\n            #Go through log file line by line\n            for l in f:\n\n                print(l)\n\n                #total reads\n                total = re.search(r\"(\\\\d+) reads; of these:\", l)\n                print(total)\n                if total:\n                    print(total)\n                    self.set_n_reads(total.group(1))\n\n\n                # Paired end reads aka the pain\n                paired = re.search(r\"(\\\\d+) \\\\([\\\\d\\\\.]+%\\\\) were paired; of these:\", l)\n                if paired:\n                    paired_total = int(paired.group(1))\n\n                    paired_numbers = {}\n\n                    # Do nested loop whilst we have this level of indentation\n                    l = f.readline()\n                    while l.startswith('    '):\n                        for k, r in regexes['paired'].items():\n                            match = re.search(r, l)\n                            if match:\n                                paired_numbers[k] = int(match.group(1))\n                        l = f.readline()\n\n\n                    align_zero_times = paired_numbers['paired_aligned_none'] + paired_numbers['paired_aligned_mate_none']\n                    if align_zero_times:\n                        self.set_align_0x(align_zero_times)\n\n                    align_one_time = paired_numbers['paired_aligned_one'] + paired_numbers['paired_aligned_mate_one']\n                    if align_one_time:\n                        self.set_align_1x(align_one_time)\n\n                    align_more_than_one_time = paired_numbers['paired_aligned_multi'] + paired_numbers['paired_aligned_mate_multi']\n                    if align_more_than_one_time:\n                        self.set_align_mt1x(align_more_than_one_time)\n\n\n                # Overall alignment rate\n                overall = re.search(r\"([\\\\d\\\\.]+)% overall alignment rate\", l)\n                if overall:\n                    self.overall_rate = float(overall.group(1))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _parse_process_name(name_str):\n\n        directives = None\n\n        fields = name_str.split(\"=\")\n        process_name = fields[0]\n\n        if len(fields) == 2:\n            _directives = fields[1].replace(\"'\", '\"')\n            try:\n                directives = json.loads(_directives)\n            except json.decoder.JSONDecodeError:\n                raise eh.ProcessError(\n                    \"Could not parse directives for process '{}'. The raw\"\n                    \" string is: {}\\n\"\n                    \"Possible causes include:\\n\"\n                    \"\\t1. Spaces inside directives\\n\"\n                    \"\\t2. Missing '=' symbol before directives\\n\"\n                    \"\\t3. Missing quotes (' or \\\") around directives\\n\"\n                    \"A valid example: process_name={{'cpus':'2'}}\".format(\n                        process_name, name_str))\n\n        return process_name, directives", "response": "Parses the process name and its directives and returns the process name and its\n            directives as a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding the list of process connections for the next flow generator.", "response": "def _build_connections(self, process_list, ignore_dependencies,\n                           auto_dependency):\n        \"\"\"Parses the process connections dictionaries into a process list\n\n        This method is called upon instantiation of the NextflowGenerator\n        class. Essentially, it sets the main input/output channel names of the\n        processes so that they can be linked correctly.\n\n        If a connection between two consecutive process is not possible due\n        to a mismatch in the input/output types, it exits with an error.\n\n        Returns\n        -------\n\n        \"\"\"\n\n        logger.debug(\"=============================\")\n        logger.debug(\"Building pipeline connections\")\n        logger.debug(\"=============================\")\n\n        logger.debug(\"Processing connections: {}\".format(process_list))\n\n        for p, con in enumerate(process_list):\n\n            logger.debug(\"Processing connection '{}': {}\".format(p, con))\n\n            # Get lanes\n            in_lane = con[\"input\"][\"lane\"]\n            out_lane = con[\"output\"][\"lane\"]\n            logger.debug(\"[{}] Input lane: {}\".format(p, in_lane))\n            logger.debug(\"[{}] Output lane: {}\".format(p, out_lane))\n\n            # Update the total number of lines of the pipeline\n            if out_lane > self.lanes:\n                self.lanes = out_lane\n\n            # Get process names and directives for the output process\n            p_in_name, p_out_name, out_directives = self._get_process_names(\n                con, p)\n\n            # Check if process is available or correctly named\n            if p_out_name not in self.process_map:\n                logger.error(colored_print(\n                    \"\\nThe process '{}' is not available.\"\n                        .format(p_out_name), \"red_bold\"))\n                guess_process(p_out_name, self.process_map)\n                sys.exit(1)\n\n            # Instance output process\n            out_process = self.process_map[p_out_name](template=p_out_name)\n\n            # Update directives, if provided\n            if out_directives:\n                out_process.update_attributes(out_directives)\n\n            # Set suffix strings for main input/output channels. Suffixes are\n            # based on the lane and the arbitrary and unique process id\n            # e.g.: 'process_1_1'\n            input_suf = \"{}_{}\".format(in_lane, p)\n            output_suf = \"{}_{}\".format(out_lane, p)\n            logger.debug(\"[{}] Setting main channels with input suffix '{}'\"\n                         \" and output suffix '{}'\".format(\n                            p, input_suf, output_suf))\n            out_process.set_main_channel_names(input_suf, output_suf, out_lane)\n\n            # Instance input process, if it exists. In case of init, the\n            # output process forks from the raw input user data\n            if p_in_name != \"__init__\":\n                # Create instance of input process\n                in_process = self.process_map[p_in_name](template=p_in_name)\n                # Test if two processes can be connected by input/output types\n                logger.debug(\"[{}] Testing connection between input and \"\n                             \"output processes\".format(p))\n                self._test_connection(in_process, out_process)\n                out_process.parent_lane = in_lane\n            else:\n                # When the input process is __init__, set the parent_lane\n                # to None. This will tell the engine that this process\n                # will receive the main input from the raw user input.\n                out_process.parent_lane = None\n            logger.debug(\"[{}] Parent lane: {}\".format(\n                p, out_process.parent_lane))\n\n            # If the current connection is a fork, add it to the fork tree\n            if in_lane != out_lane:\n                logger.debug(\"[{}] Connection is a fork. Adding lanes to \"\n                             \"fork list\".format(p))\n                self._fork_tree[in_lane].append(out_lane)\n                # Update main output fork of parent process\n                try:\n                    parent_process = [\n                        x for x in self.processes if x.lane == in_lane and\n                        x.template == p_in_name\n                    ][0]\n                    logger.debug(\n                        \"[{}] Updating main forks of parent fork '{}' with\"\n                        \" '{}'\".format(p, parent_process,\n                                       out_process.input_channel))\n                    parent_process.update_main_forks(out_process.input_channel)\n                except IndexError:\n                    pass\n            else:\n                # Get parent process, naive version\n                parent_process = self.processes[-1]\n\n                # Check if the last process' lane matches the lane of the\n                # current output process. If not, get the last process\n                # in the same lane\n                if parent_process.lane and parent_process.lane != out_lane:\n                    parent_process = [x for x in self.processes[::-1]\n                                      if x.lane == out_lane][0]\n\n                if parent_process.output_channel:\n                    logger.debug(\n                        \"[{}] Updating input channel of output process\"\n                        \" with '{}'\".format(\n                            p, parent_process.output_channel))\n                    out_process.input_channel = parent_process.output_channel\n\n            # Check for process dependencies\n            if out_process.dependencies and not ignore_dependencies:\n                logger.debug(\"[{}] Dependencies found for process '{}': \"\n                             \"{}\".format(p, p_out_name,\n                                         out_process.dependencies))\n                parent_lanes = self._get_fork_tree(out_lane)\n                for dep in out_process.dependencies:\n                    if not self._search_tree_backwards(dep, parent_lanes):\n                        if auto_dependency:\n                            self._add_dependency(\n                                out_process, dep, in_lane, out_lane, p)\n                        elif not self.export_parameters:\n                            logger.error(colored_print(\n                                \"\\nThe following dependency of the process\"\n                                \" '{}' is missing: {}\".format(p_out_name, dep),\n                                \"red_bold\"))\n                            sys.exit(1)\n\n            self.processes.append(out_process)\n\n        logger.debug(\"Completed connections: {}\".format(self.processes))\n        logger.debug(\"Fork tree: {}\".format(self._fork_tree))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_process_names(self, con, pid):\n\n        try:\n            _p_in_name = con[\"input\"][\"process\"]\n            p_in_name, _ = self._parse_process_name(_p_in_name)\n            logger.debug(\"[{}] Input channel: {}\".format(pid, p_in_name))\n            _p_out_name = con[\"output\"][\"process\"]\n            p_out_name, out_directives = self._parse_process_name(\n                _p_out_name)\n            logger.debug(\"[{}] Output channel: {}\".format(pid, p_out_name))\n        # Exception is triggered when the process name/directives cannot\n        # be processes.\n        except eh.ProcessError as ex:\n            logger.error(colored_print(ex.value, \"red_bold\"))\n            sys.exit(1)\n\n        return p_in_name, p_out_name, out_directives", "response": "Returns the input output process names and output process directives from the input process and output process."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _add_dependency(self, p, template, inlane, outlane, pid):\n\n        dependency_proc = self.process_map[template](template=template)\n\n        if dependency_proc.input_type != p.input_type:\n            logger.error(\"Cannot automatically add dependency with different\"\n                         \" input type. Input type of process '{}' is '{}.\"\n                         \" Input type of dependency '{}' is '{}'\".format(\n                            p.template, p.input_type, template,\n                            dependency_proc.input_type))\n\n        input_suf = \"{}_{}_dep\".format(inlane, pid)\n        output_suf = \"{}_{}_dep\".format(outlane, pid)\n        dependency_proc.set_main_channel_names(input_suf, output_suf, outlane)\n\n        # To insert the dependency process before the current process, we'll\n        # need to move the input channel name of the later to the former, and\n        # set a new connection between the dependency and the process.\n        dependency_proc.input_channel = p.input_channel\n        p.input_channel = dependency_proc.output_channel\n\n        # If the current process was the first in the pipeline, change the\n        # lanes so that the dependency becomes the first process\n        if not p.parent_lane:\n            p.parent_lane = outlane\n            dependency_proc.parent_lane = None\n        else:\n            dependency_proc.parent_lane = inlane\n            p.parent_lane = outlane\n\n        self.processes.append(dependency_proc)", "response": "Automatically adds a dependency of a process."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsearch the process tree backwards in search of a provided process template attribute and parent lanes.", "response": "def _search_tree_backwards(self, template, parent_lanes):\n        \"\"\"Searches the process tree backwards in search of a provided process\n\n        The search takes into consideration the provided parent lanes and\n        searches only those\n\n        Parameters\n        ----------\n        template : str\n            Name of the process template attribute being searched\n        parent_lanes : list\n            List of integers with the parent lanes to be searched\n\n        Returns\n        -------\n        bool\n            Returns True when the template is found. Otherwise returns False.\n        \"\"\"\n\n        for p in self.processes[::-1]:\n\n            # Ignore process in different lanes\n            if p.lane not in parent_lanes:\n                continue\n\n            # template found\n            if p.template == template:\n                return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _build_header(self):\n\n        logger.debug(\"===============\")\n        logger.debug(\"Building header\")\n        logger.debug(\"===============\")\n        self.template += hs.header", "response": "Builds the master header string"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds the footer template to the master template string", "response": "def _build_footer(self):\n        \"\"\"Adds the footer template to the master template string\"\"\"\n\n        logger.debug(\"===============\")\n        logger.debug(\"Building header\")\n        logger.debug(\"===============\")\n        self.template += fs.footer"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives a process, this method updates the :attr:`~Process.main_raw_inputs` attribute with the corresponding raw input channel of that process. The input channel and input type can be overridden if the `input_channel` and `input_type` arguments are provided. Parameters ---------- p : flowcraft.Process.Process Process instance whose raw input will be modified sink_channel: str Sets the channel where the raw input will fork into. It overrides the process's `input_channel` attribute. input_type: str Sets the type of the raw input. It overrides the process's `input_type` attribute.", "response": "def _update_raw_input(self, p, sink_channel=None, input_type=None):\n        \"\"\"Given a process, this method updates the\n        :attr:`~Process.main_raw_inputs` attribute with the corresponding\n        raw input channel of that process. The input channel and input type\n        can be overridden if the `input_channel` and `input_type` arguments\n        are provided.\n\n        Parameters\n        ----------\n        p : flowcraft.Process.Process\n            Process instance whose raw input will be modified\n        sink_channel: str\n            Sets the channel where the raw input will fork into. It overrides\n            the process's `input_channel` attribute.\n        input_type: str\n            Sets the type of the raw input. It overrides the process's\n            `input_type` attribute.\n        \"\"\"\n\n        process_input = input_type if input_type else p.input_type\n        process_channel = sink_channel if sink_channel else p.input_channel\n\n        logger.debug(\"[{}] Setting raw input channel \"\n                     \"with input type '{}'\".format(p.template, process_input))\n        # Get the dictionary with the raw forking information for the\n        # provided input\n        raw_in = p.get_user_channel(process_channel, process_input)\n        logger.debug(\"[{}] Fetched process raw user: {}\".format(p.template,\n                                                                raw_in))\n\n        if process_input in self.main_raw_inputs:\n            self.main_raw_inputs[process_input][\"raw_forks\"].append(\n                raw_in[\"input_channel\"])\n        else:\n            self.main_raw_inputs[process_input] = {\n                \"channel\": raw_in[\"channel\"],\n                \"channel_str\": \"{}\\n{} = {}\".format(\n                    raw_in[\"checks\"].format(raw_in[\"params\"]),\n                    raw_in[\"channel\"],\n                    raw_in[\"channel_str\"].format(raw_in[\"params\"])),\n                \"raw_forks\": [raw_in[\"input_channel\"]]\n            }\n        logger.debug(\"[{}] Updated main raw inputs: {}\".format(\n            p.template, self.main_raw_inputs))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _update_extra_inputs(self, p):\n\n        if p.extra_input:\n            logger.debug(\"[{}] Found extra input: {}\".format(\n                p.template, p.extra_input))\n\n            if p.extra_input == \"default\":\n                # Check if the default type is now present in the main raw\n                # inputs. If so, issue an error. The default param can only\n                # be used when not present in the main raw inputs\n                if p.input_type in self.main_raw_inputs:\n                    logger.error(colored_print(\n                        \"\\nThe default input param '{}' of the process '{}'\"\n                        \" is already specified as a main input parameter of\"\n                        \" the pipeline. Please choose a different extra_input\"\n                        \" name.\".format(p.input_type, p.template), \"red_bold\"))\n                    sys.exit(1)\n                param = p.input_type\n            else:\n                param = p.extra_input\n\n            dest_channel = \"EXTRA_{}_{}\".format(p.template, p.pid)\n\n            if param not in self.extra_inputs:\n                self.extra_inputs[param] = {\n                    \"input_type\": p.input_type,\n                    \"channels\": [dest_channel]\n                }\n            else:\n                if self.extra_inputs[param][\"input_type\"] != p.input_type:\n                    logger.error(colored_print(\n                        \"\\nThe extra_input parameter '{}' for process\"\n                        \" '{}' was already defined with a different \"\n                        \"input type '{}'. Please choose a different \"\n                        \"extra_input name.\".format(\n                            p.input_type, p.template,\n                            self.extra_inputs[param][\"input_type\"]),\n                        \"red_bold\"))\n                    sys.exit(1)\n                self.extra_inputs[param][\"channels\"].append(dest_channel)\n\n            logger.debug(\"[{}] Added extra channel '{}' linked to param: '{}' \"\n                         \"\".format(p.template, param,\n                                   self.extra_inputs[param]))\n            p.update_main_input(\n                \"{}.mix({})\".format(p.input_channel, dest_channel)\n            )", "response": "This method updates the process. extra_inputs attribute with the corresponding extra_input inputs of that process."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive a process, this method updates the :attr:`~Process.secondary_channels` attribute with the corresponding secondary inputs of that channel. The rationale of the secondary channels is the following: - Start storing any secondary emitting channels, by checking the `link_start` list attribute of each process. If there are channel names in the link start, it adds to the secondary channels dictionary. - Check for secondary receiving channels, by checking the `link_end` list attribute. If the link name starts with a `__` signature, it will created an implicit link with the last process with an output type after the signature. Otherwise, it will check is a corresponding link start already exists in the at least one process upstream of the pipeline and if so, it will update the ``secondary_channels`` attribute with the new link. Parameters ---------- p : flowcraft.Process.Process", "response": "def _update_secondary_channels(self, p):\n        \"\"\"Given a process, this method updates the\n        :attr:`~Process.secondary_channels` attribute with the corresponding\n        secondary inputs of that channel.\n\n        The rationale of the secondary channels is the following:\n\n            - Start storing any secondary emitting channels, by checking the\n              `link_start` list attribute of each process. If there are\n              channel names in the link start, it adds to the secondary\n              channels dictionary.\n            - Check for secondary receiving channels, by checking the\n              `link_end` list attribute. If the link name starts with a\n              `__` signature, it will created an implicit link with the last\n              process with an output type after the signature. Otherwise,\n              it will check is a corresponding link start already exists in\n              the at least one process upstream of the pipeline and if so,\n              it will update the ``secondary_channels`` attribute with the\n              new link.\n\n        Parameters\n        ----------\n        p : flowcraft.Process.Process\n        \"\"\"\n\n        # Check if the current process has a start of a secondary\n        # side channel\n        if p.link_start:\n            logger.debug(\"[{}] Found secondary link start: {}\".format(\n                p.template, p.link_start))\n            for l in p.link_start:\n                # If there are multiple link starts in the same lane, the\n                # last one is the only one saved.\n                if l in self.secondary_channels:\n                    self.secondary_channels[l][p.lane] = {\"p\": p, \"end\": []}\n                else:\n                    self.secondary_channels[l] = {p.lane: {\"p\": p, \"end\": []}}\n\n        # check if the current process receives a secondary side channel.\n        # If so, add to the links list of that side channel\n        if p.link_end:\n            logger.debug(\"[{}] Found secondary link end: {}\".format(\n                p.template, p.link_end))\n            for l in p.link_end:\n\n                # Get list of lanes from the parent forks.\n                parent_forks = self._get_fork_tree(p.lane)\n\n                # Parse special case where the secondary channel links with\n                # the main output of the specified type\n                if l[\"link\"].startswith(\"__\"):\n                    self._set_implicit_link(p, l)\n                    continue\n\n                # Skip if there is no match for the current link in the\n                # secondary channels\n                if l[\"link\"] not in self.secondary_channels:\n                    continue\n\n                for lane in parent_forks:\n                    if lane in self.secondary_channels[l[\"link\"]]:\n                        self.secondary_channels[\n                            l[\"link\"]][lane][\"end\"].append(\"{}\".format(\n                                \"{}_{}\".format(l[\"alias\"], p.pid)))\n\n        logger.debug(\"[{}] Secondary links updated: {}\".format(\n            p.template, self.secondary_channels))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the main channels for the pipeline and updates the raw input and output forks.", "response": "def _set_channels(self):\n        \"\"\"Sets the main channels for the pipeline\n\n        This method will parse de the :attr:`~Process.processes` attribute\n        and perform the following tasks for each process:\n\n            - Sets the input/output channels and main input forks and adds\n              them to the process's\n              :attr:`flowcraft.process.Process._context`\n              attribute (See\n              :func:`~NextflowGenerator.set_channels`).\n            - Automatically updates the main input channel of the first\n              process of each lane so that they fork from the user provide\n              parameters (See\n              :func:`~NextflowGenerator._update_raw_input`).\n            - Check for the presence of secondary channels and adds them to the\n              :attr:`~NextflowGenerator.secondary_channels` attribute.\n\n        Notes\n        -----\n        **On the secondary channel setup**: With this approach, there can only\n        be one secondary link start for each type of secondary link. For\n        instance, If there are two processes that start a secondary channel\n        for the ``SIDE_max_len`` channel, only the last one will be recorded,\n        and all receiving processes will get the channel from the latest\n        process. Secondary channels can only link if the source process if\n        downstream of the sink process in its \"forking\" path.\n        \"\"\"\n\n        logger.debug(\"=====================\")\n        logger.debug(\"Setting main channels\")\n        logger.debug(\"=====================\")\n\n        for i, p in enumerate(self.processes):\n\n            # Set main channels for the process\n            logger.debug(\"[{}] Setting main channels with pid: {}\".format(\n                p.template, i))\n            p.set_channels(pid=i)\n\n            # If there is no parent lane, set the raw input channel from user\n            logger.debug(\"{} {} {}\".format(p.parent_lane, p.input_type, p.template))\n            if not p.parent_lane and p.input_type:\n                self._update_raw_input(p)\n\n            self._update_extra_inputs(p)\n\n            self._update_secondary_channels(p)\n\n            logger.info(colored_print(\n                \"\\tChannels set for {} \\u2713\".format(p.template)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _set_init_process(self):\n\n        logger.debug(\"========================\")\n        logger.debug(\"Setting secondary inputs\")\n        logger.debug(\"========================\")\n\n        # Get init process\n        init_process = self.processes[0]\n        logger.debug(\"Setting main raw inputs: \"\n                     \"{}\".format(self.main_raw_inputs))\n        init_process.set_raw_inputs(self.main_raw_inputs)\n        logger.debug(\"Setting extra inputs: {}\".format(self.extra_inputs))\n        init_process.set_extra_inputs(self.extra_inputs)", "response": "Sets main raw inputs and secondary inputs on the init process for the current locale."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the secondary channels for the pipeline.", "response": "def _set_secondary_channels(self):\n        \"\"\"Sets the secondary channels for the pipeline\n\n        This will iterate over the\n        :py:attr:`NextflowGenerator.secondary_channels` dictionary that is\n        populated when executing\n        :func:`~NextflowGenerator._update_secondary_channels` method.\n        \"\"\"\n\n        logger.debug(\"==========================\")\n        logger.debug(\"Setting secondary channels\")\n        logger.debug(\"==========================\")\n\n        logger.debug(\"Setting secondary channels: {}\".format(\n            self.secondary_channels))\n\n        for source, lanes in self.secondary_channels.items():\n\n            for vals in lanes.values():\n\n                if not vals[\"end\"]:\n                    logger.debug(\"[{}] No secondary links to setup\".format(\n                        vals[\"p\"].template))\n                    continue\n\n                logger.debug(\"[{}] Setting secondary links for \"\n                             \"source {}: {}\".format(vals[\"p\"].template,\n                                                    source,\n                                                    vals[\"end\"]))\n\n                vals[\"p\"].set_secondary_channel(source, vals[\"end\"])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds general compilers to the pipeline.", "response": "def _set_general_compilers(self):\n        \"\"\"Adds compiler channels to the :attr:`processes` attribute.\n\n        This method will iterate over the pipeline's processes and check\n        if any process is feeding channels to a compiler process. If so, that\n        compiler process is added to the pipeline and those channels are\n        linked to the compiler via some operator.\n        \"\"\"\n\n        for c, c_info in self.compilers.items():\n\n            # Instantiate compiler class object and set empty channel list\n            compiler_cls = c_info[\"cls\"](template=c_info[\"template\"])\n            c_info[\"channels\"] = []\n\n            for p in self.processes:\n                if not any([isinstance(p, x) for x in self.skip_class]):\n                    # Check if process has channels to feed to a compiler\n                    if c in p.compiler:\n                        # Correct channel names according to the pid of the\n                        # process\n                        channels = [\"{}_{}\".format(i, p.pid) for i in\n                                    p.compiler[c]]\n                        c_info[\"channels\"].extend(channels)\n\n            # If one ore more channels were detected, establish connections\n            # and append compiler to the process list.\n            if c_info[\"channels\"]:\n                compiler_cls.set_compiler_channels(c_info[\"channels\"],\n                                                   operator=\"join\")\n                self.processes.append(compiler_cls)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _set_status_channels(self):\n\n        status_inst = pc.StatusCompiler(template=\"status_compiler\")\n        report_inst = pc.ReportCompiler(template=\"report_compiler\")\n\n        # Compile status channels from pipeline process\n        status_channels = []\n        for p in [p for p in self.processes]:\n            if not any([isinstance(p, x) for x in self.skip_class]):\n\n                status_channels.extend(p.status_strs)\n\n        if not status_channels:\n            logger.debug(\"No status channels found. Skipping status compiler\"\n                         \"process\")\n            return\n\n        logger.debug(\"Setting status channels: {}\".format(status_channels))\n\n        # Check for duplicate channels. Raise exception if found.\n        if len(status_channels) != len(set(status_channels)):\n            raise eh.ProcessError(\n                \"Duplicate status channels detected. Please ensure that \"\n                \"the 'status_channels' attributes of each process are \"\n                \"unique. Here are the status channels:\\n\\n{}\".format(\n                    \", \".join(status_channels)\n                ))\n\n        status_inst.set_compiler_channels(status_channels)\n\n        report_channels = [\"REPORT_{}\".format(x.lstrip(\"STATUS_\")) for x in\n                           status_channels]\n\n        report_inst.set_compiler_channels(report_channels)\n\n        self.processes.extend([status_inst, report_inst])", "response": "Sets the status channels for the status compiler process and reports process."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_resources_string(res_dict, pid):\n\n        config_str = \"\"\n        ignore_directives = [\"container\", \"version\"]\n\n        for p, directives in res_dict.items():\n\n            for d, val in directives.items():\n\n                if d in ignore_directives:\n                    continue\n\n                config_str += '\\n\\t${}_{}.{} = {}'.format(p, pid, d, val)\n\n        return config_str", "response": "Returns the nextflow resources string from a dictionary with the resources for processes and the process ID."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_container_string(cont_dict, pid):\n\n        config_str = \"\"\n\n        for p, directives in cont_dict.items():\n\n            container = \"\"\n\n            if \"container\" in directives:\n                container += directives[\"container\"]\n\n                if \"version\" in directives:\n                    container += \":{}\".format(directives[\"version\"])\n                else:\n                    container += \":latest\"\n\n            if container:\n                config_str += '\\n\\t${}_{}.container = \"{}\"'.format(p, pid, container)\n\n        return config_str", "response": "Returns the nextflow containers string from a dictionary object containing the process names and the container directives."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the nextflow params string from a dictionary object.", "response": "def _get_params_string(self):\n        \"\"\"Returns the nextflow params string from a dictionary object.\n\n        The params dict should be a set of key:value pairs with the\n        parameter name, and the default parameter value::\n\n            self.params = {\n                \"genomeSize\": 2.1,\n                \"minCoverage\": 15\n            }\n\n        The values are then added to the string as they are. For instance,\n        a ``2.1`` float will appear as ``param = 2.1`` and a\n        ``\"'teste'\" string will appear as ``param = 'teste'`` (Note the\n        string).\n\n        Returns\n        -------\n        str\n            Nextflow params configuration string\n        \"\"\"\n\n        params_str = \"\"\n\n        for p in self.processes:\n\n            logger.debug(\"[{}] Adding parameters: {}\\n\".format(\n                p.template, p.params)\n            )\n\n            # Add an header with the template name to structure the params\n            # configuration\n            if p.params and p.template != \"init\":\n\n                p.set_param_id(\"_{}\".format(p.pid))\n                params_str += \"\\n\\t/*\"\n                params_str += \"\\n\\tComponent '{}_{}'\\n\".format(p.template,\n                                                               p.pid)\n                params_str += \"\\t{}\\n\".format(\"-\" * (len(p.template) + len(p.pid) + 12))\n                params_str += \"\\t*/\\n\"\n\n            for param, val in p.params.items():\n\n                if p.template == \"init\":\n                    param_id = param\n                else:\n                    param_id = \"{}_{}\".format(param, p.pid)\n\n                params_str += \"\\t{} = {}\\n\".format(param_id, val[\"default\"])\n\n        return params_str"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the merged nextflow params string from a dictionary object.", "response": "def _get_merged_params_string(self):\n        \"\"\"Returns the merged nextflow params string from a dictionary object.\n\n        The params dict should be a set of key:value pairs with the\n        parameter name, and the default parameter value::\n\n            self.params = {\n                \"genomeSize\": 2.1,\n                \"minCoverage\": 15\n            }\n\n        The values are then added to the string as they are. For instance,\n        a ``2.1`` float will appear as ``param = 2.1`` and a\n        ``\"'teste'\" string will appear as ``param = 'teste'`` (Note the\n        string).\n\n        Identical parameters in multiple processes will be merged into the same\n        param.\n\n        Returns\n        -------\n        str\n            Nextflow params configuration string\n        \"\"\"\n\n        params_temp = {}\n\n        for p in self.processes:\n\n            logger.debug(\"[{}] Adding parameters: {}\".format(p.template,\n                                                             p.params))\n            for param, val in p.params.items():\n\n                params_temp[param] = val[\"default\"]\n\n        config_str = \"\\n\\t\" + \"\\n\\t\".join([\n            \"{} = {}\".format(param, val) for param, val in params_temp.items()\n        ])\n\n        return config_str"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_manifest_string(self):\n\n        config_str = \"\"\n\n        config_str += '\\n\\tname = \"{}\"'.format(self.pipeline_name)\n        config_str += '\\n\\tmainScript = \"{}\"'.format(self.nf_file)\n\n        return config_str", "response": "Returns the nextflow manifest config string to include in the\n            Nextflow manifest config file from the information on the pipeline."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _set_configurations(self):\n\n        logger.debug(\"======================\")\n        logger.debug(\"Setting configurations\")\n        logger.debug(\"======================\")\n\n        resources = \"\"\n        containers = \"\"\n        params = \"\"\n        manifest = \"\"\n\n        if self.merge_params:\n            params += self._get_merged_params_string()\n            help_list = self._get_merged_params_help()\n        else:\n            params += self._get_params_string()\n            help_list = self._get_params_help()\n\n        for p in self.processes:\n\n            # Skip processes with the directives attribute populated\n            if not p.directives:\n                continue\n\n            logger.debug(\"[{}] Adding directives: {}\".format(\n                p.template, p.directives))\n            resources += self._get_resources_string(p.directives, p.pid)\n            containers += self._get_container_string(p.directives, p.pid)\n\n        manifest = self._get_manifest_string()\n\n        self.resources = self._render_config(\"resources.config\", {\n            \"process_info\": resources\n        })\n        self.containers = self._render_config(\"containers.config\", {\n            \"container_info\": containers\n        })\n        self.params = self._render_config(\"params.config\", {\n            \"params_info\": params\n        })\n        self.manifest = self._render_config(\"manifest.config\", {\n            \"manifest_info\": manifest\n        })\n        self.help = self._render_config(\"Helper.groovy\", {\n            \"nf_file\": basename(self.nf_file),\n            \"help_list\": help_list,\n            \"version\": __version__,\n            \"pipeline_name\": \" \".join([x.upper() for x in self.pipeline_name])\n        })\n        self.user_config = self._render_config(\"user.config\", {})", "response": "This method will iterate over all processes and populate the nextflow configuration files with the directives that are set in the pipeline and the process configuration files with the parameters and the help list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite dag to file", "response": "def dag_to_file(self, dict_viz, output_file=\".treeDag.json\"):\n        \"\"\"Writes dag to output file\n\n        Parameters\n        ----------\n        dict_viz: dict\n            Tree like dictionary that is used to export tree data of processes\n            to html file and here for the dotfile .treeDag.json\n\n        \"\"\"\n\n        outfile_dag = open(os.path.join(dirname(self.nf_file), output_file)\n                           , \"w\")\n        outfile_dag.write(json.dumps(dict_viz))\n        outfile_dag.close()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting the pipeline attributes to json file and a graphical output showing the DAG.", "response": "def render_pipeline(self):\n        \"\"\"Write pipeline attributes to json\n\n        This function writes the pipeline and their attributes to a json file,\n        that is intended to be read by resources/pipeline_graph.html to render\n        a graphical output showing the DAG.\n\n        \"\"\"\n\n        dict_viz = {\n            \"name\": \"root\",\n            \"children\": []\n        }\n        last_of_us = {}\n\n        f_tree = self._fork_tree if self._fork_tree else {1: [1]}\n\n        for x, (k, v) in enumerate(f_tree.items()):\n            for p in self.processes[1:]:\n\n                if x == 0 and p.lane not in [k] + v:\n                    continue\n\n                if x > 0 and p.lane not in v:\n                    continue\n\n                if not p.parent_lane:\n                    lst = dict_viz[\"children\"]\n                else:\n                    lst = last_of_us[p.parent_lane]\n\n                tooltip = {\n                    \"name\": \"{}_{}\".format(p.template, p.pid),\n                    \"process\": {\n                        \"pid\": p.pid,\n                        \"input\": p.input_type,\n                        \"output\": p.output_type if p.output_type else \"None\",\n                        \"lane\": p.lane,\n                    },\n                    \"children\": []\n                }\n\n                dir_var = \"\"\n                for k2, v2 in p.directives.items():\n                    dir_var += k2\n                    for d in v2:\n                        try:\n                            # Remove quotes from string directives\n                            directive = v2[d].replace(\"'\", \"\").replace('\"', '') \\\n                                if isinstance(v2[d], str) else v2[d]\n                            dir_var += \"{}: {}\".format(d, directive)\n                        except KeyError:\n                            pass\n\n                if dir_var:\n                    tooltip[\"process\"][\"directives\"] = dir_var\n                else:\n                    tooltip[\"process\"][\"directives\"] = \"N/A\"\n\n                lst.append(tooltip)\n\n                last_of_us[p.lane] = lst[-1][\"children\"]\n\n        # write to file dict_viz\n        self.dag_to_file(dict_viz)\n\n        # Write tree forking information for dotfile\n        with open(os.path.join(dirname(self.nf_file),\n                               \".forkTree.json\"), \"w\") as fh:\n            fh.write(json.dumps(self._fork_tree))\n\n        # send with jinja to html resource\n        return self._render_config(\"pipeline_graph.html\", {\"data\": dict_viz})"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_configs(self, project_root):\n\n        # Write resources config\n        with open(join(project_root, \"resources.config\"), \"w\") as fh:\n            fh.write(self.resources)\n\n        # Write containers config\n        with open(join(project_root, \"containers.config\"), \"w\") as fh:\n            fh.write(self.containers)\n\n        # Write containers config\n        with open(join(project_root, \"params.config\"), \"w\") as fh:\n            fh.write(self.params)\n\n        # Write manifest config\n        with open(join(project_root, \"manifest.config\"), \"w\") as fh:\n            fh.write(self.manifest)\n\n        # Write user config if not present in the project directory\n        if not exists(join(project_root, \"user.config\")):\n            with open(join(project_root, \"user.config\"), \"w\") as fh:\n                fh.write(self.user_config)\n\n        lib_dir = join(project_root, \"lib\")\n        if not exists(lib_dir):\n            os.makedirs(lib_dir)\n        with open(join(lib_dir, \"Helper.groovy\"), \"w\") as fh:\n            fh.write(self.help)\n\n        # Generate the pipeline DAG\n        pipeline_to_json = self.render_pipeline()\n        with open(splitext(self.nf_file)[0] + \".html\", \"w\") as fh:\n            fh.write(pipeline_to_json)", "response": "Wrapper method that writes all configuration files to the pipeline\n        directory"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef export_params(self):\n\n        params_json = {}\n\n        # Skip first init process\n        for p in self.processes[1:]:\n            params_json[p.template] = p.params\n\n        # Flush params json to stdout\n        sys.stdout.write(json.dumps(params_json))", "response": "Export the pipeline params as a JSON to stdout"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef export_directives(self):\n\n        directives_json = {}\n\n        # Skip first init process\n        for p in self.processes[1:]:\n            directives_json[p.template] = p.directives\n\n        # Flush params json to stdout\n        sys.stdout.write(json.dumps(directives_json))", "response": "Export pipeline directives as a JSON to stdout\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fetch_docker_tags(self):\n\n        # dict to store the already parsed components (useful when forks are\n        # given to the pipeline string via -t flag\n        dict_of_parsed = {}\n\n        # fetches terminal width and subtracts 3 because we always add a\n        # new line character and we want a space at the beggining and at the end\n        # of each line\n        terminal_width = shutil.get_terminal_size().columns - 3\n\n        # first header\n        center_string = \" Selected container tags \"\n\n        # starts a list with the headers\n        tags_list = [\n            [\n                \"=\" * int(terminal_width / 4),\n                \"{0}{1}{0}\".format(\n                    \"=\" * int(((terminal_width/2 - len(center_string)) / 2)),\n                    center_string)\n                ,\n                \"{}\\n\".format(\"=\" * int(terminal_width / 4))\n            ],\n            [\"component\", \"container\", \"tags\"],\n            [\n                \"=\" * int(terminal_width / 4),\n                \"=\" * int(terminal_width / 2),\n                \"=\" * int(terminal_width / 4)\n            ]\n        ]\n\n        # Skip first init process and iterate through the others\n        for p in self.processes[1:]:\n            template = p.template\n            # if component has already been printed then skip and don't print\n            # again\n            if template in dict_of_parsed:\n                continue\n\n            # starts a list of  containers for the current process in\n            # dict_of_parsed, in which each containers will be added to this\n            # list once it gets parsed\n            dict_of_parsed[template] = {\n                \"container\": []\n            }\n\n            # fetch repo name from directives of each component.\n            for directives in p.directives.values():\n                try:\n                    repo = directives[\"container\"]\n                    default_version = directives[\"version\"]\n                except KeyError:\n                    # adds the default container if container key isn't present\n                    # this happens for instance in integrity_coverage\n                    repo = \"flowcraft/flowcraft_base\"\n                    default_version = \"1.0.0-1\"\n                # checks if repo_version already exists in list of the\n                # containers for the current component being queried\n                repo_version = repo + default_version\n                if repo_version not in dict_of_parsed[template][\"container\"]:\n                    # make the request to docker hub\n                    r = requests.get(\n                        \"https://hub.docker.com/v2/repositories/{}/tags/\"\n                        .format(repo)\n                    )\n                    # checks the status code of the request, if it is 200 then\n                    # parses docker hub entry, otherwise retrieve no tags but\n                    # alerts the user\n                    if r.status_code != 404:\n                        # parse response content to dict and fetch results key\n                        r_content = json.loads(r.content)[\"results\"]\n                        for version in r_content:\n                            printed_version = (version[\"name\"] + \"*\") \\\n                                if version[\"name\"] == default_version \\\n                                else version[\"name\"]\n                            tags_list.append([template, repo, printed_version])\n                    else:\n                        tags_list.append([template, repo, \"No DockerHub tags\"])\n\n                dict_of_parsed[template][\"container\"].append(repo_version)\n\n        # iterate through each entry in tags_list and print the list of tags\n        # for each component. Each entry (excluding the headers) contains\n        # 3 elements (component name, container and tag version)\n        for x, entry in enumerate(tags_list):\n            # adds different color to the header in the first list and\n            # if row is pair add one color and if is even add another (different\n            # background)\n            color = \"blue_bold\" if x < 3 else \\\n                (\"white\" if x % 2 != 0 else \"0;37;40m\")\n            # generates a small list with the terminal width for each column,\n            # this will be given to string formatting as the 3, 4 and 5 element\n            final_width = [\n                int(terminal_width/4),\n                int(terminal_width/2),\n                int(terminal_width/4)\n            ]\n            # writes the string to the stdout\n            sys.stdout.write(\n                colored_print(\"\\n {0: <{3}} {1: ^{4}} {2: >{5}}\".format(\n                    *entry, *final_width), color)\n            )\n        # assures that the entire line gets the same color\n        sys.stdout.write(\"\\n{0: >{1}}\\n\".format(\"(* = default)\",\n                                                terminal_width + 3))", "response": "Returns a dict of all dockerhub tags associated with each component given by the - t flag."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build(self):\n\n        logger.info(colored_print(\n            \"\\tSuccessfully connected {} process(es) with {} \"\n            \"fork(s) across {} lane(s) \\u2713\".format(\n                len(self.processes[1:]), len(self._fork_tree), self.lanes)))\n\n        # Generate regular nextflow header that sets up the shebang, imports\n        # and all possible initial channels\n        self._build_header()\n\n        self._set_channels()\n\n        self._set_init_process()\n\n        self._set_secondary_channels()\n\n        logger.info(colored_print(\n            \"\\tSuccessfully set {} secondary channel(s) \\u2713\".format(\n                len(self.secondary_channels))))\n\n        self._set_compiler_channels()\n\n        self._set_configurations()\n\n        logger.info(colored_print(\n            \"\\tFinished configurations \\u2713\"))\n\n        for p in self.processes:\n            self.template += \"\\n{}\".format(p.template_str)\n\n        self._build_footer()\n\n        project_root = dirname(self.nf_file)\n\n        # Write configs\n        self.write_configs(project_root)\n\n        # Write pipeline file\n        with open(self.nf_file, \"w\") as fh:\n            fh.write(self.template)\n\n        logger.info(colored_print(\n            \"\\tPipeline written into {} \\u2713\".format(self.nf_file)))", "response": "This method builds the main pipeline and writes the code to the nextflow file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the k - mer range of the current sample.", "response": "def set_kmers(kmer_opt, max_read_len):\n    \"\"\"Returns a kmer list based on the provided kmer option and max read len.\n\n    Parameters\n    ----------\n    kmer_opt : str\n        The k-mer option. Can be either ``'auto'``, ``'default'`` or a\n        sequence of space separated integers, ``'23, 45, 67'``.\n    max_read_len : int\n        The maximum read length of the current sample.\n\n    Returns\n    -------\n    kmers : list\n        List of k-mer values that will be provided to Spades.\n\n    \"\"\"\n\n    logger.debug(\"Kmer option set to: {}\".format(kmer_opt))\n\n    # Check if kmer option is set to auto\n    if kmer_opt == \"auto\":\n\n        if max_read_len >= 175:\n            kmers = [55, 77, 99, 113, 127]\n        else:\n            kmers = [21, 33, 55, 67, 77]\n\n        logger.debug(\"Kmer range automatically selected based on max read\"\n                     \"length of {}: {}\".format(max_read_len, kmers))\n\n    # Check if manual kmers were specified\n    elif len(kmer_opt.split()) > 1:\n\n        kmers = kmer_opt.split()\n        logger.debug(\"Kmer range manually set to: {}\".format(kmers))\n\n    else:\n\n        kmers = []\n        logger.debug(\"Kmer range set to empty (will be automatically \"\n                     \"determined by SPAdes\")\n\n    return kmers"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main(sample_id, fastq_pair, max_len, kmer, clear):\n\n    logger.info(\"Starting spades\")\n\n    logger.info(\"Setting SPAdes kmers\")\n    kmers = set_kmers(kmer, max_len)\n    logger.info(\"SPAdes kmers set to: {}\".format(kmers))\n\n    cli = [\n        \"metaspades.py\",\n        \"--only-assembler\",\n        \"--threads\",\n        \"$task.cpus\",\n        \"-o\",\n        \".\"\n    ]\n\n    # Add kmers, if any were specified\n    if kmers:\n        cli += [\"-k {}\".format(\",\".join([str(x) for x in kmers]))]\n\n    # Add FastQ files\n    cli += [\n        \"-1\",\n        fastq_pair[0],\n        \"-2\",\n        fastq_pair[1]\n    ]\n\n    logger.debug(\"Running metaSPAdes subprocess with command: {}\".format(cli))\n\n    p = subprocess.Popen(cli, stdout=PIPE, stderr=PIPE)\n    stdout, stderr = p.communicate()\n\n    # Attempt to decode STDERR output from bytes. If unsuccessful, coerce to\n    # string\n    try:\n        stderr = stderr.decode(\"utf8\")\n        stdout = stdout.decode(\"utf8\")\n    except (UnicodeDecodeError, AttributeError):\n        stderr = str(stderr)\n        stdout = str(stdout)\n\n    logger.info(\"Finished metaSPAdes subprocess with STDOUT:\\\\n\"\n                \"======================================\\\\n{}\".format(stdout))\n    logger.info(\"Fished metaSPAdes subprocesswith STDERR:\\\\n\"\n                \"======================================\\\\n{}\".format(stderr))\n    logger.info(\"Finished metaSPAdes with return code: {}\".format(\n        p.returncode))\n\n    with open(\".status\", \"w\") as fh:\n        if p.returncode != 0:\n            fh.write(\"error\")\n            return\n        else:\n            fh.write(\"pass\")\n\n    # Change the default contigs.fasta assembly name to a more informative one\n    if \"_trim.\" in fastq_pair[0]:\n        sample_id += \"_trim\"\n\n    assembly_file = \"{}_metaspades.fasta\".format(\n        sample_id)\n    os.rename(\"contigs.fasta\", assembly_file)\n    logger.info(\"Setting main assembly file to: {}\".format(assembly_file))\n\n    # Remove input fastq files when clear option is specified.\n    # Only remove temporary input when the expected output exists.\n    if clear == \"true\" and os.path.exists(assembly_file):\n        clean_up(fastq_pair)", "response": "Main function of the metaSPAdes script."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_report_id(self):\n\n        if self.watch:\n\n            # Searches for the first occurence of the nextflow pipeline\n            # file name in the .nextflow.log file\n            pipeline_path = get_nextflow_filepath(self.log_file)\n\n            # Get hash from the entire pipeline file\n            pipeline_hash = hashlib.md5()\n            with open(pipeline_path, \"rb\") as fh:\n                for chunk in iter(lambda: fh.read(4096), b\"\"):\n                    pipeline_hash.update(chunk)\n            # Get hash from the current working dir and hostname\n            workdir = os.getcwd().encode(\"utf8\")\n            hostname = socket.gethostname().encode(\"utf8\")\n            hardware_addr = str(uuid.getnode()).encode(\"utf8\")\n            dir_hash = hashlib.md5(workdir + hostname + hardware_addr)\n\n            return pipeline_hash.hexdigest() + dir_hash.hexdigest()\n\n        else:\n            with open(self.report_file) as fh:\n                report_json = json.loads(fh.read())\n\n            metadata = report_json[\"data\"][\"results\"][0][\"nfMetadata\"]\n\n            try:\n                report_id = metadata[\"scriptId\"] + metadata[\"sessionId\"]\n            except KeyError:\n                raise eh.ReportError(\"Incomplete or corrupt report JSON file \"\n                                     \"missing the 'scriptId' and/or 'sessionId' \"\n                                     \"metadata information\")\n\n            return report_id", "response": "Returns a hash of the reports JSON file containing the scriptId and sessionId."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates the status of the pipeline.", "response": "def _update_pipeline_status(self):\n        \"\"\"\n        Parses the .nextflow.log file for signatures of pipeline status and sets\n        the :attr:`status_info` attribute.\n        \"\"\"\n\n        prev_status = self.status_info\n\n        with open(self.log_file) as fh:\n\n            for line in fh:\n\n                if \"Session aborted\" in line:\n                    self.status_info = \"aborted\"\n                    self.send = True if prev_status != self.status_info \\\n                        else self.send\n                    return\n\n                if \"Execution complete -- Goodbye\" in line:\n                    self.status_info = \"complete\"\n                    self.send = True if prev_status != self.status_info \\\n                        else self.send\n                    return\n\n            self.status_info = \"running\"\n            self.send = True if prev_status != self.status_info \\\n                else self.send"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_trace_watch(self):\n\n        # Check the size stamp of the tracefile. Only proceed with the parsing\n        # if it changed from the previous size.\n        size_stamp = os.path.getsize(self.trace_file)\n        self.trace_retry = 0\n        if size_stamp and size_stamp == self.trace_sizestamp:\n            return\n        else:\n            logger.debug(\"Updating trace size stamp to: {}\".format(size_stamp))\n            self.trace_sizestamp = size_stamp\n\n        with open(self.trace_file) as fh:\n\n            # Skip potential empty lines at the start of file\n            header = next(fh).strip()\n            while not header:\n                header = next(fh).strip()\n\n            # Get header mappings before parsing the file\n            hm = self._header_mapping(header)\n\n            for line in fh:\n                # Skip empty lines\n                if line.strip() == \"\":\n                    continue\n\n                fields = line.strip().split(\"\\t\")\n\n                # Skip if task ID was already processes\n                if fields[hm[\"task_id\"]] in self.stored_ids:\n                    continue\n\n                if fields[hm[\"process\"]] == \"report\":\n                    self.report_queue.append(\n                        self._expand_path(fields[hm[\"hash\"]])\n                    )\n                    self.send = True\n\n                # Add the processed trace line to the stored ids. It will be\n                # skipped in future parsers\n                self.stored_ids.append(fields[hm[\"task_id\"]])", "response": "Parses the nextflow trace file and retrieves the path of report JSON\n        files that have not been sent to the service yet."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_log_watch(self):\n\n        # Check the size stamp of the tracefile. Only proceed with the parsing\n        # if it changed from the previous size.\n        size_stamp = os.path.getsize(self.log_file)\n        self.trace_retry = 0\n        if size_stamp and size_stamp == self.log_sizestamp:\n            return\n        else:\n            logger.debug(\"Updating log size stamp to: {}\".format(size_stamp))\n            self.log_sizestamp = size_stamp\n\n        self._update_pipeline_status()", "response": "Parses nextflow log file and updates the pipeline status"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _send_live_report(self, report_id):\n\n        # Determines the maximum number of reports sent at the same time in\n        # the same payload\n        buffer_size = 100\n        logger.debug(\"Report buffer size set to: {}\".format(buffer_size))\n\n        for i in range(0, len(self.report_queue), buffer_size):\n\n            # Reset the report compilation batch\n            reports_compilation = []\n\n            # Iterate over report JSON batches determined by buffer_size\n            for report in self.report_queue[i: i + buffer_size]:\n                try:\n                    report_file = [x for x in os.listdir(report)\n                                   if x.endswith(\".json\")][0]\n                except IndexError:\n                    continue\n                with open(join(report, report_file)) as fh:\n                    reports_compilation.append(json.loads(fh.read()))\n\n            logger.debug(\"Payload sent with size: {}\".format(\n                asizeof(json.dumps(reports_compilation))\n            ))\n            logger.debug(\"status: {}\".format(self.status_info))\n\n            try:\n                requests.put(\n                    self.broadcast_address,\n                    json={\"run_id\": report_id,\n                          \"report_json\": reports_compilation,\n                          \"status\": self.status_info}\n                )\n            except requests.exceptions.ConnectionError:\n                logger.error(colored_print(\n                    \"ERROR: Could not establish connection with server. The server\"\n                    \" may be down or there is a problem with your internet \"\n                    \"connection.\", \"red_bold\"))\n                sys.exit(1)\n\n        # When there is no change in the report queue, but there is a change\n        # in the run status of the pipeline\n        if not self.report_queue:\n\n            logger.debug(\"status: {}\".format(self.status_info))\n\n            try:\n                requests.put(\n                    self.broadcast_address,\n                    json={\"run_id\": report_id,\n                          \"report_json\": [],\n                          \"status\": self.status_info}\n                )\n            except requests.exceptions.ConnectionError:\n                logger.error(colored_print(\n                    \"ERROR: Could not establish connection with server. The\"\n                    \" server may be down or there is a problem with your \"\n                    \"internet connection.\", \"red_bold\"))\n                sys.exit(1)\n\n        # Reset the report queue after sending the request\n        self.report_queue = []", "response": "Sends a PUT request to the report queue attribute."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _init_live_reports(self, report_id):\n\n        logger.debug(\"Sending initial POST request to {} to start report live\"\n                     \" update\".format(self.broadcast_address))\n\n        try:\n            with open(\".metadata.json\") as fh:\n                metadata = [json.load(fh)]\n        except:\n            metadata = []\n\n        start_json = {\n            \"data\": {\"results\": metadata}\n        }\n\n        try:\n            requests.post(\n                self.broadcast_address,\n                json={\"run_id\": report_id, \"report_json\": start_json,\n                      \"status\": self.status_info}\n            )\n        except requests.exceptions.ConnectionError:\n            logger.error(colored_print(\n                \"ERROR: Could not establish connection with server. The server\"\n                \" may be down or there is a problem with your internet \"\n                \"connection.\", \"red_bold\"))\n            sys.exit(1)", "response": "Sends a POST request to initialize the live reports"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _close_connection(self, report_id):\n\n        logger.debug(\n            \"Closing connection and sending DELETE request to {}\".format(\n                self.broadcast_address))\n\n        try:\n            r = requests.delete(self.broadcast_address,\n                                json={\"run_id\": report_id})\n            if r.status_code != 202:\n                logger.error(colored_print(\n                    \"ERROR: There was a problem sending data to the server\"\n                    \"with reason: {}\".format(r.reason)))\n        except requests.exceptions.ConnectionError:\n            logger.error(colored_print(\n                \"ERROR: Could not establish connection with server. The server\"\n                \" may be down or there is a problem with your internet \"\n                \"connection.\", \"red_bold\"))\n            sys.exit(1)", "response": "Sends a DELETE request to the server and closes the connection."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convert_adatpers(adapter_fasta):\n\n    adapter_out = \"fastqc_adapters.tab\"\n    logger.debug(\"Setting output adapters file to: {}\".format(adapter_out))\n\n    try:\n\n        with open(adapter_fasta) as fh, \\\n                open(adapter_out, \"w\") as adap_fh:\n\n            for line in fh:\n                if line.startswith(\">\"):\n\n                    head = line[1:].strip()\n                    # Get the next line with the sequence string\n                    sequence = next(fh).strip()\n\n                    adap_fh.write(\"{}\\\\t{}\\\\n\".format(head, sequence))\n\n        logger.info(\"Converted adapters file\")\n\n        return adapter_out\n\n    # If an invalid adapters file is provided, return None.\n    except FileNotFoundError:\n        logger.warning(\"Could not find the provided adapters file: {}\".format(\n            adapter_fasta))\n        return", "response": "Converts a single adapter file from a FASTA file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending dictionary to output json file This function sends master_dict dictionary to a json file if master_dict is populated with entries, otherwise it won't create the file Parameters ---------- master_dict: dict dictionary that stores all entries for a specific query sequence in multi-fasta given to mash dist as input against patlas database last_seq: str string that stores the last sequence that was parsed before writing to file and therefore after the change of query sequence between different rows on the input file mash_output: str the name/path of input file to main function, i.e., the name/path of the mash dist output txt file. sample_id: str The name of the sample being parse to .report.json file Returns -------", "response": "def send_to_output(master_dict, mash_output, sample_id, assembly_file):\n    \"\"\"Send dictionary to output json file\n    This function sends master_dict dictionary to a json file if master_dict is\n    populated with entries, otherwise it won't create the file\n\n    Parameters\n    ----------\n    master_dict: dict\n        dictionary that stores all entries for a specific query sequence\n        in multi-fasta given to mash dist as input against patlas database\n    last_seq: str\n        string that stores the last sequence that was parsed before writing to\n        file and therefore after the change of query sequence between different\n        rows on the input file\n    mash_output: str\n        the name/path of input file to main function, i.e., the name/path of\n        the mash dist output txt file.\n    sample_id: str\n        The name of the sample being parse to .report.json file\n\n    Returns\n    -------\n\n    \"\"\"\n\n    plot_dict = {}\n\n    # create a new file only if master_dict is populated\n    if master_dict:\n        out_file = open(\"{}.json\".format(\n            \"\".join(mash_output.split(\".\")[0])), \"w\")\n        out_file.write(json.dumps(master_dict))\n        out_file.close()\n\n        # iterate through master_dict in order to make contigs the keys\n        for k,v in master_dict.items():\n            if not v[2] in plot_dict:\n                plot_dict[v[2]] = [k]\n            else:\n                plot_dict[v[2]].append(k)\n\n        number_hits = len(master_dict)\n    else:\n        number_hits = 0\n\n    json_dic = {\n        \"tableRow\": [{\n            \"sample\": sample_id,\n            \"data\": [{\n                \"header\": \"Mash Dist\",\n                \"table\": \"plasmids\",\n                \"patlas_mashdist\": master_dict,\n                \"value\": number_hits\n            }]\n        }],\n        \"plotData\": [{\n            \"sample\": sample_id,\n            \"data\": {\n                \"patlasMashDistXrange\": plot_dict\n            },\n            \"assemblyFile\": assembly_file\n        }]\n    }\n\n    with open(\".report.json\", \"w\") as json_report:\n        json_report.write(json.dumps(json_dic, separators=(\",\", \":\")))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite the versions JSON for a template file.", "response": "def build_versions(self):\n        \"\"\"Writes versions JSON for a template file\n\n        This method creates the JSON file ``.versions`` based on the metadata\n        and specific functions that are present in a given template script.\n\n        It starts by fetching the template metadata, which can be specified\n        via the ``__version__``, ``__template__`` and ``__build__``\n        attributes. If all of these attributes exist, it starts to populate\n        a JSON/dict array (Note that the absence of any one of them will\n        prevent the version from being written).\n\n        Then, it will search the\n        template scope for functions that start with the substring\n        ``__set_version`` (For example ``def __set_version_fastqc()`).\n        These functions should gather the version of\n        an arbitrary program and return a JSON/dict object with the following\n        information::\n\n            {\n                \"program\": <program_name>,\n                \"version\": <version>\n                \"build\": <build>\n            }\n\n        This JSON/dict object is then written in the ``.versions`` file.\n        \"\"\"\n\n        version_storage = []\n\n        template_version = self.context.get(\"__version__\", None)\n        template_program = self.context.get(\"__template__\", None)\n        template_build = self.context.get(\"__build__\", None)\n\n        if template_version and template_program and template_build:\n            if self.logger:\n                self.logger.debug(\"Adding template version: {}; {}; \"\n                                  \"{}\".format(template_program,\n                                              template_version,\n                                              template_build))\n            version_storage.append({\n                \"program\": template_program,\n                \"version\": template_version,\n                \"build\": template_build\n            })\n\n        for var, obj in self.context.items():\n            if var.startswith(\"__get_version\"):\n                ver = obj()\n                version_storage.append(ver)\n                if self.logger:\n                    self.logger.debug(\"Found additional software version\"\n                                      \"{}\".format(ver))\n\n        with open(\".versions\", \"w\") as fh:\n            fh.write(json.dumps(version_storage, separators=(\",\", \":\")))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting top results from mash screen txt output to json format Parameters ---------- mash_output: str this is a string that stores the path to this file, i.e, the name of the file sample_id: str sample name", "response": "def main(mash_output, sample_id):\n    '''\n    converts top results from mash screen txt output to json format\n\n    Parameters\n    ----------\n    mash_output: str\n        this is a string that stores the path to this file, i.e, the name of\n        the file\n    sample_id: str\n        sample name\n\n    '''\n    logger.info(\"Reading file : {}\".format(mash_output))\n    read_mash_output = open(mash_output)\n\n    dic = {}\n    median_list = []\n    filtered_dic = {}\n\n    logger.info(\"Generating dictionary and list to pre-process the final json\")\n    for line in read_mash_output:\n        tab_split = line.split(\"\\t\")\n        identity = tab_split[0]\n        # shared_hashes = tab_split[1]\n        median_multiplicity = tab_split[2]\n        # p_value = tab_split[3]\n        query_id = tab_split[4]\n        # query-comment should not exist here and it is irrelevant\n\n        # here identity is what in fact interests to report to json but\n        # median_multiplicity also is important since it gives an rough\n        # estimation of the coverage depth for each plasmid.\n        # Plasmids should have higher coverage depth due to their increased\n        # copy number in relation to the chromosome.\n        dic[query_id] = [identity, median_multiplicity]\n        median_list.append(float(median_multiplicity))\n\n    output_json = open(\" \".join(mash_output.split(\".\")[:-1]) + \".json\", \"w\")\n\n    # median cutoff is twice the median of all median_multiplicity values\n    # reported by mash screen. In the case of plasmids, since the database\n    # has 9k entries and reads shouldn't have that many sequences it seems ok...\n    if len(median_list) > 0:\n        # this statement assures that median_list has indeed any entries\n        median_cutoff = median(median_list)\n        logger.info(\"Generating final json to dump to a file\")\n        for k, v in dic.items():\n            # estimated copy number\n            copy_number = int(float(v[1]) / median_cutoff)\n            # assure that plasmid as at least twice the median coverage depth\n            if float(v[1]) > median_cutoff:\n                filtered_dic[\"_\".join(k.split(\"_\")[0:3])] = [\n                    round(float(v[0]),2),\n                    copy_number\n                ]\n        logger.info(\n            \"Exported dictionary has {} entries\".format(len(filtered_dic)))\n    else:\n        # if no entries were found raise an error\n        logger.error(\"No matches were found using mash screen for the queried reads\")\n\n    output_json.write(json.dumps(filtered_dic))\n    output_json.close()\n\n    json_dic = {\n        \"tableRow\": [{\n            \"sample\": sample_id,\n            \"data\": [{\n                \"header\": \"Mash Screen\",\n                \"table\": \"plasmids\",\n                \"patlas_mashscreen\": filtered_dic,\n                \"value\": len(filtered_dic)\n            }]\n        }],\n    }\n\n    with open(\".report.json\", \"w\") as json_report:\n        json_report.write(json.dumps(json_dic, separators=(\",\", \":\")))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef procs_dict_parser(procs_dict):\n\n    logger.info(colored_print(\n        \"\\n===== L I S T   O F   P R O C E S S E S =====\\n\", \"green_bold\"))\n\n    #Sort to print alphabetically ordered list of processes to ease reading\n    procs_dict_ordered = {k: procs_dict[k] for k in sorted(procs_dict)}\n\n    for template, dict_proc_info in procs_dict_ordered.items():\n        template_str = \"=> {}\".format(template)\n        logger.info(colored_print(template_str, \"blue_bold\"))\n\n        for info in dict_proc_info:\n            info_str = \"{}:\".format(info)\n\n            if isinstance(dict_proc_info[info], list):\n                if not dict_proc_info[info]:\n                    arg_msg = \"None\"\n                else:\n                    arg_msg = \", \".join(dict_proc_info[info])\n            elif info == \"directives\":\n                # this is used for the \"directives\", which is a dict\n                if not dict_proc_info[info]:\n                    # if dict is empty then add None to the message\n                    arg_msg = \"None\"\n                else:\n                    # otherwise fetch all template names within a component\n                    # and all the directives for each template to a list\n                    list_msg = [\"\\n      {}: {}\".format(\n                        templt,\n                        \" , \".join([\"{}: {}\".format(dr, val)\n                                    for dr, val in drs.items()]))\n                                for templt, drs in dict_proc_info[info].items()\n                    ]\n                    # write list to a str\n                    arg_msg = \"\".join(list_msg)\n            else:\n                arg_msg = dict_proc_info[info]\n\n            logger.info(\"   {} {}\".format(\n                colored_print(info_str, \"white_underline\"), arg_msg\n            ))", "response": "This function handles the dictionary of attributes of each Process class and prints to stdout lists of all the components that are used by the - t flag."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfunctions that collects all processes currently available and stores a dictionary of process class names and their arguments.", "response": "def proc_collector(process_map, args, pipeline_string):\n    \"\"\"\n    Function that collects all processes available and stores a dictionary of\n    the required arguments of each process class to be passed to\n    procs_dict_parser\n\n    Parameters\n    ----------\n    process_map: dict\n        The dictionary with the Processes currently available in flowcraft\n        and their corresponding classes as values\n    args: argparse.Namespace\n        The arguments passed through argparser that will be access to check the\n        type of list to be printed\n    pipeline_string: str\n        the pipeline string\n\n    \"\"\"\n\n    arguments_list = []\n\n    # prints a detailed list of the process class arguments\n    if args.detailed_list:\n        # list of attributes to be passed to proc_collector\n        arguments_list += [\n            \"input_type\",\n            \"output_type\",\n            \"description\",\n            \"dependencies\",\n            \"conflicts\",\n            \"directives\"\n        ]\n\n    # prints a short list with each process and the corresponding description\n    if args.short_list:\n        arguments_list += [\n            \"description\"\n        ]\n\n    if arguments_list:\n        # dict to store only the required entries\n        procs_dict = {}\n        # loops between all process_map Processes\n        for name, cls in process_map.items():\n\n            # instantiates each Process class\n            cls_inst = cls(template=name)\n\n            # checks if recipe is provided\n            if pipeline_string:\n                if name not in pipeline_string:\n                    continue\n\n            d = {arg_key: vars(cls_inst)[arg_key] for arg_key in\n                 vars(cls_inst) if arg_key in arguments_list}\n            procs_dict[name] = d\n\n        procs_dict_parser(procs_dict)\n\n        sys.exit(0)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef guess_file_compression(file_path, magic_dict=None):\n\n    if not magic_dict:\n        magic_dict = MAGIC_DICT\n\n    max_len = max(len(x) for x in magic_dict)\n\n    with open(file_path, \"rb\") as f:\n        file_start = f.read(max_len)\n\n    logger.debug(\"Binary signature start: {}\".format(file_start))\n\n    for magic, file_type in magic_dict.items():\n        if file_start.startswith(magic):\n            return file_type\n\n    return None", "response": "Guesses the compression of a given file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_qual_range(qual_str):\n\n    vals = [ord(c) for c in qual_str]\n\n    return min(vals), max(vals)", "response": "Returns the range of Unicode encode range for a given string of characters."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the list of all possible encodings for a given encoding range.", "response": "def get_encodings_in_range(rmin, rmax):\n    \"\"\" Returns the valid encodings for a given encoding range.\n\n    The encoding ranges are stored in the :py:data:`RANGES` dictionary, with\n    the encoding name as a string and a list as a value containing the\n    phred score and a tuple with the encoding range. For a given encoding\n    range provided via the two first arguments, this function will return\n    all possible encodings and phred scores.\n\n    Parameters\n    ----------\n    rmin : int\n        Minimum Unicode code in range.\n    rmax : int\n        Maximum Unicode code in range.\n\n    Returns\n    -------\n    valid_encodings : list\n        List of all possible encodings for the provided range.\n    valid_phred : list\n        List of all possible phred scores.\n\n    \"\"\"\n\n    valid_encodings = []\n    valid_phred = []\n\n    for encoding, (phred, (emin, emax)) in RANGES.items():\n        if rmin >= emin and rmax <= emax:\n            valid_encodings.append(encoding)\n            valid_phred.append(phred)\n\n    return valid_encodings, valid_phred"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a TSV file containing coverage information into objects.", "response": "def parse_coverage_table(coverage_file):\n    \"\"\"Parses a file with coverage information into objects.\n\n    This function parses a TSV file containing coverage results for\n    all contigs in a given assembly and will build an ``OrderedDict``\n    with the information about their coverage and length.  The length\n    information is actually gathered from the contig header using a\n    regular expression that assumes the usual header produced by Spades::\n\n        contig_len = int(re.search(\"length_(.+?)_\", line).group(1))\n\n    Parameters\n    ----------\n    coverage_file : str\n        Path to TSV file containing the coverage results.\n\n    Returns\n    -------\n    coverage_dict : OrderedDict\n        Contains the coverage and length information for each contig.\n    total_size : int\n        Total size of the assembly in base pairs.\n    total_cov : int\n        Sum of coverage values across all contigs.\n    \"\"\"\n\n    # Stores the correspondence between a contig and the corresponding coverage\n    # e.g.: {\"contig_1\": {\"cov\": 424} }\n    coverage_dict = OrderedDict()\n    # Stores the total coverage\n    total_cov = 0\n\n    with open(coverage_file) as fh:\n        for line in fh:\n            # Get contig and coverage\n            contig, cov = line.strip().split()\n            coverage_dict[contig] = {\"cov\": int(cov)}\n            # Add total coverage\n            total_cov += int(cov)\n            logger.debug(\"Processing contig '{}' with coverage '{}'\"\n                         \"\".format(contig, cov))\n\n    return coverage_dict, total_cov"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate a filtered assembly file. This function generates a filtered assembly file based on an original assembly and a minimum coverage threshold. Parameters ---------- assembly_file : str Path to original assembly file. minimum_coverage : int or float Minimum coverage required for a contig to pass the filter. coverage_info : OrderedDict or dict Dictionary containing the coverage information for each contig. output_file : str Path where the filtered assembly file will be generated.", "response": "def filter_assembly(assembly_file, minimum_coverage, coverage_info,\n                    output_file):\n    \"\"\"Generates a filtered assembly file.\n\n    This function generates a filtered assembly file based on an original\n    assembly and a minimum coverage threshold.\n\n    Parameters\n    ----------\n    assembly_file : str\n        Path to original assembly file.\n    minimum_coverage : int or float\n        Minimum coverage required for a contig to pass the filter.\n    coverage_info : OrderedDict or dict\n        Dictionary containing the coverage information for each contig.\n    output_file : str\n        Path where the filtered assembly file will be generated.\n\n    \"\"\"\n\n    # This flag will determine whether sequence data should be written or\n    # ignored because the current contig did not pass the minimum\n    # coverage threshold\n    write_flag = False\n\n    with open(assembly_file) as fh, open(output_file, \"w\") as out_fh:\n\n        for line in fh:\n            if line.startswith(\">\"):\n                # Reset write_flag\n                write_flag = False\n                # Get header of contig\n                header = line.strip()[1:]\n                # Check coverage for current contig\n                contig_cov = coverage_info[header][\"cov\"]\n                # If the contig coverage is above the threshold, write to\n                # output filtered assembly\n                if contig_cov >= minimum_coverage:\n                    write_flag = True\n                    out_fh.write(line)\n\n            elif write_flag:\n                out_fh.write(line)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef filter_bam(coverage_info, bam_file, min_coverage, output_bam):\n\n    # Get list of contigs that will be kept\n    contig_list = [x for x, vals in coverage_info.items()\n                   if vals[\"cov\"] >= min_coverage]\n\n    cli = [\n        \"samtools\",\n        \"view\",\n        \"-bh\",\n        \"-F\",\n        \"4\",\n        \"-o\",\n        output_bam,\n        \"-@\",\n        \"1\",\n        bam_file,\n    ]\n\n    cli += contig_list\n\n    logger.debug(\"Runnig samtools view subprocess with command: {}\".format(\n        cli))\n\n    p = subprocess.Popen(cli, stdout=PIPE, stderr=PIPE)\n    stdout, stderr = p.communicate()\n\n    # Attempt to decode STDERR output from bytes. If unsuccessful, coerce to\n    # string\n    try:\n        stderr = stderr.decode(\"utf8\")\n        stdout = stdout.decode(\"utf8\")\n    except (UnicodeDecodeError, AttributeError):\n        stderr = str(stderr)\n        stdout = str(stdout)\n\n    logger.info(\"Finished samtools view subprocess with STDOUT:\\\\n\"\n                \"======================================\\\\n{}\".format(stdout))\n    logger.info(\"Fished samtools view subprocesswith STDERR:\\\\n\"\n                \"======================================\\\\n{}\".format(stderr))\n    logger.info(\"Finished samtools view with return code: {}\".format(\n        p.returncode))\n\n    if not p.returncode:\n        # Create index\n        cli = [\n            \"samtools\",\n            \"index\",\n            output_bam\n        ]\n\n        logger.debug(\"Runnig samtools index subprocess with command: \"\n                     \"{}\".format(cli))\n\n        p = subprocess.Popen(cli, stdout=PIPE, stderr=PIPE)\n        stdout, stderr = p.communicate()\n\n        try:\n            stderr = stderr.decode(\"utf8\")\n            stdout = stdout.decode(\"utf8\")\n        except (UnicodeDecodeError, AttributeError):\n            stderr = str(stderr)\n            stdout = str(stdout)\n\n        logger.info(\"Finished samtools index subprocess with STDOUT:\\\\n\"\n                    \"======================================\\\\n{}\".format(\n            stdout))\n        logger.info(\"Fished samtools index subprocesswith STDERR:\\\\n\"\n                    \"======================================\\\\n{}\".format(\n            stderr))\n        logger.info(\"Finished samtools index with return code: {}\".format(\n            p.returncode))", "response": "This function uses Samtools to filter a BAM file according to minimum coverage."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_filtered_assembly(coverage_info, coverage_bp, minimum_coverage,\n                            genome_size, contig_size, max_contigs,\n                            sample_id):\n    \"\"\"Checks whether a filtered assembly passes a size threshold\n\n    Given a minimum coverage threshold, this function evaluates whether an\n    assembly will pass the minimum threshold of ``genome_size * 1e6 * 0.8``,\n    which means 80% of the expected genome size or the maximum threshold\n    of ``genome_size * 1e6 * 1.5``, which means 150% of the expected genome\n    size. It will issue a warning if any of these thresholds is crossed.\n    In the case of an expected genome size below 80% it will return False.\n\n    Parameters\n    ----------\n    coverage_info : OrderedDict or dict\n        Dictionary containing the coverage information for each contig.\n    coverage_bp : dict\n        Dictionary containing the per base coverage information for each\n        contig. Used to determine the total number of base pairs in the\n        final assembly.\n    minimum_coverage : int\n        Minimum coverage required for a contig to pass the filter.\n    genome_size : int\n        Expected genome size.\n    contig_size : dict\n        Dictionary with the len of each contig. Contig headers as keys and\n        the corresponding lenght as values.\n    max_contigs : int\n        Maximum threshold for contig number. A warning is issued if this\n        threshold is crossed.\n    sample_id : str\n        Id or name of the current sample\n\n    Returns\n    -------\n    x : bool\n        True if the filtered assembly size is higher than 80% of the\n        expected genome size.\n\n    \"\"\"\n\n    # Get size of assembly after filtering contigs below minimum_coverage\n    assembly_len = sum([v for k, v in contig_size.items()\n                        if coverage_info[k][\"cov\"] >= minimum_coverage])\n    logger.debug(\"Assembly length after filtering for minimum coverage of\"\n                 \" {}: {}\".format(minimum_coverage, assembly_len))\n    # Get number of contigs after filtering\n    ncontigs = len([x for x in coverage_info.values()\n                    if x[\"cov\"] >= minimum_coverage])\n    logger.debug(\"Number of contigs: {}\".format(ncontigs))\n    # Get number of bp after filtering\n    filtered_contigs = [k for k, v in coverage_info.items()\n                        if v[\"cov\"] >= minimum_coverage]\n    logger.debug(\"Filtered contigs for minimum coverage of \"\n                 \"{}: {}\".format(minimum_coverage, filtered_contigs))\n    total_assembled_bp = sum([sum(coverage_bp[x]) for x in filtered_contigs\n                              if x in coverage_bp])\n    logger.debug(\"Total number of assembled base pairs:\"\n                 \"{}\".format(total_assembled_bp))\n\n    warnings = []\n    fails = []\n    health = True\n\n    with open(\".warnings\", \"w\") as warn_fh, \\\n            open(\".report.json\", \"w\") as json_report:\n\n        logger.debug(\"Checking assembly size after filtering : {}\".format(\n            assembly_len))\n\n        # If the filtered assembly size is above the 150% genome size\n        # threshold, issue a warning\n        if assembly_len > genome_size * 1e6 * 1.5:\n            warn_msg = \"Assembly size ({}) smaller than the maximum\" \\\n                       \" threshold of 150% of expected genome size.\".format(\n                            assembly_len)\n            logger.warning(warn_msg)\n            warn_fh.write(warn_msg)\n            fails.append(\"Large_genome_size_({})\".format(assembly_len))\n\n        # If the number of contigs in the filtered assembly size crosses the\n        # max_contigs threshold, issue a warning\n        logger.debug(\"Checking number of contigs: {}\".format(\n                len(coverage_info)))\n        contig_threshold = max_contigs * genome_size / 1.5\n        if ncontigs > contig_threshold:\n            warn_msg = \"The number of contigs ({}) exceeds the threshold of \" \\\n                       \"100 contigs per 1.5Mb ({})\".format(\n                            ncontigs, round(contig_threshold, 1))\n            logger.warning(warn_msg)\n            warn_fh.write(warn_msg)\n            warnings.append(warn_msg)\n\n        # If the filtered assembly size falls below the 80% genome size\n        # threshold, fail this check and return False\n        if assembly_len < genome_size * 1e6 * 0.8:\n            warn_msg = \"Assembly size smaller than the minimum\" \\\n                       \" threshold of 80% of expected genome size: {}\".format(\n                            assembly_len)\n            logger.warning(warn_msg)\n            warn_fh.write(warn_msg)\n            fails.append(\"Small_genome_size_({})\".format(assembly_len))\n            assembly_len = sum([v for v in contig_size.values()])\n            total_assembled_bp = sum(\n                [sum(coverage_bp[x]) for x in coverage_info if x in\n                 coverage_bp])\n            logger.debug(\"Assembly length without coverage filtering: \"\n                         \"{}\".format(assembly_len))\n            logger.debug(\"Total number of assembled base pairs without\"\n                         \" filtering: {}\".format(total_assembled_bp))\n\n            health = False\n\n        json_dic = {\n            \"plotData\": [{\n                \"sample\": sample_id,\n                \"data\": {\n                    \"sparkline\": total_assembled_bp\n                }\n            }]\n        }\n\n        if warnings:\n            json_dic[\"warnings\"] = [{\n                \"sample\": sample_id,\n                \"table\": \"assembly\",\n                \"value\": warnings\n            }]\n        if fails:\n            json_dic[\"fail\"] = [{\n                \"sample\": sample_id,\n                \"table\": \"assembly\",\n                \"value\": [fails]\n            }]\n\n        json_report.write(json.dumps(json_dic, separators=(\",\", \":\")))\n\n    return health", "response": "Checks whether a filtered assembly passes a minimum coverage threshold Given a minimum coverage threshold this function evaluates whether the filtered assembly passes the minimum coverage threshold and returns True if the filtered assembly passes False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef evaluate_min_coverage(coverage_opt, assembly_coverage, assembly_size):\n\n    if coverage_opt == \"auto\":\n        # Get the 1/3 value of the current assembly coverage\n        min_coverage = (assembly_coverage / assembly_size) * .3\n        logger.info(\"Minimum assembly coverage automatically set to: \"\n                    \"{}\".format(min_coverage))\n        # If the 1/3 coverage is lower than 10, change it to the minimum of\n        # 10\n        if min_coverage < 10:\n            logger.info(\"Minimum assembly coverage cannot be set to lower\"\n                        \" that 10. Setting to 10\")\n            min_coverage = 10\n    else:\n        min_coverage = int(coverage_opt)\n        logger.info(\"Minimum assembly coverage manually set to: {}\".format(\n            min_coverage))\n\n    return min_coverage", "response": "Evaluates the minimum coverage threshold from the value provided in\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the number of nucleotides and the size per contig for the entire file.", "response": "def get_assembly_size(assembly_file):\n    \"\"\"Returns the number of nucleotides and the size per contig for the\n    provided assembly file path\n\n    Parameters\n    ----------\n    assembly_file : str\n        Path to assembly file.\n\n    Returns\n    -------\n    assembly_size : int\n        Size of the assembly in nucleotides\n    contig_size : dict\n        Length of each contig (contig name as key and length as value)\n\n    \"\"\"\n\n    assembly_size = 0\n    contig_size = {}\n    header = \"\"\n\n    with open(assembly_file) as fh:\n        for line in fh:\n\n            # Skip empty lines\n            if line.strip() == \"\":\n                continue\n\n            if line.startswith(\">\"):\n                header = line.strip()[1:]\n                contig_size[header] = 0\n\n            else:\n                line_len = len(line.strip())\n                assembly_size += line_len\n                contig_size[header] += line_len\n\n    return assembly_size, contig_size"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef main(sample_id, assembly_file, gsize, opts, assembler):\n\n    logger.info(\"Starting assembly file processing\")\n    warnings = []\n    fails = \"\"\n\n    min_contig_len, min_kmer_cov, max_contigs = [int(x) for x in opts]\n    logger.debug(\"Setting minimum conting length to: {}\".format(\n        min_contig_len))\n    logger.debug(\"Setting minimum kmer coverage: {}\".format(min_kmer_cov))\n\n    # Parse the spades assembly file and perform the first filtering.\n    logger.info(\"Starting assembly parsing\")\n    assembly_obj = Assembly(assembly_file, min_contig_len, min_kmer_cov,\n                               sample_id)\n\n    with open(\".warnings\", \"w\") as warn_fh:\n        t_80 = gsize * 1000000 * 0.8\n        t_150 = gsize * 1000000 * 1.5\n        # Check if assembly size of the first assembly is lower than 80% of the\n        # estimated genome size. If True, redo the filtering without the\n        # k-mer coverage filter\n        assembly_len = assembly_obj.get_assembly_length()\n        logger.debug(\"Checking assembly length: {}\".format(assembly_len))\n\n        if assembly_len < t_80:\n\n            logger.warning(\"Assembly size ({}) smaller than the minimum \"\n                           \"threshold of 80% of expected genome size. \"\n                           \"Applying contig filters without the k-mer \"\n                           \"coverage filter\".format(assembly_len))\n            assembly_obj.filter_contigs(*[\n                [\"length\", \">=\", min_contig_len]\n            ])\n\n            assembly_len = assembly_obj.get_assembly_length()\n            logger.debug(\"Checking updated assembly length: \"\n                         \"{}\".format(assembly_len))\n            if assembly_len < t_80:\n\n                warn_msg = \"Assembly size smaller than the minimum\" \\\n                           \" threshold of 80% of expected genome size: {}\".format(\n                                assembly_len)\n                logger.warning(warn_msg)\n                warn_fh.write(warn_msg)\n                fails = warn_msg\n\n        if assembly_len > t_150:\n\n            warn_msg = \"Assembly size ({}) larger than the maximum\" \\\n                       \" threshold of 150% of expected genome size.\".format(\n                            assembly_len)\n            logger.warning(warn_msg)\n            warn_fh.write(warn_msg)\n            fails = warn_msg\n\n        logger.debug(\"Checking number of contigs: {}\".format(\n            len(assembly_obj.contigs)))\n        contig_threshold = (max_contigs * gsize) / 1.5\n        if len(assembly_obj.contigs) > contig_threshold:\n\n            warn_msg = \"The number of contigs ({}) exceeds the threshold of \" \\\n                       \"{} contigs per 1.5Mb ({})\".format(\n                            len(assembly_obj.contigs),\n                            max_contigs,\n                            round(contig_threshold, 1))\n\n            logger.warning(warn_msg)\n            warn_fh.write(warn_msg)\n            warnings.append(warn_msg)\n\n    # Write filtered assembly\n    logger.debug(\"Renaming old assembly file to: {}\".format(\n        \"{}.old\".format(assembly_file)))\n    assembly_obj.write_assembly(\"{}_proc.fasta\".format(\n        os.path.splitext(assembly_file)[0]))\n    # Write report\n    output_report = \"{}.report.csv\".format(sample_id)\n    assembly_obj.write_report(output_report)\n    # Write json report\n    with open(\".report.json\", \"w\") as json_report:\n        json_dic = {\n            \"tableRow\": [{\n                \"sample\": sample_id,\n                \"data\": [\n                    {\"header\": \"Contigs ({})\".format(assembler),\n                     \"value\": len(assembly_obj.contigs),\n                     \"table\": \"assembly\",\n                     \"columnBar\": True},\n                    {\"header\": \"Assembled BP ({})\".format(assembler),\n                     \"value\": assembly_len,\n                     \"table\": \"assembly\",\n                     \"columnBar\": True}\n                ]\n            }],\n        }\n\n        if warnings:\n            json_dic[\"warnings\"] = [{\n                \"sample\": sample_id,\n                \"table\": \"assembly\",\n                \"value\": warnings\n            }]\n\n        if fails:\n            json_dic[\"fail\"] = [{\n                \"sample\": sample_id,\n                \"table\": \"assembly\",\n                \"value\": [fails]\n            }]\n\n        json_report.write(json.dumps(json_dic, separators=(\",\", \":\")))\n\n    with open(\".status\", \"w\") as status_fh:\n        status_fh.write(\"pass\")", "response": "Main function for the process_spades template."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef convert_camel_case(name):\n    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()", "response": "Convers a CamelCase string into a snake_case one"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncollects Process classes and return dict mapping templates to classes This function crawls through the components module and retrieves all classes that inherit from the Process class. Then, it converts the name of the classes (which should be CamelCase) to snake_case, which is used as the template name. Returns ------- dict Dictionary mapping the template name (snake_case) to the corresponding process class.", "response": "def collect_process_map():\n    \"\"\"Collects Process classes and return dict mapping templates to classes\n\n    This function crawls through the components module and retrieves all\n    classes that inherit from the Process class. Then, it converts the name\n    of the classes (which should be CamelCase) to snake_case, which is used\n    as the template name.\n\n    Returns\n    -------\n    dict\n        Dictionary mapping the template name (snake_case) to the corresponding\n        process class.\n    \"\"\"\n\n    process_map = {}\n\n    prefix = \"{}.\".format(components.__name__)\n    for importer, modname, _ in pkgutil.iter_modules(components.__path__,\n                                                     prefix):\n\n        _module = importer.find_module(modname).load_module(modname)\n\n        _component_classes = [\n            cls for cls in _module.__dict__.values() if\n            isinstance(cls, type) and cls.__name__ != \"Process\"\n        ]\n\n        for cls in _component_classes:\n            process_map[convert_camel_case(cls.__name__)] = cls\n\n    return process_map"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef main(newick):\n\n    logger.info(\"Starting newick file processing\")\n\n    print(newick)\n\n    tree = dendropy.Tree.get(file=open(newick, 'r'), schema=\"newick\")\n\n    tree.reroot_at_midpoint()\n\n    to_write=tree.as_string(\"newick\").strip().replace(\"[&R] \", '').replace(' ', '_').replace(\"'\", \"\")\n\n    with open(\".report.json\", \"w\") as json_report:\n        json_dic = {\n            \"treeData\": [{\n                \"trees\": [\n                    to_write\n                ]\n            }],\n        }\n\n        json_report.write(json.dumps(json_dic, separators=(\",\", \":\")))\n\n    with open(\".status\", \"w\") as status_fh:\n        status_fh.write(\"pass\")", "response": "This function is the main function of the process_newick template. It is the main function of the process_newick template. It is the main function of the process_newick template. It is the main function of the process_newick template. It is the main function of the process_newick template. It is the main function of the process_newick template."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfactorizes s. t. CUR", "response": "def factorize(self):\n        \"\"\" Factorize s.t. CUR = data\n\n            Updated Values\n            --------------\n            .C : updated values for C.\n            .U : updated values for U.\n            .R : updated values for R.\n        \"\"\"\n\n        [prow, pcol] = self.sample_probability()\n\n        self._rid = self.sample(self._rrank, prow)\n        self._cid = self.sample(self._crank, pcol)\n\n        self._cmdinit()\n\n        self.computeUCR()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef factorize(self):\n        [prow, pcol] = self.sample_probability()\n        self._rid = self.sample(self._rrank, prow)\n        self._cid = self.sample(self._crank, pcol)\n\n        self._rcnt = np.ones(len(self._rid))\n        self._ccnt = np.ones(len(self._cid))\n\n        self.computeUCR()", "response": "Factorize s. t. CUR"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef quickhull(sample):\n\n    link = lambda a, b: np.concatenate((a, b[1:]))\n    edge = lambda a, b: np.concatenate(([a], [b]))\n\n    def dome(sample, base):\n        h, t = base\n        dists = np.dot(sample - h, np.dot(((0, -1), (1, 0)), (t - h)))\n        outer = np.repeat(sample, dists > 0, axis=0)\n\n        if len(outer):\n            pivot = sample[np.argmax(dists)]\n            return link(dome(outer, edge(h, pivot)),\n                dome(outer, edge(pivot, t)))\n        else:\n            return base\n\n    if len(sample) > 2:\n        axis = sample[:, 0]\n        base = np.take(sample, [np.argmin(axis), np.argmax(axis)], axis=0)\n        return link(dome(sample, base),\n            dome(sample, base[::-1]))\n    else:\n        return sample", "response": "Find data points on the convex hull of a supplied data set."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _map_w_to_data(self):\n\n        # assign W to the next best data sample\n        self._Wmapped_index = vq(self.data, self.W)\n        self.Wmapped = np.zeros(self.W.shape)\n\n        # do not directly assign, i.e. Wdist = self.data[:,sel]\n        # as self might be unsorted (in non ascending order)\n        # -> sorting sel would screw the matching to W if\n        # self.data is stored as a hdf5 table (see h5py)\n        for i, s in enumerate(self._Wmapped_index):\n            self.Wmapped[:,i] = self.data[:,s]", "response": "Map W to data points that are most similar to basis vectors W\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef factorize(self, show_progress=False, compute_w=True, compute_h=True,\n                  compute_err=True, niter=1):\n        \"\"\" Factorize s.t. WH = data\n\n            Parameters\n            ----------\n            show_progress : bool\n                    print some extra information to stdout.\n            compute_h : bool\n                    iteratively update values for H.\n            compute_w : bool\n                    iteratively update values for W.\n            compute_err : bool\n                    compute Frobenius norm |data-WH| after each update and store\n                    it to .ferr[k].\n\n            Updated Values\n            --------------\n            .W : updated values for W.\n            .H : updated values for H.\n            .ferr : Frobenius norm |data-WH|.\n        \"\"\"\n\n        AA.factorize(self, niter=1, show_progress=show_progress,\n                  compute_w=compute_w, compute_h=compute_h,\n                  compute_err=compute_err)", "response": "Factorize s. t. WH = data\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating the W attribute of the object with new values", "response": "def update_w(self):\n        \"\"\" compute new W \"\"\"\n\n        def select_next(iterval):\n            \"\"\" select the next best data sample using robust map\n            or simply the max iterval ... \"\"\"\n\n            if self._robust_map:\n                k = np.argsort(iterval)[::-1]\n                d_sub = self.data[:,k[:self._robust_nselect]]\n                self.sub.extend(k[:self._robust_nselect])\n\n                # cluster d_sub\n                kmeans_mdl = Kmeans(d_sub, num_bases=self._robust_cluster)\n                kmeans_mdl.factorize(niter=10)\n\n                # get largest cluster\n                h = np.histogram(kmeans_mdl.assigned, range(self._robust_cluster+1))[0]\n                largest_cluster = np.argmax(h)\n                sel = pdist(kmeans_mdl.W[:, largest_cluster:largest_cluster+1], d_sub)\n                sel = k[np.argmin(sel)]\n            else:\n                sel = np.argmax(iterval)\n\n            return sel\n\n        EPS = 10**-8\n\n        if scipy.sparse.issparse(self.data):\n            norm_data = np.sqrt(self.data.multiply(self.data).sum(axis=0))\n            norm_data = np.array(norm_data).reshape((-1))\n        else:\n            norm_data = np.sqrt(np.sum(self.data**2, axis=0))\n\n\n        self.select = []\n\n        if self._method == 'pca' or self._method == 'aa':\n            iterval = norm_data.copy()\n\n        if self._method == 'nmf':\n            iterval = np.sum(self.data, axis=0)/(np.sqrt(self.data.shape[0])*norm_data)\n            iterval = 1.0 - iterval\n\n        self.select.append(select_next(iterval))\n\n\n        for l in range(1, self._num_bases):\n\n            if scipy.sparse.issparse(self.data):\n                c = self.data[:, self.select[-1]:self.select[-1]+1].T * self.data\n                c = np.array(c.todense())\n            else:\n                c = np.dot(self.data[:,self.select[-1]], self.data)\n\n            c = c/(norm_data * norm_data[self.select[-1]])\n\n            if self._method == 'pca':\n                c = 1.0 - np.abs(c)\n                c = c * norm_data\n\n            elif self._method == 'aa':\n                c = (c*-1.0 + 1.0)/2.0\n                c = c * norm_data\n\n            elif self._method == 'nmf':\n                c = 1.0 - np.abs(c)\n\n            ### update the estimated volume\n            iterval = c * iterval\n\n            # detect the next best data point\n            self.select.append(select_next(iterval))\n\n            self._logger.info('cur_nodes: ' + str(self.select))\n\n        # sort indices, otherwise h5py won't work\n        self.W = self.data[:, np.sort(self.select)]\n\n        # \"unsort\" it again to keep the correct order\n        self.W = self.W[:, np.argsort(np.argsort(self.select))]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfactorize s. t. WH = data", "response": "def factorize(self, show_progress=False, compute_w=True, compute_h=True,\n                  compute_err=True, robust_cluster=3, niter=1, robust_nselect=-1):\n        \"\"\" Factorize s.t. WH = data\n\n            Parameters\n            ----------\n            show_progress : bool\n                    print some extra information to stdout.\n                    False, default\n            compute_h : bool\n                    iteratively update values for H.\n                    True, default\n            compute_w : bool\n                    iteratively update values for W.\n                    default, True\n            compute_err : bool\n                    compute Frobenius norm |data-WH| after each update and store\n                    it to .ferr[k].\n            robust_cluster : int, optional\n                    set the number of clusters for robust map selection.\n                    3, default\n            robust_nselect : int, optional\n                    set the number of samples to consider for robust map\n                    selection.\n                    -1, default (automatically determine suitable number)\n\n            Updated Values\n            --------------\n            .W : updated values for W.\n            .H : updated values for H.\n            .ferr : Frobenius norm |data-WH|.\n        \"\"\"\n        self._robust_cluster = robust_cluster\n        self._robust_nselect = robust_nselect\n\n        if self._robust_nselect == -1:\n            self._robust_nselect = np.round(np.log(self.data.shape[1])*2)\n\n        AA.factorize(self, niter=1, show_progress=show_progress,\n                  compute_w=compute_w, compute_h=compute_h,\n                  compute_err=compute_err)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef processFlat(self):\n        self.config[\"hier\"] = False\n        est_idxs, est_labels, F = self.process()\n        assert est_idxs[0] == 0 and est_idxs[-1] == F.shape[1] - 1\n        return self._postprocess(est_idxs, est_labels)", "response": "Main process. for flat segmentation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef processHierarchical(self):\n        self.config[\"hier\"] = True\n        est_idxs, est_labels, F = self.process()\n        for layer in range(len(est_idxs)):\n            assert est_idxs[layer][0] == 0 and \\\n                est_idxs[layer][-1] == F.shape[1] - 1\n            est_idxs[layer], est_labels[layer] = \\\n                self._postprocess(est_idxs[layer], est_labels[layer])\n        return est_idxs, est_labels", "response": "Main process. for hierarchial segmentation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compute_gaussian_krnl(M):\n    g = signal.gaussian(M, M // 3., sym=True)\n    G = np.dot(g.reshape(-1, 1), g.reshape(1, -1))\n    G[M // 2:, :M // 2] = -G[M // 2:, :M // 2]\n    G[:M // 2, M // 2:] = -G[:M // 2, M // 2:]\n    return G", "response": "Creates a gaussian kernel following Foote s paper."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compute_ssm(X, metric=\"seuclidean\"):\n    D = distance.pdist(X, metric=metric)\n    D = distance.squareform(D)\n    D /= D.max()\n    return 1 - D", "response": "Computes the self - similarity matrix of X."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compute_nc(X, G):\n    N = X.shape[0]\n    M = G.shape[0]\n    nc = np.zeros(N)\n\n    for i in range(M // 2, N - M // 2 + 1):\n        nc[i] = np.sum(X[i - M // 2:i + M // 2, i - M // 2:i + M // 2] * G)\n\n    # Normalize\n    nc += nc.min()\n    nc /= nc.max()\n    return nc", "response": "Computes the novelty curve from the self - similarity matrix X and\n        the gaussian kernel G."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nobtain peaks from a novelty curve using an adaptive threshold.", "response": "def pick_peaks(nc, L=16):\n    \"\"\"Obtain peaks from a novelty curve using an adaptive threshold.\"\"\"\n    offset = nc.mean() / 20.\n\n    nc = filters.gaussian_filter1d(nc, sigma=4)  # Smooth out nc\n\n    th = filters.median_filter(nc, size=L) + offset\n    #th = filters.gaussian_filter(nc, sigma=L/2., mode=\"nearest\") + offset\n\n    peaks = []\n    for i in range(1, nc.shape[0] - 1):\n        # is it a peak?\n        if nc[i - 1] < nc[i] and nc[i] > nc[i + 1]:\n            # is it above the threshold?\n            if nc[i] > th[i]:\n                peaks.append(i)\n    #plt.plot(nc)\n    #plt.plot(th)\n    #for peak in peaks:\n        #plt.axvline(peak)\n    #plt.show()\n\n    return peaks"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef processFlat(self):\n        # Preprocess to obtain features\n        F = self._preprocess()\n\n        # Normalize\n        F = msaf.utils.normalize(F, norm_type=self.config[\"bound_norm_feats\"])\n\n        # Make sure that the M_gaussian is even\n        if self.config[\"M_gaussian\"] % 2 == 1:\n            self.config[\"M_gaussian\"] += 1\n\n        # Median filter\n        F = median_filter(F, M=self.config[\"m_median\"])\n        #plt.imshow(F.T, interpolation=\"nearest\", aspect=\"auto\"); plt.show()\n\n        # Self similarity matrix\n        S = compute_ssm(F)\n\n        # Compute gaussian kernel\n        G = compute_gaussian_krnl(self.config[\"M_gaussian\"])\n        #plt.imshow(S, interpolation=\"nearest\", aspect=\"auto\"); plt.show()\n\n        # Compute the novelty curve\n        nc = compute_nc(S, G)\n\n        # Find peaks in the novelty curve\n        est_idxs = pick_peaks(nc, L=self.config[\"L_peaks\"])\n\n        # Add first and last frames\n        est_idxs = np.concatenate(([0], est_idxs, [F.shape[0] - 1]))\n\n        # Empty labels\n        est_labels = np.ones(len(est_idxs) - 1) * -1\n\n        # Post process estimations\n        est_idxs, est_labels = self._postprocess(est_idxs, est_labels)\n\n        return est_idxs, est_labels", "response": "Main function for processing the flat data."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfactorizing s. t. WH = data - WH", "response": "def factorize(self, show_progress=False, compute_w=True, compute_h=True,\n                  compute_err=True, niter=1):\n        \"\"\" Factorize s.t. WH = data\n\n            Parameters\n            ----------\n            show_progress : bool\n                    print some extra information to stdout.\n            niter : int\n                    number of iterations.\n            compute_h : bool\n                    iteratively update values for H.\n            compute_w : bool\n                    iteratively update values for W.\n            compute_err : bool\n                    compute Frobenius norm |data-WH| after each update and store\n                    it to .ferr[k].\n\n            Updated Values\n            --------------\n            .W : updated values for W.\n            .H : updated values for H.\n            .ferr : Frobenius norm |data-WH|.\n        \"\"\"\n        if show_progress:\n            self._logger.setLevel(logging.INFO)\n        else:\n            self._logger.setLevel(logging.ERROR)\n\n        # create W and H if they don't already exist\n        # -> any custom initialization to W,H should be done before\n        if not hasattr(self,'W'):\n               self.init_w()\n\n        if not hasattr(self,'H'):\n                self.init_h()\n\n        if compute_err:\n            self.ferr = np.zeros(niter)\n\n        for i in range(niter):\n            if compute_w:\n                self.update_w()\n\n            if compute_h:\n                self.update_h()\n\n            if compute_err:\n                self.ferr[i] = self.frobenius_norm()\n                self._logger.info('Iteration ' + str(i+1) + '/' + str(niter) +\n                    ' FN:' + str(self.ferr[i]))\n            else:\n                self._logger.info('Iteration ' + str(i+1) + '/' + str(niter))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the novelty curve from the structural features X.", "response": "def compute_nc(X):\n    \"\"\"Computes the novelty curve from the structural features.\"\"\"\n    N = X.shape[0]\n    # nc = np.sum(np.diff(X, axis=0), axis=1) # Difference between SF's\n\n    nc = np.zeros(N)\n    for i in range(N - 1):\n        nc[i] = distance.euclidean(X[i, :], X[i + 1, :])\n\n    # Normalize\n    nc += np.abs(nc.min())\n    nc /= float(nc.max())\n    return nc"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pick_peaks(nc, L=16, offset_denom=0.1):\n    offset = nc.mean() * float(offset_denom)\n    th = filters.median_filter(nc, size=L) + offset\n    #th = filters.gaussian_filter(nc, sigma=L/2., mode=\"nearest\") + offset\n    #import pylab as plt\n    #plt.plot(nc)\n    #plt.plot(th)\n    #plt.show()\n    # th = np.ones(nc.shape[0]) * nc.mean() - 0.08\n    peaks = []\n    for i in range(1, nc.shape[0] - 1):\n        # is it a peak?\n        if nc[i - 1] < nc[i] and nc[i] > nc[i + 1]:\n            # is it above the threshold?\n            if nc[i] > th[i]:\n                peaks.append(i)\n    return peaks", "response": "Obtain peaks from a novelty curve using an adaptive threshold."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nshifts circularly the X squre matrix in order to get a time - lag matrix.", "response": "def circular_shift(X):\n    \"\"\"Shifts circularly the X squre matrix in order to get a\n        time-lag matrix.\"\"\"\n    N = X.shape[0]\n    L = np.zeros(X.shape)\n    for i in range(N):\n        L[i, :] = np.asarray([X[(i + j) % N, j] for j in range(N)])\n    return L"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntime - delay embedding with m dimensions and tau delays.", "response": "def embedded_space(X, m, tau=1):\n    \"\"\"Time-delay embedding with m dimensions and tau delays.\"\"\"\n    N = X.shape[0] - int(np.ceil(m))\n    Y = np.zeros((N, int(np.ceil(X.shape[1] * m))))\n    for i in range(N):\n        # print X[i:i+m,:].flatten().shape, w, X.shape\n        # print Y[i,:].shape\n        rem = int((m % 1) * X.shape[1])  # Reminder for float m\n        Y[i, :] = np.concatenate((X[i:i + int(m), :].flatten(),\n                                 X[i + int(m), :rem]))\n    return Y"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprocesses the flat data for the segment indeces and return a dictionary of the number of times and labels.", "response": "def processFlat(self):\n        \"\"\"Main process.\n        Returns\n        -------\n        est_idxs : np.array(N)\n            Estimated times for the segment boundaries in frame indeces.\n        est_labels : np.array(N-1)\n            Estimated labels for the segments.\n        \"\"\"\n        # Structural Features params\n        Mp = self.config[\"Mp_adaptive\"]   # Size of the adaptive threshold for\n                                          # peak picking\n        od = self.config[\"offset_thres\"]  # Offset coefficient for adaptive\n                                          # thresholding\n\n        M = self.config[\"M_gaussian\"]     # Size of gaussian kernel in beats\n        m = self.config[\"m_embedded\"]     # Number of embedded dimensions\n        k = self.config[\"k_nearest\"]      # k*N-nearest neighbors for the\n                                          # recurrence plot\n\n        # Preprocess to obtain features, times, and input boundary indeces\n        F = self._preprocess()\n\n        # Normalize\n        F = U.normalize(F, norm_type=self.config[\"bound_norm_feats\"])\n\n        # Check size in case the track is too short\n        if F.shape[0] > 20:\n\n            if self.framesync:\n                red = 0.1\n                F_copy = np.copy(F)\n                F = librosa.util.utils.sync(\n                    F.T, np.linspace(0, F.shape[0], num=F.shape[0] * red),\n                    pad=False).T\n\n            # Emedding the feature space (i.e. shingle)\n            E = embedded_space(F, m)\n            # plt.imshow(E.T, interpolation=\"nearest\", aspect=\"auto\"); plt.show()\n\n            # Recurrence matrix\n            R = librosa.segment.recurrence_matrix(\n                E.T,\n                k=k * int(F.shape[0]),\n                width=1,  # zeros from the diagonal\n                metric=\"euclidean\",\n                sym=True).astype(np.float32)\n\n            # Circular shift\n            L = circular_shift(R)\n            #plt.imshow(L, interpolation=\"nearest\", cmap=plt.get_cmap(\"binary\"))\n            #plt.show()\n\n            # Obtain structural features by filtering the lag matrix\n            SF = gaussian_filter(L.T, M=M, axis=1)\n            SF = gaussian_filter(L.T, M=1, axis=0)\n            # plt.imshow(SF.T, interpolation=\"nearest\", aspect=\"auto\")\n            #plt.show()\n\n            # Compute the novelty curve\n            nc = compute_nc(SF)\n\n            # Find peaks in the novelty curve\n            est_bounds = pick_peaks(nc, L=Mp, offset_denom=od)\n\n            # Re-align embedded space\n            est_bounds = np.asarray(est_bounds) + int(np.ceil(m / 2.))\n\n            if self.framesync:\n                est_bounds /= red\n                F = F_copy\n        else:\n            est_bounds = []\n\n        # Add first and last frames\n        est_idxs = np.concatenate(([0], est_bounds, [F.shape[0] - 1]))\n        est_idxs = np.unique(est_idxs)\n\n        assert est_idxs[0] == 0 and est_idxs[-1] == F.shape[0] - 1\n\n        # Empty labels\n        est_labels = np.ones(len(est_idxs) - 1) * - 1\n\n        # Post process estimations\n        est_idxs, est_labels = self._postprocess(est_idxs, est_labels)\n\n        # plt.figure(1)\n        # plt.plot(nc);\n        # [plt.axvline(p, color=\"m\", ymin=.6) for p in est_bounds]\n        # [plt.axvline(b, color=\"b\", ymax=.6, ymin=.3) for b in brian_bounds]\n        # [plt.axvline(b, color=\"g\", ymax=.3) for b in ann_bounds]\n        # plt.show()\n\n        return est_idxs, est_labels"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _plot_formatting(title, est_file, algo_ids, last_bound, N, output_file):\n    import matplotlib.pyplot as plt\n    if title is None:\n        title = os.path.basename(est_file).split(\".\")[0]\n    plt.title(title)\n    plt.yticks(np.arange(0, 1, 1 / float(N)) + 1 / (float(N) * 2))\n    plt.gcf().subplots_adjust(bottom=0.22)\n    plt.gca().set_yticklabels(algo_ids)\n    plt.xlabel(\"Time (seconds)\")\n    plt.xlim((0, last_bound))\n    plt.tight_layout()\n    if output_file is not None:\n        plt.savefig(output_file)\n    plt.show()", "response": "Formats the plot with the correct axis labels title ticks and\n    so on."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot_boundaries(all_boundaries, est_file, algo_ids=None, title=None,\n                    output_file=None):\n    \"\"\"Plots all the boundaries.\n\n    Parameters\n    ----------\n    all_boundaries: list\n        A list of np.arrays containing the times of the boundaries, one array\n        for each algorithm.\n    est_file: str\n        Path to the estimated file (JSON file)\n    algo_ids : list\n        List of algorithm ids to to read boundaries from.\n        If None, all algorithm ids are read.\n    title : str\n        Title of the plot. If None, the name of the file is printed instead.\n    \"\"\"\n    import matplotlib.pyplot as plt\n    N = len(all_boundaries)  # Number of lists of boundaries\n    if algo_ids is None:\n        algo_ids = io.get_algo_ids(est_file)\n\n    # Translate ids\n    for i, algo_id in enumerate(algo_ids):\n        algo_ids[i] = translate_ids[algo_id]\n    algo_ids = [\"GT\"] + algo_ids\n\n    figsize = (6, 4)\n    plt.figure(1, figsize=figsize, dpi=120, facecolor='w', edgecolor='k')\n    for i, boundaries in enumerate(all_boundaries):\n        color = \"b\"\n        if i == 0:\n            color = \"g\"\n        for b in boundaries:\n            plt.axvline(b, i / float(N), (i + 1) / float(N), color=color)\n        plt.axhline(i / float(N), color=\"k\", linewidth=1)\n\n    # Format plot\n    _plot_formatting(title, est_file, algo_ids, all_boundaries[0][-1], N,\n                     output_file)", "response": "Plots all the boundaries."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nplot all the labels of the boundaries.", "response": "def plot_labels(all_labels, gt_times, est_file, algo_ids=None, title=None,\n                output_file=None):\n    \"\"\"Plots all the labels.\n\n    Parameters\n    ----------\n    all_labels: list\n        A list of np.arrays containing the labels of the boundaries, one array\n        for each algorithm.\n    gt_times: np.array\n        Array with the ground truth boundaries.\n    est_file: str\n        Path to the estimated file (JSON file)\n    algo_ids : list\n        List of algorithm ids to to read boundaries from.\n        If None, all algorithm ids are read.\n    title : str\n        Title of the plot. If None, the name of the file is printed instead.\n    \"\"\"\n    import matplotlib.pyplot as plt\n    N = len(all_labels)  # Number of lists of labels\n    if algo_ids is None:\n        algo_ids = io.get_algo_ids(est_file)\n\n    # Translate ids\n    for i, algo_id in enumerate(algo_ids):\n        algo_ids[i] = translate_ids[algo_id]\n    algo_ids = [\"GT\"] + algo_ids\n\n    # Index the labels to normalize them\n    for i, labels in enumerate(all_labels):\n        all_labels[i] = mir_eval.util.index_labels(labels)[0]\n\n    # Get color map\n    cm = plt.get_cmap('gist_rainbow')\n    max_label = max(max(labels) for labels in all_labels)\n\n    # To intervals\n    gt_inters = utils.times_to_intervals(gt_times)\n\n    # Plot labels\n    figsize = (6, 4)\n    plt.figure(1, figsize=figsize, dpi=120, facecolor='w', edgecolor='k')\n    for i, labels in enumerate(all_labels):\n        for label, inter in zip(labels, gt_inters):\n            plt.axvspan(inter[0], inter[1], ymin=i / float(N),\n                        ymax=(i + 1) / float(N), alpha=0.6,\n                        color=cm(label / float(max_label)))\n        plt.axhline(i / float(N), color=\"k\", linewidth=1)\n\n    # Draw the boundary lines\n    for bound in gt_times:\n        plt.axvline(bound, color=\"g\")\n\n    # Format plot\n    _plot_formatting(title, est_file, algo_ids, gt_times[-1], N,\n                     output_file)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nplots the results of one track with ground truth if it exists.", "response": "def plot_one_track(file_struct, est_times, est_labels, boundaries_id, labels_id,\n                   title=None):\n    \"\"\"Plots the results of one track, with ground truth if it exists.\"\"\"\n    import matplotlib.pyplot as plt\n    # Set up the boundaries id\n    bid_lid = boundaries_id\n    if labels_id is not None:\n        bid_lid += \" + \" + labels_id\n    try:\n        # Read file\n        jam = jams.load(file_struct.ref_file)\n        ann = jam.search(namespace='segment_.*')[0]\n        ref_inters, ref_labels = ann.to_interval_values()\n\n        # To times\n        ref_times = utils.intervals_to_times(ref_inters)\n        all_boundaries = [ref_times, est_times]\n        all_labels = [ref_labels, est_labels]\n        algo_ids = [\"GT\", bid_lid]\n    except:\n        logging.warning(\"No references found in %s. Not plotting groundtruth\"\n                        % file_struct.ref_file)\n        all_boundaries = [est_times]\n        all_labels = [est_labels]\n        algo_ids = [bid_lid]\n\n    N = len(all_boundaries)\n\n    # Index the labels to normalize them\n    for i, labels in enumerate(all_labels):\n        all_labels[i] = mir_eval.util.index_labels(labels)[0]\n\n    # Get color map\n    cm = plt.get_cmap('gist_rainbow')\n    max_label = max(max(labels) for labels in all_labels)\n\n    figsize = (8, 4)\n    plt.figure(1, figsize=figsize, dpi=120, facecolor='w', edgecolor='k')\n    for i, boundaries in enumerate(all_boundaries):\n        color = \"b\"\n        if i == 0:\n            color = \"g\"\n        for b in boundaries:\n            plt.axvline(b, i / float(N), (i + 1) / float(N), color=color)\n        if labels_id is not None:\n            labels = all_labels[i]\n            inters = utils.times_to_intervals(boundaries)\n            for label, inter in zip(labels, inters):\n                plt.axvspan(inter[0], inter[1], ymin=i / float(N),\n                            ymax=(i + 1) / float(N), alpha=0.6,\n                            color=cm(label / float(max_label)))\n        plt.axhline(i / float(N), color=\"k\", linewidth=1)\n\n    # Format plot\n    _plot_formatting(title, os.path.basename(file_struct.audio_file), algo_ids,\n                     all_boundaries[0][-1], N, None)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nplotting a given tree containing hierarchical segmentation.", "response": "def plot_tree(T, res=None, title=None, cmap_id=\"Pastel2\"):\n    \"\"\"Plots a given tree, containing hierarchical segmentation.\n\n    Parameters\n    ----------\n    T: mir_eval.segment.tree\n        A tree object containing the hierarchical segmentation.\n    res: float\n        Frame-rate resolution of the tree (None to use seconds).\n    title: str\n        Title for the plot. `None` for no title.\n    cmap_id: str\n        Color Map ID\n    \"\"\"\n    import matplotlib.pyplot as plt\n    def round_time(t, res=0.1):\n        v = int(t / float(res)) * res\n        return v\n\n    # Get color map\n    cmap = plt.get_cmap(cmap_id)\n\n    # Get segments by level\n    level_bounds = []\n    for level in T.levels:\n        if level == \"root\":\n            continue\n        segments = T.get_segments_in_level(level)\n        level_bounds.append(segments)\n\n    # Plot axvspans for each segment\n    B = float(len(level_bounds))\n    #plt.figure(figsize=figsize)\n    for i, segments in enumerate(level_bounds):\n        labels = utils.segment_labels_to_floats(segments)\n        for segment, label in zip(segments, labels):\n            #print i, label, cmap(label)\n            if res is None:\n                start = segment.start\n                end = segment.end\n                xlabel = \"Time (seconds)\"\n            else:\n                start = int(round_time(segment.start, res=res) / res)\n                end = int(round_time(segment.end, res=res) / res)\n                xlabel = \"Time (frames)\"\n            plt.axvspan(start, end,\n                        ymax=(len(level_bounds) - i) / B,\n                        ymin=(len(level_bounds) - i - 1) / B,\n                        facecolor=cmap(label))\n\n    # Plot labels\n    L = float(len(T.levels) - 1)\n    plt.yticks(np.linspace(0, (L - 1) / L, num=L) + 1 / L / 2.,\n               T.levels[1:][::-1])\n    plt.xlabel(xlabel)\n    if title is not None:\n        plt.title(title)\n    plt.gca().set_xlim([0, end])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_feat_segments(F, bound_idxs):\n    # Make sure bound_idxs are not empty\n    assert len(bound_idxs) > 0, \"Boundaries can't be empty\"\n\n    # Make sure that boundaries are sorted\n    bound_idxs = np.sort(bound_idxs)\n\n    # Make sure we're not out of bounds\n    assert bound_idxs[0] >= 0 and bound_idxs[-1] < F.shape[0], \\\n        \"Boundaries are not correct for the given feature dimensions.\"\n\n    # Obtain the segments\n    feat_segments = []\n    for i in range(len(bound_idxs) - 1):\n        feat_segments.append(F[bound_idxs[i]:bound_idxs[i + 1], :])\n    return feat_segments", "response": "Returns a set of segments defined by the bound_idxs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving a list of feature segments return a list of 2D - Fourier MagnitudeCoefs with the maximum size as main size and zero pad the rest.", "response": "def feat_segments_to_2dfmc_max(feat_segments, offset=4):\n    \"\"\"From a list of feature segments, return a list of 2D-Fourier Magnitude\n    Coefs using the maximum segment size as main size and zero pad the rest.\n\n    Parameters\n    ----------\n    feat_segments: list\n        List of segments, one for each boundary interval.\n    offset: int >= 0\n        Number of frames to ignore from beginning and end of each segment.\n\n    Returns\n    -------\n    fmcs: np.ndarray\n        Tensor containing the 2D-FMC matrices, one matrix per segment.\n    \"\"\"\n    if len(feat_segments) == 0:\n        return []\n\n    # Get maximum segment size\n    max_len = max([feat_segment.shape[0] for feat_segment in feat_segments])\n\n    fmcs = []\n    for feat_segment in feat_segments:\n        # Zero pad if needed\n        X = np.zeros((max_len, feat_segment.shape[1]))\n\n        # Remove a set of frames in the beginning an end of the segment\n        if feat_segment.shape[0] <= offset or offset == 0:\n            X[:feat_segment.shape[0], :] = feat_segment\n        else:\n            X[:feat_segment.shape[0] - offset, :] = \\\n                feat_segment[offset // 2:-offset // 2, :]\n\n        # Compute the 2D-FMC\n        try:\n            fmcs.append(utils2d.compute_ffmc2d(X))\n        except:\n            logging.warning(\"Couldn't compute the 2D Fourier Transform\")\n            fmcs.append(np.zeros((X.shape[0] * X.shape[1]) // 2 + 1))\n\n        # Normalize\n        # fmcs[-1] = fmcs[-1] / float(fmcs[-1].max())\n\n    return np.asarray(fmcs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef compute_similarity(F, bound_idxs, dirichlet=False, xmeans=False, k=5,\n                       offset=4):\n    \"\"\"Main function to compute the segment similarity of file file_struct.\n\n    Parameters\n    ----------\n    F: np.ndarray\n        Matrix containing one feature vector per row.\n    bound_idxs: np.ndarray\n        Array with the indeces of the segment boundaries.\n    dirichlet: boolean\n        Whether to use the dirichlet estimator of the number of unique labels.\n    xmeans: boolean\n        Whether to use the xmeans estimator of the number of unique labels.\n    k: int > 0\n        If the other two predictors are `False`, use fixed number of labels.\n    offset: int >= 0\n        Number of frames to ignore from beginning and end of each segment.\n\n    Returns\n    -------\n    labels_est: np.ndarray\n        Estimated labels, containing integer identifiers.\n    \"\"\"\n    # Get the feature segments\n    feat_segments = get_feat_segments(F, bound_idxs)\n\n    # Get the 2D-FMCs segments\n    fmcs = feat_segments_to_2dfmc_max(feat_segments, offset)\n    if len(fmcs) == 0:\n        return np.arange(len(bound_idxs) - 1)\n\n    # Compute the labels using kmeans\n    if dirichlet:\n        k_init = np.min([fmcs.shape[0], k])\n        # Only compute the dirichlet method if the fmc shape is small enough\n        if fmcs.shape[1] > 500:\n            labels_est = compute_labels_kmeans(fmcs, k=k)\n        else:\n            dpgmm = mixture.DPGMM(n_components=k_init, covariance_type='full')\n            # dpgmm = mixture.VBGMM(n_components=k_init, covariance_type='full')\n            dpgmm.fit(fmcs)\n            k = len(dpgmm.means_)\n            labels_est = dpgmm.predict(fmcs)\n            # print(\"Estimated with Dirichlet Process:\", k)\n    if xmeans:\n        xm = XMeans(fmcs, plot=False)\n        k = xm.estimate_K_knee(th=0.01, maxK=8)\n        labels_est = compute_labels_kmeans(fmcs, k=k)\n        # print(\"Estimated with Xmeans:\", k)\n    else:\n        labels_est = compute_labels_kmeans(fmcs, k=k)\n\n    return labels_est", "response": "Compute the similarity of a file with a set of features."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef processFlat(self):\n        # Preprocess to obtain features, times, and input boundary indeces\n        F = self._preprocess()\n\n        # Normalize\n        F = U.normalize(F, norm_type=self.config[\"label_norm_feats\"],\n                        floor=self.config[\"label_norm_floor\"],\n                        min_db=self.config[\"label_norm_min_db\"])\n\n        # Find the labels using 2D-FMCs\n        est_labels = compute_similarity(F, self.in_bound_idxs,\n                                        dirichlet=self.config[\"dirichlet\"],\n                                        xmeans=self.config[\"xmeans\"],\n                                        k=self.config[\"k\"],\n                                        offset=self.config[\"2dfmc_offset\"])\n\n        # Post process estimations\n        self.in_bound_idxs, est_labels = self._postprocess(self.in_bound_idxs,\n                                                           est_labels)\n\n        return self.in_bound_idxs, est_labels", "response": "Main function for processing the flat indeces and labels."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfits the OLDA model to the given data and labels.", "response": "def fit(self, X, Y):\n        '''Fit the OLDA model\n\n        Parameters\n        ----------\n        X : array-like, shape [n_samples]\n            Training data: each example is an n_features-by-* data array\n\n        Y : array-like, shape [n_samples]\n            Training labels: each label is an array of change-points\n                             (eg, a list of segment boundaries)\n\n        Returns\n        -------\n        self : object\n        '''\n        \n        # Re-initialize the scatter matrices\n        self.scatter_ordinal_ = None\n        self.scatter_within_  = None\n        \n        # Reduce to partial-fit\n        self.partial_fit(X, Y)\n        \n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef partial_fit(self, X, Y):\n        '''Partial-fit the OLDA model\n\n        Parameters\n        ----------\n        X : array-like, shape [n_samples]\n            Training data: each example is an n_features-by-* data array\n\n        Y : array-like, shape [n_samples]\n            Training labels: each label is an array of change-points\n                             (eg, a list of segment boundaries)\n\n        Returns\n        -------\n        self : object\n        '''\n        \n        for (xi, yi) in itertools.izip(X, Y):\n            \n            prev_mean       = None\n            prev_length     = None\n            \n            if self.scatter_within_ is None:\n                # First round: initialize\n                d, n = xi.shape\n                \n                if yi[0] > 0:\n                    yi = np.concatenate([np.array([0]), yi])\n                if yi[-1] < n:\n                    yi = np.concatenate([yi, np.array([n])])\n                    \n                self.scatter_within_  = self.sigma * np.eye(d)\n                self.scatter_ordinal_ = np.zeros(d)\n                \n            \n            # iterate over segments\n            for (seg_start, seg_end) in zip(yi[:-1], yi[1:]):\n            \n                seg_length = seg_end - seg_start\n                \n                if seg_length < 2:\n                    continue\n\n                seg_mean = np.mean(xi[:, seg_start:seg_end], axis=1, keepdims=True)\n                seg_cov  = np.cov(xi[:, seg_start:seg_end])    \n                self.scatter_within_ = self.scatter_within_ + seg_length * seg_cov\n                \n                \n                if prev_mean is not None:\n                    diff_ord = seg_mean - (prev_length * prev_mean + seg_length * seg_mean) / (prev_length + seg_length)\n                    self.scatter_ordinal_ = self.scatter_ordinal_ + seg_length * np.dot(diff_ord, diff_ord.T)\n                    \n                    diff_ord = prev_mean - (prev_length * prev_mean + seg_length * seg_mean) / (prev_length + seg_length)\n                    self.scatter_ordinal_ = self.scatter_ordinal_ + prev_length * np.dot(diff_ord, diff_ord.T)\n\n                prev_mean = seg_mean\n                prev_length = seg_length\n        \n        e_vals, e_vecs = scipy.linalg.eig(self.scatter_ordinal_, self.scatter_within_)\n        self.e_vals_ = e_vals\n        self.e_vecs_ = e_vecs\n        self.components_ = e_vecs.T\n        return self", "response": "Partial fit the OLDA model."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compute_features(self):\n        linear_cqt = np.abs(librosa.cqt(\n            self._audio, sr=self.sr, hop_length=self.hop_length,\n            n_bins=self.n_bins, norm=self.norm, filter_scale=self.filter_scale)\n                            ) ** 2\n        cqt = librosa.amplitude_to_db(linear_cqt, ref=self.ref_power).T\n        return cqt", "response": "Compute the features for a give\n            time frame and beat."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the features for a give time frame and beat.", "response": "def compute_features(self):\n        \"\"\"Actual implementation of the features.\n\n        Returns\n        -------\n        mfcc: np.array(N, F)\n            The features, each row representing a feature vector for a give\n            time frame/beat.\n        \"\"\"\n        S = librosa.feature.melspectrogram(self._audio,\n                                           sr=self.sr,\n                                           n_fft=self.n_fft,\n                                           hop_length=self.hop_length,\n                                           n_mels=self.n_mels)\n        log_S = librosa.amplitude_to_db(S, ref=self.ref_power)\n        mfcc = librosa.feature.mfcc(S=log_S, n_mfcc=self.n_mfcc).T\n        return mfcc"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the features for a give time frame and beat.", "response": "def compute_features(self):\n        \"\"\"Actual implementation of the features.\n\n        Returns\n        -------\n        pcp: np.array(N, F)\n            The features, each row representing a feature vector for a give\n            time frame/beat.\n        \"\"\"\n        audio_harmonic, _ = self.compute_HPSS()\n        pcp_cqt = np.abs(librosa.hybrid_cqt(audio_harmonic,\n                                            sr=self.sr,\n                                            hop_length=self.hop_length,\n                                            n_bins=self.n_bins,\n                                            norm=self.norm,\n                                            fmin=self.f_min)) ** 2\n        pcp = librosa.feature.chroma_cqt(C=pcp_cqt,\n                                         sr=self.sr,\n                                         hop_length=self.hop_length,\n                                         n_octaves=self.n_octaves,\n                                         fmin=self.f_min).T\n        return pcp"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the features for a give time frame and beat.", "response": "def compute_features(self):\n        \"\"\"Actual implementation of the features.\n\n        Returns\n        -------\n        tonnetz: np.array(N, F)\n            The features, each row representing a feature vector for a give\n            time frame/beat.\n        \"\"\"\n        pcp = PCP(self.file_struct, self.feat_type, self.sr, self.hop_length,\n                  self.n_bins, self.norm, self.f_min, self.n_octaves).features\n        tonnetz = librosa.feature.tonnetz(chroma=pcp.T).T\n        return tonnetz"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the features for a give time frame.", "response": "def compute_features(self):\n        \"\"\"Actual implementation of the features.\n\n        Returns\n        -------\n        tempogram: np.array(N, F)\n            The features, each row representing a feature vector for a give\n            time frame/beat.\n        \"\"\"\n        return librosa.feature.tempogram(self._audio, sr=self.sr,\n                                         hop_length=self.hop_length,\n                                         win_length=self.win_length).T"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_estimations(est_file, boundaries_id, labels_id=None, **params):\n    # Open file and read jams\n    jam = jams.load(est_file)\n\n    # Find correct estimation\n    est = find_estimation(jam, boundaries_id, labels_id, params)\n    if est is None:\n        raise NoEstimationsError(\"No estimations for file: %s\" % est_file)\n\n    # Get data values\n    all_boundaries, all_labels = est.to_interval_values()\n\n    if params[\"hier\"]:\n        hier_bounds = defaultdict(list)\n        hier_labels = defaultdict(list)\n        for bounds, labels in zip(all_boundaries, all_labels):\n            level = labels[\"level\"]\n            hier_bounds[level].append(bounds)\n            hier_labels[level].append(labels[\"label\"])\n        # Order\n        all_boundaries = []\n        all_labels = []\n        for key in sorted(list(hier_bounds.keys())):\n            all_boundaries.append(np.asarray(hier_bounds[key]))\n            all_labels.append(np.asarray(hier_labels[key]))\n\n    return all_boundaries, all_labels", "response": "Reads the estimations of an algorithm from a JAMS file and returns the boundaries and labels."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading the boundary times and labels of a single audio file.", "response": "def read_references(audio_path, annotator_id=0):\n    \"\"\"Reads the boundary times and the labels.\n\n    Parameters\n    ----------\n    audio_path : str\n        Path to the audio file\n\n    Returns\n    -------\n    ref_times : list\n        List of boundary times\n    ref_labels : list\n        List of labels\n\n    Raises\n    ------\n    IOError: if `audio_path` doesn't exist.\n    \"\"\"\n    # Dataset path\n    ds_path = os.path.dirname(os.path.dirname(audio_path))\n\n    # Read references\n    jam_path = os.path.join(ds_path, ds_config.references_dir,\n                            os.path.basename(audio_path)[:-4] +\n                            ds_config.references_ext)\n\n    jam = jams.load(jam_path, validate=False)\n    ann = jam.search(namespace='segment_.*')[annotator_id]\n    ref_inters, ref_labels = ann.to_interval_values()\n\n    # Intervals to times\n    ref_times = utils.intervals_to_times(ref_inters)\n\n    return ref_times, ref_labels"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef align_times(times, frames):\n    dist = np.minimum.outer(times, frames)\n    bound_frames = np.argmax(np.maximum(0, dist), axis=1)\n    aligned_times = np.unique(bound_frames)\n    return aligned_times", "response": "Aligns the times to the closest frame times."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding the correct estimation from all the estimations contained in a JAMS file given the specified parameters.", "response": "def find_estimation(jam, boundaries_id, labels_id, params):\n    \"\"\"Finds the correct estimation from all the estimations contained in a\n    JAMS file given the specified arguments.\n\n    Parameters\n    ----------\n    jam : jams.JAMS\n        JAMS object.\n    boundaries_id : str\n        Identifier of the algorithm used to compute the boundaries.\n    labels_id : str\n        Identifier of the algorithm used to compute the labels.\n    params : dict\n        Additional search parameters. E.g. {\"feature\" : \"pcp\"}.\n\n    Returns\n    -------\n    ann : jams.Annotation\n        Found estimation.\n        `None` if it couldn't be found.\n    \"\"\"\n    # Use handy JAMS search interface\n    namespace = \"multi_segment\" if params[\"hier\"] else \"segment_open\"\n    # TODO: This is a workaround to issue in JAMS. Should be\n    # resolved in JAMS 0.2.3, but for now, this works too.\n    ann = jam.search(namespace=namespace).\\\n        search(**{\"Sandbox.boundaries_id\": boundaries_id}).\\\n        search(**{\"Sandbox.labels_id\": lambda x:\n                  (isinstance(x, six.string_types) and\n                   re.match(labels_id, x) is not None) or x is None})\n    for key, val in zip(params.keys(), params.values()):\n        if isinstance(val, six.string_types):\n            ann = ann.search(**{\"Sandbox.%s\" % key: val})\n        else:\n            ann = ann.search(**{\"Sandbox.%s\" % key: lambda x: x == val})\n\n    # Check estimations found\n    if len(ann) > 1:\n        logging.warning(\"More than one estimation with same parameters.\")\n\n    if len(ann) > 0:\n        ann = ann[0]\n\n    # If we couldn't find anything, let's return None\n    if not ann:\n        ann = None\n\n    return ann"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsave the segment estimations in a JAMS file.", "response": "def save_estimations(file_struct, times, labels, boundaries_id, labels_id,\n                     **params):\n    \"\"\"Saves the segment estimations in a JAMS file.\n\n    Parameters\n    ----------\n    file_struct : FileStruct\n        Object with the different file paths of the current file.\n    times : np.array or list\n        Estimated boundary times.\n        If `list`, estimated hierarchical boundaries.\n    labels : np.array(N, 2)\n        Estimated labels (None in case we are only storing boundary\n        evaluations).\n    boundaries_id : str\n        Boundary algorithm identifier.\n    labels_id : str\n        Labels algorithm identifier.\n    params : dict\n        Dictionary with additional parameters for both algorithms.\n    \"\"\"\n    # Remove features if they exist\n    params.pop(\"features\", None)\n\n    # Get duration\n    dur = get_duration(file_struct.features_file)\n\n    # Convert to intervals and sanity check\n    if 'numpy' in str(type(times)):\n        # Flat check\n        inters = utils.times_to_intervals(times)\n        assert len(inters) == len(labels), \"Number of boundary intervals \" \\\n            \"(%d) and labels (%d) do not match\" % (len(inters), len(labels))\n        # Put into lists to simplify the writing process later\n        inters = [inters]\n        labels = [labels]\n    else:\n        # Hierarchical check\n        inters = []\n        for level in range(len(times)):\n            est_inters = utils.times_to_intervals(times[level])\n            inters.append(est_inters)\n            assert len(inters[level]) == len(labels[level]), \\\n                \"Number of boundary intervals (%d) and labels (%d) do not \" \\\n                \"match in level %d\" % (len(inters[level]), len(labels[level]),\n                                       level)\n\n    # Create new estimation\n    namespace = \"multi_segment\" if params[\"hier\"] else \"segment_open\"\n    ann = jams.Annotation(namespace=namespace)\n\n    # Find estimation in file\n    if os.path.isfile(file_struct.est_file):\n        jam = jams.load(file_struct.est_file, validate=False)\n        curr_ann = find_estimation(jam, boundaries_id, labels_id, params)\n        if curr_ann is not None:\n            curr_ann.data = ann.data  # cleanup all data\n            ann = curr_ann  # This will overwrite the existing estimation\n        else:\n            jam.annotations.append(ann)\n    else:\n        # Create new JAMS if it doesn't exist\n        jam = jams.JAMS()\n        jam.file_metadata.duration = dur\n        jam.annotations.append(ann)\n\n    # Save metadata and parameters\n    ann.annotation_metadata.version = msaf.__version__\n    ann.annotation_metadata.data_source = \"MSAF\"\n    sandbox = {}\n    sandbox[\"boundaries_id\"] = boundaries_id\n    sandbox[\"labels_id\"] = labels_id\n    sandbox[\"timestamp\"] = \\\n        datetime.datetime.today().strftime(\"%Y/%m/%d %H:%M:%S\")\n    for key in params:\n        sandbox[key] = params[key]\n    ann.sandbox = sandbox\n\n    # Save actual data\n    for i, (level_inters, level_labels) in enumerate(zip(inters, labels)):\n        for bound_inter, label in zip(level_inters, level_labels):\n            dur = float(bound_inter[1]) - float(bound_inter[0])\n            label = chr(int(label) + 65)\n            if params[\"hier\"]:\n                value = {\"label\": label, \"level\": i}\n            else:\n                value = label\n            ann.append(time=bound_inter[0], duration=dur,\n                       value=value)\n\n    # Write results\n    jam.save(file_struct.est_file)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_all_boundary_algorithms():\n    algo_ids = []\n    for name in msaf.algorithms.__all__:\n        module = eval(msaf.algorithms.__name__ + \".\" + name)\n        if module.is_boundary_type:\n            algo_ids.append(module.algo_id)\n    return algo_ids", "response": "Gets all the possible boundary algorithms in MSAF."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_all_label_algorithms():\n    algo_ids = []\n    for name in msaf.algorithms.__all__:\n        module = eval(msaf.algorithms.__name__ + \".\" + name)\n        if module.is_label_type:\n            algo_ids.append(module.algo_id)\n    return algo_ids", "response": "Gets all the possible label algorithms in MSAF."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_configuration(feature, annot_beats, framesync, boundaries_id,\n                      labels_id):\n    \"\"\"Gets the configuration dictionary from the current parameters of the\n    algorithms to be evaluated.\"\"\"\n    config = {}\n    config[\"annot_beats\"] = annot_beats\n    config[\"feature\"] = feature\n    config[\"framesync\"] = framesync\n    bound_config = {}\n    if boundaries_id != \"gt\":\n        bound_config = \\\n            eval(msaf.algorithms.__name__ + \".\" + boundaries_id).config\n        config.update(bound_config)\n    if labels_id is not None:\n        label_config = \\\n            eval(msaf.algorithms.__name__ + \".\" + labels_id).config\n\n        # Make sure we don't have parameter name duplicates\n        if labels_id != boundaries_id:\n            overlap = set(bound_config.keys()). \\\n                intersection(set(label_config.keys()))\n            assert len(overlap) == 0, \\\n                \"Parameter %s must not exist both in %s and %s algorithms\" % \\\n                (overlap, boundaries_id, labels_id)\n        config.update(label_config)\n    return config", "response": "Gets the configuration dictionary from the current parameters of the the\n    algorithms to be evaluated."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the files of the given dataset.", "response": "def get_dataset_files(in_path):\n    \"\"\"Gets the files of the given dataset.\"\"\"\n    # Get audio files\n    audio_files = []\n    for ext in ds_config.audio_exts:\n        audio_files += glob.glob(\n            os.path.join(in_path, ds_config.audio_dir, \"*\" + ext))\n\n    # Make sure directories exist\n    utils.ensure_dir(os.path.join(in_path, ds_config.features_dir))\n    utils.ensure_dir(os.path.join(in_path, ds_config.estimations_dir))\n    utils.ensure_dir(os.path.join(in_path, ds_config.references_dir))\n\n    # Get the file structs\n    file_structs = []\n    for audio_file in audio_files:\n        file_structs.append(FileStruct(audio_file))\n\n    # Sort by audio file name\n    file_structs = sorted(file_structs,\n                          key=lambda file_struct: file_struct.audio_file)\n\n    return file_structs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading hierarchical references from a jams file.", "response": "def read_hier_references(jams_file, annotation_id=0, exclude_levels=[]):\n    \"\"\"Reads hierarchical references from a jams file.\n\n    Parameters\n    ----------\n    jams_file : str\n        Path to the jams file.\n    annotation_id : int > 0\n        Identifier of the annotator to read from.\n    exclude_levels: list\n        List of levels to exclude. Empty list to include all levels.\n\n    Returns\n    -------\n    hier_bounds : list\n        List of the segment boundary times in seconds for each level.\n    hier_labels : list\n        List of the segment labels for each level.\n    hier_levels : list\n        List of strings for the level identifiers.\n    \"\"\"\n    hier_bounds = []\n    hier_labels = []\n    hier_levels = []\n    jam = jams.load(jams_file)\n    namespaces = [\"segment_salami_upper\", \"segment_salami_function\",\n                  \"segment_open\", \"segment_tut\", \"segment_salami_lower\"]\n\n    # Remove levels if needed\n    for exclude in exclude_levels:\n        if exclude in namespaces:\n            namespaces.remove(exclude)\n\n    # Build hierarchy references\n    for ns in namespaces:\n        ann = jam.search(namespace=ns)\n        if not ann:\n            continue\n        ref_inters, ref_labels = ann[annotation_id].to_interval_values()\n        hier_bounds.append(utils.intervals_to_times(ref_inters))\n        hier_labels.append(ref_labels)\n        hier_levels.append(ns)\n\n    return hier_bounds, hier_labels, hier_levels"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_duration(features_file):\n    with open(features_file) as f:\n        feats = json.load(f)\n    return float(feats[\"globals\"][\"dur\"])", "response": "Reads the duration of a given features file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_mirex(times, labels, out_file):\n    inters = msaf.utils.times_to_intervals(times)\n    assert len(inters) == len(labels)\n    out_str = \"\"\n    for inter, label in zip(inters, labels):\n        out_str += \"%.3f\\t%.3f\\t%s\\n\" % (inter[0], inter[1], label)\n    with open(out_file, \"w\") as f:\n        f.write(out_str[:-1])", "response": "Writes the results to a MIREX file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_dataset_file(self, dir, ext):\n        audio_file_ext = \".\" + self.audio_file.split(\".\")[-1]\n        base_file = os.path.basename(self.audio_file).replace(\n            audio_file_ext, ext)\n        return os.path.join(self.ds_path, dir, base_file)", "response": "Gets the desired dataset file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef processFlat(self):\n        # Preprocess to obtain features (array(n_frames, n_features))\n        F = self._preprocess()\n\n        # Do something with the default parameters\n        # (these are defined in the in the config.py file).\n        assert self.config[\"my_param1\"] == 1.0\n\n        # Identify boundaries in frame indeces with the new algorithm\n        my_bounds = np.array([0, F.shape[0] - 1])\n\n        # Label the segments (use -1 to have empty segments)\n        my_labels = np.ones(len(my_bounds) - 1) * -1\n\n        # Post process estimations\n        est_idxs, est_labels = self._postprocess(my_bounds, my_labels)\n\n        # We're done!\n        return est_idxs, est_labels", "response": "Main function for processing the flat data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload a ground - truth segmentation and align times to the nearest detected beats.", "response": "def align_segmentation(beat_times, song):\n    '''Load a ground-truth segmentation, and align times to the nearest\n    detected beats.\n\n    Arguments:\n        beat_times -- array\n        song -- path to the audio file\n\n    Returns:\n        segment_beats -- array\n            beat-aligned segment boundaries\n\n        segment_times -- array\n            true segment times\n\n        segment_labels -- array\n            list of segment labels\n    '''\n    try:\n        segment_times, segment_labels = msaf.io.read_references(song)\n    except:\n        return None, None, None\n    segment_times = np.asarray(segment_times)\n\n    # Map to intervals\n    segment_intervals = msaf.utils.times_to_intervals(segment_times)\n\n    # Map beats to intervals\n    beat_intervals = np.asarray(zip(beat_times[:-1], beat_times[1:]))\n\n    # Map beats to segments\n    beat_segment_ids = librosa.util.match_intervals(beat_intervals,\n                                                    segment_intervals)\n\n    segment_beats = []\n    segment_times_out = []\n    segment_labels_out = []\n\n    # print segment_times, beat_segment_ids, len(beat_times),\n    # len(beat_segment_ids)\n    for i in range(segment_times.shape[0]):\n        hits = np.argwhere(beat_segment_ids == i)\n        if len(hits) > 0 and i < len(segment_intervals) and \\\n                i < len(segment_labels):\n            segment_beats.extend(hits[0])\n            segment_times_out.append(segment_intervals[i, :])\n            segment_labels_out.append(segment_labels[i])\n\n    # Pull out the segment start times\n    segment_beats = list(segment_beats)\n    # segment_times_out = np.asarray(\n    #  segment_times_out)[:, 0].squeeze().reshape((-1, 1))\n\n    # if segment_times_out.ndim == 0:\n    #    segment_times_out = segment_times_out[np.newaxis]\n    segment_times_out = segment_times\n\n    return segment_beats, segment_times_out, segment_labels_out"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nestimate the beats using librosa.", "response": "def estimate_beats(self):\n        \"\"\"Estimates the beats using librosa.\n\n        Returns\n        -------\n        times: np.array\n            Times of estimated beats in seconds.\n        frames: np.array\n            Frame indeces of estimated beats.\n        \"\"\"\n        # Compute harmonic-percussive source separation if needed\n        if self._audio_percussive is None:\n            self._audio_harmonic, self._audio_percussive = self.compute_HPSS()\n\n        # Compute beats\n        tempo, frames = librosa.beat.beat_track(\n            y=self._audio_percussive, sr=self.sr,\n            hop_length=self.hop_length)\n\n        # To times\n        times = librosa.frames_to_time(frames, sr=self.sr,\n                                       hop_length=self.hop_length)\n\n        # TODO: Is this really necessary?\n        if len(times) > 0 and times[0] == 0:\n            times = times[1:]\n            frames = frames[1:]\n\n        return times, frames"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_ann_beats(self):\n        times, frames = (None, None)\n\n        # Read annotations if they exist in correct folder\n        if os.path.isfile(self.file_struct.ref_file):\n            try:\n                jam = jams.load(self.file_struct.ref_file)\n            except TypeError:\n                logging.warning(\n                    \"Can't read JAMS file %s. Maybe it's not \"\n                    \"compatible with current JAMS version?\" %\n                    self.file_struct.ref_file)\n                return times, frames\n            beat_annot = jam.search(namespace=\"beat.*\")\n\n            # If beat annotations exist, get times and frames\n            if len(beat_annot) > 0:\n                beats_inters, _ = beat_annot[0].to_interval_values()\n                times = beats_inters[:, 0]\n                frames = librosa.time_to_frames(times, sr=self.sr,\n                                                hop_length=self.hop_length)\n        return times, frames", "response": "Reads the annotated beats if available."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compute_beat_sync_features(self, beat_frames, beat_times, pad):\n        if beat_frames is None:\n            return None, None\n\n        # Make beat synchronous\n        beatsync_feats = librosa.util.utils.sync(self._framesync_features.T,\n                                                 beat_frames, pad=pad).T\n\n        # Assign times (and add last time if padded)\n        beatsync_times = np.copy(beat_times)\n        if beatsync_times.shape[0] != beatsync_feats.shape[0]:\n            beatsync_times = np.concatenate((beatsync_times,\n                                             [self._framesync_times[-1]]))\n        return beatsync_feats, beatsync_times", "response": "Compute the features for the beat - synchronous."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_features(self, tol=1e-3):\n        try:\n            # Read JSON file\n            with open(self.file_struct.features_file) as f:\n                feats = json.load(f)\n\n            # Store duration\n            if self.dur is None:\n                self.dur = float(feats[\"globals\"][\"dur\"])\n\n            # Check that we have the correct global parameters\n            assert(np.isclose(\n                self.dur, float(feats[\"globals\"][\"dur\"]), rtol=tol))\n            assert(self.sr == int(feats[\"globals\"][\"sample_rate\"]))\n            assert(self.hop_length == int(feats[\"globals\"][\"hop_length\"]))\n            assert(os.path.basename(self.file_struct.audio_file) ==\n                   os.path.basename(feats[\"globals\"][\"audio_file\"]))\n\n            # Check for specific features params\n            feat_params_err = FeatureParamsError(\n                \"Couldn't find features for %s id in file %s\" %\n                (self.get_id(), self.file_struct.features_file))\n            if self.get_id() not in feats.keys():\n                raise feat_params_err\n            for param_name in self.get_param_names():\n                value = getattr(self, param_name)\n                if hasattr(value, '__call__'):\n                    # Special case of functions\n                    if value.__name__ != \\\n                            feats[self.get_id()][\"params\"][param_name]:\n                        raise feat_params_err\n                else:\n                    if str(value) != \\\n                            feats[self.get_id()][\"params\"][param_name]:\n                        raise feat_params_err\n\n            # Store actual features\n            self._est_beats_times = np.array(feats[\"est_beats\"])\n            self._est_beatsync_times = np.array(feats[\"est_beatsync_times\"])\n            self._est_beats_frames = librosa.core.time_to_frames(\n                self._est_beats_times, sr=self.sr, hop_length=self.hop_length)\n            self._framesync_features = \\\n                np.array(feats[self.get_id()][\"framesync\"])\n            self._est_beatsync_features = \\\n                np.array(feats[self.get_id()][\"est_beatsync\"])\n\n            # Read annotated beats if available\n            if \"ann_beats\" in feats.keys():\n                self._ann_beats_times = np.array(feats[\"ann_beats\"])\n                self._ann_beatsync_times = np.array(feats[\"ann_beatsync_times\"])\n                self._ann_beats_frames = librosa.core.time_to_frames(\n                    self._ann_beats_times, sr=self.sr,\n                    hop_length=self.hop_length)\n                self._ann_beatsync_features = \\\n                    np.array(feats[self.get_id()][\"ann_beatsync\"])\n        except KeyError:\n            raise WrongFeaturesFormatError(\n                \"The features file %s is not correctly formatted\" %\n                self.file_struct.features_file)\n        except AssertionError:\n            raise FeaturesNotFound(\n                \"The features for the given parameters were not found in \"\n                \"features file %s\" % self.file_struct.features_file)\n        except IOError:\n            raise NoFeaturesFileError(\"Could not find features file %s\",\n                                      self.file_struct.features_file)", "response": "Reads the features from a file and stores them in the current\n            object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_features(self):\n        out_json = collections.OrderedDict()\n        try:\n            # Only save the necessary information\n            self.read_features()\n        except (WrongFeaturesFormatError, FeaturesNotFound,\n                NoFeaturesFileError):\n            # We need to create the file or overwite it\n            # Metadata\n            out_json = collections.OrderedDict({\"metadata\": {\n                \"versions\": {\"librosa\": librosa.__version__,\n                             \"msaf\": msaf.__version__,\n                             \"numpy\": np.__version__},\n                \"timestamp\": datetime.datetime.today().strftime(\n                    \"%Y/%m/%d %H:%M:%S\")}})\n\n            # Global parameters\n            out_json[\"globals\"] = {\n                \"dur\": self.dur,\n                \"sample_rate\": self.sr,\n                \"hop_length\": self.hop_length,\n                \"audio_file\": self.file_struct.audio_file\n            }\n\n            # Beats\n            out_json[\"est_beats\"] = self._est_beats_times.tolist()\n            out_json[\"est_beatsync_times\"] = self._est_beatsync_times.tolist()\n            if self._ann_beats_times is not None:\n                out_json[\"ann_beats\"] = self._ann_beats_times.tolist()\n                out_json[\"ann_beatsync_times\"] = self._ann_beatsync_times.tolist()\n        except FeatureParamsError:\n            # We have other features in the file, simply add these ones\n            with open(self.file_struct.features_file) as f:\n                out_json = json.load(f)\n        finally:\n            # Specific parameters of the current features\n            out_json[self.get_id()] = {}\n            out_json[self.get_id()][\"params\"] = {}\n            for param_name in self.get_param_names():\n                value = getattr(self, param_name)\n                # Check for special case of functions\n                if hasattr(value, '__call__'):\n                    value = value.__name__\n                else:\n                    value = str(value)\n                out_json[self.get_id()][\"params\"][param_name] = value\n\n            # Actual features\n            out_json[self.get_id()][\"framesync\"] = \\\n                self._framesync_features.tolist()\n            out_json[self.get_id()][\"est_beatsync\"] = \\\n                self._est_beatsync_features.tolist()\n            if self._ann_beatsync_features is not None:\n                out_json[self.get_id()][\"ann_beatsync\"] = \\\n                    self._ann_beatsync_features.tolist()\n\n            # Save it\n            with open(self.file_struct.features_file, \"w\") as f:\n                json.dump(out_json, f, indent=2)", "response": "Saves the current features to a JSON file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the parameter names for these features avoiding the global parameters.", "response": "def get_param_names(self):\n        \"\"\"Returns the parameter names for these features, avoiding\n        the global parameters.\"\"\"\n        return [name for name in vars(self) if not name.startswith('_') and\n                name not in self._global_param_names]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _compute_framesync_times(self):\n        self._framesync_times = librosa.core.frames_to_time(\n            np.arange(self._framesync_features.shape[0]), self.sr,\n            self.hop_length)", "response": "Computes the framesync times based on the framesync features."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes all the features for all the beatsync and framesync files from the audio file.", "response": "def _compute_all_features(self):\n        \"\"\"Computes all the features (beatsync, framesync) from the audio.\"\"\"\n        # Read actual audio waveform\n        self._audio, _ = librosa.load(self.file_struct.audio_file,\n                                      sr=self.sr)\n\n        # Get duration of audio file\n        self.dur = len(self._audio) / float(self.sr)\n\n        # Compute actual features\n        self._framesync_features = self.compute_features()\n\n        # Compute framesync times\n        self._compute_framesync_times()\n\n        # Compute/Read beats\n        self._est_beats_times, self._est_beats_frames = self.estimate_beats()\n        self._ann_beats_times, self._ann_beats_frames = self.read_ann_beats()\n\n        # Beat-Synchronize\n        pad = True  # Always append to the end of the features\n        self._est_beatsync_features, self._est_beatsync_times = \\\n            self.compute_beat_sync_features(self._est_beats_frames,\n                                            self._est_beats_times, pad)\n        self._ann_beatsync_features, self._ann_beatsync_times = \\\n            self.compute_beat_sync_features(self._ann_beats_frames,\n                                            self._ann_beats_times, pad)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef frame_times(self):\n        frame_times = None\n        # Make sure we have already computed the features\n        self.features\n        if self.feat_type is FeatureTypes.framesync:\n            self._compute_framesync_times()\n            frame_times = self._framesync_times\n        elif self.feat_type is FeatureTypes.est_beatsync:\n            frame_times = self._est_beatsync_times\n        elif self.feat_type is FeatureTypes.ann_beatsync:\n            frame_times = self._ann_beatsync_times\n\n        return frame_times", "response": "This getter returns the frame times for the corresponding type of\n        features."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nselecting the features from the given parameters.", "response": "def select_features(cls, features_id, file_struct, annot_beats, framesync):\n        \"\"\"Selects the features from the given parameters.\n\n        Parameters\n        ----------\n        features_id: str\n            The identifier of the features (it must be a key inside the\n            `features_registry`)\n        file_struct: msaf.io.FileStruct\n            The file struct containing the files to extract the features from\n        annot_beats: boolean\n            Whether to use annotated (`True`) or estimated (`False`) beats\n        framesync: boolean\n            Whether to use framesync (`True`) or beatsync (`False`) features\n\n        Returns\n        -------\n        features: obj\n            The actual features object that inherits from `msaf.Features`\n        \"\"\"\n        if not annot_beats and framesync:\n            feat_type = FeatureTypes.framesync\n        elif annot_beats and not framesync:\n            feat_type = FeatureTypes.ann_beatsync\n        elif not annot_beats and not framesync:\n            feat_type = FeatureTypes.est_beatsync\n        else:\n            raise FeatureTypeNotFound(\"Type of features not valid.\")\n\n        # Select features with default parameters\n        if features_id not in features_registry.keys():\n            raise FeaturesNotFound(\n                \"The features '%s' are invalid (valid features are %s)\"\n                % (features_id, features_registry.keys()))\n\n        return features_registry[features_id](file_struct, feat_type)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _preprocess(self, valid_features=[\"pcp\", \"tonnetz\", \"mfcc\",\n                                          \"cqt\", \"tempogram\"]):\n        \"\"\"This method obtains the actual features.\"\"\"\n        # Use specific feature\n        if self.feature_str not in valid_features:\n            raise RuntimeError(\"Feature %s in not valid for algorithm: %s \"\n                               \"(valid features are %s).\" %\n                               (self.feature_str, __name__, valid_features))\n        else:\n            try:\n                F = self.features.features\n            except KeyError:\n                raise RuntimeError(\"Feature %s in not supported by MSAF\" %\n                                   (self.feature_str))\n\n        return F", "response": "This method obtains the actual features."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _postprocess(self, est_idxs, est_labels):\n        # Make sure we are using the previously input bounds, if any\n        if self.in_bound_idxs is not None:\n            F = self._preprocess()\n            est_labels = U.synchronize_labels(self.in_bound_idxs, est_idxs,\n                                              est_labels, F.shape[0])\n            est_idxs = self.in_bound_idxs\n\n        # Remove empty segments if needed\n        est_idxs, est_labels = U.remove_empty_segments(est_idxs, est_labels)\n\n        assert len(est_idxs) - 1 == len(est_labels), \"Number of boundaries \" \\\n            \"(%d) and number of labels(%d) don't match\" % (len(est_idxs),\n                                                           len(est_labels))\n\n        # Make sure the indeces are integers\n        est_idxs = np.asarray(est_idxs, dtype=int)\n\n        return est_idxs, est_labels", "response": "Post processes the estimations from the algorithm removing empty segments and making sure the lenghts of the boundaries and labels match."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process(in_path, annot_beats=False, feature=\"mfcc\", framesync=False,\n            boundaries_id=\"gt\", labels_id=None, n_jobs=4, config=None):\n    \"\"\"Sweeps parameters across the specified algorithm.\"\"\"\n\n    results_file = \"results_sweep_boundsE%s_labelsE%s.csv\" % (boundaries_id,\n                                                              labels_id)\n\n    if labels_id == \"cnmf3\" or boundaries_id == \"cnmf3\":\n        config = io.get_configuration(feature, annot_beats, framesync,\n                                      boundaries_id, labels_id)\n\n        hh = range(15, 33)\n        RR = range(15, 40)\n        ranks = range(3, 6)\n        RR_labels = range(11, 12)\n        ranks_labels = range(6, 7)\n        all_results = pd.DataFrame()\n        for rank in ranks:\n            for h in hh:\n                for R in RR:\n                    for rank_labels in ranks_labels:\n                        for R_labels in RR_labels:\n                            config[\"h\"] = h\n                            config[\"R\"] = R\n                            config[\"rank\"] = rank\n                            config[\"rank_labels\"] = rank_labels\n                            config[\"R_labels\"] = R_labels\n                            config[\"features\"] = None\n\n                            # Run process\n                            msaf.run.process(\n                                in_path, n_jobs=n_jobs,\n                                boundaries_id=boundaries_id,\n                                labels_id=labels_id, config=config)\n\n                            # Compute evaluations\n                            results = msaf.eval.process(\n                                in_path, boundaries_id, labels_id,\n                                save=True, n_jobs=n_jobs, config=config)\n\n                            # Save avg results\n                            new_columns = {\"config_h\": h, \"config_R\": R,\n                                           \"config_rank\": rank,\n                                           \"config_R_labels\": R_labels,\n                                           \"config_rank_labels\": rank_labels}\n                            results = results.append([new_columns],\n                                                     ignore_index=True)\n                            all_results = all_results.append(results.mean(),\n                                                             ignore_index=True)\n                            all_results.to_csv(results_file)\n\n    elif labels_id is None and boundaries_id == \"sf\":\n        config = io.get_configuration(feature, annot_beats, framesync,\n                                      boundaries_id, labels_id)\n\n        MM = range(20, 32)\n        mm = range(3, 4)\n        kk = np.arange(0.03, 0.1, 0.01)\n        Mpp = range(16, 32)\n        ott = np.arange(0.02, 0.1, 0.01)\n        all_results = pd.DataFrame()\n        for k in kk:\n            for ot in ott:\n                for m in mm:\n                    for M in MM:\n                        for Mp in Mpp:\n                            config[\"M_gaussian\"] = M\n                            config[\"m_embedded\"] = m\n                            config[\"k_nearest\"] = k\n                            config[\"Mp_adaptive\"] = Mp\n                            config[\"offset_thres\"] = ot\n                            config[\"features\"] = None\n\n                            # Run process\n                            msaf.run.process(\n                                in_path, n_jobs=n_jobs,\n                                boundaries_id=boundaries_id,\n                                labels_id=labels_id, config=config)\n\n                            # Compute evaluations\n                            results = msaf.eval.process(\n                                in_path, boundaries_id, labels_id,\n                                save=True, n_jobs=n_jobs, config=config)\n\n                            # Save avg results\n                            new_columns = {\"config_M\": M, \"config_m\": m,\n                                           \"config_k\": k, \"config_Mp\": Mp,\n                                           \"config_ot\": ot}\n                            results = results.append([new_columns],\n                                                     ignore_index=True)\n                            all_results = all_results.append(results.mean(),\n                                                             ignore_index=True)\n                            all_results.to_csv(results_file)\n\n    else:\n        logging.error(\"Can't sweep parameters for %s algorithm. \"\n                      \"Implement me! :D\")", "response": "Process the input file and return a pandas DataFrame of the results."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef main():\n    parser = argparse.ArgumentParser(\n        description=\"Runs the speficied algorithm(s) on the MSAF \"\n        \"formatted dataset.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\"in_path\",\n                        action=\"store\",\n                        help=\"Input dataset\")\n    parser.add_argument(\"-f\",\n                        action=\"store\",\n                        dest=\"feature\",\n                        default=\"pcp\",\n                        type=str,\n                        help=\"Type of features\",\n                        choices=[\"pcp\", \"tonnetz\", \"mfcc\", \"cqt\", \"tempogram\"])\n    parser.add_argument(\"-b\",\n                        action=\"store_true\",\n                        dest=\"annot_beats\",\n                        help=\"Use annotated beats\",\n                        default=False)\n    parser.add_argument(\"-fs\",\n                        action=\"store_true\",\n                        dest=\"framesync\",\n                        help=\"Use frame-synchronous features\",\n                        default=False)\n    parser.add_argument(\"-bid\",\n                        action=\"store\",\n                        help=\"Boundary algorithm identifier\",\n                        dest=\"boundaries_id\",\n                        default=\"gt\",\n                        choices=[\"gt\"] +\n                        io.get_all_boundary_algorithms())\n    parser.add_argument(\"-lid\",\n                        action=\"store\",\n                        help=\"Label algorithm identifier\",\n                        dest=\"labels_id\",\n                        default=None,\n                        choices=io.get_all_label_algorithms())\n    parser.add_argument(\"-j\",\n                        action=\"store\",\n                        dest=\"n_jobs\",\n                        default=4,\n                        type=int,\n                        help=\"The number of threads to use\")\n    args = parser.parse_args()\n    start_time = time.time()\n\n    # Run the algorithm(s)\n    process(args.in_path, annot_beats=args.annot_beats, feature=args.feature,\n            framesync=args.framesync, boundaries_id=args.boundaries_id,\n            labels_id=args.labels_id, n_jobs=args.n_jobs)\n\n    # Done!\n    logging.info(\"Done! Took %.2f seconds.\" % (time.time() - start_time))", "response": "Main function to sweep parameters of a certain algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef main():\n    parser = argparse.ArgumentParser(\n        description=\"Runs the speficied algorithm(s) on the input file and \"\n        \"the results using the MIREX format.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\"-bid\",\n                        action=\"store\",\n                        help=\"Boundary algorithm identifier\",\n                        dest=\"boundaries_id\",\n                        default=msaf.config.default_bound_id,\n                        choices=[\"gt\"] +\n                        msaf.io.get_all_boundary_algorithms())\n    parser.add_argument(\"-lid\",\n                        action=\"store\",\n                        help=\"Label algorithm identifier\",\n                        dest=\"labels_id\",\n                        default=msaf.config.default_label_id,\n                        choices=msaf.io.get_all_label_algorithms())\n    parser.add_argument(\"-i\",\n                        action=\"store\",\n                        dest=\"in_file\",\n                        help=\"Input audio file\")\n    parser.add_argument(\"-o\",\n                        action=\"store\",\n                        dest=\"out_file\",\n                        help=\"Output file with the results\",\n                        default=\"out.txt\")\n\n    args = parser.parse_args()\n    start_time = time.time()\n\n    # Setup the logger\n    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s',\n                        level=logging.INFO)\n\n    # Run MSAF\n    params = {\n        \"annot_beats\": False,\n        \"feature\": \"cqt\",\n        \"framesync\": False,\n        \"boundaries_id\": args.boundaries_id,\n        \"labels_id\": args.labels_id,\n        \"n_jobs\": 1,\n        \"hier\": False,\n        \"sonify_bounds\": False,\n        \"plot\": False\n    }\n    res = msaf.run.process(args.in_file, **params)\n    msaf.io.write_mirex(res[0], res[1], args.out_file)\n\n    # Done!\n    logging.info(\"Done! Took %.2f seconds.\" % (time.time() - start_time))", "response": "Main function to parse the arguments and call the main process."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef print_results(results):\n    if len(results) == 0:\n        logging.warning(\"No results to print!\")\n        return\n    res = results.mean()\n    logging.info(\"Results:\\n%s\" % res)", "response": "Print all the results."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compute_results(ann_inter, est_inter, ann_labels, est_labels, bins,\n                    est_file, weight=0.58):\n    \"\"\"Compute the results using all the available evaluations.\n\n    Parameters\n    ----------\n    ann_inter : np.array\n        Annotated intervals in seconds.\n    est_inter : np.array\n        Estimated intervals in seconds.\n    ann_labels : np.array\n        Annotated labels.\n    est_labels : np.array\n        Estimated labels\n    bins : int\n        Number of bins for the information gain.\n    est_file : str\n        Path to the output file to store results.\n    weight: float\n        Weight the Precision and Recall values of the hit rate boundaries\n        differently (<1 will weight Precision higher, >1 will weight Recall\n        higher).\n        The default parameter (0.58) is the one proposed in (Nieto et al. 2014)\n\n    Return\n    ------\n    results : dict\n        Contains the results of all the evaluations for the given file.\n        Keys are the following:\n            track_id: Name of the track\n            HitRate_3F: F-measure of hit rate at 3 seconds\n            HitRate_3P: Precision of hit rate at 3 seconds\n            HitRate_3R: Recall of hit rate at 3 seconds\n            HitRate_0.5F: F-measure of hit rate at 0.5 seconds\n            HitRate_0.5P: Precision of hit rate at 0.5 seconds\n            HitRate_0.5R: Recall of hit rate at 0.5 seconds\n            HitRate_w3F: F-measure of hit rate at 3 seconds weighted\n            HitRate_w0.5F: F-measure of hit rate at 0.5 seconds weighted\n            HitRate_wt3F: F-measure of hit rate at 3 seconds weighted and\n                          trimmed\n            HitRate_wt0.5F: F-measure of hit rate at 0.5 seconds weighted\n                            and trimmed\n            HitRate_t3F: F-measure of hit rate at 3 seconds (trimmed)\n            HitRate_t3P: Precision of hit rate at 3 seconds (trimmed)\n            HitRate_t3F: Recall of hit rate at 3 seconds (trimmed)\n            HitRate_t0.5F: F-measure of hit rate at 0.5 seconds (trimmed)\n            HitRate_t0.5P: Precision of hit rate at 0.5 seconds (trimmed)\n            HitRate_t0.5R: Recall of hit rate at 0.5 seconds (trimmed)\n            DevA2E: Median deviation of annotation to estimation\n            DevE2A: Median deviation of estimation to annotation\n            D: Information gain\n            PWF: F-measure of pair-wise frame clustering\n            PWP: Precision of pair-wise frame clustering\n            PWR: Recall of pair-wise frame clustering\n            Sf: F-measure normalized entropy score\n            So: Oversegmentation normalized entropy score\n            Su: Undersegmentation normalized entropy score\n    \"\"\"\n    res = {}\n\n    # --Boundaries-- #\n    # Hit Rate standard\n    res[\"HitRate_3P\"], res[\"HitRate_3R\"], res[\"HitRate_3F\"] = \\\n        mir_eval.segment.detection(ann_inter, est_inter, window=3, trim=False)\n    res[\"HitRate_0.5P\"], res[\"HitRate_0.5R\"], res[\"HitRate_0.5F\"] = \\\n        mir_eval.segment.detection(ann_inter, est_inter, window=.5, trim=False)\n\n    # Hit rate trimmed\n    res[\"HitRate_t3P\"], res[\"HitRate_t3R\"], res[\"HitRate_t3F\"] = \\\n        mir_eval.segment.detection(ann_inter, est_inter, window=3, trim=True)\n    res[\"HitRate_t0.5P\"], res[\"HitRate_t0.5R\"], res[\"HitRate_t0.5F\"] = \\\n        mir_eval.segment.detection(ann_inter, est_inter, window=.5, trim=True)\n\n    # Hit rate weighted\n    _, _, res[\"HitRate_w3F\"] = mir_eval.segment.detection(\n        ann_inter, est_inter, window=3, trim=False, beta=weight)\n    _, _, res[\"HitRate_w0.5F\"] = mir_eval.segment.detection(\n        ann_inter, est_inter, window=.5, trim=False, beta=weight)\n\n    # Hit rate weighted and trimmed\n    _, _, res[\"HitRate_wt3F\"] = mir_eval.segment.detection(\n        ann_inter, est_inter, window=3, trim=True, beta=weight)\n    _, _, res[\"HitRate_wt0.5F\"] = mir_eval.segment.detection(\n        ann_inter, est_inter, window=.5, trim=True, beta=weight)\n\n    # Information gain\n    res[\"D\"] = compute_information_gain(ann_inter, est_inter, est_file,\n                                        bins=bins)\n\n    # Median Deviations\n    res[\"DevR2E\"], res[\"DevE2R\"] = mir_eval.segment.deviation(\n        ann_inter, est_inter, trim=False)\n    res[\"DevtR2E\"], res[\"DevtE2R\"] = mir_eval.segment.deviation(\n        ann_inter, est_inter, trim=True)\n\n    # --Labels-- #\n    if est_labels is not None and (\"-1\" in est_labels or \"@\" in est_labels):\n        est_labels = None\n    if est_labels is not None and len(est_labels) != 0:\n        # Align labels with intervals\n        ann_labels = list(ann_labels)\n        est_labels = list(est_labels)\n        ann_inter, ann_labels = mir_eval.util.adjust_intervals(ann_inter,\n                                                               ann_labels)\n        est_inter, est_labels = mir_eval.util.adjust_intervals(\n            est_inter, est_labels, t_min=0.0, t_max=ann_inter.max())\n\n        # Pair-wise frame clustering\n        res[\"PWP\"], res[\"PWR\"], res[\"PWF\"] = mir_eval.segment.pairwise(\n            ann_inter, ann_labels, est_inter, est_labels)\n\n        # Normalized Conditional Entropies\n        res[\"So\"], res[\"Su\"], res[\"Sf\"] = mir_eval.segment.nce(\n            ann_inter, ann_labels, est_inter, est_labels)\n\n    # Names\n    base = os.path.basename(est_file)\n    res[\"track_id\"] = base[:-5]\n    res[\"ds_name\"] = base.split(\"_\")[0]\n\n    return res", "response": "Compute the results of all the available evaluations for the given file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the results by using the ground truth dataset identified by by the annotator parameter.", "response": "def compute_gt_results(est_file, ref_file, boundaries_id, labels_id, config,\n                       bins=251, annotator_id=0):\n    \"\"\"Computes the results by using the ground truth dataset identified by\n    the annotator parameter.\n\n    Return\n    ------\n    results : dict\n        Dictionary of the results (see function compute_results).\n    \"\"\"\n    if config[\"hier\"]:\n        ref_times, ref_labels, ref_levels = \\\n            msaf.io.read_hier_references(\n                ref_file, annotation_id=annotator_id,\n                exclude_levels=[\"segment_salami_function\"])\n    else:\n        jam = jams.load(ref_file, validate=False)\n        ann = jam.search(namespace='segment_.*')[annotator_id]\n        ref_inter, ref_labels = ann.to_interval_values()\n\n    # Read estimations with correct configuration\n    est_inter, est_labels = io.read_estimations(est_file, boundaries_id,\n                                                labels_id, **config)\n\n    # Compute the results and return\n    logging.info(\"Evaluating %s\" % os.path.basename(est_file))\n    if config[\"hier\"]:\n        # Hierarchical\n        assert len(est_inter) == len(est_labels), \"Same number of levels \" \\\n            \"are required in the boundaries and labels for the hierarchical \" \\\n            \"evaluation.\"\n        est_times = []\n        est_labels = []\n\n        # Sort based on how many segments per level\n        est_inter = sorted(est_inter, key=lambda level: len(level))\n\n        for inter in est_inter:\n            est_times.append(msaf.utils.intervals_to_times(inter))\n            # Add fake labels (hierarchical eval does not use labels --yet--)\n            est_labels.append(np.ones(len(est_times[-1]) - 1) * -1)\n\n        # Align the times\n        utils.align_end_hierarchies(est_times, ref_times, thres=1)\n\n        # To intervals\n        est_hier = [utils.times_to_intervals(times) for times in est_times]\n        ref_hier = [utils.times_to_intervals(times) for times in ref_times]\n\n        # Compute evaluations\n        res = {}\n        res[\"t_recall10\"], res[\"t_precision10\"], res[\"t_measure10\"] = \\\n            mir_eval.hierarchy.tmeasure(ref_hier, est_hier, window=10)\n        res[\"t_recall15\"], res[\"t_precision15\"], res[\"t_measure15\"] = \\\n            mir_eval.hierarchy.tmeasure(ref_hier, est_hier, window=15)\n\n        res[\"track_id\"] = os.path.basename(est_file)[:-5]\n        return res\n    else:\n        # Flat\n        return compute_results(ref_inter, est_inter, ref_labels, est_labels,\n                               bins, est_file)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the information gain of the est_file from the annotated intervals and the estimated intervals.", "response": "def compute_information_gain(ann_inter, est_inter, est_file, bins):\n    \"\"\"Computes the information gain of the est_file from the annotated\n    intervals and the estimated intervals.\"\"\"\n    ann_times = utils.intervals_to_times(ann_inter)\n    est_times = utils.intervals_to_times(est_inter)\n    return mir_eval.beat.information_gain(ann_times, est_times, bins=bins)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef process_track(file_struct, boundaries_id, labels_id, config,\n                  annotator_id=0):\n    \"\"\"Processes a single track.\n\n    Parameters\n    ----------\n    file_struct : object (FileStruct) or str\n        File struct or full path of the audio file to be evaluated.\n    boundaries_id : str\n        Identifier of the boundaries algorithm.\n    labels_id : str\n        Identifier of the labels algorithm.\n    config : dict\n        Configuration of the algorithms to be evaluated.\n    annotator_id : int\n        Number identifiying the annotator.\n\n    Returns\n    -------\n    one_res : dict\n        Dictionary of the results (see function compute_results).\n    \"\"\"\n    # Convert to file_struct if string is passed\n    if isinstance(file_struct, six.string_types):\n        file_struct = io.FileStruct(file_struct)\n\n    est_file = file_struct.est_file\n    ref_file = file_struct.ref_file\n\n    # Sanity check\n    assert os.path.basename(est_file)[:-4] == \\\n        os.path.basename(ref_file)[:-4], \"File names are different %s --- %s\" \\\n        % (os.path.basename(est_file)[:-4], os.path.basename(ref_file)[:-4])\n\n    if not os.path.isfile(ref_file):\n        raise NoReferencesError(\"Reference file %s does not exist. You must \"\n                                \"have annotated references to run \"\n                                \"evaluations.\" % ref_file)\n\n    one_res = compute_gt_results(est_file, ref_file, boundaries_id, labels_id,\n                                 config, annotator_id=annotator_id)\n\n    return one_res", "response": "Processes a single audio file and returns a dictionary of the results."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the file name to store the results.", "response": "def get_results_file_name(boundaries_id, labels_id, config,\n                          annotator_id):\n    \"\"\"Based on the config and the dataset, get the file name to store the\n    results.\"\"\"\n    utils.ensure_dir(msaf.config.results_dir)\n    file_name = os.path.join(msaf.config.results_dir, \"results\")\n    file_name += \"_boundsE%s_labelsE%s\" % (boundaries_id, labels_id)\n    file_name += \"_annotatorE%d\" % (annotator_id)\n    sorted_keys = sorted(config.keys(), key=str.lower)\n    for key in sorted_keys:\n        file_name += \"_%sE%s\" % (key, str(config[key]).replace(\"/\", \"_\"))\n\n    # Check for max file length\n    if len(file_name) > 255 - len(msaf.config.results_ext):\n        file_name = file_name[:255 - len(msaf.config.results_ext)]\n\n    return file_name + msaf.config.results_ext"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process(in_path, boundaries_id=msaf.config.default_bound_id,\n            labels_id=msaf.config.default_label_id, annot_beats=False,\n            framesync=False, feature=\"pcp\", hier=False, save=False,\n            out_file=None, n_jobs=4, annotator_id=0, config=None):\n    \"\"\"Main process to evaluate algorithms' results.\n\n    Parameters\n    ----------\n    in_path : str\n        Path to the dataset root folder.\n    boundaries_id : str\n        Boundaries algorithm identifier (e.g. siplca, cnmf)\n    labels_id : str\n        Labels algorithm identifier (e.g. siplca, cnmf)\n    ds_name : str\n        Name of the dataset to be evaluated (e.g. SALAMI). * stands for all.\n    annot_beats : boolean\n        Whether to use the annotated beats or not.\n    framesync: str\n        Whether to use framesync features or not (default: False -> beatsync)\n    feature: str\n        String representing the feature to be used (e.g. pcp, mfcc, tonnetz)\n    hier : bool\n        Whether to compute a hierarchical or flat segmentation.\n    save: boolean\n        Whether to save the results into the `out_file` csv file.\n    out_file: str\n        Path to the csv file to save the results (if `None` and `save = True`\n        it will save the results in the default file name obtained by\n        calling `get_results_file_name`).\n    n_jobs: int\n        Number of processes to run in parallel. Only available in collection\n        mode.\n    annotator_id : int\n        Number identifiying the annotator.\n    config: dict\n        Dictionary containing custom configuration parameters for the\n        algorithms.  If None, the default parameters are used.\n\n    Return\n    ------\n    results : pd.DataFrame\n        DataFrame containing the evaluations for each file.\n    \"\"\"\n\n    # Set up configuration based on algorithms parameters\n    if config is None:\n        config = io.get_configuration(feature, annot_beats, framesync,\n                                      boundaries_id, labels_id)\n\n    # Hierarchical segmentation\n    config[\"hier\"] = hier\n\n    # Remove actual features\n    config.pop(\"features\", None)\n\n    # Get out file in case we want to save results\n    if out_file is None:\n        out_file = get_results_file_name(boundaries_id, labels_id, config,\n                                         annotator_id)\n\n    # If out_file already exists, read and return them\n    if os.path.exists(out_file):\n        logging.warning(\"Results already exists, reading from file %s\" %\n                        out_file)\n        results = pd.read_csv(out_file)\n        print_results(results)\n        return results\n\n    # Perform actual evaluations\n    if os.path.isfile(in_path):\n        # Single File mode\n        evals = [process_track(in_path, boundaries_id, labels_id, config,\n                               annotator_id=annotator_id)]\n    else:\n        # Collection mode\n        # Get files\n        file_structs = io.get_dataset_files(in_path)\n\n        # Evaluate in parallel\n        logging.info(\"Evaluating %d tracks...\" % len(file_structs))\n        evals = Parallel(n_jobs=n_jobs)(delayed(process_track)(\n            file_struct, boundaries_id, labels_id, config,\n            annotator_id=annotator_id) for file_struct in file_structs[:])\n\n    # Aggregate evaluations in pandas format\n    results = pd.DataFrame()\n    for e in evals:\n        if e != []:\n            results = results.append(e, ignore_index=True)\n    logging.info(\"%d tracks analyzed\" % len(results))\n\n    # Print results\n    print_results(results)\n\n    # Save all results\n    if save:\n        logging.info(\"Writing results in %s\" % out_file)\n        results.to_csv(out_file)\n\n    return results", "response": "This function processes the algorithms in the dataset root folder and returns the results as a pandas DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_config_string(config_string, issue_warnings=True):\n    config_dict = {}\n    my_splitter = shlex.shlex(config_string, posix=True)\n    my_splitter.whitespace = ','\n    my_splitter.whitespace_split = True\n    for kv_pair in my_splitter:\n        kv_pair = kv_pair.strip()\n        if not kv_pair:\n            continue\n        kv_tuple = kv_pair.split('=', 1)\n        if len(kv_tuple) == 1:\n            if issue_warnings:\n                MsafConfigWarning.warn(\n                    (\"Config key '%s' has no value, ignoring it\" %\n                     kv_tuple[0]), stacklevel=1)\n        else:\n            k, v = kv_tuple\n            # subsequent values for k will override earlier ones\n            config_dict[k] = v\n    return config_dict", "response": "Parses a config string into a dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfetch the overriding config value for a key.", "response": "def fetch_val_for_key(key, delete_key=False):\n    \"\"\"Return the overriding config value for a key.\n    A successful search returns a string value.\n    An unsuccessful search raises a KeyError\n    The (decreasing) priority order is:\n    - MSAF_FLAGS\n    - ~./msafrc\n    \"\"\"\n\n    # first try to find it in the FLAGS\n    try:\n        if delete_key:\n            return MSAF_FLAGS_DICT.pop(key)\n        return MSAF_FLAGS_DICT[key]\n    except KeyError:\n        pass\n\n    # next try to find it in the config file\n\n    # config file keys can be of form option, or section.option\n    key_tokens = key.rsplit('.', 1)\n    if len(key_tokens) == 2:\n        section, option = key_tokens\n    else:\n        section, option = 'global', key\n    try:\n        try:\n            return msaf_cfg.get(section, option)\n        except InterpolationError:\n            return msaf_raw_cfg.get(section, option)\n    except (NoOptionError, NoSectionError):\n        raise KeyError(key)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef AddConfigVar(name, doc, configparam, root=config):\n\n    # This method also performs some of the work of initializing ConfigParam\n    # instances\n\n    if root is config:\n        # only set the name in the first call, not the recursive ones\n        configparam.fullname = name\n    sections = name.split('.')\n    if len(sections) > 1:\n        # set up a subobject\n        if not hasattr(root, sections[0]):\n            # every internal node in the config tree is an instance of its own\n            # unique class\n            class SubObj(object):\n                _i_am_a_config_class = True\n            setattr(root.__class__, sections[0], SubObj())\n        newroot = getattr(root, sections[0])\n        if (not getattr(newroot, '_i_am_a_config_class', False) or\n                isinstance(newroot, type)):\n            raise TypeError(\n                'Internal config nodes must be config class instances',\n                newroot)\n        return AddConfigVar('.'.join(sections[1:]), doc, configparam,\n                            root=newroot)\n    else:\n        if hasattr(root, name):\n            raise AttributeError('This name is already taken',\n                                 configparam.fullname)\n        configparam.doc = doc\n        # Trigger a read of the value from config files and env vars\n        # This allow to filter wrong value from the user.\n        if not callable(configparam.default):\n            configparam.__get__(root, type(root), delete_key=True)\n        else:\n            # We do not want to evaluate now the default value\n            # when it is a callable.\n            try:\n                fetch_val_for_key(configparam.fullname)\n                # The user provided a value, filter it now.\n                configparam.__get__(root, type(root), delete_key=True)\n            except KeyError:\n                pass\n        setattr(root.__class__, sections[0], configparam)\n        _config_var_list.append(configparam)", "response": "Add a new variable to the msaf. config object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef processFlat(self):\n        # Preprocess to obtain features (array(n_frames, n_features))\n\n        F = self._preprocess()\n        F = librosa.util.normalize(F, axis=0)\n        F = librosa.feature.stack_memory(F.T).T\n\n        self.config[\"hier\"] = False\n        my_bounds, my_labels, _ = main.scluster_segment(F, self.config, self.in_bound_idxs)\n\n        # Post process estimations\n        est_idxs, est_labels = self._postprocess(my_bounds, my_labels)\n\n        assert est_idxs[0] == 0 and est_idxs[-1] == F.shape[0] - 1\n        # We're done!\n        return est_idxs, est_labels", "response": "Main process.\n        Returns\n        -------\n        est_idxs : np.array(N)\n            Estimated indeces the segment boundaries in frame indeces.\n        est_labels : np.array(N-1)\n            Estimated labels for the segments."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute all features for the given file.", "response": "def compute_all_features(file_struct, framesync):\n    \"\"\"Computes all features for the given file.\"\"\"\n    for feature_id in msaf.features_registry:\n        logging.info(\"Computing %s for file %s\" % (feature_id,\n                                                   file_struct.audio_file))\n        feats = Features.select_features(feature_id, file_struct, False, framesync)\n        feats.features"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the features for the selected dataset or file.", "response": "def process(in_path, out_file, n_jobs, framesync):\n    \"\"\"Computes the features for the selected dataset or file.\"\"\"\n    if os.path.isfile(in_path):\n        # Single file mode\n        # Get (if they exitst) or compute features\n        file_struct = msaf.io.FileStruct(in_path)\n        file_struct.features_file = out_file\n        compute_all_features(file_struct, framesync)\n    else:\n        # Collection mode\n        file_structs = msaf.io.get_dataset_files(in_path)\n\n        # Call in parallel\n        return Parallel(n_jobs=n_jobs)(delayed(compute_all_features)(\n            file_struct, framesync) for file_struct in file_structs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef features(file_struct, annot_beats=False, framesync=False):\n    '''Feature-extraction for audio segmentation\n    Arguments:\n        file_struct -- msaf.io.FileStruct\n        paths to the input files in the Segmentation dataset\n\n    Returns:\n        - X -- ndarray\n\n            beat-synchronous feature matrix:\n            MFCC (mean-aggregated)\n            Chroma (median-aggregated)\n            Latent timbre repetition\n            Latent chroma repetition\n            Time index\n            Beat index\n\n        - dur -- float\n            duration of the track in seconds\n\n    '''\n    def compress_data(X, k):\n        Xtemp = X.dot(X.T)\n        if len(Xtemp) == 0:\n            return None\n        e_vals, e_vecs = np.linalg.eig(Xtemp)\n\n        e_vals = np.maximum(0.0, np.real(e_vals))\n        e_vecs = np.real(e_vecs)\n\n        idx = np.argsort(e_vals)[::-1]\n\n        e_vals = e_vals[idx]\n        e_vecs = e_vecs[:, idx]\n\n        # Truncate to k dimensions\n        if k < len(e_vals):\n            e_vals = e_vals[:k]\n            e_vecs = e_vecs[:, :k]\n\n        # Normalize by the leading singular value of X\n        Z = np.sqrt(e_vals.max())\n\n        if Z > 0:\n            e_vecs = e_vecs / Z\n\n        return e_vecs.T.dot(X)\n\n    # Latent factor repetition features\n    def repetition(X, metric='euclidean'):\n        R = librosa.segment.recurrence_matrix(\n            X, k=2 * int(np.ceil(np.sqrt(X.shape[1]))),\n            width=REP_WIDTH, metric=metric, sym=False).astype(np.float32)\n\n        P = scipy.signal.medfilt2d(librosa.segment.recurrence_to_lag(R),\n                                   [1, REP_FILTER])\n\n        # Discard empty rows.\n        # This should give an equivalent SVD, but resolves some numerical\n        # instabilities.\n        P = P[P.any(axis=1)]\n\n        return compress_data(P, N_REP)\n\n    #########\n    # '\\tloading annotations and features of ', audio_path\n    pcp_obj = Features.select_features(\"pcp\", file_struct, annot_beats,\n                                       framesync)\n    mfcc_obj = Features.select_features(\"mfcc\", file_struct, annot_beats,\n                                        framesync)\n    chroma = pcp_obj.features\n    mfcc = mfcc_obj.features\n    beats = pcp_obj.frame_times\n    dur = pcp_obj.dur\n\n    # Sampling Rate\n    sr = msaf.config.sample_rate\n\n    ##########\n    # print '\\treading beats'\n    B = beats[:chroma.shape[0]]\n    # beat_frames = librosa.time_to_frames(B, sr=sr,\n                                         #hop_length=msaf.config.hop_size)\n    #print beat_frames, len(beat_frames), uidx\n\n    #########\n    M = mfcc.T\n    #plt.imshow(M, interpolation=\"nearest\", aspect=\"auto\"); plt.show()\n\n    #########\n    # Get the beat-sync chroma\n    C = chroma.T\n    C += C.min() + 0.1\n    C = C / C.max(axis=0)\n    C = 80 * np.log10(C)  # Normalize from -80 to 0\n    #plt.imshow(C, interpolation=\"nearest\", aspect=\"auto\"); plt.show()\n\n    # Time-stamp features\n    N = np.arange(float(chroma.shape[0]))\n\n    #########\n    #print '\\tgenerating structure features'\n\n    # TODO:  This might fail if audio file (or number of beats) is too small\n    R_timbre = repetition(librosa.feature.stack_memory(M))\n    R_chroma = repetition(librosa.feature.stack_memory(C))\n    if R_timbre is None or R_chroma is None:\n        return None, dur\n\n    R_timbre += R_timbre.min()\n    R_timbre /= R_timbre.max()\n    R_chroma += R_chroma.min()\n    R_chroma /= R_chroma.max()\n    #plt.imshow(R_chroma, interpolation=\"nearest\", aspect=\"auto\"); plt.show()\n\n    # Stack it all up\n    #print M.shape, C.shape, R_timbre.shape, R_chroma.shape, len(B), len(N)\n    X = np.vstack([M, C, R_timbre, R_chroma, B, B / dur, N,\n                   N / float(chroma.shape[0])])\n\n    #plt.imshow(X, interpolation=\"nearest\", aspect=\"auto\"); plt.show()\n\n    return X, dur", "response": "Feature - extraction for audio segmentation\n           "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef gaussian_cost(X):\n    '''Return the average log-likelihood of data under a standard normal\n    '''\n\n    d, n = X.shape\n\n    if n < 2:\n        return 0\n\n    sigma = np.var(X, axis=1, ddof=1)\n\n    cost = -0.5 * d * n * np.log(2. * np.pi) - 0.5 * (n - 1.) * np.sum(sigma)\n    return cost", "response": "Return the average log - likelihood of data under a standard normal\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlogs - normalizes features such that each vector is between min_db to 0.", "response": "def lognormalize(F, floor=0.1, min_db=-80):\n    \"\"\"Log-normalizes features such that each vector is between min_db to 0.\"\"\"\n    assert min_db < 0\n    F = min_max_normalize(F, floor=floor)\n    F = np.abs(min_db) * np.log10(F)  # Normalize from min_db to 0\n    return F"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef min_max_normalize(F, floor=0.001):\n    F += -F.min() + floor\n    F = F / F.max(axis=0)\n    return F", "response": "Normalizes features such that each vector is between floor to 1."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef normalize(X, norm_type, floor=0.0, min_db=-80):\n    if isinstance(norm_type, six.string_types):\n        if norm_type == \"min_max\":\n            return min_max_normalize(X, floor=floor)\n        if norm_type == \"log\":\n            return lognormalize(X, floor=floor, min_db=min_db)\n    return librosa.util.normalize(X, norm=norm_type, axis=1)", "response": "Normalizes the given matrix of features."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_time_frames(dur, anal):\n    n_frames = get_num_frames(dur, anal)\n    return np.linspace(0, dur, num=n_frames)", "response": "Gets the time frames and puts them in a numpy array."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove_empty_segments(times, labels):\n    assert len(times) - 1 == len(labels)\n    inters = times_to_intervals(times)\n    new_inters = []\n    new_labels = []\n    for inter, label in zip(inters, labels):\n        if inter[0] < inter[1]:\n            new_inters.append(inter)\n            new_labels.append(label)\n    return intervals_to_times(np.asarray(new_inters)), new_labels", "response": "Removes empty segments if needed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsynchronizing the labels from the new_bound_idxs to the old_labels.", "response": "def synchronize_labels(new_bound_idxs, old_bound_idxs, old_labels, N):\n    \"\"\"Synchronizes the labels from the old_bound_idxs to the new_bound_idxs.\n\n    Parameters\n    ----------\n    new_bound_idxs: np.array\n        New indeces to synchronize with.\n    old_bound_idxs: np.array\n        Old indeces, same shape as labels + 1.\n    old_labels: np.array\n        Labels associated to the old_bound_idxs.\n    N: int\n        Total number of frames.\n\n    Returns\n    -------\n    new_labels: np.array\n        New labels, synchronized to the new boundary indeces.\n    \"\"\"\n    assert len(old_bound_idxs) - 1 == len(old_labels)\n\n    # Construct unfolded labels array\n    unfold_labels = np.zeros(N)\n    for i, (bound_idx, label) in enumerate(\n            zip(old_bound_idxs[:-1], old_labels)):\n        unfold_labels[bound_idx:old_bound_idxs[i + 1]] = label\n\n    # Constuct new labels\n    new_labels = np.zeros(len(new_bound_idxs) - 1)\n    for i, bound_idx in enumerate(new_bound_idxs[:-1]):\n        new_labels[i] = np.median(\n            unfold_labels[bound_idx:new_bound_idxs[i + 1]])\n\n    return new_labels"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprocesses a level of segmentation and converts it into times.", "response": "def process_segmentation_level(est_idxs, est_labels, N, frame_times, dur):\n    \"\"\"Processes a level of segmentation, and converts it into times.\n\n    Parameters\n    ----------\n    est_idxs: np.array\n        Estimated boundaries in frame indeces.\n    est_labels: np.array\n        Estimated labels.\n    N: int\n        Number of frames in the whole track.\n    frame_times: np.array\n        Time stamp for each frame.\n    dur: float\n        Duration of the audio track.\n\n    Returns\n    -------\n    est_times: np.array\n        Estimated segment boundaries in seconds.\n    est_labels: np.array\n        Estimated labels for each segment.\n    \"\"\"\n    assert est_idxs[0] == 0 and est_idxs[-1] == N - 1\n    assert len(est_idxs) - 1 == len(est_labels)\n\n    # Add silences, if needed\n    est_times = np.concatenate(([0], frame_times[est_idxs], [dur]))\n    silence_label = np.max(est_labels) + 1\n    est_labels = np.concatenate(([silence_label], est_labels, [silence_label]))\n\n    # Remove empty segments if needed\n    est_times, est_labels = remove_empty_segments(est_times, est_labels)\n\n    # Make sure that the first and last times are 0 and duration, respectively\n    assert np.allclose([est_times[0]], [0]) and \\\n        np.allclose([est_times[-1]], [dur])\n\n    return est_times, est_labels"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\naligns the end of the hierarchies such that they end at the same exact value and they have the same duration within a certain threshold.", "response": "def align_end_hierarchies(hier1, hier2, thres=0.5):\n    \"\"\"Align the end of the hierarchies such that they end at the same exact\n    second as long they have the same duration within a certain threshold.\n\n    Parameters\n    ----------\n    hier1: list\n        List containing hierarchical segment boundaries.\n    hier2: list\n        List containing hierarchical segment boundaries.\n    thres: float > 0\n        Threshold to decide whether two values are the same.\n    \"\"\"\n    # Make sure we have correctly formatted hierarchies\n    dur_h1 = hier1[0][-1]\n    for hier in hier1:\n        assert hier[-1] == dur_h1, \"hier1 is not correctly \" \\\n            \"formatted {} {}\".format(hier[-1], dur_h1)\n    dur_h2 = hier2[0][-1]\n    for hier in hier2:\n        assert hier[-1] == dur_h2, \"hier2 is not correctly formatted\"\n\n    # If durations are different, do nothing\n    if abs(dur_h1 - dur_h2) > thres:\n        return\n\n    # Align h1 with h2\n    for hier in hier1:\n        hier[-1] = dur_h2"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _distance(self, idx):\n\n        if scipy.sparse.issparse(self.data):\n            step = self.data.shape[1]\n        else:\n            step = 50000\n\n        d = np.zeros((self.data.shape[1]))\n        if idx == -1:\n            # set vec to origin if idx=-1\n            vec = np.zeros((self.data.shape[0], 1))\n            if scipy.sparse.issparse(self.data):\n                vec = scipy.sparse.csc_matrix(vec)\n        else:\n            vec = self.data[:, idx:idx+1]\n\n        self._logger.info('compute distance to node ' + str(idx))\n\n        # slice data into smaller chunks\n        for idx_start in range(0, self.data.shape[1], step):\n            if idx_start + step > self.data.shape[1]:\n                idx_end = self.data.shape[1]\n            else:\n                idx_end = idx_start + step\n\n            d[idx_start:idx_end] = self._distfunc(\n                self.data[:,idx_start:idx_end], vec)\n            self._logger.info('completed:' +\n                str(idx_end/(self.data.shape[1]/100.0)) + \"%\")\n        return d", "response": "compute distances of a specific data point to all other samples"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef estimate_K_xmeans(self, th=0.2, maxK = 10):\n\n        # Run initial K-means\n        means, labels = self.run_kmeans(self.X, self.init_K)\n\n        # Run X-means algorithm\n        stop = False\n        curr_K = self.init_K\n        while not stop:\n            stop = True\n            final_means = []\n            for k in range(curr_K):\n                # Find the data that corresponds to the k-th cluster\n                D = self.get_clustered_data(self.X, labels, k)\n                if len(D) == 0 or D.shape[0] == 1:\n                    continue\n\n                # Whiten and find whitened mean\n                stdD = np.std(D, axis=0)\n                #D = vq.whiten(D)\n                D /= float(stdD)  # Same as line above\n                mean = D.mean(axis=0)\n\n                # Cluster this subspace by half (K=2)\n                half_means, half_labels = self.run_kmeans(D, K=2)\n\n                # Compute BICs\n                bic1 = self.compute_bic(D, [mean], K=1,\n                                        labels=np.zeros(D.shape[0]),\n                                        R=D.shape[0])\n                bic2 = self.compute_bic(D, half_means, K=2,\n                                        labels=half_labels, R=D.shape[0])\n\n                # Split or not\n                max_bic = np.max([np.abs(bic1), np.abs(bic2)])\n                norm_bic1 = bic1 / float(max_bic)\n                norm_bic2 = bic2 / float(max_bic)\n                diff_bic = np.abs(norm_bic1 - norm_bic2)\n\n                # Split!\n                #print \"diff_bic\", diff_bic\n                if diff_bic > th:\n                    final_means.append(half_means[0] * stdD)\n                    final_means.append(half_means[1] * stdD)\n                    curr_K += 1\n                    stop = False\n                # Don't split\n                else:\n                    final_means.append(mean * stdD)\n\n            final_means = np.asarray(final_means)\n\n            #print \"Estimated K: \", curr_K\n            if self.plot:\n                plt.scatter(self.X[:, 0], self.X[:, 1])\n                plt.scatter(final_means[:, 0], final_means[:, 1], color=\"y\")\n                plt.show()\n\n            if curr_K >= maxK or self.X.shape[-1] != final_means.shape[-1]:\n                stop = True\n            else:\n                labels, dist = vq.vq(self.X, final_means)\n\n        return curr_K", "response": "Estimates K running X - means algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nestimating the K using K - means and BIC by sweeping various K and choosing the optimal BIC.", "response": "def estimate_K_knee(self, th=.015, maxK=12):\n        \"\"\"Estimates the K using K-means and BIC, by sweeping various K and\n            choosing the optimal BIC.\"\"\"\n        # Sweep K-means\n        if self.X.shape[0] < maxK:\n            maxK = self.X.shape[0]\n        if maxK < 2:\n            maxK = 2\n        K = np.arange(1, maxK)\n        bics = []\n        for k in K:\n            means, labels = self.run_kmeans(self.X, k)\n            bic = self.compute_bic(self.X, means, labels, K=k,\n                                   R=self.X.shape[0])\n            bics.append(bic)\n        diff_bics = np.diff(bics)\n        finalK = K[-1]\n\n        if len(bics) == 1:\n            finalK = 2\n        else:\n            # Normalize\n            bics = np.asarray(bics)\n            bics -= bics.min()\n            #bics /= bics.max()\n            diff_bics -= diff_bics.min()\n            #diff_bics /= diff_bics.max()\n\n            #print bics, diff_bics\n\n            # Find optimum K\n            for i in range(len(K[:-1])):\n                #if bics[i] > diff_bics[i]:\n                if diff_bics[i] < th and K[i] != 1:\n                    finalK = K[i]\n                    break\n\n        #print \"Estimated K: \", finalK\n        if self.plot:\n            plt.subplot(2, 1, 1)\n            plt.plot(K, bics, label=\"BIC\")\n            plt.plot(K[:-1], diff_bics, label=\"BIC diff\")\n            plt.legend(loc=2)\n            plt.subplot(2, 1, 2)\n            plt.scatter(self.X[:, 0], self.X[:, 1])\n            plt.show()\n\n        return finalK"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the data with a specific label_index using the previously splitted labels.", "response": "def get_clustered_data(self, X, labels, label_index):\n        \"\"\"Returns the data with a specific label_index, using the previously\n         learned labels.\"\"\"\n        D = X[np.argwhere(labels == label_index)]\n        return D.reshape((D.shape[0], D.shape[-1]))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns k - means and returns the labels assigned to the data.", "response": "def run_kmeans(self, X, K):\n        \"\"\"Runs k-means and returns the labels assigned to the data.\"\"\"\n        wX = vq.whiten(X)\n        means, dist = vq.kmeans(wX, K, iter=100)\n        labels, dist = vq.vq(wX, means)\n        return means, labels"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compute_bic(self, D, means, labels, K, R):\n        D = vq.whiten(D)\n        Rn = D.shape[0]\n        M = D.shape[1]\n\n        if R == K:\n            return 1\n\n        # Maximum likelihood estimate (MLE)\n        mle_var = 0\n        for k in range(len(means)):\n            X = D[np.argwhere(labels == k)]\n            X = X.reshape((X.shape[0], X.shape[-1]))\n            for x in X:\n                mle_var += distance.euclidean(x, means[k])\n                #print x, means[k], mle_var\n        mle_var /= float(R - K)\n\n        # Log-likelihood of the data\n        l_D = - Rn/2. * np.log(2*np.pi) - (Rn * M)/2. * np.log(mle_var) - \\\n            (Rn - K) / 2. + Rn * np.log(Rn) - Rn * np.log(R)\n\n        # Params of BIC\n        p = (K-1) + M * K + mle_var\n\n        #print \"BIC:\", l_D, p, R, K\n\n        # Return the bic\n        return l_D - p / 2. * np.log(R)", "response": "Computes the Bayesian Information Criterion."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate N * K 2D data points with K means and N data points for each mean and N data points for each mean.", "response": "def generate_2d_data(self, N=100, K=5):\n        \"\"\"Generates N*K 2D data points with K means and N data points\n            for each mean.\"\"\"\n        # Seed the random\n        np.random.seed(seed=int(time.time()))\n\n        # Amount of spread of the centroids\n        spread = 30\n\n        # Generate random data\n        X = np.empty((0, 2))\n        for i in range(K):\n            mean = np.array([np.random.random()*spread,\n                             np.random.random()*spread])\n            x = np.random.normal(0.0, scale=1.0, size=(N, 2)) + mean\n            X = np.append(X, x, axis=0)\n\n        return X"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef factorize(self):\n        # compute new coefficients for reconstructing data points\n        self.update_w()\n\n        # for CHNMF it is sometimes useful to only compute\n        # the basis vectors\n        if self._compute_h:\n            self.update_h()\n\n        self.W = self.mdl.W\n        self.H = self.mdl.H\n\n        self.ferr = np.zeros(1)\n        self.ferr[0] = self.mdl.frobenius_norm()\n        self._print_cur_status(' Fro:' + str(self.ferr[0]))", "response": "Do factorization s. t. data = dot product of data beta and H"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef resample_mx(X, incolpos, outcolpos):\n    noutcols = len(outcolpos)\n    Y = np.zeros((X.shape[0], noutcols))\n    # assign 'end times' to final columns\n    if outcolpos.max() > incolpos.max():\n        incolpos = np.concatenate([incolpos,[outcolpos.max()]])\n        X = np.concatenate([X, X[:,-1].reshape(X.shape[0],1)], axis=1)\n    outcolpos = np.concatenate([outcolpos, [outcolpos[-1]]])\n    # durations (default weights) of input columns)\n    incoldurs = np.concatenate([np.diff(incolpos), [1]])\n\n    for c in range(noutcols):\n        firstincol = np.where(incolpos <= outcolpos[c])[0][-1]\n        firstincolnext = np.where(incolpos < outcolpos[c+1])[0][-1]\n        lastincol = max(firstincol,firstincolnext)\n        # default weights\n        wts = copy.deepcopy(incoldurs[firstincol:lastincol+1])\n        # now fix up by partial overlap at ends\n        if len(wts) > 1:\n            wts[0] = wts[0] - (outcolpos[c] - incolpos[firstincol])\n            wts[-1] = wts[-1] - (incolpos[lastincol+1] - outcolpos[c+1])\n        wts = wts * 1. / float(sum(wts))\n        Y[:,c] = np.dot(X[:,firstincol:lastincol+1], wts)\n    # done\n    return Y", "response": "Resample X to the given time boundaries incolpos and outcolpos."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef magnitude(X):\n    r = np.real(X)\n    i = np.imag(X)\n    return np.sqrt(r * r + i * i);", "response": "Magnitude of a complex matrix."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef json_to_bounds(segments_json):\n    f = open(segments_json)\n    segments = json.load(f)[\"segments\"]\n    bounds = []\n    for segment in segments:\n        bounds.append(segment[\"start\"])\n    bounds.append(bounds[-1] + segments[-1][\"duration\"]) # Add last boundary\n    f.close()\n    return np.asarray(bounds)", "response": "Extracts the boundaries from a json file and puts them into\n        an np array."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextracting the boundaries from a json file and puts them into an np array.", "response": "def json_bounds_to_bounds(bounds_json):\n    \"\"\"Extracts the boundaries from a bounds json file and puts them into\n        an np array.\"\"\"\n    f = open(bounds_json)\n    segments = json.load(f)[\"bounds\"]\n    bounds = []\n    for segment in segments:\n        bounds.append(segment[\"start\"])\n    f.close()\n    return np.asarray(bounds)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef json_to_labels(segments_json):\n    f = open(segments_json)\n    segments = json.load(f)[\"segments\"]\n    labels = []\n    str_labels = []\n    for segment in segments:\n        if not segment[\"label\"] in str_labels:\n            str_labels.append(segment[\"label\"])\n            labels.append(len(str_labels)-1)\n        else:\n            label_idx = np.where(np.asarray(str_labels) == segment[\"label\"])[0][0]\n            labels.append(label_idx)\n    f.close()\n    return np.asarray(labels)", "response": "Extracts the labels from a json file and puts them into a np array."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nextracts the beats from the beats_json_file and puts them into an np array.", "response": "def json_to_beats(beats_json_file):\n    \"\"\"Extracts the beats from the beats_json_file and puts them into\n        an np array.\"\"\"\n    f = open(beats_json_file, \"r\")\n    beats_json = json.load(f)\n    beats = []\n    for beat in beats_json[\"beats\"]:\n        beats.append(beat[\"start\"])\n    f.close()\n    return np.asarray(beats)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compute_ffmc2d(X):\n    # 2d-fft\n    fft2 = scipy.fftpack.fft2(X)\n\n    # Magnitude\n    fft2m = magnitude(fft2)\n\n    # FFTshift and flatten\n    fftshift = scipy.fftpack.fftshift(fft2m).flatten()\n\n    #cmap = plt.cm.get_cmap('hot')\n    #plt.imshow(np.log1p(scipy.fftpack.fftshift(fft2m)).T, interpolation=\"nearest\",\n    #    aspect=\"auto\", cmap=cmap)\n    #plt.show()\n\n    # Take out redundant components\n    return fftshift[:fftshift.shape[0] // 2 + 1]", "response": "Computes the 2D - Fourier Magnitude Coefficients."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfactorize s. t. WH.", "response": "def factorize(self, niter=10, compute_w=True, compute_h=True,\n                  compute_err=True, show_progress=False):\n        \"\"\" Factorize s.t. WH = data\n\n            Parameters\n            ----------\n            niter : int\n                    number of iterations.\n            show_progress : bool\n                    print some extra information to stdout.\n            compute_h : bool\n                    iteratively update values for H.\n            compute_w : bool\n                    iteratively update values for W.\n            compute_err : bool\n                    compute Frobenius norm |data-WH| after each update and store\n                    it to .ferr[k].\n\n            Updated Values\n            --------------\n            .W : updated values for W.\n            .H : updated values for H.\n            .ferr : Frobenius norm |data-WH| for each iteration.\n        \"\"\"\n\n        if not hasattr(self,'W'):\n               self.init_w()\n\n        if not hasattr(self,'H'):\n                self.init_h()\n\n        def separate_positive(m):\n            return (np.abs(m) + m)/2.0\n\n        def separate_negative(m):\n            return (np.abs(m) - m)/2.0\n\n        if show_progress:\n            self._logger.setLevel(logging.INFO)\n        else:\n            self._logger.setLevel(logging.ERROR)\n\n        XtX = np.dot(self.data[:,:].T, self.data[:,:])\n        XtX_pos = separate_positive(XtX)\n        XtX_neg = separate_negative(XtX)\n\n        self.ferr = np.zeros(niter)\n        # iterate over W and H\n\n        for i in range(niter):\n            # update H\n            XtX_neg_x_W = np.dot(XtX_neg, self.G)\n            XtX_pos_x_W = np.dot(XtX_pos, self.G)\n\n            if compute_h:\n                H_x_WT = np.dot(self.H.T, self.G.T)\n                ha = XtX_pos_x_W + np.dot(H_x_WT, XtX_neg_x_W)\n                hb = XtX_neg_x_W + np.dot(H_x_WT, XtX_pos_x_W) + 10**-9\n                self.H = (self.H.T*np.sqrt(ha/hb)).T\n\n            # update W\n            if compute_w:\n                HT_x_H = np.dot(self.H, self.H.T)\n                wa = np.dot(XtX_pos, self.H.T) + np.dot(XtX_neg_x_W, HT_x_H)\n                wb = np.dot(XtX_neg, self.H.T) + np.dot(XtX_pos_x_W, HT_x_H) + 10**-9\n\n                self.G *= np.sqrt(wa/wb)\n                self.W = np.dot(self.data[:,:], self.G)\n\n            if compute_err:\n                self.ferr[i] = self.frobenius_norm()\n                self._logger.info('Iteration ' + str(i+1) + '/' + str(niter) +\n                ' FN:' + str(self.ferr[i]))\n            else:\n                self._logger.info('Iteration ' + str(i+1) + '/' + str(niter))\n\n            if i > 1 and compute_err:\n                if self.converged(i):\n                    self.ferr = self.ferr[:i]\n                    break"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cnmf(S, rank, niter=500, hull=False):\n    if hull:\n        nmf_mdl = pymf.CHNMF(S, num_bases=rank)\n    else:\n        nmf_mdl = pymf.CNMF(S, num_bases=rank)\n    nmf_mdl.factorize(niter=niter)\n    F = np.asarray(nmf_mdl.W)\n    G = np.asarray(nmf_mdl.H)\n    return F, G", "response": "Convex Non - Negative Matrix Factorization."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compute_labels(X, rank, R, bound_idxs, niter=300):\n\n    try:\n        F, G = cnmf(X, rank, niter=niter, hull=False)\n    except:\n        return [1]\n\n    label_frames = filter_activation_matrix(G.T, R)\n    label_frames = np.asarray(label_frames, dtype=int)\n\n    #labels = [label_frames[0]]\n    labels = []\n    bound_inters = zip(bound_idxs[:-1], bound_idxs[1:])\n    for bound_inter in bound_inters:\n        if bound_inter[1] - bound_inter[0] <= 0:\n            labels.append(np.max(label_frames) + 1)\n        else:\n            labels.append(most_frequent(\n                label_frames[bound_inter[0]: bound_inter[1]]))\n        #print bound_inter, labels[-1]\n    #labels.append(label_frames[-1])\n\n    return labels", "response": "Computes the labels using the bounds."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfiltering the activation matrix G and returns a flattened copy.", "response": "def filter_activation_matrix(G, R):\n    \"\"\"Filters the activation matrix G, and returns a flattened copy.\"\"\"\n\n    #import pylab as plt\n    #plt.imshow(G, interpolation=\"nearest\", aspect=\"auto\")\n    #plt.show()\n\n    idx = np.argmax(G, axis=1)\n    max_idx = np.arange(G.shape[0])\n    max_idx = (max_idx, idx.flatten())\n    G[:, :] = 0\n    G[max_idx] = idx + 1\n\n    # TODO: Order matters?\n    G = np.sum(G, axis=1)\n    G = median_filter(G[:, np.newaxis], R)\n\n    return G.flatten()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_segmentation(X, rank, R, rank_labels, R_labels, niter=300,\n                     bound_idxs=None, in_labels=None):\n    \"\"\"\n    Gets the segmentation (boundaries and labels) from the factorization\n    matrices.\n\n    Parameters\n    ----------\n    X: np.array()\n        Features matrix (e.g. chromagram)\n    rank: int\n        Rank of decomposition\n    R: int\n        Size of the median filter for activation matrix\n    niter: int\n        Number of iterations for k-means\n    bound_idxs : list\n        Use previously found boundaries (None to detect them)\n    in_labels : np.array()\n        List of input labels (None to compute them)\n\n    Returns\n    -------\n    bounds_idx: np.array\n        Bound indeces found\n    labels: np.array\n        Indeces of the labels representing the similarity between segments.\n    \"\"\"\n\n    #import pylab as plt\n    #plt.imshow(X, interpolation=\"nearest\", aspect=\"auto\")\n    #plt.show()\n\n    # Find non filtered boundaries\n    compute_bounds = True if bound_idxs is None else False\n    while True:\n        if bound_idxs is None:\n            try:\n                F, G = cnmf(X, rank, niter=niter, hull=False)\n            except:\n                return np.empty(0), [1]\n\n            # Filter G\n            G = filter_activation_matrix(G.T, R)\n            if bound_idxs is None:\n                bound_idxs = np.where(np.diff(G) != 0)[0] + 1\n\n        # Increase rank if we found too few boundaries\n        if compute_bounds and len(np.unique(bound_idxs)) <= 2:\n            rank += 1\n            bound_idxs = None\n        else:\n            break\n\n    # Add first and last boundary\n    bound_idxs = np.concatenate(([0], bound_idxs, [X.shape[1] - 1]))\n    bound_idxs = np.asarray(bound_idxs, dtype=int)\n    if in_labels is None:\n        labels = compute_labels(X, rank_labels, R_labels, bound_idxs,\n                                niter=niter)\n    else:\n        labels = np.ones(len(bound_idxs) - 1)\n\n    #plt.imshow(G[:, np.newaxis], interpolation=\"nearest\", aspect=\"auto\")\n    #for b in bound_idxs:\n        #plt.axvline(b, linewidth=2.0, color=\"k\")\n    #plt.show()\n\n    return bound_idxs, labels", "response": "Returns the segmentation of a factorization matrix X with rank rank_labels R_labels and labels in in_labels."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprocess the flat data.", "response": "def processFlat(self):\n        \"\"\"Main process.\n        Returns\n        -------\n        est_idxs : np.array(N)\n            Estimated indeces for the segment boundaries in frames.\n        est_labels : np.array(N-1)\n            Estimated labels for the segments.\n        \"\"\"\n        # C-NMF params\n        niter = self.config[\"niters\"]  # Iterations for the MF and clustering\n\n        # Preprocess to obtain features, times, and input boundary indeces\n        F = self._preprocess()\n\n        # Normalize\n        F = U.normalize(F, norm_type=self.config[\"norm_feats\"])\n\n        if F.shape[0] >= self.config[\"h\"]:\n            # Median filter\n            F = median_filter(F, M=self.config[\"h\"])\n            #plt.imshow(F.T, interpolation=\"nearest\", aspect=\"auto\"); plt.show()\n\n            # Find the boundary indices and labels using matrix factorization\n            est_idxs, est_labels = get_segmentation(\n                F.T, self.config[\"rank\"], self.config[\"R\"],\n                self.config[\"rank_labels\"], self.config[\"R_labels\"],\n                niter=niter, bound_idxs=self.in_bound_idxs, in_labels=None)\n\n            # Remove empty segments if needed\n            est_idxs, est_labels = U.remove_empty_segments(est_idxs, est_labels)\n        else:\n            # The track is too short. We will only output the first and last\n            # time stamps\n            if self.in_bound_idxs is None:\n                est_idxs = np.array([0, F.shape[0] - 1])\n                est_labels = [1]\n            else:\n                est_idxs = self.in_bound_idxs\n                est_labels = [1] * (len(est_idxs) + 1)\n\n        # Make sure that the first and last boundaries are included\n        assert est_idxs[0] == 0 and est_idxs[-1] == F.shape[0] - 1\n\n        # Post process estimations\n        est_idxs, est_labels = self._postprocess(est_idxs, est_labels)\n\n        return est_idxs, est_labels"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_boundaries_module(boundaries_id):\n    if boundaries_id == \"gt\":\n        return None\n    try:\n        module = eval(algorithms.__name__ + \".\" + boundaries_id)\n    except AttributeError:\n        raise RuntimeError(\"Algorithm %s can not be found in msaf!\" %\n                           boundaries_id)\n    if not module.is_boundary_type:\n        raise RuntimeError(\"Algorithm %s can not identify boundaries!\" %\n                           boundaries_id)\n    return module", "response": "Returns the object containing the selected boundary module."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the label module given a label algorithm identificator.", "response": "def get_labels_module(labels_id):\n    \"\"\"Obtains the label module given a label algorithm identificator.\n\n    Parameters\n    ----------\n    labels_id: str\n        Label algorithm identificator (e.g., fmc2d, cnmf).\n\n    Returns\n    -------\n    module: object\n        Object containing the selected label module.\n        None for not computing the labeling part of music segmentation.\n    \"\"\"\n    if labels_id is None:\n        return None\n    try:\n        module = eval(algorithms.__name__ + \".\" + labels_id)\n    except AttributeError:\n        raise RuntimeError(\"Algorithm %s can not be found in msaf!\" %\n                           labels_id)\n    if not module.is_label_type:\n        raise RuntimeError(\"Algorithm %s can not label segments!\" %\n                           labels_id)\n    return module"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun hierarchical algorithms with the specified identifiers on the audio_file.", "response": "def run_hierarchical(audio_file, bounds_module, labels_module, frame_times,\n                     config, annotator_id=0):\n    \"\"\"Runs hierarchical algorithms with the specified identifiers on the\n    audio_file. See run_algorithm for more information.\n    \"\"\"\n    # Sanity check\n    if bounds_module is None:\n        raise NoHierBoundaryError(\"A boundary algorithm is needed when using \"\n                                  \"hierarchical segmentation.\")\n\n    # Get features to make code nicer\n    features = config[\"features\"].features\n\n    # Compute boundaries\n    S = bounds_module.Segmenter(audio_file, **config)\n    est_idxs, est_labels = S.processHierarchical()\n\n    # Compute labels if needed\n    if labels_module is not None and \\\n            bounds_module.__name__ != labels_module.__name__:\n        # Compute labels for each level in the hierarchy\n        flat_config = deepcopy(config)\n        flat_config[\"hier\"] = False\n        for i, level_idxs in enumerate(est_idxs):\n            S = labels_module.Segmenter(audio_file,\n                                        in_bound_idxs=level_idxs,\n                                        **flat_config)\n            est_labels[i] = S.processFlat()[1]\n\n    # Make sure the first and last boundaries are included for each\n    # level in the hierarchy\n    est_times = []\n    cleaned_est_labels = []\n    for level in range(len(est_idxs)):\n        est_level_times, est_level_labels = \\\n            utils.process_segmentation_level(\n                est_idxs[level], est_labels[level], features.shape[0],\n                frame_times, config[\"features\"].dur)\n        est_times.append(est_level_times)\n        cleaned_est_labels.append(est_level_labels)\n    est_labels = cleaned_est_labels\n\n    return est_times, est_labels"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns the flat algorithm with the specified identifiers on the the audio_file.", "response": "def run_flat(file_struct, bounds_module, labels_module, frame_times, config,\n             annotator_id):\n    \"\"\"Runs the flat algorithms with the specified identifiers on the\n    audio_file. See run_algorithm for more information.\n    \"\"\"\n    # Get features to make code nicer\n    features = config[\"features\"].features\n\n    # Segment using the specified boundaries and labels\n    # Case when boundaries and labels algorithms are the same\n    if bounds_module is not None and labels_module is not None and \\\n            bounds_module.__name__ == labels_module.__name__:\n        S = bounds_module.Segmenter(file_struct, **config)\n        est_idxs, est_labels = S.processFlat()\n    # Different boundary and label algorithms\n    else:\n        # Identify segment boundaries\n        if bounds_module is not None:\n            S = bounds_module.Segmenter(file_struct, in_labels=[], **config)\n            est_idxs, est_labels = S.processFlat()\n        else:\n            try:\n                # Ground-truth boundaries\n                est_times, est_labels = io.read_references(\n                    file_struct.audio_file, annotator_id=annotator_id)\n                est_idxs = io.align_times(est_times, frame_times)\n                if est_idxs[0] != 0:\n                    est_idxs = np.concatenate(([0], est_idxs))\n            except IOError:\n                logging.warning(\"No references found for file: %s\" %\n                                file_struct.audio_file)\n                return [], []\n\n        # Label segments\n        if labels_module is not None:\n            if len(est_idxs) == 2:\n                est_labels = np.array([0])\n            else:\n                S = labels_module.Segmenter(file_struct,\n                                            in_bound_idxs=est_idxs,\n                                            **config)\n                est_labels = S.processFlat()[1]\n\n    # Make sure the first and last boundaries are included\n    est_times, est_labels = utils.process_segmentation_level(\n        est_idxs, est_labels, features.shape[0], frame_times,\n        config[\"features\"].dur)\n\n    return est_times, est_labels"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run_algorithms(file_struct, boundaries_id, labels_id, config,\n                   annotator_id=0):\n    \"\"\"Runs the algorithms with the specified identifiers on the audio_file.\n\n    Parameters\n    ----------\n    file_struct: `msaf.io.FileStruct`\n        Object with the file paths.\n    boundaries_id: str\n        Identifier of the boundaries algorithm to use (\"gt\" for ground truth).\n    labels_id: str\n        Identifier of the labels algorithm to use (None for not labeling).\n    config: dict\n        Dictionary containing the custom parameters of the algorithms to use.\n    annotator_id: int\n        Annotator identificator in the ground truth.\n\n    Returns\n    -------\n    est_times: np.array or list\n        List of estimated times for the segment boundaries.\n        If `list`, it will be a list of np.arrays, sorted by segmentation\n        layer.\n    est_labels: np.array or list\n        List of all the labels associated segments.\n        If `list`, it will be a list of np.arrays, sorted by segmentation\n        layer.\n    \"\"\"\n    # Check that there are enough audio frames\n    if config[\"features\"].features.shape[0] <= msaf.config.minimum_frames:\n        logging.warning(\"Audio file too short, or too many few beats \"\n                        \"estimated. Returning empty estimations.\")\n        return np.asarray([0, config[\"features\"].dur]), \\\n            np.asarray([0], dtype=int)\n\n    # Get the corresponding modules\n    bounds_module = get_boundaries_module(boundaries_id)\n    labels_module = get_labels_module(labels_id)\n\n    # Get the correct frame times\n    frame_times = config[\"features\"].frame_times\n\n    # Segment audio based on type of segmentation\n    run_fun = run_hierarchical if config[\"hier\"] else run_flat\n    est_times, est_labels = run_fun(file_struct, bounds_module, labels_module,\n                                    frame_times, config, annotator_id)\n\n    return est_times, est_labels", "response": "Runs the algorithms with the specified identifiers on the audio_file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npreparing the parameters runs the algorithms and saves the results in the estimator file.", "response": "def process_track(file_struct, boundaries_id, labels_id, config,\n                  annotator_id=0):\n    \"\"\"Prepares the parameters, runs the algorithms, and saves results.\n\n    Parameters\n    ----------\n    file_struct: `msaf.io.FileStruct`\n        FileStruct containing the paths of the input files (audio file,\n        features file, reference file, output estimation file).\n    boundaries_id: str\n        Identifier of the boundaries algorithm to use (\"gt\" for ground truth).\n    labels_id: str\n        Identifier of the labels algorithm to use (None for not labeling).\n    config: dict\n        Dictionary containing the custom parameters of the algorithms to use.\n    annotator_id: int\n        Annotator identificator in the ground truth.\n\n    Returns\n    -------\n    est_times: np.array\n        List of estimated times for the segment boundaries.\n    est_labels: np.array\n        List of all the labels associated segments.\n    \"\"\"\n    logging.info(\"Segmenting %s\" % file_struct.audio_file)\n\n    # Get features\n    config[\"features\"] = Features.select_features(\n        config[\"feature\"], file_struct, config[\"annot_beats\"],\n        config[\"framesync\"])\n\n    # Get estimations\n    est_times, est_labels = run_algorithms(file_struct,\n                                           boundaries_id, labels_id, config,\n                                           annotator_id=annotator_id)\n\n    # Save\n    logging.info(\"Writing results in: %s\" % file_struct.est_file)\n    io.save_estimations(file_struct, est_times, est_labels,\n                        boundaries_id, labels_id, **config)\n\n    return est_times, est_labels"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate W under the convexity of the class", "response": "def update_w(self):\n        \"\"\" alternating least squares step, update W under the convexity\n        constraint \"\"\"\n        def update_single_w(i):\n            \"\"\" compute single W[:,i] \"\"\"\n            # optimize beta     using qp solver from cvxopt\n            FB = base.matrix(np.float64(np.dot(-self.data.T, W_hat[:,i])))\n            be = solvers.qp(HB, FB, INQa, INQb, EQa, EQb)\n            self.beta[i,:] = np.array(be['x']).reshape((1, self._num_samples))\n\n        # float64 required for cvxopt\n        HB = base.matrix(np.float64(np.dot(self.data[:,:].T, self.data[:,:])))\n        EQb = base.matrix(1.0, (1, 1))\n        W_hat = np.dot(self.data, pinv(self.H))\n        INQa = base.matrix(-np.eye(self._num_samples))\n        INQb = base.matrix(0.0, (self._num_samples, 1))\n        EQa = base.matrix(1.0, (1, self._num_samples))\n\n        for i in range(self._num_bases):\n            update_single_w(i)\n\n        self.W = np.dot(self.beta, self.data.T).T"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef main():\n    '''\n    Main Entry point for translator and argument parser\n    '''\n    args      = command_line()\n    translate = partial(translator, args.source, args.dest,\n                        version=' '.join([__version__, __build__]))\n\n    return source(spool(set_task(translate, translit=args.translit)), args.text)", "response": "Main entry point for translator and argument parser\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninitializing coroutine essentially priming it to the yield statement. Used as a decorator over functions that generate coroutines. .. code-block:: python # Basic coroutine producer/consumer pattern from translate import coroutine @coroutine def coroutine_foo(bar): try: while True: baz = (yield) bar.send(baz) except GeneratorExit: bar.close() :param func: Unprimed Generator :type func: Function :return: Initialized Coroutine :rtype: Function", "response": "def coroutine(func):\n    \"\"\"\n    Initializes coroutine essentially priming it to the yield statement.\n    Used as a decorator over functions that generate coroutines.\n\n    .. code-block:: python\n\n        # Basic coroutine producer/consumer pattern\n        from translate import coroutine\n\n        @coroutine\n        def coroutine_foo(bar):\n            try:\n                while True:\n                    baz = (yield)\n                    bar.send(baz)\n\n            except GeneratorExit:\n                bar.close()\n\n    :param func: Unprimed Generator\n    :type func: Function\n\n    :return: Initialized Coroutine\n    :rtype: Function\n    \"\"\"\n\n    @wraps(func)\n    def initialization(*args, **kwargs):\n\n        start = func(*args, **kwargs)\n        next(start)\n\n        return start\n\n    return initialization"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef accumulator(init, update):\n    return (\n        init + len(update)\n            if isinstance(init, int) else\n        init + update\n    )", "response": "Generic accumulator function.\n\n    .. code-block:: python\n\n        # Simplest Form\n        >>> a = 'this' + ' '\n        >>> b = 'that'\n        >>> c = functools.reduce(accumulator, a, b)\n        >>> c\n        'this that'\n\n        # The type of the initial value determines output type.\n        >>> a = 5\n        >>> b = Hello\n        >>> c = functools.reduce(accumulator, a, b)\n        >>> c\n        10\n\n    :param init:  Initial Value\n    :param update: Value to accumulate\n\n    :return: Combined Values"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting a stream of text to stdout.", "response": "def write_stream(script, output='trans'):\n    \"\"\"\n    :param script: Translated Text\n    :type script: Iterable\n\n    :param output: Output Type (either 'trans' or 'translit')\n    :type output: String\n    \"\"\"\n    first    = operator.itemgetter(0)\n    sentence, _ = script\n    printer  = partial(print, file=sys.stdout, end='')\n\n    for line in sentence:\n        if isinstance(first(line), str):\n            printer(first(line))\n        else:\n            printer(first(line).encode('UTF-8'))\n\n    printer('\\n')\n\n    return sys.stdout.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_task(translator, translit=False):\n    # Initialize Task Queue\n    task    = str()\n    queue   = list()\n\n    # Function Partial\n    output  = ('translit' if translit else 'trans')\n    stream  = partial(write_stream, output=output)\n    workers = ThreadPoolExecutor(max_workers=8)\n\n    try:\n        while True:\n\n            task = yield\n            queue.append(task)\n\n    except GeneratorExit:\n        list(map(stream, workers.map(translator, queue)))", "response": "This function sets the Task of the next available language."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading the entire sequence of words from the given iterable and spools them together for more io efficient processes.", "response": "def spool(iterable, maxlen=1250):\n    \"\"\"\n    Consumes text streams and spools them together for more io\n    efficient processes.\n\n    :param iterable: Sends text stream for further processing\n    :type iterable: Coroutine\n\n    :param maxlen: Maximum query string size\n    :type maxlen: Integer\n    \"\"\"\n    words = int()\n    text  = str()\n\n    try:\n        while True:\n\n            while words < maxlen:\n                stream = yield\n                text   = reduce(accumulator, stream, text)\n                words  = reduce(accumulator, stream, words)\n\n            iterable.send(text)\n            words = int()\n            text  = str()\n\n    except GeneratorExit:\n        iterable.send(text)\n        iterable.close()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef source(target, inputstream=sys.stdin):\n    for line in inputstream:\n\n        while len(line) > 600:\n            init, sep, line = line.partition(' ')\n            assert len(init) <= 600\n            target.send(''.join([init, sep]))\n\n        target.send(line)\n\n    inputstream.close()\n\n    return target.close()", "response": "Coroutine starting point. Produces text stream and forwards to consumers\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef push_url(interface):\n    '''\n    Decorates a function returning the url of translation API.\n    Creates and maintains HTTP connection state\n\n    Returns a dict response object from the server containing the translated\n    text and metadata of the request body\n\n    :param interface: Callable Request Interface\n    :type interface: Function\n    '''\n\n    @functools.wraps(interface)\n    def connection(*args, **kwargs):\n        \"\"\"\n        Extends and wraps a HTTP interface.\n\n        :return: Response Content\n        :rtype: Dictionary\n        \"\"\"\n        session = Session()\n        session.mount('http://',  HTTPAdapter(max_retries=2))\n        session.mount('https://', HTTPAdapter(max_retries=2))\n\n        request  = Request(**interface(*args, **kwargs))\n        prepare  = session.prepare_request(request)\n        response = session.send(prepare, verify=True)\n\n        if response.status_code != requests.codes.ok:\n            response.raise_for_status()\n\n        cleanup = re.subn(r',(?=,)', '', response.content.decode('utf-8'))[0]\n\n        return json.loads(cleanup.replace(r'\\xA0', r' ').replace('[,', '[1,'), encoding='UTF-8')\n\n    return connection", "response": "Decorator that creates a HTTP connection to the translation API and returns the url of the translation API."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntranslate a source language code into a target language code.", "response": "def translator(source, target, phrase, version='0.0 test', charset='utf-8'):\n    \"\"\"\n    Returns the url encoded string that will be pushed to the translation\n    server for parsing.\n\n    List of acceptable language codes for source and target languages\n    can be found as a JSON file in the etc directory.\n\n    Some source languages are limited in scope of the possible target languages\n    that are available.\n\n    .. code-block:: python\n\n        >>> from translate import translator\n        >>> translator('en', 'zh-TW', 'Hello World!')\n            '\u4f60\u597d\u4e16\u754c\uff01'\n\n    :param source: Language code for translation source\n    :type source: String\n\n    :param target: Language code that source will be translate into\n    :type target: String\n\n    :param phrase: Text body string that will be url encoded and translated\n    :type phrase: String\n\n    :return: Request Interface\n    :rtype: Dictionary\n    \"\"\"\n\n    url     = 'https://translate.google.com/translate_a/single'\n    agent   = 'User-Agent',   'py-translate v{}'.format(version)\n    content = 'Content-Type', 'application/json; charset={}'.format(charset)\n\n    params  = {'client': 'a', 'ie': charset, 'oe': charset,\n                   'dt': 't', 'sl':  source, 'tl':  target,  'q': phrase}\n\n    request = {'method': 'GET',\n                  'url': url,\n               'params': params,\n              'headers': dict([agent, content])}\n\n    return request"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads in the language codes and prints them out.", "response": "def translation_table(language, filepath='supported_translations.json'):\n    '''\n    Opens up file located under the etc directory containing language\n    codes and prints them out.\n\n    :param file: Path to location of json file\n    :type file: str\n\n    :return: language codes\n    :rtype: dict\n    '''\n    fullpath = abspath(join(dirname(__file__), 'etc', filepath))\n\n    if not isfile(fullpath):\n        raise IOError('File does not exist at {0}'.format(fullpath))\n\n    with open(fullpath, 'rt') as fp:\n        raw_data = json.load(fp).get(language, None)\n        assert(raw_data is not None)\n\n    return dict((code['language'], code['name']) for code in raw_data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a formatted table of language codes", "response": "def print_table(language):\n    '''\n    Generates a formatted table of language codes\n    '''\n    table = translation_table(language)\n\n    for code, name in sorted(table.items(), key=operator.itemgetter(0)):\n        print(u'{language:<8} {name:\\u3000<20}'.format(\n            name=name, language=code\n        ))\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_nodes(network, rm_nodes):\n    rm_nodes = set(rm_nodes)\n    ndf = network.nodes_df\n    edf = network.edges_df\n\n    nodes_to_keep = ~ndf.index.isin(rm_nodes)\n    edges_to_keep = ~(edf['from'].isin(rm_nodes) | edf['to'].isin(rm_nodes))\n\n    return ndf.loc[nodes_to_keep], edf.loc[edges_to_keep]", "response": "Create DataFrames of nodes and edges that do not include specified nodes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef network_to_pandas_hdf5(network, filename, rm_nodes=None):\n    if rm_nodes is not None:\n        nodes, edges = remove_nodes(network, rm_nodes)\n    else:\n        nodes, edges = network.nodes_df, network.edges_df\n\n    with pd.HDFStore(filename, mode='w') as store:\n        store['nodes'] = nodes\n        store['edges'] = edges\n\n        store['two_way'] = pd.Series([network._twoway])\n        store['impedance_names'] = pd.Series(network.impedance_names)", "response": "Save a Network to a Pandas HDFStore."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding a Network from data in a Pandas HDF5 file.", "response": "def network_from_pandas_hdf5(cls, filename):\n    \"\"\"\n    Build a Network from data in a Pandas HDFStore.\n\n    Parameters\n    ----------\n    cls : class\n        Class to instantiate, usually pandana.Network.\n    filename : str\n\n    Returns\n    -------\n    network : pandana.Network\n\n    \"\"\"\n    with pd.HDFStore(filename) as store:\n        nodes = store['nodes']\n        edges = store['edges']\n        two_way = store['two_way'][0]\n        imp_names = store['impedance_names'].tolist()\n\n    return cls(\n        nodes['x'], nodes['y'], edges['from'], edges['to'], edges[imp_names],\n        twoway=two_way)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the bounding box for the nodes in this network", "response": "def bbox(self):\n        \"\"\"\n        The bounding box for nodes in this network [xmin, ymin, xmax, ymax]\n        \"\"\"\n        return [self.nodes_df.x.min(), self.nodes_df.y.min(),\n                self.nodes_df.x.max(), self.nodes_df.y.max()]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncharacterizing urban space with a variable that is related to nodes in the network. Parameters ---------- node_ids : Pandas Series, int A series of node_ids which are usually computed using get_node_ids on this object. variable : Pandas Series, numeric, optional A series which represents some variable defined in urban space. It could be the location of buildings, or the income of all households - just about anything can be aggregated using the network queries provided here and this provides the api to set the variable at its disaggregate locations. Note that node_id and variable should have the same index (although the index is not actually used). If variable is not set, then it is assumed that the variable is all \"ones\" at the location specified by node_ids. This could be, for instance, the location of all coffee shops which don't really have a variable to aggregate. The variable is connected to the closest node in the Pandana network which assumes no impedance between the location of the variable and the location of the closest network node. name : string, optional Name the variable. This is optional in the sense that if you don't specify it, the default name will be used. Since the same default name is used by aggregate on this object, you can alternate between characterize and aggregate calls without setting names. Returns ------- Nothing", "response": "def set(self, node_ids, variable=None, name=\"tmp\"):\n        \"\"\"\n        Characterize urban space with a variable that is related to nodes in\n        the network.\n\n        Parameters\n        ----------\n        node_ids : Pandas Series, int\n            A series of node_ids which are usually computed using\n            get_node_ids on this object.\n        variable : Pandas Series, numeric, optional\n            A series which represents some variable defined in urban space.\n            It could be the location of buildings, or the income of all\n            households - just about anything can be aggregated using the\n            network queries provided here and this provides the api to set\n            the variable at its disaggregate locations.  Note that node_id\n            and variable should have the same index (although the index is\n            not actually used).  If variable is not set, then it is assumed\n            that the variable is all \"ones\" at the location specified by\n            node_ids.  This could be, for instance, the location of all\n            coffee shops which don't really have a variable to aggregate. The\n            variable is connected to the closest node in the Pandana network\n            which assumes no impedance between the location of the variable\n            and the location of the closest network node.\n        name : string, optional\n            Name the variable.  This is optional in the sense that if you don't\n            specify it, the default name will be used.  Since the same\n            default name is used by aggregate on this object, you can\n            alternate between characterize and aggregate calls without\n            setting names.\n\n        Returns\n        -------\n        Nothing\n        \"\"\"\n\n        if variable is None:\n            variable = pd.Series(np.ones(len(node_ids)), index=node_ids.index)\n\n        df = pd.DataFrame({name: variable,\n                           \"node_idx\": self._node_indexes(node_ids)})\n\n        length = len(df)\n        df = df.dropna(how=\"any\")\n        newl = len(df)\n        if length-newl > 0:\n            print(\n                \"Removed %d rows because they contain missing values\" %\n                (length-newl))\n\n        self.variable_names.add(name)\n\n        self.net.initialize_access_var(name.encode('utf-8'),\n                                       df.node_idx.values.astype('int'),\n                                       df[name].values.astype('double'))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef aggregate(self, distance, type=\"sum\", decay=\"linear\", imp_name=None,\n                  name=\"tmp\"):\n        \"\"\"\n        Aggregate information for every source node in the network - this is\n        really the main purpose of this library.  This allows you to touch\n        the data specified by calling set and perform some aggregation on it\n        within the specified distance.  For instance, summing the population\n        within 1000 meters.\n\n        Parameters\n        ----------\n        distance : float\n            The maximum distance to aggregate data within. 'distance' can\n            represent any impedance unit that you have set as your edge\n            weight. This will usually be a distance unit in meters however\n            if you have customized the impedance this could be in other\n            units such as utility or time etc.\n        type : string\n            The type of aggregation, can be one of \"ave\", \"sum\", \"std\",\n            \"count\", and now \"min\", \"25pct\", \"median\", \"75pct\", and \"max\" will\n            compute the associated quantiles.  (Quantiles are computed by\n            sorting so might be slower than the others.)\n        decay : string\n            The type of decay to apply, which makes things that are further\n            away count less in the aggregation - must be one of \"linear\",\n            \"exponential\" or \"flat\" (which means no decay).  Linear is the\n            fastest computation to perform.  When performing an \"ave\",\n            the decay is typically \"flat\"\n        imp_name : string, optional\n            The impedance name to use for the aggregation on this network.\n            Must be one of the impedance names passed in the constructor of\n            this object.  If not specified, there must be only one impedance\n            passed in the constructor, which will be used.\n        name : string, optional\n            The variable to aggregate.  This variable will have been created\n            and named by a call to set.  If not specified, the default\n            variable name will be used so that the most recent call to set\n            without giving a name will be the variable used.\n\n        Returns\n        -------\n        agg : Pandas Series\n            Returns a Pandas Series for every origin node in the network,\n            with the index which is the same as the node_ids passed to the\n            init method and the values are the aggregations for each source\n            node in the network.\n        \"\"\"\n\n        imp_num = self._imp_name_to_num(imp_name)\n        type = type.lower()\n        if type == \"ave\":\n            type = \"mean\"  # changed generic ave to mean\n\n        assert name in self.variable_names, \"A variable with that name \" \\\n                                            \"has not yet been initialized\"\n\n        res = self.net.get_all_aggregate_accessibility_variables(distance,\n                                                                 name.encode('utf-8'),\n                                                                 type.encode('utf-8'),\n                                                                 decay.encode('utf-8'),\n                                                                 imp_num)\n\n        return pd.Series(res, index=self.node_ids)", "response": "Aggregate the data for every source node in the network within the specified distance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nassigning node_ids to data specified by x_col and y_col Parameters ---------- x_col : Pandas series (float) A Pandas Series where values specify the x (e.g. longitude) location of dataset. y_col : Pandas series (float) A Pandas Series where values specify the y (e.g. latitude) location of dataset. x_col and y_col should use the same index. mapping_distance : float, optional The maximum distance that will be considered a match between the x, y data and the nearest node in the network. This will usually be a distance unit in meters however if you have customized the impedance this could be in other units such as utility or time etc. If not specified, every x, y coordinate will be mapped to the nearest node. Returns ------- node_ids : Pandas series (int) Returns a Pandas Series of node_ids for each x, y in the input data. The index is the same as the indexes of the x, y input data, and the values are the mapped node_ids. If mapping distance is not passed and if there are no nans in the x, y data, this will be the same length as the x, y data. If the mapping is imperfect, this function returns all the input x, y's that were successfully mapped to node_ids.", "response": "def get_node_ids(self, x_col, y_col, mapping_distance=None):\n        \"\"\"\n        Assign node_ids to data specified by x_col and y_col\n\n        Parameters\n        ----------\n        x_col : Pandas series (float)\n            A Pandas Series where values specify the x (e.g. longitude)\n            location of dataset.\n        y_col : Pandas series (float)\n            A Pandas Series where values specify the y (e.g. latitude)\n            location of dataset.  x_col and y_col should use the same index.\n        mapping_distance : float, optional\n            The maximum distance that will be considered a match between the\n            x, y data and the nearest node in the network.  This will usually\n            be a distance unit in meters however if you have customized the\n            impedance this could be in other units such as utility or time\n            etc. If not specified, every x, y coordinate will be mapped to\n            the nearest node.\n\n        Returns\n        -------\n        node_ids : Pandas series (int)\n            Returns a Pandas Series of node_ids for each x, y in the\n            input data. The index is the same as the indexes of the\n            x, y input data, and the values are the mapped node_ids.\n            If mapping distance is not passed and if there are no nans in the\n            x, y data, this will be the same length as the x, y data.\n            If the mapping is imperfect, this function returns all the\n            input x, y's that were successfully mapped to node_ids.\n        \"\"\"\n        xys = pd.DataFrame({'x': x_col, 'y': y_col})\n\n        distances, indexes = self.kdtree.query(xys.as_matrix())\n        indexes = np.transpose(indexes)[0]\n        distances = np.transpose(distances)[0]\n\n        node_ids = self.nodes_df.iloc[indexes].index\n\n        df = pd.DataFrame({\"node_id\": node_ids, \"distance\": distances},\n                          index=xys.index)\n\n        if mapping_distance is not None:\n            df = df[df.distance <= mapping_distance]\n\n        return df.node_id"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plot(\n            self, data, bbox=None, plot_type='scatter',\n            fig_kwargs=None, bmap_kwargs=None, plot_kwargs=None,\n            cbar_kwargs=None):\n        \"\"\"\n        Plot an array of data on a map using matplotlib and Basemap,\n        automatically matching the data to the Pandana network node positions.\n\n        Keyword arguments are passed to the plotting routine.\n\n        Parameters\n        ----------\n        data : pandas.Series\n            Numeric data with the same length and index as the nodes\n            in the network.\n        bbox : tuple, optional\n            (lat_min, lng_min, lat_max, lng_max)\n        plot_type : {'hexbin', 'scatter'}, optional\n        fig_kwargs : dict, optional\n            Keyword arguments that will be passed to\n            matplotlib.pyplot.subplots. Use this to specify things like\n            figure size or background color.\n        bmap_kwargs : dict, optional\n            Keyword arguments that will be passed to the Basemap constructor.\n            This can be used to specify a projection or coastline resolution.\n        plot_kwargs : dict, optional\n            Keyword arguments that will be passed to the matplotlib plotting\n            command used. Use this to control plot styles and color maps used.\n        cbar_kwargs : dict, optional\n            Keyword arguments passed to the Basemap.colorbar method.\n            Use this to control color bar location and label.\n\n        Returns\n        -------\n        bmap : Basemap\n        fig : matplotlib.Figure\n        ax : matplotlib.Axes\n\n        \"\"\"\n        from mpl_toolkits.basemap import Basemap\n\n        fig_kwargs = fig_kwargs or {}\n        bmap_kwargs = bmap_kwargs or {}\n        plot_kwargs = plot_kwargs or {}\n        cbar_kwargs = cbar_kwargs or {}\n\n        if not bbox:\n            bbox = (\n                self.nodes_df.y.min(),\n                self.nodes_df.x.min(),\n                self.nodes_df.y.max(),\n                self.nodes_df.x.max())\n\n        fig, ax = plt.subplots(**fig_kwargs)\n\n        bmap = Basemap(\n            bbox[1], bbox[0], bbox[3], bbox[2], ax=ax, **bmap_kwargs)\n        bmap.drawcoastlines()\n        bmap.drawmapboundary()\n\n        x, y = bmap(self.nodes_df.x.values, self.nodes_df.y.values)\n\n        if plot_type == 'scatter':\n            plot = bmap.scatter(\n                x, y, c=data.values, **plot_kwargs)\n        elif plot_type == 'hexbin':\n            plot = bmap.hexbin(\n                x, y, C=data.values, **plot_kwargs)\n\n        bmap.colorbar(plot, **cbar_kwargs)\n\n        return bmap, fig, ax", "response": "Plots an array of data on a map using matplotlib and Basemap."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_pois(self, category, maxdist, maxitems, x_col, y_col):\n        if category not in self.poi_category_names:\n            self.poi_category_names.append(category)\n\n        self.max_pois = maxitems\n\n        node_ids = self.get_node_ids(x_col, y_col)\n\n        self.poi_category_indexes[category] = node_ids.index\n\n        node_idx = self._node_indexes(node_ids)\n\n        self.net.initialize_category(maxdist, maxitems, category.encode('utf-8'), node_idx.values)", "response": "This method sets the location of all the pois of this category."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef nearest_pois(self, distance, category, num_pois=1, max_distance=None,\n                     imp_name=None, include_poi_ids=False):\n        \"\"\"\n        Find the distance to the nearest pois from each source node.  The\n        bigger values in this case mean less accessibility.\n\n        Parameters\n        ----------\n        distance : float\n            The maximum distance to look for pois. This will usually be a\n            distance unit in meters however if you have customized the\n            impedance this could be in other units such as utility or time\n            etc.\n        category : string\n            The name of the category of poi to look for\n        num_pois : int\n            The number of pois to look for, this also sets the number of\n            columns in the DataFrame that gets returned\n        max_distance : float, optional\n            The value to set the distance to if there is NO poi within the\n            specified distance - if not specified, gets set to distance. This\n            will usually be a distance unit in meters however if you have\n            customized the impedance this could be in other units such as\n            utility or time etc.\n        imp_name : string, optional\n            The impedance name to use for the aggregation on this network.\n            Must be one of the impedance names passed in the constructor of\n            this object.  If not specified, there must be only one impedance\n            passed in the constructor, which will be used.\n        include_poi_ids : bool, optional\n            If this flag is set to true, the call will add columns to the\n            return DataFrame - instead of just returning the distance for\n            the nth POI, it will also return the id of that POI.  The names\n            of the columns with the poi ids will be poi1, poi2, etc - it\n            will take roughly twice as long to include these ids as to not\n            include them\n\n        Returns\n        -------\n        d : Pandas DataFrame\n            Like aggregate, this series has an index of all the node ids for\n            the network.  Unlike aggregate, this method returns a dataframe\n            with the number of columns equal to the distances to the Nth\n            closest poi.  For instance, if you ask for the 10 closest poi to\n            each node, column d[1] wil be the distance to the 1st closest poi\n            of that category while column d[2] will be the distance to the 2nd\n            closest poi, and so on.\n        \"\"\"\n        if max_distance is None:\n            max_distance = distance\n\n        if category not in self.poi_category_names:\n            assert 0, \"Need to call set_pois for this category\"\n\n        if num_pois > self.max_pois:\n            assert 0, \"Asking for more pois than set in init_pois\"\n\n        imp_num = self._imp_name_to_num(imp_name)\n\n        dists, poi_ids = self.net.find_all_nearest_pois(\n            distance,\n            num_pois,\n            category.encode('utf-8'),\n            imp_num)\n        dists[dists == -1] = max_distance\n\n        df = pd.DataFrame(dists, index=self.node_ids)\n        df.columns = list(range(1, num_pois+1))\n\n        if include_poi_ids:\n            df2 = pd.DataFrame(poi_ids, index=self.node_ids)\n            df2.columns = [\"poi%d\" % i for i in range(1, num_pois+1)]\n            for col in df2.columns:\n                # if this is still all working according to plan at this point\n                # the great magic trick is now to turn the integer position of\n                # the poi, which is painstakingly returned from the c++ code,\n                # and turn it into the actual index that was used when it was\n                # initialized as a pandas series - this really is pandas-like\n                # thinking.  it's complicated on the inside, but quite\n                # intuitive to the user I think\n                s = df2[col].astype('int')\n                df2[col] = self.poi_category_indexes[category].values[s]\n                df2.loc[s == -1, col] = np.nan\n\n            df = pd.concat([df, df2], axis=1)\n\n        return df", "response": "This method returns a dataframe that contains the distance to the nearest POI from each source node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of low connectivity nodes in the network that are connected to more than some threshold within a given distance.", "response": "def low_connectivity_nodes(self, impedance, count, imp_name=None):\n        \"\"\"\n        Identify nodes that are connected to fewer than some threshold\n        of other nodes within a given distance.\n\n        Parameters\n        ----------\n        impedance : float\n            Distance within which to search for other connected nodes. This\n            will usually be a distance unit in meters however if you have\n            customized the impedance this could be in other units such as\n            utility or time etc.\n        count : int\n            Threshold for connectivity. If a node is connected to fewer\n            than this many nodes within `impedance` it will be identified\n            as \"low connectivity\".\n        imp_name : string, optional\n            The impedance name to use for the aggregation on this network.\n            Must be one of the impedance names passed in the constructor of\n            this object.  If not specified, there must be only one impedance\n            passed in the constructor, which will be used.\n\n        Returns\n        -------\n        node_ids : array\n            List of \"low connectivity\" node IDs.\n\n        \"\"\"\n        # set a counter variable on all nodes\n        self.set(self.node_ids.to_series(), name='counter')\n\n        # count nodes within impedance range\n        agg = self.aggregate(\n            impedance, type='count', imp_name=imp_name, name='counter')\n\n        return np.array(agg[agg < count].index)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a Pandana network from a bounding box.", "response": "def pdna_network_from_bbox(\n        lat_min=None, lng_min=None, lat_max=None, lng_max=None, bbox=None,\n        network_type='walk', two_way=True,\n        timeout=180, memory=None, max_query_area_size=50 * 1000 * 50 * 1000):\n    \"\"\"\n    Make a Pandana network from a bounding lat/lon box\n    request to the Overpass API. Distance will be in the default units meters.\n\n    Parameters\n    ----------\n    lat_min, lng_min, lat_max, lng_max : float\n    bbox : tuple\n        Bounding box formatted as a 4 element tuple:\n        (lng_max, lat_min, lng_min, lat_max)\n    network_type : {'walk', 'drive'}, optional\n        Specify whether the network will be used for walking or driving.\n        A value of 'walk' attempts to exclude things like freeways,\n        while a value of 'drive' attempts to exclude things like\n        bike and walking paths.\n    two_way : bool, optional\n        Whether the routes are two-way. If True, node pairs will only\n        occur once.\n    timeout : int, optional\n        the timeout interval for requests and to pass to Overpass API\n    memory : int, optional\n        server memory allocation size for the query, in bytes.\n        If none, server will use its default allocation size\n    max_query_area_size : float, optional\n        max area for any part of the geometry, in the units the geometry is in\n\n    Returns\n    -------\n    network : pandana.Network\n\n    \"\"\"\n\n    nodes, edges = network_from_bbox(lat_min=lat_min, lng_min=lng_min,\n                                     lat_max=lat_max, lng_max=lng_max,\n                                     bbox=bbox, network_type=network_type,\n                                     two_way=two_way, timeout=timeout,\n                                     memory=memory,\n                                     max_query_area_size=max_query_area_size)\n\n    return Network(\n        nodes['x'], nodes['y'],\n        edges['from'], edges['to'], edges[['distance']])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process_node(e):\n\n    uninteresting_tags = {\n        'source',\n        'source_ref',\n        'source:ref',\n        'history',\n        'attribution',\n        'created_by',\n        'tiger:tlid',\n        'tiger:upload_uuid',\n    }\n\n    node = {\n        'id': e['id'],\n        'lat': e['lat'],\n        'lon': e['lon']\n    }\n\n    if 'tags' in e:\n        for t, v in list(e['tags'].items()):\n            if t not in uninteresting_tags:\n                node[t] = v\n\n    return node", "response": "Process a node element entry into a dict suitable for going into a Pandas DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_osm_query(query):\n    osm_url = 'http://www.overpass-api.de/api/interpreter'\n    req = requests.get(osm_url, params={'data': query})\n    req.raise_for_status()\n\n    return req.json()", "response": "Make a request to OSM and return the parsed JSON."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_node_query(lat_min, lng_min, lat_max, lng_max, tags=None):\n    if tags is not None:\n        if isinstance(tags, str):\n            tags = [tags]\n        tags = ''.join('[{}]'.format(t) for t in tags)\n    else:\n        tags = ''\n\n    query_fmt = (\n        '[out:json];'\n        '('\n        '  node'\n        '  {tags}'\n        '  ({lat_min},{lng_min},{lat_max},{lng_max});'\n        ');'\n        'out;')\n\n    return query_fmt.format(\n        lat_min=lat_min, lng_min=lng_min, lat_max=lat_max, lng_max=lng_max,\n        tags=tags)", "response": "Builds a string for a node - based OSM query."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsearches for OSM nodes within a bounding box that match given tags.", "response": "def node_query(lat_min, lng_min, lat_max, lng_max, tags=None):\n    \"\"\"\n    Search for OSM nodes within a bounding box that match given tags.\n\n    Parameters\n    ----------\n    lat_min, lng_min, lat_max, lng_max : float\n    tags : str or list of str, optional\n        Node tags that will be used to filter the search.\n        See http://wiki.openstreetmap.org/wiki/Overpass_API/Language_Guide\n        for information about OSM Overpass queries\n        and http://wiki.openstreetmap.org/wiki/Map_Features\n        for a list of tags.\n\n    Returns\n    -------\n    nodes : pandas.DataFrame\n        Will have 'lat' and 'lon' columns, plus other columns for the\n        tags associated with the node (these will vary based on the query).\n        Index will be the OSM node IDs.\n\n    \"\"\"\n    node_data = make_osm_query(build_node_query(\n        lat_min, lng_min, lat_max, lng_max, tags=tags))\n\n    if len(node_data['elements']) == 0:\n        raise RuntimeError('OSM query results contain no data.')\n\n    nodes = [process_node(n) for n in node_data['elements']]\n    return pd.DataFrame.from_records(nodes, index='id')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef equal(x, y):\n    if PY_3:\n        return test_case().assertEqual(x, y) or True\n\n    assert x == y", "response": "Shortcut function for unittest. TestCase. assertEqual."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning True if x matches y.", "response": "def matches(x, y, regex_expr=False):\n    \"\"\"\n    Tries to match a regular expression value ``x`` against ``y``.\n    Aliast``unittest.TestCase.assertEqual()``\n\n    Arguments:\n        x (regex|str): regular expression to test.\n        y (str): value to match.\n        regex_expr (bool): enables regex string based expression matching.\n\n    Raises:\n        AssertionError: in case of mismatching.\n\n    Returns:\n        bool\n    \"\"\"\n    # Parse regex expression, if needed\n    x = strip_regex(x) if regex_expr and isregex_expr(x) else x\n\n    # Run regex assertion\n    if PY_3:\n        # Retrieve original regex pattern\n        x = x.pattern if isregex(x) else x\n        # Assert regular expression via unittest matchers\n        return test_case().assertRegex(y, x) or True\n\n    # Primitive regex matching for Python 2.7\n    if isinstance(x, str):\n        x = re.compile(x, re.IGNORECASE)\n\n    assert x.match(y) is not None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if the given expression value is a regular expression with prefix re and suffix re and otherwise False.", "response": "def isregex_expr(expr):\n    \"\"\"\n    Returns ``True`` is the given expression value is a regular expression\n    like string with prefix ``re/`` and suffix ``/``, otherwise ``False``.\n\n    Arguments:\n        expr (mixed): expression value to test.\n\n    Returns:\n        bool\n    \"\"\"\n    if not isinstance(expr, str):\n        return False\n\n    return all([\n        len(expr) > 3,\n        expr.startswith('re/'),\n        expr.endswith('/')\n    ])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning True if the input value is a native regular expression object otherwise False.", "response": "def isregex(value):\n    \"\"\"\n    Returns ``True`` if the input argument object is a native\n    regular expression object, otherwise ``False``.\n\n    Arguments:\n        value (mixed): input value to test.\n\n    Returns:\n        bool\n    \"\"\"\n    if not value:\n        return False\n    return any((isregex_expr(value), isinstance(value, retype)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompare two values with regular expression matching support.", "response": "def compare(self, value, expectation, regex_expr=False):\n        \"\"\"\n        Compares two values with regular expression matching support.\n\n        Arguments:\n            value (mixed): value to compare.\n            expectation (mixed): value to match.\n            regex_expr (bool, optional): enables string based regex matching.\n\n        Returns:\n            bool\n        \"\"\"\n        return compare(value, expectation, regex_expr=regex_expr)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fluent(fn):\n    @functools.wraps(fn)\n    def wrapper(self, *args, **kw):\n        # Trigger method proxy\n        result = fn(self, *args, **kw)\n        # Return self instance or method result\n        return self if result is None else result\n    return wrapper", "response": "Decorator allowing easy method chaining."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compare(expr, value, regex_expr=False):\n    # Strict equality comparison\n    if expr == value:\n        return True\n\n    # Infer negate expression to match, if needed\n    negate = False\n    if isinstance(expr, str):\n        negate = expr.startswith(NEGATE)\n        expr = strip_negate(expr) if negate else expr\n\n    try:\n        # RegExp or strict equality comparison\n        test(expr, value, regex_expr=regex_expr)\n    except Exception as err:\n        if negate:\n            return True\n        else:\n            raise err\n\n    return True", "response": "Compares an expression against a given value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef trigger_methods(instance, args):\n    # Start the magic\n    for name in sorted(args):\n        value = args[name]\n        target = instance\n\n        # If response attibutes\n        if name.startswith('response_') or name.startswith('reply_'):\n            name = name.replace('response_', '').replace('reply_', '')\n            # If instance has response attribute, use it\n            if hasattr(instance, '_response'):\n                target = instance._response\n\n        # Retrieve class member for inspection and future use\n        member = getattr(target, name, None)\n\n        # Is attribute\n        isattr = name in dir(target)\n        iscallable = ismethod(member) and not isfunction(member)\n\n        if not iscallable and not isattr:\n            raise PookInvalidArgument('Unsupported argument: {}'.format(name))\n\n        # Set attribute or trigger method\n        if iscallable:\n            member(value)\n        else:\n            setattr(target, name, value)", "response": "Triggers specific class methods using a simple reflection mechanism based on the input dictionary params."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmatch the given HTTP request instance against the registered sequence of registered trust classes.", "response": "def match(self, request):\n        \"\"\"\n        Match the given HTTP request instance against the registered\n        matcher functions in the current engine.\n\n        Arguments:\n            request (pook.Request): outgoing request to match.\n\n        Returns:\n            tuple(bool, list[Exception]): ``True`` if all matcher tests\n                passes, otherwise ``False``. Also returns an optional list\n                of error exceptions.\n        \"\"\"\n        errors = []\n\n        def match(matcher):\n            try:\n                return matcher.match(request)\n            except Exception as err:\n                err = '{}: {}'.format(type(matcher).__name__, err)\n                errors.append(err)\n                return False\n\n        return all([match(matcher) for matcher in self]), errors"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a matcher instance by class or alias name.", "response": "def get(name):\n    \"\"\"\n    Returns a matcher instance by class or alias name.\n\n    Arguments:\n        name (str): matcher class name or alias.\n\n    Returns:\n        matcher: found matcher instance, otherwise ``None``.\n    \"\"\"\n    for matcher in matchers:\n        if matcher.__name__ == name or getattr(matcher, 'name', None) == name:\n            return matcher"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninitializing a new matcher instance passing variadic arguments to its constructor. Acts as a delegator proxy.", "response": "def init(name, *args):\n    \"\"\"\n    Initializes a matcher instance passing variadic arguments to\n    its constructor. Acts as a delegator proxy.\n\n    Arguments:\n        name (str): matcher class name or alias to execute.\n        *args (mixed): variadic argument\n\n    Returns:\n        matcher: matcher instance.\n\n    Raises:\n        ValueError: if matcher was not found.\n    \"\"\"\n    matcher = get(name)\n    if not matcher:\n        raise ValueError('Cannot find matcher: {}'.format(name))\n    return matcher(*args)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef header(self, key, value):\n        if type(key) is tuple:\n            key, value = str(key[0]), key[1]\n\n        headers = {key: value}\n        self._headers.extend(headers)", "response": "Defines a new response header."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndefine the response body data.", "response": "def body(self, body):\n        \"\"\"\n        Defines response body data.\n\n        Arguments:\n            body (str|bytes): response body to use.\n\n        Returns:\n            self: ``pook.Response`` current instance.\n        \"\"\"\n        if isinstance(body, bytes):\n            body = body.decode('utf-8')\n\n        self._body = body"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef json(self, data):\n        self._headers['Content-Type'] = 'application/json'\n        if not isinstance(data, str):\n            data = json.dumps(data, indent=4)\n        self._body = data", "response": "Defines the mock response JSON body."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set(self, key, val):\n        key_lower = key.lower()\n        new_vals = key, val\n        # Keep the common case aka no item present as fast as possible\n        vals = self._container.setdefault(key_lower, new_vals)\n        if new_vals is not vals:\n            self._container[key_lower] = [vals[0], vals[1], val]", "response": "Sets a header field with the given value removing previous values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _append_funcs(target, items):\n    [target.append(item) for item in items\n     if isfunction(item) or ismethod(item)]", "response": "Helper function to append functions into a given list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntriggering request mock definition methods dynamically based on input keyword arguments passed to pook. Mock constructor.", "response": "def _trigger_request(instance, request):\n    \"\"\"\n    Triggers request mock definition methods dynamically based on input\n    keyword arguments passed to `pook.Mock` constructor.\n\n    This is used to provide a more Pythonic interface vs chainable API\n    approach.\n    \"\"\"\n    if not isinstance(request, Request):\n        raise TypeError('request must be instance of pook.Request')\n\n    # Register request matchers\n    for key in request.keys:\n        if hasattr(instance, key):\n            getattr(instance, key)(getattr(request, key))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the mock URL to match.", "response": "def url(self, url):\n        \"\"\"\n        Defines the mock URL to match.\n        It can be a full URL with path and query params.\n\n        Protocol schema is optional, defaults to ``http://``.\n\n        Arguments:\n            url (str): mock URL to match. E.g: ``server.com/api``.\n\n        Returns:\n            self: current Mock instance.\n        \"\"\"\n        self._request.url = url\n        self.add_matcher(matcher('URLMatcher', url))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef method(self, method):\n        self._request.method = method\n        self.add_matcher(matcher('MethodMatcher', method))", "response": "Sets the HTTP method to match."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndefining a URL path to match.", "response": "def path(self, path):\n        \"\"\"\n        Defines a URL path to match.\n\n        Only call this method if the URL has no path already defined.\n\n        Arguments:\n            path (str): URL path value to match. E.g: ``/api/users``.\n\n        Returns:\n            self: current Mock instance.\n        \"\"\"\n        url = furl(self._request.rawurl)\n        url.path = path\n        self._request.url = url.url\n        self.add_matcher(matcher('PathMatcher', path))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef header(self, name, value):\n        headers = {name: value}\n        self._request.headers = headers\n        self.add_matcher(matcher('HeadersMatcher', headers))", "response": "Defines a header name and value to match."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef headers(self, headers=None, **kw):\n        headers = kw if kw else headers\n        self._request.headers = headers\n        self.add_matcher(matcher('HeadersMatcher', headers))", "response": "Sets the headers of the current request."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndefine a new header matcher expectation that must be present in the current Mock instance.", "response": "def header_present(self, *names):\n        \"\"\"\n        Defines a new header matcher expectation that must be present in the\n        outgoing request in order to be satisfied, no matter what value it\n        hosts.\n\n        Header keys are case insensitive.\n\n        Arguments:\n            *names (str): header or headers names to match.\n\n        Returns:\n            self: current Mock instance.\n\n        Example::\n\n            (pook.get('server.com/api')\n                .header_present('content-type'))\n        \"\"\"\n        for name in names:\n            headers = {name: re.compile('(.*)')}\n            self.add_matcher(matcher('HeadersMatcher', headers))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef headers_present(self, headers):\n        headers = {name: re.compile('(.*)') for name in headers}\n        self.add_matcher(matcher('HeadersMatcher', headers))", "response": "Sets the matcher for the set of headers that must be present in the current Mock instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndefining the Content - Type outgoing header value to match.", "response": "def content(self, value):\n        \"\"\"\n        Defines the ``Content-Type`` outgoing header value to match.\n\n        You can pass one of the following type aliases instead of the full\n        MIME type representation:\n\n        - ``json`` = ``application/json``\n        - ``xml`` = ``application/xml``\n        - ``html`` = ``text/html``\n        - ``text`` = ``text/plain``\n        - ``urlencoded`` = ``application/x-www-form-urlencoded``\n        - ``form`` = ``application/x-www-form-urlencoded``\n        - ``form-data`` = ``application/x-www-form-urlencoded``\n\n        Arguments:\n            value (str): type alias or header value to match.\n\n        Returns:\n            self: current Mock instance.\n        \"\"\"\n        header = {'Content-Type': TYPES.get(value, value)}\n        self._request.headers = header\n        self.add_matcher(matcher('HeadersMatcher', header))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef params(self, params):\n        url = furl(self._request.rawurl)\n        url = url.add(params)\n        self._request.url = url.url\n        self.add_matcher(matcher('QueryMatcher', params))", "response": "Defines a set of URL query params to match."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the body data to match.", "response": "def body(self, body):\n        \"\"\"\n        Defines the body data to match.\n\n        ``body`` argument can be a ``str``, ``binary`` or a regular expression.\n\n        Arguments:\n            body (str|binary|regex): body data to match.\n\n        Returns:\n            self: current Mock instance.\n        \"\"\"\n        self._request.body = body\n        self.add_matcher(matcher('BodyMatcher', body))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef json(self, json):\n        self._request.json = json\n        self.add_matcher(matcher('JSONMatcher', json))", "response": "Sets the JSON body to match."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the current request s XML body value.", "response": "def xml(self, xml):\n        \"\"\"\n        Defines a XML body value to match.\n\n        Arguments:\n            xml (str|regex): body XML to match.\n\n        Returns:\n            self: current Mock instance.\n        \"\"\"\n        self._request.xml = xml\n        self.add_matcher(matcher('XMLMatcher', xml))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread the body from a disk file.", "response": "def file(self, path):\n        \"\"\"\n        Reads the body to match from a disk file.\n\n        Arguments:\n            path (str): relative or absolute path to file to read from.\n\n        Returns:\n            self: current Mock instance.\n        \"\"\"\n        with open(path, 'r') as f:\n            self.body(str(f.read()))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nenables persistent mode for the current mock instance.", "response": "def persist(self, status=None):\n        \"\"\"\n        Enables persistent mode for the current mock.\n\n        Returns:\n            self: current Mock instance.\n        \"\"\"\n        self._persist = status if type(status) is bool else True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef error(self, error):\n        self._error = RuntimeError(error) if isinstance(error, str) else error", "response": "Defines a simulated exception that will be raised."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndefine the mock response for this instance.", "response": "def reply(self, status=200, new_response=False, **kw):\n        \"\"\"\n        Defines the mock response.\n\n        Arguments:\n            status (int, optional): response status code. Defaults to ``200``.\n            **kw (dict): optional keyword arguments passed to ``pook.Response``\n                constructor.\n\n        Returns:\n            pook.Response: mock response definition instance.\n        \"\"\"\n        # Use or create a Response mock instance\n        res = Response(**kw) if new_response else self._response\n        # Define HTTP mandatory response status\n        res.status(status or res._status)\n        # Expose current mock instance in response for self-reference\n        res.mock = self\n        # Define mock response\n        self._response = res\n        # Return response\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmatches an incoming HTTP request against the current mock matchers.", "response": "def match(self, request):\n        \"\"\"\n        Matches an outgoing HTTP request against the current mock matchers.\n\n        This method acts like a delegator to `pook.MatcherEngine`.\n\n        Arguments:\n            request (pook.Request): request instance to match.\n\n        Raises:\n            Exception: if the mock has an exception defined.\n\n        Returns:\n            tuple(bool, list[Exception]): ``True`` if the mock matches\n                the outgoing HTTP request, otherwise ``False``. Also returns\n                an optional list of error exceptions.\n        \"\"\"\n        # If mock already expired, fail it\n        if self._times <= 0:\n            raise PookExpiredMock('Mock expired')\n\n        # Trigger mock filters\n        for test in self.filters:\n            if not test(request, self):\n                return False, []\n\n        # Trigger mock mappers\n        for mapper in self.mappers:\n            request = mapper(request, self)\n            if not request:\n                raise ValueError('map function must return a request object')\n\n        # Match incoming request against registered mock matchers\n        matches, errors = self.matchers.match(request)\n\n        # If not matched, return False\n        if not matches:\n            return False, errors\n\n        # Register matched request for further inspecion and reference\n        self._calls.append(request)\n\n        # Increase mock call counter\n        self._matches += 1\n        if not self._persist:\n            self._times -= 1\n\n        # Raise simulated error\n        if self._error:\n            raise self._error\n\n        # Trigger callback when matched\n        for callback in self.callbacks:\n            callback(request, self)\n\n        return True, []"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef activate_async(fn, _engine):\n    @coroutine\n    @functools.wraps(fn)\n    def wrapper(*args, **kw):\n        _engine.activate()\n        try:\n            if iscoroutinefunction(fn):\n                yield from fn(*args, **kw)  # noqa\n            else:\n                fn(*args, **kw)\n        finally:\n            _engine.disable()\n\n    return wrapper", "response": "A decorator that activates the given function asynchronously."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset a custom mock interceptor engine that will be used to generate the HTTP traffic for the specified object.", "response": "def set_mock_engine(self, engine):\n        \"\"\"\n        Sets a custom mock engine, replacing the built-in one.\n\n        This is particularly useful if you want to replace the built-in\n        HTTP traffic mock interceptor engine with your custom one.\n\n        For mock engine implementation details, see `pook.MockEngine`.\n\n        Arguments:\n            engine (pook.MockEngine): custom mock engine to use.\n        \"\"\"\n        if not engine:\n            raise TypeError('engine must be a valid object')\n\n        # Instantiate mock engine\n        mock_engine = engine(self)\n\n        # Validate minimum viable interface\n        methods = ('activate', 'disable')\n        if not all([hasattr(mock_engine, method) for method in methods]):\n            raise NotImplementedError('engine must implementent the '\n                                      'required methods')\n\n        # Use the custom mock engine\n        self.mock_engine = mock_engine\n\n        # Enable mock engine, if needed\n        if self.active:\n            self.mock_engine.activate()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nenabling real networking mode for the specified list of hostnames.", "response": "def enable_network(self, *hostnames):\n        \"\"\"\n        Enables real networking mode, optionally passing one or multiple\n        hostnames that would be used as filter.\n\n        If at least one hostname matches with the outgoing traffic, the\n        request will be executed via the real network.\n\n        Arguments:\n            *hostnames: optional list of host names to enable real network\n                against them. hostname value can be a regular expression.\n        \"\"\"\n        def hostname_filter(hostname, req):\n            if isregex(hostname):\n                return hostname.match(req.url.hostname)\n            return req.url.hostname == hostname\n\n        for hostname in hostnames:\n            self.use_network_filter(partial(hostname_filter, hostname))\n\n        self.networking = True"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate and registers a new HTTP mock instance for the current engine.", "response": "def mock(self, url=None, **kw):\n        \"\"\"\n        Creates and registers a new HTTP mock in the current engine.\n\n        Arguments:\n            url (str): request URL to mock.\n            activate (bool): force mock engine activation.\n                Defaults to ``False``.\n            **kw (mixed): variadic keyword arguments for ``Mock`` constructor.\n\n        Returns:\n            pook.Mock: new mock instance.\n        \"\"\"\n        # Activate mock engine, if explicitly requested\n        if kw.get('activate'):\n            kw.pop('activate')\n            self.activate()\n\n        # Create the new HTTP mock expectation\n        mock = Mock(url=url, **kw)\n        # Expose current engine instance via mock\n        mock._engine = self\n        # Register the mock in the current engine\n        self.add_mock(mock)\n\n        # Return it for consumer satisfaction\n        return mock"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves a specific mock instance by object reference.", "response": "def remove_mock(self, mock):\n        \"\"\"\n        Removes a specific mock instance by object reference.\n\n        Arguments:\n            mock (pook.Mock): mock instance to remove.\n        \"\"\"\n        self.mocks = [m for m in self.mocks if m is not mock]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nactivates the registered interceptors in the mocking engine.", "response": "def activate(self):\n        \"\"\"\n        Activates the registered interceptors in the mocking engine.\n\n        This means any HTTP traffic captures by those interceptors will\n        trigger the HTTP mock matching engine in order to determine if a given\n        HTTP transaction should be mocked out or not.\n        \"\"\"\n        if self.active:\n            return None\n\n        # Activate mock engine\n        self.mock_engine.activate()\n        # Enable engine state\n        self.active = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndisables interceptors and stops intercepting any outgoing HTTP traffic.", "response": "def disable(self):\n        \"\"\"\n        Disables interceptors and stops intercepting any outgoing HTTP traffic.\n        \"\"\"\n        if not self.active:\n            return None\n\n        # Disable current mock engine\n        self.mock_engine.disable()\n        # Disable engine state\n        self.active = False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nverify if real networking mode should be used for the given HTTP request.", "response": "def should_use_network(self, request):\n        \"\"\"\n        Verifies if real networking mode should be used for the given\n        request, passing it to the registered network filters.\n\n        Arguments:\n            request (pook.Request): outgoing HTTP request to test.\n\n        Returns:\n            bool\n        \"\"\"\n        return (self.networking and\n                all((fn(request) for fn in self.network_filters)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmatching a given request instance contract against the registered mock definitions.", "response": "def match(self, request):\n        \"\"\"\n        Matches a given Request instance contract against the registered mocks.\n\n        If a mock passes all the matchers, its response will be returned.\n\n        Arguments:\n            request (pook.Request): Request contract to match.\n\n        Raises:\n            pook.PookNoMatches: if networking is disabled and no mock matches\n                with the given request contract.\n\n        Returns:\n            pook.Response: the mock response to be used by the interceptor.\n        \"\"\"\n        # Trigger engine-level request filters\n        for test in self.filters:\n            if not test(request, self):\n                return False\n\n        # Trigger engine-level request mappers\n        for mapper in self.mappers:\n            request = mapper(request, self)\n            if not request:\n                raise ValueError('map function must return a request object')\n\n        # Store list of mock matching errors for further debugging\n        match_errors = []\n\n        # Try to match the request against registered mock definitions\n        for mock in self.mocks[:]:\n            try:\n                # Return the first matched HTTP request mock\n                matches, errors = mock.match(request.copy())\n                if len(errors):\n                    match_errors += errors\n                if matches:\n                    return mock\n            except PookExpiredMock:\n                # Remove the mock if already expired\n                self.mocks.remove(mock)\n\n        # Validate that we have a mock\n        if not self.should_use_network(request):\n            msg = 'pook error!\\n\\n'\n\n            msg += (\n                '=> Cannot match any mock for the '\n                'following request:\\n{}'.format(request)\n            )\n\n            # Compose unmatch error details, if debug mode is enabled\n            if self.debug:\n                err = '\\n\\n'.join([str(err) for err in match_errors])\n                if err:\n                    msg += '\\n\\n=> Detailed matching errors:\\n{}\\n'.format(err)\n\n            # Raise no matches exception\n            raise PookNoMatches(msg)\n\n        # Register unmatched request\n        self.unmatched_reqs.append(request)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef copy(self):\n        req = type(self)()\n        req.__dict__ = self.__dict__.copy()\n        req._headers = self.headers.copy()\n        return req", "response": "Returns a copy of the current Request instance for side - effects purposes."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef activate(fn=None):\n    # If not used as decorator, activate the engine and exit\n    if not isfunction(fn):\n        _engine.activate()\n        return None\n\n    # If used as decorator for an async coroutine, wrap it\n    if iscoroutinefunction is not None and iscoroutinefunction(fn):\n        return activate_async(fn, _engine)\n\n    @functools.wraps(fn)\n    def wrapper(*args, **kw):\n        _engine.activate()\n        try:\n            fn(*args, **kw)\n        finally:\n            _engine.disable()\n\n    return wrapper", "response": "Decorator for activate a new object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new isolated mock engine to be used via context manager. Example:: with pook.use() as engine: pook.mock('server.com/foo').reply(404) res = requests.get('server.com/foo') assert res.status_code == 404", "response": "def use(network=False):\n    \"\"\"\n    Creates a new isolated mock engine to be used via context manager.\n\n    Example::\n\n        with pook.use() as engine:\n            pook.mock('server.com/foo').reply(404)\n\n            res = requests.get('server.com/foo')\n            assert res.status_code == 404\n    \"\"\"\n    global _engine\n\n    # Create temporal engine\n    __engine = _engine\n    activated = __engine.active\n    if activated:\n        __engine.disable()\n\n    _engine = Engine(network=network)\n    _engine.activate()\n\n    # Yield enfine to be used by the context manager\n    yield _engine\n\n    # Restore engine state\n    _engine.disable()\n    if network:\n        _engine.disable_network()\n\n    # Restore previous engine\n    _engine = __engine\n    if activated:\n        _engine.activate()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef regex(expression, flags=re.IGNORECASE):\n    return re.compile(expression, flags=flags)", "response": "Returns a regular expression that matches the given string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_interceptor(self, *interceptors):\n        for interceptor in interceptors:\n            self.interceptors.append(interceptor(self.engine))", "response": "Adds one or multiple HTTP traffic interceptors to the current mocking engine."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove an interceptor by name.", "response": "def remove_interceptor(self, name):\n        \"\"\"\n        Removes a specific interceptor by name.\n\n        Arguments:\n            name (str): interceptor name to disable.\n\n        Returns:\n            bool: `True` if the interceptor was disabled, otherwise `False`.\n        \"\"\"\n        for index, interceptor in enumerate(self.interceptors):\n            matches = (\n                type(interceptor).__name__ == name or\n                getattr(interceptor, 'name') == name\n            )\n            if matches:\n                self.interceptors.pop(index)\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_setting(connection, key):\n    if key in connection.settings_dict:\n        return connection.settings_dict[key]\n    else:\n        return getattr(settings, key)", "response": "Get key from connection or default to settings."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef as_sql(self, compiler, connection):\n        sql, params = super(DecryptedCol, self).as_sql(compiler, connection)\n        sql = self.target.get_decrypt_sql(connection) % (sql, self.target.get_cast_sql())\n        return sql, params", "response": "Build SQL with decryption and casting."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pre_save(self, model_instance, add):\n        if self.original:\n            original_value = getattr(model_instance, self.original)\n            setattr(model_instance, self.attname, original_value)\n\n        return super(HashMixin, self).pre_save(model_instance, add)", "response": "Save the original value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_placeholder(self, value=None, compiler=None, connection=None):\n        if value is None or value.startswith('\\\\x'):\n            return '%s'\n\n        return self.get_encrypt_sql(connection)", "response": "Return the placeholder for the current value."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the decryption for col.", "response": "def get_col(self, alias, output_field=None):\n        \"\"\"Get the decryption for col.\"\"\"\n        if output_field is None:\n            output_field = self\n        if alias != self.model._meta.db_table or output_field != self:\n            return DecryptedCol(\n                alias,\n                self,\n                output_field\n            )\n        else:\n            return self.cached_col"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntell postgres to encrypt this field using PGP.", "response": "def get_placeholder(self, value=None, compiler=None, connection=None):\n        \"\"\"Tell postgres to encrypt this field using PGP.\"\"\"\n        return self.encrypt_sql.format(get_setting(connection, 'PUBLIC_PGP_KEY'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef base_regression(Q, slope=None):\n    if slope is None:\n        slope = (Q[dtavgii] - Q[tavgii]*Q[davgii]/Q[sii]) \\\n                /(Q[tsqii] - Q[tavgii]**2/Q[sii])\n        only_intercept=False\n    else:\n        only_intercept=True\n\n    intercept = (Q[davgii] - Q[tavgii]*slope)/Q[sii]\n\n    if only_intercept:\n        return {'slope':slope, 'intercept':intercept,\n                'chisq': 0.5*(Q[dsqii]/Q[sii] - Q[davgii]**2/Q[sii]**2)}\n\n    chisq = 0.5*(Q[dsqii] - Q[davgii]**2/Q[sii]\n                - (Q[dtavgii] - Q[davgii]*Q[tavgii]/Q[sii])**2/(Q[tsqii]\n                - Q[tavgii]**2/Q[sii]))\n\n    estimator_hessian = np.array([[Q[tsqii], Q[tavgii]], [Q[tavgii], Q[sii]]])\n\n    return {'slope':slope, 'intercept':intercept,\n            'chisq':chisq, 'hessian':estimator_hessian,\n            'cov':np.linalg.inv(estimator_hessian)}", "response": "This function calculates the regression coefficients for a\n    given vector containing averages of tip and branch\n    quantities."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Cov(self):\n        # accumulate the covariance matrix by adding 'squares'\n        M = np.zeros((self.N, self.N))\n        for n in self.tree.find_clades():\n            if n == self.tree.root:\n                continue\n            M[np.meshgrid(n._ii, n._ii)] += self.branch_variance(n)\n        return M", "response": "Calculates the covariance matrix of the tips assuming variance\n        has accumulated along branches of the tree accoriding to the provided\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef CovInv(self):\n        self.recurse(full_matrix=True)\n        return self.tree.root.cinv", "response": "Returns the inverse of the covariance matrix."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef recurse(self, full_matrix=False):\n        for n in self.tree.get_nonterminals(order='postorder'):\n            n_leaves = len(n._ii)\n            if full_matrix: M = np.zeros((n_leaves, n_leaves), dtype=float)\n            r = np.zeros(n_leaves, dtype=float)\n            c_count = 0\n            for c in n:\n                ssq = self.branch_variance(c)\n                nc = len(c._ii)\n                if c.is_terminal():\n                    if full_matrix:\n                        M[c_count, c_count] = 1.0/ssq\n                    r[c_count] = 1.0/ssq\n                else:\n                    if full_matrix:\n                        M[c_count:c_count+nc, c_count:c_count+nc] = c.cinv - ssq*np.outer(c.r,c.r)/(1+ssq*c.s)\n                    r[c_count:c_count+nc] = c.r/(1+ssq*c.s)\n                c_count += nc\n\n            if full_matrix: n.cinv = M\n            n.r = r #M.sum(axis=1)\n            n.s = n.r.sum()", "response": "This function recursively calculates the inverse covariance matrix of the tree."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _calculate_averages(self):\n        for n in self.tree.get_nonterminals(order='postorder'):\n            Q = np.zeros(6, dtype=float)\n            for c in n:\n                tv = self.tip_value(c)\n                bv = self.branch_value(c)\n                var = self.branch_variance(c)\n                Q += self.propagate_averages(c, tv, bv, var)\n            n.Q=Q\n\n        for n in self.tree.find_clades(order='preorder'):\n            O = np.zeros(6, dtype=float)\n            if n==self.tree.root:\n                n.Qtot = n.Q\n                continue\n\n            for c in n.up:\n                if c==n:\n                    continue\n\n                tv = self.tip_value(c)\n                bv = self.branch_value(c)\n                var = self.branch_variance(c)\n                O += self.propagate_averages(c, tv, bv, var)\n\n            if n.up!=self.tree.root:\n                c = n.up\n                tv = self.tip_value(c)\n                bv = self.branch_value(c)\n                var = self.branch_variance(c)\n                O += self.propagate_averages(c, tv, bv, var, outgroup=True)\n            n.O = O\n\n            if not n.is_terminal():\n                tv = self.tip_value(n)\n                bv = self.branch_value(n)\n                var = self.branch_variance(n)\n                n.Qtot = n.Q + self.propagate_averages(n, tv, bv, var, outgroup=True)", "response": "calculate the weighted sums of the tip and branch values and their second moments."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef propagate_averages(self, n, tv, bv, var, outgroup=False):\n        if n.is_terminal() and outgroup==False:\n            if tv is None or np.isinf(tv) or np.isnan(tv):\n                res = np.array([0, 0, 0, 0, 0, 0])\n            elif var==0:\n                res = np.array([np.inf, np.inf, np.inf, np.inf, np.inf, np.inf])\n            else:\n                res = np.array([\n                    tv/var,\n                    bv/var,\n                    tv**2/var,\n                    bv*tv/var,\n                    bv**2/var,\n                    1.0/var], dtype=float)\n        else:\n            tmpQ = n.O if outgroup else n.Q\n            denom = 1.0/(1+var*tmpQ[sii])\n            res = np.array([\n                tmpQ[tavgii]*denom,\n                (tmpQ[davgii] + bv*tmpQ[sii])*denom,\n                tmpQ[tsqii] - var*tmpQ[tavgii]**2*denom,\n                tmpQ[dtavgii] + tmpQ[tavgii]*bv - var*tmpQ[tavgii]*(tmpQ[davgii] + bv*tmpQ[sii])*denom,\n                tmpQ[dsqii] + 2*bv*tmpQ[davgii] + bv**2*tmpQ[sii] - var*(tmpQ[davgii]**2 + 2*bv*tmpQ[davgii]*tmpQ[sii] + bv**2*tmpQ[sii]**2)*denom,\n                tmpQ[sii]*denom]\n            )\n\n        return res", "response": "This function operates on the means variance and covariances along a branch."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating standard explained variance of the root - to - tip distance and time", "response": "def explained_variance(self):\n        \"\"\"calculate standard explained variance\n\n        Returns\n        -------\n        float\n            r-value of the root-to-tip distance and time.\n            independent of regression model, but dependent on root choice\n        \"\"\"\n        self.tree.root._v=0\n        for n in self.tree.get_nonterminals(order='preorder'):\n            for c in n:\n                c._v = n._v + self.branch_value(c)\n        raw = np.array([(self.tip_value(n), n._v) for n in self.tree.get_terminals()\n                         if self.tip_value(n) is not None])\n        return np.corrcoef(raw.T)[0,1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nregresses tip values against branch values", "response": "def regression(self, slope=None):\n        \"\"\"regress tip values against branch values\n\n        Parameters\n        ----------\n        slope : None, optional\n            if given, the slope isn't optimized\n\n        Returns\n        -------\n        dict\n            regression parameters\n        \"\"\"\n        self._calculate_averages()\n\n        clock_model = base_regression(self.tree.root.Q, slope)\n        clock_model['r_val'] = self.explained_variance()\n\n        return clock_model"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_best_root(self, force_positive=True, slope=None):\n        self._calculate_averages()\n        best_root = {\"chisq\": np.inf}\n        for n in self.tree.find_clades():\n            if n==self.tree.root:\n                continue\n\n            tv = self.tip_value(n)\n            bv = self.branch_value(n)\n            var = self.branch_variance(n)\n            x, chisq = self._optimal_root_along_branch(n, tv, bv, var, slope=slope)\n\n            if (chisq<best_root[\"chisq\"]):\n                tmpQ = self.propagate_averages(n, tv, bv*x, var*x) \\\n                     + self.propagate_averages(n, tv, bv*(1-x), var*(1-x), outgroup=True)\n                reg = base_regression(tmpQ, slope=slope)\n                if reg[\"slope\"]>=0 or (force_positive==False):\n                    best_root = {\"node\":n, \"split\":x}\n                    best_root.update(reg)\n\n        if 'node' not in best_root:\n            print(\"TreeRegression.find_best_root: No valid root found!\", force_positive)\n            return None\n\n        if 'hessian' in best_root:\n            # calculate differentials with respect to x\n            deriv = []\n            n = best_root[\"node\"]\n            tv = self.tip_value(n)\n            bv = self.branch_value(n)\n            var = self.branch_variance(n)\n            for dx in [-0.001, 0.001]:\n                y = min(1.0, max(0.0, best_root[\"split\"]+dx))\n                tmpQ = self.propagate_averages(n, tv, bv*y, var*y) \\\n                     + self.propagate_averages(n, tv, bv*(1-y), var*(1-y), outgroup=True)\n                reg = base_regression(tmpQ, slope=slope)\n                deriv.append([y,reg['chisq'], tmpQ[tavgii], tmpQ[davgii]])\n\n            estimator_hessian = np.zeros((3,3))\n            estimator_hessian[:2,:2] = best_root['hessian']\n            estimator_hessian[2,2] = (deriv[0][1] + deriv[1][1] - 2.0*best_root['chisq'])/(deriv[0][0] - deriv[1][0])**2\n            # estimator_hessian[2,0] = (deriv[0][2] - deriv[1][2])/(deriv[0][0] - deriv[1][0])\n            # estimator_hessian[2,1] = (deriv[0][3] - deriv[1][3])/(deriv[0][0] - deriv[1][0])\n            estimator_hessian[0,2] = estimator_hessian[2,0]\n            estimator_hessian[1,2] = estimator_hessian[2,1]\n            best_root['hessian'] = estimator_hessian\n            best_root['cov'] = np.linalg.inv(estimator_hessian)\n\n        return best_root", "response": "Find the best root node for the tree."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndetermining the best root and reroot the tree to this value. Note that this can change the parent child relations of the tree and values associated with branches rather than nodes (e.g. confidence) might need to be re-evaluated afterwards Parameters ---------- force_positive : bool, optional if True, the search for a root will only consider positive rate estimates slope : float, optional if given, it will find the optimal root given a fixed rate. If slope==0, this corresponds to minimal root-to-tip variance rooting (min_dev) Returns ------- dict regression parameters", "response": "def optimal_reroot(self, force_positive=True, slope=None):\n        \"\"\"\n        determine the best root and reroot the tree to this value.\n        Note that this can change the parent child relations of the tree\n        and values associated with branches rather than nodes\n        (e.g. confidence) might need to be re-evaluated afterwards\n\n        Parameters\n        ----------\n        force_positive : bool, optional\n            if True, the search for a root will only consider positive rate estimates\n\n        slope : float, optional\n            if given, it will find the optimal root given a fixed rate. If slope==0, this\n            corresponds to minimal root-to-tip variance rooting (min_dev)\n\n        Returns\n        -------\n        dict\n            regression parameters\n        \"\"\"\n        best_root = self.find_best_root(force_positive=force_positive, slope=slope)\n        best_node = best_root[\"node\"]\n\n        x = best_root[\"split\"]\n        if x<1e-5:\n            new_node = best_node\n        elif x>1.0-1e-5:\n            new_node = best_node.up\n        else:\n            # create new node in the branch and root the tree to it\n            new_node = Phylo.BaseTree.Clade()\n\n            # insert the new node in the middle of the branch\n            # by simple re-wiring the links on the both sides of the branch\n            # and fix the branch lengths\n            new_node.branch_length = best_node.branch_length*(1-x)\n            new_node.up = best_node.up\n            new_node.clades = [best_node]\n            new_node.up.clades = [k if k!=best_node else new_node\n                                  for k in best_node.up.clades]\n\n            best_node.branch_length *= x\n            best_node.up = new_node\n\n        new_node.rtt_regression = best_root\n        self.tree.root_with_outgroup(new_node)\n\n        self.tree.ladderize()\n        for n in self.tree.get_nonterminals(order='postorder'):\n            for c in n:\n                c.up=n\n        return best_root"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nplotting the root - to - tip distance vs time as a basic time - tree diagnostic.", "response": "def clock_plot(self, add_internal=False, ax=None, regression=None,\n                   confidence=True, n_sigma = 2, fs=14):\n        \"\"\"Plot root-to-tip distance vs time as a basic time-tree diagnostic\n\n        Parameters\n        ----------\n        add_internal : bool, optional\n            add internal nodes. this will only work if the tree has been dated already\n        ax : None, optional\n            an matplotlib axis to plot into. if non provided, a new figure is opened\n        regression : None, optional\n            a dict containing parameters of a root-to-tip vs time regression as\n            returned by the function base_regression\n        confidence : bool, optional\n            add confidence area to the regression line\n        n_sigma : int, optional\n            number of standard deviations for the confidence area.\n        fs : int, optional\n            fontsize\n\n        \"\"\"\n        import matplotlib.pyplot as plt\n        if ax is None:\n            plt.figure()\n            ax=plt.subplot(111)\n\n        self.tree.root._v=0\n        for n in self.tree.get_nonterminals(order='preorder'):\n            for c in n:\n                c._v = n._v + self.branch_value(c)\n\n        tips = self.tree.get_terminals()\n        internal = self.tree.get_nonterminals()\n\n        # get values of terminals\n        xi = np.array([self.tip_value(n) for n in tips])\n        yi = np.array([n._v for n in tips])\n        ind = np.array([n.bad_branch  if hasattr(n, 'bad_branch') else False for n in tips])\n        if add_internal:\n            xi_int = np.array([n.numdate for n in internal])\n            yi_int = np.array([n._v for n in internal])\n            ind_int = np.array([n.bad_branch  if hasattr(n, 'bad_branch') else False  for n in internal])\n\n        if regression:\n            # plot regression line\n            t_mrca = -regression['intercept']/regression['slope']\n            if add_internal:\n                time_span = np.max(xi_int[~ind_int]) - np.min(xi_int[~ind_int])\n                x_vals = np.array([max(np.min(xi_int[~ind_int]), t_mrca) - 0.1*time_span, np.max(xi[~ind])+0.05*time_span])\n            else:\n                time_span = np.max(xi[~ind]) - np.min(xi[~ind])\n                x_vals = np.array([max(np.min(xi[~ind]), t_mrca) - 0.1*time_span, np.max(xi[~ind]+0.05*time_span)])\n\n            # plot confidence interval\n            if confidence and 'cov' in regression:\n                x_vals = np.linspace(x_vals[0], x_vals[1], 100)\n                y_vals = regression['slope']*x_vals + regression['intercept']\n                dev = n_sigma*np.array([np.sqrt(regression['cov'][:2,:2].dot(np.array([x, 1])).dot(np.array([x,1]))) for x in x_vals])\n                dev_slope = n_sigma*np.sqrt(regression['cov'][0,0])\n                ax.fill_between(x_vals, y_vals-dev, y_vals+dev, alpha=0.2)\n                dp = np.array([regression['intercept']/regression['slope']**2,-1./regression['slope']])\n                dev_rtt = n_sigma*np.sqrt(regression['cov'][:2,:2].dot(dp).dot(dp))\n\n            else:\n                dev_rtt = None\n                dev_slope = None\n\n            ax.plot(x_vals, regression['slope']*x_vals + regression['intercept'],\n                    label = r\"$y=\\alpha + \\beta t$\"+\"\\n\"+\n                            r\"$\\beta=$%1.2e\"%(regression[\"slope\"])\n                            + (\"+/- %1.e\"%dev_slope if dev_slope else \"\") +\n                            \"\\nroot date: %1.1f\"%(-regression['intercept']/regression['slope']) +\n                            (\"+/- %1.2f\"%dev_rtt if dev_rtt else \"\"))\n\n\n        ax.scatter(xi[~ind], yi[~ind], label=(\"tips\" if add_internal else None))\n        if ind.sum():\n            try:\n                # note: this is treetime specific\n                tmp_x = np.array([np.mean(n.raw_date_constraint) if n.raw_date_constraint else None\n                                  for n in self.tree.get_terminals()])\n                ax.scatter(tmp_x[ind], yi[ind], label=\"ignored tips\", c='r')\n            except:\n                pass\n        if add_internal:\n            ax.scatter(xi_int[~ind_int], yi_int[~ind_int], label=\"internal nodes\")\n\n        ax.set_ylabel('root-to-tip distance', fontsize=fs)\n        ax.set_xlabel('date', fontsize=fs)\n        ax.ticklabel_format(useOffset=False)\n        ax.tick_params(labelsize=fs*0.8)\n        ax.set_ylim([0, 1.1*np.max(yi)])\n        plt.tight_layout()\n        plt.legend(fontsize=fs*0.8)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef F81(mu=1.0, pi=None, alphabet=\"nuc\", **kwargs):\n\n    if pi is None:\n        pi=0.25*np.ones(4, dtype=float)\n    num_chars = len(alphabets[alphabet])\n\n    pi = np.array(pi, dtype=float)\n\n    if num_chars != len(pi) :\n        pi = np.ones((num_chars, ), dtype=float)\n        print (\"GTR: Warning!The number of the characters in the alphabet does not match the \"\n            \"shape of the vector of equilibrium frequencies Pi -- assuming equal frequencies for all states.\")\n\n    W = np.ones((num_chars,num_chars))\n    pi /= (1.0 * np.sum(pi))\n    gtr = GTR(alphabet=alphabets[alphabet])\n    gtr.assign_rates(mu=mu, pi=pi, W=W)\n    return gtr", "response": "F90 model for the Seismic Algebraic Tree Algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef T92(mu=1.0, pi_GC=0.5, kappa=0.1, **kwargs):\n\n    W = _create_transversion_transition_W(kappa)\n    # A C G T\n    if pi_CG >=1.:\n        raise ValueError(\"The relative CG content specified is larger than 1.0!\")\n    pi = np.array([(1.-pi_CG)*0.5, pi_CG*0.5, pi_CG*0.5, (1-pi_CG)*0.5])\n    gtr = GTR(alphabet=alphabets['nuc_nogap'])\n    gtr.assign_rates(mu=mu, pi=pi, W=W)\n    return gtr", "response": "T92 model for the base class of a base class"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _create_transversion_transition_W(kappa):\n    W = np.ones((4,4))\n    W[0, 2]=W[1, 3]=W[2, 0]=W[3,1]=kappa\n    return W", "response": "Create transition matrix for the transversion of a given kappa."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninitializing the merger model with a coalescent time.", "response": "def set_Tc(self, Tc, T=None):\n        '''\n        initialize the merger model with a coalescent time\n\n        Args:\n            - Tc:   a float or an iterable, if iterable another argument T of same shape is required\n            - T:    an array like of same shape as Tc that specifies the time pivots corresponding to Tc\n        Returns:\n            - None\n        '''\n        if isinstance(Tc, Iterable):\n            if len(Tc)==len(T):\n                x = np.concatenate(([-ttconf.BIG_NUMBER], T, [ttconf.BIG_NUMBER]))\n                y = np.concatenate(([Tc[0]], Tc, [Tc[-1]]))\n                self.Tc = interp1d(x,y)\n            else:\n                self.logger(\"need Tc values and Timepoints of equal length\",2,warn=True)\n                self.Tc = interp1d([-ttconf.BIG_NUMBER, ttconf.BIG_NUMBER], [1e-5, 1e-5])\n        else:\n            self.Tc = interp1d([-ttconf.BIG_NUMBER, ttconf.BIG_NUMBER],\n                               [Tc+ttconf.TINY_NUMBER, Tc+ttconf.TINY_NUMBER])\n        self.calc_integral_merger_rate()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef calc_branch_count(self):\n        '''\n        calculates an interpolation object that maps time to the number of\n        concurrent branches in the tree. The result is stored in self.nbranches\n        '''\n\n        # make a list of (time, merger or loss event) by root first iteration\n        self.tree_events = np.array(sorted([(n.time_before_present, len(n.clades)-1)\n                                for n in self.tree.find_clades() if not n.bad_branch],\n                                key=lambda x:-x[0]))\n\n        # collapse multiple events at one time point into sum of changes\n        from collections import defaultdict\n        dn_branch = defaultdict(int)\n        for (t, dn) in self.tree_events:\n            dn_branch[t]+=dn\n        unique_mergers = np.array(sorted(dn_branch.items(), key = lambda x:-x[0]))\n\n        # calculate the branch count at each point summing the delta branch counts\n        nbranches = [[ttconf.BIG_NUMBER, 1], [unique_mergers[0,0]+ttconf.TINY_NUMBER, 1]]\n        for ti, (t, dn) in enumerate(unique_mergers[:-1]):\n            new_n = nbranches[-1][1]+dn\n            next_t = unique_mergers[ti+1,0]+ttconf.TINY_NUMBER\n            nbranches.append([t, new_n])\n            nbranches.append([next_t, new_n])\n\n        new_n += unique_mergers[-1,1]\n        nbranches.append([next_t, new_n])\n        nbranches.append([-ttconf.BIG_NUMBER, new_n])\n        nbranches=np.array(nbranches)\n\n        self.nbranches = interp1d(nbranches[:,0], nbranches[:,1], kind='linear')", "response": "Calculates an interpolation object that maps time to the number of concurrent branches in the tree. The result is stored in self. nbranches."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the integral int_0^t ( k - 1 ) - 1)/2Tc ( t ) dt and stores it as self. integral_merger_rate.", "response": "def calc_integral_merger_rate(self):\n        '''\n        calculates the integral int_0^t (k(t')-1)/2Tc(t') dt' and stores it as\n        self.integral_merger_rate. This differences of this quantity evaluated at\n        different times points are the cost of a branch.\n        '''\n        # integrate the piecewise constant branch count function.\n        tvals = np.unique(self.nbranches.x[1:-1])\n        rate = self.branch_merger_rate(tvals)\n        avg_rate = 0.5*(rate[1:] + rate[:-1])\n        cost = np.concatenate(([0],np.cumsum(np.diff(tvals)*avg_rate)))\n        # make interpolation objects for the branch count and its integral\n        # the latter is scaled by 0.5/Tc\n        # need to add extra point at very large time before present to\n        # prevent 'out of interpolation range' errors\n        self.integral_merger_rate = interp1d(np.concatenate(([-ttconf.BIG_NUMBER], tvals,[ttconf.BIG_NUMBER])),\n                                  np.concatenate(([cost[0]], cost,[cost[-1]])), kind='linear')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the cost associated with a branch starting at t_node with a branch starting at t_node with a branch length of branch_length.", "response": "def cost(self, t_node, branch_length, multiplicity=2.0):\n        '''\n        returns the cost associated with a branch starting at t_node\n        t_node is time before present, the branch goes back in time\n\n        Args:\n            - t_node:           time of the node\n            - branch_length:    branch length, determines when this branch merges with sister\n            - multiplicity:     2 if merger is binary, higher if this is a polytomy\n        '''\n        merger_time = t_node+branch_length\n        return self.integral_merger_rate(merger_time) - self.integral_merger_rate(t_node)\\\n                 - np.log(self.total_merger_rate(merger_time))*(multiplicity-1.0)/multiplicity"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nattach the merger cost to each branch length interpolator in the tree.", "response": "def attach_to_tree(self):\n        '''\n        attaches the the merger cost to each branch length interpolator in the tree.\n        '''\n        for clade in self.tree.find_clades():\n            if clade.up is not None:\n                clade.branch_length_interpolator.merger_cost = self.cost"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef optimize_Tc(self):\n        '''\n        determines the coalescent time scale that optimizes the coalescent likelihood of the tree\n        '''\n        from scipy.optimize import minimize_scalar\n        initial_Tc = self.Tc\n        def cost(Tc):\n            self.set_Tc(Tc)\n            return -self.total_LH()\n\n        sol = minimize_scalar(cost, bounds=[ttconf.TINY_NUMBER,10.0])\n        if \"success\" in sol and sol[\"success\"]:\n            self.set_Tc(sol['x'])\n        else:\n            self.logger(\"merger_models:optimze_Tc: optimization of coalescent time scale failed: \" + str(sol), 0, warn=True)\n            self.set_Tc(initial_Tc.y, T=initial_Tc.x)", "response": "This function determines the coalescent time scale that optimizes the coalescent likelihood of the tree\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef optimize_skyline(self, n_points=20, stiffness=2.0, method = 'SLSQP',\n                         tol=0.03, regularization=10.0, **kwarks):\n        '''\n        optimize the trajectory of the merger rate 1./T_c to maximize the\n        coalescent likelihood.\n        parameters:\n            n_points    --  number of pivots of the Tc interpolation object\n            stiffness   --  penalty for rapid changes in log(Tc)\n            methods     --  method used to optimize\n            tol         --  optimization tolerance\n            regularization --  cost of moving logTc outsize of the range [-100,0]\n                               merger rate is measured in branch length units, no\n                               plausible rates should never be outside this window\n        '''\n        self.logger(\"Coalescent:optimize_skyline:... current LH: %f\"%self.total_LH(),2)\n        from scipy.optimize import minimize\n        initial_Tc = self.Tc\n        tvals = np.linspace(self.tree_events[0,0], self.tree_events[-1,0], n_points)\n        def cost(logTc):\n            # cap log Tc to avoid under or overflow and nan in logs\n            self.set_Tc(np.exp(np.maximum(-200,np.minimum(100,logTc))), tvals)\n            neglogLH = -self.total_LH() + stiffness*np.sum(np.diff(logTc)**2) \\\n                       + np.sum((logTc>0)*logTc*regularization)\\\n                       - np.sum((logTc<-100)*logTc*regularization)\n            return neglogLH\n\n        sol = minimize(cost, np.ones_like(tvals)*np.log(self.Tc.y.mean()), method=method, tol=tol)\n        if \"success\" in sol and sol[\"success\"]:\n            dlogTc = 0.1\n            opt_logTc = sol['x']\n            dcost = []\n            for ii in range(len(opt_logTc)):\n                tmp = opt_logTc.copy()\n                tmp[ii]+=dlogTc\n                cost_plus = cost(tmp)\n                tmp[ii]-=2*dlogTc\n                cost_minus = cost(tmp)\n                dcost.append([cost_minus, cost_plus])\n\n            dcost = np.array(dcost)\n            optimal_cost = cost(opt_logTc)\n            self.confidence = -dlogTc/(2*optimal_cost - dcost[:,0] - dcost[:,1])\n            self.logger(\"Coalescent:optimize_skyline:...done. new LH: %f\"%self.total_LH(),2)\n        else:\n            self.set_Tc(initial_Tc.y, T=initial_Tc.x)\n            self.logger(\"Coalescent:optimize_skyline:...failed:\"+str(sol),0, warn=True)", "response": "Optimize the trajectory of the merger rate 1. / T_c to maximize the coalescent likelihood."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef skyline_empirical(self, gen=1.0, n_points = 20):\n        '''\n        returns the skyline, i.e., an estimate of the inverse rate of coalesence.\n        Here, the skyline is estimated from a sliding window average of the observed\n        mergers, i.e., without reference to the coalescence likelihood.\n        parameters:\n            gen -- number of generations per year.\n        '''\n\n        mergers = self.tree_events[:,1]>0\n        merger_tvals = self.tree_events[mergers,0]\n        nlineages = self.nbranches(merger_tvals-ttconf.TINY_NUMBER)\n        expected_merger_density = nlineages*(nlineages-1)*0.5\n\n        nmergers = len(mergers)\n        et = merger_tvals\n        ev = 1.0/expected_merger_density\n        # reduce the window size if there are few events in the tree\n        if 2*n_points>len(expected_merger_density):\n            n_points = len(ev)//4\n\n        # smoothes with a sliding window over data points\n        avg = np.sum(ev)/np.abs(et[0]-et[-1])\n        dt = et[0]-et[-1]\n        mid_points = np.concatenate(([et[0]-0.5*(et[1]-et[0])],\n                                      0.5*(et[1:] + et[:-1]),\n                                     [et[-1]+0.5*(et[-1]-et[-2])]))\n\n        # this smoothes the ratio of expected and observed merger rate\n        self.Tc_inv = interp1d(mid_points[n_points:-n_points],\n                        [np.sum(ev[(et>=l)&(et<u)])/(u-l+dt/nmergers)\n                        for u,l in zip(mid_points[:-2*n_points],mid_points[2*n_points:])])\n\n        return interp1d(self.date2dist.to_numdate(self.Tc_inv.x), gen/self.date2dist.clock_rate/self.Tc_inv.y)", "response": "This method returns the inverse rate of coalesence of the coalesced tree entry point."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef skyline_inferred(self, gen=1.0, confidence=False):\n        '''\n        return the skyline, i.e., an estimate of the inverse rate of coalesence.\n        This function merely returns the merger rate self.Tc that was set or\n        estimated by other means. If it was determined using self.optimize_skyline,\n        the returned skyline will maximize the coalescent likelihood.\n        parameters:\n            gen -- number of generations per year. Unit of time is branch length,\n                   hence this needs to be the inverse substitution rate per generation\n            confidence -- False, or number of standard deviations of confidence intervals\n        '''\n        if len(self.Tc.x)<=2:\n            print(\"no skyline has been inferred, returning constant population size\")\n            return gen/self.date2dist.clock_rate*self.Tc.y[-1]\n\n        skyline = interp1d(self.date2dist.to_numdate(self.Tc.x[1:-1]), gen/self.date2dist.clock_rate*self.Tc.y[1:-1])\n        if confidence and hasattr(self, 'confidence'):\n            conf = [skyline.y*np.exp(-confidence*self.confidence), skyline.y*np.exp(confidence*self.confidence)]\n            return skyline, conf\n        else:\n            return skyline", "response": "return the skyline for the specified generation and confidence intervals"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a profile to sequence and normalize it across sites.", "response": "def prof2seq(profile, gtr, sample_from_prof=False, normalize=True):\n    \"\"\"\n    Convert profile to sequence and normalize profile across sites.\n\n    Parameters\n    ----------\n\n     profile : numpy 2D array\n        Profile. Shape of the profile should be (L x a), where L - sequence\n        length, a - alphabet size.\n\n     gtr : gtr.GTR\n        Instance of the GTR class to supply the sequence alphabet\n\n     collapse_prof : bool\n        Whether to convert the profile to the delta-function\n\n    Returns\n    -------\n     seq : numpy.array\n        Sequence as numpy array of length L\n\n     prof_values :  numpy.array\n        Values of the profile for the chosen sequence characters (length L)\n\n     idx : numpy.array\n        Indices chosen from profile as array of length L\n    \"\"\"\n\n    # normalize profile such that probabilities at each site sum to one\n    if normalize:\n        tmp_profile, pre=normalize_profile(profile, return_offset=False)\n    else:\n        tmp_profile = profile\n\n    # sample sequence according to the probabilities in the profile\n    # (sampling from cumulative distribution over the different states)\n    if sample_from_prof:\n        cumdis = tmp_profile.cumsum(axis=1).T\n        randnum = np.random.random(size=cumdis.shape[1])\n        idx = np.argmax(cumdis>=randnum, axis=0)\n    else:\n        idx = tmp_profile.argmax(axis=1)\n    seq = gtr.alphabet[idx]  # max LH over the alphabet\n\n    prof_values = tmp_profile[np.arange(tmp_profile.shape[0]), idx]\n\n    return seq, prof_values, idx"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a normalized version of a profile matrix", "response": "def normalize_profile(in_profile, log=False, return_offset = True):\n    \"\"\"return a normalized version of a profile matrix\n\n    Parameters\n    ----------\n    in_profile : np.array\n        shape Lxq, will be normalized to one across each row\n    log : bool, optional\n        treat the input as log probabilities\n    return_offset : bool, optional\n        return the log of the scale factor for each row\n\n    Returns\n    -------\n    tuple\n        normalized profile (fresh np object) and offset (if return_offset==True)\n    \"\"\"\n    if log:\n        tmp_prefactor = in_profile.max(axis=1)\n        tmp_prof = np.exp(in_profile.T - tmp_prefactor).T\n    else:\n        tmp_prefactor = 0.0\n        tmp_prof = in_profile\n\n    norm_vector = tmp_prof.sum(axis=1)\n    return (np.copy(np.einsum('ai,a->ai',tmp_prof,1.0/norm_vector)),\n            (np.log(norm_vector) + tmp_prefactor) if return_offset else None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprints log message msg to stdout.", "response": "def logger(self, msg, level, warn=False):\n        \"\"\"\n        Print log message *msg* to stdout.\n\n        Parameters\n        -----------\n\n         msg : str\n            String to print on the screen\n\n         level : int\n            Log-level. Only the messages with a level higher than the\n            current verbose level will be shown.\n\n         warn : bool\n            Warning flag. If True, the message will be displayed\n            regardless of its log-level.\n\n        \"\"\"\n        if level<self.verbose or (warn and level<=self.verbose):\n            dt = time.time() - self.t_start\n            outstr = '\\n' if level<2 else ''\n            outstr += format(dt, '4.2f')+'\\t'\n            outstr += level*'-'\n            outstr += msg\n            print(outstr, file=sys.stdout)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the GTR object for this object.", "response": "def gtr(self, value):\n        \"\"\"\n        Set a new GTR object\n\n        Parameters\n        -----------\n\n         value : GTR\n            the new GTR object\n        \"\"\"\n        if not (isinstance(value, GTR) or isinstance(value, GTR_site_specific)):\n            raise TypeError(\" GTR instance expected\")\n        self._gtr = value"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the GTR model of the TreeAnc class.", "response": "def set_gtr(self, in_gtr, **kwargs):\n        \"\"\"\n        Create new GTR model if needed, and set the model as an attribute of the\n        TreeAnc class\n\n        Parameters\n        -----------\n\n         in_gtr : str, GTR\n            The gtr model to be assigned. If string is passed,\n            it is taken as the name of a standard GTR model, and is\n            attempted to be created through :code:`GTR.standard()` interface. If a\n            GTR instance is passed, it is set directly .\n\n         **kwargs\n            Keyword arguments to construct the GTR model. If none are passed, defaults\n            are assumed.\n\n        \"\"\"\n        if isinstance(in_gtr, str):\n            self._gtr = GTR.standard(model=in_gtr, **kwargs)\n            self._gtr.logger = self.logger\n\n        elif isinstance(in_gtr, GTR) or isinstance(in_gtr, GTR_site_specific):\n            self._gtr = in_gtr\n            self._gtr.logger=self.logger\n        else:\n            self.logger(\"TreeAnc.gtr_setter: can't interpret GTR model\", 1, warn=True)\n            raise TypeError(\"Cannot set GTR model in TreeAnc class: GTR or \"\n                \"string expected\")\n\n        if self._gtr.ambiguous is None:\n            self.fill_overhangs=False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tree(self, in_tree):\n        '''\n        assigns a tree to the internal self._tree variable. The tree is either\n        loaded from file (if in_tree is str) or assigned (if in_tree is a Phylo.tree)\n        '''\n\n        from os.path import isfile\n        if isinstance(in_tree, Phylo.BaseTree.Tree):\n            self._tree = in_tree\n        elif type(in_tree) in string_types and isfile(in_tree):\n            try:\n                self._tree=Phylo.read(in_tree, 'newick')\n            except:\n                fmt = in_tree.split('.')[-1]\n                if fmt in ['nexus', 'nex']:\n                    self._tree=Phylo.read(in_tree, 'nexus')\n                else:\n                    self.logger('TreeAnc: could not load tree, format needs to be nexus or newick! input was '+str(in_tree),1)\n                    self._tree = None\n                    return ttconf.ERROR\n        else:\n            self.logger('TreeAnc: could not load tree! input was '+str(in_tree),0)\n            self._tree = None\n            return ttconf.ERROR\n\n        # remove all existing sequence attributes\n        for node in self._tree.find_clades():\n            if hasattr(node, \"sequence\"):\n                node.__delattr__(\"sequence\")\n            node.original_length = node.branch_length\n            node.mutation_length = node.branch_length\n        self.prepare_tree()\n        return ttconf.SUCCESS", "response": "This function assigns a tree to the internal _tree variable."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef aln(self,in_aln):\n        # load alignment from file if necessary\n        from os.path import isfile\n        from Bio.Align import MultipleSeqAlignment\n        self._aln = None\n        if in_aln is None:\n            return\n        elif isinstance(in_aln, MultipleSeqAlignment):\n            self._aln = in_aln\n        elif type(in_aln) in string_types and isfile(in_aln):\n            for fmt in ['fasta', 'phylip-relaxed', 'nexus']:\n                try:\n                    self._aln=AlignIO.read(in_aln, fmt)\n                    break\n                except:\n                    continue\n        elif type(in_aln) in [defaultdict, dict]:  #if is read in from VCF file\n            self._aln = in_aln\n            self.is_vcf = True\n\n        if self._aln is None:\n            self.logger(\"TreeAnc: loading alignment failed... \",1, warn=True)\n            return ttconf.ERROR\n\n        #Convert to uppercase here, rather than in _attach_sequences_to_nodes\n        #(which used to do it through seq2array in seq_utils.py)\n        #so that it is controlled by param convert_upper. This way for\n        #mugration (ancestral reconstruction of non-sequences), you can\n        #use upper- and lower case characters for discrete states!\n        if (not self.is_vcf) and self.convert_upper:\n            self._aln = MultipleSeqAlignment([seq.upper() for seq in self._aln])\n\n        if self.seq_len:\n            if self.is_vcf and self.seq_len!=len(self.ref):\n                self.logger(\"TreeAnc.aln: specified sequence length doesn't match reference length, ignoring sequence length.\", 1, warn=True)\n                self._seq_len = len(self.ref)\n            else:\n                self.logger(\"TreeAnc.aln: specified sequence length doesn't match alignment length. Treating difference as constant sites.\", 2, warn=True)\n                self.additional_constant_sites = max(0, self.seq_len - self.aln.get_alignment_length())\n        else:\n            if self.is_vcf:\n                self.seq_len = len(self.ref)\n            else:\n                self.seq_len = self.aln.get_alignment_length()\n\n        # check whether the alignment is consistent with a nucleotide alignment.\n        likely_alphabet = self._guess_alphabet()\n        from .seq_utils import alphabets\n        # if likely alignment is not nucleotide but the gtr alignment is, WARN\n        if likely_alphabet=='aa' and self.gtr.n_states==len(alphabets['nuc']) and np.all(self.gtr.alphabet==alphabets['nuc']):\n            self.logger('WARNING: small fraction of ACGT-N in alignment. Really a nucleotide alignment? if not, rerun with --aa', 1, warn=True)\n        # conversely, warn if alignment is consistent with nucleotide but gtr has a long alphabet\n        if likely_alphabet=='nuc' and self.gtr.n_states>10:\n            self.logger('WARNING: almost exclusively ACGT-N in alignment. Really a protein alignment?', 1, warn=True)\n\n        if hasattr(self, '_tree') and (self.tree is not None):\n            self._attach_sequences_to_nodes()\n        else:\n            self.logger(\"TreeAnc.aln: sequences not yet attached to tree\", 3, warn=True)", "response": "Reads in the alignment and sets tree - related parameters and attaches sequences\n        to the tree nodes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the length of the uncompressed sequence.", "response": "def seq_len(self,L):\n        \"\"\"set the length of the uncompressed sequence. its inverse 'one_mutation'\n        is frequently used as a general length scale. This can't be changed once\n        it is set.\n\n        Parameters\n        ----------\n        L : int\n            length of the sequence alignment\n        \"\"\"\n        if (not hasattr(self, '_seq_len')) or self._seq_len is None:\n            if L:\n                self._seq_len = int(L)\n        else:\n            self.logger(\"TreeAnc: one_mutation and sequence length can't be reset\",1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _attach_sequences_to_nodes(self):\n        '''\n        For each node of the tree, check whether there is a sequence available\n        in the alignment and assign this sequence as a character array\n        '''\n        failed_leaves= 0\n        if self.is_vcf:\n            # if alignment is specified as difference from ref\n            dic_aln = self.aln\n        else:\n            # if full alignment is specified\n            dic_aln = {k.name: seq2array(k.seq, fill_overhangs=self.fill_overhangs,\n                                                   ambiguous_character=self.gtr.ambiguous)\n                                for k in self.aln} #\n\n        # loop over leaves and assign multiplicities of leaves (e.g. number of identical reads)\n        for l in self.tree.get_terminals():\n            if l.name in self.seq_multiplicity:\n                l.count = self.seq_multiplicity[l.name]\n            else:\n                l.count = 1.0\n\n\n        # loop over tree, and assign sequences\n        for l in self.tree.find_clades():\n            if l.name in dic_aln:\n                l.sequence= dic_aln[l.name]\n            elif l.is_terminal():\n                self.logger(\"***WARNING: TreeAnc._attach_sequences_to_nodes: NO SEQUENCE FOR LEAF: %s\" % l.name, 0, warn=True)\n                failed_leaves += 1\n                l.sequence = seq2array(self.gtr.ambiguous*self.seq_len, fill_overhangs=self.fill_overhangs,\n                                                 ambiguous_character=self.gtr.ambiguous)\n                if failed_leaves > self.tree.count_terminals()/3:\n                    self.logger(\"ERROR: At least 30\\\\% terminal nodes cannot be assigned with a sequence!\\n\", 0, warn=True)\n                    self.logger(\"Are you sure the alignment belongs to the tree?\", 2, warn=True)\n                    break\n            else: # could not assign sequence for internal node - is OK\n                pass\n\n        if failed_leaves:\n            self.logger(\"***WARNING: TreeAnc: %d nodes don't have a matching sequence in the alignment.\"\n                        \" POSSIBLE ERROR.\"%failed_leaves, 0, warn=True)\n\n        # extend profile to contain additional unknown characters\n        self.extend_profile()\n        return self.make_reduced_alignment()", "response": "This function is used to assign sequences to nodes in the tree."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_reduced_alignment(self):\n\n        self.logger(\"TreeAnc: making reduced alignment...\", 1)\n\n        # bind positions in real sequence to that of the reduced (compressed) sequence\n        self.full_to_reduced_sequence_map = np.zeros(self.seq_len, dtype=int)\n\n        # bind position in reduced sequence to the array of positions in real (expanded) sequence\n        self.reduced_to_full_sequence_map = {}\n\n        #if is a dict, want to be efficient and not iterate over a bunch of const_sites\n        #so pre-load alignment_patterns with the location of const sites!\n        #and get the sites that we want to iterate over only!\n        if self.is_vcf:\n            tmp_reduced_aln, alignment_patterns, positions = self.process_alignment_dict()\n            seqNames = self.aln.keys() #store seqName order to put back on tree\n        elif self.reduce_alignment:\n            # transpose real alignment, for ease of iteration\n            alignment_patterns = {}\n            tmp_reduced_aln = []\n            # NOTE the order of tree traversal must be the same as below\n            # for assigning the cseq attributes to the nodes.\n            seqs = [n.sequence for n in self.tree.find_clades() if hasattr(n, 'sequence')]\n            if len(np.unique([len(x) for x in seqs]))>1:\n                self.logger(\"TreeAnc: Sequences differ in in length! ABORTING\",0, warn=True)\n                aln_transpose = None\n                return\n            else:\n                aln_transpose = np.array(seqs).T\n                positions = range(aln_transpose.shape[0])\n        else:\n            self.multiplicity = np.ones(self.seq_len, dtype=float)\n            self.full_to_reduced_sequence_map = np.arange(self.seq_len)\n            self.reduced_to_full_sequence_map = {p:np.array([p]) for p in np.arange(self.seq_len)}\n            for n in self.tree.find_clades():\n                if hasattr(n, 'sequence'):\n                    n.original_cseq = np.copy(n.sequence)\n                    n.cseq = np.copy(n.sequence)\n            return ttconf.SUCCESS\n\n        for pi in positions:\n            if self.is_vcf:\n                pattern = [ self.aln[k][pi] if pi in self.aln[k].keys()\n                            else self.ref[pi] for k,v in self.aln.items() ]\n            else:\n                pattern = aln_transpose[pi]\n\n            str_pat = \"\".join(pattern)\n            # if the column contains only one state and ambiguous nucleotides, replace\n            # those with the state in other strains right away\n            unique_letters = list(np.unique(pattern))\n            if hasattr(self.gtr, \"ambiguous\"):\n                if len(unique_letters)==2 and self.gtr.ambiguous in unique_letters:\n                    other = [c for c in unique_letters if c!=self.gtr.ambiguous][0]\n                    str_pat = str_pat.replace(self.gtr.ambiguous, other)\n                    unique_letters = [other]\n            # if there is a mutation in this column, give it its private pattern\n            # this is required when sampling mutations from reconstructed profiles.\n            # otherwise, all mutations corresponding to the same pattern will be coupled.\n            if len(unique_letters)>1:\n                str_pat += '_%d'%pi\n\n            # if the pattern is not yet seen,\n            if str_pat not in alignment_patterns:\n                # bind the index in the reduced aln, index in sequence to the pattern string\n                alignment_patterns[str_pat] = (len(tmp_reduced_aln), [pi])\n                # append this pattern to the reduced alignment\n                tmp_reduced_aln.append(pattern)\n            else:\n                # if the pattern is already seen, append the position in the real\n                # sequence to the reduced aln<->sequence_pos_indexes map\n                alignment_patterns[str_pat][1].append(pi)\n\n        # add constant alignment column not in the alignment. We don't know where they\n        # are, so just add them to the end. First, determine sequence composition.\n        if self.additional_constant_sites:\n            character_counts = {c:np.sum(aln_transpose==c) for c in self.gtr.alphabet\n                                if c not in [self.gtr.ambiguous, '-']}\n            total = np.sum(list(character_counts.values()))\n            additional_columns = [(c,int(np.round(self.additional_constant_sites*n/total)))\n                                  for c, n in character_counts.items()]\n            columns_left = self.additional_constant_sites\n            pi = len(positions)\n            for c,n in additional_columns:\n                if c==additional_columns[-1][0]:  # make sure all additions add up to the correct number to avoid rounding\n                    n = columns_left\n                str_pat = c*len(self.aln)\n                pos_list = list(range(pi, pi+n))\n\n                if str_pat in alignment_patterns:\n                    alignment_patterns[str_pat][1].extend(pos_list)\n                else:\n                    alignment_patterns[str_pat] = (len(tmp_reduced_aln), pos_list)\n                    tmp_reduced_aln.append(np.array(list(str_pat)))\n                pi += n\n                columns_left -= n\n\n\n        # count how many times each column is repeated in the real alignment\n        self.multiplicity = np.zeros(len(alignment_patterns))\n        for p, pos in alignment_patterns.values():\n            self.multiplicity[p]=len(pos)\n\n        # create the reduced alignment as np array\n        self.reduced_alignment = np.array(tmp_reduced_aln).T\n\n        # create map to compress a sequence\n        for p, pos in alignment_patterns.values():\n            self.full_to_reduced_sequence_map[np.array(pos)]=p\n\n        # create a map to reconstruct full sequence from the reduced (compressed) sequence\n        for p, val in alignment_patterns.items():\n            self.reduced_to_full_sequence_map[val[0]]=np.array(val[1], dtype=int)\n\n        # assign compressed sequences to all nodes of the tree, which have sequence assigned\n        # for dict we cannot assume this is in the same order, as it does below!\n        # so do it explicitly\n        #\n        # sequences are overwritten during reconstruction and\n        # ambiguous sites change. Keep orgininals for reference\n        if self.is_vcf:\n            seq_reduce_align = {n:self.reduced_alignment[i]\n                                for i, n in enumerate(seqNames)}\n            for n in self.tree.find_clades():\n                if hasattr(n, 'sequence'):\n                    n.original_cseq = seq_reduce_align[n.name]\n                    n.cseq = np.copy(n.original_cseq)\n        else:\n            # NOTE the order of tree traversal must be the same as above to catch the\n            # index in the reduced alignment correctly\n            seq_count = 0\n            for n in self.tree.find_clades():\n                if hasattr(n, 'sequence'):\n                    n.original_cseq = self.reduced_alignment[seq_count]\n                    n.cseq = np.copy(n.original_cseq)\n                    seq_count+=1\n\n        self.logger(\"TreeAnc: constructed reduced alignment...\", 1)\n        return ttconf.SUCCESS", "response": "Create the reduced alignment from the full sequences attached to the tree nodes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef process_alignment_dict(self):\n\n        # number of sequences in alignment\n        nseq = len(self.aln)\n\n        inv_map = defaultdict(list)\n        for k,v in self.aln.items():\n            for pos, bs in v.items():\n                inv_map[pos].append(bs)\n\n        self.nonref_positions = np.sort(list(inv_map.keys()))\n        self.inferred_const_sites = []\n\n        ambiguous_char = self.gtr.ambiguous\n        nonref_const = []\n        nonref_alleles = []\n        ambiguous_const = []\n        variable_pos = []\n        for pos, bs in inv_map.items(): #loop over positions and patterns\n            bases = \"\".join(np.unique(bs))\n            if len(bs) == nseq:\n                if (len(bases)<=2 and ambiguous_char in bases) or len(bases)==1:\n                    # all sequences different from reference, but only one state\n                    # (other than ambiguous_char) in column\n                    nonref_const.append(pos)\n                    nonref_alleles.append(bases.replace(ambiguous_char, ''))\n                    if ambiguous_char in bases: #keep track of sites 'made constant'\n                        self.inferred_const_sites.append(pos)\n                else:\n                    # at least two non-reference alleles\n                    variable_pos.append(pos)\n            else:\n                # not every sequence different from reference\n                if bases==ambiguous_char:\n                    ambiguous_const.append(pos)\n                    self.inferred_const_sites.append(pos) #keep track of sites 'made constant'\n                else:\n                    # at least one non ambiguous non-reference allele not in\n                    # every sequence\n                    variable_pos.append(pos)\n\n        refMod = np.array(list(self.ref))\n        # place constant non reference positions by their respective allele\n        refMod[nonref_const] = nonref_alleles\n        # mask variable positions\n        states = self.gtr.alphabet\n        # maybe states = np.unique(refMod)\n        refMod[variable_pos] = '.'\n\n        # for each base in the gtr, make constant alignment pattern and\n        # assign it to all const positions in the modified reference sequence\n        reduced_alignment_const = []\n        alignment_patterns_const = {}\n        for base in states:\n            p = base*nseq\n            pos = list(np.where(refMod==base)[0])\n            #if the alignment doesn't have a const site of this base, don't add! (ex: no '----' site!)\n            if len(pos):\n                alignment_patterns_const[p] = [len(reduced_alignment_const), pos]\n                reduced_alignment_const.append(list(p))\n\n\n        return reduced_alignment_const, alignment_patterns_const, variable_pos", "response": "process the alignment dictionary for non - variable postitions and non - reference alleles and non - reference positions"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef prepare_tree(self):\n        self.tree.root.branch_length = 0.001\n        self.tree.root.mutation_length = self.tree.root.branch_length\n        self.tree.root.mutations = []\n        self.tree.ladderize()\n        self._prepare_nodes()\n        self._leaves_lookup = {node.name:node for node in self.tree.get_terminals()}", "response": "Prepare the tree for the tree."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets auxilliary parameters to every node of the tree.", "response": "def _prepare_nodes(self):\n        \"\"\"\n        Set auxilliary parameters to every node of the tree.\n        \"\"\"\n        self.tree.root.up = None\n        self.tree.root.bad_branch=self.tree.root.bad_branch if hasattr(self.tree.root, 'bad_branch') else False\n        internal_node_count = 0\n        for clade in self.tree.get_nonterminals(order='preorder'): # parents first\n            internal_node_count+=1\n            if clade.name is None:\n                clade.name = \"NODE_\" + format(self._internal_node_count, '07d')\n                self._internal_node_count += 1\n            for c in clade.clades:\n                if c.is_terminal():\n                    c.bad_branch = c.bad_branch if hasattr(c, 'bad_branch') else False\n                c.up = clade\n\n        for clade in self.tree.get_nonterminals(order='postorder'): # parents first\n            clade.bad_branch = all([c.bad_branch for c in clade])\n\n        self._calc_dist2root()\n        self._internal_node_count = max(internal_node_count, self._internal_node_count)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _calc_dist2root(self):\n        self.tree.root.dist2root = 0.0\n        for clade in self.tree.get_nonterminals(order='preorder'): # parents first\n            for c in clade.clades:\n                if not hasattr(c, 'mutation_length'):\n                    c.mutation_length=c.branch_length\n                c.dist2root = c.up.dist2root + c.mutation_length", "response": "Calculates the distance between the root and the root node."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef infer_gtr(self, print_raw=False, marginal=False, normalized_rate=True,\n                  fixed_pi=None, pc=5.0, **kwargs):\n        \"\"\"\n        Calculates a GTR model given the multiple sequence alignment and the tree.\n        It performs ancestral sequence inferrence (joint or marginal), followed by\n        the branch lengths optimization. Then, the numbers of mutations are counted\n        in the optimal tree and related to the time within the mutation happened.\n        From these statistics, the relative state transition probabilities are inferred,\n        and the transition matrix is computed.\n\n        The result is used to construct the new GTR model of type 'custom'.\n        The model is assigned to the TreeAnc and is used in subsequent analysis.\n\n        Parameters\n        -----------\n\n         print_raw : bool\n            If True, print the inferred GTR model\n\n         marginal : bool\n            If True, use marginal sequence reconstruction\n\n         normalized_rate : bool\n            If True, sets the mutation rate prefactor to 1.0.\n\n         fixed_pi : np.array\n            Provide the equilibrium character concentrations.\n            If None is passed, the concentrations will be inferred from the alignment.\n\n         pc: float\n            Number of pseudo counts to use in gtr inference\n\n        Returns\n        -------\n\n         gtr : GTR\n            The inferred GTR model\n        \"\"\"\n\n        # decide which type of the Maximum-likelihood reconstruction use\n        # (marginal) or (joint)\n        if marginal:\n            _ml_anc = self._ml_anc_marginal\n        else:\n            _ml_anc = self._ml_anc_joint\n\n        self.logger(\"TreeAnc.infer_gtr: inferring the GTR model from the tree...\", 1)\n        if (self.tree is None) or (self.aln is None):\n            self.logger(\"TreeAnc.infer_gtr: ERROR, alignment or tree are missing\", 0)\n            return ttconf.ERROR\n\n        _ml_anc(final=True, **kwargs) # call one of the reconstruction types\n        alpha = list(self.gtr.alphabet)\n        n=len(alpha)\n        # matrix of mutations n_{ij}: i = derived state, j=ancestral state\n        nij = np.zeros((n,n))\n        Ti = np.zeros(n)\n\n        self.logger(\"TreeAnc.infer_gtr: counting mutations...\", 2)\n        for node in self.tree.find_clades():\n            if hasattr(node,'mutations'):\n                for a,pos, d in node.mutations:\n                    i,j = alpha.index(d), alpha.index(a)\n                    nij[i,j]+=1\n                    Ti[j] += 0.5*self._branch_length_to_gtr(node)\n                    Ti[i] -= 0.5*self._branch_length_to_gtr(node)\n                for ni,nuc in enumerate(node.cseq):\n                    i = alpha.index(nuc)\n                    Ti[i] += self._branch_length_to_gtr(node)*self.multiplicity[ni]\n        self.logger(\"TreeAnc.infer_gtr: counting mutations...done\", 3)\n        if print_raw:\n            print('alphabet:',alpha)\n            print('n_ij:', nij, nij.sum())\n            print('T_i:', Ti, Ti.sum())\n        root_state = np.array([np.sum((self.tree.root.cseq==nuc)*self.multiplicity) for nuc in alpha])\n\n        self._gtr = GTR.infer(nij, Ti, root_state, fixed_pi=fixed_pi, pc=pc,\n                              alphabet=self.gtr.alphabet, logger=self.logger,\n                              prof_map = self.gtr.profile_map)\n\n        if normalized_rate:\n            self.logger(\"TreeAnc.infer_gtr: setting overall rate to 1.0...\", 2)\n            self._gtr.mu=1.0\n        return self._gtr", "response": "This function calculates the GTR model from the tree and the tree s alignment and the tree s tree and the tree s pseudo counts."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreconstructs the ancestral sequences for a particular tree and anomaly level.", "response": "def reconstruct_anc(self, method='probabilistic', infer_gtr=False,\n                        marginal=False, **kwargs):\n        \"\"\"Reconstruct ancestral sequences\n\n        Parameters\n        ----------\n        method : str\n           Method to use. Supported values are \"fitch\" and \"ml\"\n\n        infer_gtr : bool\n           Infer a GTR model before reconstructing the sequences\n\n        marginal : bool\n           Assign sequences that are most likely after averaging over all other nodes\n           instead of the jointly most likely sequences.\n        **kwargs\n            additional keyword arguments that are passed down to :py:meth:`TreeAnc.infer_gtr` and :py:meth:`TreeAnc._ml_anc`\n\n        Returns\n        -------\n        N_diff : int\n           Number of nucleotides different from the previous\n           reconstruction.  If there were no pre-set sequences, returns N*L\n\n        \"\"\"\n        self.logger(\"TreeAnc.infer_ancestral_sequences with method: %s, %s\"%(method, 'marginal' if marginal else 'joint'), 1)\n        if (self.tree is None) or (self.aln is None):\n            self.logger(\"TreeAnc.infer_ancestral_sequences: ERROR, alignment or tree are missing\", 0)\n            return ttconf.ERROR\n\n        if method in ['ml', 'probabilistic']:\n            if marginal:\n                _ml_anc = self._ml_anc_marginal\n            else:\n                _ml_anc = self._ml_anc_joint\n        else:\n            _ml_anc = self._fitch_anc\n\n        if infer_gtr:\n            tmp = self.infer_gtr(marginal=marginal, **kwargs)\n            if tmp==ttconf.ERROR:\n                return tmp\n            N_diff = _ml_anc(**kwargs)\n        else:\n            N_diff = _ml_anc(**kwargs)\n\n        return N_diff"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the mutations on a tree branch.", "response": "def get_mutations(self, node, keep_var_ambigs=False):\n        \"\"\"\n        Get the mutations on a tree branch. Take compressed sequences from both sides\n        of the branch (attached to the node), compute mutations between them, and\n        expand these mutations to the positions in the real sequences.\n\n        Parameters\n        ----------\n        node : PhyloTree.Clade\n           Tree node, which is the child node attached to the branch.\n\n        keep_var_ambigs : boolean\n           If true, generates mutations based on the *original* compressed sequence, which\n           may include ambiguities. Note sites that only have 1 unambiguous base and ambiguous\n           bases (\"AAAAANN\") are stripped of ambiguous bases *before* compression, so ambiguous\n           bases will **not** be preserved.\n\n        Returns\n        -------\n        muts : list\n          List of mutations. Each mutation is represented as tuple of\n          :code:`(parent_state, position, child_state)`.\n        \"\"\"\n\n        # if ambiguous site are to be restored and node is terminal,\n        # assign original sequence, else reconstructed cseq\n        node_seq = node.cseq\n        if keep_var_ambigs and hasattr(node, \"original_cseq\") and node.is_terminal():\n            node_seq = node.original_cseq\n\n        muts = []\n        diff_pos = np.where(node.up.cseq!=node_seq)[0]\n        for p in diff_pos:\n            anc = node.up.cseq[p]\n            der = node_seq[p]\n            # expand to the positions in real sequence\n            muts.extend([(anc, pos, der) for pos in self.reduced_to_full_sequence_map[p]])\n\n        #sort by position\n        return sorted(muts, key=lambda x:x[1])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_branch_mutation_matrix(self, node, full_sequence=False):\n        pp,pc = self.marginal_branch_profile(node)\n\n        # calculate pc_i [e^Qt]_ij pp_j for each site\n        expQt = self.gtr.expQt(self._branch_length_to_gtr(node))\n        if len(expQt.shape)==3: # site specific model\n            mut_matrix_stack = np.einsum('ai,aj,ija->aij', pc, pp, expQt)\n        else:\n            mut_matrix_stack = np.einsum('ai,aj,ij->aij', pc, pp, expQt)\n\n        # normalize this distribution\n        normalizer = mut_matrix_stack.sum(axis=2).sum(axis=1)\n        mut_matrix_stack = np.einsum('aij,a->aij', mut_matrix_stack, 1.0/normalizer)\n\n        # expand to full sequence if requested\n        if full_sequence:\n            return mut_matrix_stack[self.full_to_reduced_sequence_map]\n        else:\n            return mut_matrix_stack", "response": "uses results from marginal ancestral inference to return a joint mutation matrix of the sequence states at both ends of the branch."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef expanded_sequence(self, node, include_additional_constant_sites=False):\n        if include_additional_constant_sites:\n            L = self.seq_len\n        else:\n            L = self.seq_len - self.additional_constant_sites\n\n        return node.cseq[self.full_to_reduced_sequence_map[:L]]", "response": "Expand a nodes compressed sequence into the real sequence"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dict_sequence(self, node, keep_var_ambigs=False):\n        seq = {}\n\n        node_seq = node.cseq\n        if keep_var_ambigs and hasattr(node, \"original_cseq\") and node.is_terminal():\n            node_seq = node.original_cseq\n\n        for pos in self.nonref_positions:\n            cseqLoc = self.full_to_reduced_sequence_map[pos]\n            base = node_seq[cseqLoc]\n            if self.ref[pos] != base:\n                seq[pos] = base\n\n        return seq", "response": "Returns a dict of all the basepairs of the node s variants and their positions."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _fitch_anc(self, **kwargs):\n        # set fitch profiiles to each terminal node\n\n        for l in self.tree.get_terminals():\n            l.state = [[k] for k in l.cseq]\n\n        L = len(self.tree.get_terminals()[0].cseq)\n\n        self.logger(\"TreeAnc._fitch_anc: Walking up the tree, creating the Fitch profiles\",2)\n        for node in self.tree.get_nonterminals(order='postorder'):\n            node.state = [self._fitch_state(node, k) for k in range(L)]\n\n        ambs = [i for i in range(L) if len(self.tree.root.state[i])>1]\n        if len(ambs) > 0:\n            for amb in ambs:\n                self.logger(\"Ambiguous state of the root sequence \"\n                                    \"in the position %d: %s, \"\n                                    \"choosing %s\" % (amb, str(self.tree.root.state[amb]),\n                                                     self.tree.root.state[amb][0]), 4)\n        self.tree.root.cseq = np.array([k[np.random.randint(len(k)) if len(k)>1 else 0]\n                                           for k in self.tree.root.state])\n\n        if self.is_vcf:\n            self.tree.root.sequence = self.dict_sequence(self.tree.root)\n        else:\n            self.tree.root.sequence = self.expanded_sequence(self.tree.root)\n\n\n        self.logger(\"TreeAnc._fitch_anc: Walking down the self.tree, generating sequences from the \"\n                         \"Fitch profiles.\", 2)\n        N_diff = 0\n        for node in self.tree.get_nonterminals(order='preorder'):\n            if node.up != None: # not root\n                sequence =  np.array([node.up.cseq[i]\n                        if node.up.cseq[i] in node.state[i]\n                        else node.state[i][0] for i in range(L)])\n                if hasattr(node, 'sequence'):\n                    N_diff += (sequence!=node.cseq).sum()\n                else:\n                    N_diff += L\n                node.cseq = sequence\n                if self.is_vcf:\n                    node.sequence = self.dict_sequence(node)\n                else:\n                    node.sequence = self.expanded_sequence(node)\n                node.mutations = self.get_mutations(node)\n\n            node.profile = seq2prof(node.cseq, self.gtr.profile_map)\n            del node.state # no need to store Fitch states\n        self.logger(\"Done ancestral state reconstruction\",3)\n        for node in self.tree.get_terminals():\n            node.profile = seq2prof(node.original_cseq, self.gtr.profile_map)\n        return N_diff", "response": "This method reconstructs the ancestral states using Fitch s algorithm."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _fitch_state(self, node, pos):\n        state = self._fitch_intersect([k.state[pos] for k in node.clades])\n        if len(state) == 0:\n            state = np.concatenate([k.state[pos] for k in node.clades])\n        return state", "response": "Returns the Fitch state for a single character of the given node at the given position."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds the intersection of any number of 1D arrays.", "response": "def _fitch_intersect(self, arrays):\n        \"\"\"\n        Find the intersection of any number of 1D arrays.\n        Return the sorted, unique values that are in all of the input arrays.\n        Adapted from numpy.lib.arraysetops.intersect1d\n        \"\"\"\n        def pairwise_intersect(arr1, arr2):\n            s2 = set(arr2)\n            b3 = [val for val in arr1 if val in s2]\n            return b3\n\n        arrays = list(arrays) # allow assignment\n        N = len(arrays)\n        while N > 1:\n            arr1 = arrays.pop()\n            arr2 = arrays.pop()\n            arr = pairwise_intersect(arr1, arr2)\n            arrays.append(arr)\n            N = len(arrays)\n\n        return arrays[0]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sequence_LH(self, pos=None, full_sequence=False):\n        if not hasattr(self.tree, \"total_sequence_LH\"):\n            self.logger(\"TreeAnc.sequence_LH: you need to run marginal ancestral inference first!\", 1)\n            self.infer_ancestral_sequences(marginal=True)\n        if pos is not None:\n            if full_sequence:\n                compressed_pos = self.full_to_reduced_sequence_map[pos]\n            else:\n                compressed_pos = pos\n            return self.tree.sequence_LH[compressed_pos]\n        else:\n            return self.tree.total_sequence_LH", "response": "return the likelihood of the observed sequences given the tree"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the likelihood of the given realization of the sequences in the tree returns the likelihood of the given realization of the sequences in the tree", "response": "def ancestral_likelihood(self):\n        \"\"\"\n        Calculate the likelihood of the given realization of the sequences in\n        the tree\n\n        Returns\n        -------\n\n         log_lh : float\n            The tree likelihood given the sequences\n        \"\"\"\n        log_lh = np.zeros(self.multiplicity.shape[0])\n        for node in self.tree.find_clades(order='postorder'):\n\n            if node.up is None: #  root node\n                # 0-1 profile\n                profile = seq2prof(node.cseq, self.gtr.profile_map)\n                # get the probabilities to observe each nucleotide\n                profile *= self.gtr.Pi\n                profile = profile.sum(axis=1)\n                log_lh += np.log(profile) # product over all characters\n                continue\n\n            t = node.branch_length\n\n            indices = np.array([(np.argmax(self.gtr.alphabet==a),\n                        np.argmax(self.gtr.alphabet==b)) for a, b in zip(node.up.cseq, node.cseq)])\n\n            logQt = np.log(self.gtr.expQt(t))\n            lh = logQt[indices[:, 1], indices[:, 0]]\n            log_lh += lh\n\n        return log_lh"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the branch lengths to either mutation lengths of given branch lengths.", "response": "def _branch_length_to_gtr(self, node):\n        \"\"\"\n        Set branch lengths to either mutation lengths of given branch lengths.\n        The assigend values are to be used in the following ML analysis.\n        \"\"\"\n        if self.use_mutation_length:\n            return max(ttconf.MIN_BRANCH_LENGTH*self.one_mutation, node.mutation_length)\n        else:\n            return max(ttconf.MIN_BRANCH_LENGTH*self.one_mutation, node.branch_length)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nperform marginal ML reconstruction of the ancestral states. In contrast to joint reconstructions, this needs to access the probabilities rather than only log probabilities and is hence handled by a separate function. Parameters ---------- store_compressed : bool, default True attach a reduced representation of sequence changed to each branch final : bool, default True stop full length by expanding sites with identical alignment patterns sample_from_profile : bool or str assign sequences probabilistically according to the inferred probabilities of ancestral states instead of to their ML value. This parameter can also take the value 'root' in which case probabilistic sampling will happen at the root but at no other node.", "response": "def _ml_anc_marginal(self, store_compressed=False, final=True, sample_from_profile=False,\n                         debug=False, **kwargs):\n        \"\"\"\n        Perform marginal ML reconstruction of the ancestral states. In contrast to\n        joint reconstructions, this needs to access the probabilities rather than only\n        log probabilities and is hence handled by a separate function.\n\n        Parameters\n        ----------\n\n         store_compressed : bool, default True\n            attach a reduced representation of sequence changed to each branch\n\n         final : bool, default True\n            stop full length by expanding sites with identical alignment patterns\n\n         sample_from_profile : bool or str\n            assign sequences probabilistically according to the inferred probabilities\n            of ancestral states instead of to their ML value. This parameter can also\n            take the value 'root' in which case probabilistic sampling will happen\n            at the root but at no other node.\n\n        \"\"\"\n\n        tree = self.tree\n        # number of nucleotides changed from prev reconstruction\n        N_diff = 0\n\n        L = self.multiplicity.shape[0]\n        n_states = self.gtr.alphabet.shape[0]\n        self.logger(\"TreeAnc._ml_anc_marginal: type of reconstruction: Marginal\", 2)\n\n        self.logger(\"Attaching sequence profiles to leafs... \", 3)\n        #  set the leaves profiles\n        for leaf in tree.get_terminals():\n            # in any case, set the profile\n            leaf.marginal_subtree_LH = seq2prof(leaf.original_cseq, self.gtr.profile_map)\n            leaf.marginal_subtree_LH_prefactor = np.zeros(L)\n\n        self.logger(\"Walking up the tree, computing likelihoods... \", 3)\n        # propagate leaves --> root, set the marginal-likelihood messages\n        for node in tree.get_nonterminals(order='postorder'): #leaves -> root\n            # regardless of what was before, set the profile to ones\n            tmp_log_subtree_LH = np.zeros((L,n_states), dtype=float)\n            node.marginal_subtree_LH_prefactor = np.zeros(L, dtype=float)\n            for ch in node.clades:\n                ch.marginal_log_Lx = self.gtr.propagate_profile(ch.marginal_subtree_LH,\n                    self._branch_length_to_gtr(ch), return_log=True) # raw prob to transfer prob up\n                tmp_log_subtree_LH += ch.marginal_log_Lx\n                node.marginal_subtree_LH_prefactor += ch.marginal_subtree_LH_prefactor\n\n            node.marginal_subtree_LH, offset = normalize_profile(tmp_log_subtree_LH, log=True)\n            node.marginal_subtree_LH_prefactor += offset # and store log-prefactor\n\n        self.logger(\"Computing root node sequence and total tree likelihood...\",3)\n        # Msg to the root from the distant part (equ frequencies)\n        if len(self.gtr.Pi.shape)==1:\n            tree.root.marginal_outgroup_LH = np.repeat([self.gtr.Pi], tree.root.marginal_subtree_LH.shape[0], axis=0)\n        else:\n            tree.root.marginal_outgroup_LH = np.copy(self.gtr.Pi.T)\n\n        tree.root.marginal_profile, pre = normalize_profile(tree.root.marginal_outgroup_LH*tree.root.marginal_subtree_LH)\n        marginal_LH_prefactor = tree.root.marginal_subtree_LH_prefactor + pre\n\n        # choose sequence characters from this profile.\n        # treat root node differently to avoid piling up mutations on the longer branch\n        if sample_from_profile=='root':\n            root_sample_from_profile = True\n            other_sample_from_profile = False\n        elif isinstance(sample_from_profile, bool):\n            root_sample_from_profile = sample_from_profile\n            other_sample_from_profile = sample_from_profile\n\n        seq, prof_vals, idxs = prof2seq(tree.root.marginal_profile,\n                                        self.gtr, sample_from_prof=root_sample_from_profile, normalize=False)\n\n        self.tree.sequence_LH = marginal_LH_prefactor\n        self.tree.total_sequence_LH = (self.tree.sequence_LH*self.multiplicity).sum()\n        self.tree.root.cseq = seq\n        gc.collect()\n\n        if final:\n            if self.is_vcf:\n                self.tree.root.sequence = self.dict_sequence(self.tree.root)\n            else:\n                self.tree.root.sequence = self.expanded_sequence(self.tree.root)\n\n        self.logger(\"Walking down the tree, computing maximum likelihood sequences...\",3)\n        # propagate root -->> leaves, reconstruct the internal node sequences\n        # provided the upstream message + the message from the complementary subtree\n        for node in tree.find_clades(order='preorder'):\n            if node.up is None: # skip if node is root\n                continue\n\n            # integrate the information coming from parents with the information\n            # of all children my multiplying it to the prev computed profile\n            node.marginal_outgroup_LH, pre = normalize_profile(np.log(node.up.marginal_profile) - node.marginal_log_Lx,\n                                                               log=True, return_offset=False)\n            tmp_msg_from_parent = self.gtr.evolve(node.marginal_outgroup_LH,\n                                                 self._branch_length_to_gtr(node), return_log=False)\n\n            node.marginal_profile, pre = normalize_profile(node.marginal_subtree_LH * tmp_msg_from_parent, return_offset=False)\n\n            # choose sequence based maximal marginal LH.\n            seq, prof_vals, idxs = prof2seq(node.marginal_profile, self.gtr,\n                                            sample_from_prof=other_sample_from_profile, normalize=False)\n\n            if hasattr(node, 'cseq') and node.cseq is not None:\n                N_diff += (seq!=node.cseq).sum()\n            else:\n                N_diff += L\n\n            #assign new sequence\n            node.cseq = seq\n            if final:\n                if self.is_vcf:\n                    node.sequence = self.dict_sequence(node)\n                else:\n                    node.sequence = self.expanded_sequence(node)\n                node.mutations = self.get_mutations(node)\n\n\n        # note that the root doesn't contribute to N_diff (intended, since root sequence is often ambiguous)\n        self.logger(\"TreeAnc._ml_anc_marginal: ...done\", 3)\n        if store_compressed:\n            self._store_compressed_sequence_pairs()\n\n        # do clean-up:\n        if not debug:\n            for node in self.tree.find_clades():\n                try:\n                    del node.marginal_log_Lx\n                    del node.marginal_subtree_LH_prefactor\n                except:\n                    pass\n        gc.collect()\n\n        return N_diff"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _ml_anc_joint(self, store_compressed=True, final=True, sample_from_profile=False,\n                            debug=False, **kwargs):\n\n        \"\"\"\n        Perform joint ML reconstruction of the ancestral states. In contrast to\n        marginal reconstructions, this only needs to compare and multiply LH and\n        can hence operate in log space.\n\n        Parameters\n        ----------\n\n         store_compressed : bool, default True\n            attach a reduced representation of sequence changed to each branch\n\n         final : bool, default True\n            stop full length by expanding sites with identical alignment patterns\n\n         sample_from_profile : str\n            This parameter can take the value 'root' in which case probabilistic\n            sampling will happen at the root. otherwise sequences at ALL nodes are\n            set to the value that jointly optimized the likelihood.\n\n        \"\"\"\n        N_diff = 0 # number of sites differ from perv reconstruction\n        L = self.multiplicity.shape[0]\n        n_states = self.gtr.alphabet.shape[0]\n\n        self.logger(\"TreeAnc._ml_anc_joint: type of reconstruction: Joint\", 2)\n\n        self.logger(\"TreeAnc._ml_anc_joint: Walking up the tree, computing likelihoods... \", 3)\n        # for the internal nodes, scan over all states j of this node, maximize the likelihood\n        for node in self.tree.find_clades(order='postorder'):\n            if node.up is None:\n                node.joint_Cx=None # not needed for root\n                continue\n\n            # preallocate storage\n            node.joint_Lx = np.zeros((L, n_states))             # likelihood array\n            node.joint_Cx = np.zeros((L, n_states), dtype=int)  # max LH indices\n            branch_len = self._branch_length_to_gtr(node)\n            # transition matrix from parent states to the current node states.\n            # denoted as Pij(i), where j - parent state, i - node state\n            log_transitions = np.log(np.maximum(ttconf.TINY_NUMBER, self.gtr.expQt(branch_len)))\n            if node.is_terminal():\n                try:\n                    msg_from_children = np.log(np.maximum(seq2prof(node.original_cseq, self.gtr.profile_map), ttconf.TINY_NUMBER))\n                except:\n                    raise ValueError(\"sequence assignment to node \"+node.name+\" failed\")\n                msg_from_children[np.isnan(msg_from_children) | np.isinf(msg_from_children)] = -ttconf.BIG_NUMBER\n            else:\n                # Product (sum-Log) over all child subtree likelihoods.\n                # this is prod_ch L_x(i)\n                msg_from_children = np.sum(np.stack([c.joint_Lx for c in node.clades], axis=0), axis=0)\n\n            # for every possible state of the parent node,\n            # get the best state of the current node\n            # and compute the likelihood of this state\n            for char_i, char in enumerate(self.gtr.alphabet):\n                # Pij(i) * L_ch(i) for given parent state j\n                msg_to_parent = (log_transitions[:,char_i].T + msg_from_children)\n                # For this parent state, choose the best state of the current node:\n                node.joint_Cx[:, char_i] = msg_to_parent.argmax(axis=1)\n                # compute the likelihood of the best state of the current node\n                # given the state of the parent (char_i)\n                node.joint_Lx[:, char_i] = msg_to_parent.max(axis=1)\n\n        # root node profile = likelihood of the total tree\n        msg_from_children = np.sum(np.stack([c.joint_Lx for c in self.tree.root], axis = 0), axis=0)\n        # Pi(i) * Prod_ch Lch(i)\n        self.tree.root.joint_Lx = msg_from_children + np.log(self.gtr.Pi).T\n        normalized_profile = (self.tree.root.joint_Lx.T - self.tree.root.joint_Lx.max(axis=1)).T\n\n        # choose sequence characters from this profile.\n        # treat root node differently to avoid piling up mutations on the longer branch\n        if sample_from_profile=='root':\n            root_sample_from_profile = True\n        elif isinstance(sample_from_profile, bool):\n            root_sample_from_profile = sample_from_profile\n\n        seq, anc_lh_vals, idxs = prof2seq(np.exp(normalized_profile),\n                                    self.gtr, sample_from_prof = root_sample_from_profile)\n\n        # compute the likelihood of the most probable root sequence\n        self.tree.sequence_LH = np.choose(idxs, self.tree.root.joint_Lx.T)\n        self.tree.sequence_joint_LH = (self.tree.sequence_LH*self.multiplicity).sum()\n        self.tree.root.cseq = seq\n        self.tree.root.seq_idx = idxs\n        if final:\n            if self.is_vcf:\n                self.tree.root.sequence = self.dict_sequence(self.tree.root)\n            else:\n                self.tree.root.sequence = self.expanded_sequence(self.tree.root)\n\n        self.logger(\"TreeAnc._ml_anc_joint: Walking down the tree, computing maximum likelihood sequences...\",3)\n        # for each node, resolve the conditioning on the parent node\n        for node in self.tree.find_clades(order='preorder'):\n\n            # root node has no mutations, everything else has been alread y set\n            if node.up is None:\n                node.mutations = []\n                continue\n\n            # choose the value of the Cx(i), corresponding to the state of the\n            # parent node i. This is the state of the current node\n            node.seq_idx = np.choose(node.up.seq_idx, node.joint_Cx.T)\n            # reconstruct seq, etc\n            tmp_sequence = np.choose(node.seq_idx, self.gtr.alphabet)\n            if hasattr(node, 'sequence') and node.cseq is not None:\n                N_diff += (tmp_sequence!=node.cseq).sum()\n            else:\n                N_diff += L\n\n            node.cseq = tmp_sequence\n            if final:\n                node.mutations = self.get_mutations(node)\n                if self.is_vcf:\n                    node.sequence = self.dict_sequence(node)\n                else:\n                    node.sequence = self.expanded_sequence(node)\n\n\n        self.logger(\"TreeAnc._ml_anc_joint: ...done\", 3)\n        if store_compressed:\n            self._store_compressed_sequence_pairs()\n\n        # do clean-up\n        if not debug:\n            for node in self.tree.find_clades(order='preorder'):\n                del node.joint_Lx\n                del node.joint_Cx\n                del node.seq_idx\n\n        return N_diff", "response": "Compute joint ML reconstruction of the ancestral states."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _store_compressed_sequence_pairs(self):\n        self.logger(\"TreeAnc._store_compressed_sequence_pairs...\",2)\n        for node in self.tree.find_clades():\n            if node.up is None:\n                continue\n            self._store_compressed_sequence_to_node(node)\n        self.logger(\"TreeAnc._store_compressed_sequence_pairs...done\",3)", "response": "Store the compressed sequence pairs in the tree."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming optimization for the branch lengths of the entire tree. This method only does a single path and needs to be iterated. **Note** this method assumes that each node stores information about its sequence as numpy.array object (node.sequence attribute). Therefore, before calling this method, sequence reconstruction with either of the available models must be performed. Parameters ---------- mode : str Optimize branch length assuming the joint ML sequence assignment of both ends of the branch (:code:`joint`), or trace over all possible sequence assignments on both ends of the branch (:code:`marginal`) (slower, experimental). **kwargs : Keyword arguments Keyword Args ------------ verbose : int Output level store_old : bool If True, the old lengths will be saved in :code:`node._old_dist` attribute. Useful for testing, and special post-processing.", "response": "def optimize_branch_length(self, mode='joint', **kwargs):\n        \"\"\"\n        Perform optimization for the branch lengths of the entire tree.\n        This method only does a single path and needs to be iterated.\n\n        **Note** this method assumes that each node stores information\n        about its sequence as numpy.array object (node.sequence attribute).\n        Therefore, before calling this method, sequence reconstruction with\n        either of the available models must be performed.\n\n        Parameters\n        ----------\n\n         mode : str\n            Optimize branch length assuming the joint ML sequence assignment\n            of both ends of the branch (:code:`joint`), or trace over all possible sequence\n            assignments on both ends of the branch (:code:`marginal`) (slower, experimental).\n\n         **kwargs :\n            Keyword arguments\n\n        Keyword Args\n        ------------\n\n         verbose : int\n            Output level\n\n         store_old : bool\n            If True, the old lengths will be saved in :code:`node._old_dist` attribute.\n            Useful for testing, and special post-processing.\n\n\n        \"\"\"\n\n        self.logger(\"TreeAnc.optimize_branch_length: running branch length optimization in mode %s...\"%mode,1)\n        if (self.tree is None) or (self.aln is None):\n            self.logger(\"TreeAnc.optimize_branch_length: ERROR, alignment or tree are missing\", 0)\n            return ttconf.ERROR\n\n        store_old_dist = False\n\n        if 'store_old' in kwargs:\n            store_old_dist = kwargs['store_old']\n\n        if mode=='marginal':\n            # a marginal ancestral reconstruction is required for\n            # marginal branch length inference\n            if not hasattr(self.tree.root, \"marginal_profile\"):\n                self.infer_ancestral_sequences(marginal=True)\n\n        max_bl = 0\n        for node in self.tree.find_clades(order='postorder'):\n            if node.up is None: continue # this is the root\n            if store_old_dist:\n                node._old_length = node.branch_length\n\n            if mode=='marginal':\n                new_len = self.optimal_marginal_branch_length(node)\n            elif mode=='joint':\n                new_len = self.optimal_branch_length(node)\n            else:\n                self.logger(\"treeanc.optimize_branch_length: unsupported optimization mode\",4, warn=True)\n                new_len = node.branch_length\n\n            if new_len < 0:\n                continue\n\n            self.logger(\"Optimization results: old_len=%.4e, new_len=%.4e, naive=%.4e\"\n                   \" Updating branch length...\"%(node.branch_length, new_len, len(node.mutations)*self.one_mutation), 5)\n\n            node.branch_length = new_len\n            node.mutation_length=new_len\n            max_bl = max(max_bl, new_len)\n\n        # as branch lengths changed, the params must be fixed\n        self.tree.root.up = None\n        self.tree.root.dist2root = 0.0\n        if max_bl>0.15 and mode=='joint':\n            self.logger(\"TreeAnc.optimize_branch_length: THIS TREE HAS LONG BRANCHES.\"\n                        \" \\n\\t ****TreeTime IS NOT DESIGNED TO OPTIMIZE LONG BRANCHES.\"\n                        \" \\n\\t ****PLEASE OPTIMIZE BRANCHES WITH ANOTHER TOOL AND RERUN WITH\"\n                        \" \\n\\t ****branch_length_mode='input'\", 0, warn=True)\n        self._prepare_nodes()\n        return ttconf.SUCCESS"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef optimal_branch_length(self, node):\n        '''\n        Calculate optimal branch length given the sequences of node and parent\n\n        Parameters\n        ----------\n        node : PhyloTree.Clade\n           TreeNode, attached to the branch.\n\n        Returns\n        -------\n        new_len : float\n           Optimal length of the given branch\n\n        '''\n        if node.up is None:\n            return self.one_mutation\n\n        parent = node.up\n        if hasattr(node, 'compressed_sequence'):\n            new_len = self.gtr.optimal_t_compressed(node.compressed_sequence['pair'],\n                                                    node.compressed_sequence['multiplicity'])\n        else:\n            new_len = self.gtr.optimal_t(parent.cseq, node.cseq,\n                                         pattern_multiplicity=self.multiplicity,\n                                         ignore_gaps=self.ignore_gaps)\n        return new_len", "response": "Calculate the optimal branch length given the sequences of node and parent."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef marginal_branch_profile(self, node):\n        '''\n        calculate the marginal distribution of sequence states on both ends\n        of the branch leading to node,\n\n        Parameters\n        ----------\n        node : PhyloTree.Clade\n           TreeNode, attached to the branch.\n\n\n        Returns\n        -------\n        pp, pc : Pair of vectors (profile parent, pp) and (profile child, pc)\n           that are of shape (L,n) where L is sequence length and n is alphabet size.\n           note that this correspond to the compressed sequences.\n        '''\n        parent = node.up\n        if parent is None:\n            raise Exception(\"Branch profiles can't be calculated for the root!\")\n        if not hasattr(node, 'marginal_outgroup_LH'):\n            raise Exception(\"marginal ancestral inference needs to be performed first!\")\n\n        pc = node.marginal_subtree_LH\n        pp = node.marginal_outgroup_LH\n        return pp, pc", "response": "Calculates the marginal distribution of sequence states on both ends of the branch leading to node and returns the parent profile and child profile."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef optimal_marginal_branch_length(self, node, tol=1e-10):\n        '''\n        calculate the marginal distribution of sequence states on both ends\n        of the branch leading to node,\n\n        Parameters\n        ----------\n        node : PhyloTree.Clade\n           TreeNode, attached to the branch.\n\n        Returns\n        -------\n        branch_length : float\n           branch length of the branch leading to the node.\n           note: this can be unstable on iteration\n        '''\n\n        if node.up is None:\n            return self.one_mutation\n        pp, pc = self.marginal_branch_profile(node)\n        return self.gtr.optimal_t_compressed((pp, pc), self.multiplicity, profiles=True, tol=tol)", "response": "Calculates the optimal marginal distribution of sequence states on both ends of the branch leading to node."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prune_short_branches(self):\n        self.logger(\"TreeAnc.prune_short_branches: pruning short branches (max prob at zero)...\", 1)\n        for node in self.tree.find_clades():\n            if node.up is None or node.is_terminal():\n                continue\n\n            # probability of the two seqs separated by zero time is not zero\n            if self.gtr.prob_t(node.up.cseq, node.cseq, 0.0,\n                               pattern_multiplicity=self.multiplicity) > 0.1:\n                # re-assign the node children directly to its parent\n                node.up.clades = [k for k in node.up.clades if k != node] + node.clades\n                for clade in node.clades:\n                    clade.up = node.up", "response": "Removes the short branches from the tree."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef optimize_seq_and_branch_len(self,reuse_branch_len=True, prune_short=True,\n                                    marginal_sequences=False, branch_length_mode='joint',\n                                    max_iter=5, infer_gtr=False, **kwargs):\n        \"\"\"\n        Iteratively set branch lengths and reconstruct ancestral sequences until\n        the values of either former or latter do not change. The algorithm assumes\n        knowing only the topology of the tree, and requires that sequences are assigned\n        to all leaves of the tree.\n\n        The first step is to pre-reconstruct ancestral\n        states using Fitch reconstruction algorithm or ML using existing branch length\n        estimates. Then, optimize branch lengths and re-do reconstruction until\n        convergence using ML method.\n\n        Parameters\n        -----------\n\n         reuse_branch_len : bool\n            If True, rely on the initial branch lengths, and start with the\n            maximum-likelihood ancestral sequence inference using existing branch\n            lengths. Otherwise, do initial reconstruction of ancestral states with\n            Fitch algorithm, which uses only the tree topology.\n\n         prune_short : bool\n            If True, the branches with zero optimal length will be pruned from\n            the tree, creating polytomies. The polytomies could be further\n            processed using :py:meth:`treetime.TreeTime.resolve_polytomies` from the TreeTime class.\n\n         marginal_sequences : bool\n            Assign sequences to their marginally most likely value, rather than\n            the values that are jointly most likely across all nodes.\n\n         branch_length_mode : str\n            'joint', 'marginal', or 'input'. Branch lengths are left unchanged in case\n            of 'input'. 'joint' and 'marginal' cause branch length optimization\n            while setting sequences to the ML value or tracing over all possible\n            internal sequence states.\n\n         max_iter : int\n            Maximal number of times sequence and branch length iteration are optimized\n\n         infer_gtr : bool\n            Infer a GTR model from the observed substitutions.\n\n        \"\"\"\n        if branch_length_mode=='marginal':\n            marginal_sequences = True\n\n        self.logger(\"TreeAnc.optimize_sequences_and_branch_length: sequences...\", 1)\n        if reuse_branch_len:\n            N_diff = self.reconstruct_anc(method='probabilistic', infer_gtr=infer_gtr,\n                                          marginal=marginal_sequences, **kwargs)\n            self.optimize_branch_len(verbose=0, store_old=False, mode=branch_length_mode)\n        else:\n            N_diff = self.reconstruct_anc(method='fitch', infer_gtr=infer_gtr, **kwargs)\n\n            self.optimize_branch_len(verbose=0, store_old=False, marginal=False)\n\n        n = 0\n        while n<max_iter:\n            n += 1\n            if prune_short:\n                self.prune_short_branches()\n            N_diff = self.reconstruct_anc(method='probabilistic', infer_gtr=False,\n                                          marginal=marginal_sequences, **kwargs)\n\n            self.logger(\"TreeAnc.optimize_sequences_and_branch_length: Iteration %d.\"\n                   \" #Nuc changed since prev reconstructions: %d\" %(n, N_diff), 2)\n\n            if N_diff < 1:\n                break\n            self.optimize_branch_len(verbose=0, store_old=False, mode=branch_length_mode)\n\n        self.tree.unconstrained_sequence_LH = (self.tree.sequence_LH*self.multiplicity).sum()\n        self._prepare_nodes() # fix dist2root and up-links after reconstruction\n        self.logger(\"TreeAnc.optimize_sequences_and_branch_length: Unconstrained sequence LH:%f\" % self.tree.unconstrained_sequence_LH , 2)\n        return ttconf.SUCCESS", "response": "Optimize the sequences and branch lengths of the tree."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the multiple sequence alignment including reconstructed sequences for the internal nodes.", "response": "def get_reconstructed_alignment(self):\n        \"\"\"\n        Get the multiple sequence alignment, including reconstructed sequences for\n        the internal nodes.\n\n        Returns\n        -------\n        new_aln : MultipleSeqAlignment\n           Alignment including sequences of all internal nodes\n\n        \"\"\"\n        from Bio.Align import MultipleSeqAlignment\n        from Bio.Seq import Seq\n        from Bio.SeqRecord import SeqRecord\n        self.logger(\"TreeAnc.get_reconstructed_alignment ...\",2)\n        if not hasattr(self.tree.root, 'sequence'):\n            self.logger(\"TreeAnc.reconstructed_alignment... reconstruction not yet done\",3)\n            self.reconstruct_anc('probabilistic')\n\n        new_aln = MultipleSeqAlignment([SeqRecord(id=n.name, seq=Seq(\"\".join(n.sequence)), description=\"\")\n                                        for n in self.tree.find_clades()])\n\n        return new_aln"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_tree_dict(self, keep_var_ambigs=False):\n        if self.is_vcf:\n            tree_dict = {}\n            tree_dict['reference'] = self.ref\n            tree_dict['positions'] = self.nonref_positions\n\n            tree_aln = {}\n            for n in self.tree.find_clades():\n                if hasattr(n, 'sequence'):\n                    if keep_var_ambigs: #regenerate dict to include ambig bases\n                        tree_aln[n.name] = self.dict_sequence(n, keep_var_ambigs)\n                    else:\n                        tree_aln[n.name] = n.sequence\n\n            tree_dict['sequences'] = tree_aln\n\n            if len(self.inferred_const_sites) != 0:\n                tree_dict['inferred_const_sites'] = self.inferred_const_sites\n\n            return tree_dict\n        else:\n            raise TypeError(\"A dict can only be returned for trees created with VCF-input!\")", "response": "Returns a nested dict with all the information required to reconstruct sequences for all the nodes in the VCF."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfunction that returns the product of the transition matrix and the equilibrium frequencies to obtain the rate matrix of the GTR model", "response": "def Q(self):\n        \"\"\"function that return the product of the transition matrix\n           and the equilibrium frequencies to obtain the rate matrix\n           of the GTR model\n        \"\"\"\n        tmp = np.einsum('ia,ij->ija', self.Pi, self.W)\n        diag_vals = np.sum(tmp, axis=0)\n        for x in range(tmp.shape[-1]):\n            np.fill_diagonal(tmp[:,:,x], -diag_vals[:,x])\n        return tmp"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef assign_rates(self, mu=1.0, pi=None, W=None):\n        n = len(self.alphabet)\n        self.mu = np.copy(mu)\n\n        if pi is not None and pi.shape[0]==n:\n            self.seq_len = pi.shape[-1]\n            Pi = np.copy(pi)\n        else:\n            if pi is not None and len(pi)!=n:\n                self.logger(\"length of equilibrium frequency vector does not match alphabet length\", 4, warn=True)\n                self.logger(\"Ignoring input equilibrium frequencies\", 4, warn=True)\n            Pi = np.ones(shape=(n,self.seq_len))\n\n        self.Pi = Pi/np.sum(Pi, axis=0)\n\n        if W is None or W.shape!=(n,n):\n            if (W is not None) and W.shape!=(n,n):\n                self.logger(\"Substitution matrix size does not match alphabet size\", 4, warn=True)\n                self.logger(\"Ignoring input substitution matrix\", 4, warn=True)\n            # flow matrix\n            W = np.ones((n,n))\n        else:\n            W=0.5*(np.copy(W)+np.copy(W).T)\n\n        np.fill_diagonal(W,0)\n        avg_pi = self.Pi.mean(axis=-1)\n        average_rate = W.dot(avg_pi).dot(avg_pi)\n        self.W = W/average_rate\n        self.mu *=average_rate\n\n        self._eig()", "response": "Assign the rates of the current GTR model to the given data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef random(cls, L=1, avg_mu=1.0, alphabet='nuc', pi_dirichlet_alpha=1,\n               W_dirichlet_alpha=3.0, mu_gamma_alpha=3.0):\n        \"\"\"\n        Creates a random GTR model\n\n        Parameters\n        ----------\n\n         mu : float\n            Substitution rate\n\n         alphabet : str\n            Alphabet name (should be standard: 'nuc', 'nuc_gap', 'aa', 'aa_gap')\n\n\n        \"\"\"\n        from scipy.stats import gamma\n        alphabet=alphabets[alphabet]\n        gtr = cls(alphabet=alphabet, seq_len=L)\n        n = gtr.alphabet.shape[0]\n\n        if pi_dirichlet_alpha:\n            pi = 1.0*gamma.rvs(pi_dirichlet_alpha, size=(n,L))\n        else:\n            pi = np.ones((n,L))\n\n        pi /= pi.sum(axis=0)\n        if W_dirichlet_alpha:\n            tmp = 1.0*gamma.rvs(W_dirichlet_alpha, size=(n,n))\n        else:\n            tmp = np.ones((n,n))\n        tmp = np.tril(tmp,k=-1)\n        W = tmp + tmp.T\n\n        if mu_gamma_alpha:\n            mu = gamma.rvs(mu_gamma_alpha, size=(L,))\n        else:\n            mu = np.ones(L)\n\n        gtr.assign_rates(mu=mu, pi=pi, W=W)\n        gtr.mu *= avg_mu/np.mean(gtr.mu)\n\n        return gtr", "response": "Creates a random GTR model from a set of random values."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a custom GTR model by specifying the matrix explicitly passed as .", "response": "def custom(cls, mu=1.0, pi=None, W=None, **kwargs):\n        \"\"\"\n        Create a GTR model by specifying the matrix explicitly\n\n        Parameters\n        ----------\n\n         mu : float\n            Substitution rate\n\n         W : nxn matrix\n            Substitution matrix\n\n         pi : n vector\n            Equilibrium frequencies\n\n         **kwargs:\n            Key word arguments to be passed\n\n        Keyword Args\n        ------------\n\n         alphabet : str\n            Specify alphabet when applicable. If the alphabet specification is\n            required, but no alphabet is specified, the nucleotide alphabet will be used as\n            default.\n\n        \"\"\"\n        gtr = cls(**kwargs)\n        gtr.assign_rates(mu=mu, pi=pi, W=W)\n        return gtr"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninferring a GTR model by specifying the number of transitions and time spent in each character state.", "response": "def infer(cls, sub_ija, T_ia, root_state, pc=0.01,\n              gap_limit=0.01, Nit=30, dp=1e-5, **kwargs):\n        \"\"\"\n        Infer a GTR model by specifying the number of transitions and time spent in each\n        character. The basic equation that is being solved is\n\n        :math:`n_{ij} = pi_i W_{ij} T_j`\n\n        where :math:`n_{ij}` are the transitions, :math:`pi_i` are the equilibrium\n        state frequencies, :math:`W_{ij}` is the \"substitution attempt matrix\",\n        while :math:`T_i` is the time on the tree spent in character state\n        :math:`i`. To regularize the process, we add pseudocounts and also need\n        to account for the fact that the root of the tree is in a particular\n        state. the modified equation is\n\n        :math:`n_{ij} + pc = pi_i W_{ij} (T_j+pc+root\\_state)`\n\n        Parameters\n        ----------\n\n         nija : nxn matrix\n            The number of times a change in character state is observed\n            between state j and i at position a\n\n         Tia :n vector\n            The time spent in each character state at position a\n\n         root_state : n vector\n            The number of characters in state i in the sequence\n            of the root node.\n\n         pc : float\n            Pseudocounts, this determines the lower cutoff on the rate when\n            no substitutions are observed\n\n         **kwargs:\n            Key word arguments to be passed\n\n        Keyword Args\n        ------------\n\n         alphabet : str\n            Specify alphabet when applicable. If the alphabet specification\n            is required, but no alphabet is specified, the nucleotide alphabet will be used as default.\n\n        \"\"\"\n        from scipy import linalg as LA\n        gtr = cls(**kwargs)\n        gtr.logger(\"GTR: model inference \",1)\n        q = len(gtr.alphabet)\n        L = sub_ija.shape[-1]\n\n        n_iter = 0\n        n_ija = np.copy(sub_ija)\n        n_ija[range(q),range(q),:] = 0\n        n_ij = n_ija.sum(axis=-1)\n\n        m_ia = np.sum(n_ija,axis=1) + root_state + pc\n        n_a = n_ija.sum(axis=1).sum(axis=0) + pc\n\n        Lambda = np.sum(root_state,axis=0) + q*pc\n        p_ia_old=np.zeros((q,L))\n        p_ia = np.ones((q,L))/q\n        mu_a = np.ones(L)\n\n        W_ij = np.ones((q,q)) - np.eye(q)\n\n        while (LA.norm(p_ia_old-p_ia)>dp) and n_iter<Nit:\n            n_iter += 1\n            p_ia_old = np.copy(p_ia)\n            S_ij = np.einsum('a,ia,ja',mu_a, p_ia, T_ia)\n            W_ij = (n_ij + n_ij.T + pc)/(S_ij + S_ij.T + pc)\n\n            avg_pi = p_ia.mean(axis=-1)\n            average_rate = W_ij.dot(avg_pi).dot(avg_pi)\n            W_ij = W_ij/average_rate\n            mu_a *=average_rate\n\n            p_ia = m_ia/(mu_a*np.dot(W_ij,T_ia)+Lambda)\n            p_ia = p_ia/p_ia.sum(axis=0)\n\n            mu_a = n_a/(pc+np.einsum('ia,ij,ja->a', p_ia, W_ij, T_ia))\n\n        if n_iter >= Nit:\n            gtr.logger('WARNING: maximum number of iterations has been reached in GTR inference',3, warn=True)\n            if LA.norm(p_ia_old-p_ia) > dp:\n                gtr.logger('the iterative scheme has not converged',3,warn=True)\n        if gtr.gap_index is not None:\n            for p in range(p_ia.shape[-1]):\n                if p_ia[gtr.gap_index,p]<gap_limit:\n                    gtr.logger('The model allows for gaps which are estimated to occur at a low fraction of %1.3e'%p_ia[gtr.gap_index]+\n                           '\\n\\t\\tthis can potentially result in artificats.'+\n                           '\\n\\t\\tgap fraction will be set to %1.4f'%gap_limit,2,warn=True)\n                p_ia[gtr.gap_index,p] = gap_limit\n                p_ia[:,p] /= p_ia[:,p].sum()\n\n        gtr.assign_rates(mu=mu_a, W=W_ij, pi=p_ia)\n        return gtr"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the probability to observe seq_ch after time t starting from seq_p.", "response": "def prob_t(self, seq_p, seq_ch, t, pattern_multiplicity = None,\n               return_log=False, ignore_gaps=True):\n        \"\"\"\n        Compute the probability to observe seq_ch (child sequence) after time t starting from seq_p\n        (parent sequence).\n\n        Parameters\n        ----------\n\n         seq_p : character array\n            Parent sequence\n\n         seq_c : character array\n            Child sequence\n\n         t : double\n            Time (branch len) separating the profiles.\n\n         pattern_multiplicity : numpy array\n            If sequences are reduced by combining identical alignment patterns,\n            these multplicities need to be accounted for when counting the number\n            of mutations across a branch. If None, all pattern are assumed to\n            occur exactly once.\n\n         return_log : bool\n            It True, return log-probability.\n\n        Returns\n        -------\n         prob : np.array\n            Resulting probability\n\n        \"\"\"\n        if t<0:\n            logP = -ttconf.BIG_NUMBER\n        else:\n            tmp_eQT = self.expQt(t)\n            bad_indices=(tmp_eQT==0)\n            logQt = np.log(tmp_eQT + ttconf.TINY_NUMBER*(bad_indices))\n            logQt[np.isnan(logQt) | np.isinf(logQt) | bad_indices] = -ttconf.BIG_NUMBER\n            seq_indices_c = np.zeros(len(seq_ch), dtype=int)\n            seq_indices_p = np.zeros(len(seq_p), dtype=int)\n            for ai, a in self.alphabet:\n                seq_indices_p[seq_p==a] = ai\n                seq_indices_c[seq_ch==a] = ai\n\n            if len(logQt.shape)==2:\n                logP = np.sum(logQt[seq_indices_p, seq_indices_c]*pattern_multiplicity)\n            else:\n                logP = np.sum(logQt[seq_indices_p, seq_indices_c, np.arange(len(seq_c))]*pattern_multiplicity)\n\n        return logP if return_log else np.exp(logP)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef assign_rates(self, mu=1.0, pi=None, W=None):\n        n = len(self.alphabet)\n        self.mu = mu\n\n        if pi is not None and len(pi)==n:\n            Pi = np.array(pi)\n        else:\n            if pi is not None and len(pi)!=n:\n                self.logger(\"length of equilibrium frequency vector does not match alphabet length\", 4, warn=True)\n                self.logger(\"Ignoring input equilibrium frequencies\", 4, warn=True)\n            Pi = np.ones(shape=(n,))\n\n        self.Pi = Pi/np.sum(Pi)\n\n        if W is None or W.shape!=(n,n):\n            if (W is not None) and W.shape!=(n,n):\n                self.logger(\"Substitution matrix size does not match alphabet size\", 4, warn=True)\n                self.logger(\"Ignoring input substitution matrix\", 4, warn=True)\n            # flow matrix\n            W = np.ones((n,n))\n            np.fill_diagonal(W, 0.0)\n            np.fill_diagonal(W, - W.sum(axis=0))\n        else:\n            W=np.array(W)\n\n        self.W = 0.5*(W+W.T)\n        self._check_fix_Q(fixed_mu=True)\n        self._eig()", "response": "Assign the rates mu pi and W to the GTR model."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef standard(model, **kwargs):\n        from .nuc_models import JC69, K80, F81, HKY85, T92, TN93\n        from .aa_models  import JTT92\n\n        if model.lower() in ['jc', 'jc69', 'jukes-cantor', 'jukes-cantor69', 'jukescantor', 'jukescantor69']:\n            return JC69(**kwargs)\n        elif model.lower() in ['k80', 'kimura80', 'kimura1980']:\n            return K80(**kwargs)\n        elif model.lower() in ['f81', 'felsenstein81', 'felsenstein1981']:\n            return F81(**kwargs)\n        elif model.lower() in ['hky', 'hky85', 'hky1985']:\n            return HKY85(**kwargs)\n        elif model.lower() in ['t92', 'tamura92', 'tamura1992']:\n            return T92(**kwargs)\n        elif model.lower() in ['tn93', 'tamura_nei_93', 'tamuranei93']:\n            return TN93(**kwargs)\n        elif model.lower() in ['jtt', 'jtt92']:\n            return JTT92(**kwargs)\n        else:\n            raise KeyError(\"The GTR model '{}' is not in the list of available models.\"\n                \"\".format(model))", "response": "Create a standard evolution model of the specified model."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a random GTR model from a random mu and alphabet", "response": "def random(cls, mu=1.0, alphabet='nuc'):\n        \"\"\"\n        Creates a random GTR model\n\n        Parameters\n        ----------\n\n         mu : float\n            Substitution rate\n\n         alphabet : str\n            Alphabet name (should be standard: 'nuc', 'nuc_gap', 'aa', 'aa_gap')\n\n\n        \"\"\"\n\n        alphabet=alphabets[alphabet]\n        gtr = cls(alphabet)\n        n = gtr.alphabet.shape[0]\n        pi = 1.0*np.random.randint(0,100,size=(n))\n        W = 1.0*np.random.randint(0,100,size=(n,n)) # with gaps\n\n        gtr.assign_rates(mu=mu, pi=pi, W=W)\n        return gtr"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninferring a GTR model by specifying the number of times a change in each character state is observed in each character state Ti and the time spent in each character state Ti.", "response": "def infer(cls, nij, Ti, root_state, fixed_pi=None, pc=5.0, gap_limit=0.01, **kwargs):\n        \"\"\"\n        Infer a GTR model by specifying the number of transitions and time spent in each\n        character. The basic equation that is being solved is\n\n        :math:`n_{ij} = pi_i W_{ij} T_j`\n\n        where :math:`n_{ij}` are the transitions, :math:`pi_i` are the equilibrium\n        state frequencies, :math:`W_{ij}` is the \"substitution attempt matrix\",\n        while :math:`T_i` is the time on the tree spent in character state\n        :math:`i`. To regularize the process, we add pseudocounts and also need\n        to account for the fact that the root of the tree is in a particular\n        state. the modified equation is\n\n        :math:`n_{ij} + pc = pi_i W_{ij} (T_j+pc+root\\_state)`\n\n        Parameters\n        ----------\n\n         nij : nxn matrix\n            The number of times a change in character state is observed\n            between state j and i\n\n         Ti :n vector\n            The time spent in each character state\n\n         root_state : n vector\n            The number of characters in state i in the sequence\n            of the root node.\n\n         pc : float\n            Pseudocounts, this determines the lower cutoff on the rate when\n            no substitutions are observed\n\n         **kwargs:\n            Key word arguments to be passed\n\n        Keyword Args\n        ------------\n\n         alphabet : str\n            Specify alphabet when applicable. If the alphabet specification\n            is required, but no alphabet is specified, the nucleotide alphabet will be used as default.\n\n        \"\"\"\n        from scipy import linalg as LA\n        gtr = cls(**kwargs)\n        gtr.logger(\"GTR: model inference \",1)\n        dp = 1e-5\n        Nit = 40\n        pc_mat = pc*np.ones_like(nij)\n        np.fill_diagonal(pc_mat, 0.0)\n        count = 0\n        pi_old = np.zeros_like(Ti)\n        if fixed_pi is None:\n            pi = np.ones_like(Ti)\n        else:\n            pi = np.copy(fixed_pi)\n        pi/=pi.sum()\n        W_ij = np.ones_like(nij)\n        mu = nij.sum()/Ti.sum()\n        # if pi is fixed, this will immediately converge\n        while LA.norm(pi_old-pi) > dp and count < Nit:\n            gtr.logger(' '.join(map(str, ['GTR inference iteration',count,'change:',LA.norm(pi_old-pi)])), 3)\n            count += 1\n            pi_old = np.copy(pi)\n            W_ij = (nij+nij.T+2*pc_mat)/mu/(np.outer(pi,Ti) + np.outer(Ti,pi)\n                                                    + ttconf.TINY_NUMBER + 2*pc_mat)\n\n            np.fill_diagonal(W_ij, 0)\n            scale_factor = np.einsum('i,ij,j',pi,W_ij,pi)\n\n            W_ij = W_ij/scale_factor\n            if fixed_pi is None:\n                pi = (np.sum(nij+pc_mat,axis=1)+root_state)/(ttconf.TINY_NUMBER + mu*np.dot(W_ij,Ti)+root_state.sum()+np.sum(pc_mat, axis=1))\n                pi /= pi.sum()\n            mu = nij.sum()/(ttconf.TINY_NUMBER + np.sum(pi * (W_ij.dot(Ti))))\n        if count >= Nit:\n            gtr.logger('WARNING: maximum number of iterations has been reached in GTR inference',3, warn=True)\n            if LA.norm(pi_old-pi) > dp:\n                gtr.logger('the iterative scheme has not converged',3,warn=True)\n            elif np.abs(1-np.max(pi.sum(axis=0))) > dp:\n                gtr.logger('the iterative scheme has converged, but proper normalization was not reached',3,warn=True)\n        if gtr.gap_index is not None:\n            if pi[gtr.gap_index]<gap_limit:\n                gtr.logger('The model allows for gaps which are estimated to occur at a low fraction of %1.3e'%pi[gtr.gap_index]+\n                       '\\n\\t\\tthis can potentially result in artificats.'+\n                       '\\n\\t\\tgap fraction will be set to %1.4f'%gap_limit,2,warn=True)\n            pi[gtr.gap_index] = gap_limit\n            pi /= pi.sum()\n\n        gtr.assign_rates(mu=mu, W=W_ij, pi=pi)\n        return gtr"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfixes the main diagonal of the rate matrix and fix the mu if fixed_mu is True.", "response": "def _check_fix_Q(self, fixed_mu=False):\n        \"\"\"\n        Check the main diagonal of Q and fix it in case it does not corresond\n        the definition of the rate matrix. Should be run every time when creating\n        custom GTR model.\n        \"\"\"\n        # fix Q\n        self.Pi /= self.Pi.sum() # correct the Pi manually\n        # NEEDED TO BREAK RATE MATRIX DEGENERACY AND FORCE NP TO RETURN REAL ORTHONORMAL EIGENVECTORS\n        self.W += self.break_degen + self.break_degen.T\n        # fix W\n        np.fill_diagonal(self.W, 0)\n        Wdiag = -(self.Q).sum(axis=0)/self.Pi\n        np.fill_diagonal(self.W, Wdiag)\n        scale_factor = -np.sum(np.diagonal(self.Q)*self.Pi)\n        self.W /= scale_factor\n        if not fixed_mu:\n            self.mu *= scale_factor\n        if (self.Q.sum(axis=0) < 1e-10).sum() <  self.alphabet.shape[0]: # fix failed\n            print (\"Cannot fix the diagonal of the GTR rate matrix. Should be all zero\", self.Q.sum(axis=0))\n            import ipdb; ipdb.set_trace()\n            raise ArithmeticError(\"Cannot fix the diagonal of the GTR rate matrix.\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nperform eigendecomposition of the rate matrix and store the left and right - sequence profiles in self. v and self. v_inv.", "response": "def _eig(self):\n        \"\"\"\n        Perform eigendecompositon of the rate matrix and stores the left- and right-\n        matrices to convert the sequence profiles to the GTR matrix eigenspace\n        and hence to speed-up the computations.\n        \"\"\"\n        # eigendecomposition of the rate matrix\n        eigvals, eigvecs = np.linalg.eig(self.Q)\n        self.v = np.real(eigvecs)\n        self.v_inv = np.linalg.inv(self.v)\n        self.eigenvals = np.real(eigvals)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming eigendecompositon of the rate matrix and stores the left - and right - matrices and the GTR matrix eigenvals.", "response": "def _eig_sym(self):\n        \"\"\"\n        Perform eigendecompositon of the rate matrix and stores the left- and right-\n        matrices to convert the sequence profiles to the GTR matrix eigenspace\n        and hence to speed-up the computations.\n        \"\"\"\n        # eigendecomposition of the rate matrix\n        tmpp = np.sqrt(self.Pi)\n        symQ = self.W*np.outer(tmpp, tmpp)\n        eigvals, eigvecs = np.linalg.eigh(symQ)\n        tmp_v = eigvecs.T*tmpp\n        one_norm = np.sum(np.abs(tmp_v), axis=1)\n        self.v = tmp_v.T/one_norm\n        self.v_inv = (eigvecs*one_norm).T/tmpp\n        self.eigenvals = eigvals"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmake a compressed representation of a pair of sequences, only counting the number of times a particular pair of states (e.g. (A,T)) is observed in the aligned sequences of parent and child. Parameters ---------- seq_p: numpy array Parent sequence as numpy array of chars seq_ch: numpy array Child sequence as numpy array of chars pattern_multiplicity : numpy array If sequences are reduced by combining identical alignment patterns, these multplicities need to be accounted for when counting the number of mutations across a branch. If None, all pattern are assumed to occur exactly once. ignore_gap: bool Whether or not to include gapped positions of the alignment in the multiplicity count Returns ------- seq_pair : list :code:`[(0,1), (2,2), (3,4)]` list of parent_child state pairs as indices in the alphabet multiplicity : numpy array Number of times a particular pair is observed", "response": "def compress_sequence_pair(self, seq_p, seq_ch, pattern_multiplicity=None,\n                               ignore_gaps=False):\n        '''\n        Make a compressed representation of a pair of sequences, only counting\n        the number of times a particular pair of states (e.g. (A,T)) is observed\n        in the aligned sequences of parent and child.\n\n        Parameters\n        ----------\n\n         seq_p: numpy array\n            Parent sequence as numpy array of chars\n\n         seq_ch: numpy array\n            Child sequence as numpy array of chars\n\n         pattern_multiplicity : numpy array\n            If sequences are reduced by combining identical alignment patterns,\n            these multplicities need to be accounted for when counting the number\n            of mutations across a branch. If None, all pattern are assumed to\n            occur exactly once.\n\n         ignore_gap: bool\n            Whether or not to include gapped positions of the alignment\n            in the multiplicity count\n\n        Returns\n        -------\n          seq_pair : list\n            :code:`[(0,1), (2,2), (3,4)]` list of parent_child state pairs\n            as indices in the alphabet\n\n          multiplicity : numpy array\n            Number of times a particular pair is observed\n\n        '''\n        if pattern_multiplicity is None:\n            pattern_multiplicity = np.ones_like(seq_p, dtype=float)\n\n        from collections import Counter\n        if seq_ch.shape != seq_p.shape:\n            raise ValueError(\"GTR.compress_sequence_pair: Sequence lengths do not match!\")\n\n        if len(self.alphabet)<10: # for small alphabet, repeatedly check array for all state pairs\n            pair_count = []\n            bool_seqs_p = []\n            bool_seqs_ch = []\n            for seq, bs in [(seq_p,bool_seqs_p), (seq_ch, bool_seqs_ch)]:\n                for ni,nuc in enumerate(self.alphabet):\n                    bs.append(seq==nuc)\n\n            for n1,nuc1 in enumerate(self.alphabet):\n                if (self.gap_index is None) or (not ignore_gaps) or (n1!=self.gap_index):\n                    for n2,nuc2 in enumerate(self.alphabet):\n                        if (self.gap_index is None) or (not ignore_gaps) or (n2!=self.gap_index):\n                            count = ((bool_seqs_p[n1]&bool_seqs_ch[n2])*pattern_multiplicity).sum()\n                            if count: pair_count.append(((n1,n2), count))\n        else: # enumerate state pairs of the sequence for large alphabets\n            num_seqs = []\n            for seq in [seq_p, seq_ch]: # for each sequence (parent and child) construct a numerical sequence [0,5,3,1,2,3...]\n                tmp = np.ones_like(seq, dtype=int)\n                for ni,nuc in enumerate(self.alphabet):\n                    tmp[seq==nuc] = ni  # set each position corresponding to a state to the corresponding index\n                num_seqs.append(tmp)\n            pair_count = defaultdict(int)\n            if ignore_gaps:  # if gaps are ignored skip positions where one or the other sequence is gapped\n                for i in range(len(seq_p)):\n                    if self.gap_index!=num_seqs[0][i] and self.gap_index!=num_seqs[1][i]:\n                        pair_count[(num_seqs[0][i],num_seqs[1][i])]+=pattern_multiplicity[i]\n            else: # otherwise, just count\n                for i in range(len(seq_p)):\n                    pair_count[(num_seqs[0][i],num_seqs[1][i])]+=pattern_multiplicity[i]\n            pair_count = pair_count.items()\n\n        return (np.array([x[0] for x in pair_count], dtype=int),    # [(child_nuc, parent_nuc),()...]\n                np.array([x[1] for x in pair_count], dtype=int))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef prob_t_compressed(self, seq_pair, multiplicity, t, return_log=False):\n        '''\n        Calculate the probability of observing a sequence pair at a distance t,\n        for compressed sequences\n\n        Parameters\n        ----------\n\n          seq_pair : numpy array\n            :code:`np.array([(0,1), (2,2), ()..])` as indicies of\n            pairs of aligned positions. (e.g. 'A'==0, 'C'==1 etc).\n            This only lists all occuring parent-child state pairs, order is irrelevant\n\n          multiplicity : numpy array\n            The number of times a parent-child state pair is observed.\n            This allows compression of the sequence representation\n\n          t : float\n            Length of the branch separating parent and child\n\n          return_log : bool\n            Whether or not to exponentiate the result\n\n        '''\n        if t<0:\n            logP = -ttconf.BIG_NUMBER\n        else:\n            tmp_eQT = self.expQt(t)\n            bad_indices=(tmp_eQT==0)\n            logQt = np.log(tmp_eQT + ttconf.TINY_NUMBER*(bad_indices))\n            logQt[np.isnan(logQt) | np.isinf(logQt) | bad_indices] = -ttconf.BIG_NUMBER\n            logP = np.sum(logQt[seq_pair[:,1], seq_pair[:,0]]*multiplicity)\n\n        return logP if return_log else np.exp(logP)", "response": "Calculates the probability of observing a sequence pair at a distance t for compressed sequences."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the probability to observe seq_ch after time t starting from seq_p.", "response": "def prob_t(self, seq_p, seq_ch, t, pattern_multiplicity = None,\n               return_log=False, ignore_gaps=True):\n        \"\"\"\n        Compute the probability to observe seq_ch (child sequence) after time t starting from seq_p\n        (parent sequence).\n\n        Parameters\n        ----------\n\n         seq_p : character array\n            Parent sequence\n\n         seq_c : character array\n            Child sequence\n\n         t : double\n            Time (branch len) separating the profiles.\n\n         pattern_multiplicity : numpy array\n            If sequences are reduced by combining identical alignment patterns,\n            these multplicities need to be accounted for when counting the number\n            of mutations across a branch. If None, all pattern are assumed to\n            occur exactly once.\n\n         return_log : bool\n            It True, return log-probability.\n\n        Returns\n        -------\n         prob : np.array\n            Resulting probability\n\n        \"\"\"\n        seq_pair, multiplicity = self.compress_sequence_pair(seq_p, seq_ch,\n                                        pattern_multiplicity=pattern_multiplicity, ignore_gaps=ignore_gaps)\n        return self.prob_t_compressed(seq_pair, multiplicity, t, return_log=return_log)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef optimal_t(self, seq_p, seq_ch, pattern_multiplicity=None, ignore_gaps=False):\n        '''\n        Find the optimal distance between the two sequences\n\n        Parameters\n        ----------\n\n         seq_p : character array\n            Parent sequence\n\n         seq_c : character array\n            Child sequence\n\n         pattern_multiplicity : numpy array\n            If sequences are reduced by combining identical alignment patterns,\n            these multplicities need to be accounted for when counting the number\n            of mutations across a branch. If None, all pattern are assumed to\n            occur exactly once.\n\n         ignore_gaps : bool\n            If True, ignore gaps in distance calculations\n\n        '''\n        seq_pair, multiplicity = self.compress_sequence_pair(seq_p, seq_ch,\n                                                            pattern_multiplicity = pattern_multiplicity,\n                                                            ignore_gaps=ignore_gaps)\n        return self.optimal_t_compressed(seq_pair, multiplicity)", "response": "This function returns the optimal distance between two sequences."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef optimal_t_compressed(self, seq_pair, multiplicity, profiles=False, tol=1e-10):\n\n        def _neg_prob(t, seq_pair, multiplicity):\n            \"\"\"\n            Probability to observe a child given the the parent state, transition\n            matrix, and the time of evolution (branch length).\n\n            Parameters\n            ----------\n\n             t : double\n                Branch length (time between sequences)\n\n             parent :  numpy.array\n                Parent sequence\n\n             child : numpy.array\n                Child sequence\n\n             tm :  GTR\n                Model of evolution\n\n            Returns\n            -------\n\n             prob : double\n                Negative probability of the two given sequences\n                to be separated by the time t.\n            \"\"\"\n            if profiles:\n                res = -1.0*self.prob_t_profiles(seq_pair, multiplicity,t**2, return_log=True)\n                return res\n            else:\n                return -1.0*self.prob_t_compressed(seq_pair, multiplicity,t**2, return_log=True)\n\n        try:\n            from scipy.optimize import minimize_scalar\n            opt = minimize_scalar(_neg_prob,\n                    bounds=[-np.sqrt(ttconf.MAX_BRANCH_LENGTH),np.sqrt(ttconf.MAX_BRANCH_LENGTH)],\n                    args=(seq_pair, multiplicity), tol=tol)\n            new_len = opt[\"x\"]**2\n            if 'success' not in opt:\n                opt['success'] = True\n                self.logger(\"WARNING: the optimization result does not contain a 'success' flag:\"+str(opt),4, warn=True)\n        except:\n            import scipy\n            print('legacy scipy', scipy.__version__)\n            from scipy.optimize import fminbound\n            new_len = fminbound(_neg_prob,\n                      -np.sqrt(ttconf.MAX_BRANCH_LENGTH),np.sqrt(ttconf.MAX_BRANCH_LENGTH),\n                       args=(seq_pair, multiplicity))\n            new_len = new_len**2\n            opt={'success':True}\n\n        if new_len > .9 * ttconf.MAX_BRANCH_LENGTH:\n            self.logger(\"WARNING: GTR.optimal_t_compressed -- The branch length seems to be very long!\", 4, warn=True)\n\n        if opt[\"success\"] != True:\n            # return hamming distance: number of state pairs where state differs/all pairs\n            new_len =  np.sum(multiplicity[seq_pair[:,1]!=seq_pair[:,0]])/np.sum(multiplicity)\n\n        return new_len", "response": "This function calculates the optimal distance between two compressed sequences at a given time."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef prob_t_profiles(self, profile_pair, multiplicity, t,\n                        return_log=False, ignore_gaps=True):\n        '''\n        Calculate the probability of observing a node pair at a distance t\n\n        Parameters\n        ----------\n\n          profile_pair: numpy arrays\n            Probability distributions of the nucleotides at either\n            end of the branch. pp[0] = parent, pp[1] = child\n\n          multiplicity : numpy array\n            The number of times an alignment pattern is observed\n\n          t : float\n            Length of the branch separating parent and child\n\n          ignore_gaps: bool\n            If True, ignore mutations to and from gaps in distance calculations\n\n          return_log : bool\n            Whether or not to exponentiate the result\n\n        '''\n        if t<0:\n            logP = -ttconf.BIG_NUMBER\n        else:\n            Qt = self.expQt(t)\n            if len(Qt.shape)==3:\n                res = np.einsum('ai,ija,aj->a', profile_pair[1], Qt, profile_pair[0])\n            else:\n                res = np.einsum('ai,ij,aj->a', profile_pair[1], Qt, profile_pair[0])\n            if ignore_gaps and (self.gap_index is not None): # calculate the probability that neither outgroup/node has a gap\n                non_gap_frac = (1-profile_pair[0][:,self.gap_index])*(1-profile_pair[1][:,self.gap_index])\n                # weigh log LH by the non-gap probability\n                logP = np.sum(multiplicity*np.log(res)*non_gap_frac)\n            else:\n                logP = np.sum(multiplicity*np.log(res))\n\n        return logP if return_log else np.exp(logP)", "response": "Calculates the probability of observing a node pair at a given distance t."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef propagate_profile(self, profile, t, return_log=False):\n        Qt = self.expQt(t)\n        res = profile.dot(Qt)\n\n        return np.log(res) if return_log else res", "response": "Given a profile compute the probability of the sequence state of the parent at time t0 and given the sequence state of the child at time t1."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef evolve(self, profile, t, return_log=False):\n        Qt = self.expQt(t).T\n        res = profile.dot(Qt)\n        return np.log(res) if return_log else res", "response": "Compute the probability of the sequence state of the child at time t later given the parent profile."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the array of values exp that is less than the given time", "response": "def _exp_lt(self, t):\n        \"\"\"\n        Parameters\n        ----------\n\n         t : float\n            time to propagate\n\n        Returns\n        --------\n\n         exp_lt : numpy.array\n            Array of values exp(lambda(i) * t),\n            where (i) - alphabet index (the eigenvalue number).\n        \"\"\"\n        return np.exp(self.mu * t * self.eigenvals)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef expQt(self, t):\n        '''\n        Parameters\n        ----------\n\n         t : float\n            Time to propagate\n\n        Returns\n        --------\n\n         expQt : numpy.array\n            Matrix exponential of exo(Qt)\n        '''\n        eLambdaT = np.diag(self._exp_lt(t)) # vector length = a\n        Qs = self.v.dot(eLambdaT.dot(self.v_inv))   # This is P(nuc1 | given nuc_2)\n        return np.maximum(0,Qs)", "response": "Calculates the exponential of the avec of avec with respect to the time t."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef expQsds(self, s):\n        '''\n        Returns\n        -------\n        Qtds :  Returns 2 V_{ij} \\lambda_j s e^{\\lambda_j s**2 } V^{-1}_{jk}\n                This is the derivative of the branch probability with respect to s=\\sqrt(t)\n        '''\n        lambda_eLambdaT = np.diag(2.0*self._exp_lt(s**2)*self.eigenvals*s) # vector length = a\n        Qsds = self.v.dot(lambda_eLambdaT.dot(self.v_inv))\n        return Qsds", "response": "Returns the derivative of the derivative of the branch probability with respect to s"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the second derivative of the branch probability wrt time", "response": "def expQsdsds(self, s):\n        '''\n        Returns\n        -------\n        Qtdtdt :  Returns V_{ij} \\lambda_j^2 e^{\\lambda_j s**2} V^{-1}_{jk}\n                This is the second derivative of the branch probability wrt time\n        '''\n        t=s**2\n        elt = self._exp_lt(t)\n        lambda_eLambdaT = np.diag(elt*(4.0*t*self.eigenvals**2 + 2.0*self.eigenvals))\n        Qsdsds = self.v.dot(lambda_eLambdaT.dot(self.v_inv))\n        return Qsdsds"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sequence_logLH(self,seq, pattern_multiplicity=None):\n        if pattern_multiplicity is None:\n            pattern_multiplicity = np.ones_like(seq, dtype=float)\n        return np.sum([np.sum((seq==state)*pattern_multiplicity*np.log(self.Pi[si]))\n                      for si,state in enumerate(self.alphabet)])", "response": "Returns the log - likelihood of sampling a sequence from equilibrium frequency."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plot_vs_years(tt, step = None, ax=None, confidence=None, ticks=True, **kwargs):\n    '''\n    Converts branch length to years and plots the time tree on a time axis.\n\n    Parameters\n    ----------\n     tt : TreeTime object\n        A TreeTime instance after a time tree is inferred\n\n     step : int\n        Width of shaded boxes indicating blocks of years. Will be inferred if not specified.\n        To switch off drawing of boxes, set to 0\n\n     ax : matplotlib axes\n        Axes to be used to plot, will create new axis if None\n\n     confidence : tuple, float\n        Draw confidence intervals. This assumes that marginal time tree inference was run.\n        Confidence intervals are either specified as an interval of the posterior distribution\n        like (0.05, 0.95) or as the weight of the maximal posterior region , e.g. 0.9\n\n     **kwargs : dict\n        Key word arguments that are passed down to Phylo.draw\n\n    '''\n    import matplotlib.pyplot as plt\n    tt.branch_length_to_years()\n    nleafs = tt.tree.count_terminals()\n\n    if ax is None:\n        fig = plt.figure(figsize=(12,10))\n        ax = plt.subplot(111)\n    else:\n        fig = None\n    # draw tree\n    if \"label_func\" not in kwargs:\n        kwargs[\"label_func\"] = lambda x:x.name if (x.is_terminal() and nleafs<30) else \"\"\n    Phylo.draw(tt.tree, axes=ax, **kwargs)\n\n    offset = tt.tree.root.numdate - tt.tree.root.branch_length\n    date_range = np.max([n.numdate for n in tt.tree.get_terminals()])-offset\n\n    # estimate year intervals if not explicitly specified\n    if step is None or (step>0 and date_range/step>100):\n        step = 10**np.floor(np.log10(date_range))\n        if date_range/step<2:\n            step/=5\n        elif date_range/step<5:\n            step/=2\n        step = max(1.0/12,step)\n\n    # set axis labels\n    if step:\n        dtick = step\n        min_tick = step*(offset//step)\n        extra = dtick if dtick<date_range else dtick\n        tick_vals = np.arange(min_tick, min_tick+date_range+extra, dtick)\n        xticks = tick_vals - offset\n    else:\n        xticks = ax.get_xticks()\n        dtick = xticks[1]-xticks[0]\n        shift = offset - dtick*(offset//dtick)\n        xticks -= shift\n        tick_vals = [x+offset-shift for x in xticks]\n\n    ax.set_xticks(xticks)\n    ax.set_xticklabels(map(str, tick_vals))\n    ax.set_xlabel('year')\n    ax.set_ylabel('')\n    ax.set_xlim((0,date_range))\n\n    # put shaded boxes to delineate years\n    if step:\n        ylim = ax.get_ylim()\n        xlim = ax.get_xlim()\n        from matplotlib.patches import Rectangle\n        for yi,year in enumerate(np.arange(np.floor(tick_vals[0]), tick_vals[-1]+.01, step)):\n            pos = year - offset\n            r = Rectangle((pos, ylim[1]-5),\n                          step, ylim[0]-ylim[1]+10,\n                          facecolor=[0.7+0.1*(1+yi%2)] * 3,\n                          edgecolor=[1,1,1])\n            ax.add_patch(r)\n            if year in tick_vals and pos>=xlim[0] and pos<=xlim[1] and ticks:\n                label_str = str(step*(year//step)) if step<1 else  str(int(year))\n                ax.text(pos,ylim[0]-0.04*(ylim[1]-ylim[0]), label_str,\n                        horizontalalignment='center')\n        ax.set_axis_off()\n\n    # add confidence intervals to the tree graph -- grey bars\n    if confidence:\n        tree_layout(tt.tree)\n        if not hasattr(tt.tree.root, \"marginal_inverse_cdf\"):\n            print(\"marginal time tree reconstruction required for confidence intervals\")\n            return ttconf.ERROR\n        elif type(confidence) is float:\n            cfunc = tt.get_max_posterior_region\n        elif len(confidence)==2:\n            cfunc = tt.get_confidence_interval\n        else:\n            print(\"confidence needs to be either a float (for max posterior region) or a two numbers specifying lower and upper bounds\")\n            return ttconf.ERROR\n\n        for n in tt.tree.find_clades():\n            pos = cfunc(n, confidence)\n            ax.plot(pos-offset, np.ones(len(pos))*n.ypos, lw=3, c=(0.5,0.5,0.5))\n    return fig, ax", "response": "Plots the time tree vs. years of the time tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run(self, root=None, infer_gtr=True, relaxed_clock=None, n_iqd = None,\n            resolve_polytomies=True, max_iter=0, Tc=None, fixed_clock_rate=None,\n            time_marginal=False, sequence_marginal=False, branch_length_mode='auto',\n            vary_rate=False, use_covariation=False, **kwargs):\n\n        \"\"\"\n        Run TreeTime reconstruction. Based on the input parameters, it divides\n        the analysis into semi-independent jobs and conquers them one-by-one,\n        gradually optimizing the tree given the temporal constarints and leaf\n        node sequences.\n\n        Parameters\n        ----------\n        root : str\n           Try to find better root position on a given tree. If string is passed,\n           the root will be searched according to the specified method. If none,\n           use tree as-is.\n\n           See :py:meth:`treetime.TreeTime.reroot` for available rooting methods.\n\n        infer_gtr : bool\n           If True, infer GTR model\n\n        relaxed_clock : dic\n           If not None, use autocorrelated molecular clock model. Specify the\n           clock parameters as :code:`{slack:<slack>, coupling:<coupling>}` dictionary.\n\n        n_iqd : int\n           If not None, filter tree nodes which do not obey the molecular clock\n           for the particular tree. The nodes, which deviate more than\n           :code:`n_iqd` interquantile intervals from the molecular clock\n           regression will be marked as 'BAD' and not used in the TreeTime\n           analysis\n\n        resolve_polytomies : bool\n           If True, attempt to resolve multiple mergers\n\n        max_iter : int\n           Maximum number of iterations to optimize the tree\n\n        Tc : float, str\n           If not None, use coalescent model to correct the branch lengths by\n           introducing merger costs.\n\n           If Tc is float, it is interpreted as the coalescence time scale\n\n           If Tc is str, it should be one of (:code:`opt`, :code:`const`, :code:`skyline`)\n\n        fixed_clock_rate : float\n           Fixed clock rate to be used. If None, infer clock rate from the molecular clock.\n\n        time_marginal : bool\n           If True, perform a final round of marginal reconstruction of the node's positions.\n\n        sequence_marginal : bool, optional\n            use marginal reconstruction for ancestral sequences\n\n        branch_length_mode : str\n           Should be one of: :code:`joint`, :code:`marginal`, :code:`input`.\n\n           If 'input', rely on the branch lengths in the input tree and skip directly\n           to the maximum-likelihood ancestral sequence reconstruction.\n           Otherwise, perform preliminary sequence reconstruction using parsimony\n           algorithm and do branch length optimization\n\n        vary_rate : bool or float, optional\n            redo the time tree estimation for rates +/- one standard deviation.\n            if a float is passed, it is interpreted as standard deviation,\n            otherwise this standard deviation is estimated from the root-to-tip regression\n\n        use_covariation : bool, optional\n            default False, if False, rate estimates will be performed using simple\n            regression ignoring phylogenetic covaration between nodes.\n\n        **kwargs\n           Keyword arguments needed by the downstream functions\n\n\n        Returns\n        -------\n        TreeTime error/succces code : str\n            return value depending on success or error\n\n\n        \"\"\"\n\n        # register the specified covaration mode\n        self.use_covariation = use_covariation\n\n        if (self.tree is None) or (self.aln is None and self.seq_len is None):\n            self.logger(\"TreeTime.run: ERROR, alignment or tree are missing\", 0)\n            return ttconf.ERROR\n        if (self.aln is None):\n            branch_length_mode='input'\n\n        self._set_branch_length_mode(branch_length_mode)\n\n        # determine how to reconstruct and sample sequences\n        seq_kwargs = {\"marginal_sequences\":sequence_marginal or (self.branch_length_mode=='marginal'),\n                      \"sample_from_profile\":\"root\"}\n\n        tt_kwargs = {'clock_rate':fixed_clock_rate, 'time_marginal':False}\n        tt_kwargs.update(kwargs)\n\n        seq_LH = 0\n        if \"fixed_pi\" in kwargs:\n            seq_kwargs[\"fixed_pi\"] = kwargs[\"fixed_pi\"]\n        if \"do_marginal\" in kwargs:\n            time_marginal=kwargs[\"do_marginal\"]\n\n        # initially, infer ancestral sequences and infer gtr model if desired\n        if self.branch_length_mode=='input':\n            if self.aln:\n                self.infer_ancestral_sequences(infer_gtr=infer_gtr, **seq_kwargs)\n                self.prune_short_branches()\n        else:\n            self.optimize_sequences_and_branch_length(infer_gtr=infer_gtr,\n                                                      max_iter=1, prune_short=True, **seq_kwargs)\n        avg_root_to_tip = np.mean([x.dist2root for x in self.tree.get_terminals()])\n\n        # optionally reroot the tree either by oldest, best regression or with a specific leaf\n        if n_iqd or root=='clock_filter':\n            if \"plot_rtt\" in kwargs and kwargs[\"plot_rtt\"]:\n                plot_rtt=True\n            else:\n                plot_rtt=False\n            reroot_mechanism = 'least-squares' if root=='clock_filter' else root\n            if self.clock_filter(reroot=reroot_mechanism, n_iqd=n_iqd, plot=plot_rtt)==ttconf.ERROR:\n                return ttconf.ERROR\n        elif root is not None:\n            if self.reroot(root=root)==ttconf.ERROR:\n                return ttconf.ERROR\n\n        if self.branch_length_mode=='input':\n            if self.aln:\n                self.infer_ancestral_sequences(**seq_kwargs)\n        else:\n            self.optimize_sequences_and_branch_length(max_iter=1, prune_short=False,\n                                                      **seq_kwargs)\n\n        # infer time tree and optionally resolve polytomies\n        self.logger(\"###TreeTime.run: INITIAL ROUND\",0)\n        self.make_time_tree(**tt_kwargs)\n\n        if self.aln:\n            seq_LH = self.tree.sequence_marginal_LH if seq_kwargs['marginal_sequences'] else self.tree.sequence_joint_LH\n        self.LH =[[seq_LH, self.tree.positional_joint_LH, 0]]\n\n        if root is not None and max_iter:\n            if self.reroot(root='least-squares' if root=='clock_filter' else root)==ttconf.ERROR:\n                return ttconf.ERROR\n\n        # iteratively reconstruct ancestral sequences and re-infer\n        # time tree to ensure convergence.\n        niter = 0\n        ndiff = 0\n        need_new_time_tree=False\n        while niter < max_iter:\n            self.logger(\"###TreeTime.run: ITERATION %d out of %d iterations\"%(niter+1,max_iter),0)\n            # add coalescent prior\n            if Tc:\n                if Tc=='skyline' and niter<max_iter-1:\n                    tmpTc='const'\n                else:\n                    tmpTc=Tc\n                self.add_coalescent_model(tmpTc, **kwargs)\n                need_new_time_tree = True\n\n            # estimate a relaxed molecular clock\n            if relaxed_clock:\n                print(\"relaxed_clock\", relaxed_clock)\n                self.relaxed_clock(**relaxed_clock)\n                need_new_time_tree = True\n\n            n_resolved=0\n            if resolve_polytomies:\n                # if polytomies are found, rerun the entire procedure\n                n_resolved = self.resolve_polytomies()\n                if n_resolved:\n                    self.prepare_tree()\n                    if self.branch_length_mode!='input': # otherwise reoptimize branch length while preserving branches without mutations\n                        self.optimize_sequences_and_branch_length(prune_short=False,\n                                                          max_iter=0, **seq_kwargs)\n\n                    need_new_time_tree = True\n\n            if need_new_time_tree:\n                self.make_time_tree(**tt_kwargs)\n                if self.aln:\n                    ndiff = self.infer_ancestral_sequences('ml',**seq_kwargs)\n            else: # no refinements, just iterate\n                if self.aln:\n                    ndiff = self.infer_ancestral_sequences('ml',**seq_kwargs)\n                self.make_time_tree(**tt_kwargs)\n\n            self.tree.coalescent_joint_LH = self.merger_model.total_LH() if Tc else 0.0\n\n            if self.aln:\n                seq_LH = self.tree.sequence_marginal_LH if seq_kwargs['marginal_sequences'] else self.tree.sequence_joint_LH\n            self.LH.append([seq_LH, self.tree.positional_joint_LH, self.tree.coalescent_joint_LH])\n            niter+=1\n\n            if ndiff==0 and n_resolved==0 and Tc!='skyline':\n                self.logger(\"###TreeTime.run: CONVERGED\",0)\n                break\n\n        # if the rate is too be varied and the rate estimate has a valid confidence interval\n        # rerun the estimation for variations of the rate\n        if vary_rate:\n            if type(vary_rate)==float:\n                res = self.calc_rate_susceptibility(rate_std=vary_rate, params=tt_kwargs)\n            elif self.clock_model['valid_confidence']:\n                res = self.calc_rate_susceptibility(params=tt_kwargs)\n            else:\n                res = ttconf.ERROR\n\n            if res==ttconf.ERROR:\n                self.logger(\"TreeTime.run: rate variation failed and can't be used for confidence estimation\", 1, warn=True)\n\n        # if marginal reconstruction requested, make one more round with marginal=True\n        # this will set marginal_pos_LH, which to be used as error bar estimations\n        if time_marginal:\n            self.logger(\"###TreeTime.run: FINAL ROUND - confidence estimation via marginal reconstruction\", 0)\n            tt_kwargs['time_marginal']=time_marginal\n            self.make_time_tree(**tt_kwargs)\n\n        # explicitly print out which branches are bad and whose dates don't correspond to the input dates\n        bad_branches =[n for n in self.tree.get_terminals()\n                       if n.bad_branch and n.raw_date_constraint]\n        if bad_branches:\n            self.logger(\"TreeTime: The following tips don't fit the clock model, \"\n                        \"please remove them from the tree. Their dates have been reset:\",0,warn=True)\n            for n in bad_branches:\n                self.logger(\"%s, input date: %s, apparent date: %1.2f\"%(n.name, str(n.raw_date_constraint), n.numdate),0,warn=True)\n\n        return ttconf.SUCCESS", "response": "This method is used to run the TreeTime reconstruction on a given tree."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _set_branch_length_mode(self, branch_length_mode):\n        '''\n        if branch_length mode is not explicitly set, set according to\n        empirical branch length distribution in input tree\n\n        Parameters\n        ----------\n\n         branch_length_mode : str, 'input', 'joint', 'marginal'\n            if the maximal branch length in the tree is longer than 0.05, this will\n            default to 'input'. Otherwise set to 'joint'\n        '''\n        if branch_length_mode in ['joint', 'marginal', 'input']:\n            self.branch_length_mode = branch_length_mode\n        elif self.aln:\n            bl_dis = [n.branch_length for n in self.tree.find_clades() if n.up]\n            max_bl = np.max(bl_dis)\n            if max_bl>0.1:\n                bl_mode = 'input'\n            else:\n                bl_mode = 'joint'\n            self.logger(\"TreeTime._set_branch_length_mode: maximum branch length is %1.3e, using branch length mode %s\"%(max_bl, bl_mode),1)\n            self.branch_length_mode = bl_mode\n        else:\n            self.branch_length_mode = 'input'", "response": "Set the branch length mode according to the empirical branch length distribution in input tree."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clock_filter(self, reroot='least-squares', n_iqd=None, plot=False):\n        '''\n        Labels outlier branches that don't seem to follow a molecular clock\n        and excludes them from subsequent molecular clock estimation and\n        the timetree propagation.\n\n        Parameters\n        ----------\n         reroot : str\n            Method to find the best root in the tree (see :py:meth:`treetime.TreeTime.reroot` for options)\n\n         n_iqd : int\n            Number of iqd intervals. The outlier nodes are those which do not fall\n            into :math:`IQD\\cdot n_iqd` interval (:math:`IQD` is the interval between\n            75\\ :sup:`th` and 25\\ :sup:`th` percentiles)\n\n            If None, the default (3) assumed\n\n         plot : bool\n            If True, plot the results\n\n        '''\n        if n_iqd is None:\n            n_iqd = ttconf.NIQD\n        if type(reroot) is list and len(reroot)==1:\n            reroot=str(reroot[0])\n\n        terminals = self.tree.get_terminals()\n        if reroot:\n            if self.reroot(root='least-squares' if reroot=='best' else reroot, covariation=False)==ttconf.ERROR:\n                return ttconf.ERROR\n        else:\n            self.get_clock_model(covariation=False)\n\n        clock_rate = self.clock_model['slope']\n        icpt = self.clock_model['intercept']\n        res = {}\n        for node in terminals:\n            if hasattr(node, 'raw_date_constraint') and  (node.raw_date_constraint is not None):\n                res[node] = node.dist2root - clock_rate*np.mean(node.raw_date_constraint) - icpt\n\n        residuals = np.array(list(res.values()))\n        iqd = np.percentile(residuals,75) - np.percentile(residuals,25)\n        for node,r in res.items():\n            if abs(r)>n_iqd*iqd and node.up.up is not None:\n                self.logger('TreeTime.ClockFilter: marking %s as outlier, residual %f interquartile distances'%(node.name,r/iqd), 3, warn=True)\n                node.bad_branch=True\n            else:\n                node.bad_branch=False\n\n        # redo root estimation after outlier removal\n        if reroot and self.reroot(root=reroot)==ttconf.ERROR:\n                return ttconf.ERROR\n\n        if plot:\n            self.plot_root_to_tip()\n\n        return ttconf.SUCCESS", "response": "This method filters outlier branches that do not follow a molecular clock and excludes them from subsequent molecular clock estimation and the timetree propagation."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nplotting the root - to - tip regression of the current node.", "response": "def plot_root_to_tip(self, add_internal=False, label=True, ax=None):\n        \"\"\"\n        Plot root-to-tip regression\n\n        Parameters\n        ----------\n        add_internal : bool\n           If true, plot inte`rnal node positions\n\n        label : bool\n           If true, label the plots\n\n        ax : matplotlib axes\n           If not None, use the provided matplotlib axes to plot the results\n        \"\"\"\n        Treg = self.setup_TreeRegression()\n        if self.clock_model and 'cov' in self.clock_model:\n            cf = self.clock_model['valid_confidence']\n        else:\n            cf = False\n        Treg.clock_plot(ax=ax, add_internal=add_internal, confidence=cf, n_sigma=2,\n                        regression=self.clock_model)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfinding best root and re-root the tree to the new root Parameters ---------- root : str Which method should be used to find the best root. Available methods are: :code:`best`, `least-squares` - minimize squared residual or likelihood of root-to-tip regression :code:`min_dev` - minimize variation of root-to-tip distance :code:`oldest` - reroot on the oldest node :code:`<node_name>` - reroot to the node with name :code:`<node_name>` :code:`[<node_name1>, <node_name2>, ...]` - reroot to the MRCA of these nodes force_positive : bool only consider positive rates when searching for the optimal root covariation : bool account for covariation in root-to-tip regression", "response": "def reroot(self, root='least-squares', force_positive=True, covariation=None):\n        \"\"\"\n        Find best root and re-root the tree to the new root\n\n        Parameters\n        ----------\n\n         root : str\n            Which method should be used to find the best root. Available methods are:\n\n            :code:`best`, `least-squares` - minimize squared residual or likelihood of root-to-tip regression\n\n            :code:`min_dev` - minimize variation of root-to-tip distance\n\n            :code:`oldest` - reroot on the oldest node\n\n            :code:`<node_name>` - reroot to the node with name :code:`<node_name>`\n\n            :code:`[<node_name1>, <node_name2>, ...]` - reroot to the MRCA of these nodes\n\n          force_positive : bool\n            only consider positive rates when searching for the optimal root\n\n          covariation : bool\n             account for covariation in root-to-tip regression\n        \"\"\"\n        if type(root) is list and len(root)==1:\n            root=str(root[0])\n\n        if root=='best':\n            root='least-squares'\n\n        use_cov = self.use_covariation if covariation is None else covariation\n\n        self.logger(\"TreeTime.reroot: with method or node: %s\"%root,0)\n        for n in self.tree.find_clades():\n            n.branch_length=n.mutation_length\n\n        if (type(root) is str) and \\\n           (root in rerooting_mechanisms or root in deprecated_rerooting_mechanisms):\n            if root in deprecated_rerooting_mechanisms:\n                if \"ML\" in root:\n                    use_cov=True\n                self.logger('TreeTime.reroot: rerooting mechanisms %s has been renamed to %s'\n                             %(root, deprecated_rerooting_mechanisms[root]), 1, warn=True)\n                root = deprecated_rerooting_mechanisms[root]\n\n            self.logger(\"TreeTime.reroot: rerooting will %s covariance and shared ancestry.\"%(\"account for\" if use_cov else \"ignore\"),0)\n            new_root = self._find_best_root(covariation=use_cov,\n                                            slope = 0.0 if root.startswith('min_dev') else None,\n                                            force_positive=force_positive and (not root.startswith('min_dev')))\n        else:\n            if isinstance(root,Phylo.BaseTree.Clade):\n                new_root = root\n            elif isinstance(root, list):\n                new_root = self.tree.common_ancestor(*root)\n            elif root in self._leaves_lookup:\n                new_root = self._leaves_lookup[root]\n            elif root=='oldest':\n                new_root = sorted([n for n in self.tree.get_terminals()\n                                   if n.raw_date_constraint is not None],\n                                   key=lambda x:np.mean(x.raw_date_constraint))[0]\n            else:\n                self.logger('TreeTime.reroot -- ERROR: unsupported rooting mechanisms or root not found',0,warn=True)\n                return ttconf.ERROR\n\n            #this forces a bifurcating root, as we want. Branch lengths will be reoptimized anyway.\n            #(Without outgroup_branch_length, gives a trifurcating root, but this will mean\n            #mutations may have to occur multiple times.)\n            self.tree.root_with_outgroup(new_root, outgroup_branch_length=new_root.branch_length/2)\n            self.get_clock_model(covariation=use_cov)\n\n\n        if new_root == ttconf.ERROR:\n            return ttconf.ERROR\n\n        self.logger(\"TreeTime.reroot: Tree was re-rooted to node \"\n                    +('new_node' if new_root.name is None else new_root.name), 2)\n\n        self.tree.root.branch_length = self.one_mutation\n        self.tree.root.clock_length = self.one_mutation\n        self.tree.root.raw_date_constraint = None\n        if hasattr(new_root, 'time_before_present'):\n            self.tree.root.time_before_present = new_root.time_before_present\n        if hasattr(new_root, 'numdate'):\n            self.tree.root.numdate = new_root.numdate\n        # set root.gamma bc root doesn't have a branch_length_interpolator but gamma is needed\n        if not hasattr(self.tree.root, 'gamma'):\n            self.tree.root.gamma = 1.0\n        for n in self.tree.find_clades():\n            n.mutation_length = n.branch_length\n            if not hasattr(n, 'clock_length'):\n                n.clock_length = n.branch_length\n        self.prepare_tree()\n\n        self.get_clock_model(covariation=self.use_covariation)\n\n        return ttconf.SUCCESS"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef resolve_polytomies(self, merge_compressed=False):\n        self.logger(\"TreeTime.resolve_polytomies: resolving multiple mergers...\",1)\n        poly_found=0\n\n        for n in self.tree.find_clades():\n            if len(n.clades) > 2:\n                prior_n_clades = len(n.clades)\n                self._poly(n, merge_compressed)\n                poly_found+=prior_n_clades - len(n.clades)\n\n        obsolete_nodes = [n for n in self.tree.find_clades() if len(n.clades)==1 and n.up is not None]\n        for node in obsolete_nodes:\n            self.logger('TreeTime.resolve_polytomies: remove obsolete node '+node.name,4)\n            if node.up is not None:\n                self.tree.collapse(node)\n\n        if poly_found:\n            self.logger('TreeTime.resolve_polytomies: introduces %d new nodes'%poly_found,3)\n        else:\n            self.logger('TreeTime.resolve_polytomies: No more polytomies to resolve',3)\n        return poly_found", "response": "Resolves the polytomies on the tree."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _poly(self, clade, merge_compressed):\n\n        \"\"\"\n        Function to resolve polytomies for a given parent node. If the\n        number of the direct decendants is less than three (not a polytomy), does\n        nothing. Otherwise, for each pair of nodes, assess the possible LH increase\n        which could be gained by merging the two nodes. The increase in the LH is\n        basically the tradeoff between the gain of the LH due to the changing the\n        branch lenghts towards the optimal values and the decrease due to the\n        introduction of the new branch with zero optimal length.\n        \"\"\"\n\n        from .branch_len_interpolator import BranchLenInterpolator\n\n        zero_branch_slope = self.gtr.mu*self.seq_len\n\n        def _c_gain(t, n1, n2, parent):\n            \"\"\"\n            cost gain if nodes n1, n2 are joined and their parent is placed at time t\n            cost gain = (LH loss now) - (LH loss when placed at time t)\n            \"\"\"\n            cg2 = n2.branch_length_interpolator(parent.time_before_present - n2.time_before_present) - n2.branch_length_interpolator(t - n2.time_before_present)\n            cg1 = n1.branch_length_interpolator(parent.time_before_present - n1.time_before_present) - n1.branch_length_interpolator(t - n1.time_before_present)\n            cg_new = - zero_branch_slope * (parent.time_before_present - t) # loss in LH due to the new branch\n            return -(cg2+cg1+cg_new)\n\n        def cost_gain(n1, n2, parent):\n            \"\"\"\n            cost gained if the two nodes would have been connected.\n            \"\"\"\n            try:\n                cg = sciopt.minimize_scalar(_c_gain,\n                    bounds=[max(n1.time_before_present,n2.time_before_present), parent.time_before_present],\n                    method='Bounded',args=(n1,n2, parent))\n                return cg['x'], - cg['fun']\n            except:\n                self.logger(\"TreeTime._poly.cost_gain: optimization of gain failed\", 3, warn=True)\n                return parent.time_before_present, 0.0\n\n\n        def merge_nodes(source_arr, isall=False):\n            mergers = np.array([[cost_gain(n1,n2, clade) if i1<i2 else (0.0,-1.0)\n                                    for i1,n1 in enumerate(source_arr)]\n                                for i2, n2 in enumerate(source_arr)])\n            LH = 0\n            while len(source_arr) > 1 + int(isall):\n                # max possible gains of the cost when connecting the nodes:\n                # this is only a rough approximation because it assumes the new node positions\n                # to be optimal\n                new_positions = mergers[:,:,0]\n                cost_gains = mergers[:,:,1]\n                # set zero to large negative value and find optimal pair\n                np.fill_diagonal(cost_gains, -1e11)\n                idxs = np.unravel_index(cost_gains.argmax(),cost_gains.shape)\n                if (idxs[0] == idxs[1]) or cost_gains.max()<0:\n                    self.logger(\"TreeTime._poly.merge_nodes: node is not fully resolved \"+clade.name,4)\n                    return LH\n\n                n1, n2 = source_arr[idxs[0]], source_arr[idxs[1]]\n                LH += cost_gains[idxs]\n\n                new_node = Phylo.BaseTree.Clade()\n\n                # fix positions and branch lengths\n                new_node.time_before_present = new_positions[idxs]\n                new_node.branch_length = clade.time_before_present - new_node.time_before_present\n                new_node.clades = [n1,n2]\n                n1.branch_length = new_node.time_before_present - n1.time_before_present\n                n2.branch_length = new_node.time_before_present - n2.time_before_present\n\n                # set parameters for the new node\n                new_node.up = clade\n                n1.up = new_node\n                n2.up = new_node\n                if hasattr(clade, \"cseq\"):\n                    new_node.cseq = clade.cseq\n                    self._store_compressed_sequence_to_node(new_node)\n\n                new_node.mutations = []\n                new_node.mutation_length = 0.0\n                new_node.branch_length_interpolator = BranchLenInterpolator(new_node, self.gtr, one_mutation=self.one_mutation,\n                                                                            branch_length_mode = self.branch_length_mode)\n                clade.clades.remove(n1)\n                clade.clades.remove(n2)\n                clade.clades.append(new_node)\n                self.logger('TreeTime._poly.merge_nodes: creating new node as child of '+clade.name,3)\n                self.logger(\"TreeTime._poly.merge_nodes: Delta-LH = \" + str(cost_gains[idxs].round(3)), 3)\n\n                # and modify source_arr array for the next loop\n                if len(source_arr)>2: # if more than 3 nodes in polytomy, replace row/column\n                    for ii in np.sort(idxs)[::-1]:\n                        tmp_ind = np.arange(mergers.shape[0])!=ii\n                        mergers = mergers[tmp_ind].swapaxes(0,1)\n                        mergers = mergers[tmp_ind].swapaxes(0,1)\n\n                    source_arr.remove(n1)\n                    source_arr.remove(n2)\n                    new_gains = np.array([[cost_gain(n1,new_node, clade) for n1 in source_arr]])\n                    mergers = np.vstack((mergers, new_gains)).swapaxes(0,1)\n\n                    source_arr.append(new_node)\n                    new_gains = np.array([[cost_gain(n1,new_node, clade) for n1 in source_arr]])\n                    mergers = np.vstack((mergers, new_gains)).swapaxes(0,1)\n                else: # otherwise just recalculate matrix\n                    source_arr.remove(n1)\n                    source_arr.remove(n2)\n                    source_arr.append(new_node)\n                    mergers = np.array([[cost_gain(n1,n2, clade) for n1 in source_arr]\n                                       for n2 in source_arr])\n\n            return LH\n\n        stretched = [c for c  in clade.clades if c.mutation_length < c.clock_length]\n        compressed = [c for c in clade.clades if c not in stretched]\n\n        if len(stretched)==1 and merge_compressed is False:\n            return 0.0\n\n        LH = merge_nodes(stretched, isall=len(stretched)==len(clade.clades))\n        if merge_compressed and len(compressed)>1:\n            LH += merge_nodes(compressed, isall=len(compressed)==len(clade.clades))\n\n        return LH", "response": "Function to resolve the polytomies for a given parent node."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprints the total likelihood of the tree given the constrained leaves and the constrained leaves.", "response": "def print_lh(self, joint=True):\n        \"\"\"\n        Print the total likelihood of the tree given the constrained leaves\n\n        Parameters\n        ----------\n\n         joint : bool\n            If true, print joint LH, else print marginal LH\n\n        \"\"\"\n        try:\n            u_lh = self.tree.unconstrained_sequence_LH\n            if joint:\n                s_lh = self.tree.sequence_joint_LH\n                t_lh = self.tree.positional_joint_LH\n                c_lh = self.tree.coalescent_joint_LH\n            else:\n                s_lh = self.tree.sequence_marginal_LH\n                t_lh = self.tree.positional_marginal_LH\n                c_lh = 0\n\n            print (\"###  Tree Log-Likelihood  ###\\n\"\n                \" Sequence log-LH without constraints: \\t%1.3f\\n\"\n                \" Sequence log-LH with constraints:    \\t%1.3f\\n\"\n                \" TreeTime sequence log-LH:            \\t%1.3f\\n\"\n                \" Coalescent log-LH:                   \\t%1.3f\\n\"\n               \"#########################\"%(u_lh, s_lh,t_lh, c_lh))\n        except:\n            print(\"ERROR. Did you run the corresponding inference (joint/marginal)?\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a coalescent model to the tree and optionally optimze the coalescent model with the specified time scale.", "response": "def add_coalescent_model(self, Tc, **kwargs):\n        \"\"\"Add a coalescent model to the tree and optionally optimze\n\n        Parameters\n        ----------\n        Tc : float,str\n            If this is a float, it will be interpreted as the inverse merger\n            rate in molecular clock units, if its is a\n        \"\"\"\n        from .merger_models import Coalescent\n        self.logger('TreeTime.run: adding coalescent prior with Tc='+str(Tc),1)\n        self.merger_model = Coalescent(self.tree,\n                                       date2dist=self.date2dist, logger=self.logger)\n\n        if Tc=='skyline': # restrict skyline model optimization to last iteration\n            self.merger_model.optimize_skyline(**kwargs)\n            self.logger(\"optimized a skyline \", 2)\n        else:\n            if Tc in ['opt', 'const']:\n                self.merger_model.optimize_Tc()\n                self.logger(\"optimized Tc to %f\"%self.merger_model.Tc.y[0], 2)\n            else:\n                try:\n                    self.merger_model.set_Tc(Tc)\n                except:\n                    self.logger(\"setting of coalescent time scale failed\", 1, warn=True)\n\n        self.merger_model.attach_to_tree()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nallowing the mutation rate to vary on the tree (relaxed molecular clock). Changes of the mutation rates from one branch to another are penalized. In addition, deviation of the mutation rate from the mean rate is penalized. Parameters ---------- slack : float Maximum change in substitution rate between parent and child nodes coupling : float Maximum difference in substitution rates in sibling nodes", "response": "def relaxed_clock(self, slack=None, coupling=None, **kwargs):\n        \"\"\"\n        Allow the mutation rate to vary on the tree (relaxed molecular clock).\n        Changes of the mutation rates from one branch to another are penalized.\n        In addition, deviation of the mutation rate from the mean rate is\n        penalized.\n\n        Parameters\n        ----------\n         slack : float\n            Maximum change in substitution rate between parent and child nodes\n\n         coupling : float\n            Maximum difference in substitution rates in sibling nodes\n\n        \"\"\"\n        if slack is None: slack=ttconf.MU_ALPHA\n        if coupling is None: coupling=ttconf.MU_BETA\n        self.logger(\"TreeTime.relaxed_clock: slack=%f, coupling=%f\"%(slack, coupling),2)\n\n        c=1.0/self.one_mutation\n        for node in self.tree.find_clades(order='postorder'):\n            opt_len = node.mutation_length\n            act_len = node.clock_length if hasattr(node, 'clock_length') else node.branch_length\n\n            # opt_len \\approx 1.0*len(node.mutations)/node.profile.shape[0] but calculated via gtr model\n            # stiffness is the expectation of the inverse variance of branch length (one_mutation/opt_len)\n            # contact term: stiffness*(g*bl - bl_opt)^2 + slack(g-1)^2 =\n            #               (slack+bl^2) g^2 - 2 (bl*bl_opt+1) g + C= k2 g^2 + k1 g + C\n            node._k2 = slack + c*act_len**2/(opt_len+self.one_mutation)\n            node._k1 = -2*(c*act_len*opt_len/(opt_len+self.one_mutation) + slack)\n            # coupling term: \\sum_c coupling*(g-g_c)^2 + Cost_c(g_c|g)\n            # given g, g_c needs to be optimal-> 2*coupling*(g-g_c) = 2*child.k2 g_c  + child.k1\n            # hence g_c = (coupling*g - 0.5*child.k1)/(coupling+child.k2)\n            # substituting yields\n            for child in node.clades:\n                denom = coupling+child._k2\n                node._k2 += coupling*(1.0-coupling/denom)**2 + child._k2*coupling**2/denom**2\n                node._k1 += (coupling*(1.0-coupling/denom)*child._k1/denom \\\n                            - coupling*child._k1*child._k2/denom**2 \\\n                            + coupling*child._k1/denom)\n\n        for node in self.tree.find_clades(order='preorder'):\n            if node.up is None:\n                node.gamma = max(0.1, -0.5*node._k1/node._k2)\n            else:\n                if node.up.up is None:\n                    g_up = node.up.gamma\n                else:\n                    g_up = node.up.branch_length_interpolator.gamma\n                node.branch_length_interpolator.gamma = max(0.1,(coupling*g_up - 0.5*node._k1)/(coupling+node._k2))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _find_best_root(self, covariation=True, force_positive=True, slope=0, **kwarks):\n        '''\n        Determine the node that, when the tree is rooted on this node, results\n        in the best regression of temporal constraints and root to tip distances.\n\n        Parameters\n        ----------\n\n         infer_gtr : bool\n            If True, infer new GTR model after re-root\n\n         covariation : bool\n            account for covariation structure when rerooting the tree\n\n         force_positive : bool\n            only accept positive evolutionary rate estimates when rerooting the tree\n\n        '''\n        for n in self.tree.find_clades():\n            n.branch_length=n.mutation_length\n        self.logger(\"TreeTime._find_best_root: searching for the best root position...\",2)\n        Treg = self.setup_TreeRegression(covariation=covariation)\n        return Treg.optimal_reroot(force_positive=force_positive, slope=slope)['node']", "response": "Find the best root node in the tree."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfunction that attempts to load a tree and build it from the alignment", "response": "def assure_tree(params, tmp_dir='treetime_tmp'):\n    \"\"\"\n    Function that attempts to load a tree and build it from the alignment\n    if no tree is provided.\n    \"\"\"\n    if params.tree is None:\n        params.tree = os.path.basename(params.aln)+'.nwk'\n        print(\"No tree given: inferring tree\")\n        utils.tree_inference(params.aln, params.tree, tmp_dir = tmp_dir)\n\n    if os.path.isdir(tmp_dir):\n        shutil.rmtree(tmp_dir)\n\n    try:\n        tt = TreeAnc(params.tree)\n    except:\n        print(\"Tree loading/building failed.\")\n        return 1\n    return 0"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a GTR model from the input arguments.", "response": "def create_gtr(params):\n    \"\"\"\n    parse the arguments referring to the GTR model and return a GTR structure\n    \"\"\"\n    model = params.gtr\n    gtr_params = params.gtr_params\n    if model == 'infer':\n        gtr = GTR.standard('jc', alphabet='aa' if params.aa else 'nuc')\n    else:\n        try:\n            kwargs = {}\n            if gtr_params is not None:\n                for param in gtr_params:\n                    keyval = param.split('=')\n                    if len(keyval)!=2: continue\n                    if keyval[0] in ['pis', 'pi', 'Pi', 'Pis']:\n                        keyval[0] = 'pi'\n                        keyval[1] = list(map(float, keyval[1].split(',')))\n                    elif keyval[0] not in ['alphabet']:\n                        keyval[1] = float(keyval[1])\n                    kwargs[keyval[0]] = keyval[1]\n            else:\n                print (\"GTR params are not specified. Creating GTR model with default parameters\")\n\n            gtr = GTR.standard(model, **kwargs)\n            infer_gtr = False\n        except:\n            print (\"Could not create GTR model from input arguments. Using default (Jukes-Cantor 1969)\")\n            gtr = GTR.standard('jc', alphabet='aa' if params.aa else 'nuc')\n            infer_gtr = False\n    return gtr"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_if_vcf(params):\n    ref = None\n    aln = params.aln\n    fixed_pi = None\n    if hasattr(params, 'aln') and params.aln is not None:\n        if any([params.aln.lower().endswith(x) for x in ['.vcf', '.vcf.gz']]):\n            if not params.vcf_reference:\n                print(\"ERROR: a reference Fasta is required with VCF-format alignments\")\n                return -1\n            compress_seq = read_vcf(params.aln, params.vcf_reference)\n            sequences = compress_seq['sequences']\n            ref = compress_seq['reference']\n            aln = sequences\n\n            if not hasattr(params, 'gtr') or params.gtr==\"infer\": #if not specified, set it:\n                alpha = alphabets['aa'] if params.aa else alphabets['nuc']\n                fixed_pi = [ref.count(base)/len(ref) for base in alpha]\n                if fixed_pi[-1] == 0:\n                    fixed_pi[-1] = 0.05\n                    fixed_pi = [v-0.01 for v in fixed_pi]\n\n    return aln, ref, fixed_pi", "response": "Checks if input is VCF and reads in appropriately if it is\n           "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef scan_homoplasies(params):\n    if assure_tree(params, tmp_dir='homoplasy_tmp'):\n        return 1\n\n    gtr = create_gtr(params)\n\n    ###########################################################################\n    ### READ IN VCF\n    ###########################################################################\n    #sets ref and fixed_pi to None if not VCF\n    aln, ref, fixed_pi = read_if_vcf(params)\n    is_vcf = True if ref is not None else False\n\n    ###########################################################################\n    ### ANCESTRAL RECONSTRUCTION\n    ###########################################################################\n    treeanc = TreeAnc(params.tree, aln=aln, ref=ref, gtr=gtr, verbose=1,\n                      fill_overhangs=True)\n    if treeanc.aln is None: # if alignment didn't load, exit\n        return 1\n\n    if is_vcf:\n        L = len(ref) + params.const\n    else:\n        L = treeanc.aln.get_alignment_length() + params.const\n\n    N_seq = len(treeanc.aln)\n    N_tree = treeanc.tree.count_terminals()\n    if params.rescale!=1.0:\n        for n in treeanc.tree.find_clades():\n            n.branch_length *= params.rescale\n            n.mutation_length = n.branch_length\n\n    print(\"read alignment from file %s with %d sequences of length %d\"%(params.aln,N_seq,L))\n    print(\"read tree from file %s with %d leaves\"%(params.tree,N_tree))\n    print(\"\\ninferring ancestral sequences...\")\n\n    ndiff = treeanc.infer_ancestral_sequences('ml', infer_gtr=params.gtr=='infer',\n                                      marginal=False, fixed_pi=fixed_pi)\n    print(\"...done.\")\n    if ndiff==ttconf.ERROR: # if reconstruction failed, exit\n        print(\"Something went wrong during ancestral reconstruction, please check your input files.\", file=sys.stderr)\n        return 1\n    else:\n        print(\"...done.\")\n\n    if is_vcf:\n        treeanc.recover_var_ambigs()\n\n    ###########################################################################\n    ### analysis of reconstruction\n    ###########################################################################\n    from collections import defaultdict\n    from scipy.stats import poisson\n    offset = 0 if params.zero_based else 1\n\n    if params.drms:\n        DRM_info = read_in_DRMs(params.drms, offset)\n        drms = DRM_info['DRMs']\n\n    # construct dictionaries gathering mutations and positions\n    mutations = defaultdict(list)\n    positions = defaultdict(list)\n    terminal_mutations = defaultdict(list)\n    for n in treeanc.tree.find_clades():\n        if n.up is None:\n            continue\n\n        if len(n.mutations):\n            for (a,pos, d) in n.mutations:\n                if '-' not in [a,d] and 'N' not in [a,d]:\n                    mutations[(a,pos+offset,d)].append(n)\n                    positions[pos+offset].append(n)\n            if n.is_terminal():\n                for (a,pos, d) in n.mutations:\n                    if '-' not in [a,d] and 'N' not in [a,d]:\n                        terminal_mutations[(a,pos+offset,d)].append(n)\n\n    # gather homoplasic mutations by strain\n    mutation_by_strain = defaultdict(list)\n    for n in treeanc.tree.get_terminals():\n        for a,pos,d in n.mutations:\n            if pos+offset in positions and len(positions[pos+offset])>1:\n                if '-' not in [a,d] and 'N' not in [a,d]:\n                    mutation_by_strain[n.name].append([(a,pos+offset,d), len(positions[pos])])\n\n\n    # total_branch_length is the expected number of substitutions\n    # corrected_branch_length is the expected number of observable substitutions\n    # (probability of an odd number of substitutions at a particular site)\n    total_branch_length = treeanc.tree.total_branch_length()\n    corrected_branch_length = np.sum([np.exp(-x.branch_length)*np.sinh(x.branch_length)\n                                      for x in treeanc.tree.find_clades()])\n    corrected_terminal_branch_length = np.sum([np.exp(-x.branch_length)*np.sinh(x.branch_length)\n                                      for x in treeanc.tree.get_terminals()])\n    expected_mutations = L*corrected_branch_length\n    expected_terminal_mutations = L*corrected_terminal_branch_length\n\n    # make histograms and sum mutations in different categories\n    multiplicities = np.bincount([len(x) for x in mutations.values()])\n    total_mutations = np.sum([len(x) for x in mutations.values()])\n\n    multiplicities_terminal = np.bincount([len(x) for x in terminal_mutations.values()])\n    terminal_mutation_count = np.sum([len(x) for x in terminal_mutations.values()])\n\n    multiplicities_positions = np.bincount([len(x) for x in positions.values()])\n    multiplicities_positions[0] = L - np.sum(multiplicities_positions)\n\n    ###########################################################################\n    ### Output the distribution of times particular mutations are observed\n    ###########################################################################\n    print(\"\\nThe TOTAL tree length is %1.3e and %d mutations were observed.\"\n          %(total_branch_length,total_mutations))\n    print(\"Of these %d mutations,\"%total_mutations\n            +\"\".join(['\\n\\t - %d occur %d times'%(n,mi)\n                      for mi,n in enumerate(multiplicities) if n]))\n    # additional optional output this for terminal mutations only\n    if params.detailed:\n        print(\"\\nThe TERMINAL branch length is %1.3e and %d mutations were observed.\"\n              %(corrected_terminal_branch_length,terminal_mutation_count))\n        print(\"Of these %d mutations,\"%terminal_mutation_count\n                +\"\".join(['\\n\\t - %d occur %d times'%(n,mi)\n                          for mi,n in enumerate(multiplicities_terminal) if n]))\n\n\n    ###########################################################################\n    ### Output the distribution of times mutations at particular positions are observed\n    ###########################################################################\n    print(\"\\nOf the %d positions in the genome,\"%L\n            +\"\".join(['\\n\\t - %d were hit %d times (expected %1.2f)'%(n,mi,L*poisson.pmf(mi,1.0*total_mutations/L))\n                      for mi,n in enumerate(multiplicities_positions) if n]))\n\n\n    # compare that distribution to a Poisson distribution with the same mean\n    p = poisson.pmf(np.arange(10*multiplicities_positions.max()),1.0*total_mutations/L)\n    print(\"\\nlog-likelihood difference to Poisson distribution with same mean: %1.3e\"%(\n            - L*np.sum(p*np.log(p+1e-100))\n            + np.sum(multiplicities_positions*np.log(p[:len(multiplicities_positions)]+1e-100))))\n\n\n    ###########################################################################\n    ### Output the mutations that are observed most often\n    ###########################################################################\n    if params.drms:\n        print(\"\\n\\nThe ten most homoplasic mutations are:\\n\\tmut\\tmultiplicity\\tDRM details (gene drug AAmut)\")\n        mutations_sorted = sorted(mutations.items(), key=lambda x:len(x[1])-0.1*x[0][1]/L, reverse=True)\n        for mut, val in mutations_sorted[:params.n]:\n            if len(val)>1:\n                print(\"\\t%s%d%s\\t%d\\t%s\"%(mut[0], mut[1], mut[2], len(val),\n                    \" \".join([drms[mut[1]]['gene'], drms[mut[1]]['drug'], drms[mut[1]]['alt_base'][mut[2]]]) if mut[1] in drms else \"\"))\n            else:\n                break\n    else:\n        print(\"\\n\\nThe ten most homoplasic mutations are:\\n\\tmut\\tmultiplicity\")\n        mutations_sorted = sorted(mutations.items(), key=lambda x:len(x[1])-0.1*x[0][1]/L, reverse=True)\n        for mut, val in mutations_sorted[:params.n]:\n            if len(val)>1:\n                print(\"\\t%s%d%s\\t%d\"%(mut[0], mut[1], mut[2], len(val)))\n            else:\n                break\n\n    # optional output specifically for mutations on terminal branches\n    if params.detailed:\n        if params.drms:\n            print(\"\\n\\nThe ten most homoplasic mutation on terminal branches are:\\n\\tmut\\tmultiplicity\\tDRM details (gene drug AAmut)\")\n            terminal_mutations_sorted = sorted(terminal_mutations.items(), key=lambda x:len(x[1])-0.1*x[0][1]/L, reverse=True)\n            for mut, val in terminal_mutations_sorted[:params.n]:\n                if len(val)>1:\n                    print(\"\\t%s%d%s\\t%d\\t%s\"%(mut[0], mut[1], mut[2], len(val),\n                        \" \".join([drms[mut[1]]['gene'], drms[mut[1]]['drug'], drms[mut[1]]['alt_base'][mut[2]]]) if mut[1] in drms else \"\"))\n                else:\n                    break\n        else:\n            print(\"\\n\\nThe ten most homoplasic mutation on terminal branches are:\\n\\tmut\\tmultiplicity\")\n            terminal_mutations_sorted = sorted(terminal_mutations.items(), key=lambda x:len(x[1])-0.1*x[0][1]/L, reverse=True)\n            for mut, val in terminal_mutations_sorted[:params.n]:\n                if len(val)>1:\n                    print(\"\\t%s%d%s\\t%d\"%(mut[0], mut[1], mut[2], len(val)))\n                else:\n                    break\n\n    ###########################################################################\n    ### Output strains that have many homoplasic mutations\n    ###########################################################################\n    # TODO: add statistical criterion\n    if params.detailed:\n        if params.drms:\n            print(\"\\n\\nTaxons that carry positions that mutated elsewhere in the tree:\\n\\ttaxon name\\t#of homoplasic mutations\\t# DRM\")\n            mutation_by_strain_sorted = sorted(mutation_by_strain.items(), key=lambda x:len(x[1]), reverse=True)\n            for name, val in mutation_by_strain_sorted[:params.n]:\n                if len(val):\n                    print(\"\\t%s\\t%d\\t%d\"%(name, len(val),\n                        len([mut for mut,l in val if mut[1] in drms])))\n        else:\n            print(\"\\n\\nTaxons that carry positions that mutated elsewhere in the tree:\\n\\ttaxon name\\t#of homoplasic mutations\")\n            mutation_by_strain_sorted = sorted(mutation_by_strain.items(), key=lambda x:len(x[1]), reverse=True)\n            for name, val in mutation_by_strain_sorted[:params.n]:\n                if len(val):\n                    print(\"\\t%s\\t%d\"%(name, len(val)))\n\n\n    return 0", "response": "function implementing treetime homoplasies\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreconstructs the ancestral tree", "response": "def ancestral_reconstruction(params):\n    \"\"\"\n    implementing treetime ancestral\n    \"\"\"\n\n    # set up\n    if assure_tree(params, tmp_dir='ancestral_tmp'):\n        return 1\n\n    outdir = get_outdir(params, '_ancestral')\n    basename = get_basename(params, outdir)\n\n    gtr = create_gtr(params)\n\n    ###########################################################################\n    ### READ IN VCF\n    ###########################################################################\n    #sets ref and fixed_pi to None if not VCF\n    aln, ref, fixed_pi = read_if_vcf(params)\n    is_vcf = True if ref is not None else False\n\n    treeanc = TreeAnc(params.tree, aln=aln, ref=ref, gtr=gtr, verbose=1,\n                      fill_overhangs=not params.keep_overhangs)\n    ndiff =treeanc.infer_ancestral_sequences('ml', infer_gtr=params.gtr=='infer',\n                                             marginal=params.marginal, fixed_pi=fixed_pi)\n    if ndiff==ttconf.ERROR: # if reconstruction failed, exit\n        return 1\n\n    ###########################################################################\n    ### OUTPUT and saving of results\n    ###########################################################################\n    if params.gtr==\"infer\":\n        print('\\nInferred GTR model:')\n        print(treeanc.gtr)\n\n    export_sequences_and_tree(treeanc, basename, is_vcf, params.zero_based,\n                              report_ambiguous=params.report_ambiguous)\n\n    return 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mugration(params):\n\n    ###########################################################################\n    ### Parse states\n    ###########################################################################\n    if os.path.isfile(params.states):\n        states = pd.read_csv(params.states, sep='\\t' if params.states[-3:]=='tsv' else ',',\n                             skipinitialspace=True)\n    else:\n        print(\"file with states does not exist\")\n        return 1\n\n    outdir = get_outdir(params, '_mugration')\n\n    taxon_name = 'name' if 'name' in states.columns else states.columns[0]\n    if params.attribute:\n        if params.attribute in states.columns:\n            attr = params.attribute\n        else:\n            print(\"The specified attribute was not found in the metadata file \"+params.states, file=sys.stderr)\n            print(\"Available columns are: \"+\", \".join(states.columns), file=sys.stderr)\n            return 1\n    else:\n        attr = states.columns[1]\n        print(\"Attribute for mugration inference was not specified. Using \"+attr, file=sys.stderr)\n\n    leaf_to_attr = {x[taxon_name]:x[attr] for xi, x in states.iterrows()\n                    if x[attr]!=params.missing_data}\n    unique_states = sorted(set(leaf_to_attr.values()))\n    nc = len(unique_states)\n    if nc>180:\n        print(\"mugration: can't have more than 180 states!\", file=sys.stderr)\n        return 1\n    elif nc<2:\n        print(\"mugration: only one or zero states found -- this doesn't make any sense\", file=sys.stderr)\n        return 1\n\n    ###########################################################################\n    ### make a single character alphabet that maps to discrete states\n    ###########################################################################\n    alphabet = [chr(65+i) for i,state in enumerate(unique_states)]\n    missing_char = chr(65+nc)\n    letter_to_state = {a:unique_states[i] for i,a in enumerate(alphabet)}\n    letter_to_state[missing_char]=params.missing_data\n    reverse_alphabet = {v:k for k,v in letter_to_state.items()}\n\n    ###########################################################################\n    ### construct gtr model\n    ###########################################################################\n    if params.weights:\n        params.infer_gtr = True\n        tmp_weights = pd.read_csv(params.weights, sep='\\t' if params.states[-3:]=='tsv' else ',',\n                             skipinitialspace=True)\n        weights = {row[0]:row[1] for ri,row in tmp_weights.iterrows()}\n        mean_weight = np.mean(list(weights.values()))\n        weights = np.array([weights[c] if c in weights else mean_weight for c in unique_states], dtype=float)\n        weights/=weights.sum()\n    else:\n        weights = np.ones(nc, dtype=float)/nc\n\n    # set up dummy matrix\n    W = np.ones((nc,nc), dtype=float)\n\n    mugration_GTR = GTR.custom(pi = weights, W=W, alphabet = np.array(alphabet))\n    mugration_GTR.profile_map[missing_char] = np.ones(nc)\n    mugration_GTR.ambiguous=missing_char\n\n    ###########################################################################\n    ### set up treeanc\n    ###########################################################################\n    treeanc = TreeAnc(params.tree, gtr=mugration_GTR, verbose=params.verbose,\n                      convert_upper=False, one_mutation=0.001)\n    pseudo_seqs = [SeqRecord(id=n.name,name=n.name,\n                   seq=Seq(reverse_alphabet[leaf_to_attr[n.name]]\n                           if n.name in leaf_to_attr else missing_char))\n                   for n in treeanc.tree.get_terminals()]\n    treeanc.aln = MultipleSeqAlignment(pseudo_seqs)\n\n    ndiff = treeanc.infer_ancestral_sequences(method='ml', infer_gtr=True,\n            store_compressed=False, pc=params.pc, marginal=True, normalized_rate=False,\n            fixed_pi=weights if params.weights else None)\n    if ndiff==ttconf.ERROR: # if reconstruction failed, exit\n        return 1\n\n\n    ###########################################################################\n    ### output\n    ###########################################################################\n    print(\"\\nCompleted mugration model inference of attribute '%s' for\"%attr,params.tree)\n\n    basename = get_basename(params, outdir)\n    gtr_name = basename + 'GTR.txt'\n    with open(gtr_name, 'w') as ofile:\n        ofile.write('Character to attribute mapping:\\n')\n        for state in unique_states:\n            ofile.write('  %s: %s\\n'%(reverse_alphabet[state], state))\n        ofile.write('\\n\\n'+str(treeanc.gtr)+'\\n')\n        print(\"\\nSaved inferred mugration model as:\", gtr_name)\n\n    terminal_count = 0\n    for n in treeanc.tree.find_clades():\n        if n.up is None:\n            continue\n        n.confidence=None\n        # due to a bug in older versions of biopython that truncated filenames in nexus export\n        # we truncate them by hand and make them unique.\n        if n.is_terminal() and len(n.name)>40 and bioversion<\"1.69\":\n            n.name = n.name[:35]+'_%03d'%terminal_count\n            terminal_count+=1\n        n.comment= '&%s=\"'%attr + letter_to_state[n.sequence[0]] +'\"'\n\n    if params.confidence:\n        conf_name = basename+'confidence.csv'\n        with open(conf_name, 'w') as ofile:\n            ofile.write('#name, '+', '.join(unique_states)+'\\n')\n            for n in treeanc.tree.find_clades():\n                ofile.write(n.name + ', '+', '.join([str(x) for x in n.marginal_profile[0]])+'\\n')\n        print(\"Saved table with ancestral state confidences as:\", conf_name)\n\n    # write tree to file\n    outtree_name = basename+'annotated_tree.nexus'\n    Phylo.write(treeanc.tree, outtree_name, 'nexus')\n    print(\"Saved annotated tree as:\",outtree_name)\n\n    return 0", "response": "Parse the states file and return the treetime mugration sequence."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef estimate_clock_model(params):\n\n    if assure_tree(params, tmp_dir='clock_model_tmp'):\n        return 1\n    dates = utils.parse_dates(params.dates)\n    if len(dates)==0:\n        return 1\n\n    outdir = get_outdir(params, '_clock')\n\n    ###########################################################################\n    ### READ IN VCF\n    ###########################################################################\n    #sets ref and fixed_pi to None if not VCF\n    aln, ref, fixed_pi = read_if_vcf(params)\n    is_vcf = True if ref is not None else False\n\n    ###########################################################################\n    ### ESTIMATE ROOT (if requested) AND DETERMINE TEMPORAL SIGNAL\n    ###########################################################################\n    if params.aln is None and params.sequence_length is None:\n        print(\"one of arguments '--aln' and '--sequence-length' is required.\", file=sys.stderr)\n        return 1\n\n    basename = get_basename(params, outdir)\n    myTree = TreeTime(dates=dates, tree=params.tree, aln=aln, gtr='JC69',\n                      verbose=params.verbose, seq_len=params.sequence_length,\n                      ref=ref)\n    myTree.tip_slack=params.tip_slack\n    if myTree.tree is None:\n        print(\"ERROR: tree loading failed. exiting...\")\n        return 1\n\n    if params.clock_filter:\n        n_bad = [n.name for n in myTree.tree.get_terminals() if n.bad_branch]\n        myTree.clock_filter(n_iqd=params.clock_filter, reroot=params.reroot or 'least-squares')\n        n_bad_after = [n.name for n in myTree.tree.get_terminals() if n.bad_branch]\n        if len(n_bad_after)>len(n_bad):\n            print(\"The following leaves don't follow a loose clock and \"\n                  \"will be ignored in rate estimation:\\n\\t\"\n                  +\"\\n\\t\".join(set(n_bad_after).difference(n_bad)))\n\n    if not params.keep_root:\n        # reroot to optimal root, this assigns clock_model to myTree\n        if params.covariation: # this requires branch length estimates\n            myTree.run(root=\"least-squares\", max_iter=0,\n                       use_covariation=params.covariation)\n\n        res = myTree.reroot(params.reroot,\n                      force_positive=not params.allow_negative_rate)\n        myTree.get_clock_model(covariation=params.covariation)\n\n        if res==ttconf.ERROR:\n            print(\"ERROR: unknown root or rooting mechanism!\\n\"\n                  \"\\tvalid choices are 'least-squares', 'ML', and 'ML-rough'\")\n            return 1\n    else:\n        myTree.get_clock_model(covariation=params.covariation)\n\n    d2d = utils.DateConversion.from_regression(myTree.clock_model)\n    print('\\n',d2d)\n    print('The R^2 value indicates the fraction of variation in'\n          '\\nroot-to-tip distance explained by the sampling times.'\n          '\\nHigher values corresponds more clock-like behavior (max 1.0).')\n\n    print('\\nThe rate is the slope of the best fit of the date to'\n          '\\nthe root-to-tip distance and provides an estimate of'\n          '\\nthe substitution rate. The rate needs to be positive!'\n          '\\nNegative rates suggest an inappropriate root.\\n')\n\n    print('\\nThe estimated rate and tree correspond to a root date:')\n    if params.covariation:\n        reg = myTree.clock_model\n        dp = np.array([reg['intercept']/reg['slope']**2,-1./reg['slope']])\n        droot = np.sqrt(reg['cov'][:2,:2].dot(dp).dot(dp))\n        print('\\n--- root-date:\\t %3.2f +/- %1.2f (one std-dev)\\n\\n'%(-d2d.intercept/d2d.clock_rate, droot))\n    else:\n        print('\\n--- root-date:\\t %3.2f\\n\\n'%(-d2d.intercept/d2d.clock_rate))\n\n    if not params.keep_root:\n        # write rerooted tree to file\n        outtree_name = basename+'rerooted.newick'\n        Phylo.write(myTree.tree, outtree_name, 'newick')\n        print(\"--- re-rooted tree written to \\n\\t%s\\n\"%outtree_name)\n\n    table_fname = basename+'rtt.csv'\n    with open(table_fname, 'w') as ofile:\n        ofile.write(\"#name, date, root-to-tip distance\\n\")\n        ofile.write(\"#Dates of nodes that didn't have a specified date are inferred from the root-to-tip regression.\\n\")\n        for n in myTree.tree.get_terminals():\n            if hasattr(n, \"raw_date_constraint\") and (n.raw_date_constraint is not None):\n                if np.isscalar(n.raw_date_constraint):\n                    tmp_str = str(n.raw_date_constraint)\n                elif len(n.raw_date_constraint):\n                    tmp_str = str(n.raw_date_constraint[0])+'-'+str(n.raw_date_constraint[1])\n                else:\n                    tmp_str = ''\n                ofile.write(\"%s, %s, %f\\n\"%(n.name, tmp_str, n.dist2root))\n            else:\n                ofile.write(\"%s, %f, %f\\n\"%(n.name, d2d.numdate_from_dist2root(n.dist2root), n.dist2root))\n        for n in myTree.tree.get_nonterminals(order='preorder'):\n            ofile.write(\"%s, %f, %f\\n\"%(n.name, d2d.numdate_from_dist2root(n.dist2root), n.dist2root))\n        print(\"--- wrote dates and root-to-tip distances to \\n\\t%s\\n\"%table_fname)\n\n\n    ###########################################################################\n    ### PLOT AND SAVE RESULT\n    ###########################################################################\n    plot_rtt(myTree, outdir+params.plot_rtt)\n    return 0", "response": "estimate the clock model for a single tree time"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate delta function distribution.", "response": "def delta_function(cls, x_pos, weight=1., min_width=MIN_INTEGRATION_PEAK):\n        \"\"\"\n        Create delta function distribution.\n        \"\"\"\n\n        distribution = cls(x_pos,0.,is_log=True, min_width=min_width)\n        distribution.weight  = weight\n        return distribution"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef multiply(dists):\n        '''\n        multiplies a list of Distribution objects\n        '''\n        if  not all([isinstance(k, Distribution) for k in dists]):\n            raise NotImplementedError(\"Can only multiply Distribution objects\")\n\n        n_delta = np.sum([k.is_delta for k in dists])\n        min_width = np.max([k.min_width for k in dists])\n        if n_delta>1:\n            raise ArithmeticError(\"Cannot multiply more than one delta functions!\")\n        elif n_delta==1:\n            delta_dist_ii = np.where([k.is_delta for k in dists])[0][0]\n            delta_dist = dists[delta_dist_ii]\n            new_xpos = delta_dist.peak_pos\n            new_weight  = np.prod([k.prob(new_xpos) for k in dists if k!=delta_dist_ii]) * delta_dist.weight\n            res = Distribution.delta_function(new_xpos, weight = new_weight,min_width=min_width)\n        else:\n            new_xmin = np.max([k.xmin for k in dists])\n            new_xmax = np.min([k.xmax for k in dists])\n\n            x_vals = np.unique(np.concatenate([k.x for k in dists]))\n            x_vals = x_vals[(x_vals>new_xmin-TINY_NUMBER)&(x_vals<new_xmax+TINY_NUMBER)]\n            y_vals = np.sum([k.__call__(x_vals) for k in dists], axis=0)\n            peak = y_vals.min()\n            ind = (y_vals-peak)<BIG_NUMBER/1000\n            n_points = ind.sum()\n            if n_points == 0:\n                print (\"ERROR in distribution multiplication: Distributions do not overlap\")\n                x_vals = [0,1]\n                y_vals = [BIG_NUMBER,BIG_NUMBER]\n                res = Distribution(x_vals, y_vals, is_log=True,\n                                   min_width=min_width, kind='linear')\n            elif n_points == 1:\n                res = Distribution.delta_function(x_vals[0])\n            else:\n                res = Distribution(x_vals[ind], y_vals[ind], is_log=True,\n                                   min_width=min_width, kind='linear', assume_sorted=True)\n\n        return res", "response": "multiplies a list of Distribution objects a list of Distribution objects"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nassign dates to nodes", "response": "def _assign_dates(self):\n        \"\"\"assign dates to nodes\n\n        Returns\n        -------\n        str\n            success/error code\n        \"\"\"\n        if self.tree is None:\n            self.logger(\"ClockTree._assign_dates: tree is not set, can't assign dates\", 0)\n            return ttconf.ERROR\n\n        bad_branch_counter = 0\n        for node in self.tree.find_clades(order='postorder'):\n            if node.name in self.date_dict:\n                tmp_date = self.date_dict[node.name]\n                if np.isscalar(tmp_date) and np.isnan(tmp_date):\n                    self.logger(\"WARNING: ClockTree.init: node %s has a bad date: %s\"%(node.name, str(tmp_date)), 2, warn=True)\n                    node.raw_date_constraint = None\n                    node.bad_branch = True\n                else:\n                    try:\n                        tmp = np.mean(tmp_date)\n                        node.raw_date_constraint = tmp_date\n                        node.bad_branch = False\n                    except:\n                        self.logger(\"WARNING: ClockTree.init: node %s has a bad date: %s\"%(node.name, str(tmp_date)), 2, warn=True)\n                        node.raw_date_constraint = None\n                        node.bad_branch = True\n            else: # nodes without date contraints\n\n                node.raw_date_constraint = None\n\n                if node.is_terminal():\n                    # Terminal branches without date constraints marked as 'bad'\n                    node.bad_branch = True\n                else:\n                    # If all branches dowstream are 'bad', and there is no date constraint for\n                    # this node, the branch is marked as 'bad'\n                    node.bad_branch = np.all([x.bad_branch for x in node])\n\n            if node.is_terminal() and node.bad_branch:\n                bad_branch_counter += 1\n\n        if bad_branch_counter>self.tree.count_terminals()-3:\n            self.logger(\"ERROR: ALMOST NO VALID DATE CONSTRAINTS, EXITING\", 1, warn=True)\n            return ttconf.ERROR\n\n        return ttconf.SUCCESS"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the precision of the current locale", "response": "def _set_precision(self, precision):\n        '''\n        function that sets precision to an (hopfully) reasonable guess based\n        on the length of the sequence if not explicitly set\n        '''\n        # if precision is explicitly specified, use it.\n\n        if self.one_mutation:\n            self.min_width = 10*self.one_mutation\n        else:\n            self.min_width = 0.001\n        if precision in [0,1,2,3]:\n            self.precision=precision\n            if self.one_mutation and self.one_mutation<1e-4 and precision<2:\n                self.logger(\"ClockTree._set_precision: FOR LONG SEQUENCES (>1e4) precision>=2 IS RECOMMENDED.\"\n                            \" \\n\\t **** precision %d was specified by the user\"%precision, level=0)\n        else:\n            # otherwise adjust it depending on the minimal sensible branch length\n            if self.one_mutation:\n                if self.one_mutation>1e-4:\n                    self.precision=1\n                else:\n                    self.precision=2\n            else:\n                self.precision=1\n            self.logger(\"ClockTree: Setting precision to level %s\"%self.precision, 2)\n\n        if self.precision==0:\n            self.node_grid_points = ttconf.NODE_GRID_SIZE_ROUGH\n            self.branch_grid_points = ttconf.BRANCH_GRID_SIZE_ROUGH\n            self.n_integral = ttconf.N_INTEGRAL_ROUGH\n        elif self.precision==2:\n            self.node_grid_points = ttconf.NODE_GRID_SIZE_FINE\n            self.branch_grid_points = ttconf.BRANCH_GRID_SIZE_FINE\n            self.n_integral = ttconf.N_INTEGRAL_FINE\n        elif self.precision==3:\n            self.node_grid_points = ttconf.NODE_GRID_SIZE_ULTRA\n            self.branch_grid_points = ttconf.BRANCH_GRID_SIZE_ULTRA\n            self.n_integral = ttconf.N_INTEGRAL_ULTRA\n        else:\n            self.node_grid_points = ttconf.NODE_GRID_SIZE\n            self.branch_grid_points = ttconf.BRANCH_GRID_SIZE\n            self.n_integral = ttconf.N_INTEGRAL"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef init_date_constraints(self, ancestral_inference=False, clock_rate=None, **kwarks):\n        self.logger(\"ClockTree.init_date_constraints...\",2)\n        self.tree.coalescent_joint_LH = 0\n        if self.aln and (ancestral_inference or (not hasattr(self.tree.root, 'sequence'))):\n            self.infer_ancestral_sequences('probabilistic', marginal=self.branch_length_mode=='marginal',\n                                            sample_from_profile='root',**kwarks)\n\n        # set the None  for the date-related attributes in the internal nodes.\n        # make interpolation objects for the branches\n        self.logger('ClockTree.init_date_constraints: Initializing branch length interpolation objects...',3)\n        has_clock_length = []\n        for node in self.tree.find_clades(order='postorder'):\n            if node.up is None:\n                node.branch_length_interpolator = None\n            else:\n                has_clock_length.append(hasattr(node, 'clock_length'))\n                # copy the merger rate and gamma if they are set\n                if hasattr(node,'branch_length_interpolator') and node.branch_length_interpolator is not None:\n                    gamma = node.branch_length_interpolator.gamma\n                    merger_cost = node.branch_length_interpolator.merger_cost\n                else:\n                    gamma = 1.0\n                    merger_cost = None\n\n                if self.branch_length_mode=='marginal':\n                    node.profile_pair = self.marginal_branch_profile(node)\n\n                node.branch_length_interpolator = BranchLenInterpolator(node, self.gtr,\n                            pattern_multiplicity = self.multiplicity, min_width=self.min_width,\n                            one_mutation=self.one_mutation, branch_length_mode=self.branch_length_mode)\n\n                node.branch_length_interpolator.merger_cost = merger_cost\n                node.branch_length_interpolator.gamma = gamma\n\n        # use covariance in clock model only after initial timetree estimation is done\n        use_cov = (np.sum(has_clock_length) > len(has_clock_length)*0.7) and self.use_covariation\n        self.get_clock_model(covariation=use_cov, slope=clock_rate)\n\n        # make node distribution objects\n        for node in self.tree.find_clades(order=\"postorder\"):\n            # node is constrained\n            if hasattr(node, 'raw_date_constraint') and node.raw_date_constraint is not None:\n                # set the absolute time before present in branch length units\n                if np.isscalar(node.raw_date_constraint):\n                    tbp = self.date2dist.get_time_before_present(node.raw_date_constraint)\n                    node.date_constraint = Distribution.delta_function(tbp, weight=1.0, min_width=self.min_width)\n                else:\n                    tbp = self.date2dist.get_time_before_present(np.array(node.raw_date_constraint))\n                    node.date_constraint = Distribution(tbp, np.ones_like(tbp), is_log=False, min_width=self.min_width)\n\n                if hasattr(node, 'bad_branch') and node.bad_branch is True:\n                    self.logger(\"ClockTree.init_date_constraints -- WARNING: Branch is marked as bad\"\n                                \", excluding it from the optimization process.\\n\"\n                                \"\\t\\tDate constraint will be ignored!\", 4, warn=True)\n            else: # node without sampling date set\n                node.raw_date_constraint = None\n                node.date_constraint = None", "response": "Initializes the internal branch length interpolation objects for the internal nodes."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_time_tree(self, time_marginal=False, clock_rate=None, **kwargs):\n        '''\n        Use the date constraints to calculate the most likely positions of\n        unconstrained nodes.\n\n        Parameters\n        ----------\n\n         time_marginal : bool\n            If true, use marginal reconstruction for node positions\n\n         **kwargs\n            Key word arguments to initialize dates constraints\n\n        '''\n        self.logger(\"ClockTree: Maximum likelihood tree optimization with temporal constraints\",1)\n\n        self.init_date_constraints(clock_rate=clock_rate, **kwargs)\n\n        if time_marginal:\n            self._ml_t_marginal(assign_dates = time_marginal==\"assign\")\n        else:\n            self._ml_t_joint()\n\n        self.convert_dates()", "response": "This function creates a time tree based on the given parameters."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _ml_t_joint(self):\n\n        def _cleanup():\n            for node in self.tree.find_clades():\n                del node.joint_pos_Lx\n                del node.joint_pos_Cx\n\n\n        self.logger(\"ClockTree - Joint reconstruction:  Propagating leaves -> root...\", 2)\n        # go through the nodes from leaves towards the root:\n        for node in self.tree.find_clades(order='postorder'):  # children first, msg to parents\n            # Lx is the maximal likelihood of a subtree given the parent position\n            # Cx is the branch length corresponding to the maximally likely subtree\n            if node.bad_branch:\n                # no information at the node\n                node.joint_pos_Lx = None\n                node.joint_pos_Cx = None\n            else: # all other nodes\n                if node.date_constraint is not None and node.date_constraint.is_delta: # there is a time constraint\n                    # subtree probability given the position of the parent node\n                    # Lx.x is the position of the parent node\n                    # Lx.y is the probablity of the subtree (consisting of one terminal node in this case)\n                    # Cx.y is the branch length corresponding the optimal subtree\n                    bl = node.branch_length_interpolator.x\n                    x = bl + node.date_constraint.peak_pos\n                    node.joint_pos_Lx = Distribution(x, node.branch_length_interpolator(bl),\n                                                     min_width=self.min_width, is_log=True)\n                    node.joint_pos_Cx = Distribution(x, bl, min_width=self.min_width) # map back to the branch length\n                else: # all nodes without precise constraint but positional information\n                    msgs_to_multiply = [node.date_constraint] if node.date_constraint is not None else []\n                    msgs_to_multiply.extend([child.joint_pos_Lx for child in node.clades\n                                             if child.joint_pos_Lx is not None])\n\n                    # subtree likelihood given the node's constraint and child messages\n                    if len(msgs_to_multiply) == 0: # there are no constraints\n                        node.joint_pos_Lx = None\n                        node.joint_pos_Cx = None\n                        continue\n                    elif len(msgs_to_multiply)>1: # combine the different msgs and constraints\n                        subtree_distribution = Distribution.multiply(msgs_to_multiply)\n                    else: # there is exactly one constraint.\n                        subtree_distribution = msgs_to_multiply[0]\n\n                    if node.up is None: # this is the root, set dates\n                        subtree_distribution._adjust_grid(rel_tol=self.rel_tol_prune)\n                        # set root position and joint likelihood of the tree\n                        node.time_before_present = subtree_distribution.peak_pos\n                        node.joint_pos_Lx = subtree_distribution\n                        node.joint_pos_Cx = None\n                        node.clock_length = node.branch_length\n                    else: # otherwise propagate to parent\n                        res, res_t = NodeInterpolator.convolve(subtree_distribution,\n                                        node.branch_length_interpolator,\n                                        max_or_integral='max',\n                                        inverse_time=True,\n                                        n_grid_points = self.node_grid_points,\n                                        n_integral=self.n_integral,\n                                        rel_tol=self.rel_tol_refine)\n\n                        res._adjust_grid(rel_tol=self.rel_tol_prune)\n\n                        node.joint_pos_Lx = res\n                        node.joint_pos_Cx = res_t\n\n\n        # go through the nodes from root towards the leaves and assign joint ML positions:\n        self.logger(\"ClockTree - Joint reconstruction:  Propagating root -> leaves...\", 2)\n        for node in self.tree.find_clades(order='preorder'):  # root first, msgs to children\n\n            if node.up is None: # root node\n                continue # the position was already set on the previous step\n\n            if node.joint_pos_Cx is None: # no constraints or branch is bad - reconstruct from the branch len interpolator\n                node.branch_length = node.branch_length_interpolator.peak_pos\n\n            elif isinstance(node.joint_pos_Cx, Distribution):\n                # NOTE the Lx distribution is the likelihood, given the position of the parent\n                # (Lx.x = parent position, Lx.y = LH of the node_pos given Lx.x,\n                # the length of the branch corresponding to the most likely\n                # subtree is node.Cx(node.time_before_present))\n                subtree_LH = node.joint_pos_Lx(node.up.time_before_present)\n                node.branch_length = node.joint_pos_Cx(max(node.joint_pos_Cx.xmin,\n                                            node.up.time_before_present)+ttconf.TINY_NUMBER)\n\n            node.time_before_present = node.up.time_before_present - node.branch_length\n            node.clock_length = node.branch_length\n\n            # just sanity check, should never happen:\n            if node.branch_length < 0 or node.time_before_present < 0:\n                if node.branch_length<0 and node.branch_length>-ttconf.TINY_NUMBER:\n                    self.logger(\"ClockTree - Joint reconstruction: correcting rounding error of %s\"%node.name, 4)\n                    node.branch_length = 0\n\n        self.tree.positional_joint_LH = self.timetree_likelihood()\n        # cleanup, if required\n        if not self.debug:\n            _cleanup()", "response": "Compute the joint maximum likelihood assignment of the internal nodes positions by propagating from the tree leaves towards the root."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the likelihood of the data given the current branch length in the tree.", "response": "def timetree_likelihood(self):\n        '''\n        Return the likelihood of the data given the current branch length in the tree\n        '''\n        LH = 0\n        for node in self.tree.find_clades(order='preorder'):  # sum the likelihood contributions of all branches\n            if node.up is None: # root node\n                continue\n            LH -= node.branch_length_interpolator(node.branch_length)\n\n        # add the root sequence LH and return\n        if self.aln:\n            LH += self.gtr.sequence_logLH(self.tree.root.cseq, pattern_multiplicity=self.multiplicity)\n        return LH"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the marginal probability distribution of the internal nodes positions by propagating from the tree leaves towards the root.", "response": "def _ml_t_marginal(self, assign_dates=False):\n        \"\"\"\n        Compute the marginal probability distribution of the internal nodes positions by\n        propagating from the tree leaves towards the root. The result of\n        this operation are the probability distributions of each internal node,\n        conditional on the constraints on all leaves of the tree, which have sampling dates.\n        The probability distributions are set as marginal_pos_LH attributes to the nodes.\n\n        Parameters\n        ----------\n\n         assign_dates : bool, default False\n            If True, the inferred dates will be assigned to the nodes as\n            :code:`time_before_present' attributes, and their branch lengths\n            will be corrected accordingly.\n            .. Note::\n                Normally, the dates are assigned by running joint reconstruction.\n\n        Returns\n        -------\n\n         None\n            Every internal node is assigned the probability distribution in form\n            of an interpolation object and sends this distribution further towards the\n            root.\n\n        \"\"\"\n\n        def _cleanup():\n            for node in self.tree.find_clades():\n                try:\n                    del node.marginal_pos_Lx\n                    del node.subtree_distribution\n                    del node.msg_from_parent\n                    #del node.marginal_pos_LH\n                except:\n                    pass\n\n\n        self.logger(\"ClockTree - Marginal reconstruction:  Propagating leaves -> root...\", 2)\n        # go through the nodes from leaves towards the root:\n        for node in self.tree.find_clades(order='postorder'):  # children first, msg to parents\n            if node.bad_branch:\n                # no information\n                node.marginal_pos_Lx = None\n            else: # all other nodes\n                if node.date_constraint is not None and node.date_constraint.is_delta: # there is a time constraint\n                    # initialize the Lx for nodes with precise date constraint:\n                    # subtree probability given the position of the parent node\n                    # position of the parent node is given by the branch length\n                    # distribution attached to the child node position\n                    node.subtree_distribution = node.date_constraint\n                    bl = node.branch_length_interpolator.x\n                    x = bl + node.date_constraint.peak_pos\n                    node.marginal_pos_Lx = Distribution(x, node.branch_length_interpolator(bl),\n                                                        min_width=self.min_width, is_log=True)\n\n                else: # all nodes without precise constraint but positional information\n                      # subtree likelihood given the node's constraint and child msg:\n                    msgs_to_multiply = [node.date_constraint] if node.date_constraint is not None else []\n                    msgs_to_multiply.extend([child.marginal_pos_Lx for child in node.clades\n                                             if child.marginal_pos_Lx is not None])\n\n                    # combine the different msgs and constraints\n                    if len(msgs_to_multiply)==0:\n                        # no information\n                        node.marginal_pos_Lx = None\n                        continue\n                    elif len(msgs_to_multiply)==1:\n                        node.subtree_distribution = msgs_to_multiply[0]\n                    else: # combine the different msgs and constraints\n                        node.subtree_distribution = Distribution.multiply(msgs_to_multiply)\n\n                    if node.up is None: # this is the root, set dates\n                        node.subtree_distribution._adjust_grid(rel_tol=self.rel_tol_prune)\n                        node.marginal_pos_Lx = node.subtree_distribution\n                        node.marginal_pos_LH = node.subtree_distribution\n                        self.tree.positional_marginal_LH = -node.subtree_distribution.peak_val\n                    else: # otherwise propagate to parent\n                        res, res_t = NodeInterpolator.convolve(node.subtree_distribution,\n                                        node.branch_length_interpolator,\n                                        max_or_integral='integral',\n                                        n_grid_points = self.node_grid_points,\n                                        n_integral=self.n_integral,\n                                        rel_tol=self.rel_tol_refine)\n                        res._adjust_grid(rel_tol=self.rel_tol_prune)\n                        node.marginal_pos_Lx = res\n\n        self.logger(\"ClockTree - Marginal reconstruction:  Propagating root -> leaves...\", 2)\n        from scipy.interpolate import interp1d\n        for node in self.tree.find_clades(order='preorder'):\n\n            ## The root node\n            if node.up is None:\n                node.msg_from_parent = None # nothing beyond the root\n            # all other cases (All internal nodes + unconstrained terminals)\n            else:\n                parent = node.up\n                # messages from the complementary subtree (iterate over all sister nodes)\n                complementary_msgs = [sister.marginal_pos_Lx for sister in parent.clades\n                                            if (sister != node) and (sister.marginal_pos_Lx is not None)]\n\n                # if parent itself got smth from the root node, include it\n                if parent.msg_from_parent is not None:\n                    complementary_msgs.append(parent.msg_from_parent)\n                elif parent.marginal_pos_Lx is not None:\n                    complementary_msgs.append(parent.marginal_pos_LH)\n\n                if len(complementary_msgs):\n                    msg_parent_to_node = NodeInterpolator.multiply(complementary_msgs)\n                    msg_parent_to_node._adjust_grid(rel_tol=self.rel_tol_prune)\n                else:\n                    x = [parent.numdate, numeric_date()]\n                    msg_parent_to_node = NodeInterpolator(x, [1.0, 1.0],min_width=self.min_width)\n\n                # integral message, which delivers to the node the positional information\n                # from the complementary subtree\n                res, res_t = NodeInterpolator.convolve(msg_parent_to_node, node.branch_length_interpolator,\n                                                    max_or_integral='integral',\n                                                    inverse_time=False,\n                                                    n_grid_points = self.node_grid_points,\n                                                    n_integral=self.n_integral,\n                                                    rel_tol=self.rel_tol_refine)\n\n                node.msg_from_parent = res\n                if node.marginal_pos_Lx is None:\n                    node.marginal_pos_LH = node.msg_from_parent\n                else:\n                    node.marginal_pos_LH = NodeInterpolator.multiply((node.msg_from_parent, node.subtree_distribution))\n\n                self.logger('ClockTree._ml_t_root_to_leaves: computed convolution'\n                                ' with %d points at node %s'%(len(res.x),node.name),4)\n\n                if self.debug:\n                    tmp = np.diff(res.y-res.peak_val)\n                    nsign_changed = np.sum((tmp[1:]*tmp[:-1]<0)&(res.y[1:-1]-res.peak_val<500))\n                    if nsign_changed>1:\n                        import matplotlib.pyplot as plt\n                        plt.ion()\n                        plt.plot(res.x, res.y-res.peak_val, '-o')\n                        plt.plot(res.peak_pos - node.branch_length_interpolator.x,\n                                 node.branch_length_interpolator(node.branch_length_interpolator.x)-node.branch_length_interpolator.peak_val, '-o')\n                        plt.plot(msg_parent_to_node.x,msg_parent_to_node.y-msg_parent_to_node.peak_val, '-o')\n                        plt.ylim(0,100)\n                        plt.xlim(-0.05, 0.05)\n                        import ipdb; ipdb.set_trace()\n\n            # assign positions of nodes and branch length only when desired\n            # since marginal reconstruction can result in negative branch length\n            if assign_dates:\n                node.time_before_present = node.marginal_pos_LH.peak_pos\n                if node.up:\n                    node.clock_length = node.up.time_before_present - node.time_before_present\n                    node.branch_length = node.clock_length\n\n            # construct the inverse cumulant distribution to evaluate confidence intervals\n            if node.marginal_pos_LH.is_delta:\n                node.marginal_inverse_cdf=interp1d([0,1], node.marginal_pos_LH.peak_pos*np.ones(2), kind=\"linear\")\n            else:\n                dt = np.diff(node.marginal_pos_LH.x)\n                y = node.marginal_pos_LH.prob_relative(node.marginal_pos_LH.x)\n                int_y = np.concatenate(([0], np.cumsum(dt*(y[1:]+y[:-1])/2.0)))\n                int_y/=int_y[-1]\n                node.marginal_inverse_cdf = interp1d(int_y, node.marginal_pos_LH.x, kind=\"linear\")\n                node.marginal_cdf = interp1d(node.marginal_pos_LH.x, int_y, kind=\"linear\")\n\n        if not self.debug:\n            _cleanup()\n\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef branch_length_to_years(self):\n        '''\n        This function sets branch length to reflect the date differences between parent and child\n        nodes measured in years. Should only be called after :py:meth:`timetree.ClockTree.convert_dates` has been called.\n\n        Returns\n        -------\n         None\n            All manipulations are done in place on the tree\n\n        '''\n        self.logger('ClockTree.branch_length_to_years: setting node positions in units of years', 2)\n        if not hasattr(self.tree.root, 'numdate'):\n            self.logger('ClockTree.branch_length_to_years: infer ClockTree first', 2,warn=True)\n        self.tree.root.branch_length = 0.1\n        for n in self.tree.find_clades(order='preorder'):\n            if n.up is not None:\n                n.branch_length = n.numdate - n.up.numdate", "response": "This function sets the branch length to reflect the date differences between parent and child nodes measured in years."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the time tree estimation of evolutionary rates + / - one - time rate standard deviation form the ML estimate", "response": "def calc_rate_susceptibility(self, rate_std=None, params=None):\n        \"\"\"return the time tree estimation of evolutionary rates +/- one\n        standard deviation form the ML estimate.\n\n        Returns\n        -------\n        TreeTime.return_code : str\n            success or failure\n        \"\"\"\n        params = params or {}\n        if rate_std is None:\n            if not (self.clock_model['valid_confidence'] and 'cov' in self.clock_model):\n                self.logger(\"ClockTree.calc_rate_susceptibility: need valid standard deviation of the clock rate to estimate dating error.\", 1, warn=True)\n                return ttconf.ERROR\n            rate_std = np.sqrt(self.clock_model['cov'][0,0])\n\n        current_rate = np.abs(self.clock_model['slope'])\n        upper_rate = self.clock_model['slope'] + rate_std\n        lower_rate = max(0.1*current_rate, self.clock_model['slope'] - rate_std)\n        for n in self.tree.find_clades():\n            if n.up:\n                n._orig_gamma = n.branch_length_interpolator.gamma\n                n.branch_length_interpolator.gamma = n._orig_gamma*upper_rate/current_rate\n\n        self.logger(\"###ClockTree.calc_rate_susceptibility: run with upper bound of rate estimate\", 1)\n        self.make_time_tree(**params)\n        self.logger(\"###ClockTree.calc_rate_susceptibility: rate: %f, LH:%f\"%(upper_rate, self.tree.positional_joint_LH), 2)\n        for n in self.tree.find_clades():\n            n.numdate_rate_variation = [(upper_rate, n.numdate)]\n            if n.up:\n                n.branch_length_interpolator.gamma = n._orig_gamma*lower_rate/current_rate\n\n        self.logger(\"###ClockTree.calc_rate_susceptibility: run with lower bound of rate estimate\", 1)\n        self.make_time_tree(**params)\n        self.logger(\"###ClockTree.calc_rate_susceptibility: rate: %f, LH:%f\"%(lower_rate, self.tree.positional_joint_LH), 2)\n        for n in self.tree.find_clades():\n            n.numdate_rate_variation.append((lower_rate, n.numdate))\n            if n.up:\n                n.branch_length_interpolator.gamma  = n._orig_gamma\n\n        self.logger(\"###ClockTree.calc_rate_susceptibility: run with central rate estimate\", 1)\n        self.make_time_tree(**params)\n        self.logger(\"###ClockTree.calc_rate_susceptibility: rate: %f, LH:%f\"%(current_rate, self.tree.positional_joint_LH), 2)\n        for n in self.tree.find_clades():\n            n.numdate_rate_variation.append((current_rate, n.numdate))\n            n.numdate_rate_variation.sort(key=lambda x:x[1]) # sort estimates for different rates by numdate\n\n        return ttconf.SUCCESS"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef date_uncertainty_due_to_rate(self, node, interval=(0.05, 0.095)):\n        if hasattr(node, \"numdate_rate_variation\"):\n            from scipy.special import erfinv\n            nsig = [np.sqrt(2.0)*erfinv(-1.0 + 2.0*x) if x*(1.0-x) else 0\n                    for x in interval]\n            l,c,u = [x[1] for x in node.numdate_rate_variation]\n            return np.array([c + x*np.abs(y-c) for x,y in zip(nsig, (l,u))])\n\n        else:\n            return None", "response": "estimate the uncertainty due to rate variation of a particular node"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_max_posterior_region(self, node, fraction = 0.9):\n        '''\n        If temporal reconstruction was done using the marginal ML mode, the entire distribution of\n        times is available. This function determines the interval around the highest\n        posterior probability region that contains the specified fraction of the probability mass.\n        In absense of marginal reconstruction, it will return uncertainty based on rate\n        variation. If both are present, the wider interval will be returned.\n\n        Parameters\n        ----------\n\n         node : PhyloTree.Clade\n            The node for which the posterior region is to be calculated\n\n         interval : float\n            Float specifying who much of the posterior probability is\n            to be contained in the region\n\n        Returns\n        -------\n         max_posterior_region : numpy array\n            Array with two numerical dates delineating the high posterior region\n\n        '''\n        if node.marginal_inverse_cdf==\"delta\":\n            return np.array([node.numdate, node.numdate])\n\n\n        min_max = (node.marginal_pos_LH.xmin, node.marginal_pos_LH.xmax)\n        min_date, max_date = [self.date2dist.to_numdate(x) for x in min_max][::-1]\n        if node.marginal_pos_LH.peak_pos == min_max[0]: #peak on the left\n            return self.get_confidence_interval(node, (0, fraction))\n        elif node.marginal_pos_LH.peak_pos == min_max[1]: #peak on the right\n            return self.get_confidence_interval(node, (1.0-fraction ,1.0))\n        else: # peak in the center of the distribution\n            rate_contribution = self.date_uncertainty_due_to_rate(node, ((1-fraction)*0.5, 1.0-(1.0-fraction)*0.5))\n\n            # construct height to position interpolators left and right of the peak\n            # this assumes there is only one peak --- might fail in odd cases\n            from scipy.interpolate import interp1d\n            from scipy.optimize import minimize_scalar as minimize\n            pidx = np.argmin(node.marginal_pos_LH.y)\n            pval = np.min(node.marginal_pos_LH.y)\n            left =  interp1d(node.marginal_pos_LH.y[:(pidx+1)]-pval, node.marginal_pos_LH.x[:(pidx+1)],\n                            kind='linear', fill_value=min_max[0], bounds_error=False)\n            right = interp1d(node.marginal_pos_LH.y[pidx:]-pval, node.marginal_pos_LH.x[pidx:],\n                            kind='linear', fill_value=min_max[1], bounds_error=False)\n\n            # function to minimize -- squared difference between prob mass and desired fracion\n            def func(x, thres):\n                interval = np.array([left(x), right(x)]).squeeze()\n                return (thres - np.diff(node.marginal_cdf(np.array(interval))))**2\n\n            # minimza and determine success\n            sol = minimize(func, bracket=[0,10], args=(fraction,))\n            if sol['success']:\n                mutation_contribution = self.date2dist.to_numdate(np.array([right(sol['x']), left(sol['x'])]).squeeze())\n            else: # on failure, return standard confidence interval\n                mutation_contribution = None\n\n            return self.combine_confidence(node.numdate, (min_date, max_date),\n                                      c1=rate_contribution, c2=mutation_contribution)", "response": "This function determines the interval around the highest posterior probability region that contains the specified fraction of the probability mass."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads in a vcf file and associated reference sequence fasta file and returns a dictionary with the appropriate fields.", "response": "def read_vcf(vcf_file, ref_file):\r\n    \"\"\"\r\n    Reads in a vcf/vcf.gz file and associated\r\n    reference sequence fasta (to which the VCF file is mapped).\r\n\r\n    Parses mutations, insertions, and deletions and stores them in a nested dict,\r\n    see 'returns' for the dict structure.\r\n\r\n    Calls with heterozygous values 0/1, 0/2, etc and no-calls (./.) are\r\n    replaced with Ns at the associated sites.\r\n\r\n    Positions are stored to correspond the location in the reference sequence\r\n    in Python (numbering is transformed to start at 0)\r\n\r\n    Parameters\r\n    ----------\r\n    vcf_file : string\r\n        Path to the vcf or vcf.gz file to be read in\r\n    ref_file : string\r\n        Path to the fasta reference file to be read in\r\n\r\n    Returns\r\n    --------\r\n    compress_seq : nested dict\r\n        In the format: ::\r\n\r\n            {\r\n            'reference':'AGCTCGA..A',\r\n            'sequences': { 'seq1':{4:'A', 7:'-'}, 'seq2':{100:'C'} },\r\n            'insertions': { 'seq1':{4:'ATT'}, 'seq3':{1:'TT', 10:'CAG'} },\r\n            'positions': [1,4,7,10,100...]\r\n            }\r\n\r\n        references : string\r\n            String of the reference sequence read from the Fasta, to which\r\n            the variable sites are mapped\r\n        sequences : nested dict\r\n            Dict containing sequence names as keys which map to dicts\r\n            that have position as key and the single-base mutation (or deletion)\r\n            as values\r\n        insertions : nested dict\r\n            Dict in the same format as the above, which stores insertions and their\r\n            locations. The first base of the insertion is the same as whatever is\r\n            currently in that position (Ref if no mutation, mutation in 'sequences'\r\n            otherwise), so the current base can be directly replaced by the bases held here.\r\n        positions : list\r\n            Python list of all positions with a mutation, insertion, or deletion.\r\n\r\n    \"\"\"\r\n\r\n    #Programming Note:\r\n    # Note on VCF Format\r\n    # -------------------\r\n    # 'Insertion where there are also deletions' (special handling)\r\n    #     Ex:\r\n    #       REF     ALT         Seq1    Seq2\r\n    #       GC      GCC,G       1/1     2/2\r\n    #     Insertions formatted differently - don't know how many bp match\r\n    #     the Ref (unlike simple insert below). Could be mutations, also.\r\n    # 'Deletion'\r\n    #     Ex:\r\n    #       REF     ALT\r\n    #       GC      G\r\n    #     Alt does not have to be 1 bp - any length shorter than Ref.\r\n    # 'Insertion'\r\n    #     Ex:\r\n    #       REF     ALT\r\n    #       A       ATT\r\n    #     First base always matches Ref.\r\n    # 'No indel'\r\n    #     Ex:\r\n    #       REF     ALT\r\n    #       A       G\r\n\r\n    #define here, so that all sub-functions can access them\r\n    sequences = defaultdict(dict)\r\n    insertions = defaultdict(dict) #Currently not used, but kept in case of future use.\r\n\r\n    #TreeTime handles 2-3 base ambig codes, this will allow that.\r\n    def getAmbigCode(bp1, bp2, bp3=\"\"):\r\n        bps = [bp1,bp2,bp3]\r\n        bps.sort()\r\n        key = \"\".join(bps)\r\n\r\n        return {\r\n            'CT': 'Y',\r\n            'AG': 'R',\r\n            'AT': 'W',\r\n            'CG': 'S',\r\n            'GT': 'K',\r\n            'AC': 'M',\r\n            'AGT': 'D',\r\n            'ACG': 'V',\r\n            'ACT': 'H',\r\n            'CGT': 'B'\r\n        }[key]\r\n\r\n    #Parses a 'normal' (not hetero or no-call) call depending if insertion+deletion, insertion,\r\n    #deletion, or single bp subsitution\r\n    def parseCall(snps, ins, pos, ref, alt):\r\n\r\n        #Insertion where there are also deletions (special handling)\r\n        if len(ref) > 1 and len(alt)>len(ref):\r\n            for i in range(len(ref)):\r\n                #if the pos doesn't match, store in sequences\r\n                if ref[i] != alt[i]:\r\n                    snps[pos+i] = alt[i] if alt[i] != '.' else 'N' #'.' = no-call\r\n                #if about to run out of ref, store rest:\r\n                if (i+1) >= len(ref):\r\n                    ins[pos+i] = alt[i:]\r\n        #Deletion\r\n        elif len(ref) > 1:\r\n            for i in range(len(ref)):\r\n                #if ref is longer than alt, these are deletion positions\r\n                if i+1 > len(alt):\r\n                    snps[pos+i] = '-'\r\n                #if not, there may be mutations\r\n                else:\r\n                    if ref[i] != alt[i]:\r\n                        snps[pos+i] = alt[i] if alt[i] != '.' else 'N' #'.' = no-call\r\n        #Insertion\r\n        elif len(alt) > 1:\r\n            ins[pos] = alt\r\n        #No indel\r\n        else:\r\n            snps[pos] = alt\r\n\r\n\r\n    #Parses a 'bad' (hetero or no-call) call depending on what it is\r\n    def parseBadCall(snps, ins, pos, ref, ALT):\r\n        #Deletion\r\n        #   REF     ALT     Seq1    Seq2    Seq3\r\n        #   GCC     G       1/1     0/1     ./.\r\n        # Seq1 (processed by parseCall, above) will become 'G--'\r\n        # Seq2 will become 'GNN'\r\n        # Seq3 will become 'GNN'\r\n        if len(ref) > 1:\r\n            #Deleted part becomes Ns\r\n            if gen[0] == '0' or gen[0] == '.':\r\n                if gen[0] == '0':   #if het, get first bp\r\n                    alt = str(ALT[int(gen[2])-1])\r\n                else: #if no-call, there is no alt, so just put Ns after 1st ref base\r\n                    alt = ref[0]\r\n                for i in range(len(ref)):\r\n                    #if ref is longer than alt, these are deletion positions\r\n                    if i+1 > len(alt):\r\n                        snps[pos+i] = 'N'\r\n                    #if not, there may be mutations\r\n                    else:\r\n                        if ref[i] != alt[i]:\r\n                            snps[pos+i] = alt[i] if alt[i] != '.' else 'N' #'.' = no-call\r\n\r\n        #If not deletion, need to know call type\r\n        #if het, see if proposed alt is 1bp mutation\r\n        elif gen[0] == '0':\r\n            alt = str(ALT[int(gen[2])-1])\r\n            if len(alt)==1:\r\n                #alt = getAmbigCode(ref,alt) #if want to allow ambig\r\n                alt = 'N' #if you want to disregard ambig\r\n                snps[pos] = alt\r\n            #else a het-call insertion, so ignore.\r\n\r\n        #else it's a no-call; see if all alts have a length of 1\r\n        #(meaning a simple 1bp mutation)\r\n        elif len(ALT)==len(\"\".join(ALT)):\r\n            alt = 'N'\r\n            snps[pos] = alt\r\n        #else a no-call insertion, so ignore.\r\n\r\n\r\n    #House code is *much* faster than pyvcf because we don't care about all info\r\n    #about coverage, quality, counts, etc, which pyvcf goes to effort to parse\r\n    #(and it's not easy as there's no standard ordering). Custom code can completely\r\n    #ignore all of this.\r\n    import gzip\r\n    from Bio import SeqIO\r\n    import numpy as np\r\n\r\n    nsamp = 0\r\n    posLoc = 0\r\n    refLoc = 0\r\n    altLoc = 0\r\n    sampLoc = 9\r\n\r\n    #Use different openers depending on whether compressed\r\n    opn = gzip.open if vcf_file.endswith(('.gz', '.GZ')) else open\r\n\r\n    with opn(vcf_file, mode='rt') as f:\r\n        for line in f:\r\n            if line[0] != '#':\r\n                #actual data - most common so first in 'if-list'!\r\n                line = line.strip()\r\n                dat = line.split('\\t')\r\n                POS = int(dat[posLoc])\r\n                REF = dat[refLoc]\r\n                ALT = dat[altLoc].split(',')\r\n                calls = np.array(dat[sampLoc:])\r\n\r\n                #get samples that differ from Ref at this site\r\n                recCalls = {}\r\n                for sname, sa in zip(samps, calls):\r\n                    if ':' in sa: #if proper VCF file (followed by quality/coverage info)\r\n                        gt = sa.split(':')[0]\r\n                    else: #if 'pseudo' VCF file (nextstrain output, or otherwise stripped)\r\n                        gt = sa\r\n                    if gt == '0' or gt == '1': #for haploid calls in VCF\r\n                        gt = '0/0' if gt == '0' else '1/1'\r\n\r\n                    #ignore if ref call: '.' or '0/0', depending on VCF\r\n                    if ('/' in gt and gt != '0/0') or ('|' in gt and gt != '0|0'):\r\n                        recCalls[sname] = gt\r\n\r\n                #store the position and the alt\r\n                for seq, gen in recCalls.items():\r\n                    ref = REF\r\n                    pos = POS-1     #VCF numbering starts from 1, but Reference seq numbering\r\n                                    #will be from 0 because it's python!\r\n                    #Accepts only calls that are 1/1, 2/2 etc. Rejects hets and no-calls\r\n                    if gen[0] != '0' and gen[2] != '0' and gen[0] != '.' and gen[2] != '.':\r\n                        alt = str(ALT[int(gen[0])-1])   #get the index of the alternate\r\n                        if seq not in sequences.keys():\r\n                            sequences[seq] = {}\r\n\r\n                        parseCall(sequences[seq],insertions[seq], pos, ref, alt)\r\n\r\n                    #If is heterozygote call (0/1) or no call (./.)\r\n                    else:\r\n                        #alt will differ here depending on het or no-call, must pass original\r\n                        parseBadCall(sequences[seq],insertions[seq], pos, ref, ALT)\r\n\r\n            elif line[0] == '#' and line[1] == 'C':\r\n                #header line, get all the information\r\n                header = line.strip().split('\\t')\r\n                posLoc = header.index(\"POS\")\r\n                refLoc = header.index('REF')\r\n                altLoc = header.index('ALT')\r\n                sampLoc = header.index('FORMAT')+1\r\n                samps = header[sampLoc:]\r\n                samps = [ x.strip() for x in samps ] #ensure no leading/trailing spaces\r\n                nsamp = len(samps)\r\n\r\n            #else you are a comment line, ignore.\r\n\r\n    #Gather all variable positions\r\n    positions = set()\r\n    for seq, muts in sequences.items():\r\n        positions.update(muts.keys())\r\n\r\n    #One or more seqs are same as ref! (No non-ref calls) So haven't been 'seen' yet\r\n    if nsamp > len(sequences):\r\n        missings = set(samps).difference(sequences.keys())\r\n        for s in missings:\r\n            sequences[s] = {}\r\n\r\n    refSeq = SeqIO.read(ref_file, format='fasta')\r\n    refSeq = refSeq.upper() #convert to uppercase to avoid unknown chars later\r\n    refSeqStr = str(refSeq.seq)\r\n\r\n    compress_seq = {'reference':refSeqStr,\r\n                    'sequences': sequences,\r\n                    'insertions': insertions,\r\n                    'positions': sorted(positions)}\r\n\r\n    return compress_seq"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_vcf(tree_dict, file_name):#, compress=False):\r\n\r\n#   Programming Logic Note:\r\n#\r\n#    For a sequence like:\r\n#    Pos     1 2 3 4 5 6\r\n#    Ref     A C T T A C\r\n#    Seq1    A C - - - G\r\n#\r\n#    In a dict it is stored:\r\n#    Seq1:{3:'-', 4:'-', 5:'-', 6:'G'}  (Numbering from 1 for simplicity)\r\n#\r\n#    In a VCF it needs to be:\r\n#    POS REF     ALT     Seq1\r\n#    2   CTTA    C       1/1\r\n#    6   C       G       1/1\r\n#\r\n#    If a position is deleted (pos 3), need to get invariable position preceeding it\r\n#\r\n#    However, in alternative case, the base before a deletion is mutant, so need to check\r\n#        that next position isn't a deletion (as otherwise won't be found until after the\r\n#        current single bp mutation is written out)\r\n#\r\n#    When deleted position found, need to gather up all adjacent mutant positions with deletions,\r\n#        but not include adjacent mutant positions that aren't deletions (pos 6)\r\n#\r\n#    Don't run off the 'end' of the position list if deletion is the last thing to be included\r\n#        in the VCF file\r\n\r\n    sequences = tree_dict['sequences']\r\n    ref = tree_dict['reference']\r\n    positions = tree_dict['positions']\r\n\r\n    def handleDeletions(i, pi, pos, ref, delete, pattern):\r\n        refb = ref[pi]\r\n        if delete: #Need to get the position before\r\n            i-=1    #As we'll next go to this position again\r\n            pi-=1\r\n            pos = pi+1\r\n            refb = ref[pi]\r\n            #re-get pattern\r\n            pattern = []\r\n            for k,v in sequences.items():\r\n                try:\r\n                    pattern.append(sequences[k][pi])\r\n                except KeyError:\r\n                    pattern.append(ref[pi])\r\n            pattern = np.array(pattern)\r\n\r\n        sites = []\r\n        sites.append(pattern)\r\n\r\n        #Gather all positions affected by deletion - but don't run off end of position list\r\n        while (i+1) < len(positions) and positions[i+1] == pi+1:\r\n            i+=1\r\n            pi = positions[i]\r\n            pattern = []\r\n            for k,v in sequences.items():\r\n                try:\r\n                    pattern.append(sequences[k][pi])\r\n                except KeyError:\r\n                    pattern.append(ref[pi])\r\n            pattern = np.array(pattern)\r\n\r\n            #Stops 'greedy' behaviour from adding mutations adjacent to deletions\r\n            if any(pattern == '-'): #if part of deletion, append\r\n                sites.append(pattern)\r\n                refb = refb+ref[pi]\r\n            else: #this is another mutation next to the deletion!\r\n                i-=1    #don't append, break this loop\r\n\r\n        #Rotate them into 'calls'\r\n        sites = np.asarray(sites)\r\n        align = np.rot90(sites)\r\n        align = np.flipud(align)\r\n\r\n        #Get rid of '-', and put '.' for calls that match ref\r\n        #Only removes trailing '-'. This breaks VCF convension, but the standard\r\n        #VCF way of handling this* is really complicated, and the situation is rare.\r\n        #(*deletions and mutations at the same locations)\r\n        fullpat = []\r\n        for pt in align:\r\n            gp = len(pt)-1\r\n            while pt[gp] == '-':\r\n                pt[gp] = ''\r\n                gp-=1\r\n            pat = \"\".join(pt)\r\n            if pat == refb:\r\n                fullpat.append('.')\r\n            else:\r\n                fullpat.append(pat)\r\n\r\n        pattern = np.array(fullpat)\r\n\r\n        return i, pi, pos, refb, pattern\r\n\r\n\r\n    #prepare the header of the VCF & write out\r\n    header=[\"#CHROM\",\"POS\",\"ID\",\"REF\",\"ALT\",\"QUAL\",\"FILTER\",\"INFO\",\"FORMAT\"]+list(sequences.keys())\r\n    with open(file_name, 'w') as the_file:\r\n        the_file.write( \"##fileformat=VCFv4.2\\n\"+\r\n                        \"##source=NextStrain\\n\"+\r\n                        \"##FORMAT=<ID=GT,Number=1,Type=String,Description=\\\"Genotype\\\">\\n\")\r\n        the_file.write(\"\\t\".join(header)+\"\\n\")\r\n\r\n    vcfWrite = []\r\n    errorPositions = []\r\n    explainedErrors = 0\r\n\r\n    #Why so basic? Because we sometimes have to back up a position!\r\n    i=0\r\n    while i < len(positions):\r\n        #Get the 'pattern' of all calls at this position.\r\n        #Look out specifically for current (this pos) or upcoming (next pos) deletions\r\n        #But also distinguish these two, as handled differently.\r\n\r\n        pi = positions[i]\r\n        pos = pi+1 #change numbering to match VCF, not python, for output\r\n        refb = ref[pi] #reference base at this position\r\n        delete = False #deletion at this position - need to grab previous base (invariable)\r\n        deleteGroup = False #deletion at next position (mutation at this pos) - do not need to get prev base\r\n\r\n        #try/except is much more efficient than 'if' statements for constructing patterns,\r\n        #as on average a 'variable' location will not be variable for any given sequence\r\n        pattern = []\r\n        #pattern2 gets the pattern at next position to check for upcoming deletions\r\n        #it's more efficient to get both here rather than loop through sequences twice!\r\n        pattern2 = []\r\n        for k,v in sequences.items():\r\n            try:\r\n                pattern.append(sequences[k][pi])\r\n            except KeyError:\r\n                pattern.append(ref[pi])\r\n\r\n            try:\r\n                pattern2.append(sequences[k][pi+1])\r\n            except KeyError:\r\n                pattern2.append(ref[pi+1])\r\n\r\n        pattern = np.array(pattern)\r\n        pattern2 = np.array(pattern2)\r\n\r\n        #If a deletion here, need to gather up all bases, and position before\r\n        if any(pattern == '-'):\r\n            if pos != 1:\r\n                deleteGroup = True\r\n                delete = True\r\n            else:\r\n                #If theres a deletion in 1st pos, VCF files do not handle this well.\r\n                #Proceed keeping it as '-' for alt (violates VCF), but warn user to check output.\r\n                #(This is rare)\r\n                print (\"WARNING: You have a deletion in the first position of your alignment. VCF format does not handle this well. Please check the output to ensure it is correct.\")\r\n        else:\r\n            #If a deletion in next pos, need to gather up all bases\r\n            if any(pattern2 == '-'):\r\n                deleteGroup = True\r\n\r\n        #If deletion, treat affected bases as 1 'call':\r\n        if delete or deleteGroup:\r\n            i, pi, pos, refb, pattern = handleDeletions(i, pi, pos, ref, delete, pattern)\r\n        #If no deletion, replace ref with '.', as in VCF format\r\n        else:\r\n            pattern[pattern==refb] = '.'\r\n\r\n        #Get the list of ALTs - minus any '.'!\r\n        uniques = np.unique(pattern)\r\n        uniques = uniques[np.where(uniques!='.')]\r\n\r\n        #Convert bases to the number that matches the ALT\r\n        j=1\r\n        for u in uniques:\r\n            pattern[np.where(pattern==u)[0]] = str(j)\r\n            j+=1\r\n        #Now convert these calls to #/# (VCF format)\r\n        calls = [ j+\"/\"+j if j!='.' else '.' for j in pattern ]\r\n\r\n        #What if there's no variation at a variable site??\r\n        #This can happen when sites are modified by TreeTime - see below.\r\n        printPos = True\r\n        if len(uniques)==0:\r\n            #If we expect it (it was made constant by TreeTime), it's fine.\r\n            if 'inferred_const_sites' in tree_dict and pi in tree_dict['inferred_const_sites']:\r\n                explainedErrors += 1\r\n                printPos = False #and don't output position to the VCF\r\n            else:\r\n                #If we don't expect, raise an error\r\n                errorPositions.append(str(pi))\r\n\r\n        #Write it out - Increment positions by 1 so it's in VCF numbering\r\n        #If no longer variable, and explained, don't write it out\r\n        if printPos:\r\n            output = [\"MTB_anc\", str(pos), \".\", refb, \",\".join(uniques), \".\", \"PASS\", \".\", \"GT\"] + calls\r\n            vcfWrite.append(\"\\t\".join(output))\r\n\r\n        i+=1\r\n\r\n    #Note: The number of 'inferred_const_sites' passed back by TreeTime will often be longer\r\n    #than the number of 'site that were made constant' that prints below. This is because given the site:\r\n    # Ref   Alt     Seq\r\n    # G     A       AANAA\r\n    #This will be converted to 'AAAAA' and listed as an 'inferred_const_sites'. However, for VCF\r\n    #purposes, because the site is 'variant' against the ref, it is variant, as expected, and so\r\n    #won't be counted in the below list, which is only sites removed from the VCF.\r\n\r\n    if 'inferred_const_sites' in tree_dict and explainedErrors != 0:\r\n        print ( \"Sites that were constant except for ambiguous bases were made constant by TreeTime. This happened {} times. These sites are now excluded from the VCF.\".format(explainedErrors))\r\n\r\n    if len(errorPositions) != 0:\r\n        print (\"\\n***WARNING: vcf_utils.py\"\r\n            \"\\n{} sites were found that had no alternative bases. If this data has been \"\r\n            \"run through TreeTime and contains ambiguous bases, try calling get_tree_dict with \"\r\n            \"var_ambigs=True to see if this clears the error.\"\r\n            \"\\n\\nAlternative causes:\"\r\n            \"\\n- Not all sequences in your alignment are in the tree (if you are running TreeTime via commandline \"\r\n            \"this is most likely)\"\r\n            \"\\n- In TreeTime, can be caused by overwriting variants in tips with small branch lengths (debug)\"\r\n            \"\\n\\nThese are the positions affected (numbering starts at 0):\".format(str(len(errorPositions))))\r\n        print (\",\".join(errorPositions))\r\n\r\n    with open(file_name, 'a') as the_file:\r\n        the_file.write(\"\\n\".join(vcfWrite))\r\n\r\n    if file_name.endswith(('.gz', '.GZ')):\r\n        import os\r\n        #must temporarily remove .gz ending, or gzip won't zip it!\r\n        os.rename(file_name, file_name[:-3])\r\n        call = [\"gzip\", file_name[:-3]]\r\n        os.system(\" \".join(call))", "response": "Writes out a VCF - style file for the given tree_dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _convolution_integrand(t_val, f, g,\n                           inverse_time=None, return_log=False):\n    '''\n    Evaluates int_tau f(t+tau)*g(tau) or int_tau f(t-tau)g(tau) if inverse time is TRUE\n\n    Parameters\n    -----------\n\n     t_val : double\n        Time point\n\n     f : Interpolation object\n        First multiplier in convolution\n\n     g : Interpolation object\n        Second multiplier in convolution\n\n     inverse_time : bool, None\n        time direction. If True, then the f(t-tau)*g(tau) is calculated, otherwise,\n        f(t+tau)*g(tau)\n\n     return_log : bool\n        If True, the logarithm will be returned\n\n\n    Returns\n    -------\n\n     FG : Distribution\n        The function to be integrated as Distribution object (interpolator)\n\n    '''\n\n    if inverse_time is None:\n        raise Exception(\"Inverse time argument must be set!\")\n\n    # determine integration boundaries:\n    if inverse_time:\n        ## tau>g.xmin and t-tau<f.xmax\n        tau_min = max(t_val - f.xmax, g.xmin)\n        ## tau<g.xmax and t-tau>f.xmin\n        tau_max = min(t_val - f.xmin, g.xmax)\n    else:\n        ## tau>g.xmin and t+tau>f.xmin\n        tau_min = max(f.xmin-t_val, g.xmin)\n        ## tau<g.xmax and t+tau<f.xmax\n        tau_max = min(f.xmax-t_val, g.xmax)\n        #print(tau_min, tau_max)\n\n\n    if tau_max <= tau_min:\n        if return_log:\n            return ttconf.BIG_NUMBER\n        else:\n            return 0.0 #  functions do not overlap\n\n    else:\n        # create the tau-grid for the interpolation object in the overlap region\n        if inverse_time:\n            tau = np.unique(np.concatenate((g.x, t_val-f.x,[tau_min,tau_max])))\n        else:\n            tau = np.unique(np.concatenate((g.x, f.x-t_val,[tau_min,tau_max])))\n        tau = tau[(tau>tau_min-ttconf.TINY_NUMBER)&(tau<tau_max+ttconf.TINY_NUMBER)]\n        if len(tau)<10:\n            tau = np.linspace(tau_min, tau_max, 10)\n\n        if inverse_time: # add negative logarithms\n            tnode = t_val - tau\n            fg = f(tnode) + g(tau, tnode=tnode)\n        else:\n            fg = f(t_val + tau) + g(tau, tnode=t_val)\n\n        # create the interpolation object on this grid\n        FG = Distribution(tau, fg, is_log=True, min_width = np.max([f.min_width, g.min_width]),\n                          kind='linear', assume_sorted=True)\n        return FG", "response": "This function evaluates the integral of the function f and g and returns the result of the function f and g."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _max_of_integrand(t_val, f, g, inverse_time=None, return_log=False):\n\n    '''\n    Evaluates max_tau f(t+tau)*g(tau) or max_tau f(t-tau)g(tau) if inverse time is TRUE\n\n    Parameters\n    -----------\n\n     t_val : double\n        Time point\n\n     f : Interpolation object\n        First multiplier in convolution\n\n     g : Interpolation object\n        Second multiplier in convolution\n\n     inverse_time : bool, None\n        time direction. If True, then the f(t-tau)*g(tau) is calculated, otherwise,\n        f(t+tau)*g(tau)\n\n     return_log : bool\n        If True, the logarithm will be returned\n\n\n    Returns\n    -------\n\n     FG : Distribution\n        The function to be integrated as Distribution object (interpolator)\n\n    '''\n    # return log is always True\n    FG = _convolution_integrand(t_val, f, g, inverse_time, return_log=True)\n\n    if FG == ttconf.BIG_NUMBER:\n        res = ttconf.BIG_NUMBER, 0\n\n    else:\n        X = FG.x[FG.y.argmin()]\n        Y = FG.y.min()\n        res =  Y, X\n\n    if not return_log:\n        res[0] = np.log(res[0])\n\n\n    return res", "response": "Evaluates the maximum of an integral of a function."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _evaluate_convolution(t_val, f, g,  n_integral = 100, inverse_time=None, return_log=False):\n\n    FG = _convolution_integrand(t_val, f, g, inverse_time, return_log)\n\n    #integrate the interpolation object, return log, make neg_log\n        #print('FG:',FG.xmin, FG.xmax, FG(FG.xmin), FG(FG.xmax))\n    if (return_log and FG == ttconf.BIG_NUMBER) or \\\n        (not return_log and FG == 0.0): # distributions do not overlap\n        res = ttconf.BIG_NUMBER # we integrate log funcitons\n    else:\n        res = -FG.integrate(a=FG.xmin, b=FG.xmax, n=n_integral, return_log=True)\n\n    if return_log:\n        return res, -1\n    else:\n        return np.exp(-res), -1", "response": "Evaluate the convolution of a time series"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates H(t) = \\int_tau f(t-tau)g(tau) if inverse_time=True H(t) = \\int_tau f(t+tau)g(tau) if inverse_time=False This function determines the time points of the grid of the result to ensure an accurate approximation.", "response": "def convolve(cls, node_interp, branch_interp, max_or_integral='integral',\n                 n_grid_points = ttconf.NODE_GRID_SIZE, n_integral=ttconf.N_INTEGRAL,\n                 inverse_time=True, rel_tol=0.05, yc=10):\n\n        '''\n        calculate H(t) = \\int_tau f(t-tau)g(tau) if inverse_time=True\n                  H(t) = \\int_tau f(t+tau)g(tau) if inverse_time=False\n\n        This function determines the time points of the grid of the result to\n        ensure an accurate approximation.\n        '''\n\n        if max_or_integral not in ['max', 'integral']:\n            raise Exception(\"Max_or_integral expected to be 'max' or 'integral', got \"\n                            + str(max_or_integral)  + \" instead.\")\n\n        def conv_in_point(time_point):\n\n            if max_or_integral == 'integral': # compute integral of the convolution\n                return _evaluate_convolution(time_point, node_interp, branch_interp,\n                                               n_integral=n_integral, return_log=True,\n                                               inverse_time = inverse_time)\n\n            else: # compute max of the convolution\n                return _max_of_integrand(time_point, node_interp, branch_interp,\n                                               return_log=True, inverse_time = inverse_time)\n\n        # estimate peak and width\n        joint_fwhm  = (node_interp.fwhm + branch_interp.fwhm)\n        min_fwhm  = min(node_interp.fwhm, branch_interp.fwhm)\n        # determine support of the resulting convolution\n        # in order to be positive, the flipped support of f, shifted by t and g need to overlap\n        if inverse_time:\n            new_peak_pos = node_interp.peak_pos + branch_interp.peak_pos\n            tmin = node_interp.xmin+branch_interp.xmin\n            tmax = node_interp.xmax+branch_interp.xmax\n        else:\n            new_peak_pos = node_interp.peak_pos - branch_interp.peak_pos\n            tmin = node_interp.xmin - branch_interp.xmax\n            tmax = node_interp.xmax - branch_interp.xmin\n\n        # make initial node grid consisting of linearly spaced points around\n        # the center and quadratically spaced points at either end\n        n = n_grid_points/3\n        center_width = 3*joint_fwhm\n        grid_center = new_peak_pos + np.linspace(-1, 1, n)*center_width\n\n        # add the right and left grid if it is needed\n        right_range = (tmax - grid_center[-1])\n        if right_range>4*center_width:\n            grid_right = grid_center[-1] + right_range*(np.linspace(0, 1, n)**2.0)\n        elif right_range>0: # use linear grid the right_range is comparable to center_width\n            grid_right = grid_center[-1] + right_range*np.linspace(0,1, int(min(n,1+0.5*n*right_range/center_width)))\n        else:\n            grid_right =[]\n\n        left_range = grid_center[0]-tmin\n        if left_range>4*center_width:\n            grid_left = tmin + left_range*(np.linspace(0, 1, n)**2.0)\n        elif left_range>0:\n            grid_left = tmin + left_range*np.linspace(0,1, int(min(n,1+0.5*n*left_range/center_width)))\n        else:\n            grid_left =[]\n\n\n        if tmin>-1:\n            grid_zero_left = tmin + (tmax-tmin)*np.linspace(0,0.01,11)**2\n        else:\n            grid_zero_left = [tmin]\n        if tmax<1:\n            grid_zero_right = tmax - (tmax-tmin)*np.linspace(0,0.01,11)**2\n        else:\n            grid_zero_right = [tmax]\n\n        # make grid and calculate convolution\n        t_grid_0 = np.unique(np.concatenate([grid_zero_left, grid_left[:-1], grid_center, grid_right[1:], grid_zero_right]))\n        t_grid_0 = t_grid_0[(t_grid_0 > tmin-ttconf.TINY_NUMBER) & (t_grid_0 < tmax+ttconf.TINY_NUMBER)]\n\n        # res0 - the values of the convolution (integral or max)\n        # t_0  - the value, at which the res0 achieves maximum\n        #        (when determining the maximum of the integrand, otherwise meaningless)\n        res_0, t_0 = np.array([conv_in_point(t_val) for t_val in t_grid_0]).T\n\n        # refine grid as necessary and add new points\n        # calculate interpolation error at all internal points [2:-2] bc end points are sometime off scale\n        interp_error = np.abs(res_0[3:-1]+res_0[1:-3]-2*res_0[2:-2])\n        # determine the number of extra points needed, criterion depends on distance from peak dy\n        dy = (res_0[2:-2]-res_0.min())\n        dx = np.diff(t_grid_0)\n        refine_factor = np.minimum(np.minimum(np.array(np.floor(np.sqrt(interp_error/(rel_tol*(1+(dy/yc)**4)))), dtype=int),\n                                   np.array(100*(dx[1:-2]+dx[2:-1])/min_fwhm, dtype=int)), 10)\n\n        insert_point_idx = np.zeros(interp_error.shape[0]+1, dtype=int)\n        insert_point_idx[1:] = refine_factor\n        insert_point_idx[:-1] += refine_factor\n        # add additional points if there are any to add\n        if np.sum(insert_point_idx):\n            add_x = np.concatenate([np.linspace(t1,t2,n+2)[1:-1] for t1,t2,n in\n                               zip(t_grid_0[1:-2], t_grid_0[2:-1], insert_point_idx) if n>0])\n            # calculate convolution at these points\n            add_y, add_t = np.array([conv_in_point(t_val) for t_val in add_x]).T\n\n            t_grid_0 = np.concatenate((t_grid_0, add_x))\n            res_0 = np.concatenate ((res_0, add_y))\n            t_0 = np.concatenate ((t_0, add_t))\n\n        # instantiate the new interpolation object and return\n        res_y = cls(t_grid_0, res_0, is_log=True, kind='linear')\n\n        # the interpolation object, which is used to store the value of the\n        # grid, which maximizes the convolution (for 'max' option),\n        # or flat -1 distribution (for 'integral' option)\n        # this grid is the optimal branch length\n        res_t = Distribution(t_grid_0, t_0, is_log=True,\n                             min_width=node_interp.min_width, kind='linear')\n\n        return res_y, res_t"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef min_interp(interp_object):\n    try:\n        return interp_object.x[interp_object(interp_object.x).argmin()]\n    except Exception as e:\n        s = \"Cannot find minimum of the interpolation object\" + str(interp_object.x) + \\\n        \"Minimal x: \" + str(interp_object.x.min()) + \"Maximal x: \" + str(interp_object.x.max())\n        raise e", "response": "Find the global minimum of a function represented as an interpolation object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef median_interp(interp_object):\n    new_grid = np.sort(np.concatenate([interp_object.x[:-1] + 0.1*ii*np.diff(interp_object.x)\n                                       for ii in range(10)]).flatten())\n\n    tmp_prop = np.exp(-(interp_object(new_grid)-interp_object.y.min()))\n    tmp_cumsum = np.cumsum(0.5*(tmp_prop[1:]+tmp_prop[:-1])*np.diff(new_grid))\n    median_index = min(len(tmp_cumsum)-3, max(2,np.searchsorted(tmp_cumsum, tmp_cumsum[-1]*0.5)+1))\n    return new_grid[median_index]", "response": "Find the median of the function represented as an interpolation object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts datetime object to numeric date.", "response": "def numeric_date(dt=None):\n    \"\"\"\n    Convert datetime object to the numeric date.\n    The numeric date format is YYYY.F, where F is the fraction of the year passed\n\n    Parameters\n    ----------\n     dt:  datetime.datetime, None\n        date of to be converted. if None, assume today\n\n    \"\"\"\n    if dt is None:\n        dt = datetime.datetime.now()\n\n    try:\n        res = dt.year + dt.timetuple().tm_yday / 365.25\n    except:\n        res = None\n\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_dates(date_file):\n    print(\"\\nAttempting to parse dates...\")\n    dates = {}\n    if not os.path.isfile(date_file):\n        print(\"\\n\\tERROR: file %s does not exist, exiting...\"%date_file)\n        return dates\n    # separator for the csv/tsv file. If csv, we'll strip extra whitespace around ','\n    full_sep = '\\t' if date_file.endswith('.tsv') else r'\\s*,\\s*'\n\n    try:\n        # read the metadata file into pandas dataframe.\n        df = pd.read_csv(date_file, sep=full_sep, engine='python')\n        # check the metadata has strain names in the first column\n        # look for the column containing sampling dates\n        # We assume that the dates might be given either in human-readable format\n        # (e.g. ISO dates), or be already converted to the numeric format.\n        potential_date_columns = []\n        potential_numdate_columns = []\n        potential_index_columns = []\n        # Scan the dataframe columns and find ones which likely to store the\n        # dates\n        for ci,col in enumerate(df.columns):\n            d = df.iloc[0,ci]\n            # strip quotation marks\n            if type(d)==str and d[0] in ['\"', \"'\"] and d[-1] in ['\"', \"'\"]:\n                for i,tmp_d in enumerate(df.iloc[:,ci]):\n                    df.iloc[i,ci] = tmp_d.strip(d[0])\n            if 'date' in col.lower():\n                potential_date_columns.append((ci, col))\n            if any([x==col.lower() for x in ['name', 'strain', 'accession']]):\n                potential_index_columns.append((ci, col))\n\n        dates = {}\n        # if a potential numeric date column was found, use it\n        # (use the first, if there are more than one)\n        if not len(potential_index_columns):\n            print(\"ERROR: Cannot read metadata: need at least one column that contains the taxon labels.\"\n                  \" Looking for the first column that contains 'name', 'strain', or 'accession' in the header.\", file=sys.stderr)\n            return dates\n        else:\n            # use the first column that is either 'name', 'strain', 'accession'\n            index_col = sorted(potential_index_columns)[0][1]\n            print(\"\\tUsing column '%s' as name. This needs match the taxon names in the tree!!\"%index_col)\n\n        if len(potential_date_columns)>=1:\n            #try to parse the csv file with dates in the idx column:\n            idx = potential_date_columns[0][0]\n            col_name = potential_date_columns[0][1]\n            print(\"\\tUsing column '%s' as date.\"%col_name)\n            for ri, row in df.iterrows():\n                date_str = row.loc[col_name]\n                k = row.loc[index_col]\n                # try parsing as a float first\n                try:\n                    dates[k] = float(date_str)\n                    continue\n                except ValueError:\n                    # try whether the date string can be parsed as [2002.2:2004.3]\n                    # to indicate general ambiguous ranges\n                    if date_str[0]=='[' and date_str[-1]==']' and len(date_str[1:-1].split(':'))==2:\n                        try:\n                            dates[k] = [float(x) for x in date_str[1:-1].split(':')]\n                            continue\n                        except ValueError:\n                            pass\n                    # try date format parsing 2017-08-12\n                    try:\n                        tmp_date = pd.to_datetime(date_str)\n                        dates[k] = numeric_date(tmp_date)\n                    except ValueError:  # try ambiguous date format parsing 2017-XX-XX\n                        lower, upper = ambiguous_date_to_date_range(date_str, '%Y-%m-%d')\n                        if lower is not None:\n                            dates[k] = [numeric_date(x) for x in [lower, upper]]\n\n        else:\n            print(\"ERROR: Metadata file has no column which looks like a sampling date!\", file=sys.stderr)\n\n        if all(v is None for v in dates.values()):\n            print(\"ERROR: Cannot parse dates correctly! Check date format.\", file=sys.stderr)\n            return {}\n        return dates\n    except:\n        print(\"ERROR: Cannot read the metadata file!\", file=sys.stderr)\n        return {}", "response": "Parse the dates from the arguments and return a dictionary mapping taxon names to numerical dates."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing an ambiguous date string such as 2017 - xx - xx to 2017 - xx.", "response": "def ambiguous_date_to_date_range(mydate, fmt=\"%Y-%m-%d\", min_max_year=None):\n    \"\"\"parse an abiguous date such as 2017-XX-XX to [2017,2017.999]\n\n    Parameters\n    ----------\n    mydate : str\n        date string to be parsed\n    fmt : str\n        format descriptor. default is %Y-%m-%d\n    min_max_year : None, optional\n        if date is completely unknown, use this as bounds.\n\n    Returns\n    -------\n    tuple\n        upper and lower bounds on the date. return (None, None) if errors\n    \"\"\"\n    from datetime import datetime\n    sep = fmt.split('%')[1][-1]\n    min_date, max_date = {}, {}\n    today = datetime.today().date()\n\n    for val, field  in zip(mydate.split(sep), fmt.split(sep+'%')):\n        f = 'year' if 'y' in field.lower() else ('day' if 'd' in field.lower() else 'month')\n        if 'XX' in val:\n            if f=='year':\n                if min_max_year:\n                    min_date[f]=min_max_year[0]\n                    if len(min_max_year)>1:\n                        max_date[f]=min_max_year[1]\n                    elif len(min_max_year)==1:\n                        max_date[f]=4000 #will be replaced by 'today' below.\n                else:\n                    return None, None\n            elif f=='month':\n                min_date[f]=1\n                max_date[f]=12\n            elif f=='day':\n                min_date[f]=1\n                max_date[f]=31\n        else:\n            try:\n                min_date[f]=int(val)\n                max_date[f]=int(val)\n            except ValueError:\n                print(\"Can't parse date string: \"+mydate, file=sys.stderr)\n                return None, None\n    max_date['day'] = min(max_date['day'], 31 if max_date['month'] in [1,3,5,7,8,10,12]\n                                           else 28 if max_date['month']==2 else 30)\n    lower_bound = datetime(year=min_date['year'], month=min_date['month'], day=min_date['day']).date()\n    upper_bound = datetime(year=max_date['year'], month=max_date['month'], day=max_date['day']).date()\n    return (lower_bound, upper_bound if upper_bound<today else today)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating the object automatically from the TreeRegression object.", "response": "def from_regression(cls, clock_model):\n        \"\"\"\n        Create the conversion object automatically from the tree\n\n        Parameters\n        ----------\n\n         clock_model : dict\n            dictionary as returned from TreeRegression with fields intercept and slope\n\n        \"\"\"\n        dc = cls()\n        dc.clock_rate = clock_model['slope']\n        dc.intercept = clock_model['intercept']\n        dc.chisq = clock_model['chisq'] if 'chisq' in clock_model else None\n        dc.valid_confidence = clock_model['valid_confidence'] if 'valid_confidence' in clock_model else False\n        if 'cov' in clock_model and dc.valid_confidence:\n            dc.cov = clock_model['cov']\n        dc.r_val = clock_model['r_val']\n        return dc"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef client(self):\n        if not self._client:\n            self._client = socket.create_connection(\n                (self.host, self.port), self.timeout)\n            self.logger.debug('Client connected with guacd server (%s, %s, %s)'\n                              % (self.host, self.port, self.timeout))\n\n        return self._client", "response": "Returns a socket connection."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nclose connection with Guacamole guacd server.", "response": "def close(self):\n        \"\"\"\n        Terminate connection with Guacamole guacd server.\n        \"\"\"\n        self.client.close()\n        self._client = None\n        self.connected = False\n        self.logger.debug('Connection closed.')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreceive instructions from Guacamole guacd server.", "response": "def receive(self):\n        \"\"\"\n        Receive instructions from Guacamole guacd server.\n        \"\"\"\n        start = 0\n\n        while True:\n            idx = self._buffer.find(INST_TERM.encode(), start)\n            if idx != -1:\n                # instruction was fully received!\n                line = self._buffer[:idx + 1].decode()\n                self._buffer = self._buffer[idx + 1:]\n                self.logger.debug('Received instruction: %s' % line)\n                return line\n            else:\n                start = len(self._buffer)\n                # we are still waiting for instruction termination\n                buf = self.client.recv(BUF_LEN)\n                if not buf:\n                    # No data recieved, connection lost?!\n                    self.close()\n                    self.logger.debug(\n                        'Failed to receive instruction. Closing.')\n                    return None\n                self._buffer.extend(buf)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send(self, data):\n        self.logger.debug('Sending data: %s' % data)\n        self.client.sendall(data.encode())", "response": "Send data to guacd server."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send_instruction(self, instruction):\n        self.logger.debug('Sending instruction: %s' % str(instruction))\n        return self.send(instruction.encode())", "response": "Send an instruction after encoding."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nestablish connection with Guacamole guacd server via handshake.", "response": "def handshake(self, protocol='vnc', width=1024, height=768, dpi=96,\n                  audio=None, video=None, image=None, **kwargs):\n        \"\"\"\n        Establish connection with Guacamole guacd server via handshake.\n        \"\"\"\n        if protocol not in PROTOCOLS:\n            self.logger.debug('Invalid protocol: %s' % protocol)\n            raise GuacamoleError('Cannot start Handshake. Missing protocol.')\n\n        if audio is None:\n            audio = list()\n\n        if video is None:\n            video = list()\n\n        if image is None:\n            image = list()\n\n        # 1. Send 'select' instruction\n        self.logger.debug('Send `select` instruction.')\n        self.send_instruction(Instruction('select', protocol))\n\n        # 2. Receive `args` instruction\n        instruction = self.read_instruction()\n        self.logger.debug('Expecting `args` instruction, received: %s'\n                          % str(instruction))\n\n        if not instruction:\n            self.close()\n            raise GuacamoleError(\n                'Cannot establish Handshake. Connection Lost!')\n\n        if instruction.opcode != 'args':\n            self.close()\n            raise GuacamoleError(\n                'Cannot establish Handshake. Expected opcode `args`, '\n                'received `%s` instead.' % instruction.opcode)\n\n        # 3. Respond with size, audio & video support\n        self.logger.debug('Send `size` instruction (%s, %s, %s)'\n                          % (width, height, dpi))\n        self.send_instruction(Instruction('size', width, height, dpi))\n\n        self.logger.debug('Send `audio` instruction (%s)' % audio)\n        self.send_instruction(Instruction('audio', *audio))\n\n        self.logger.debug('Send `video` instruction (%s)' % video)\n        self.send_instruction(Instruction('video', *video))\n\n        self.logger.debug('Send `image` instruction (%s)' % image)\n        self.send_instruction(Instruction('image', *image))\n\n        # 4. Send `connect` instruction with proper values\n        connection_args = [\n            kwargs.get(arg.replace('-', '_'), '') for arg in instruction.args\n        ]\n\n        self.logger.debug('Send `connect` instruction (%s)' % connection_args)\n        self.send_instruction(Instruction('connect', *connection_args))\n\n        # 5. Receive ``ready`` instruction, with client ID.\n        instruction = self.read_instruction()\n        self.logger.debug('Expecting `ready` instruction, received: %s'\n                          % str(instruction))\n\n        if instruction.opcode != 'ready':\n            self.logger.warning(\n                'Expected `ready` instruction, received: %s instead')\n\n        if instruction.args:\n            self._id = instruction.args[0]\n            self.logger.debug(\n                'Established connection with client id: %s' % self.id)\n\n        self.logger.debug('Handshake completed.')\n        self.connected = True"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a utf - 8 encoded string from a valid unicode string.", "response": "def utf8(unicode_str):\n    \"\"\"\n    Return a utf-8 encoded string from a valid unicode string.\n\n    :param unicode_str: Unicode string.\n\n    :return: str\n    \"\"\"\n    if six.PY2 and isinstance(unicode_str, __unicode__):\n        return unicode_str.encode('utf-8')\n\n    return unicode_str"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load(cls, instruction):\n        if not instruction.endswith(INST_TERM):\n            raise InvalidInstruction('Instruction termination not found.')\n\n        args = cls.decode_instruction(instruction)\n\n        return cls(args[0], *args[1:])", "response": "Loads a GuacamoleInstruction from an encoded instruction string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndecode whole instruction and return list of args.", "response": "def decode_instruction(instruction):\n        \"\"\"\n        Decode whole instruction and return list of args.\n        Usually, returned arg[0] is the instruction opcode.\n\n        example:\n        >> args = decode_instruction('4.size,4.1024;')\n        >> args == ['size', '1024']\n        >> True\n\n        :param instruction: Instruction string.\n\n        :return: list\n        \"\"\"\n        if not instruction.endswith(INST_TERM):\n            raise InvalidInstruction('Instruction termination not found.')\n\n        # Use proper encoding\n        instruction = utf8(instruction)\n\n        # Get arg size\n        elems = instruction.split(ELEM_SEP, 1)\n\n        try:\n            arg_size = int(elems[0])\n        except Exception:\n            # Expected ValueError\n            raise InvalidInstruction(\n                'Invalid arg length.' +\n                ' Possibly due to missing element separator!')\n\n        arg_str = elems[1][:arg_size]\n\n        remaining = elems[1][arg_size:]\n\n        args = [arg_str]\n\n        if remaining.startswith(ARG_SEP):\n            # Ignore the ARG_SEP to parse next arg.\n            remaining = remaining[1:]\n        elif remaining == INST_TERM:\n            # This was the last arg!\n            return args\n        else:\n            # The remaining is neither starting with ARG_SEP nor INST_TERM.\n            raise InvalidInstruction(\n                'Instruction arg (%s) has invalid length.' % arg_str)\n\n        next_args = GuacamoleInstruction.decode_instruction(remaining)\n\n        if next_args:\n            args = args + next_args\n\n        return args"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef encode_arg(arg):\n        arg_utf8 = utf8(arg)\n\n        return ELEM_SEP.join([str(len(str(arg_utf8))), str(arg_utf8)])", "response": "Encode an argument to be sent in a valid GuacamoleInstruction."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef encode(self):\n        instruction_iter = itertools.chain([self.opcode], self.args)\n\n        elems = ARG_SEP.join(self.encode_arg(arg) for arg in instruction_iter)\n\n        return elems + INST_TERM", "response": "Encode the instruction to be sent over the wire."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a versioned URI string for this class", "response": "def class_url(cls):\n        \"\"\"Returns a versioned URI string for this class\"\"\"\n        base = 'v{0}'.format(getattr(cls, 'RESOURCE_VERSION', '1'))\n        return \"/{0}/{1}\".format(base, class_to_api_name(cls.class_name()))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget instance URL by ID", "response": "def instance_url(self):\n        \"\"\"Get instance URL by ID\"\"\"\n        id_ = self.get(self.ID_ATTR)\n        base = self.class_url()\n\n        if id_:\n            return '/'.join([base, six.text_type(id_)])\n        else:\n            raise Exception(\n                'Could not determine which URL to request: %s instance '\n                'has invalid ID: %r' % (type(self).__name__, id_),\n                self.ID_ATTR)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a versioned URI string for this class and don t pluralize the class name.", "response": "def class_url(cls):\n        \"\"\"\n        Returns a versioned URI string for this class,\n        and don't pluralize the class name.\n        \"\"\"\n        base = 'v{0}'.format(getattr(cls, 'RESOURCE_VERSION', '1'))\n        return \"/{0}/{1}\".format(base, class_to_api_name(\n            cls.class_name(), pluralize=False))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndownloads the file to the specified directory or file path.", "response": "def download(self, path=None, **kwargs):\n        \"\"\"\n        Download the file to the specified directory or file path.\n        Downloads to a temporary directory if no path is specified.\n\n        Returns the absolute path to the file.\n        \"\"\"\n        download_url = self.download_url(**kwargs)\n        try:\n            # For vault objects, use the object's filename\n            # as the fallback if none is specified.\n            filename = self.filename\n        except AttributeError:\n            # If the object has no filename attribute,\n            # extract one from the download URL.\n            filename = download_url.split('%3B%20filename%3D')[1]\n            # Remove additional URL params from the name and \"unquote\" it.\n            filename = unquote(filename.split('&')[0])\n\n        if path:\n            path = os.path.expanduser(path)\n            # If the path is a dir, use the extracted filename\n            if os.path.isdir(path):\n                path = os.path.join(path, filename)\n        else:\n            # Create a temporary directory for the file\n            path = os.path.join(tempfile.gettempdir(), filename)\n\n        try:\n            response = requests.request(method='get', url=download_url)\n        except Exception as e:\n            _handle_request_error(e)\n\n        if not (200 <= response.status_code < 400):\n            _handle_api_error(response)\n\n        with open(path, 'wb') as fileobj:\n            fileobj.write(response._content)\n\n        return path"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the commit objects parent Import or Migration", "response": "def parent_object(self):\n        \"\"\" Get the commit objects parent Import or Migration \"\"\"\n        from . import types\n        parent_klass = types.get(self.parent_job_model.split('.')[1])\n        return parent_klass.retrieve(self.parent_job_id, client=self._client)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nask the user for their email and password.", "response": "def _ask_for_credentials():\n    \"\"\"\n    Asks the user for their email and password.\n    \"\"\"\n    _print_msg('Please enter your SolveBio credentials')\n    domain = raw_input('Domain (e.g. <domain>.solvebio.com): ')\n    # Check to see if this domain supports password authentication\n    try:\n        account = client.request('get', '/p/accounts/{}'.format(domain))\n        auth = account['authentication']\n    except:\n        raise SolveError('Invalid domain: {}'.format(domain))\n\n    # Account must support password-based login\n    if auth.get('login') or auth.get('SAML', {}).get('simple_login'):\n        email = raw_input('Email: ')\n        password = getpass.getpass('Password (typing will be hidden): ')\n        return (domain, email, password)\n    else:\n        _print_msg(\n            'Your domain uses Single Sign-On (SSO). '\n            'Please visit https://{}.solvebio.com/settings/security '\n            'for instructions on how to log in.'.format(domain))\n        sys.exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef login(*args, **kwargs):\n    if args and args[0].api_key:\n        # Handle command-line arguments if provided.\n        solvebio.login(api_key=args[0].api_key)\n    elif kwargs:\n        # Run the global login() if kwargs are provided\n        # or local credentials are found.\n        solvebio.login(**kwargs)\n    else:\n        interactive_login()\n\n    # Print information about the current user\n    user = client.whoami()\n\n    if user:\n        print_user(user)\n        save_credentials(user['email'].lower(), solvebio.api_key)\n        _print_msg('Updated local credentials.')\n        return True\n    else:\n        _print_msg('Invalid credentials. You may not be logged-in.')\n        return False", "response": "Prompt user for login information."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef interactive_login():\n    solvebio.access_token = None\n    solvebio.api_key = None\n    client.set_token()\n\n    domain, email, password = _ask_for_credentials()\n    if not all([domain, email, password]):\n        print(\"Domain, email, and password are all required.\")\n        return\n\n    try:\n        response = client.post('/v1/auth/token', {\n            'domain': domain.replace('.solvebio.com', ''),\n            'email': email,\n            'password': password\n        })\n    except SolveError as e:\n        print('Login failed: {0}'.format(e))\n    else:\n        solvebio.api_key = response['token']\n        client.set_token()", "response": "Force an interactive login via the command line."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprinting information about the current user.", "response": "def whoami(*args, **kwargs):\n    \"\"\"\n    Prints information about the current user.\n    Assumes the user is already logged-in.\n    \"\"\"\n    user = client.whoami()\n\n    if user:\n        print_user(user)\n    else:\n        print('You are not logged-in.')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef print_user(user):\n    email = user['email']\n    domain = user['account']['domain']\n    role = user['role']\n    print('You are logged-in to the \"{0}\" domain '\n          'as {1} with role {2}.'\n          .format(domain, email, role))", "response": "Print the user information about the current user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_string(cls, string, exact=False):\n        try:\n            chromosome, pos = string.split(':')\n        except ValueError:\n            raise ValueError('Please use UCSC-style format: \"chr2:1000-2000\"')\n\n        if '-' in pos:\n            start, stop = pos.replace(',', '').split('-')\n        else:\n            start = stop = pos.replace(',', '')\n\n        return cls(chromosome, start, stop, exact=exact)", "response": "Create a new instance of the class from a string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef filter(self, *filters, **kwargs):\n        f = list(filters)\n\n        if kwargs:\n            f += [Filter(**kwargs)]\n\n        return self._clone(filters=f)", "response": "Returns a new Query instance with the specified filters combined with existing set with AND."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef range(self, chromosome, start, stop, exact=False):\n        return self._clone(\n            filters=[GenomicFilter(chromosome, start, stop, exact)])", "response": "Returns a new object with the range of genomic data for the specified chromosome."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a new instance with a single position filter on genomic datasets.", "response": "def position(self, chromosome, position, exact=False):\n        \"\"\"\n        Shortcut to do a single position filter on genomic datasets.\n        \"\"\"\n        return self._clone(\n            filters=[GenomicFilter(chromosome, position, exact=exact)])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a dictionary with the requested facets.", "response": "def facets(self, *args, **kwargs):\n        \"\"\"\n        Returns a dictionary with the requested facets.\n\n        The facets function supports string args, and keyword\n        args.\n\n        q.facets('field_1', 'field_2') will return facets for\n        field_1 and field_2.\n        q.facets(field_1={'limit': 0}, field_2={'limit': 10})\n        will return all facets for field_1 and 10 facets for field_2.\n        \"\"\"\n        # Combine args and kwargs into facet format.\n        facets = dict((a, {}) for a in args)\n        facets.update(kwargs)\n\n        if not facets:\n            raise AttributeError('Faceting requires at least one field')\n\n        for f in facets.keys():\n            if not isinstance(f, six.string_types):\n                raise AttributeError('Facet field arguments must be strings')\n\n        q = self._clone()\n        q._limit = 0\n        q.execute(offset=0, facets=facets)\n        return q._response.get('facets')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntakes a list of filters and returns a list of JSON API filters that can be used to create a new object.", "response": "def _process_filters(cls, filters):\n        \"\"\"Takes a list of filters and returns JSON\n\n        :Parameters:\n        - `filters`: List of Filters, (key, val) tuples, or dicts\n\n        Returns: List of JSON API filters\n        \"\"\"\n        data = []\n\n        # Filters should always be a list\n        for f in filters:\n            if isinstance(f, Filter):\n                if f.filters:\n                    data.extend(cls._process_filters(f.filters))\n            elif isinstance(f, dict):\n                key = list(f.keys())[0]\n                val = f[key]\n\n                if isinstance(val, dict):\n                    # pass val (a dict) as list\n                    # so that it gets processed properly\n                    filter_filters = cls._process_filters([val])\n                    if len(filter_filters) == 1:\n                        filter_filters = filter_filters[0]\n                    data.append({key: filter_filters})\n                else:\n                    data.append({key: cls._process_filters(val)})\n            else:\n                data.extend((f,))\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef next(self):\n        if not hasattr(self, '_cursor'):\n            # Iterator not initialized yet\n            self.__iter__()\n\n        # len(self) returns `min(limit, total)` results\n        if self._cursor == len(self):\n            raise StopIteration()\n\n        if self._buffer_idx == len(self._buffer):\n            self.execute(self._page_offset + self._buffer_idx)\n            self._buffer_idx = 0\n\n        self._cursor += 1\n        self._buffer_idx += 1\n        return self._buffer[self._buffer_idx - 1]", "response": "Returns the next result from the cache."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef execute(self, offset=0, **query):\n        _params = self._build_query(**query)\n        self._page_offset = offset\n\n        _params.update(\n            offset=self._page_offset,\n            limit=min(self._page_size, self._limit)\n        )\n\n        logger.debug('executing query. from/limit: %6d/%d' %\n                     (_params['offset'], _params['limit']))\n\n        # If the request results in a SolveError (ie bad filter) set the error.\n        try:\n            self._response = self._client.post(self._data_url, _params)\n        except SolveError as e:\n            self._error = e\n            raise\n\n        logger.debug('query response took: %(took)d ms, total: %(total)d'\n                     % self._response)\n        return _params, self._response", "response": "Executes a query and returns the request parameters and the raw query response."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmigrate the data from the Query to a target dataset.", "response": "def migrate(self, target, follow=True, **kwargs):\n        \"\"\"\n        Migrate the data from the Query to a target dataset.\n\n        Valid optional kwargs include:\n\n        * target_fields\n        * include_errors\n        * validation_params\n        * metadata\n        * commit_mode\n\n        \"\"\"\n        from solvebio import Dataset\n        from solvebio import DatasetMigration\n\n        # Target can be provided as a Dataset, or as an ID.\n        if isinstance(target, Dataset):\n            target_id = target.id\n        else:\n            target_id = target\n\n        # If a limit is set in the Query and not overridden here, use it.\n        limit = kwargs.pop('limit', None)\n        if not limit and self._limit < float('inf'):\n            limit = self._limit\n\n        # Build the source_params\n        params = self._build_query(limit=limit)\n        params.pop('offset', None)\n        params.pop('ordering', None)\n\n        migration = DatasetMigration.create(\n            source_id=self._dataset_id,\n            target_id=target_id,\n            source_params=params,\n            client=self._client,\n            **kwargs)\n\n        if follow:\n            migration.follow()\n\n        return migration"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlogs in to SolveBio.", "response": "def login(**kwargs):\n    \"\"\"\n    Sets up the auth credentials using the provided key/token,\n    or checks the credentials file (if no token provided).\n\n    Lookup order:\n        1. access_token\n        2. api_key\n        3. local credentials\n\n    No errors are raised if no key is found.\n    \"\"\"\n    from .cli.auth import get_credentials\n    global access_token, api_key, api_host\n\n    # Clear any existing auth keys\n    access_token, api_key = None, None\n    # Update the host\n    api_host = kwargs.get('api_host') or api_host\n\n    if kwargs.get('access_token'):\n        access_token = kwargs.get('access_token')\n    elif kwargs.get('api_key'):\n        api_key = kwargs.get('api_key')\n    else:\n        api_key = get_credentials()\n\n    if not (api_key or access_token):\n        print('No credentials found. Requests to SolveBio may fail.')\n    else:\n        from solvebio.client import client\n        # Update the client host and token\n        client.set_host()\n        client.set_token()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd subcommands to the base parser", "response": "def _add_subcommands(self):\n        \"\"\"\n            The _add_subcommands method must be separate from the __init__\n            method, as infinite recursion will occur otherwise, due to the fact\n            that the __init__ method itself will be called when instantiating\n            a subparser, as we do below\n        \"\"\"\n        subcmd_params = {\n            'title': 'SolveBio Commands',\n            'dest': 'subcommands'\n        }\n        subcmd = self.add_subparsers(\n            **subcmd_params)  # pylint: disable=star-args\n\n        subcommands = copy.deepcopy(self.subcommands)\n        for name, params in subcommands.items():\n            p = subcmd.add_parser(name, help=params['help'])\n            p.set_defaults(func=params['func'])\n            for arg in params.get('arguments', []):\n                name_or_flags = arg.pop('name', None) or arg.pop('flags', None)\n                p.add_argument(name_or_flags, **arg)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the args and add subcommands.", "response": "def parse_solvebio_args(self, args=None, namespace=None):\n        \"\"\"\n            Try to parse the args first, and then add the subparsers. We want\n            to do this so that we can check to see if there are any unknown\n            args. We can assume that if, by this point, there are no unknown\n            args, we can append shell to the unknown args as a default.\n            However, to do this, we have to suppress stdout/stderr during the\n            initial parsing, in case the user calls the help method (in which\n            case we want to add the additional arguments and *then* call the\n            help method. This is a hack to get around the fact that argparse\n            doesn't allow default subcommands.\n        \"\"\"\n        try:\n            sys.stdout = sys.stderr = open(os.devnull, 'w')\n            _, unknown_args = self.parse_known_args(args, namespace)\n            if not unknown_args:\n                args.insert(0, 'shell')\n        except SystemExit:\n            pass\n        finally:\n            sys.stdout.flush()\n            sys.stderr.flush()\n            sys.stdout, sys.stderr = sys.__stdout__, sys.__stderr__\n        self._add_subcommands()\n        return super(SolveArgumentParser, self).parse_args(args, namespace)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef download_vault_folder(remote_path, local_path, dry_run=False, force=False):\n\n    local_path = os.path.normpath(os.path.expanduser(local_path))\n    if not os.access(local_path, os.W_OK):\n        raise Exception(\n            'Write access to local path ({}) is required'\n            .format(local_path))\n\n    full_path, path_dict = solvebio.Object.validate_full_path(remote_path)\n    vault = solvebio.Vault.get_by_full_path(path_dict['vault'])\n    print('Downloading all files from {} to {}'.format(full_path, local_path))\n\n    if path_dict['path'] == '/':\n        parent_object_id = None\n    else:\n        parent_object = solvebio.Object.get_by_full_path(\n            remote_path, assert_type='folder')\n        parent_object_id = parent_object.id\n\n    # Scan the folder for all sub-folders and create them locally\n    print('Creating local directory structure at: {}'.format(local_path))\n    if not os.path.exists(local_path):\n        if not dry_run:\n            os.makedirs(local_path)\n\n    folders = vault.folders(parent_object_id=parent_object_id)\n    for f in folders:\n        path = os.path.normpath(local_path + f.path)\n        if not os.path.exists(path):\n            print('Creating folder: {}'.format(path))\n            if not dry_run:\n                os.makedirs(path)\n\n    files = vault.files(parent_object_id=parent_object_id)\n    for f in files:\n        path = os.path.normpath(local_path + f.path)\n        if os.path.exists(path):\n            if force:\n                # Delete the local copy\n                print('Deleting local file (force download): {}'.format(path))\n                if not dry_run:\n                    os.remove(path)\n            else:\n                print('Skipping file (already exists): {}'.format(path))\n                continue\n\n        print('Downloading file: {}'.format(path))\n        if not dry_run:\n            f.download(path)", "response": "Recursively downloads a folder in a vault to a local directory."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef construct_from(cls, values, **kwargs):\n        instance = cls(values.get(cls.ID_ATTR), **kwargs)\n        instance.refresh_from(values)\n        return instance", "response": "Used to create a new object from an HTTP response"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef logout(self):\n        if self._oauth_client_secret:\n            try:\n                oauth_token = flask.request.cookies[self.TOKEN_COOKIE_NAME]\n                # Revoke the token\n                requests.post(\n                    urljoin(self._api_host, self.OAUTH2_REVOKE_TOKEN_PATH),\n                    data={\n                        'client_id': self._oauth_client_id,\n                        'client_secret': self._oauth_client_secret,\n                        'token': oauth_token\n                    })\n            except:\n                pass\n\n        response = flask.redirect('/')\n        self.clear_cookies(response)\n        return response", "response": "Revoke the token and remove the cookie."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlaunches the SolveBio Python shell.", "response": "def launch_ipython_shell(args):  # pylint: disable=unused-argument\n    \"\"\"Open the SolveBio shell (IPython wrapper)\"\"\"\n    try:\n        import IPython  # noqa\n    except ImportError:\n        _print(\"The SolveBio Python shell requires IPython.\\n\"\n               \"To install, type: 'pip install ipython'\")\n        return False\n\n    if hasattr(IPython, \"version_info\"):\n        if IPython.version_info > (5, 0, 0, ''):\n            return launch_ipython_5_shell(args)\n\n    _print(\"WARNING: Please upgrade IPython (you are running version: {})\"\n           .format(IPython.__version__))\n    return launch_ipython_legacy_shell(args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlaunches the SolveBio Python shell with IPython 5 +", "response": "def launch_ipython_5_shell(args):\n    \"\"\"Open the SolveBio shell (IPython wrapper) with IPython 5+\"\"\"\n    import IPython  # noqa\n    from traitlets.config import Config\n\n    c = Config()\n    path = os.path.dirname(os.path.abspath(__file__))\n\n    try:\n        # see if we're already inside IPython\n        get_ipython  # pylint: disable=undefined-variable\n        _print(\"WARNING: Running IPython within IPython.\")\n    except NameError:\n        c.InteractiveShell.banner1 = 'SolveBio Python shell started.\\n'\n\n    c.InteractiveShellApp.exec_files = ['{}/ipython_init.py'.format(path)]\n    IPython.start_ipython(argv=[], config=c)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlaunches the SolveBio Python shell for older IPython versions.", "response": "def launch_ipython_legacy_shell(args):  # pylint: disable=unused-argument\n    \"\"\"Open the SolveBio shell (IPython wrapper) for older IPython versions\"\"\"\n    try:\n        from IPython.config.loader import Config\n    except ImportError:\n        _print(\"The SolveBio Python shell requires IPython.\\n\"\n               \"To install, type: 'pip install ipython'\")\n        return False\n\n    try:\n        # see if we're already inside IPython\n        get_ipython  # pylint: disable=undefined-variable\n    except NameError:\n        cfg = Config()\n        prompt_config = cfg.PromptManager\n        prompt_config.in_template = '[SolveBio] In <\\\\#>: '\n        prompt_config.in2_template = '   .\\\\D.: '\n        prompt_config.out_template = 'Out<\\\\#>: '\n        banner1 = '\\nSolveBio Python shell started.'\n\n        exit_msg = 'Quitting SolveBio shell.'\n    else:\n        _print(\"Running nested copies of IPython.\")\n        cfg = Config()\n        banner1 = exit_msg = ''\n\n    # First import the embeddable shell class\n    try:\n        from IPython.terminal.embed import InteractiveShellEmbed\n    except ImportError:\n        # pylint: disable=import-error,no-name-in-module\n        from IPython.frontend.terminal.embed import InteractiveShellEmbed\n\n    path = os.path.dirname(os.path.abspath(__file__))\n    init_file = '{}/ipython_init.py'.format(path)\n    exec(compile(open(init_file).read(), init_file, 'exec'),\n         globals(), locals())\n\n    InteractiveShellEmbed(config=cfg, banner1=banner1, exit_msg=exit_msg)()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nissue an HTTP GET across the wire via the Python requests library.", "response": "def get(self, url, params, **kwargs):\n        \"\"\"Issues an HTTP GET across the wire via the Python requests\n        library. See *request()* for information on keyword args.\"\"\"\n        kwargs['params'] = params\n        return self.request('GET', url, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete(self, url, data, **kwargs):\n        kwargs['data'] = data\n        return self.request('DELETE', url, **kwargs)", "response": "Issues an HTTP DELETE across the wire via the Python requests\n        library."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef request(self, method, url, **kwargs):\n\n        opts = {\n            'allow_redirects': True,\n            'auth': self._auth,\n            'data': {},\n            'files': None,\n            'headers': dict(self._headers),\n            'params': {},\n            'timeout': 80,\n            'verify': True\n        }\n\n        raw = kwargs.pop('raw', False)\n        debug = kwargs.pop('debug', False)\n        opts.update(kwargs)\n        method = method.upper()\n\n        if opts['files']:\n            # Don't use application/json for file uploads or GET requests\n            opts['headers'].pop('Content-Type', None)\n        else:\n            opts['data'] = json.dumps(opts['data'])\n\n        if not url.startswith(self._host):\n            url = urljoin(self._host, url)\n\n        logger.debug('API %s Request: %s' % (method, url))\n\n        if debug:\n            self._log_raw_request(method, url, **opts)\n\n        try:\n            response = self._session.request(method, url, **opts)\n        except Exception as e:\n            _handle_request_error(e)\n\n        if 429 == response.status_code:\n            delay = int(response.headers['retry-after']) + 1\n            logger.warn('Too many requests. Retrying in {0}s.'.format(delay))\n            time.sleep(delay)\n            return self.request(method, url, **kwargs)\n\n        if not (200 <= response.status_code < 400):\n            _handle_api_error(response)\n\n        # 204 is used on deletion. There is no JSON here.\n        if raw or response.status_code in [204, 301, 302]:\n            return response\n\n        return response.json()", "response": "Issues an HTTP request across the wire."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef child_object(self):\n        from . import types\n        child_klass = types.get(self.task_type.split('.')[1])\n        return child_klass.retrieve(self.task_id, client=self._client)", "response": "Get Task child object class"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nspecialize INFO field parser for SnpEff ANN fields.", "response": "def _parse_info_snpeff(self, info):\n        \"\"\"\n        Specialized INFO field parser for SnpEff ANN fields.\n        Requires self._snpeff_ann_fields to be set.\n        \"\"\"\n        ann = info.pop('ANN', []) or []\n        # Overwrite the existing ANN with something parsed\n        # Split on '|', merge with the ANN keys parsed above.\n        # Ensure empty values are None rather than empty string.\n        items = []\n        for a in ann:\n            # For multi-allelic records, we may have already\n            # processed ANN. If so, quit now.\n            if isinstance(a, dict):\n                info['ANN'] = ann\n                return info\n\n            values = [i or None for i in a.split('|')]\n            item = dict(zip(self._snpeff_ann_fields, values))\n\n            # Further split the Annotation field by '&'\n            if item.get('Annotation'):\n                item['Annotation'] = item['Annotation'].split('&')\n\n            items.append(item)\n\n        info['ANN'] = items\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef next(self):\n\n        def _alt(alt):\n            \"\"\"Parses the VCF row ALT object.\"\"\"\n            # If alt is '.' in VCF, PyVCF returns None, convert back to '.'\n            if not alt:\n                return '.'\n            else:\n                return str(alt)\n\n        if not self._next:\n            row = next(self.reader)\n            alternate_alleles = list(map(_alt, row.ALT))\n\n            for allele in alternate_alleles:\n                self._next.append(\n                    self.row_to_dict(\n                        row,\n                        allele=allele,\n                        alternate_alleles=alternate_alleles))\n\n            # Source line number, only increment if reading a new row.\n            self._line_number += 1\n\n        return self._next.pop()", "response": "Parses the next record in the VCF file and returns the record ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef row_to_dict(self, row, allele, alternate_alleles):\n\n        def _variant_sbid(**kwargs):\n            \"\"\"Generates a SolveBio variant ID (SBID).\"\"\"\n            return '{build}-{chromosome}-{start}-{stop}-{allele}'\\\n                .format(**kwargs).upper()\n\n        if allele == '.':\n            # Try to use the ref, if '.' is supplied for alt.\n            allele = row.REF or allele\n\n        genomic_coordinates = {\n            'build': self.genome_build,\n            'chromosome': row.CHROM,\n            'start': row.POS,\n            'stop': row.POS + len(row.REF) - 1\n        }\n\n        # SolveBio standard variant format\n        variant_sbid = _variant_sbid(allele=allele,\n                                     **genomic_coordinates)\n\n        return {\n            'genomic_coordinates': genomic_coordinates,\n            'variant': variant_sbid,\n            'allele': allele,\n            'row_id': row.ID,\n            'reference_allele': row.REF,\n            'alternate_alleles': alternate_alleles,\n            'info': self._parse_info(row.INFO),\n            'qual': row.QUAL,\n            'filter': row.FILTER\n        }", "response": "Return a parsed dictionary for JSON."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the user s stored API key if a valid credentials file is found. Raises CredentialsError if no valid credentials file is found.", "response": "def get_credentials():\n    \"\"\"\n    Returns the user's stored API key if a valid credentials file is found.\n    Raises CredentialsError if no valid credentials file is found.\n    \"\"\"\n    try:\n        netrc_path = netrc.path()\n        auths = netrc(netrc_path).authenticators(\n            urlparse(solvebio.api_host).netloc)\n    except (IOError, TypeError, NetrcParseError) as e:\n        raise CredentialsError(\n            'Could not open credentials file: ' + str(e))\n\n    if auths:\n        # auths = (login, account, password)\n        return auths[2]\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save(self, path):\n        rep = \"\"\n        for host in self.hosts.keys():\n            attrs = self.hosts[host]\n            rep = rep + \"machine \" + host + \"\\n\\tlogin \" \\\n                + six.text_type(attrs[0]) + \"\\n\"\n            if attrs[1]:\n                rep = rep + \"account \" + six.text_type(attrs[1])\n            rep = rep + \"\\tpassword \" + six.text_type(attrs[2]) + \"\\n\"\n        for macro in self.macros.keys():\n            rep = rep + \"macdef \" + macro + \"\\n\"\n            for line in self.macros[macro]:\n                rep = rep + line\n            rep = rep + \"\\n\"\n\n        f = open(path, 'w')\n        f.write(rep)\n        f.close()", "response": "Save the class data in the format of a. netrc file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if a string is an integer.", "response": "def _isint(string):\n    \"\"\"\n    >>> _isint(\"123\")\n    True\n    >>> _isint(\"123.45\")\n    False\n    \"\"\"\n    return type(string) is int or \\\n        (isinstance(string, _binary_type) or\n         isinstance(string, string_types)) and \\\n        _isconvertible(int, string)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\naligns a list of strings in a column of size minwidth.", "response": "def _align_column(strings, alignment, minwidth=0, has_invisible=True):\n    \"\"\"\n    [string] -> [padded_string]\n\n    >>> list(map(str,_align_column( \\\n        [\"12.345\", \"-1234.5\", \"1.23\", \"1234.5\", \\\n         \"1e+234\", \"1.0e234\"], \"decimal\")))\n    ['   12.345  ', '-1234.5    ', '    1.23   ', \\\n     ' 1234.5    ', '    1e+234 ', '    1.0e234']\n\n    \"\"\"\n    if alignment == \"right\":\n        strings = [s.strip() for s in strings]\n        padfn = _padleft\n    elif alignment in \"center\":\n        strings = [s.strip() for s in strings]\n        padfn = _padboth\n    elif alignment in \"decimal\":\n        decimals = [_afterpoint(s) for s in strings]\n        maxdecimals = max(decimals)\n        strings = [s + (maxdecimals - decs) * \" \"\n                   for s, decs in zip(strings, decimals)]\n        padfn = _padleft\n    else:\n        strings = [s.strip() for s in strings]\n        padfn = _padright\n\n    if has_invisible:\n        width_fn = _visible_width\n    else:\n        width_fn = len\n\n    maxwidth = max(max(list(map(width_fn, strings))), minwidth)\n    padded_strings = [padfn(maxwidth, s, has_invisible) for s in strings]\n    return padded_strings"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nformatting a value accoding to its type.", "response": "def _format(val, valtype, floatfmt, missingval=\"\"):\n    \"\"\"\n    Format a value accoding to its type.\n\n    Unicode is supported:\n\n    >>> hrow = ['\\u0431\\u0443\\u043a\\u0432\\u0430', \\\n                '\\u0446\\u0438\\u0444\\u0440\\u0430'] ; \\\n        tbl = [['\\u0430\\u0437', 2], ['\\u0431\\u0443\\u043a\\u0438', 4]] ; \\\n        good_result = '\\\\u0431\\\\u0443\\\\u043a\\\\u0432\\\\u0430      \\\n                        \\\\u0446\\\\u0438\\\\u0444\\\\u0440\\\\u0430\\\\n-------\\\n                          -------\\\\n\\\\u0430\\\\u0437             \\\n                          2\\\\n\\\\u0431\\\\u0443\\\\u043a\\\\u0438           4' ; \\\n        tabulate(tbl, headers=hrow) == good_result\n    True\n\n    \"\"\"\n    if val is None:\n        return missingval\n\n    if valtype in [int, _binary_type, _text_type]:\n        return \"{0}\".format(val)\n    elif valtype is float:\n        return format(float(val), floatfmt)\n    else:\n        return \"{0}\".format(val)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntransforms a tabular data type to a list of lists and a list of headers.", "response": "def _normalize_tabular_data(tabular_data, headers, sort=True):\n    \"\"\"\n    Transform a supported data type to a list of lists, and a list of headers.\n\n    Supported tabular data types:\n\n    * list-of-lists or another iterable of iterables\n\n    * 2D NumPy arrays\n\n    * dict of iterables (usually used with headers=\"keys\")\n\n    * pandas.DataFrame (usually used with headers=\"keys\")\n\n    The first row can be used as headers if headers=\"firstrow\",\n    column indices can be used as headers if headers=\"keys\".\n\n    \"\"\"\n\n    if hasattr(tabular_data, \"keys\") and hasattr(tabular_data, \"values\"):\n        # dict-like and pandas.DataFrame?\n        if hasattr(tabular_data.values, \"__call__\"):\n            # likely a conventional dict\n            keys = list(tabular_data.keys())\n            # columns have to be transposed\n            rows = list(izip_longest(*list(tabular_data.values())))\n        elif hasattr(tabular_data, \"index\"):\n            # values is a property, has .index then\n            # it's likely a pandas.DataFrame (pandas 0.11.0)\n            keys = list(tabular_data.keys())\n            # values matrix doesn't need to be transposed\n            vals = tabular_data.values\n            names = tabular_data.index\n            rows = [[v] + list(row) for v, row in zip(names, vals)]\n        else:\n            raise ValueError(\"tabular data doesn't appear to be a dict \"\n                             \"or a DataFrame\")\n\n        if headers == \"keys\":\n            headers = list(map(_text_type, keys))  # headers should be strings\n\n    else:  # it's, as usual, an iterable of iterables, or a NumPy array\n        rows = list(tabular_data)\n\n        if headers == \"keys\" and len(rows) > 0:  # keys are column indices\n            headers = list(map(_text_type, list(range(len(rows[0])))))\n\n    # take headers from the first row if necessary\n    if headers == \"firstrow\" and len(rows) > 0:\n        headers = list(map(_text_type, rows[0]))  # headers should be strings\n        rows = rows[1:]\n\n    headers = list(headers)\n\n    rows = list(map(list, rows))\n\n    if sort and len(rows) > 1:\n        rows = sorted(rows, key=lambda x: x[0])\n\n    # pad with empty headers for initial columns if necessary\n    if headers and len(rows) > 0:\n        nhs = len(headers)\n        ncols = len(rows[0])\n        if nhs < ncols:\n            headers = [\"\"] * (ncols - nhs) + headers\n\n    return rows, headers"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _build_row(cells, padding, begin, sep, end):\n    \"Return a string which represents a row of data cells.\"\n\n    pad = \" \" * padding\n    padded_cells = [pad + cell + pad for cell in cells]\n\n    # SolveBio: we're only displaying Key-Value tuples (dimension of 2).\n    #  enforce that we don't wrap lines by setting a max\n    #  limit on row width which is equal to TTY_COLS (see printing)\n    rendered_cells = (begin + sep.join(padded_cells) + end).rstrip()\n    if len(rendered_cells) > TTY_COLS:\n        if not cells[-1].endswith(\" \") and not cells[-1].endswith(\"-\"):\n            terminating_str = \" ... \"\n        else:\n            terminating_str = \"\"\n        rendered_cells = \"{0}{1}{2}\".format(\n            rendered_cells[:TTY_COLS - len(terminating_str) - 1],\n            terminating_str, end)\n\n    return rendered_cells", "response": "Return a string which represents a row of data cells."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a string which represents a horizontal line.", "response": "def _build_line(colwidths, padding, begin, fill, sep, end):\n    \"Return a string which represents a horizontal line.\"\n    cells = [fill * (w + 2 * padding) for w in colwidths]\n    return _build_row(cells, 0, begin, sep, end)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprefixes every cell in a row with an HTML alignment attribute.", "response": "def _mediawiki_cell_attrs(row, colaligns):\n    \"Prefix every cell in a row with an HTML alignment attribute.\"\n    alignment = {\"left\": '',\n                 \"right\": 'align=\"right\"| ',\n                 \"center\": 'align=\"center\"| ',\n                 \"decimal\": 'align=\"right\"| '}\n    row2 = [alignment[a] + c for c, a in zip(row, colaligns)]\n    return row2"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _line_segment_with_colons(linefmt, align, colwidth):\n    fill = linefmt.hline\n    w = colwidth\n    if align in [\"right\", \"decimal\"]:\n        return (fill[0] * (w - 1)) + \":\"\n    elif align == \"center\":\n        return \":\" + (fill[0] * (w - 2)) + \":\"\n    elif align == \"left\":\n        return \":\" + (fill[0] * (w - 1))\n    else:\n        return fill[0] * w", "response": "Return a horizontal line with optional colons which\n    indicate column s alignment."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nproducing a plain - text representation of the table.", "response": "def _format_table(fmt, headers, rows, colwidths, colaligns):\n    \"\"\"Produce a plain-text representation of the table.\"\"\"\n    lines = []\n    hidden = fmt.with_header_hide if headers else fmt.without_header_hide\n    pad = fmt.padding\n    headerrow = fmt.headerrow if fmt.headerrow else fmt.datarow\n\n    if fmt.lineabove and \"lineabove\" not in hidden:\n        lines.append(_build_line(colwidths, pad, *fmt.lineabove))\n\n    if headers:\n        lines.append(_build_row(headers, pad, *headerrow))\n\n    if fmt.linebelowheader and \"linebelowheader\" not in hidden:\n        begin, fill, sep, end = fmt.linebelowheader\n        if fmt.usecolons:\n            segs = [\n                _line_segment_with_colons(fmt.linebelowheader, a, w + 2 * pad)\n                for w, a in zip(colwidths, colaligns)]\n            lines.append(_build_row(segs, 0, begin, sep, end))\n        else:\n            lines.append(_build_line(colwidths, pad, *fmt.linebelowheader))\n\n    if rows and fmt.linebetweenrows and \"linebetweenrows\" not in hidden:\n        # initial rows with a line below\n        for row in rows[:-1]:\n            lines.append(_build_row(row, pad, *fmt.datarow))\n            lines.append(_build_line(colwidths, pad, *fmt.linebetweenrows))\n        # the last row without a line below\n        lines.append(_build_row(rows[-1], pad, *fmt.datarow))\n    else:\n        for row in rows:\n            lines.append(_build_row(row, pad, *fmt.datarow))\n\n    if fmt.linebelow and \"linebelow\" not in hidden:\n        lines.append(_build_line(colwidths, pad, *fmt.linebelow))\n\n    return \"\\n\".join(lines)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef import_file(self, path, **kwargs):\n        from . import Manifest\n        from . import DatasetImport\n\n        if 'id' not in self or not self['id']:\n            raise Exception(\n                'No Dataset ID found. '\n                'Please instantiate or retrieve a dataset '\n                'with an ID.')\n\n        manifest = Manifest()\n        manifest.add(path)\n        return DatasetImport.create(\n            dataset_id=self['id'],\n            manifest=manifest.manifest,\n            **kwargs)", "response": "Creates a new DatasetImport object for the given path."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef migrate(self, target, follow=True, **kwargs):\n        if 'id' not in self or not self['id']:\n            raise Exception(\n                'No source dataset ID found. '\n                'Please instantiate the Dataset '\n                'object with an ID.')\n\n        # Target can be provided as a Dataset, or as an ID.\n        if isinstance(target, Dataset):\n            target_id = target.id\n        else:\n            target_id = target\n\n        migration = DatasetMigration.create(\n            source_id=self['id'],\n            target_id=target_id,\n            **kwargs)\n\n        if follow:\n            migration.follow()\n\n        return migration", "response": "Migrate the data from this dataset to a target dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate_full_path(cls, full_path, **kwargs):\n        from solvebio.resource.vault import Vault\n\n        _client = kwargs.pop('client', None) or cls._client or client\n\n        if not full_path:\n            raise Exception(\n                'Invalid path: ',\n                'Full path must be in one of the following formats: '\n                '\"vault:/path\", \"domain:vault:/path\", or \"~/path\"')\n\n        # Parse the vault's full_path, using overrides if any\n        input_vault = kwargs.get('vault') or full_path\n        try:\n            vault_full_path, path_dict = \\\n                Vault.validate_full_path(input_vault, client=_client)\n        except Exception as err:\n            raise Exception('Could not determine vault from \"{0}\": {1}'\n                            .format(input_vault, err))\n\n        if kwargs.get('path'):\n            # Allow override of the object_path.\n            full_path = '{0}:/{1}'.format(vault_full_path, kwargs['path'])\n\n        match = cls.PATH_RE.match(full_path)\n        if match:\n            object_path = match.groupdict()['path']\n        else:\n            raise Exception(\n                'Cannot find a valid object path in \"{0}\". '\n                'Full path must be in one of the following formats: '\n                '\"vault:/path\", \"domain:vault:/path\", or \"~/path\"'\n                .format(full_path))\n\n        # Remove double slashes\n        object_path = re.sub('//+', '/', object_path)\n        if object_path != '/':\n            # Remove trailing slash\n            object_path = object_path.rstrip('/')\n\n        path_dict['path'] = object_path\n        # TODO: parent_path and filename\n        full_path = '{domain}:{vault}:{path}'.format(**path_dict)\n        path_dict['full_path'] = full_path\n        return full_path, path_dict", "response": "Validate a full path and return a dict containing the full path parts."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new dataset from a template ID or a JSON file.", "response": "def create_dataset(args):\n    \"\"\"\n    Attempt to create a new dataset given the following params:\n\n        * template_id\n        * template_file\n        * capacity\n        * create_vault\n        * [argument] dataset name or full path\n\n    NOTE: genome_build has been deprecated and is no longer used.\n\n    \"\"\"\n    # For backwards compatibility, the \"full_path\" argument\n    # can be a dataset filename, but only if vault and path\n    # are set. If vault/path are both provided and there\n    # are no forward-slashes in the \"full_path\", assume\n    # the user has provided a dataset filename.\n    if '/' not in args.full_path and args.vault and args.path:\n        full_path, path_dict = Object.validate_full_path(\n            '{0}:/{1}/{2}'.format(args.vault, args.path, args.full_path))\n    else:\n        full_path, path_dict = Object.validate_full_path(\n            args.full_path, vault=args.vault, path=args.path)\n\n    # Accept a template_id or a template_file\n    if args.template_id:\n        # Validate the template ID\n        try:\n            tpl = solvebio.DatasetTemplate.retrieve(args.template_id)\n        except solvebio.SolveError as e:\n            if e.status_code != 404:\n                raise e\n            print(\"No template with ID {0} found!\"\n                  .format(args.template_id))\n            sys.exit(1)\n    elif args.template_file:\n        mode = 'r'\n        fopen = open\n        if check_gzip_path(args.template_file):\n            mode = 'rb'\n            fopen = gzip.open\n\n        # Validate the template file\n        with fopen(args.template_file, mode) as fp:\n            try:\n                tpl_json = json.load(fp)\n            except:\n                print('Template file {0} could not be loaded. Please '\n                      'pass valid JSON'.format(args.template_file))\n                sys.exit(1)\n\n        tpl = solvebio.DatasetTemplate.create(**tpl_json)\n        print(\"A new dataset template was created with id: {0}\".format(tpl.id))\n    else:\n        print(\"Creating a new dataset {0} without a template.\"\n              .format(full_path))\n        tpl = None\n        fields = []\n        entity_type = None\n        description = None\n\n    if tpl:\n        print(\"Creating new dataset {0} using the template '{1}'.\"\n              .format(full_path, tpl.name))\n        fields = tpl.fields\n        entity_type = tpl.entity_type\n        # include template used to create\n        description = 'Created with dataset template: {0}'.format(str(tpl.id))\n\n    return solvebio.Dataset.get_or_create_by_full_path(\n        full_path,\n        capacity=args.capacity,\n        entity_type=entity_type,\n        fields=fields,\n        description=description,\n        create_vault=args.create_vault,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef upload(args):\n    base_remote_path, path_dict = Object.validate_full_path(\n        args.full_path, vault=args.vault, path=args.path)\n\n    # Assert the vault exists and is accessible\n    vault = Vault.get_by_full_path(path_dict['vault_full_path'])\n\n    # If not the vault root, validate remote path exists and is a folder\n    if path_dict['path'] != '/':\n        Object.get_by_full_path(base_remote_path, assert_type='folder')\n\n    for local_path in args.local_path:\n        local_path = local_path.rstrip('/')\n        local_start = os.path.basename(local_path)\n\n        if os.path.isdir(local_path):\n            _upload_folder(path_dict['domain'], vault,\n                           base_remote_path, local_path, local_start)\n        else:\n            Object.upload_file(local_path, path_dict['path'],\n                               vault.full_path)", "response": "Upload all the folders and files contained within a folder or file to the remote."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupload and import a file into a new dataset.", "response": "def import_file(args):\n    \"\"\"\n    Given a dataset and a local path, upload and import the file(s).\n\n    Command arguments (args):\n\n        * create_dataset\n        * template_id\n        * full_path\n        * vault (optional, overrides the vault in full_path)\n        * path (optional, overrides the path in full_path)\n        * commit_mode\n        * capacity\n        * file (list)\n        * follow (default: False)\n\n    \"\"\"\n    full_path, path_dict = Object.validate_full_path(\n        args.full_path, vault=args.vault, path=args.path)\n\n    # Ensure the dataset exists. Create if necessary.\n    if args.create_dataset:\n        dataset = create_dataset(args)\n    else:\n        try:\n            dataset = solvebio.Dataset.get_by_full_path(full_path)\n        except solvebio.SolveError as e:\n            if e.status_code != 404:\n                raise e\n\n            print(\"Dataset not found: {0}\".format(full_path))\n            print(\"Tip: use the --create-dataset flag \"\n                  \"to create one from a template\")\n            sys.exit(1)\n\n    # Generate a manifest from the local files\n    manifest = solvebio.Manifest()\n    manifest.add(*args.file)\n\n    # Create the manifest-based import\n    imp = solvebio.DatasetImport.create(\n        dataset_id=dataset.id,\n        manifest=manifest.manifest,\n        commit_mode=args.commit_mode\n    )\n\n    if args.follow:\n        imp.follow()\n    else:\n        mesh_url = 'https://my.solvebio.com/activity/'\n        print(\"Your import has been submitted, view details at: {0}\"\n              .format(mesh_url))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef validate_full_path(cls, full_path, **kwargs):\n        _client = kwargs.pop('client', None) or cls._client or client\n\n        full_path = full_path.strip()\n        if not full_path:\n            raise Exception(\n                'Vault path \"{0}\" is invalid. Path must be in the format: '\n                '\"domain:vault:/path\" or \"vault:/path\".'.format(full_path)\n            )\n\n        match = cls.VAULT_PATH_RE.match(full_path)\n        if not match:\n            raise Exception(\n                'Vault path \"{0}\" is invalid. Path must be in the format: '\n                '\"domain:vault:/path\" or \"vault:/path\".'.format(full_path)\n            )\n        path_parts = match.groupdict()\n\n        # Handle the special case where \"~\" means personal vault\n        if path_parts.get('vault') == '~':\n            path_parts = dict(domain=None, vault=None)\n\n        # If any values are None, set defaults from the user.\n        if None in path_parts.values():\n            user = _client.get('/v1/user', {})\n            defaults = {\n                'domain': user['account']['domain'],\n                'vault': 'user-{0}'.format(user['id'])\n            }\n            path_parts = dict((k, v or defaults.get(k))\n                              for k, v in path_parts.items())\n\n        # Rebuild the full path\n        full_path = '{domain}:{vault}'.format(**path_parts)\n        path_parts['vault_full_path'] = full_path\n        return full_path, path_parts", "response": "Validate a full path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nvalidating SolveBio API host url.", "response": "def validate_api_host_url(url):\n    \"\"\"\n    Validate SolveBio API host url.\n\n    Valid urls must not be empty and\n    must contain either HTTP or HTTPS scheme.\n    \"\"\"\n    if not url:\n        raise SolveError('No SolveBio API host is set')\n\n    parsed = urlparse(url)\n    if parsed.scheme not in ['http', 'https']:\n        raise SolveError(\n            'Invalid API host: %s. '\n            'Missing url scheme (HTTP or HTTPS).' % url\n        )\n\n    elif not parsed.netloc:\n        raise SolveError('Invalid API host: %s.' % url)\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd one or more files or URLs to the manifest.", "response": "def add(self, *args):\n        \"\"\"\n        Add one or more files or URLs to the manifest.\n        If files contains a glob, it is expanded.\n\n        All files are uploaded to SolveBio. The Upload\n        object is used to fill the manifest.\n        \"\"\"\n        def _is_url(path):\n            p = urlparse(path)\n            return bool(p.scheme)\n\n        for path in args:\n            path = os.path.expanduser(path)\n            if _is_url(path):\n                self.add_url(path)\n            elif os.path.isfile(path):\n                self.add_file(path)\n            elif os.path.isdir(path):\n                for f in os.listdir(path):\n                    self.add_file(f)\n            elif glob.glob(path):\n                for f in glob.glob(path):\n                    self.add_file(f)\n            else:\n                raise ValueError(\n                    'Path: \"{0}\" is not a valid format or does not exist. '\n                    'Manifest paths must be files, directories, or URLs.'\n                    .format(path)\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef annotate(self, records, **kwargs):\n        # Update annotator_params with any kwargs\n        self.annotator_params.update(**kwargs)\n        chunk_size = self.annotator_params.get('chunk_size', self.CHUNK_SIZE)\n\n        chunk = []\n        for i, record in enumerate(records):\n            chunk.append(record)\n            if (i + 1) % chunk_size == 0:\n                for r in self._execute(chunk):\n                    yield r\n                chunk = []\n\n        if chunk:\n            for r in self._execute(chunk):\n                yield r\n            chunk = []", "response": "Annotate a set of records with stored fields."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nevaluates the expression with the provided context and format.", "response": "def evaluate(self, data=None, data_type='string', is_list=False):\n        \"\"\"Evaluates the expression with the provided context and format.\"\"\"\n        payload = {\n            'data': data,\n            'expression': self.expr,\n            'data_type': data_type,\n            'is_list': is_list\n        }\n        res = self._client.post('/v1/evaluate', payload)\n        return res['result']"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef format_output(data, headers, format_name, **kwargs):\n    formatter = TabularOutputFormatter(format_name=format_name)\n    return formatter.format_output(data, headers, **kwargs)", "response": "Format output using format_name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef format_name(self, format_name):\n        if format_name in self.supported_formats:\n            self._format_name = format_name\n        else:\n            raise ValueError('unrecognized format_name \"{}\"'.format(\n                format_name))", "response": "Set the default format name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nregister a new output formatter.", "response": "def register_new_formatter(cls, format_name, handler, preprocessors=(),\n                               kwargs=None):\n        \"\"\"Register a new output formatter.\n\n        :param str format_name: The name of the format.\n        :param callable handler: The function that formats the data.\n        :param tuple preprocessors: The preprocessors to call before\n            formatting.\n        :param dict kwargs: Keys/values for keyword argument defaults.\n\n        \"\"\"\n        cls._output_formats[format_name] = OutputFormatHandler(\n            format_name, preprocessors, handler, kwargs or {})"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nformat the data using a specific formatter.", "response": "def format_output(self, data, headers, format_name=None,\n                      preprocessors=(), column_types=None, **kwargs):\n        \"\"\"Format the headers and data using a specific formatter.\n\n        *format_name* must be a supported formatter (see\n        :attr:`supported_formats`).\n\n        :param iterable data: An :term:`iterable` (e.g. list) of rows.\n        :param iterable headers: The column headers.\n        :param str format_name: The display format to use (optional, if the\n            :class:`TabularOutputFormatter` object has a default format set).\n        :param tuple preprocessors: Additional preprocessors to call before\n                                    any formatter preprocessors.\n        :param \\*\\*kwargs: Optional arguments for the formatter.\n        :return: The formatted data.\n        :rtype: str\n        :raises ValueError: If the *format_name* is not recognized.\n\n        \"\"\"\n        format_name = format_name or self._format_name\n        if format_name not in self.supported_formats:\n            raise ValueError('unrecognized format \"{}\"'.format(format_name))\n\n        (_, _preprocessors, formatter,\n         fkwargs) = self._output_formats[format_name]\n        fkwargs.update(kwargs)\n        if column_types is None:\n            data = list(data)\n            column_types = self._get_column_types(data)\n        for f in unique_items(preprocessors + _preprocessors):\n            data, headers = f(data, headers, column_types=column_types,\n                              **fkwargs)\n        return formatter(list(data), headers, column_types=column_types, **fkwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a list of the data types for each column in data.", "response": "def _get_column_types(self, data):\n        \"\"\"Get a list of the data types for each column in *data*.\"\"\"\n        columns = list(zip_longest(*data))\n        return [self._get_column_type(column) for column in columns]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the most generic data type for iterable column.", "response": "def _get_column_type(self, column):\n        \"\"\"Get the most generic data type for iterable *column*.\"\"\"\n        type_values = [TYPES[self._get_type(v)] for v in column]\n        inverse_types = {v: k for k, v in TYPES.items()}\n        return inverse_types[max(type_values)]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the data type for the value.", "response": "def _get_type(self, value):\n        \"\"\"Get the data type for *value*.\"\"\"\n        if value is None:\n            return type(None)\n        elif type(value) in int_types:\n            return int\n        elif type(value) in float_types:\n            return float\n        elif isinstance(value, binary_type):\n            return binary_type\n        else:\n            return text_type"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwraps tabulate inside a function for TabularOutputFormatter.", "response": "def adapter(data, headers, table_format=None, preserve_whitespace=False,\n            **kwargs):\n    \"\"\"Wrap tabulate inside a function for TabularOutputFormatter.\"\"\"\n    keys = ('floatfmt', 'numalign', 'stralign', 'showindex', 'disable_numparse')\n    tkwargs = {'tablefmt': table_format}\n    tkwargs.update(filter_dict_by_key(kwargs, keys))\n\n    if table_format in supported_markup_formats:\n        tkwargs.update(numalign=None, stralign=None)\n\n    tabulate.PRESERVE_WHITESPACE = preserve_whitespace\n\n    return iter(tabulate.tabulate(data, headers, **tkwargs).split('\\n'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the config folder for the user.", "response": "def get_user_config_dir(app_name, app_author, roaming=True, force_xdg=True):\n    \"\"\"Returns the config folder for the application.  The default behavior\n    is to return whatever is most appropriate for the operating system.\n\n    For an example application called ``\"My App\"`` by ``\"Acme\"``,\n    something like the following folders could be returned:\n\n    macOS (non-XDG):\n      ``~/Library/Application Support/My App``\n    Mac OS X (XDG):\n      ``~/.config/my-app``\n    Unix:\n      ``~/.config/my-app``\n    Windows 7 (roaming):\n      ``C:\\\\Users\\<user>\\AppData\\Roaming\\Acme\\My App``\n    Windows 7 (not roaming):\n      ``C:\\\\Users\\<user>\\AppData\\Local\\Acme\\My App``\n\n    :param app_name: the application name. This should be properly capitalized\n                     and can contain whitespace.\n    :param app_author: The app author's name (or company). This should be\n                       properly capitalized and can contain whitespace.\n    :param roaming: controls if the folder should be roaming or not on Windows.\n                    Has no effect on non-Windows systems.\n    :param force_xdg: if this is set to `True`, then on macOS the XDG Base\n                      Directory Specification will be followed. Has no effect\n                      on non-macOS systems.\n\n    \"\"\"\n    if WIN:\n        key = 'APPDATA' if roaming else 'LOCALAPPDATA'\n        folder = os.path.expanduser(os.environ.get(key, '~'))\n        return os.path.join(folder, app_author, app_name)\n    if MAC and not force_xdg:\n        return os.path.join(os.path.expanduser(\n            '~/Library/Application Support'), app_name)\n    return os.path.join(\n        os.path.expanduser(os.environ.get('XDG_CONFIG_HOME', '~/.config')),\n        _pathify(app_name))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_system_config_dirs(app_name, app_author, force_xdg=True):\n    if WIN:\n        folder = os.environ.get('PROGRAMDATA')\n        return [os.path.join(folder, app_author, app_name)]\n    if MAC and not force_xdg:\n        return [os.path.join('/Library/Application Support', app_name)]\n    dirs = os.environ.get('XDG_CONFIG_DIRS', '/etc/xdg')\n    paths = [os.path.expanduser(x) for x in dirs.split(os.pathsep)]\n    return [os.path.join(d, _pathify(app_name)) for d in paths]", "response": "r Returns a list of system - wide config folders for the application."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread the default config file.", "response": "def read_default_config(self):\n        \"\"\"Read the default config file.\n\n        :raises DefaultConfigValidationError: There was a validation error with\n                                              the *default* file.\n        \"\"\"\n        if self.validate:\n            self.default_config = ConfigObj(configspec=self.default_file,\n                                            list_values=False, _inspec=True,\n                                            encoding='utf8')\n            valid = self.default_config.validate(Validator(), copy=True,\n                                                 preserve_errors=True)\n            if valid is not True:\n                for name, section in valid.items():\n                    if section is True:\n                        continue\n                    for key, value in section.items():\n                        if isinstance(value, ValidateError):\n                            raise DefaultConfigValidationError(\n                                'section [{}], key \"{}\": {}'.format(\n                                    name, key, value))\n        elif self.default_file:\n            self.default_config, _ = self.read_config_file(self.default_file)\n\n        self.update(self.default_config)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read(self):\n        if self.default_file:\n            self.read_default_config()\n        return self.read_config_files(self.all_config_files())", "response": "Read the default additional system and user config files."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef user_config_file(self):\n        return os.path.join(\n            get_user_config_dir(self.app_name, self.app_author),\n            self.filename)", "response": "Get the absolute path to the user config file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a list of absolute paths to the system config files.", "response": "def system_config_files(self):\n        \"\"\"Get a list of absolute paths to the system config files.\"\"\"\n        return [os.path.join(f, self.filename) for f in get_system_config_dirs(\n            self.app_name, self.app_author)]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef additional_files(self):\n        return [os.path.join(f, self.filename) for f in self.additional_dirs]", "response": "Get a list of absolute paths to the additional config files."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite the default config to the user s config file.", "response": "def write_default_config(self, overwrite=False):\n        \"\"\"Write the default config to the user's config file.\n\n        :param bool overwrite: Write over an existing config if it exists.\n        \"\"\"\n        destination = self.user_config_file()\n        if not overwrite and os.path.exists(destination):\n            return\n\n        with io.open(destination, mode='wb') as f:\n            self.default_config.write(f)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write(self, outfile=None, section=None):\n        with io.open(outfile or self.user_config_file(), 'wb') as f:\n            self.data.write(outfile=f, section=section)", "response": "Write the current config to a file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_config_file(self, f):\n        configspec = self.default_file if self.validate else None\n        try:\n            config = ConfigObj(infile=f, configspec=configspec,\n                               interpolation=False, encoding='utf8')\n        except ConfigObjError as e:\n            logger.warning(\n                'Unable to parse line {} of config file {}'.format(\n                    e.line_number, f))\n            config = e.config\n\n        valid = True\n        if self.validate:\n            valid = config.validate(Validator(), preserve_errors=True,\n                                    copy=True)\n        if bool(config):\n            self.config_filenames.append(config.filename)\n\n        return config, valid", "response": "Read a config file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_config_files(self, files):\n        errors = {}\n        for _file in files:\n            config, valid = self.read_config_file(_file)\n            self.update(config)\n            if valid is not True:\n                errors[_file] = valid\n        return errors or True", "response": "Read a list of config files."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bytes_to_string(b):\n    if isinstance(b, binary_type):\n        try:\n            return b.decode('utf8')\n        except UnicodeDecodeError:\n            return '0x' + binascii.hexlify(b).decode('ascii')\n    return b", "response": "Convert bytes to a string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef filter_dict_by_key(d, keys):\n    return {k: v for k, v in d.items() if k in keys}", "response": "Filter the dict d to remove keys not in keys."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unique_items(seq):\n    seen = set()\n    return [x for x in seq if not (x in seen or seen.add(x))]", "response": "Return the unique items from iterable seq in order."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreplaces multiple values in a string", "response": "def replace(s, replace):\n    \"\"\"Replace multiple values in a string\"\"\"\n    for r in replace:\n        s = s.replace(*r)\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwraps the formatting inside a function for TabularOutputFormatter.", "response": "def adapter(data, headers, **kwargs):\n    \"\"\"Wrap the formatting inside a function for TabularOutputFormatter.\"\"\"\n    for row in chain((headers,), data):\n        yield \"\\t\".join((replace(r, (('\\n', r'\\n'), ('\\t', r'\\t'))) for r in row))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef call_and_exit(self, cmd, shell=True):\n        sys.exit(subprocess.call(cmd, shell=shell))", "response": "Run the cmd and exit with the proper exit code."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning multiple commmands in a row exiting if one fails.", "response": "def call_in_sequence(self, cmds, shell=True):\n        \"\"\"Run multiple commmands in a row, exiting if one fails.\"\"\"\n        for cmd in cmds:\n            if subprocess.call(cmd, shell=shell) == 1:\n                sys.exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\napplying command - line options.", "response": "def apply_options(self, cmd, options=()):\n        \"\"\"Apply command-line options.\"\"\"\n        for option in (self.default_cmd_options + options):\n            cmd = self.apply_option(cmd, option,\n                                    active=getattr(self, option, False))\n        return cmd"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\napply a command - line option.", "response": "def apply_option(self, cmd, option, active=True):\n        \"\"\"Apply a command-line option.\"\"\"\n        return re.sub(r'{{{}\\:(?P<option>[^}}]*)}}'.format(option),\n                      '\\g<option>' if active else '', cmd)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef initialize_options(self):\n        self.branch = 'master'\n        self.fix = False\n        super(lint, self).initialize_options()", "response": "Set the default options."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate and view the documentation.", "response": "def run(self):\n        \"\"\"Generate and view the documentation.\"\"\"\n        cmds = (self.clean_docs_cmd, self.html_docs_cmd, self.view_docs_cmd)\n        self.call_in_sequence(cmds)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef truncate_string(data, headers, max_field_width=None, **_):\n    return (([utils.truncate_string(v, max_field_width) for v in row] for row in data),\n            [utils.truncate_string(h, max_field_width) for h in headers])", "response": "Truncate very long strings. Only needed for tabular\n    representation."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef convert_to_string(data, headers, **_):\n    return (([utils.to_string(v) for v in row] for row in data),\n            [utils.to_string(h) for h in headers])", "response": "Convert all data and headers to strings."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\noverride missing values in the data with the given value.", "response": "def override_missing_value(data, headers, missing_value='', **_):\n    \"\"\"Override missing values in the *data* with *missing_value*.\n\n    A missing value is any value that is :data:`None`.\n\n    :param iterable data: An :term:`iterable` (e.g. list) of rows.\n    :param iterable headers: The column headers.\n    :param missing_value: The default value to use for missing data.\n    :return: The processed data and headers.\n    :rtype: tuple\n\n    \"\"\"\n    return (([missing_value if v is None else v for v in row] for row in data),\n            headers)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\noverrides tab values in the data with new_value.", "response": "def override_tab_value(data, headers, new_value='    ', **_):\n    \"\"\"Override tab values in the *data* with *new_value*.\n\n    :param iterable data: An :term:`iterable` (e.g. list) of rows.\n    :param iterable headers: The column headers.\n    :param new_value: The new value to use for tab.\n    :return: The processed data and headers.\n    :rtype: tuple\n\n    \"\"\"\n    return (([v.replace('\\t', new_value) if isinstance(v, text_type) else v\n              for v in row] for row in data),\n            headers)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts all data and headers bytes to strings.", "response": "def bytes_to_string(data, headers, **_):\n    \"\"\"Convert all *data* and *headers* bytes to strings.\n\n    Binary data that cannot be decoded is converted to a hexadecimal\n    representation via :func:`binascii.hexlify`.\n\n    :param iterable data: An :term:`iterable` (e.g. list) of rows.\n    :param iterable headers: The column headers.\n    :return: The processed data and headers.\n    :rtype: tuple\n\n    \"\"\"\n    return (([utils.bytes_to_string(v) for v in row] for row in data),\n            [utils.bytes_to_string(h) for h in headers])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nalign numbers in data on their decimal points.", "response": "def align_decimals(data, headers, column_types=(), **_):\n    \"\"\"Align numbers in *data* on their decimal points.\n\n    Whitespace padding is added before a number so that all numbers in a\n    column are aligned.\n\n    Outputting data before aligning the decimals::\n\n        1\n        2.1\n        10.59\n\n    Outputting data after aligning the decimals::\n\n         1\n         2.1\n        10.59\n\n    :param iterable data: An :term:`iterable` (e.g. list) of rows.\n    :param iterable headers: The column headers.\n    :param iterable column_types: The columns' type objects (e.g. int or float).\n    :return: The processed data and headers.\n    :rtype: tuple\n\n    \"\"\"\n    pointpos = len(headers) * [0]\n    data = list(data)\n    for row in data:\n        for i, v in enumerate(row):\n            if column_types[i] is float and type(v) in float_types:\n                v = text_type(v)\n                pointpos[i] = max(utils.intlen(v), pointpos[i])\n\n    def results(data):\n        for row in data:\n            result = []\n            for i, v in enumerate(row):\n                if column_types[i] is float and type(v) in float_types:\n                    v = text_type(v)\n                    result.append((pointpos[i] - utils.intlen(v)) * \" \" + v)\n                else:\n                    result.append(v)\n            yield result\n\n    return results(data), headers"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef quote_whitespaces(data, headers, quotestyle=\"'\", **_):\n    whitespace = tuple(string.whitespace)\n    quote = len(headers) * [False]\n    data = list(data)\n    for row in data:\n        for i, v in enumerate(row):\n            v = text_type(v)\n            if v.startswith(whitespace) or v.endswith(whitespace):\n                quote[i] = True\n\n    def results(data):\n        for row in data:\n            result = []\n            for i, v in enumerate(row):\n                quotation = quotestyle if quote[i] else ''\n                result.append('{quotestyle}{value}{quotestyle}'.format(\n                    quotestyle=quotation, value=v))\n            yield result\n    return results(data), headers", "response": "Quote leading or trailing whitespace in data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstyle the data and headers.", "response": "def style_output(data, headers, style=None,\n                 header_token='Token.Output.Header',\n                 odd_row_token='Token.Output.OddRow',\n                 even_row_token='Token.Output.EvenRow', **_):\n    \"\"\"Style the *data* and *headers* (e.g. bold, italic, and colors)\n\n    .. NOTE::\n        This requires the `Pygments <http://pygments.org/>`_ library to\n        be installed. You can install it with CLI Helpers as an extra::\n            $ pip install cli_helpers[styles]\n\n    Example usage::\n\n        from cli_helpers.tabular_output.preprocessors import style_output\n        from pygments.style import Style\n        from pygments.token import Token\n\n        class YourStyle(Style):\n            default_style = \"\"\n            styles = {\n                Token.Output.Header: 'bold #ansired',\n                Token.Output.OddRow: 'bg:#eee #111',\n                Token.Output.EvenRow: '#0f0'\n            }\n\n        headers = ('First Name', 'Last Name')\n        data = [['Fred', 'Roberts'], ['George', 'Smith']]\n\n        data, headers = style_output(data, headers, style=YourStyle)\n\n    :param iterable data: An :term:`iterable` (e.g. list) of rows.\n    :param iterable headers: The column headers.\n    :param str/pygments.style.Style style: A Pygments style. You can `create\n        your own styles <http://pygments.org/docs/styles/#creating-own-styles>`_.\n    :param str header_token: The token type to be used for the headers.\n    :param str odd_row_token: The token type to be used for odd rows.\n    :param str even_row_token: The token type to be used for even rows.\n    :return: The styled data and headers.\n    :rtype: tuple\n\n    \"\"\"\n    if style and HAS_PYGMENTS:\n        formatter = Terminal256Formatter(style=style)\n\n        def style_field(token, field):\n            \"\"\"Get the styled text for a *field* using *token* type.\"\"\"\n            s = StringIO()\n            formatter.format(((token, field),), s)\n            return s.getvalue()\n\n        headers = [style_field(header_token, header) for header in headers]\n        data = ([style_field(odd_row_token if i % 2 else even_row_token, f)\n                 for f in r] for i, r in enumerate(data, 1))\n\n    return iter(data), headers"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef format_numbers(data, headers, column_types=(), integer_format=None,\n                   float_format=None, **_):\n    \"\"\"Format numbers according to a format specification.\n\n    This uses Python's format specification to format numbers of the following\n    types: :class:`int`, :class:`py2:long` (Python 2), :class:`float`, and\n    :class:`~decimal.Decimal`. See the :ref:`python:formatspec` for more\n    information about the format strings.\n\n    .. NOTE::\n       A column is only formatted if all of its values are the same type\n       (except for :data:`None`).\n\n    :param iterable data: An :term:`iterable` (e.g. list) of rows.\n    :param iterable headers: The column headers.\n    :param iterable column_types: The columns' type objects (e.g. int or float).\n    :param str integer_format: The format string to use for integer columns.\n    :param str float_format: The format string to use for float columns.\n    :return: The processed data and headers.\n    :rtype: tuple\n\n    \"\"\"\n    if (integer_format is None and float_format is None) or not column_types:\n        return iter(data), headers\n\n    def _format_number(field, column_type):\n        if integer_format and column_type is int and type(field) in int_types:\n            return format(field, integer_format)\n        elif float_format and column_type is float and type(field) in float_types:\n            return format(field, float_format)\n        return field\n\n    data = ([_format_number(v, column_types[i]) for i, v in enumerate(row)] for row in data)\n    return data, headers", "response": "Formats the number of the data according to a format specification."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a row separator for row num.", "response": "def _get_separator(num, sep_title, sep_character, sep_length):\n    \"\"\"Get a row separator for row *num*.\"\"\"\n    left_divider_length = right_divider_length = sep_length\n    if isinstance(sep_length, tuple):\n        left_divider_length, right_divider_length = sep_length\n    left_divider = sep_character * left_divider_length\n    right_divider = sep_character * right_divider_length\n    title = sep_title.format(n=num + 1)\n\n    return \"{left_divider}[ {title} ]{right_divider}\\n\".format(\n        left_divider=left_divider, right_divider=right_divider, title=title)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef vertical_table(data, headers, sep_title='{n}. row', sep_character='*',\n                   sep_length=27):\n    \"\"\"Format *data* and *headers* as an vertical table.\n\n    The values in *data* and *headers* must be strings.\n\n    :param iterable data: An :term:`iterable` (e.g. list) of rows.\n    :param iterable headers: The column headers.\n    :param str sep_title: The title given to each row separator. Defaults to\n                          ``'{n}. row'``. Any instance of ``'{n}'`` is\n                          replaced by the record number.\n    :param str sep_character: The character used to separate rows. Defaults to\n                              ``'*'``.\n    :param int/tuple sep_length: The number of separator characters that should\n                                 appear on each side of the *sep_title*. Use\n                                 a tuple to specify the left and right values\n                                 separately.\n    :return: The formatted data.\n    :rtype: str\n\n    \"\"\"\n    header_len = max([len(x) for x in headers])\n    padded_headers = [x.ljust(header_len) for x in headers]\n    formatted_rows = [_format_row(padded_headers, row) for row in data]\n\n    output = []\n    for i, result in enumerate(formatted_rows):\n        yield _get_separator(i, sep_title, sep_character, sep_length) + result", "response": "Format data and headers as a vertical table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef adapter(data, headers, **kwargs):\n    keys = ('sep_title', 'sep_character', 'sep_length')\n    return vertical_table(data, headers, **filter_dict_by_key(kwargs, keys))", "response": "Wrap vertical table in a function for TabularOutputFormatter."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef adapter(data, headers, table_format='csv', **kwargs):\n    keys = ('dialect', 'delimiter', 'doublequote', 'escapechar',\n            'quotechar', 'quoting', 'skipinitialspace', 'strict')\n    if table_format == 'csv':\n        delimiter = ','\n    elif table_format == 'csv-tab':\n        delimiter = '\\t'\n    else:\n        raise ValueError('Invalid table_format specified.')\n\n    ckwargs = {'delimiter': delimiter, 'lineterminator': ''}\n    ckwargs.update(filter_dict_by_key(kwargs, keys))\n\n    l = linewriter()\n    writer = csv.writer(l, **ckwargs)\n    writer.writerow(headers)\n    yield l.line\n\n    for row in data:\n        l.reset()\n        writer.writerow(row)\n        yield l.line", "response": "Wrap the formatting inside a function for TabularOutputFormatter."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef adapter(data, headers, table_format=None, **kwargs):\n    keys = ('title', )\n\n    table = table_format_handler[table_format]\n\n    t = table([headers] + list(data), **filter_dict_by_key(kwargs, keys))\n\n    dimensions = terminaltables.width_and_alignment.max_dimensions(\n        t.table_data,\n        t.padding_left,\n        t.padding_right)[:3]\n    for r in t.gen_table(*dimensions):\n        yield u''.join(r)", "response": "Wrap terminaltables inside a function for TabularOutputFormatter."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncopying template and substitute template strings File template_file is copied to dst_file.", "response": "def render_template(template_file, dst_file, **kwargs):\n    \"\"\"Copy template and substitute template strings\n\n    File `template_file` is copied to `dst_file`. Then, each template variable\n    is replaced by a value. Template variables are of the form\n\n        {{val}}\n\n    Example:\n\n    Contents of template_file:\n\n        VAR1={{val1}}\n        VAR2={{val2}}\n        VAR3={{val3}}\n\n    render_template(template_file, output_file, val1=\"hello\", val2=\"world\")\n\n    Contents of output_file:\n\n        VAR1=hello\n        VAR2=world\n        VAR3={{val3}}\n\n    :param template_file: Path to the template file.\n    :param dst_file: Path to the destination file.\n    :param kwargs: Keys correspond to template variables.\n    :return:\n    \"\"\"\n    with open(template_file) as f:\n        template_text = f.read()\n\n    dst_text = template_text\n\n    for key, value in kwargs.iteritems():\n        dst_text = dst_text .replace(\"{{\" + key + \"}}\", value)\n\n    with open(dst_file, \"wt\") as f:\n        f.write(dst_text)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_dict(self):\n        # all the attibutes defined by PKCS#11\n        all_attributes = PyKCS11.CKA.keys()\n\n        # only use the integer values and not the strings like 'CKM_RSA_PKCS'\n        all_attributes = [attr for attr in all_attributes if\n                          isinstance(attr, int)]\n\n        # all the attributes of the object\n        attributes = self.session.getAttributeValue(self, all_attributes)\n\n        dico = dict()\n        for key, attr in zip(all_attributes, attributes):\n            if attr is None:\n                continue\n            if key == CKA_CLASS:\n                dico[PyKCS11.CKA[key]] = PyKCS11.CKO[attr]\n            elif key == CKA_CERTIFICATE_TYPE:\n                dico[PyKCS11.CKA[key]] = PyKCS11.CKC[attr]\n            elif key == CKA_KEY_TYPE:\n                dico[PyKCS11.CKA[key]] = PyKCS11.CKK[attr]\n            else:\n                dico[PyKCS11.CKA[key]] = attr\n        return dico", "response": "convert the fields of the object into a dictionnary of the object"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef flags2text(self):\n        r = []\n        for v in self.flags_dict.keys():\n            if self.flags & v:\n                r.append(self.flags_dict[v])\n        return r", "response": "parse the self. flags field and create a list of strings\n        corresponding to bits set in flags\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert the fields of the object into a dictionnary", "response": "def to_dict(self):\n        \"\"\"\n        convert the fields of the object into a dictionnary\n        \"\"\"\n        dico = dict()\n        for field in self.fields.keys():\n            if field == \"flags\":\n                dico[field] = self.flags2text()\n            elif field == \"state\":\n                dico[field] = self.state2text()\n            else:\n                dico[field] = eval(\"self.\" + field)\n        return dico"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading a PKCS#11 library into the object", "response": "def load(self, pkcs11dll_filename=None, *init_string):\n        \"\"\"\n        load a PKCS#11 library\n\n        :type pkcs11dll_filename: string\n        :param pkcs11dll_filename: the library name.\n          If this parameter is not set then the environment variable\n          `PYKCS11LIB` is used instead\n        :returns: a :class:`PyKCS11Lib` object\n        :raises: :class:`PyKCS11Error` (-1): when the load fails\n        \"\"\"\n        if pkcs11dll_filename is None:\n            pkcs11dll_filename = os.getenv(\"PYKCS11LIB\")\n            if pkcs11dll_filename is None:\n                raise PyKCS11Error(-1, \"No PKCS11 library specified (set PYKCS11LIB env variable)\")\n        rv = self.lib.Load(pkcs11dll_filename)\n        if rv == 0:\n            raise PyKCS11Error(-1, pkcs11dll_filename)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getSlotList(self, tokenPresent=False):\n        slotList = PyKCS11.LowLevel.ckintlist()\n        rv = self.lib.C_GetSlotList(CK_TRUE if tokenPresent else CK_FALSE,\n                                    slotList)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n\n        s = []\n        for x in range(len(slotList)):\n            s.append(slotList[x])\n        return s", "response": "A method to get a list of available slots."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getSlotInfo(self, slot):\n        slotInfo = PyKCS11.LowLevel.CK_SLOT_INFO()\n        rv = self.lib.C_GetSlotInfo(slot, slotInfo)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n\n        s = CK_SLOT_INFO()\n        s.slotDescription = slotInfo.GetSlotDescription()\n        s.manufacturerID = slotInfo.GetManufacturerID()\n        s.flags = slotInfo.flags\n        s.hardwareVersion = slotInfo.GetHardwareVersion()\n        s.firmwareVersion = slotInfo.GetFirmwareVersion()\n\n        return s", "response": "C_GetSlotInfo returns the information about a slot."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getTokenInfo(self, slot):\n        tokeninfo = PyKCS11.LowLevel.CK_TOKEN_INFO()\n        rv = self.lib.C_GetTokenInfo(slot, tokeninfo)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n\n        t = CK_TOKEN_INFO()\n        t.label = tokeninfo.GetLabel()\n        t.manufacturerID = tokeninfo.GetManufacturerID()\n        t.model = tokeninfo.GetModel()\n        t.serialNumber = tokeninfo.GetSerialNumber()\n        t.flags = tokeninfo.flags\n        t.ulMaxSessionCount = tokeninfo.ulMaxSessionCount\n        if t.ulMaxSessionCount == CK_UNAVAILABLE_INFORMATION:\n            t.ulMaxSessionCount = -1\n        t.ulSessionCount = tokeninfo.ulSessionCount\n        if t.ulSessionCount == CK_UNAVAILABLE_INFORMATION:\n            t.ulSessionCount = -1\n        t.ulMaxRwSessionCount = tokeninfo.ulMaxRwSessionCount\n        if t.ulMaxRwSessionCount == CK_UNAVAILABLE_INFORMATION:\n            t.ulMaxRwSessionCount = -1\n        t.ulRwSessionCount = tokeninfo.ulRwSessionCount\n        if t.ulRwSessionCount == CK_UNAVAILABLE_INFORMATION:\n            t.ulRwSessionCount = -1\n        t.ulMaxPinLen = tokeninfo.ulMaxPinLen\n        t.ulMinPinLen = tokeninfo.ulMinPinLen\n        t.ulTotalPublicMemory = tokeninfo.ulTotalPublicMemory\n\n        if t.ulTotalPublicMemory == CK_UNAVAILABLE_INFORMATION:\n            t.ulTotalPublicMemory = -1\n        t.ulFreePublicMemory = tokeninfo.ulFreePublicMemory\n        if t.ulFreePublicMemory == CK_UNAVAILABLE_INFORMATION:\n            t.ulFreePublicMemory = -1\n        t.ulTotalPrivateMemory = tokeninfo.ulTotalPrivateMemory\n        if t.ulTotalPrivateMemory == CK_UNAVAILABLE_INFORMATION:\n            t.ulTotalPrivateMemory = -1\n        t.ulFreePrivateMemory = tokeninfo.ulFreePrivateMemory\n        if t.ulFreePrivateMemory == CK_UNAVAILABLE_INFORMATION:\n            t.ulFreePrivateMemory = -1\n        t.hardwareVersion = (tokeninfo.hardwareVersion.major,\n                             tokeninfo.hardwareVersion.minor)\n        t.firmwareVersion = (tokeninfo.firmwareVersion.major,\n                             tokeninfo.firmwareVersion.minor)\n        t.utcTime = tokeninfo.GetUtcTime().replace('\\000', ' ')\n\n        return t", "response": "C_GetTokenInfo\n        - Gets the information for a specific token."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef openSession(self, slot, flags=0):\n        se = PyKCS11.LowLevel.CK_SESSION_HANDLE()\n        flags |= CKF_SERIAL_SESSION\n        rv = self.lib.C_OpenSession(slot, flags, se)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n\n        return Session(self, se)", "response": "This method returns a new session object for the given slot."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nclose all the session for the specified slot.", "response": "def closeAllSessions(self, slot):\n        \"\"\"\n        C_CloseAllSessions\n\n        :param slot: slot number\n        :type slot: integer\n        \"\"\"\n        rv = self.lib.C_CloseAllSessions(slot)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getMechanismList(self, slot):\n        mechanismList = PyKCS11.LowLevel.ckintlist()\n        rv = self.lib.C_GetMechanismList(slot, mechanismList)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n\n        m = []\n        for x in range(len(mechanismList)):\n            mechanism = mechanismList[x]\n            if mechanism >= CKM_VENDOR_DEFINED:\n                k = 'CKR_VENDOR_DEFINED_%X' % (mechanism - CKM_VENDOR_DEFINED)\n                CKM[k] = mechanism\n                CKM[mechanism] = k\n            m.append(CKM[mechanism])\n        return m", "response": "C_GetMechanismList\n        : returns the list of available mechanisms for a given slot"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getMechanismInfo(self, slot, type):\n        info = PyKCS11.LowLevel.CK_MECHANISM_INFO()\n        rv = self.lib.C_GetMechanismInfo(slot, CKM[type], info)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n\n        i = CK_MECHANISM_INFO()\n        i.ulMinKeySize = info.ulMinKeySize\n        i.ulMaxKeySize = info.ulMaxKeySize\n        i.flags = info.flags\n\n        return i", "response": "C_GetMechanismInfo returns information about a mechanism."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update(self, data):\n        data1 = ckbytelist(data)\n        rv = self._lib.C_DigestUpdate(self._session, data1)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n        return self", "response": "Update the digest of the current state of the user s entry."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the object with the given handle.", "response": "def digestKey(self, handle):\n        \"\"\"\n        C_DigestKey\n\n        :param handle: key handle\n        :type handle: CK_OBJECT_HANDLE\n        \"\"\"\n        rv = self._lib.C_DigestKey(self._session, handle)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef final(self):\n        digest = ckbytelist()\n        # Get the size of the digest\n        rv = self._lib.C_DigestFinal(self._session, digest)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n        # Get the actual digest\n        rv = self._lib.C_DigestFinal(self._session, digest)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n        return digest", "response": "C_DigestFinal\n        - Returns the final digest of the current session."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlogs-in to the specified user.", "response": "def login(self, pin, user_type=CKU_USER):\n        \"\"\"\n        C_Login\n\n        :param pin: the user's PIN or None for CKF_PROTECTED_AUTHENTICATION_PATH\n        :type pin: string\n        :param user_type: the user type. The default value is\n          CKU_USER. You may also use CKU_SO\n        :type user_type: integer\n        \"\"\"\n        pin1 = ckbytelist(pin)\n        rv = self.lib.C_Login(self.session, user_type, pin1)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef logout(self):\n        rv = self.lib.C_Logout(self.session)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n\n        del self", "response": "Logout from the session."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setPin(self, old_pin, new_pin):\n        old_pin1 = ckbytelist(old_pin)\n        new_pin1 = ckbytelist(new_pin)\n        rv = self.lib.C_SetPIN(self.session, old_pin1, new_pin1)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)", "response": "Set the PIN of the current object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef createObject(self, template):\n        attrs = self._template2ckattrlist(template)\n        handle = PyKCS11.LowLevel.CK_OBJECT_HANDLE()\n        rv = self.lib.C_CreateObject(self.session, attrs, handle)\n        if rv != PyKCS11.CKR_OK:\n            raise PyKCS11.PyKCS11Error(rv)\n        return handle", "response": "C_CreateObject returns object handle"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef destroyObject(self, obj):\n        rv = self.lib.C_DestroyObject(self.session, obj)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)", "response": "C_DestroyObject - Destroy an object from the session."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a new DigestSession object for the given mecha.", "response": "def digestSession(self, mecha=MechanismSHA1):\n        \"\"\"\n        C_DigestInit/C_DigestUpdate/C_DigestKey/C_DigestFinal\n\n        :param mecha: the digesting mechanism to be used\n          (use `MechanismSHA1` for `CKM_SHA_1`)\n        :type mecha: :class:`Mechanism`\n        :return: A :class:`DigestSession` object\n        :rtype: DigestSession\n        \"\"\"\n        return DigestSession(self.lib, self.session, mecha)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the digest of the data.", "response": "def digest(self, data, mecha=MechanismSHA1):\n        \"\"\"\n        C_DigestInit/C_Digest\n\n        :param data: the data to be digested\n        :type data:  (binary) sring or list/tuple of bytes\n        :param mecha: the digesting mechanism to be used\n          (use `MechanismSHA1` for `CKM_SHA_1`)\n        :type mecha: :class:`Mechanism`\n        :return: the computed digest\n        :rtype: list of bytes\n\n        :note: the returned value is an istance of :class:`ckbytelist`.\n          You can easly convert it to a binary string with:\n          ``bytes(ckbytelistDigest)``\n          or, for Python 2:\n          ``''.join(chr(i) for i in ckbytelistDigest)``\n\n        \"\"\"\n        digest = ckbytelist()\n        m = mecha.to_native()\n        data1 = ckbytelist(data)\n        rv = self.lib.C_DigestInit(self.session, m)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n        # first call get digest size\n        rv = self.lib.C_Digest(self.session, data1, digest)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n        # second call get actual digest data\n        rv = self.lib.C_Digest(self.session, data1, digest)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n        return digest"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sign(self, key, data, mecha=MechanismRSAPKCS1):\n        m = mecha.to_native()\n        signature = ckbytelist()\n        data1 = ckbytelist(data)\n        rv = self.lib.C_SignInit(self.session, m, key)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n        # first call get signature size\n        rv = self.lib.C_Sign(self.session, data1, signature)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n        # second call get actual signature data\n        rv = self.lib.C_Sign(self.session, data1, signature)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n        return signature", "response": "Sign the data with the key."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nverifying the signature of a key.", "response": "def verify(self, key, data, signature, mecha=MechanismRSAPKCS1):\n        \"\"\"\n        C_VerifyInit/C_Verify\n\n        :param key: a key handle, obtained calling :func:`findObjects`.\n        :type key: integer\n        :param data: the data that was signed\n        :type data:  (binary) string or list/tuple of bytes\n        :param signature: the signature to be verified\n        :type signature:  (binary) string or list/tuple of bytes\n        :param mecha: the signing mechanism to be used\n          (use `MechanismRSAPKCS1` for `CKM_RSA_PKCS`)\n        :type mecha: :class:`Mechanism`\n        :return: True if signature is valid, False otherwise\n        :rtype: bool\n\n        \"\"\"\n        m = mecha.to_native()\n        data1 = ckbytelist(data)\n        rv = self.lib.C_VerifyInit(self.session, m, key)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n        rv = self.lib.C_Verify(self.session, data1, signature)\n        if rv == CKR_OK:\n            return True\n        elif rv == CKR_SIGNATURE_INVALID:\n            return False\n        else:\n            raise PyKCS11Error(rv)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef wrapKey(self, wrappingKey, key, mecha=MechanismRSAPKCS1):\n        wrapped = ckbytelist()\n        native = mecha.to_native()\n        # first call get wrapped size\n        rv = self.lib.C_WrapKey(self.session, native, wrappingKey, key,\n                                wrapped)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n        # second call get actual wrapped key data\n        rv = self.lib.C_WrapKey(self.session, native, wrappingKey, key,\n                                wrapped)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n        return wrapped", "response": "Wrap a key in a key handle."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef unwrapKey(self, unwrappingKey, wrappedKey, template,\n                  mecha=MechanismRSAPKCS1):\n        \"\"\"\n        C_UnwrapKey\n\n        :param unwrappingKey: the unwrapping key handle\n        :type unwrappingKey: integer\n        :param wrappedKey: the bytes of the wrapped key\n        :type wrappedKey:  (binary) string or list/tuple of bytes\n        :param template: template for the unwrapped key\n        :param mecha: the decrypt mechanism to be used (use\n          `MechanismRSAPKCS1` for `CKM_RSA_PKCS`)\n        :type mecha: :class:`Mechanism`\n        :return: the unwrapped key object\n        :rtype: integer\n\n        \"\"\"\n        m = mecha.to_native()\n        data1 = ckbytelist(wrappedKey)\n        handle = PyKCS11.LowLevel.CK_OBJECT_HANDLE()\n        attrs = self._template2ckattrlist(template)\n        rv = self.lib.C_UnwrapKey(self.session, m, unwrappingKey,\n                                  data1, attrs, handle)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n        return handle", "response": "Unwrap a key in a given template."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if the type of the certificate is a numerical value.", "response": "def isNum(self, type):\n        \"\"\"\n        is the type a numerical value?\n\n        :param type: PKCS#11 type like `CKA_CERTIFICATE_TYPE`\n        :rtype: bool\n        \"\"\"\n        if type in (CKA_CERTIFICATE_TYPE,\n                    CKA_CLASS,\n                    CKA_KEY_GEN_MECHANISM,\n                    CKA_KEY_TYPE,\n                    CKA_MODULUS_BITS,\n                    CKA_VALUE_BITS,\n                    CKA_VALUE_LEN):\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef isBool(self, type):\n        if type in (CKA_ALWAYS_SENSITIVE,\n                    CKA_DECRYPT,\n                    CKA_DERIVE,\n                    CKA_ENCRYPT,\n                    CKA_EXTRACTABLE,\n                    CKA_HAS_RESET,\n                    CKA_LOCAL,\n                    CKA_MODIFIABLE,\n                    CKA_NEVER_EXTRACTABLE,\n                    CKA_PRIVATE,\n                    CKA_RESET_ON_INIT,\n                    CKA_SECONDARY_AUTH,\n                    CKA_SENSITIVE,\n                    CKA_SIGN,\n                    CKA_SIGN_RECOVER,\n                    CKA_TOKEN,\n                    CKA_TRUSTED,\n                    CKA_UNWRAP,\n                    CKA_VERIFY,\n                    CKA_VERIFY_RECOVER,\n                    CKA_WRAP,\n                    CKA_WRAP_WITH_TRUSTED):\n            return True\n        return False", "response": "Returns a boolean value of the given type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nam the type a byte array value?", "response": "def isBin(self, type):\n        \"\"\"\n        is the type a byte array value?\n\n        :param type: PKCS#11 type like `CKA_MODULUS`\n        :rtype: bool\n        \"\"\"\n        return (not self.isBool(type)) \\\n            and (not self.isString(type)) \\\n            and (not self.isNum(type))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a secret key for the specified template.", "response": "def generateKey(self, template, mecha=MechanismAESGENERATEKEY):\n        \"\"\"\n        generate a secret key\n\n        :param template: template for the secret key\n        :param mecha: mechanism to use\n        :return: handle of the generated key\n        :rtype: PyKCS11.LowLevel.CK_OBJECT_HANDLE\n        \"\"\"\n        t = self._template2ckattrlist(template)\n        ck_handle = PyKCS11.LowLevel.CK_OBJECT_HANDLE()\n        m = mecha.to_native()\n        rv = self.lib.C_GenerateKey(self.session, m, t, ck_handle)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n        return ck_handle"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generateKeyPair(self, templatePub, templatePriv,\n                        mecha=MechanismRSAGENERATEKEYPAIR):\n        \"\"\"\n        generate a key pair\n\n        :param templatePub: template for the public key\n        :param templatePriv:  template for the private key\n        :param mecha: mechanism to use\n        :return: a tuple of handles (pub, priv)\n        :rtype: tuple\n        \"\"\"\n        tPub = self._template2ckattrlist(templatePub)\n        tPriv = self._template2ckattrlist(templatePriv)\n        ck_pub_handle = PyKCS11.LowLevel.CK_OBJECT_HANDLE()\n        ck_prv_handle = PyKCS11.LowLevel.CK_OBJECT_HANDLE()\n        m = mecha.to_native()\n        rv = self.lib.C_GenerateKeyPair(self.session, m, tPub, tPriv,\n                                        ck_pub_handle, ck_prv_handle)\n\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n        return ck_pub_handle, ck_prv_handle", "response": "Generate a key pair for the specified public and private keys."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef findObjects(self, template=()):\n        t = self._template2ckattrlist(template)\n\n        # we search for 10 objects by default. speed/memory tradeoff\n        result = PyKCS11.LowLevel.ckobjlist(10)\n\n        rv = self.lib.C_FindObjectsInit(self.session, t)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n\n        res = []\n        while True:\n            rv = self.lib.C_FindObjects(self.session, result)\n            if rv != CKR_OK:\n                raise PyKCS11Error(rv)\n            for x in result:\n                # make a copy of the handle: the original value get\n                # corrupted (!!)\n                a = CK_OBJECT_HANDLE(self)\n                a.assign(x.value())\n                res.append(a)\n            if len(result) == 0:\n                break\n\n        rv = self.lib.C_FindObjectsFinal(self.session)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n        return res", "response": "Find the objects matching the template pattern."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef seedRandom(self, seed):\n        low_seed = ckbytelist(seed)\n        rv = self.lib.C_SeedRandom(self.session, low_seed)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)", "response": "Seed the random set of keys."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generateRandom(self, size=16):\n        low_rand = ckbytelist([0] * size)\n        rv = self.lib.C_GenerateRandom(self.session, low_rand)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n        return low_rand", "response": "This function generates random bytes from the CKBYTLIST."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef qrcode(\n        cls,\n        data,\n        mode=\"base64\",\n        version=None,\n        error_correction=\"L\",\n        box_size=10,\n        border=0,\n        fit=True,\n        fill_color=\"black\",\n        back_color=\"white\",\n        **kwargs\n    ):\n        \"\"\"Makes qr image using qrcode as qrc. See documentation\n        for qrcode (https://pypi.python.org/pypi/qrcode) package for more info.\n\n        :param data: String data.\n        :param mode: Output mode, [base64|raw].\n        :param version: The size of the QR Code (1-40).\n        :param error_correction: The error correction used for the QR Code.\n        :param box_size: The number of pixels for each \"box\" of the QR code.\n        :param border: The number of box for border.\n        :param fit: If `True`, find the best fit for the data.\n        :param fill_color: Frontend color.\n        :param back_color: Background color.\n\n        :param icon_img: Small icon image name or url.\n        :param factor: Resize for icon image (default: 4, one-fourth of QRCode)\n        :param icon_box: Icon image position [left, top] (default: image center)\n        \"\"\"\n        qr = qrc.QRCode(\n            version=version,\n            error_correction=cls.correction_levels[error_correction],\n            box_size=box_size,\n            border=border,\n        )\n        qr.add_data(data)\n        qr.make(fit=fit)\n\n        fcolor = (\n            fill_color\n            if fill_color.lower() in cls.color or fill_color.startswith(\"#\")\n            else \"#\" + fill_color\n        )\n        bcolor = (\n            back_color\n            if back_color.lower() in cls.color or back_color.startswith(\"#\")\n            else \"#\" + back_color\n        )\n\n        # creates qrcode base64\n        out = BytesIO()\n        qr_img = qr.make_image(back_color=bcolor, fill_color=fcolor)\n        qr_img = qr_img.convert(\"RGBA\")\n        qr_img = cls._insert_img(qr_img, **kwargs)\n        qr_img.save(out, \"PNG\")\n        out.seek(0)\n\n        if mode == \"base64\":\n            return u\"data:image/png;base64,\" + base64.b64encode(out.getvalue()).decode(\n                \"ascii\"\n            )\n        elif mode == \"raw\":\n            return out", "response": "Creates a QR Code image for the current locale."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninsert a small icon to QR Code image", "response": "def _insert_img(qr_img, icon_img=None, factor=4, icon_box=None, static_dir=None):\n        \"\"\"Inserts a small icon to QR Code image\"\"\"\n        img_w, img_h = qr_img.size\n        size_w = int(img_w) / int(factor)\n        size_h = int(img_h) / int(factor)\n\n        try:\n            # load icon from current dir\n            icon_fp = os.path.join(icon_img)\n            if static_dir:\n                # load icon from app's static dir\n                icon_fp = os.path.join(static_dir, icon_img)\n            if icon_img.split(\"://\")[0] in [\"http\", \"https\", \"ftp\"]:\n                icon_fp = BytesIO(urlopen(icon_img).read())  # download icon\n            icon = Image.open(icon_fp)\n        except:\n            return qr_img\n\n        icon_w, icon_h = icon.size\n        icon_w = size_w if icon_w > size_w else icon_w\n        icon_h = size_h if icon_h > size_h else icon_h\n        icon = icon.resize((int(icon_w), int(icon_h)), Image.ANTIALIAS)\n        icon = icon.convert(\"RGBA\")\n\n        left = int((img_w - icon_w) / 2)\n        top = int((img_h - icon_h) / 2)\n        icon_box = (int(icon_box[0]), int(icon_box[1])) if icon_box else (left, top)\n        qr_img.paste(im=icon, box=icon_box, mask=icon)\n        return qr_img"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef panel(context, panel, build, bed, version):\n    LOG.info(\"Running scout export panel\")\n    adapter = context.obj['adapter']\n    # Save all chromosomes found in the collection if panels\n    chromosomes_found = set()\n    \n    if not panel:\n        LOG.warning(\"Please provide at least one gene panel\")\n        context.abort()\n\n    LOG.info(\"Exporting panels: {}\".format(', '.join(panel)))\n    if bed:\n        if version:\n            version = [version]\n        lines = export_panels(\n            adapter=adapter, \n            panels=panel, \n            versions=version, \n            build=build,\n        )\n    else:\n        lines = export_gene_panels(\n            adapter=adapter, \n            panels=panel, \n            version=version,\n            )\n    for line in lines:\n        click.echo(line)", "response": "Export gene panels to. bed like format."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _first_weekday(weekday, d):\n    while weekday != d.weekday():\n        d += timedelta(days=1)\n    return d", "response": "Given a weekday and a date returns the date that is the first weekday of that date."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _chunk_fill_out_first_week(year, month, count, event, diff):\n    first_of_the_month = date(year, month, 1)\n    d = _first_weekday(event.l_end_date.weekday(), first_of_the_month)\n    d2 = _first_weekday(event.l_start_date.weekday(), first_of_the_month)\n    diff_weekdays = d.day - d2.day\n    day = first_of_the_month.day\n    start = event.l_start_date.weekday()\n    first = first_of_the_month.weekday()\n\n    if start == first or diff_weekdays == diff:\n        return count\n    elif start > first:\n        end = event.l_end_date.weekday()\n        diff = end - first + 1\n    elif start < first:\n        diff = d.day\n    for i in xrange(diff):\n        if event.end_repeat is not None and \\\n                date(year, month, day) >= event.end_repeat:\n            break\n        count[day].append((event.title, event.pk))\n        day += 1\n    return count", "response": "Fills out the first week of the chunk."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncounting the days that are in the current month until we reach end_repeat.", "response": "def repeat(self, day=None):\n        \"\"\"\n        Add 'num' to the day and count that day until we reach end_repeat, or\n        until we're outside of the current month, counting the days\n        as we go along.\n        \"\"\"\n        if day is None:\n            day = self.day\n\n        try:\n            d = date(self.year, self.month, day)\n        except ValueError:  # out of range day\n            return self.count\n\n        if self.count_first and d <= self.end_repeat:\n            self.count_it(d.day)\n\n        d += timedelta(days=self.num)\n\n        if self.end_on is not None:\n            while d.month == self.month and \\\n                    d <= self.end_repeat and \\\n                    d.day <= self.end_on:\n                self.count_it(d.day)\n                d += timedelta(days=self.num)\n        else:\n            while d.month == self.month and d <= self.end_repeat:\n                self.count_it(d.day)\n                d += timedelta(days=self.num)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef repeat_weekdays(self):\n        try:\n            d = date(self.year, self.month, self.day)\n        except ValueError:  # out of range day\n            return self.count\n\n        if self.count_first and \\\n                d <= self.end_repeat and \\\n                d.weekday() not in (5, 6):\n            self.count_it(d.day)\n\n        d += timedelta(days=1)\n\n        while d.month == self.month and d <= self.end_repeat:\n            if d.weekday() not in (5, 6):\n                self.count_it(d.day)\n            d += timedelta(days=1)", "response": "This method is used to repeat every weekday."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef repeat_reverse(self, start, end):\n        day = start\n        diff = start - end\n        try:\n            if date(self.year, self.month, day) <= self.end_repeat:\n                self.count_it(day)\n        # a value error likely means the event runs past the end of the month,\n        # like an event that ends on the 31st, but the month doesn't have that\n        # many days. Ignore it b/c the dates won't be added to calendar anyway\n        except ValueError:\n            pass\n        for i in xrange(diff):\n            day -= 1\n            try:\n                if date(self.year, self.month, day) <= self.end_repeat:\n                    self.count_it(day)\n            except ValueError:\n                pass", "response": "Repeats the calendar for a given start and end date."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncounting the entries that are repeat every year.", "response": "def repeat_it(self):\n        \"\"\"\n        Events that repeat every year should be shown every year\n        on the same date they started e.g. an event that starts on March 23rd\n        would appear on March 23rd every year it is scheduled to repeat.\n        If the event is a chunk event, hand it over to _repeat_chunk().\n        \"\"\"\n        # The start day will be counted if we're in the start year,\n        # so only count the day if we're in the same month as\n        # l_start_date, but not in the same year.\n        if self.event.l_start_date.month == self.month and \\\n                self.event.l_start_date.year != self.year:\n            self.count_it(self.event.l_start_date.day)\n        # If we're in the same mo & yr as l_start_date,\n        # should already be filled in\n        if self.event.is_chunk() and not \\\n                self.event.starts_same_year_month_as(self.year, self.month):\n            self._repeat_chunk()\n        return self.count"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef repeat_it(self):\n        start_day = self.event.l_start_date.day\n        if not self.event.starts_same_month_as(self.month):\n            self.count_it(start_day)\n        elif self.event.starts_same_month_not_year_as(self.month, self.year):\n            self.count_it(start_day)\n\n        if self.event.is_chunk():\n            self._repeat_chunk()\n        return self.count", "response": "Count the events that repeat every month."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating to take some of the load off of _handle_weekly_repeat_out", "response": "def _biweekly_helper(self):\n        \"\"\"Created to take some of the load off of _handle_weekly_repeat_out\"\"\"\n        self.num = 14\n        mycount = self.repeat_biweekly()\n        if mycount:\n            if self.event.is_chunk() and min(mycount) not in xrange(1, 8):\n                mycount = _chunk_fill_out_first_week(\n                    self.year, self.month, mycount, self.event,\n                    diff=self.event.start_end_diff,\n                )\n            for k, v in mycount.items():\n                for item in v:\n                    self.count[k].append(item)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhandle repeating an event weekly or biweekly.", "response": "def _handle_weekly_repeat_out(self):\n        \"\"\"\n        Handles repeating an event weekly (or biweekly) if the current\n        year and month are outside of its start year and month.\n        It takes care of cases 3 and 4 in _handle_weekly_repeat_in() comments.\n        \"\"\"\n        start_d = _first_weekday(\n            self.event.l_start_date.weekday(), date(self.year, self.month, 1)\n        )\n        self.day = start_d.day\n        self.count_first = True\n        if self.event.repeats('BIWEEKLY'):\n            self._biweekly_helper()\n        elif self.event.repeats('WEEKLY'):\n            # Note count_first=True b/c although the start date isn't this\n            # month, the event does begin repeating this month and start_date\n            # has not yet been counted.\n            # Also note we start from start_d.day and not\n            # event.l_start_date.day\n            self.repeat()\n            if self.event.is_chunk():\n                diff = self.event.start_end_diff\n                self.count = _chunk_fill_out_first_week(\n                    self.year, self.month, self.count, self.event, diff\n                )\n                for i in xrange(diff):\n                    # count the chunk days, then repeat them\n                    self.day = start_d.day + i + 1\n                    self.repeat()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _handle_weekly_repeat_in(self):\n        self.day = self.event.l_start_date.day\n        self.count_first = False\n        repeats = {'WEEKLY': 7, 'BIWEEKLY': 14}\n        if self.event.starts_same_year_month_as(self.year, self.month):\n            # This takes care of 1 and 2 above.\n            # Note that 'count' isn't incremented before adding a week (in\n            # Repeater.repeat()), b/c it's assumed that l_start_date\n            # was already counted.\n            for repeat, num in repeats.items():\n                self.num = num\n                if self.event.repeats(repeat):\n                    self.repeat()\n                    if self.event.is_chunk():\n                        self.repeat_chunk(diff=self.event.start_end_diff)", "response": "Handles weekly and biweekly events."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload a manually curated panel into a scout object.", "response": "def load_panel(panel_path, adapter, date=None, display_name=None, version=None, panel_type=None, \n               panel_id=None, institute=None):\n    \"\"\"Load a manually curated gene panel into scout\n    \n    Args:\n        panel_path(str): path to gene panel file\n        adapter(scout.adapter.MongoAdapter)\n        date(str): date of gene panel on format 2017-12-24\n        display_name(str)\n        version(float)\n        panel_type(str)\n        panel_id(str)\n        institute(str)\n    \n    \"\"\"\n    panel_lines = get_file_handle(panel_path)\n\n    try:\n        # This will parse panel metadata if includeed in panel file\n        panel_info = get_panel_info(\n            panel_lines=panel_lines,\n            panel_id=panel_id,\n            institute=institute,\n            version=version,\n            date=date,\n            display_name=display_name\n            )\n    except Exception as err:\n        raise err\n\n    version = None\n    if panel_info.get('version'):\n        version = float(panel_info['version'])\n\n    panel_id = panel_info['panel_id']\n    display_name = panel_info['display_name'] or panel_id\n    institute = panel_info['institute']\n    date = panel_info['date']\n\n    if not institute:\n        raise SyntaxError(\"A Panel has to belong to a institute\")\n\n    #Check if institute exists in database\n    if not adapter.institute(institute):\n        raise SyntaxError(\"Institute {0} does not exist in database\".format(institute))\n\n    if not panel_id:\n        raise SyntaxError(\"A Panel has to have a panel id\")\n    \n    if version:\n        existing_panel = adapter.gene_panel(panel_id, version)\n    else:\n        ## Assuming version 1.0\n        existing_panel = adapter.gene_panel(panel_id)\n        version = 1.0\n        LOG.info(\"Set version to %s\", version)\n\n    if existing_panel:\n        LOG.info(\"found existing panel\")\n        if version == existing_panel['version']:\n            LOG.warning(\"Panel with same version exists in database\")\n            LOG.info(\"Reload with updated version\")\n            raise SyntaxError()\n        display_name = display_name or existing_panel['display_name']\n        institute = institute or existing_panel['institute']\n    \n    parsed_panel = parse_gene_panel(\n        path=panel_path,\n        institute=institute,\n        panel_type=panel_type,\n        date=date,\n        version=version,\n        panel_id=panel_id,\n        display_name=display_name,\n    )\n    \n    try:\n        adapter.load_panel(parsed_panel=parsed_panel)\n    except Exception as err:\n        raise err"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_panel_app(adapter, panel_id=None, institute='cust000'):\n    base_url = 'https://panelapp.genomicsengland.co.uk/WebServices/{0}/'\n    \n    hgnc_map = adapter.genes_by_alias()\n    \n    if panel_id:\n        panel_ids = [panel_id]\n\n    if not panel_id:\n        \n        LOG.info(\"Fetching all panel app panels\")\n        data = get_request(base_url.format('list_panels'))\n    \n        json_lines = json.loads(data)\n        \n        panel_ids = [panel_info['Panel_Id'] for panel_info in json_lines['result']]\n    \n    for panel_id in panel_ids:\n        panel_data = get_request(base_url.format('get_panel') + panel_id)\n        \n        parsed_panel = parse_panel_app_panel(\n            panel_info = json.loads(panel_data)['result'], \n            hgnc_map=hgnc_map,\n            institute=institute\n        )\n        parsed_panel['panel_id'] = panel_id\n        \n        if len(parsed_panel['genes']) == 0:\n            LOG.warning(\"Panel {} is missing genes. Skipping.\".format(parsed_panel['display_name']))\n            continue\n        \n        try:\n            adapter.load_panel(parsed_panel=parsed_panel)\n        except Exception as err:\n            raise err", "response": "Load PanelApp panels into scout database."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef export_variants(adapter, collaborator, document_id=None, case_id=None):\n\n    # Store the variants in a list for sorting\n    variants = []\n    if document_id:\n        yield adapter.variant(document_id)\n        return\n\n    variant_ids = adapter.get_causatives(\n        institute_id=collaborator,\n        case_id=case_id\n        )\n    ##TODO add check so that same variant is not included more than once\n    for document_id in variant_ids:\n\n        variant_obj = adapter.variant(document_id)\n        chrom = variant_obj['chromosome']\n        # Convert chromosome to integer for sorting\n        chrom_int = CHROMOSOME_INTEGERS.get(chrom)\n        if not chrom_int:\n            LOG.info(\"Unknown chromosome %s\", chrom)\n            continue\n\n        # Add chromosome and position to prepare for sorting\n        variants.append((chrom_int, variant_obj['position'], variant_obj))\n\n    # Sort varants based on position\n    variants.sort(key=lambda x: (x[0], x[1]))\n\n    for variant in variants:\n        variant_obj = variant[2]\n        yield variant_obj", "response": "Export causative variants for a particular collaborator."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef export_verified_variants(aggregate_variants, unique_callers):\n    document_lines = []\n    for variant in aggregate_variants:\n        # get genotype and allele depth for each sample\n        samples = []\n        for sample in variant['samples']:\n            line = [] # line elements corespond to contants.variants_export.VERIFIED_VARIANTS_HEADER\n            line.append(variant['institute'])\n            line.append(variant['_id']) # variant database ID\n            line.append(variant['category'])\n            line.append(variant['variant_type'])\n            line.append(variant['display_name'][:30]) # variant display name\n            # Build local link to variant:\n            case_name = variant['case_obj']['display_name']  # case display name\n            local_link = '/'.join([ '', variant['institute'], case_name, variant['_id'] ])\n            line.append(local_link)\n            line.append(variant.get('validation'))\n            line.append(case_name)\n            case_individual = next(ind for ind in variant['case_obj']['individuals'] if ind['individual_id'] == sample['sample_id'])\n            if case_individual['phenotype'] == 2:\n                line.append(' '.join([sample.get('display_name'),'(A)'])) # label sample as affected\n            else:\n                line.append(sample.get('display_name'))\n            line.append(''.join(['chr',variant['chromosome'],':',str(variant['position'])])) # position\n            line.append('>'.join([variant.get('reference')[:10],variant.get('alternative')[:10]])) # change\n            genes = []\n            prot_effect = []\n            funct_anno = []\n            for gene in variant.get('genes'): # this will be a unique long field in the document\n                genes.append(gene.get('hgnc_symbol',''))\n                funct_anno.append(gene.get('functional_annotation'))\n                for transcript in gene.get('transcripts'):\n                    if transcript.get('is_canonical') and transcript.get('protein_sequence_name'):\n                        prot_effect.append(urllib.parse.unquote(transcript.get('protein_sequence_name')))\n            line.append(','.join(prot_effect))\n            line.append(','.join(funct_anno))\n            line.append(','.join(genes))\n            line.append(variant.get('rank_score'))\n            line.append(variant.get('cadd_score'))\n            line.append(sample.get('genotype_call'))\n            line.append(sample['allele_depths'][0])\n            line.append(sample['allele_depths'][1])\n            line.append(sample['genotype_quality'])\n\n            # Set callers values. One cell per caller, leave blank if not applicable\n            for caller in unique_callers:\n                if variant.get(caller):\n                    line.append(variant.get(caller))\n                else:\n                    line.append('-')\n            document_lines.append(line)\n    return document_lines", "response": "Create the lines for an Excel file with verified variants for the given set of callers."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef export_mt_variants(variants, sample_id):\n    document_lines = []\n    for variant in variants:\n        line = []\n        position = variant.get('position')\n        change = '>'.join([variant.get('reference'),variant.get('alternative')])\n        line.append(position)\n        line.append(change)\n        line.append(str(position)+change)\n        genes = []\n        prot_effect = []\n        for gene in variant.get('genes'):\n            genes.append(gene.get('hgnc_symbol',''))\n            for transcript in gene.get('transcripts'):\n                if transcript.get('is_canonical') and transcript.get('protein_sequence_name'):\n                    prot_effect.append(urllib.parse.unquote(transcript.get('protein_sequence_name')))\n        line.append(','.join(prot_effect))\n        line.append(','.join(genes))\n        ref_ad = ''\n        alt_ad = ''\n        for sample in variant['samples']:\n            if sample.get('sample_id') == sample_id:\n                ref_ad = sample['allele_depths'][0]\n                alt_ad = sample['allele_depths'][1]\n        line.append(ref_ad)\n        line.append(alt_ad)\n        document_lines.append(line)\n    return document_lines", "response": "Export mitochondrial variants for a case to create a MT excel report"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate a user in the database.", "response": "def user(context, user_id, update_role, add_institute, remove_admin, remove_institute):\n    \"\"\"\n    Update a user in the database\n    \"\"\"\n    adapter = context.obj['adapter']\n\n    user_obj = adapter.user(user_id)\n\n    if not user_obj:\n        LOG.warning(\"User %s could not be found\", user_id)\n        context.abort()\n\n    existing_roles = set(user_obj.get('roles',[]))\n    if update_role:\n        if not update_role in user_obj['roles']:\n            existing_roles = set(user_obj['roles'])\n            existing_roles.add(update_role)\n            LOG.info(\"Adding role %s to user\", update_role)\n        else:\n            LOG.warning(\"User already have role %s\", update_role)\n\n    if remove_admin:\n        try:\n            existing_roles.remove('admin')\n            LOG.info(\"Removing admin rights from user %s\", user_id)\n        except KeyError as err:\n            LOG.info(\"User %s does not have admin rights\", user_id)\n\n    user_obj['roles'] = list(existing_roles)\n\n    existing_institutes = set(user_obj.get('institutes',[]))\n    for institute_id in add_institute:\n        institute_obj = adapter.institute(institute_id)\n        if not institute_obj:\n            LOG.warning(\"Institute %s could not be found\", institute_id)\n        else:\n            existing_institutes.add(institute_id)\n            LOG.info(\"Adding institute %s to user\", institute_id)\n\n    for institute_id in remove_institute:\n        try:\n            existing_institutes.remove(institute_id)\n            LOG.info(\"Removing institute %s from user\", institute_id)\n        except KeyError as err:\n            LOG.info(\"User does not have access to institute %s\", institute_id)\n\n    user_obj['institutes'] = list(existing_institutes)\n\n    updated_user = adapter.update_user(user_obj)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndisplay a list of SNV variants.", "response": "def variants(institute_id, case_name):\n    \"\"\"Display a list of SNV variants.\"\"\"\n    page = int(request.form.get('page', 1))\n\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    variant_type = request.args.get('variant_type', 'clinical')\n\n    # Update filter settings if Clinical Filter was requested\n\n    default_panels = []\n    for panel in case_obj['panels']:\n        if panel.get('is_default'):\n            default_panels.append(panel['panel_name'])\n\n    request.form.get('gene_panels')\n    if bool(request.form.get('clinical_filter')):\n        clinical_filter = MultiDict({\n            'variant_type': 'clinical',\n            'region_annotations': ['exonic','splicing'],\n            'functional_annotations': SEVERE_SO_TERMS,\n            'clinsig': [4,5],\n            'clinsig_confident_always_returned': True,\n            'gnomad_frequency': str(institute_obj['frequency_cutoff']),\n            'variant_type': 'clinical',\n            'gene_panels': default_panels\n             })\n\n    if(request.method == \"POST\"):\n        if bool(request.form.get('clinical_filter')):\n            form = FiltersForm(clinical_filter)\n            form.csrf_token = request.args.get('csrf_token')\n        else:\n            form = FiltersForm(request.form)\n    else:\n        form = FiltersForm(request.args)\n\n    # populate available panel choices\n    available_panels = case_obj.get('panels', []) + [\n        {'panel_name': 'hpo', 'display_name': 'HPO'}]\n\n    panel_choices = [(panel['panel_name'], panel['display_name'])\n                     for panel in available_panels]\n\n    form.gene_panels.choices = panel_choices\n\n    # upload gene panel if symbol file exists\n    if (request.files):\n        file = request.files[form.symbol_file.name]\n\n    if request.files and file and file.filename != '':\n        log.debug(\"Upload file request files: {0}\".format(request.files.to_dict()))\n        try:\n            stream = io.StringIO(file.stream.read().decode('utf-8'), newline=None)\n        except UnicodeDecodeError as error:\n            flash(\"Only text files are supported!\", 'warning')\n            return redirect(request.referrer)\n\n        hgnc_symbols_set = set(form.hgnc_symbols.data)\n        log.debug(\"Symbols prior to upload: {0}\".format(hgnc_symbols_set))\n        new_hgnc_symbols = controllers.upload_panel(store, institute_id, case_name, stream)\n        hgnc_symbols_set.update(new_hgnc_symbols)\n        form.hgnc_symbols.data = hgnc_symbols_set\n        # reset gene panels\n        form.gene_panels.data = ''\n\n    # update status of case if vistited for the first time\n    if case_obj['status'] == 'inactive' and not current_user.is_admin:\n        flash('You just activated this case!', 'info')\n        user_obj = store.user(current_user.email)\n        case_link = url_for('cases.case', institute_id=institute_obj['_id'],\n                            case_name=case_obj['display_name'])\n        store.update_status(institute_obj, case_obj, user_obj, 'active', case_link)\n\n    # check if supplied gene symbols exist\n    hgnc_symbols = []\n    non_clinical_symbols = []\n    not_found_symbols = []\n    not_found_ids = []\n    if (form.hgnc_symbols.data) and len(form.hgnc_symbols.data) > 0:\n        is_clinical = form.data.get('variant_type', 'clinical') == 'clinical'\n        clinical_symbols = store.clinical_symbols(case_obj) if is_clinical else None\n        for hgnc_symbol in form.hgnc_symbols.data:\n            if hgnc_symbol.isdigit():\n                hgnc_gene = store.hgnc_gene(int(hgnc_symbol))\n                if hgnc_gene is None:\n                    not_found_ids.append(hgnc_symbol)\n                else:\n                    hgnc_symbols.append(hgnc_gene['hgnc_symbol'])\n            elif store.hgnc_genes(hgnc_symbol).count() == 0:\n                  not_found_symbols.append(hgnc_symbol)\n            elif is_clinical and (hgnc_symbol not in clinical_symbols):\n                 non_clinical_symbols.append(hgnc_symbol)\n            else:\n                hgnc_symbols.append(hgnc_symbol)\n\n    if (not_found_ids):\n        flash(\"HGNC id not found: {}\".format(\", \".join(not_found_ids)), 'warning')\n    if (not_found_symbols):\n        flash(\"HGNC symbol not found: {}\".format(\", \".join(not_found_symbols)), 'warning')\n    if (non_clinical_symbols):\n        flash(\"Gene not included in clinical list: {}\".format(\", \".join(non_clinical_symbols)), 'warning')\n    form.hgnc_symbols.data = hgnc_symbols\n\n    # handle HPO gene list separately\n    if form.data['gene_panels'] == ['hpo']:\n        hpo_symbols = list(set(term_obj['hgnc_symbol'] for term_obj in\n                               case_obj['dynamic_gene_list']))\n        form.hgnc_symbols.data = hpo_symbols\n\n    variants_query = store.variants(case_obj['_id'], query=form.data)\n    data = {}\n\n    if request.form.get('export'):\n        document_header = controllers.variants_export_header(case_obj)\n        export_lines = []\n        if form.data['chrom'] == 'MT':\n            # Return all MT variants\n            export_lines = controllers.variant_export_lines(store, case_obj, variants_query)\n        else:\n            # Return max 500 variants\n            export_lines = controllers.variant_export_lines(store, case_obj, variants_query.limit(500))\n\n        def generate(header, lines):\n            yield header + '\\n'\n            for line in lines:\n                yield line + '\\n'\n\n        headers = Headers()\n        headers.add('Content-Disposition','attachment', filename=str(case_obj['display_name'])+'-filtered_variants.csv')\n\n        # return a csv with the exported variants\n        return Response(generate(\",\".join(document_header), export_lines), mimetype='text/csv',\n                        headers=headers)\n\n    data = controllers.variants(store, institute_obj, case_obj, variants_query, page)\n\n    return dict(institute=institute_obj, case=case_obj, form=form,\n                    severe_so_terms=SEVERE_SO_TERMS, page=page, **data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef variant(institute_id, case_name, variant_id):\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    log.debug(\"Variants view requesting data for variant {}\".format(variant_id))\n    data = controllers.variant(store, institute_obj, case_obj, variant_id=variant_id)\n    if data is None:\n        log.warning(\"An error occurred: variants view requesting data for variant {}\".format(variant_id))\n        flash('An error occurred while retrieving variant object', 'danger')\n        return redirect(request.referrer)\n\n    if current_app.config.get('LOQUSDB_SETTINGS'):\n        data['observations'] = controllers.observations(store, loqusdb,\n            case_obj, data['variant'])\n    data['cancer'] = request.args.get('cancer') == 'yes'\n    return dict(institute=institute_obj, case=case_obj, **data)", "response": "Display a specific SNV variant."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndisplaying a list of STR variants.", "response": "def str_variants(institute_id, case_name):\n    \"\"\"Display a list of STR variants.\"\"\"\n    page = int(request.args.get('page', 1))\n    variant_type = request.args.get('variant_type', 'clinical')\n\n    form = StrFiltersForm(request.args)\n\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n\n    query = form.data\n    query['variant_type'] = variant_type\n\n    variants_query = store.variants(case_obj['_id'], category='str',\n        query=query)\n    data = controllers.str_variants(store, institute_obj, case_obj,\n        variants_query, page)\n    return dict(institute=institute_obj, case=case_obj,\n        variant_type = variant_type, form=form, page=page, **data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sv_variants(institute_id, case_name):\n    page = int(request.form.get('page', 1))\n\n    variant_type = request.args.get('variant_type', 'clinical')\n\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n\n    form = SvFiltersForm(request.form)\n\n    default_panels = []\n    for panel in case_obj['panels']:\n        if panel['is_default']:\n            default_panels.append(panel['panel_name'])\n\n    request.form.get('gene_panels')\n    if bool(request.form.get('clinical_filter')):\n        clinical_filter = MultiDict({\n            'variant_type': 'clinical',\n            'region_annotations': ['exonic','splicing'],\n            'functional_annotations': SEVERE_SO_TERMS,\n            'thousand_genomes_frequency': str(institute_obj['frequency_cutoff']),\n            'variant_type': 'clinical',\n            'clingen_ngi': 10,\n            'swegen': 10,\n            'size': 100,\n            'gene_panels': default_panels\n             })\n\n    if(request.method == \"POST\"):\n        if bool(request.form.get('clinical_filter')):\n            form = SvFiltersForm(clinical_filter)\n            form.csrf_token = request.args.get('csrf_token')\n        else:\n            form = SvFiltersForm(request.form)\n    else:\n        form = SvFiltersForm(request.args)\n\n    available_panels = case_obj.get('panels', []) + [\n        {'panel_name': 'hpo', 'display_name': 'HPO'}]\n\n    panel_choices = [(panel['panel_name'], panel['display_name'])\n                     for panel in available_panels]\n    form.gene_panels.choices = panel_choices\n\n    if form.data['gene_panels'] == ['hpo']:\n        hpo_symbols = list(set(term_obj['hgnc_symbol'] for term_obj in\n                               case_obj['dynamic_gene_list']))\n        form.hgnc_symbols.data = hpo_symbols\n\n    # update status of case if vistited for the first time\n    if case_obj['status'] == 'inactive' and not current_user.is_admin:\n        flash('You just activated this case!', 'info')\n        user_obj = store.user(current_user.email)\n        case_link = url_for('cases.case', institute_id=institute_obj['_id'],\n                            case_name=case_obj['display_name'])\n        store.update_status(institute_obj, case_obj, user_obj, 'active', case_link)\n\n    variants_query = store.variants(case_obj['_id'], category='sv',\n                                    query=form.data)\n    data = {}\n    # if variants should be exported\n    if request.form.get('export'):\n        document_header = controllers.variants_export_header(case_obj)\n        export_lines = []\n        # Return max 500 variants\n        export_lines = controllers.variant_export_lines(store, case_obj, variants_query.limit(500))\n\n        def generate(header, lines):\n            yield header + '\\n'\n            for line in lines:\n                yield line + '\\n'\n\n        headers = Headers()\n        headers.add('Content-Disposition','attachment', filename=str(case_obj['display_name'])+'-filtered_sv-variants.csv')\n        return Response(generate(\",\".join(document_header), export_lines), mimetype='text/csv', headers=headers) # return a csv with the exported variants\n\n    else:\n        data = controllers.sv_variants(store, institute_obj, case_obj,\n                                       variants_query, page)\n\n    return dict(institute=institute_obj, case=case_obj, variant_type=variant_type,\n                form=form, severe_so_terms=SEVERE_SO_TERMS, page=page, **data)", "response": "Display a list of structural variants."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sv_variant(institute_id, case_name, variant_id):\n    data = controllers.sv_variant(store, institute_id, case_name, variant_id)\n    return data", "response": "Display a specific structural variant."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef str_variant(institute_id, case_name, variant_id):\n    data = controllers.str_variant(store, institute_id, case_name, variant_id)\n    return data", "response": "Display a specific STR variant."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate user - defined information about a variant.", "response": "def variant_update(institute_id, case_name, variant_id):\n    \"\"\"Update user-defined information about a variant: manual rank & ACMG.\"\"\"\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    variant_obj = store.variant(variant_id)\n    user_obj = store.user(current_user.email)\n    link = request.referrer\n\n    manual_rank = request.form.get('manual_rank')\n    if manual_rank:\n        new_manual_rank = int(manual_rank) if manual_rank != '-1' else None\n        store.update_manual_rank(institute_obj, case_obj, user_obj, link, variant_obj,\n                                 new_manual_rank)\n        if new_manual_rank:\n            flash(\"updated variant tag: {}\".format(new_manual_rank), 'info')\n        else:\n            flash(\"reset variant tag: {}\".format(variant_obj.get('manual_rank', 'NA')), 'info')\n    elif request.form.get('acmg_classification'):\n        new_acmg = request.form['acmg_classification']\n        acmg_classification = variant_obj.get('acmg_classification')\n        if isinstance(acmg_classification, int) and (new_acmg == ACMG_MAP[acmg_classification]):\n            new_acmg = None\n        store.update_acmg(institute_obj, case_obj, user_obj, link, variant_obj, new_acmg)\n        flash(\"updated ACMG classification: {}\".format(new_acmg), 'info')\n\n    new_dismiss = request.form.getlist('dismiss_variant')\n    if request.form.getlist('dismiss_variant'):\n        store.update_dismiss_variant(institute_obj, case_obj, user_obj, link, variant_obj,\n                                     new_dismiss)\n        if new_dismiss:\n            flash(\"Dismissed variant: {}\".format(new_dismiss), 'info')\n\n    if variant_obj.get('dismiss_variant') and not new_dismiss:\n        if 'dismiss' in request.form:\n            store.update_dismiss_variant(institute_obj, case_obj, user_obj, link, variant_obj,\n                                     new_dismiss)\n            flash(\"Reset variant dismissal: {}\".format(variant_obj.get('dismiss_variant')), 'info')\n        else:\n            log.debug(\"DO NOT reset variant dismissal: {}\".format(variant_obj.get('dismiss_variant')), 'info')\n\n    mosaic_tags = request.form.getlist('mosaic_tags')\n    if mosaic_tags:\n        store.update_mosaic_tags(institute_obj, case_obj, user_obj, link, variant_obj,\n                                     mosaic_tags)\n        if new_dismiss:\n            flash(\"Added mosaic tags: {}\".format(mosaic_tags), 'info')\n\n    if variant_obj.get('mosaic_tags') and not mosaic_tags:\n        if 'mosaic' in request.form:\n            store.update_mosaic_tags(institute_obj, case_obj, user_obj, link, variant_obj,\n                                     mosaic_tags)\n            flash(\"Reset mosaic tags: {}\".format(variant_obj.get('mosaic_tags')), 'info')\n\n    return redirect(request.referrer)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef verify(institute_id, case_name, variant_id, variant_category, order):\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    variant_obj = store.variant(variant_id)\n    user_obj = store.user(current_user.email)\n\n    comment = request.form.get('verification_comment')\n    \n    try:\n        controllers.variant_verification(store=store, mail=mail, institute_obj=institute_obj, case_obj=case_obj, user_obj=user_obj, comment=comment,\n                           variant_obj=variant_obj, sender=current_app.config['MAIL_USERNAME'], variant_url=request.referrer, order=order, url_builder=url_for)\n    except controllers.MissingVerificationRecipientError:\n        flash('No verification recipients added to institute.', 'danger')\n\n    return redirect(request.referrer)", "response": "Start procedure to validate a single variant using other techniques."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clinvar(institute_id, case_name, variant_id):\n    data = controllers.clinvar_export(store, institute_id, case_name, variant_id)\n    if request.method == 'GET':\n        return data\n    else: #POST\n        form_dict = request.form.to_dict()\n        submission_objects = set_submission_objects(form_dict) # A tuple of submission objects (variants and casedata objects)\n\n        # Add submission data to an open clinvar submission object,\n        # or create a new if no open submission is found in database\n        open_submission = store.get_open_clinvar_submission(current_user.email, institute_id)\n        updated_submission = store.add_to_submission(open_submission['_id'], submission_objects)\n\n        # Redirect to clinvar submissions handling page, and pass it the updated_submission_object\n        return redirect(url_for('cases.clinvar_submissions', institute_id=institute_id))", "response": "Build a clinVar submission form for a variant."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nshowing cancer variants overview.", "response": "def cancer_variants(institute_id, case_name):\n    \"\"\"Show cancer variants overview.\"\"\"\n    data = controllers.cancer_variants(store, request.args, institute_id, case_name)\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nshowing or delete an ACMG evaluation.", "response": "def evaluation(evaluation_id):\n    \"\"\"Show or delete an ACMG evaluation.\"\"\"\n    evaluation_obj = store.get_evaluation(evaluation_id)\n    controllers.evaluation(store, evaluation_obj)\n    if request.method == 'POST':\n        link = url_for('.variant', institute_id=evaluation_obj['institute']['_id'],\n                       case_name=evaluation_obj['case']['display_name'],\n                       variant_id=evaluation_obj['variant_specific'])\n        store.delete_evaluation(evaluation_obj)\n        return redirect(link)\n    return dict(evaluation=evaluation_obj, institute=evaluation_obj['institute'],\n                case=evaluation_obj['case'], variant=evaluation_obj['variant'],\n                CRITERIA=ACMG_CRITERIA)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates an ACMG classification from the submitted criteria.", "response": "def acmg():\n    \"\"\"Calculate an ACMG classification from submitted criteria.\"\"\"\n    criteria = request.args.getlist('criterion')\n    classification = get_acmg(criteria)\n    return jsonify(dict(classification=classification))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nuploading a single gene panel to the store.", "response": "def upload_panel(institute_id, case_name):\n    \"\"\"Parse gene panel file and fill in HGNC symbols for filter.\"\"\"\n    file = form.symbol_file.data\n\n    if file.filename == '':\n        flash('No selected file', 'warning')\n        return redirect(request.referrer)\n\n    try:\n        stream = io.StringIO(file.stream.read().decode('utf-8'), newline=None)\n    except UnicodeDecodeError as error:\n        flash(\"Only text files are supported!\", 'warning')\n        return redirect(request.referrer)\n\n    category = request.args.get('category')\n\n    if(category == 'sv'):\n        form = SvFiltersForm(request.args)\n    else:\n        form = FiltersForm(request.args)\n\n    hgnc_symbols = set(form.hgnc_symbols.data)\n    new_hgnc_symbols = controllers.upload_panel(store, institute_id, case_name, stream)\n    hgnc_symbols.update(new_hgnc_symbols)\n    form.hgnc_symbols.data = ','.join(hgnc_symbols)\n    # reset gene panels\n    form.gene_panels.data = ''\n    # HTTP redirect code 307 asks that the browser preserves the method of request (POST).\n    if(category == 'sv'):\n        return redirect(url_for('.sv_variants', institute_id=institute_id, case_name=case_name,\n                            **form.data), code=307)\n    else:\n        return redirect(url_for('.variants', institute_id=institute_id, case_name=case_name,\n                            **form.data), code=307)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndownloads all verified variants for user s cases", "response": "def download_verified():\n    \"\"\"Download all verified variants for user's cases\"\"\"\n    user_obj = store.user(current_user.email)\n    user_institutes = user_obj.get('institutes')\n    temp_excel_dir = os.path.join(variants_bp.static_folder, 'verified_folder')\n    os.makedirs(temp_excel_dir, exist_ok=True)\n\n    written_files = controllers.verified_excel_file(store, user_institutes, temp_excel_dir)\n    if written_files:\n        today = datetime.datetime.now().strftime('%Y-%m-%d')\n        # zip the files on the fly and serve the archive to the user\n        data = io.BytesIO()\n        with zipfile.ZipFile(data, mode='w') as z:\n            for f_name in pathlib.Path(temp_excel_dir).iterdir():\n                zipfile.ZipFile\n                z.write(f_name, os.path.basename(f_name))\n        data.seek(0)\n\n        # remove temp folder with excel files in it\n        shutil.rmtree(temp_excel_dir)\n\n        return send_file(\n            data,\n            mimetype='application/zip',\n            as_attachment=True,\n            attachment_filename='_'.join(['scout', 'verified_variants', today])+'.zip'\n        )\n    else:\n        flash(\"No verified variants could be exported for user's institutes\", 'warning')\n        return redirect(request.referrer)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef genes_by_alias(hgnc_genes):\n    alias_genes = {}\n    for hgnc_id in hgnc_genes:\n        gene = hgnc_genes[hgnc_id]\n        # This is the primary symbol:\n        hgnc_symbol = gene['hgnc_symbol']\n\n        for alias in gene['previous_symbols']:\n            true_id = None\n            if alias == hgnc_symbol:\n                true_id = hgnc_id\n            if alias in alias_genes:\n                alias_genes[alias.upper()]['ids'].add(hgnc_id)\n                if true_id:\n                    alias_genes[alias.upper()]['true_id'] = hgnc_id\n            else:\n                alias_genes[alias.upper()] = {\n                    'true': true_id,\n                    'ids': set([hgnc_id])\n                }\n\n    return alias_genes", "response": "Return a dictionary with the hgnc symbols as keys\n    Value of the dictionaries are information about the hgnc ids for a gene."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_ensembl_info(genes, ensembl_lines):\n    \n    LOG.info(\"Adding ensembl coordinates\")\n    # Parse and add the ensembl gene info\n    if isinstance(ensembl_lines, DataFrame):\n        ensembl_genes = parse_ensembl_gene_request(ensembl_lines)\n    else:\n        ensembl_genes = parse_ensembl_genes(ensembl_lines)\n\n    for ensembl_gene in ensembl_genes:\n        gene_obj = genes.get(ensembl_gene['hgnc_id'])\n        if not gene_obj:\n            continue\n        gene_obj['chromosome'] = ensembl_gene['chrom']\n        gene_obj['start'] = ensembl_gene['gene_start']\n        gene_obj['end'] = ensembl_gene['gene_end']\n        # ensembl ids can differ between builds. There is one stated in HGNC\n        # that is true for build 38. So we add information from ensembl\n        gene_obj['ensembl_gene_id'] = ensembl_gene['ensembl_gene_id']", "response": "Add the coordinates from the ensembl lines to the base ensembl record set."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd information from the exac lines to the gene level", "response": "def add_exac_info(genes, alias_genes, exac_lines):\n    \"\"\"Add information from the exac genes\n    \n    Currently we only add the pLi score on gene level\n    \n    The exac resource only use HGNC symbol to identify genes so we need\n    our alias mapping.\n    \n    Args:\n        genes(dict): Dictionary with all genes\n        alias_genes(dict): Genes mapped to all aliases\n        ensembl_lines(iteable): Iteable with raw ensembl info\n        \n    \"\"\"\n    LOG.info(\"Add exac pli scores\")\n    for exac_gene in parse_exac_genes(exac_lines):\n        hgnc_symbol = exac_gene['hgnc_symbol'].upper()\n        pli_score = exac_gene['pli_score']\n        \n        for hgnc_id in get_correct_ids(hgnc_symbol, alias_genes):\n            genes[hgnc_id]['pli_score'] = pli_score"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds omim info to the list of genes that are associated with a gene.", "response": "def add_omim_info(genes, alias_genes, genemap_lines, mim2gene_lines):\n    \"\"\"Add omim information\n    \n    We collect information on what phenotypes that are associated with a gene,\n    what inheritance models that are associated and the correct omim id.\n    \n    Args:\n        genes(dict): Dictionary with all genes\n        alias_genes(dict): Genes mapped to all aliases\n        genemap_lines(iterable): Iterable with raw omim info\n        mim2gene_lines(iterable): Iterable with raw omim info\n\n    \"\"\"\n    LOG.info(\"Add omim info\")\n    omim_genes = get_mim_genes(genemap_lines, mim2gene_lines)\n    \n    for hgnc_symbol in omim_genes:\n        omim_info = omim_genes[hgnc_symbol]\n        inheritance = omim_info.get('inheritance', set())\n        \n        for hgnc_id in get_correct_ids(hgnc_symbol, alias_genes):\n            gene_info = genes[hgnc_id]\n\n            # Update the omim id to the one found in omim\n            gene_info['omim_id'] = omim_info['mim_number']\n\n            gene_info['inheritance_models'] = list(inheritance)\n            gene_info['phenotypes'] = omim_info.get('phenotypes', [])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_incomplete_penetrance(genes, alias_genes, hpo_lines):\n    LOG.info(\"Add incomplete penetrance info\")\n    for hgnc_symbol in get_incomplete_penetrance_genes(hpo_lines):\n        for hgnc_id in get_correct_ids(hgnc_symbol, alias_genes):\n            genes[hgnc_id]['incomplete_penetrance'] = True", "response": "Add incomplete penetrance info to the given genes."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntry to get the correct gene based on a resource HGNC symbol.", "response": "def get_correct_ids(hgnc_symbol, alias_genes):\n    \"\"\"Try to get the correct gene based on hgnc_symbol\n    \n    The HGNC symbol is unfortunately not a persistent gene identifier.\n    Many of the resources that are used by Scout only provides the hgnc symbol to \n    identify a gene. We need a way to guess what gene is pointed at.\n    \n    Args:\n        hgnc_symbol(str): The symbol used by a resource\n        alias_genes(dict): A dictionary with all the alias symbols (including the current symbol)\n                           for all genes\n    \n    Returns:\n        hgnc_ids(iterable(int)): Hopefully only one but a symbol could map to several ids\n    \"\"\"\n    hgnc_ids = set()\n    hgnc_symbol = hgnc_symbol.upper()\n    if hgnc_symbol in alias_genes:\n        hgnc_id_info = alias_genes[hgnc_symbol]\n        if hgnc_id_info['true']:\n            return set([hgnc_id_info['true']])\n        else:\n            return set(hgnc_id_info['ids'])\n    return hgnc_ids"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive a list of lines of ensembl and exac lines and a dictionary of gene ids and a dictionary of gene ids and their corresponding genotypes return a dictionary of gene ids and their respective gene IDs.", "response": "def link_genes(ensembl_lines, hgnc_lines, exac_lines, mim2gene_lines,\n               genemap_lines, hpo_lines):\n    \"\"\"Gather information from different sources and return a gene dict\n\n    Extract information collected from a number of sources and combine them\n    into a gene dict with HGNC symbols as keys.\n\n    hgnc_id works as the primary symbol and it is from this source we gather\n    as much information as possible (hgnc_complete_set.txt)\n\n    Coordinates are gathered from ensemble and the entries are linked from hgnc\n    to ensembl via ENSGID.\n\n    From exac the gene intolerance scores are collected, genes are linked to hgnc\n    via hgnc symbol. This is a unstable symbol since they often change.\n\n\n        Args:\n            ensembl_lines(iterable(str)): Strings with ensembl gene information\n            hgnc_lines(iterable(str)): Strings with hgnc gene information\n            exac_lines(iterable(str)): Strings with exac PLi score info\n            mim2gene_lines(iterable(str))\n            genemap_lines(iterable(str))\n            hpo_lines(iterable(str)): Strings with hpo gene information\n\n        Yields:\n            gene(dict): A dictionary with gene information\n    \"\"\"\n    genes = {}\n    LOG.info(\"Linking genes\")\n    # HGNC genes are the main source, these define the gene dataset to use\n    # Try to use as much information as possible from hgnc\n    for hgnc_gene in parse_hgnc_genes(hgnc_lines):\n        hgnc_id = hgnc_gene['hgnc_id']\n        genes[hgnc_id] = hgnc_gene\n\n    add_ensembl_info(genes, ensembl_lines)\n\n    symbol_to_id = genes_by_alias(genes)\n\n    add_exac_info(genes, symbol_to_id, exac_lines)\n\n    add_omim_info(genes, symbol_to_id, genemap_lines, mim2gene_lines)\n\n    add_incomplete_penetrance(genes, symbol_to_id, hpo_lines)\n\n    return genes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef matchmaker_request(url, token, method, content_type=None, accept=None, data=None):\n    headers = Headers()\n    headers = { 'X-Auth-Token': token}\n    if content_type:\n        headers['Content-Type'] = content_type\n    if accept:\n        headers['Accept'] = accept\n\n    #sending data anyway so response will not be cached\n    req_data = data or {'timestamp' : datetime.datetime.now().timestamp()}\n    json_response = None\n    try:\n        LOG.info('Sending {} request to MME url {}. Data sent: {}'.format(\n            method, url, req_data))\n        resp = requests.request(\n            method = method,\n            url = url,\n            headers = headers,\n            data = json.dumps(req_data)\n        )\n        json_response = resp.json()\n        LOG.info('MME server response was:{}'.format(json_response))\n\n        if isinstance(json_response, str):\n            json_response = {\n                'message' : json_response,\n            }\n        elif isinstance(json_response, list): #asking for connected nodes\n            return json_response\n        json_response['status_code'] = resp.status_code\n    except Exception as err:\n        LOG.info('An error occurred while sending HTTP request to server ({})'.format(err))\n        json_response = {\n            'message' : str(err)\n        }\n    return json_response", "response": "Send a request to MatchMaker and return the response"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the available nodes for the given MME service.", "response": "def mme_nodes(mme_base_url, token):\n    \"\"\"Return the available MatchMaker nodes\n\n    Args:\n        mme_base_url(str): base URL of MME service\n        token(str): MME server authorization token\n\n    Returns:\n        nodes(list): a list of node disctionaries\n    \"\"\"\n    nodes = []\n    if not mme_base_url or not token:\n        return nodes\n    url = ''.join([mme_base_url, '/nodes'])\n    nodes = matchmaker_request(url=url, token=token, method='GET')\n    LOG.info('Matchmaker has the following connected nodes:{}'.format(nodes))\n    return nodes"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the cytoband coordinate for a position in a given set of chromosomes.", "response": "def get_cytoband_coordinates(chrom, pos):\n    \"\"\"Get the cytoband coordinate for a position\n\n    Args:\n        chrom(str)\n        pos(int)\n\n    Returns:\n        coordinate(str)\n    \"\"\"\n    coordinate = \"\"\n\n    if chrom in CYTOBANDS:\n        for interval in CYTOBANDS[chrom][pos]:\n            coordinate = interval.data\n\n    return coordinate"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the subcategory for a VCF variant", "response": "def get_sub_category(alt_len, ref_len, category, svtype=None):\n    \"\"\"Get the subcategory for a VCF variant\n\n    The sub categories are:\n        'snv', 'indel', 'del', 'ins', 'dup', 'bnd', 'inv'\n\n    Args:\n        alt_len(int)\n        ref_len(int)\n        category(str)\n        svtype(str)\n\n    Returns:\n        subcategory(str)\n    \"\"\"\n    subcategory = ''\n\n    if category in ('snv', 'indel', 'cancer'):\n        if ref_len == alt_len:\n            subcategory = 'snv'\n        else:\n            subcategory = 'indel'\n    elif category == 'sv':\n        subcategory = svtype\n\n    return subcategory"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the length of a variant in the sequence alt_len ref_len category pos end svtype svlen end Returns - 1 if there is no length", "response": "def get_length(alt_len, ref_len, category, pos, end, svtype=None, svlen=None):\n    \"\"\"Return the length of a variant\n\n    Args:\n        alt_len(int)\n        ref_len(int)\n        category(str)\n        svtype(str)\n        svlen(int)\n    \"\"\"\n    # -1 would indicate uncertain length\n    length = -1\n    if category in ('snv', 'indel', 'cancer'):\n        if ref_len == alt_len:\n            length = alt_len\n        else:\n            length = abs(ref_len - alt_len)\n\n    elif category == 'sv':\n        if svtype == 'bnd':\n            length = int(10e10)\n        else:\n            if svlen:\n                length = abs(int(svlen))\n            # Some software does not give a length but they give END\n            elif end:\n                if end != pos:\n                    length = end - pos\n    return length"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_end(pos, alt, category, snvend=None, svend=None, svlen=None):\n    # If nothing is known we set end to be same as start\n    end = pos\n    # If variant is snv or indel we know that cyvcf2 can handle end pos\n    if category in ('snv', 'indel', 'cancer'):\n        end = snvend\n\n    # With SVs we have to be a bit more careful\n    elif category == 'sv':\n        # The END field from INFO usually works fine\n        end = svend\n\n        # For some cases like insertions the callers set end to same as pos\n        # In those cases we can hope that there is a svlen...\n        if svend == pos:\n            if svlen:\n                end = pos + svlen\n        # If variant is 'BND' they have ':' in alt field\n        # Information about other end is in the alt field\n        if ':' in alt:\n            match = BND_ALT_PATTERN.match(alt)\n            if match:\n                end = int(match.group(2))\n\n    return end", "response": "Return the end coordinate for a variant in a sequence."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_coordinates(variant, category):\n    ref = variant.REF\n\n    if variant.ALT:\n        alt = variant.ALT[0]\n    if category==\"str\" and not variant.ALT:\n        alt = '.'\n\n    chrom_match = CHR_PATTERN.match(variant.CHROM)\n    chrom = chrom_match.group(2)\n\n    svtype = variant.INFO.get('SVTYPE')\n    if svtype:\n        svtype = svtype.lower()\n\n    mate_id = variant.INFO.get('MATEID')\n\n    svlen = variant.INFO.get('SVLEN')\n\n    svend = variant.INFO.get('END')\n    snvend = int(variant.end)\n\n    position = int(variant.POS)\n\n    ref_len = len(ref)\n    alt_len = len(alt)\n\n    sub_category = get_sub_category(alt_len, ref_len, category, svtype)\n    end = get_end(position, alt, category, snvend, svend)\n\n    length = get_length(alt_len, ref_len, category, position, end, svtype, svlen)\n    end_chrom = chrom\n\n    if sub_category == 'bnd':\n        if ':' in alt:\n            match = BND_ALT_PATTERN.match(alt)\n            # BND will often be translocations between different chromosomes\n            if match:\n                other_chrom = match.group(1)\n                match = CHR_PATTERN.match(other_chrom)\n                end_chrom = match.group(2)\n\n    cytoband_start = get_cytoband_coordinates(chrom, position)\n    cytoband_end = get_cytoband_coordinates(end_chrom, end)\n\n    coordinates = {\n        'position': position,\n        'end': end,\n        'length': length,\n        'sub_category': sub_category,\n        'mate_id': mate_id,\n        'cytoband_start': cytoband_start,\n        'cytoband_end': cytoband_end,\n        'end_chrom': end_chrom,\n    }\n\n\n    return coordinates", "response": "Parse the coordinates of a single variant into a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse iterable with cytoband coordinates and return a dictionary with chromosome names as keys and interval trees as values", "response": "def parse_cytoband(lines):\n    \"\"\"Parse iterable with cytoband coordinates\n    \n    \n    Args:\n        lines(iterable): Strings on format \"chr1\\t2300000\\t5400000\\tp36.32\\tgpos25\"\n    \n    Returns:\n        cytobands(dict): Dictionary with chromosome names as keys and \n                         interval trees as values\n    \"\"\"\n    cytobands = {}\n    for line in lines:\n        line = line.rstrip()\n        splitted_line = line.split('\\t')\n        chrom = splitted_line[0].lstrip('chr')\n        start = int(splitted_line[1])\n        stop = int(splitted_line[2])\n        name = splitted_line[3]\n        if chrom in cytobands:\n            # Add interval to existing tree\n            cytobands[chrom][start:stop] = name\n        else:\n            # Create a new interval tree\n            new_tree = intervaltree.IntervalTree()\n            # create the interval\n            new_tree[start:stop] = name\n            # Add the interval tree \n            cytobands[chrom] = new_tree\n    \n    return cytobands"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_panel(adapter, panel_name, panel_version, new_version=None, new_date=None):\n    panel_obj = adapter.gene_panel(panel_name, panel_version)\n\n    if not panel_obj:\n        raise IntegrityError(\"Panel %s version %s does not exist\" % (panel_name, panel_version))\n\n    updated_panel = adapter.update_panel(panel_obj, new_version, new_date)\n    \n    panel_id = updated_panel['_id']\n\n    # We need to alter the embedded panels in all affected cases\n    update = {'$set': {}}\n    if new_version:\n        update['$set']['panels.$.version'] = updated_panel['version']\n    if new_date:\n        update['$set']['panels.$.updated_at'] = updated_panel['date']\n    \n    LOG.info('Updating affected cases with {0}'.format(update))\n    \n    query = {'panels': { '$elemMatch': {'panel_name': panel_name}}}\n    adapter.case_collection.update_many(query, update)\n    \n    return updated_panel", "response": "Update a single panel in the database."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cli(context, mongodb, username, password, authdb, host, port, loglevel, config, demo):\n    # log_format = \"%(message)s\" if sys.stdout.isatty() else None\n    log_format = None\n    coloredlogs.install(level=loglevel, fmt=log_format)\n    LOG.info(\"Running scout version %s\", __version__)\n    LOG.debug(\"Debug logging enabled.\")\n\n    mongo_config = {}\n    cli_config = {}\n    if config:\n        LOG.debug(\"Use config file %s\", config)\n        with open(config, 'r') as in_handle:\n            cli_config = yaml.load(in_handle)\n\n    mongo_config['mongodb'] = (mongodb or cli_config.get('mongodb') or 'scout')\n    if demo:\n        mongo_config['mongodb'] = 'scout-demo'\n\n    mongo_config['host'] = (host or cli_config.get('host') or 'localhost')\n    mongo_config['port'] = (port or cli_config.get('port') or 27017)\n    mongo_config['username'] = username or cli_config.get('username')\n    mongo_config['password'] = password or cli_config.get('password')\n    mongo_config['authdb'] = authdb or cli_config.get('authdb') or mongo_config['mongodb']\n    mongo_config['omim_api_key'] = cli_config.get('omim_api_key')\n\n    if context.invoked_subcommand in ('setup', 'serve'):\n        mongo_config['adapter'] = None\n    else:\n        LOG.info(\"Setting database name to %s\", mongo_config['mongodb'])\n        LOG.debug(\"Setting host to %s\", mongo_config['host'])\n        LOG.debug(\"Setting port to %s\", mongo_config['port'])\n\n        try:\n            client = get_connection(**mongo_config)\n        except ConnectionFailure:\n            context.abort()\n\n        database = client[mongo_config['mongodb']]\n\n        LOG.info(\"Setting up a mongo adapter\")\n        mongo_config['client'] = client\n        adapter = MongoAdapter(database)\n        mongo_config['adapter'] = adapter\n        \n        LOG.info(\"Check if authenticated...\")\n        try:\n            for ins_obj in adapter.institutes():\n                pass\n        except OperationFailure as err:\n            LOG.info(\"User not authenticated\")\n            context.abort()\n        \n\n    context.obj = mongo_config", "response": "Manage interactions with a scout instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing an exac formated line into a dict with the relevant info", "response": "def parse_exac_line(line, header):\n    \"\"\"Parse an exac formated line\n    \n        Args:\n            line(list): A list with exac gene info\n            header(list): A list with the header info\n        \n        Returns:\n            exac_info(dict): A dictionary with the relevant info\n    \"\"\"\n    exac_gene = {}\n    splitted_line = line.rstrip().split('\\t')\n    exac_gene = dict(zip(header, splitted_line))\n    exac_gene['hgnc_symbol'] = exac_gene['gene']\n    exac_gene['pli_score'] = float(exac_gene['pLI'])\n    exac_gene['raw'] = line\n    \n    return exac_gene"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses lines with exac formated genes Shipment", "response": "def parse_exac_genes(lines):\n    \"\"\"Parse lines with exac formated genes\n        \n        This is designed to take a dump with genes from exac.\n        This is downloaded from: \n            ftp.broadinstitute.org/pub/ExAC_release//release0.3/functional_gene_constraint/\n            fordist_cleaned_exac_r03_march16_z_pli_rec_null_data.txt\n    \n        Args:\n            lines(iterable(str)): An iterable with ExAC formated genes\n        Yields:\n            exac_gene(dict): A dictionary with the relevant information\n    \"\"\"\n    header = []\n    logger.info(\"Parsing exac genes...\")\n    for index, line in enumerate(lines):\n        if index == 0:\n            header = line.rstrip().split('\\t')\n        elif len(line) > 0:\n            exac_gene = parse_exac_line(line, header)\n            \n            yield exac_gene"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef panels():\n    if request.method == 'POST':\n        # update an existing panel\n        csv_file = request.files['csv_file']\n        content = csv_file.stream.read()\n        lines = None\n        try:\n            if b'\\n' in content:\n                lines = content.decode('utf-8', 'ignore').split('\\n')\n            else:\n                lines = content.decode('windows-1252').split('\\r')\n        except Exception as err:\n            flash('Something went wrong while parsing the panel CSV file! ({})'.format(err), 'danger')\n            return redirect(request.referrer)\n\n        new_panel_name = request.form.get('new_panel_name')\n        if new_panel_name: #create a new panel\n            new_panel_id = controllers.new_panel(\n                store=store,\n                institute_id=request.form['institute'],\n                panel_name=new_panel_name,\n                display_name=request.form['display_name'],\n                csv_lines=lines,\n            )\n            if new_panel_id is None:\n                flash('Something went wrong and the panel list was not updated!','warning')\n                return redirect(request.referrer)\n            else:\n                flash(\"new gene panel added, {}!\".format(new_panel_name),'success')\n            return redirect(url_for('panels.panel', panel_id=new_panel_id))\n\n        else: # modify an existing panel\n            update_option = request.form['modify_option']\n            panel_obj= controllers.update_panel(\n                                   store=store,\n                                   panel_name=request.form['panel_name'],\n                                   csv_lines=lines,\n                                   option=update_option\n             )\n            if panel_obj is None:\n                return abort(404, \"gene panel not found: {}\".format(request.form['panel_name']))\n            else:\n                return redirect(url_for('panels.panel', panel_id=panel_obj['_id']))\n\n    institutes = list(user_institutes(store, current_user))\n    panel_names = [name\n                   for institute in institutes\n                   for name in\n                   store.gene_panels(institute_id=institute['_id']).distinct('panel_name')]\n\n    panel_versions = {}\n    for name in panel_names:\n        panel_versions[name]=store.gene_panels(panel_id=name)\n\n    panel_groups = []\n    for institute_obj in institutes:\n         institute_panels = store.latest_panels(institute_obj['_id'])\n         panel_groups.append((institute_obj, institute_panels))\n\n    return dict(panel_groups=panel_groups, panel_names=panel_names,\n                panel_versions=panel_versions, institutes=institutes)", "response": "Show all panels for a case."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef panel(panel_id):\n    panel_obj = store.gene_panel(panel_id) or store.panel(panel_id)\n    if request.method == 'POST':\n        raw_hgnc_id = request.form['hgnc_id']\n        if '|' in raw_hgnc_id:\n            raw_hgnc_id = raw_hgnc_id.split(' | ', 1)[0]\n        hgnc_id = 0\n        try:\n            hgnc_id = int(raw_hgnc_id)\n        except:\n            flash(\"Provided HGNC is not valid : '{}'\". format(raw_hgnc_id), 'danger')\n            return redirect(request.referrer)\n        action = request.form['action']\n        gene_obj = store.hgnc_gene(hgnc_id)\n        if gene_obj is None:\n            flash(\"HGNC id not found: {}\".format(hgnc_id), 'warning')\n            return redirect(request.referrer)\n\n        if action == 'add':\n            panel_gene = controllers.existing_gene(store, panel_obj, hgnc_id)\n            if panel_gene:\n                flash(\"gene already in panel: {}\".format(panel_gene['symbol']),\n                      'warning')\n            else:\n                # ask user to fill-in more information about the gene\n                return redirect(url_for('.gene_edit', panel_id=panel_id,\n                                        hgnc_id=hgnc_id))\n        elif action == 'delete':\n            log.debug(\"marking gene to be deleted: %s\", hgnc_id)\n            panel_obj = store.add_pending(panel_obj, gene_obj, action='delete')\n\n    data = controllers.panel(store, panel_obj)\n    if request.args.get('case_id'):\n        data['case'] = store.case(request.args['case_id'])\n    if request.args.get('institute_id'):\n        data['institute'] = store.institute(request.args['institute_id'])\n    return data", "response": "Display and add pending updates to a specific gene panel."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating a panel to a new version.", "response": "def panel_update(panel_id):\n    \"\"\"Update panel to a new version.\"\"\"\n    panel_obj = store.panel(panel_id)\n    update_version = request.form.get('version', None)\n    new_panel_id = store.apply_pending(panel_obj, update_version)\n    return redirect(url_for('panels.panel', panel_id=new_panel_id))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexport panel to PDF file", "response": "def panel_export(panel_id):\n    \"\"\"Export panel to PDF file\"\"\"\n    panel_obj = store.panel(panel_id)\n    data = controllers.panel_export(store, panel_obj)\n    data['report_created_at'] = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n    html_report = render_template('panels/panel_pdf_simple.html', **data)\n    return render_pdf(HTML(string=html_report), download_filename=data['panel']['panel_name']+'_'+str(data['panel']['version'])+'_'+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'_scout.pdf')"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nedits additional information about a panel gene.", "response": "def gene_edit(panel_id, hgnc_id):\n    \"\"\"Edit additional information about a panel gene.\"\"\"\n    panel_obj = store.panel(panel_id)\n    hgnc_gene = store.hgnc_gene(hgnc_id)\n    panel_gene = controllers.existing_gene(store, panel_obj, hgnc_id)\n\n    form = PanelGeneForm()\n    transcript_choices = []\n    for transcript in hgnc_gene['transcripts']:\n        if transcript.get('refseq_id'):\n            refseq_id = transcript.get('refseq_id')\n            transcript_choices.append((refseq_id, refseq_id))\n    form.disease_associated_transcripts.choices = transcript_choices\n\n    if form.validate_on_submit():\n        action = 'edit' if panel_gene else 'add'\n        info_data = form.data.copy()\n        if 'csrf_token' in info_data:\n            del info_data['csrf_token']\n        store.add_pending(panel_obj, hgnc_gene, action=action, info=info_data)\n        return redirect(url_for('.panel', panel_id=panel_id))\n\n    if panel_gene:\n        for field_key in ['disease_associated_transcripts', 'reduced_penetrance',\n                          'mosaicism', 'inheritance_models', 'database_entry_version', 'comment']:\n            form_field = getattr(form, field_key)\n            if not form_field.data:\n                panel_value = panel_gene.get(field_key)\n                if panel_value is not None:\n                    form_field.process_data(panel_value)\n    return dict(panel=panel_obj, form=form, gene=hgnc_gene, panel_gene=panel_gene)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds delivery report to an existing case.", "response": "def delivery_report(context, case_id, report_path,\n                    update):\n    \"\"\"Add delivery report to an existing case.\"\"\"\n\n    adapter = context.obj['adapter']\n\n    try:\n        load_delivery_report(adapter=adapter, case_id=case_id,\n                             report_path=report_path, update=update)\n        LOG.info(\"saved report to case!\")\n    except Exception as e:\n        LOG.error(e)\n        context.abort()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a peddy. ped file containing a list of dicts.", "response": "def parse_peddy_ped(lines):\n    \"\"\"Parse a peddy.ped file\n    \n    Args:\n        lines(iterable(str))\n    \n    Returns:\n        peddy_ped(list(dict))\n    \"\"\"\n    peddy_ped = []\n    header = []\n    for i,line in enumerate(lines):\n        line = line.rstrip()\n        if i == 0:\n            # Header line\n            header = line.lstrip('#').split('\\t')\n        else:\n            ind_info = dict(zip(header, line.split('\\t')))\n            \n            # PC1/PC2/PC3/PC4: the first 4 values after this sample was \n            # projected onto the thousand genomes principle components.\n            ind_info['PC1'] = convert_number(ind_info['PC1'])\n            ind_info['PC2'] = convert_number(ind_info['PC2'])\n            ind_info['PC3'] = convert_number(ind_info['PC3'])\n            # ancestry-prediction one of AFR AMR EAS EUR SAS UNKNOWN\n\n            ind_info['het_call_rate'] = convert_number(ind_info['het_call_rate'])\n\n            # idr_baf: inter-decile range (90th percentile - 10th percentile) \n            # of b-allele frequency. We make a distribution of all sites of \n            # alts / (ref + alts) and then report the difference between the\n            # 90th and the 10th percentile. \n            # Large values indicated likely sample contamination.\n            ind_info['het_idr_baf'] = convert_number(ind_info['het_idr_baf'])\n\n            ind_info['het_mean_depth'] = convert_number(ind_info['het_mean_depth'])\n            \n            peddy_ped.append(ind_info)\n    return peddy_ped"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_peddy_ped_check(lines):\n    ped_check = []\n    header = []\n    for i,line in enumerate(lines):\n        line = line.rstrip()\n        if i == 0:\n            # Header line\n            header = line.lstrip('#').split(',')\n        else:\n            pair_info = dict(zip(header, line.split(',')))\n            \n            # the number of sites at which sample_a was heterozygous\n            pair_info['hets_a'] = convert_number(pair_info['hets_a'])\n            \n            # the number of sites at which sample_b was heterozygous\n            pair_info['hets_b'] = convert_number(pair_info['hets_b'])\n            \n            # the number of sites at which the 2 samples shared no alleles \n            # (should approach 0 for parent-child pairs).\n            pair_info['ibs0'] = convert_number(pair_info['ibs0'])\n            \n            # the number of sites and which the 2 samples where both \n            # hom-ref, both het, or both hom-alt.\n            pair_info['ibs2'] = convert_number(pair_info['ibs2'])\n\n            # the number of sites that was used to predict the relatedness.\n            pair_info['n'] = convert_number(pair_info['n'])\n            \n            # the relatedness reported in the ped file.\n            pair_info['rel'] = convert_number(pair_info['rel'])\n\n            # the relatedness reported in the ped file.\n            pair_info['pedigree_relatedness'] = convert_number(pair_info['pedigree_relatedness'])\n            \n            # difference between the preceding 2 colummns.\n            pair_info['rel_difference'] = convert_number(pair_info['rel_difference'])\n\n            # the number of sites at which both samples were hets.\n            pair_info['shared_hets'] = convert_number(pair_info['shared_hets'])\n\n            # boolean indicating that this pair is a parent-child pair \n            # according to the ped file.\n            pair_info['pedigree_parents'] = make_bool(pair_info.get('pedigree_parents'))\n            \n            # boolean indicating that this pair is expected to be a parent-child\n            # pair according to the ibs0 (< 0.012) calculated from the genotypes.\n            pair_info['predicted_parents'] = make_bool(pair_info.get('predicted_parents'))\n\n            # boolean indicating that the preceding 2 columns do not match\n            pair_info['parent_error'] = make_bool(pair_info.get('parent_error'))\n\n            #  boolean indicating that rel > 0.75 and ibs0 < 0.012\n            pair_info['sample_duplication_error'] = make_bool(pair_info.get('sample_duplication_error'))\n            \n            \n            ped_check.append(pair_info)\n\n    return ped_check", "response": "Parse a. ped_check. csv file and return a list of dicts."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_peddy_sex_check(lines):\n    sex_check = []\n    header = []\n    for i,line in enumerate(lines):\n        line = line.rstrip()\n        if i == 0:\n            # Header line\n            header = line.lstrip('#').split(',')\n        else:\n            ind_info = dict(zip(header, line.split(',')))\n\n            # boolean indicating wether there is a mismatch between X \n            # genotypes and ped sex.\n            ind_info['error'] = make_bool(ind_info.get('error'))\n\n            # number of homozygous-alternate calls\n            ind_info['hom_alt_count'] = convert_number(ind_info['hom_alt_count'])\n            #number of homozygous-reference calls\n            ind_info['hom_ref_count'] = convert_number(ind_info['hom_ref_count'])\n            # number of heterozygote calls\n            ind_info['het_count'] = convert_number(ind_info['het_count'])\n\n            # ratio of het_count / hom_alt_count. Low for males, high for females\n            ind_info['het_ratio'] = convert_number(ind_info['het_ratio'])\n\n            sex_check.append(ind_info)\n\n    return sex_check", "response": "Parse a. ped_check. csv file and return a list of dicts."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hpo_terms(store, query = None, limit = None):\n    hpo_phenotypes = {}\n    if limit:\n        limit=int(limit)\n\n    hpo_phenotypes['phenotypes'] = list(store.hpo_terms(text=query, limit=limit))\n    return hpo_phenotypes", "response": "Retrieves a list of HPO terms from the scout database\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nshowing all objects in the whitelist collection", "response": "def whitelist(context):\n    \"\"\"Show all objects in the whitelist collection\"\"\"\n    LOG.info(\"Running scout view users\")\n    adapter = context.obj['adapter']\n\n    ## TODO add a User interface to the adapter\n    for whitelist_obj in adapter.whitelist_collection.find():\n        click.echo(whitelist_obj['_id'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds a small phenotype object with phenotype_id and description", "response": "def build_phenotype(phenotype_id, adapter):\n    \"\"\"Build a small phenotype object\n\n        Build a dictionary with phenotype_id and description\n\n    Args:\n        phenotype_id (str): The phenotype id\n        adapter (scout.adapter.MongoAdapter)\n\n    Returns:\n        phenotype_obj (dict):\n\n        dict(\n            phenotype_id = str,\n            feature = str, # description of phenotype\n            )\n    \"\"\"\n    phenotype_obj = {}\n    phenotype = adapter.hpo_term(phenotype_id)\n    if phenotype:\n        phenotype_obj['phenotype_id'] = phenotype['hpo_id']\n        phenotype_obj['feature'] = phenotype['description']\n    return phenotype"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef build_case(case_data, adapter):\n    log.info(\"build case with id: {0}\".format(case_data['case_id']))\n    case_obj = {\n        '_id': case_data['case_id'],\n        'display_name': case_data.get('display_name', case_data['case_id']),\n    }\n\n    # Check if institute exists in database\n    try:\n        institute_id = case_data['owner']\n    except KeyError as err:\n        raise ConfigError(\"Case has to have a institute\")\n    institute_obj = adapter.institute(institute_id)\n    if not institute_obj:\n        raise IntegrityError(\"Institute %s not found in database\" % institute_id)\n    case_obj['owner'] = case_data['owner']\n\n    # Owner allways has to be part of collaborators\n    collaborators = set(case_data.get('collaborators', []))\n    collaborators.add(case_data['owner'])\n    case_obj['collaborators'] = list(collaborators)\n\n    if case_data.get('assignee'):\n        case_obj['assignees'] = [case_data['assignee']]\n\n    # Individuals\n    ind_objs = []\n    try:\n        for individual in case_data.get('individuals', []):\n            ind_objs.append(build_individual(individual))\n    except Exception as error:\n        ## TODO add some action here\n        raise error\n    # sort the samples to put the affected individual first\n    sorted_inds = sorted(ind_objs, key=lambda ind: -ind['phenotype'])\n    case_obj['individuals'] = sorted_inds\n\n    now = datetime.now()\n    case_obj['created_at'] = now\n    case_obj['updated_at'] = now\n\n    if case_data.get('suspects'):\n        case_obj['suspects'] = case_data['suspects']\n    if case_data.get('causatives'):\n        case_obj['causatives'] = case_data['causatives']\n\n    case_obj['synopsis'] = case_data.get('synopsis', '')\n\n    case_obj['status'] = 'inactive'\n    case_obj['is_research'] = False\n    case_obj['research_requested'] = False\n    case_obj['rerun_requested'] = False\n\n    analysis_date = case_data.get('analysis_date')\n    if analysis_date:\n        case_obj['analysis_date'] = analysis_date\n\n    # We store some metadata and references about gene panels in 'panels'\n    case_panels = case_data.get('gene_panels', [])\n    default_panels = case_data.get('default_panels', [])\n    panels = []\n\n    for panel_name in case_panels:\n        panel_obj = adapter.gene_panel(panel_name)\n        if not panel_obj:\n            raise IntegrityError(\"Panel %s does not exist in database\" % panel_name)\n        panel = {\n            'panel_id': panel_obj['_id'],\n            'panel_name': panel_obj['panel_name'],\n            'display_name': panel_obj['display_name'],\n            'version': panel_obj['version'],\n            'updated_at': panel_obj['date'],\n            'nr_genes': len(panel_obj['genes'])\n        }\n        if panel_name in default_panels:\n            panel['is_default'] = True\n        else:\n            panel['is_default'] = False\n        panels.append(panel)\n\n    case_obj['panels'] = panels\n\n    case_obj['dynamic_gene_list'] = {}\n\n    # Meta data\n    genome_build = case_data.get('genome_build', '37')\n    if not genome_build in ['37', '38']:\n        pass\n        ##TODO raise exception if invalid genome build was used\n\n    case_obj['genome_build'] = genome_build\n    case_obj['genome_version'] = case_data.get('genome_version')\n\n    if case_data.get('rank_model_version'):\n        case_obj['rank_model_version'] = str(case_data['rank_model_version'])\n\n    if case_data.get('sv_rank_model_version'):\n        case_obj['sv_rank_model_version'] = str(case_data['sv_rank_model_version'])\n\n    if case_data.get('rank_score_threshold'):\n        case_obj['rank_score_threshold'] = float(case_data['rank_score_threshold'])\n\n    # phenotype information\n    phenotypes = []\n    for phenotype in case_data.get('phenotype_terms', []):\n        phenotype_obj = build_phenotype(phenotype, adapter)\n        if phenotype_obj:\n            phenotypes.append(phenotype_obj)\n    if phenotypes:\n        case_obj['phenotype_terms'] = phenotypes\n\n    # phenotype groups\n    phenotype_groups = []\n    for phenotype in case_data.get('phenotype_groups', []):\n        phenotype_obj = build_phenotype(phenotype, adapter)\n        if phenotype_obj:\n            phenotype_groups.append(phenotype_obj)\n    if phenotype_groups:\n        case_obj['phenotype_groups'] = phenotype_groups\n\n\n    # Files\n    case_obj['madeline_info'] = case_data.get('madeline_info')\n    if 'multiqc' in case_data:\n        case_obj['multiqc'] = case_data.get('multiqc')\n    case_obj['vcf_files'] = case_data.get('vcf_files', {})\n    case_obj['delivery_report'] = case_data.get('delivery_report')\n\n    case_obj['has_svvariants'] = False\n    if (case_obj['vcf_files'].get('vcf_sv') or case_obj['vcf_files'].get('vcf_sv_research')):\n        case_obj['has_svvariants'] = True\n\n    case_obj['has_strvariants'] = False\n    if (case_obj['vcf_files'].get('vcf_str')):\n        case_obj['has_strvariants'] = True\n\n    case_obj['is_migrated'] = False\n\n    case_obj['track'] = case_data.get('track', 'rare')\n\n\n    return case_obj", "response": "Build a case object that is to be inserted into the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef gene(store, hgnc_id):\n    res = {'builds': {'37': None, '38': None}, 'symbol': None, 'description': None, 'ensembl_id': None, 'record': None}\n\n    for build in res['builds']:\n        record = store.hgnc_gene(hgnc_id, build=build)\n        if record:\n\n            record['position'] = \"{this[chromosome]}:{this[start]}-{this[end]}\".format(this=record)\n            res['aliases'] = record['aliases']\n            res['hgnc_id'] = record['hgnc_id']\n            res['description'] = record['description']\n            res['builds'][build] = record\n            res['symbol'] = record['hgnc_symbol']\n            res['description'] = record['description']\n            res['entrez_id'] = record.get('entrez_id')\n            res['pli_score'] = record.get('pli_score')\n\n            add_gene_links(record, int(build))\n\n            res['omim_id'] = record.get('omim_id')\n            res['incomplete_penetrance'] = record.get('incomplete_penetrance',False)\n            res['inheritance_models'] = record.get('inheritance_models',[])\n            for transcript in record['transcripts']:\n                transcript['position'] = (\"{this[chrom]}:{this[start]}-{this[end]}\"\n                                          .format(this=transcript))\n                add_tx_links(transcript, build)\n\n            for phenotype in record.get('phenotypes',[]):\n                phenotype['omim_link'] = omim(phenotype.get('mim_number'))\n\n            if not res['record']:\n                res['record'] = record\n\n    # If none of the genes where found\n    if not any(res.values()):\n        raise ValueError\n\n    return res", "response": "Parse information about a gene."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfetching matching genes and convert to JSON.", "response": "def genes_to_json(store, query):\n    \"\"\"Fetch matching genes and convert to JSON.\"\"\"\n    gene_query = store.hgnc_genes(query, search=True)\n    json_terms = [{'name': \"{} | {} ({})\".format(gene['hgnc_id'], gene['hgnc_symbol'],\n                                                 ', '.join(gene['aliases'])),\n                   'id': gene['hgnc_id']} for gene in gene_query]\n    return json_terms"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndisplays the Scout dashboard.", "response": "def index():\n    \"\"\"Display the Scout dashboard.\"\"\"\n    accessible_institutes = current_user.institutes\n    if not 'admin' in current_user.roles:\n        accessible_institutes = current_user.institutes\n        if not accessible_institutes:\n            flash('Not allowed to see information - please visit the dashboard later!')\n            return redirect(url_for('cases.dahboard_general.html'))\n\n    LOG.debug('User accessible institutes: {}'.format(accessible_institutes))\n    institutes = [inst for inst in store.institutes(accessible_institutes)]\n\n    # Insert a entry that displays all institutes in the beginning of the array\n    institutes.insert(0, {'_id': None, 'display_name': 'All institutes'})\n\n    institute_id = None\n    slice_query = None\n    panel=1\n    if request.method=='POST':\n        institute_id = request.form.get('institute')\n        slice_query = request.form.get('query')\n        panel=request.form.get('pane_id')\n\n    elif request.method=='GET':\n        institute_id = request.args.get('institute')\n        slice_query = request.args.get('query')\n\n    # User should be restricted to their own institute if:\n    #1) Their default institute when the page is first loaded\n    #2) if they ask for an institute that they don't belong to\n    #3) if they want perform a query on all institutes\n\n    if not institute_id:\n        institute_id = accessible_institutes[0]\n    elif (not current_user.is_admin) and (slice_query and institute_id == 'None'):\n        institute_id = accessible_institutes[0]\n    elif (not institute_id in accessible_institutes) and not (institute_id == 'None'):\n        institute_id = accessible_institutes[0]\n\n    LOG.info(\"Fetch all cases with institute: %s\", institute_id)\n\n    data = get_dashboard_info(store, institute_id, slice_query)\n    data['institutes'] = institutes\n    data['choice'] = institute_id\n    total_cases = data['total_cases']\n\n    LOG.info(\"Found %s cases\", total_cases)\n    if total_cases == 0:\n        flash('no cases found for institute {} (with that query) - please visit the dashboard later!'.format(institute_id), 'info')\n#        return redirect(url_for('cases.index'))\n\n    return render_template(\n        'dashboard/dashboard_general.html', institute=institute_id, query=slice_query, panel=panel, **data)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a requests response from url", "response": "def get_request(url):\n    \"\"\"Return a requests response from url\n    \n    Args:\n        url(str)\n    \n    Returns:\n        decoded_data(str): Decoded response\n    \"\"\"\n    try:\n        LOG.info(\"Requesting %s\", url)\n        response = urllib.request.urlopen(url)\n        if url.endswith('.gz'):\n            LOG.info(\"Decompress zipped file\")\n            data = gzip.decompress(response.read())      # a `bytes` object\n        else:\n            data = response.read()      # a `bytes` object\n        decoded_data = data.decode('utf-8')\n    except HTTPError as err:\n        LOG.warning(\"Something went wrong, perhaps the api key is not valid?\")\n        raise err\n    except URLError as err:\n        LOG.warning(\"Something went wrong, are you connected to internet?\")\n        raise err\n    \n    if 'Error' in decoded_data:\n        raise URLError(\"Seems like url {} does not exist\".format(url))\n    \n    return decoded_data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fetch_resource(url):\n    try:\n        data = get_request(url)\n        lines = data.split('\\n')\n    except Exception as err:\n        raise err\n    \n    return lines", "response": "Fetch a resource and return the resulting lines in a list\n    Send file_name to get more clean log messages\n    \n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfetching the necessary files from the OMIM API.", "response": "def fetch_mim_files(api_key, mim2genes=False, mimtitles=False, morbidmap=False, genemap2=False):\n    \"\"\"Fetch the necessary mim files using a api key\n    \n    Args:\n        api_key(str): A api key necessary to fetch mim data\n    \n    Returns:\n        mim_files(dict): A dictionary with the neccesary files\n    \"\"\"\n\n    LOG.info(\"Fetching OMIM files from https://omim.org/\")\n    mim2genes_url =  'https://omim.org/static/omim/data/mim2gene.txt'\n    mimtitles_url= 'https://data.omim.org/downloads/{0}/mimTitles.txt'.format(api_key)\n    morbidmap_url = 'https://data.omim.org/downloads/{0}/morbidmap.txt'.format(api_key)\n    genemap2_url =  'https://data.omim.org/downloads/{0}/genemap2.txt'.format(api_key)\n        \n    mim_files = {}\n    mim_urls = {}\n    \n    if mim2genes is True:\n        mim_urls['mim2genes'] = mim2genes_url\n    if mimtitles is True:\n        mim_urls['mimtitles'] = mimtitles_url\n    if morbidmap is True:\n        mim_urls['morbidmap'] = morbidmap_url\n    if genemap2 is True:\n        mim_urls['genemap2'] = genemap2_url\n\n    for file_name in mim_urls:\n        url = mim_urls[file_name]\n        mim_files[file_name] = fetch_resource(url)\n\n    return mim_files"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfetches the ensembl genes from HSAPIEN", "response": "def fetch_ensembl_genes(build='37'):\n    \"\"\"Fetch the ensembl genes\n    \n    Args:\n        build(str): ['37', '38']\n    \"\"\"\n    if build == '37':\n        url = 'http://grch37.ensembl.org'\n    else:\n        url = 'http://www.ensembl.org'\n    \n    LOG.info(\"Fetching ensembl genes from %s\", url)\n    dataset_name = 'hsapiens_gene_ensembl'\n    \n    dataset = pybiomart.Dataset(name=dataset_name, host=url)\n    \n    attributes = [\n        'chromosome_name',\n        'start_position',\n        'end_position',\n        'ensembl_gene_id',\n        'hgnc_symbol',\n        'hgnc_id',\n    ]\n    \n    filters = {\n        'chromosome_name': CHROMOSOMES,\n    }\n    \n    result = dataset.query(\n        attributes = attributes,\n        filters = filters,\n        use_attr_names=True,\n    )\n    \n    return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfetches the ensembl exons from HSAPIens.", "response": "def fetch_ensembl_exons(build='37'):\n    \"\"\"Fetch the ensembl genes\n    \n    Args:\n        build(str): ['37', '38']\n    \"\"\"\n    LOG.info(\"Fetching ensembl exons build %s ...\", build)\n    if build == '37':\n        url = 'http://grch37.ensembl.org'\n    else:\n        url = 'http://www.ensembl.org'\n    \n    dataset_name = 'hsapiens_gene_ensembl'\n    \n    dataset = pybiomart.Dataset(name=dataset_name, host=url)\n    \n    attributes = [\n        'chromosome_name',\n        'ensembl_gene_id',\n        'ensembl_transcript_id',\n        'ensembl_exon_id',\n        'exon_chrom_start',\n        'exon_chrom_end',\n        '5_utr_start',\n        '5_utr_end',\n        '3_utr_start',\n        '3_utr_end',\n        'strand',\n        'rank'\n    ]\n    \n    filters = {\n        'chromosome_name': CHROMOSOMES,\n    }\n    \n    result = dataset.query(\n        attributes = attributes,\n        filters = filters\n    )\n    \n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfetch the hgnc genes file from ftp://ftp. ebi. ac. uk", "response": "def fetch_hgnc():\n    \"\"\"Fetch the hgnc genes file from \n        ftp://ftp.ebi.ac.uk/pub/databases/genenames/new/tsv/hgnc_complete_set.txt\n    \n    Returns:\n        hgnc_gene_lines(list(str))\n    \"\"\"\n    file_name = \"hgnc_complete_set.txt\"\n    url = 'ftp://ftp.ebi.ac.uk/pub/databases/genenames/new/tsv/{0}'.format(file_name)\n    LOG.info(\"Fetching HGNC genes\")\n    \n    hgnc_lines = fetch_resource(url)\n    \n    return hgnc_lines"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fetch_exac_constraint():\n    file_name = 'fordist_cleaned_exac_r03_march16_z_pli_rec_null_data.txt' \n    url = ('ftp://ftp.broadinstitute.org/pub/ExAC_release/release0.3/functional_gene_constraint'\n           '/{0}').format(file_name)\n    \n    LOG.info(\"Fetching ExAC genes\")\n    \n    try:\n        exac_lines = fetch_resource(url)\n    except URLError as err:\n        LOG.info(\"Failed to fetch exac constraint scores file from ftp server\")\n        LOG.info(\"Try to fetch from google bucket...\")\n        url = (\"https://storage.googleapis.com/gnomad-public/legacy/exacv1_downloads/release0.3.1\"\n               \"/manuscript_data/forweb_cleaned_exac_r03_march16_z_data_pLI.txt.gz\")\n\n    exac_lines = fetch_resource(url)\n\n    return exac_lines", "response": "Fetch the file with exac constraint scores\n    \n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fetch_hpo_files(hpogenes=False, hpoterms=False, phenotype_to_terms=False, hpodisease=False):\n\n    LOG.info(\"Fetching HPO information from http://compbio.charite.de\")\n    base_url = ('http://compbio.charite.de/jenkins/job/hpo.annotations.monthly/'\n                'lastStableBuild/artifact/annotation/{}')\n    hpogenes_url =  base_url.format('ALL_SOURCES_ALL_FREQUENCIES_genes_to_phenotype.txt')\n    hpoterms_url= base_url.format('ALL_SOURCES_ALL_FREQUENCIES_phenotype_to_genes.txt')\n    hpo_phenotype_to_terms_url = base_url.format('ALL_SOURCES_ALL_FREQUENCIES_diseases_to_genes_to_phenotypes.txt')\n    hpodisease_url = base_url.format('diseases_to_genes.txt')\n        \n    hpo_files = {}\n    hpo_urls = {}\n    \n    if hpogenes is True:\n        hpo_urls['hpogenes'] = hpogenes_url\n    if hpoterms is True:\n        hpo_urls['hpoterms'] = hpoterms_url\n    if phenotype_to_terms is True:\n        hpo_urls['phenotype_to_terms'] = hpo_phenotype_to_terms_url\n    if hpodisease is True:\n        hpo_urls['hpodisease'] = hpodisease_url\n\n    for file_name in hpo_urls:\n        url = hpo_urls[file_name]\n        hpo_files[file_name] = request_file(url)\n\n    return hpo_files", "response": "Fetch the necessary mim files using a api key"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef transcripts(context, build, hgnc_id, json):\n    LOG.info(\"Running scout view transcripts\")\n    adapter = context.obj['adapter']\n\n    if not json:\n        click.echo(\"Chromosome\\tstart\\tend\\ttranscript_id\\thgnc_id\\trefseq\\tis_primary\")\n    for tx_obj in adapter.transcripts(build=build, hgnc_id=hgnc_id):\n        if json:\n            pp(tx_obj)\n            continue\n        click.echo(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\".format(\n            tx_obj['chrom'],\n            tx_obj['start'],\n            tx_obj['end'],\n            tx_obj['ensembl_transcript_id'],\n            tx_obj['hgnc_id'],\n            tx_obj.get('refseq_id', ''),\n            tx_obj.get('is_primary') or '',\n        ))", "response": "Show all transcripts in the database"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd an occurrence key to the event object w/ a list of occurrences", "response": "def add_occurrences(events, count):\n    \"\"\"\n    Adds an occurrence key to the event object w/ a list of occurrences\n    and adds a popover (for use with twitter bootstrap).\n    The occurrence is added so that each event can be aware of what\n    day(s) it occurs in the month.\n    \"\"\"\n    for day in count:\n        for item in count[day]:\n            for event in events:\n                if event.pk == item[1]:\n                    try:\n                        event.occurrence.append(day)\n                    except AttributeError:\n                        event.occurrence = []\n                        event.occurrence.append(day)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the events that occur on the given day.", "response": "def day_display(year, month, all_month_events, day):\n    \"\"\"\n    Returns the events that occur on the given day.\n    Works by getting all occurrences for the month, then drilling\n    down to only those occurring on the given day.\n    \"\"\"\n    # Get a dict with all of the events for the month\n    count = CountHandler(year, month, all_month_events).get_count()\n    pks = [x[1] for x in count[day]]  # list of pks for events on given day\n    # List enables sorting.\n    # See the comments in EventMonthView in views.py for more info\n    day_events = list(Event.objects.filter(pk__in=pks).order_by(\n        'start_date').prefetch_related('cancellations'))\n    day_events.sort(key=lambda x: x.l_start_date.hour)\n    return day_events"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef variants(store, institute_obj, case_obj, variants_query, page=1, per_page=50):\n    variant_count = variants_query.count()\n    skip_count = per_page * max(page - 1, 0)\n    more_variants = True if variant_count > (skip_count + per_page) else False\n    variant_res = variants_query.skip(skip_count).limit(per_page)\n\n    genome_build = case_obj.get('genome_build', '37')\n    if genome_build not in ['37','38']:\n        genome_build = '37'\n\n    variants = []\n    for variant_obj in variant_res:\n        overlapping_svs = [sv for sv in store.overlapping(variant_obj)]\n        variant_obj['overlapping'] = overlapping_svs or None\n        variants.append(parse_variant(store, institute_obj, case_obj, variant_obj,\n                        update=True, genome_build=genome_build))\n\n    return {\n        'variants': variants,\n        'more_variants': more_variants,\n    }", "response": "Pre - process list of variants."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef str_variants(store, institute_obj, case_obj, variants_query, page=1, per_page=50):\n    # Nothing unique to STRs on this level. Inheritance?\n    return variants(store, institute_obj, case_obj, variants_query, page, per_page)", "response": "Retrieve a list of STR variants."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sv_variant(store, institute_id, case_name, variant_id=None, variant_obj=None, add_case=True,\n               get_overlapping=True):\n    \"\"\"Pre-process an SV variant entry for detail page.\n\n    Adds information to display variant\n\n    Args:\n        store(scout.adapter.MongoAdapter)\n        institute_id(str)\n        case_name(str)\n        variant_id(str)\n        variant_obj(dcit)\n        add_case(bool): If information about case files should be added\n\n    Returns:\n        detailed_information(dict): {\n            'institute': <institute_obj>,\n            'case': <case_obj>,\n            'variant': <variant_obj>,\n            'overlapping_snvs': <overlapping_snvs>,\n            'manual_rank_options': MANUAL_RANK_OPTIONS,\n            'dismiss_variant_options': DISMISS_VARIANT_OPTIONS\n        }\n    \"\"\"\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n\n    if not variant_obj:\n        variant_obj = store.variant(variant_id)\n\n    if add_case:\n        # fill in information for pilup view\n        variant_case(store, case_obj, variant_obj)\n\n    # frequencies\n    variant_obj['frequencies'] = [\n        ('1000G', variant_obj.get('thousand_genomes_frequency')),\n        ('1000G (left)', variant_obj.get('thousand_genomes_frequency_left')),\n        ('1000G (right)', variant_obj.get('thousand_genomes_frequency_right')),\n        ('ClinGen CGH (benign)', variant_obj.get('clingen_cgh_benign')),\n        ('ClinGen CGH (pathogenic)', variant_obj.get('clingen_cgh_pathogenic')),\n        ('ClinGen NGI', variant_obj.get('clingen_ngi')),\n        ('SweGen', variant_obj.get('swegen')),\n        ('Decipher', variant_obj.get('decipher')),\n    ]\n\n    variant_obj['callers'] = callers(variant_obj, category='sv')\n\n    overlapping_snvs = []\n    if get_overlapping:\n        overlapping_snvs = (parse_variant(store, institute_obj, case_obj, variant) for variant in\n                            store.overlapping(variant_obj))\n\n    # parse_gene function is not called for SVs, but a link to ensembl gene is required\n    for gene_obj in variant_obj['genes']:\n        if gene_obj.get('common'):\n            ensembl_id = gene_obj['common']['ensembl_id']\n            try:\n                build = int(gene_obj['common'].get('build','37'))\n            except Exception:\n                build = 37\n            gene_obj['ensembl_link'] = ensembl(ensembl_id, build=build)\n\n    variant_obj['comments'] = store.events(institute_obj, case=case_obj,\n                                           variant_id=variant_obj['variant_id'], comments=True)\n\n    case_clinvars = store.case_to_clinVars(case_obj.get('display_name'))\n    if variant_id in case_clinvars:\n        variant_obj['clinvar_clinsig'] = case_clinvars.get(variant_id)['clinsig']\n\n    if not 'end_chrom' in variant_obj:\n        variant_obj['end_chrom'] = variant_obj['chromosome']\n\n    return {\n        'institute': institute_obj,\n        'case': case_obj,\n        'variant': variant_obj,\n        'overlapping_snvs': overlapping_snvs,\n        'manual_rank_options': MANUAL_RANK_OPTIONS,\n        'dismiss_variant_options': DISMISS_VARIANT_OPTIONS\n    }", "response": "Pre - process an SV variant entry for detail page."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_variant(store, institute_obj, case_obj, variant_obj, update=False, genome_build='37',\n                  get_compounds = True):\n    \"\"\"Parse information about variants.\n\n    - Adds information about compounds\n    - Updates the information about compounds if necessary and 'update=True'\n\n    Args:\n        store(scout.adapter.MongoAdapter)\n        institute_obj(scout.models.Institute)\n        case_obj(scout.models.Case)\n        variant_obj(scout.models.Variant)\n        update(bool): If variant should be updated in database\n        genome_build(str)\n\n    \"\"\"\n    has_changed = False\n    compounds = variant_obj.get('compounds', [])\n    if compounds and get_compounds:\n        # Check if we need to add compound information\n        # If it is the first time the case is viewed we fill in some compound information\n        if 'not_loaded' not in compounds[0]:\n            new_compounds = store.update_variant_compounds(variant_obj)\n            variant_obj['compounds'] = new_compounds\n            has_changed = True\n\n        # sort compounds on combined rank score\n        variant_obj['compounds'] = sorted(variant_obj['compounds'],\n                                          key=lambda compound: -compound['combined_score'])\n\n    # Update the hgnc symbols if they are incorrect\n    variant_genes = variant_obj.get('genes')\n    if variant_genes is not None:\n        for gene_obj in variant_genes:\n            # If there is no hgnc id there is nothin we can do\n            if not gene_obj['hgnc_id']:\n                continue\n            # Else we collect the gene object and check the id\n            if gene_obj.get('hgnc_symbol') is None:\n                hgnc_gene = store.hgnc_gene(gene_obj['hgnc_id'], build=genome_build)\n                if not hgnc_gene:\n                    continue\n                has_changed = True\n                gene_obj['hgnc_symbol'] = hgnc_gene['hgnc_symbol']\n\n    # We update the variant if some information was missing from loading\n    # Or if symbold in reference genes have changed\n    if update and has_changed:\n        variant_obj = store.update_variant(variant_obj)\n\n    variant_obj['comments'] = store.events(institute_obj, case=case_obj,\n                                           variant_id=variant_obj['variant_id'], comments=True)\n\n    if variant_genes:\n        variant_obj.update(get_predictions(variant_genes))\n        if variant_obj.get('category') == 'cancer':\n            variant_obj.update(get_variant_info(variant_genes))\n\n    for compound_obj in compounds:\n        compound_obj.update(get_predictions(compound_obj.get('genes', [])))\n\n    if isinstance(variant_obj.get('acmg_classification'), int):\n        acmg_code = ACMG_MAP[variant_obj['acmg_classification']]\n        variant_obj['acmg_classification'] = ACMG_COMPLETE_MAP[acmg_code]\n\n\n    # convert length for SV variants\n    variant_length = variant_obj.get('length')\n    variant_obj['length'] = {100000000000: 'inf', -1: 'n.d.'}.get(variant_length, variant_length)\n    if not 'end_chrom' in variant_obj:\n        variant_obj['end_chrom'] = variant_obj['chromosome']\n\n    return variant_obj", "response": "Parse a single variant into a single object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget variants info to be exported to file one list ( line per variant.", "response": "def variant_export_lines(store, case_obj, variants_query):\n    \"\"\"Get variants info to be exported to file, one list (line) per variant.\n\n        Args:\n            store(scout.adapter.MongoAdapter)\n            case_obj(scout.models.Case)\n            variants_query: a list of variant objects, each one is a dictionary\n\n        Returns:\n            export_variants: a list of strings. Each string  of the list corresponding to the fields\n                             of a variant to be exported to file, separated by comma\n    \"\"\"\n\n    export_variants = []\n\n    for variant in variants_query:\n        variant_line = []\n        position = variant['position']\n        change = variant['reference']+'>'+variant['alternative']\n        variant_line.append(variant['rank_score'])\n        variant_line.append(variant['chromosome'])\n        variant_line.append(position)\n        variant_line.append(change)\n        variant_line.append('_'.join([str(position), change]))\n\n        # gather gene info:\n        gene_list = variant.get('genes') #this is a list of gene objects\n        gene_ids = []\n        gene_names = []\n        hgvs_c = []\n\n        # if variant is in genes\n        if len(gene_list) > 0:\n            for gene_obj in gene_list:\n                hgnc_id = gene_obj['hgnc_id']\n                gene_name = gene(store, hgnc_id)['symbol']\n\n                gene_ids.append(hgnc_id)\n                gene_names.append(gene_name)\n\n                hgvs_nucleotide = '-'\n                # gather HGVS info from gene transcripts\n                transcripts_list = gene_obj.get('transcripts')\n                for transcript_obj in transcripts_list:\n                    if transcript_obj.get('is_canonical') and transcript_obj.get('is_canonical') is True:\n                        hgvs_nucleotide = str(transcript_obj.get('coding_sequence_name'))\n                hgvs_c.append(hgvs_nucleotide)\n\n            variant_line.append(';'.join( str(x) for x in  gene_ids))\n            variant_line.append(';'.join( str(x) for x in  gene_names))\n            variant_line.append(';'.join( str(x) for x in  hgvs_c))\n        else:\n            while i < 4:\n                variant_line.append('-') # instead of gene ids\n                i = i+1\n\n        variant_gts = variant['samples'] # list of coverage and gt calls for case samples\n        for individual in case_obj['individuals']:\n            for variant_gt in variant_gts:\n                if individual['individual_id'] == variant_gt['sample_id']:\n                    # gather coverage info\n                    variant_line.append(variant_gt['allele_depths'][0]) # AD reference\n                    variant_line.append(variant_gt['allele_depths'][1]) # AD alternate\n                    # gather genotype quality info\n                    variant_line.append(variant_gt['genotype_quality'])\n\n        variant_line = [str(i) for i in variant_line]\n        export_variants.append(\",\".join(variant_line))\n\n    return export_variants"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a header for the CSV file with the filtered variants to be exported.", "response": "def variants_export_header(case_obj):\n    \"\"\"Returns a header for the CSV file with the filtered variants to be exported.\n\n        Args:\n            case_obj(scout.models.Case)\n\n        Returns:\n            header: includes the fields defined in scout.constants.variants_export EXPORT_HEADER\n                    + AD_reference, AD_alternate, GT_quality for each sample analysed for a case\n    \"\"\"\n    header = []\n    header = header + EXPORT_HEADER\n    # Add fields specific for case samples\n    for individual in case_obj['individuals']:\n        display_name = str(individual['display_name'])\n        header.append('AD_reference_'+display_name) # Add AD reference field for a sample\n        header.append('AD_alternate_'+display_name) # Add AD alternate field for a sample\n        header.append('GT_quality_'+display_name) # Add Genotype quality field for a sample\n    return header"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget variant information from a list of genes.", "response": "def get_variant_info(genes):\n    \"\"\"Get variant information\"\"\"\n    data = {'canonical_transcripts': []}\n    for gene_obj in genes:\n        if not gene_obj.get('canonical_transcripts'):\n            tx = gene_obj['transcripts'][0]\n            tx_id = tx['transcript_id']\n            exon = tx.get('exon', '-')\n            c_seq = tx.get('coding_sequence_name', '-')\n        else:\n            tx_id = gene_obj['canonical_transcripts']\n            exon = gene_obj.get('exon', '-')\n            c_seq = gene_obj.get('hgvs_identifier', '-')\n\n        if len(c_seq) > 20:\n            c_seq = c_seq[:20] + '...'\n\n        if len(genes) == 1:\n            value = ':'.join([tx_id,exon,c_seq])\n        else:\n            gene_id = gene_obj.get('hgnc_symbol') or str(gene_obj['hgnc_id'])\n            value = ':'.join([gene_id, tx_id,exon,c_seq])\n        data['canonical_transcripts'].append(value)\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_predictions(genes):\n    data = {\n        'sift_predictions': [],\n        'polyphen_predictions': [],\n        'region_annotations': [],\n        'functional_annotations': []\n    }\n    for gene_obj in genes:\n        for pred_key in data:\n            gene_key = pred_key[:-1]\n            if len(genes) == 1:\n                value = gene_obj.get(gene_key, '-')\n            else:\n                gene_id = gene_obj.get('hgnc_symbol') or str(gene_obj['hgnc_id'])\n                value = ':'.join([gene_id, gene_obj.get(gene_key, '-')])\n            data[pred_key].append(value)\n\n    return data", "response": "Get sift predictions from genes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef variant_case(store, case_obj, variant_obj):\n    case_obj['bam_files'] = []\n    case_obj['mt_bams'] = []\n    case_obj['bai_files'] = []\n    case_obj['mt_bais'] = []\n    case_obj['sample_names'] = []\n    for individual in case_obj['individuals']:\n        bam_path = individual.get('bam_file')\n        mt_bam = individual.get('mt_bam')\n        case_obj['sample_names'].append(individual.get('display_name'))\n        if bam_path and os.path.exists(bam_path):\n            case_obj['bam_files'].append(individual['bam_file'])\n            case_obj['bai_files'].append(find_bai_file(individual['bam_file']))\n        if mt_bam and os.path.exists(mt_bam):\n            case_obj['mt_bams'].append(individual['mt_bam'])\n            case_obj['mt_bais'].append(find_bai_file(individual['mt_bam']))\n\n        else:\n            LOG.debug(\"%s: no bam file found\", individual['individual_id'])\n\n    try:\n        genes = variant_obj.get('genes', [])\n        if len(genes) == 1:\n            hgnc_gene_obj = store.hgnc_gene(variant_obj['genes'][0]['hgnc_id'])\n            if hgnc_gene_obj:\n                vcf_path = store.get_region_vcf(case_obj, gene_obj=hgnc_gene_obj)\n                case_obj['region_vcf_file'] = vcf_path\n            else:\n                case_obj['region_vcf_file'] = None\n        elif len(genes) > 1:\n            chrom = variant_obj['genes'][0]['common']['chromosome']\n            start = min(gene['common']['start'] for gene in variant_obj['genes'])\n            end = max(gene['common']['end'] for gene in variant_obj['genes'])\n            # Create a reduced VCF with variants in the region\n            vcf_path = store.get_region_vcf(case_obj, chrom=chrom, start=start, end=end)\n            case_obj['region_vcf_file'] = vcf_path\n    except (SyntaxError, Exception):\n        LOG.warning(\"skip VCF region for alignment view\")", "response": "Pre - process the variant view."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_bai_file(bam_file):\n    bai_file = bam_file.replace('.bam', '.bai')\n    if not os.path.exists(bai_file):\n        # try the other convention\n        bai_file = \"{}.bai\".format(bam_file)\n    return bai_file", "response": "Find out BAI file by extension given the BAM file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef variant(store, institute_obj, case_obj, variant_id=None, variant_obj=None, add_case=True,\n            add_other=True, get_overlapping=True):\n    \"\"\"Pre-process a single variant for the detailed variant view.\n\n    Adds information from case and institute that is not present on the variant\n    object\n\n    Args:\n        store(scout.adapter.MongoAdapter)\n        institute_obj(scout.models.Institute)\n        case_obj(scout.models.Case)\n        variant_id(str)\n        variant_obj(dict)\n        add_case(bool): If info about files case should be added\n        add_other(bool): If information about other causatives should be added\n        get_overlapping(bool): If overlapping svs should be collected\n\n    Returns:\n        variant_info(dict): {\n            'variant': <variant_obj>,\n            'causatives': <list(other_causatives)>,\n            'events': <list(events)>,\n            'overlapping_svs': <list(overlapping svs)>,\n            'manual_rank_options': MANUAL_RANK_OPTIONS,\n            'dismiss_variant_options': DISMISS_VARIANT_OPTIONS,\n            'ACMG_OPTIONS': ACMG_OPTIONS,\n            'evaluations': <list(evaluations)>,\n        }\n\n    \"\"\"\n    # If the variant is already collected we skip this part\n    if not variant_obj:\n        default_panels = []\n        # Add default panel information to variant\n        for panel in case_obj['panels']:\n            if not panel.get('is_default'):\n                continue\n            panel_obj = store.gene_panel(panel['panel_name'], panel.get('version'))\n            if not panel:\n                LOG.warning(\"Panel {0} version {1} could not be found\".format(\n                    panel['panel_name'], panel.get('version')))\n                continue\n            default_panels.append(panel_obj)\n\n        # NOTE this will query with variant_id == document_id, not the variant_id.\n        variant_obj = store.variant(variant_id, gene_panels=default_panels)\n\n\n    genome_build = case_obj.get('genome_build', '37')\n    if genome_build not in ['37','38']:\n        genome_build = '37'\n\n    if variant_obj is None:\n        return None\n    # Add information to case_obj\n    if add_case:\n        variant_case(store, case_obj, variant_obj)\n\n    # Collect all the events for the variant\n    events = list(store.events(institute_obj, case=case_obj, variant_id=variant_obj['variant_id']))\n    for event in events:\n        event['verb'] = VERBS_MAP[event['verb']]\n\n    other_causatives = []\n    # Adds information about other causative variants\n    if add_other:\n        for other_variant in store.other_causatives(case_obj, variant_obj):\n            # This should work with old and new ids\n            case_id = other_variant['case_id']\n            other_case = store.case(case_id)\n            if not other_case:\n                continue\n            other_variant['case_display_name'] = other_case.get('display_name', case_id)\n            other_causatives.append(other_variant)\n\n    variant_obj = parse_variant(store, institute_obj, case_obj, variant_obj, genome_build=genome_build)\n\n    variant_obj['end_position'] = end_position(variant_obj)\n    variant_obj['frequency'] = frequency(variant_obj)\n    variant_obj['clinsig_human'] = (clinsig_human(variant_obj) if variant_obj.get('clnsig')\n                                    else None)\n    variant_obj['thousandg_link'] = thousandg_link(variant_obj, genome_build)\n    variant_obj['exac_link'] = exac_link(variant_obj)\n    variant_obj['gnomad_link'] = gnomad_link(variant_obj)\n    variant_obj['swegen_link'] = swegen_link(variant_obj)\n    variant_obj['cosmic_link'] = cosmic_link(variant_obj)\n    variant_obj['beacon_link'] = beacon_link(variant_obj, genome_build)\n    variant_obj['ucsc_link'] = ucsc_link(variant_obj, genome_build)\n    variant_obj['alamut_link'] = alamut_link(variant_obj)\n    variant_obj['spidex_human'] = spidex_human(variant_obj)\n    variant_obj['expected_inheritance'] = expected_inheritance(variant_obj)\n    variant_obj['callers'] = callers(variant_obj, category='snv')\n\n    individuals = {individual['individual_id']: individual for individual in\n                   case_obj['individuals']}\n    for sample_obj in variant_obj['samples']:\n        individual = individuals[sample_obj.get('sample_id')]\n        if not individual:\n            return None\n        sample_obj['is_affected'] = True if individual['phenotype'] == 2 else False\n\n    gene_models = set()\n    variant_obj['disease_associated_transcripts'] = []\n\n    # Parse the gene models, both from panels and genes\n    for gene_obj in variant_obj.get('genes', []):\n        # Adds gene level links\n        parse_gene(gene_obj, genome_build)\n        omim_models = set()\n        for disease_term in gene_obj.get('disease_terms', []):\n            omim_models.update(disease_term.get('inheritance', []))\n        gene_obj['omim_inheritance'] = list(omim_models)\n\n        # Build strings for the disease associated transcripts from gene panel\n        for refseq_id in gene_obj.get('disease_associated_transcripts', []):\n            hgnc_symbol = (gene_obj['common']['hgnc_symbol'] if gene_obj.get('common') else\n                           gene_obj['hgnc_id'])\n            transcript_str = \"{}:{}\".format(hgnc_symbol, refseq_id)\n            variant_obj['disease_associated_transcripts'].append(transcript_str)\n\n        gene_models = gene_models | omim_models\n\n    if variant_obj.get('genetic_models'):\n        variant_models = set(model.split('_', 1)[0] for model in variant_obj['genetic_models'])\n        variant_obj['is_matching_inheritance'] = variant_models & gene_models\n\n    evaluations = []\n    for evaluation_obj in store.get_evaluations(variant_obj):\n        evaluation(store, evaluation_obj)\n        evaluations.append(evaluation_obj)\n\n    case_clinvars = store.case_to_clinVars(case_obj.get('display_name'))\n    if variant_id in case_clinvars:\n        variant_obj['clinvar_clinsig'] = case_clinvars.get(variant_id)['clinsig']\n\n    svs = []\n    if get_overlapping:\n        svs = (parse_variant(store, institute_obj, case_obj, variant_obj) for\n                            variant_obj in store.overlapping(variant_obj))\n\n    return {\n        'variant': variant_obj,\n        'causatives': other_causatives,\n        'events': events,\n        'overlapping_svs': svs,\n        'manual_rank_options': MANUAL_RANK_OPTIONS,\n        'dismiss_variant_options': DISMISS_VARIANT_OPTIONS,\n        'mosaic_variant_options': MOSAICISM_OPTIONS,\n        'ACMG_OPTIONS': ACMG_OPTIONS,\n        'evaluations': evaluations,\n    }", "response": "Pre - process a single variant for the detailed variant view."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nquerying observations for a variant.", "response": "def observations(store, loqusdb, case_obj, variant_obj):\n    \"\"\"Query observations for a variant.\"\"\"\n    composite_id = (\"{this[chromosome]}_{this[position]}_{this[reference]}_\"\n                    \"{this[alternative]}\".format(this=variant_obj))\n    obs_data = loqusdb.get_variant({'_id': composite_id}) or {}\n    obs_data['total'] = loqusdb.case_count()\n\n    obs_data['cases'] = []\n    institute_id = variant_obj['institute']\n    for case_id in obs_data.get('families', []):\n        if case_id != variant_obj['case_id'] and case_id.startswith(institute_id):\n            other_variant = store.variant(variant_obj['variant_id'], case_id=case_id)\n            other_case = store.case(case_id)\n            obs_data['cases'].append(dict(case=other_case, variant=other_variant))\n\n    return obs_data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a single gene.", "response": "def parse_gene(gene_obj, build=None):\n    \"\"\"Parse variant genes.\"\"\"\n    build = build or 37\n\n    if gene_obj.get('common'):\n        add_gene_links(gene_obj, build)\n        refseq_transcripts = []\n        for tx_obj in gene_obj['transcripts']:\n            parse_transcript(gene_obj, tx_obj, build)\n\n            # select refseq transcripts as \"primary\"\n            if not tx_obj.get('refseq_id'):\n                continue\n\n            refseq_transcripts.append(tx_obj)\n\n        gene_obj['primary_transcripts'] = (refseq_transcripts if refseq_transcripts else [])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_transcript(gene_obj, tx_obj, build=None):\n    build = build or 37\n    add_tx_links(tx_obj, build)\n\n    if tx_obj.get('refseq_id'):\n        gene_name = (gene_obj['common']['hgnc_symbol'] if gene_obj['common'] else\n                     gene_obj['hgnc_id'])\n        tx_obj['change_str'] = transcript_str(tx_obj, gene_name)", "response": "Parse variant gene transcript."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef transcript_str(transcript_obj, gene_name=None):\n    if transcript_obj.get('exon'):\n        gene_part, part_count_raw = 'exon', transcript_obj['exon']\n    elif transcript_obj.get('intron'):\n        gene_part, part_count_raw = 'intron', transcript_obj['intron']\n    else:\n        # variant between genes\n        gene_part, part_count_raw = 'intergenic', '0'\n\n    part_count = part_count_raw.rpartition('/')[0]\n    change_str = \"{}:{}{}:{}:{}\".format(\n        transcript_obj.get('refseq_id', ''),\n        gene_part,\n        part_count,\n        transcript_obj.get('coding_sequence_name', 'NA'),\n        transcript_obj.get('protein_sequence_name', 'NA'),\n    )\n    if gene_name:\n        change_str = \"{}:\".format(gene_name) + change_str\n    return change_str", "response": "Generate amino acid change as a string."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the end position of a variant.", "response": "def end_position(variant_obj):\n    \"\"\"Calculate end position for a variant.\"\"\"\n    alt_bases = len(variant_obj['alternative'])\n    num_bases = max(len(variant_obj['reference']), alt_bases)\n    return variant_obj['position'] + (num_bases - 1)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef frequency(variant_obj):\n    most_common_frequency = max(variant_obj.get('thousand_genomes_frequency') or 0,\n                                variant_obj.get('exac_frequency') or 0)\n    if most_common_frequency > .05:\n        return 'common'\n    elif most_common_frequency > .01:\n        return 'uncommon'\n    else:\n        return 'rare'", "response": "Returns a judgement on the overall frequency of the variant."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef clinsig_human(variant_obj):\n    for clinsig_obj in variant_obj['clnsig']:\n        # The clinsig objects allways have a accession\n        if isinstance(clinsig_obj['accession'], int):\n            # New version\n            link = \"https://www.ncbi.nlm.nih.gov/clinvar/variation/{}\"\n        else:\n            # Old version\n            link = \"https://www.ncbi.nlm.nih.gov/clinvar/{}\"\n\n        human_str = 'not provided'\n        if clinsig_obj.get('value'):\n            try:\n                # Old version\n                int(clinsig_obj['value'])\n                human_str = CLINSIG_MAP.get(clinsig_obj['value'], 'not provided')\n            except ValueError:\n                # New version\n                human_str = clinsig_obj['value']\n\n        clinsig_obj['human'] = human_str\n        clinsig_obj['link'] = link.format(clinsig_obj['accession'])\n\n        yield clinsig_obj", "response": "Convert to human readable version of CLINSIG evaluation."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomposes link to 1000G page for detailed information.", "response": "def thousandg_link(variant_obj, build=None):\n    \"\"\"Compose link to 1000G page for detailed information.\"\"\"\n    dbsnp_id = variant_obj.get('dbsnp_id')\n    build = build or 37\n\n    if not dbsnp_id:\n        return None\n\n    if build == 37:\n        url_template = (\"http://grch37.ensembl.org/Homo_sapiens/Variation/Explore\"\n                        \"?v={};vdb=variation\")\n    else:\n        url_template = (\"http://www.ensembl.org/Homo_sapiens/Variation/Explore\"\n                        \"?v={};vdb=variation\")\n\n    return url_template.format(dbsnp_id)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cosmic_link(variant_obj):\n\n    cosmic_ids = variant_obj.get('cosmic_ids')\n\n    if not cosmic_ids:\n        return None\n    else:\n        cosmic_id = cosmic_ids[0]\n        url_template = (\"https://cancer.sanger.ac.uk/cosmic/mutation/overview?id={}\")\n\n\n    return url_template.format(cosmic_id)", "response": "Compose link to COSMIIC database."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef beacon_link(variant_obj, build=None):\n    build = build or 37\n    url_template = (\"https://beacon-network.org/#/search?pos={this[position]}&\"\n                    \"chrom={this[chromosome]}&allele={this[alternative]}&\"\n                    \"ref={this[reference]}&rs=GRCh37\")\n    # beacon does not support build 38 at the moment\n    # if build == '38':\n    #     url_template = (\"https://beacon-network.org/#/search?pos={this[position]}&\"\n    #                     \"chrom={this[chromosome]}&allele={this[alternative]}&\"\n    #                     \"ref={this[reference]}&rs=GRCh38\")\n\n    return url_template.format(this=variant_obj)", "response": "Compose link to Beacon Network."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomposing link to UCSC.", "response": "def ucsc_link(variant_obj, build=None):\n    \"\"\"Compose link to UCSC.\"\"\"\n    build = build or 37\n    url_template = (\"http://genome.ucsc.edu/cgi-bin/hgTracks?db=hg19&\"\n                        \"position=chr{this[chromosome]}:{this[position]}\"\n                        \"-{this[position]}&dgv=pack&knownGene=pack&omimGene=pack\")\n    if build == 38:\n        url_template = (\"http://genome.ucsc.edu/cgi-bin/hgTracks?db=hg20&\"\n                        \"position=chr{this[chromosome]}:{this[position]}\"\n                        \"-{this[position]}&dgv=pack&knownGene=pack&omimGene=pack\")\n\n    return url_template.format(this=variant_obj)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef spidex_human(variant_obj):\n    if variant_obj.get('spidex') is None:\n        return 'not_reported'\n    elif abs(variant_obj['spidex']) < SPIDEX_HUMAN['low']['pos'][1]:\n        return 'low'\n    elif abs(variant_obj['spidex']) < SPIDEX_HUMAN['medium']['pos'][1]:\n        return 'medium'\n    else:\n        return 'high'", "response": "Translate SPIDEX annotation to human readable string."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef expected_inheritance(variant_obj):\n    manual_models = set()\n    for gene in variant_obj.get('genes', []):\n        manual_models.update(gene.get('manual_inheritance', []))\n    return list(manual_models)", "response": "Gather information from common gene information."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef callers(variant_obj, category='snv'):\n    calls = set()\n    for caller in CALLERS[category]:\n        if variant_obj.get(caller['id']):\n            calls.add((caller['name'], variant_obj[caller['id']]))\n\n    return list(calls)", "response": "Return info about callers."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsands a verification email and register the verification in the database Args: store(scout.adapter.MongoAdapter) mail(scout.server.extensions.mail): an instance of flask_mail.Mail institute_obj(dict): an institute object case_obj(dict): a case object user_obj(dict): a user object variant_obj(dict): a variant object (snv or sv) sender(str): current_app.config['MAIL_USERNAME'] variant_url(str): the complete url to the variant (snv or sv), a link that works from outside scout domain. order(str): False == cancel order, True==order verification comment(str): sender's entered comment from form url_builder(flask.url_for): for testing purposes, otherwise test verification email fails because out of context", "response": "def variant_verification(store, mail, institute_obj, case_obj, user_obj, variant_obj, sender, variant_url, order, comment, url_builder=url_for):\n    \"\"\"Sand a verification email and register the verification in the database\n\n        Args:\n            store(scout.adapter.MongoAdapter)\n            mail(scout.server.extensions.mail): an instance of flask_mail.Mail\n            institute_obj(dict): an institute object\n            case_obj(dict): a case object\n            user_obj(dict): a user object\n            variant_obj(dict): a variant object (snv or sv)\n            sender(str): current_app.config['MAIL_USERNAME']\n            variant_url(str): the complete url to the variant (snv or sv), a link that works from outside scout domain.\n            order(str): False == cancel order, True==order verification\n            comment(str): sender's entered comment from form\n            url_builder(flask.url_for): for testing purposes, otherwise test verification email fails because out of context\n    \"\"\"\n\n    recipients = institute_obj['sanger_recipients']\n    if len(recipients) == 0:\n        raise MissingSangerRecipientError()\n\n    view_type = None\n    email_subject = None\n    category = variant_obj.get('category')\n    display_name = None\n    chromosome = variant_obj['chromosome']\n    end_chrom = variant_obj.get('end_chrom', chromosome)\n    breakpoint_1 = ':'.join([chromosome, str(variant_obj['position'])])\n    breakpoint_2 = ':'.join([end_chrom, str(variant_obj.get('end'))])\n    variant_size = variant_obj.get('length')\n    panels = ', '.join(variant_obj.get('panels', []))\n    hgnc_symbol = ', '.join(variant_obj['hgnc_symbols'])\n    email_subj_gene_symbol = None\n    if len(variant_obj['hgnc_symbols']) > 3:\n        email_subj_gene_symbol = ' '.join([ str(len(variant_obj['hgnc_symbols'])) + 'genes'])\n    else:\n        email_subj_gene_symbol = hgnc_symbol\n\n    gtcalls = [\"<li>{}: {}</li>\".format(sample_obj['display_name'],\n                                        sample_obj['genotype_call'])\n               for sample_obj in variant_obj['samples']]\n    tx_changes = []\n\n    if category == 'snv': #SNV\n        view_type = 'variants.variant'\n        display_name = variant_obj.get('display_name')\n        tx_changes = []\n\n        for gene_obj in variant_obj.get('genes', []):\n            for tx_obj in gene_obj['transcripts']:\n                parse_transcript(gene_obj, tx_obj)\n                # select refseq transcripts as \"primary\"\n                if not tx_obj.get('refseq_id'):\n                    continue\n\n                for refseq_id in tx_obj.get('refseq_identifiers'):\n                    transcript_line = []\n                    if \"common\" in gene_obj:\n                        transcript_line.append(gene_obj['common']['hgnc_symbol'])\n                    else:\n                        transcript_line.append(gene_obj['hgnc_id'])\n                    transcript_line.append('-'.join([refseq_id, tx_obj['transcript_id']]))\n                    if \"exon\" in tx_obj:\n                        transcript_line.append(''.join([ \"exon\", tx_obj[\"exon\"]]))\n                    elif \"intron\" in tx_obj:\n                        transcript_line.append(''.join([ \"intron\", tx_obj[\"intron\"]]))\n                    else:\n                        transcript_line.append('intergenic')\n                    if \"coding_sequence_name\" in tx_obj:\n                        transcript_line.append(urllib.parse.unquote(tx_obj['coding_sequence_name']))\n                    else:\n                        transcript_line.append('')\n                    if \"protein_sequence_name\" in tx_obj:\n                        transcript_line.append(urllib.parse.unquote(tx_obj['protein_sequence_name']))\n                    else:\n                        transcript_line.append('')\n\n                    tx_changes.append(\"<li>{}</li>\".format(':'.join(transcript_line)))\n\n    else: #SV\n        view_type = 'variants.sv_variant'\n        display_name = '_'.join([breakpoint_1, variant_obj.get('sub_category').upper()])\n\n    # body of the email\n    html = verification_email_body(\n        case_name = case_obj['display_name'],\n        url = variant_url, #this is the complete url to the variant, accessible when clicking on the email link\n        display_name = display_name,\n        category = category.upper(),\n        subcategory = variant_obj.get('sub_category').upper(),\n        breakpoint_1 = breakpoint_1,\n        breakpoint_2 = breakpoint_2,\n        hgnc_symbol = hgnc_symbol,\n        panels = panels,\n        gtcalls = ''.join(gtcalls),\n        tx_changes = ''.join(tx_changes) or 'Not available',\n        name = user_obj['name'].encode('utf-8'),\n        comment = comment\n    )\n\n    # build a local the link to the variant to be included in the events objects (variant and case) created in the event collection.\n    local_link = url_builder(view_type, institute_id=institute_obj['_id'],\n                           case_name=case_obj['display_name'],\n                           variant_id=variant_obj['_id'])\n\n    if order == 'True': # variant verification should be ordered\n        # pin variant if it's not already pinned\n        if case_obj.get('suspects') is None or variant_obj['_id'] not in case_obj['suspects']:\n            store.pin_variant(institute_obj, case_obj, user_obj, local_link, variant_obj)\n\n        email_subject = \"SCOUT: validation of {} variant {}, ({})\".format( category.upper(), display_name, email_subj_gene_symbol)\n        store.order_verification(institute=institute_obj, case=case_obj, user=user_obj, link=local_link, variant=variant_obj)\n\n    else: # variant verification should be cancelled\n        email_subject = \"SCOUT: validation of {} variant {}, ({}), was CANCELLED!\".format(category.upper(), display_name, email_subj_gene_symbol)\n        store.cancel_verification(institute=institute_obj, case=case_obj, user=user_obj, link=local_link, variant=variant_obj)\n\n    kwargs = dict(subject=email_subject, html=html, sender=sender, recipients=recipients,\n        # cc the sender of the email for confirmation\n        cc=[user_obj['email']])\n\n    message = Message(**kwargs)\n    # send email using flask_mail\n    mail.send(message)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef verification_email_body(case_name, url, display_name, category, subcategory, breakpoint_1, breakpoint_2, hgnc_symbol, panels, gtcalls, tx_changes, name, comment):\n    html = \"\"\"\n       <ul>\n         <li>\n           <strong>Case {case_name}</strong>: <a href=\"{url}\">{display_name}</a>\n         </li>\n         <li><strong>Variant type</strong>: {category} ({subcategory})\n         <li><strong>Breakpoint 1</strong>: {breakpoint_1}</li>\n         <li><strong>Breakpoint 2</strong>: {breakpoint_2}</li>\n         <li><strong>HGNC symbols</strong>: {hgnc_symbol}</li>\n         <li><strong>Gene panels</strong>: {panels}</li>\n         <li><strong>GT call</strong></li>\n         {gtcalls}\n         <li><strong>Amino acid changes</strong></li>\n         {tx_changes}\n         <li><strong>Comment</strong>: {comment}</li>\n         <li><strong>Ordered by</strong>: {name}</li>\n       </ul>\n    \"\"\".format(\n        case_name=case_name,\n        url=url,\n        display_name=display_name,\n        category=category,\n        subcategory=subcategory,\n        breakpoint_1=breakpoint_1,\n        breakpoint_2=breakpoint_2,\n        hgnc_symbol=hgnc_symbol,\n        panels=panels,\n        gtcalls=gtcalls,\n        tx_changes=tx_changes,\n        name=name,\n        comment=comment)\n\n    return html", "response": "Builds the html body of the variant verification email"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cancer_variants(store, request_args, institute_id, case_name):\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    form = CancerFiltersForm(request_args)\n    variants_query = store.variants(case_obj['_id'], category='cancer', query=form.data).limit(50)\n    data = dict(\n        institute=institute_obj,\n        case=case_obj,\n        variants=(parse_variant(store, institute_obj, case_obj, variant, update=True) for\n                  variant in variants_query),\n        form=form,\n        variant_type=request_args.get('variant_type', 'clinical'),\n    )\n    return data", "response": "Fetch data related to cancer variants for a given institute and case."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clinvar_export(store, institute_id, case_name, variant_id):\n\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    pinned = [store.variant(variant_id) or variant_id for variant_id in\n                  case_obj.get('suspects', [])]\n    variant_obj = store.variant(variant_id)\n    return dict(\n        today = str(date.today()),\n        institute=institute_obj,\n        case=case_obj,\n        variant=variant_obj,\n        pinned_vars=pinned\n    )", "response": "Return a dictionary with all the required data for creating the clinvar submission form for the given institute and case."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_clinvar_submission(store, institute_id, case_name, variant_id, submission_id):\n\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    pinned = [store.variant(variant_id) or variant_id for variant_id in\n                  case_obj.get('suspects', [])]\n    variant_obj = store.variant(variant_id)\n    clinvar_submission_objs = store.clinvars(submission_id=submission_id)\n    return dict(\n        today = str(date.today()),\n        institute=institute_obj,\n        case=case_obj,\n        variant=variant_obj,\n        pinned_vars=pinned,\n        clinvars = clinvar_submission_objs\n    )", "response": "Returns a dictionary with all the data to display for a specific clinvar submission."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef variant_acmg(store, institute_id, case_name, variant_id):\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    variant_obj = store.variant(variant_id)\n    return dict(institute=institute_obj, case=case_obj, variant=variant_obj,\n                CRITERIA=ACMG_CRITERIA, ACMG_OPTIONS=ACMG_OPTIONS)", "response": "Collect data relevant for rendering ACMG classification form."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef variant_acmg_post(store, institute_id, case_name, variant_id, user_email, criteria):\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    variant_obj = store.variant(variant_id)\n    user_obj = store.user(user_email)\n    variant_link = url_for('variants.variant', institute_id=institute_id,\n                           case_name=case_name, variant_id=variant_id)\n    classification = store.submit_evaluation(\n        institute_obj=institute_obj,\n        case_obj=case_obj,\n        variant_obj=variant_obj,\n        user_obj=user_obj,\n        link=variant_link,\n        criteria=criteria,\n    )\n    return classification", "response": "Calculate an ACMG classification based on a list of criteria."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef evaluation(store, evaluation_obj):\n    evaluation_obj['institute'] = store.institute(evaluation_obj['institute_id'])\n    evaluation_obj['case'] = store.case(evaluation_obj['case_id'])\n    evaluation_obj['variant'] = store.variant(evaluation_obj['variant_specific'])\n    evaluation_obj['criteria'] = {criterion['term']: criterion for criterion in\n                                  evaluation_obj['criteria']}\n    evaluation_obj['classification'] = ACMG_COMPLETE_MAP[evaluation_obj['classification']]\n    return evaluation_obj", "response": "Fetch and fill - in evaluation object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef upload_panel(store, institute_id, case_name, stream):\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    raw_symbols = [line.strip().split('\\t')[0] for line in stream if\n                   line and not line.startswith('#')]\n    # check if supplied gene symbols exist\n    hgnc_symbols = []\n    for raw_symbol in raw_symbols:\n        if store.hgnc_genes(raw_symbol).count() == 0:\n            flash(\"HGNC symbol not found: {}\".format(raw_symbol), 'warning')\n        else:\n            hgnc_symbols.append(raw_symbol)\n    return hgnc_symbols", "response": "Parse out HGNC symbols from a stream."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating an Excel file containing all verified variants for each customer and save them to a temp_excel_dir.", "response": "def verified_excel_file(store, institute_list, temp_excel_dir):\n    \"\"\"Collect all verified variants in a list on institutes and save them to file\n\n    Args:\n        store(adapter.MongoAdapter)\n        institute_list(list): a list of institute ids\n        temp_excel_dir(os.Path): folder where the temp excel files are written to\n\n    Returns:\n        written_files(int): the number of files written to temp_excel_dir\n    \"\"\"\n    document_lines = []\n    written_files = 0\n    today = datetime.datetime.now().strftime('%Y-%m-%d')\n    LOG.info('Creating verified variant document..')\n\n    for cust in institute_list:\n        verif_vars = store.verified(institute_id=cust)\n        LOG.info('Found {} verified variants for customer {}'.format(len(verif_vars), cust))\n\n        if not verif_vars:\n            continue\n        unique_callers = set()\n        for var_type, var_callers in CALLERS.items():\n            for caller in var_callers:\n                unique_callers.add(caller.get('id'))\n        cust_verified = export_verified_variants(verif_vars, unique_callers)\n\n        document_name = '.'.join([cust, '_verified_variants', today]) + '.xlsx'\n        workbook = Workbook(os.path.join(temp_excel_dir,document_name))\n        Report_Sheet = workbook.add_worksheet()\n\n        # Write the column header\n        row = 0\n        for col,field in enumerate(VERIFIED_VARIANTS_HEADER + list(unique_callers)):\n            Report_Sheet.write(row,col,field)\n\n        # Write variant lines, after header (start at line 1)\n        for row, line in enumerate(cust_verified,1): # each line becomes a row in the document\n            for col, field in enumerate(line): # each field in line becomes a cell\n                Report_Sheet.write(row,col,field)\n        workbook.close()\n\n        if os.path.exists(os.path.join(temp_excel_dir,document_name)):\n            written_files += 1\n\n    return written_files"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild a HPO term object from a dictionary with the HPO information.", "response": "def build_hpo_term(hpo_info):\n    \"\"\"Build a hpo_term object\n    \n    Check that the information is correct and add the correct hgnc ids to the \n    array of genes.\n    \n        Args:\n            hpo_info(dict)\n        \n        Returns:\n            hpo_obj(scout.models.HpoTerm): A dictionary with hpo information\n    \n    \"\"\"\n    \n    try:\n        hpo_id = hpo_info['hpo_id']\n    except KeyError:\n        raise KeyError(\"Hpo terms has to have a hpo_id\")\n\n    LOG.debug(\"Building hpo term %s\", hpo_id)\n\n    # Add description to HPO term\n    try:\n        description = hpo_info['description']\n    except KeyError:\n        raise KeyError(\"Hpo terms has to have a description\")\n\n    hpo_obj = HpoTerm(\n        hpo_id = hpo_id,\n        description = description\n    )\n    \n    # Add links to hgnc genes if any\n    hgnc_ids = hpo_info.get('genes', set())\n    if hgnc_ids:\n        hpo_obj['genes'] = list(hgnc_ids)\n    \n    return hpo_obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef export_genes(adapter, build='37'):\n    LOG.info(\"Exporting all genes to .bed format\")\n    \n    for gene_obj in adapter.all_genes(build=build):\n        yield gene_obj", "response": "Export all genes from the database to. bed format"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the clnsig information from the vcf file.", "response": "def parse_clnsig(acc, sig, revstat, transcripts):\n    \"\"\"Get the clnsig information\n\n    Args:\n        acc(str): The clnsig accession number, raw from vcf\n        sig(str): The clnsig significance score, raw from vcf\n        revstat(str): The clnsig revstat, raw from vcf\n        transcripts(iterable(dict))\n\n    Returns:\n        clnsig_accsessions(list): A list with clnsig accessions\n    \"\"\"\n    clnsig_accsessions = []\n\n    if acc:\n    # New format of clinvar allways have integers as accession numbers\n        try:\n            acc = int(acc)\n        except ValueError:\n            pass\n        # There are sometimes different separators so we need to check which\n        # one to use\n        if isinstance(acc, int):\n            revstat_groups = []\n            if revstat:\n                revstat_groups = [rev.lstrip('_') for rev in revstat.split(',')]\n\n            sig_groups = []\n            if sig:\n                for significance in sig.split('/'):\n                    splitted_word = significance.split('_')\n                    sig_groups.append(' '.join(splitted_word[:2]))\n\n            for sign_term in sig_groups:\n                clnsig_accsessions.append({\n                    'value': sign_term,\n                    'accession': int(acc),\n                    'revstat': ', '.join(revstat_groups),\n                })\n        else:\n            # There are sometimes different separators so we need to check which\n            # one to use\n            acc_groups = acc.split('|')\n            sig_groups = sig.split('|')\n            revstat_groups = revstat.split('|')\n            for acc_group, sig_group, revstat_group in zip(acc_groups, sig_groups, revstat_groups):\n                accessions = acc_group.split(',')\n                significances = sig_group.split(',')\n                revstats = revstat_group.split(',')\n                for accession, significance, revstat in zip(accessions, significances, revstats):\n                    clnsig_accsessions.append({\n                        'value': int(significance),\n                        'accession': accession,\n                        'revstat': revstat,\n                    })\n\n    elif transcripts:\n        clnsig = set()\n        for transcript in transcripts:\n            for annotation in transcript.get('clinsig', []):\n                clnsig.add(annotation)\n        for annotation in clnsig:\n            clnsig_accsessions.append({'value': annotation})\n\n    return clnsig_accsessions"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nshows all indexes in the database", "response": "def index(context, collection_name):\n    \"\"\"Show all indexes in the database\"\"\"\n    LOG.info(\"Running scout view index\")\n    adapter = context.obj['adapter']\n\n    i = 0\n    click.echo(\"collection\\tindex\")\n    for collection_name in adapter.collections():\n        for index in adapter.indexes(collection_name):\n            click.echo(\"{0}\\t{1}\".format(collection_name, index))\n            i += 1\n\n    if i == 0:\n        LOG.info(\"No indexes found\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the phenotype for a single institute.", "response": "def groups(context, institute_id, phenotype_group, group_abbreviation, group_file, add):\n    \"\"\"\n    Update the phenotype for a institute.\n    If --add the groups will be added to the default groups. Else the groups will be replaced.\n    \"\"\"\n    adapter = context.obj['adapter']\n    LOG.info(\"Running scout update institute\")\n    if group_file:\n        phenotype_group = []\n        group_abbreviation = []\n        for line in group_file:\n            if line.startswith('#'):\n                continue\n            if len(line) < 7:\n                continue\n            line = line.rstrip().split('\\t')\n            phenotype_group.append(line[0])\n            if line[1]:\n                group_abbreviation.append(line[1])\n    \n    if not phenotype_group:\n        LOG.info(\"Please provide some groups\")\n        return\n\n    if (phenotype_group and group_abbreviation):\n        if not len(phenotype_group) == len(group_abbreviation):\n            LOG.warning(\"Specify same number of groups and abbreviations\")\n            return\n\n    # try:\n    adapter.update_institute(\n        internal_id=institute_id, \n        phenotype_groups=phenotype_group,\n        group_abbreviations=group_abbreviation,\n        add_groups = add,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_compounds(compound_info, case_id, variant_type):\n    # We need the case to construct the correct id\n    compounds = []\n    if compound_info:\n        for family_info in compound_info.split(','):\n            splitted_entry = family_info.split(':')\n            # This is the family id\n            if splitted_entry[0] == case_id:\n                for compound in splitted_entry[1].split('|'):\n                    splitted_compound = compound.split('>')\n                    compound_obj = {}\n                    compound_name = splitted_compound[0]\n                    compound_obj['variant'] = generate_md5_key(compound_name.split('_') +\n                                                               [variant_type, case_id])\n\n                    try:\n                        compound_score = float(splitted_compound[1])\n                    except (TypeError, IndexError):\n                        compound_score = 0.0\n\n                    compound_obj['score'] = compound_score\n                    compound_obj['display_name'] = compound_name\n\n                    compounds.append(compound_obj)\n\n    return compounds", "response": "Parse the compounds for this variant."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexports all genes from a build", "response": "def genes(context, build, json):\n    \"\"\"Export all genes from a build\"\"\"\n    LOG.info(\"Running scout export genes\")\n    adapter = context.obj['adapter']\n\n    result = adapter.all_genes(build=build)\n    if json:\n        click.echo(dumps(result))\n        return\n        \n    gene_string = (\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\")\n    click.echo(\"#Chromosom\\tStart\\tEnd\\tHgnc_id\\tHgnc_symbol\")\n    for gene_obj in result:\n        click.echo(gene_string.format(\n            gene_obj['chromosome'],\n            gene_obj['start'],\n            gene_obj['end'],\n            gene_obj['hgnc_id'],\n            gene_obj['hgnc_symbol'],\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_individual(ind):\n\n    try:\n        ind_obj = dict(\n            individual_id=ind['individual_id']\n        )\n        log.info(\"Building Individual with id:{0}\".format(ind['individual_id']))\n    except KeyError as err:\n        raise PedigreeError(\"Individual is missing individual_id\")\n\n    ind_obj['display_name'] = ind.get('display_name', ind_obj['individual_id'])\n\n    sex = ind.get('sex', 'unknown')\n    # Convert sex to .ped\n    try:\n        # Check if sex is coded as an integer\n        int(sex)\n        ind_obj['sex'] = str(sex)\n    except ValueError as err:\n        try:\n            # Sex are numbers in the database\n            ind_obj['sex'] = REV_SEX_MAP[sex]\n        except KeyError as err:\n            raise(PedigreeError(\"Unknown sex: %s\" % sex))\n\n    phenotype = ind.get('phenotype', 'unknown')\n    # Make the phenotype integers\n    try:\n        ped_phenotype = REV_PHENOTYPE_MAP[phenotype]\n        if ped_phenotype == -9:\n            ped_phenotype = 0\n        ind_obj['phenotype'] = ped_phenotype\n    except KeyError as err:\n        raise(PedigreeError(\"Unknown phenotype: %s\" % phenotype))\n\n    ind_obj['father'] = ind.get('father')\n    ind_obj['mother'] = ind.get('mother')\n    ind_obj['capture_kits'] = ind.get('capture_kits', [])\n    ind_obj['bam_file'] = ind.get('bam_file')\n    ind_obj['mt_bam'] = ind.get('mt_bam')\n    ind_obj['vcf2cytosure'] = ind.get('vcf2cytosure')\n    ind_obj['confirmed_sex'] = ind.get('confirmed_sex')\n    ind_obj['confirmed_parent'] = ind.get('confirmed_parent')\n    ind_obj['predicted_ancestry'] = ind.get('predicted_ancestry')\n\n    # Check if the analysis type is ok\n    # Can be anyone of ('wgs', 'wes', 'mixed', 'unknown')\n    analysis_type = ind.get('analysis_type', 'unknown')\n    if not analysis_type in ANALYSIS_TYPES:\n        raise PedigreeError(\"Analysis type %s not allowed\", analysis_type)\n    ind_obj['analysis_type'] = analysis_type\n    \n    if 'tmb' in ind:\n        ind_obj['tmb'] = ind['tmb']\n\n    if 'msi' in ind:\n        ind_obj['msi'] = ind['msi']\n\n    if 'tumor_purity' in ind:\n        ind_obj['tumor_purity'] = ind['tumor_purity']\n\n    if 'tumor_type' in ind:\n        ind_obj['tumor_type'] = ind['tumor_type']\n\n    return ind_obj", "response": "Build a single Individual object from a dictionary with individual information."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupload variants to a case.", "response": "def variants(context, case_id, institute, force, cancer, cancer_research, sv, \n             sv_research, snv, snv_research, str_clinical, chrom, start, end, hgnc_id, \n             hgnc_symbol, rank_treshold):\n    \"\"\"Upload variants to a case\n\n        Note that the files has to be linked with the case, \n        if they are not use 'scout update case'.\n    \"\"\"\n    LOG.info(\"Running scout load variants\")\n    adapter = context.obj['adapter']\n    \n    if institute:\n        case_id = \"{0}-{1}\".format(institute, case_id)\n    else:\n        institute = case_id.split('-')[0]\n    case_obj = adapter.case(case_id=case_id)\n    if case_obj is None:\n        LOG.info(\"No matching case found\")\n        context.abort()\n\n    files = [\n        {'category': 'cancer', 'variant_type': 'clinical', 'upload': cancer},\n        {'category': 'cancer', 'variant_type': 'research', 'upload': cancer_research},\n        {'category': 'sv', 'variant_type': 'clinical', 'upload': sv},\n        {'category': 'sv', 'variant_type': 'research', 'upload': sv_research},\n        {'category': 'snv', 'variant_type': 'clinical', 'upload': snv},\n        {'category': 'snv', 'variant_type': 'research', 'upload': snv_research},\n        {'category': 'str', 'variant_type': 'clinical', 'upload': str_clinical},\n    ]\n    \n    gene_obj = None\n    if (hgnc_id or hgnc_symbol):\n        if hgnc_id:\n            gene_obj = adapter.hgnc_gene(hgnc_id)\n        if hgnc_symbol:\n            for res in adapter.gene_by_alias(hgnc_symbol):\n                gene_obj = res\n        if not gene_obj:\n            LOG.warning(\"The gene could not be found\")\n            context.abort()\n        \n    i = 0\n    for file_type in files:\n        variant_type = file_type['variant_type']\n        category = file_type['category']\n        \n        if file_type['upload']:\n            i += 1\n            if variant_type == 'research':\n                if not (force or case_obj['research_requested']):\n                    LOG.warn(\"research not requested, use '--force'\")\n                    context.abort()\n            \n            LOG.info(\"Delete {0} {1} variants for case {2}\".format(\n                         variant_type, category, case_id))\n            adapter.delete_variants(case_id=case_obj['_id'], \n                                    variant_type=variant_type,\n                                    category=category)\n            \n            LOG.info(\"Load {0} {1} variants for case {2}\".format(\n                         variant_type, category, case_id))\n            \n            try:\n                adapter.load_variants(\n                    case_obj=case_obj, \n                    variant_type=variant_type, \n                    category=category,\n                    rank_threshold=rank_treshold, \n                    chrom=chrom, \n                    start=start, \n                    end=end,\n                    gene_obj=gene_obj\n                )\n            except Exception as e:\n                LOG.warning(e)\n                context.abort()\n    if i == 0:\n        LOG.info(\"No files where specified to upload variants from\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef variant(institute_id, case_name, variant_id):\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    variant_obj = store.variant(variant_id)\n    return Response(json_util.dumps(variant_obj), mimetype='application/json')", "response": "Display a specific SNV variant."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef collections(context):\n    LOG.info(\"Running scout view collections\")\n\n    adapter = context.obj['adapter']\n\n    for collection_name in adapter.collections():\n        click.echo(collection_name)", "response": "Show all collections in the database"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef institute(ctx, internal_id, display_name, sanger_recipients):\n    adapter = ctx.obj['adapter']\n\n    if not internal_id:\n        logger.warning(\"A institute has to have an internal id\")\n        ctx.abort()\n\n    if not display_name:\n        display_name = internal_id\n\n    if sanger_recipients:\n        sanger_recipients = list(sanger_recipients)\n\n    try:\n        load_institute(\n            adapter=adapter,\n            internal_id=internal_id,\n            display_name=display_name,\n            sanger_recipients=sanger_recipients\n        )\n    except Exception as e:\n        logger.warning(e)\n        ctx.abort()", "response": "Create a new institute and add it to the database"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_file_handle(file_path):\n\n    if file_path.endswith('.gz'):\n        file_handle = getreader('utf-8')(gzip.open(file_path, 'r'), errors='replace')\n    else:\n        file_handle = open(file_path, 'r', encoding='utf-8')\n\n    return file_handle", "response": "Return a opened file handle"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _inc_day(year, month, day, net):\n    d = date(year, month, day)\n    new_d = d + timezone.timedelta(days=net)\n    return new_d.year, new_d.month, new_d.day", "response": "Increments the day by converting to a datetime. date object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the net of any next and prev querystrings.", "response": "def get_net(req):\n    \"\"\"Get the net of any 'next' and 'prev' querystrings.\"\"\"\n    try:\n        nxt, prev = map(\n            int, (req.GET.get('cal_next', 0), req.GET.get('cal_prev', 0))\n        )\n        net = nxt - prev\n    except Exception:\n        net = 0\n    return net"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of tuples that looks like by title then sort them alphabetically by day.", "response": "def order_events(events, d=False):\n    \"\"\"\n    Group events that occur on the same day, then sort them alphabetically\n    by title, then sort by day. Returns a list of tuples that looks like\n    [(day: [events])], where day is the day of the event(s), and [events]\n    is an alphabetically sorted list of the events for the day.\n    \"\"\"\n    ordered_events = {}\n    for event in events:\n        try:\n            for occ in event.occurrence:\n                try:\n                    ordered_events[occ].append(event)\n                except Exception:\n                    ordered_events[occ] = [event]\n        except AttributeError:  # no occurrence for this event\n            # This shouldn't happen, since an event w/o an occurrence\n            # shouldn't get this far, but if it does, just skip it since\n            # it shouldn't be displayed on the calendar anyway.\n            pass\n\n    if d:\n        # return as a dict without sorting by date\n        return ordered_events\n    else:\n        # return ordered_events as a list tuples sorted by date\n        return sorted(ordered_events.items())"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_next_and_prev(net):\n    if net == 0:\n        nxt = prev = 1\n    elif net > 0:\n        nxt = net + 1\n        prev = -(net - 1)\n    else:\n        nxt = net + 1\n        prev = abs(net) + 1\n    return nxt, prev", "response": "Returns what the next and prev querystrings should be."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking that the year is within 50 years from now.", "response": "def _check_year(year, month, error, error_msg):\n    \"\"\"Checks that the year is within 50 years from now.\"\"\"\n    if year not in xrange((now.year - 50), (now.year + 51)):\n        year = now.year\n        month = now.month\n        error = error_msg\n    return year, month, error"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_weekday(year, month, day, reverse=False):\n    d = date(year, month, day)\n    while d.weekday() in (5, 6):\n        if reverse:\n            d -= timedelta(days=1)\n        else:\n            d += timedelta(days=1)\n    return d.year, d.month, d.day", "response": "Check if any event day we send back for weekday repeating\n    events is not a weekend."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_case_data(config=None, ped=None, owner=None, vcf_snv=None,\n                    vcf_sv=None, vcf_cancer=None, vcf_str=None, peddy_ped=None,\n                    peddy_sex=None, peddy_check=None, delivery_report=None, multiqc=None):\n    \"\"\"Parse all data necessary for loading a case into scout\n\n    This can be done either by providing a VCF file and other information\n    on the command line. Or all the information can be specified in a config file.\n    Please see Scout documentation for further instructions.\n\n    Args:\n        config(dict): A yaml formatted config file\n        ped(iterable(str)): A ped formatted family file\n        owner(str): The institute that owns a case\n        vcf_snv(str): Path to a vcf file\n        vcf_str(str): Path to a VCF file\n        vcf_sv(str): Path to a vcf file\n        vcf_cancer(str): Path to a vcf file\n        peddy_ped(str): Path to a peddy ped\n        multiqc(str): Path to dir with multiqc information\n\n    Returns:\n        config_data(dict): Holds all the necessary information for loading\n                           Scout\n    \"\"\"\n    config_data = copy.deepcopy(config) or {}\n    # Default the analysis date to now if not specified in load config\n    if 'analysis_date' not in config_data:\n        config_data['analysis_date'] = datetime.datetime.now()\n\n    # If the family information is in a ped file we nned to parse that\n    if ped:\n        family_id, samples = parse_ped(ped)\n        config_data['family'] = family_id\n        config_data['samples'] = samples\n\n    # Each case has to have a owner. If not provided in config file it needs to be given as a\n    # argument\n    if 'owner' not in config_data:\n        if not owner:\n            raise SyntaxError(\"Case has no owner\")\n        else:\n            config_data['owner'] = owner\n\n    if 'gene_panels' in config_data:\n        # handle whitespace in gene panel names\n        config_data['gene_panels'] = [panel.strip() for panel in\n                                      config_data['gene_panels']]\n        config_data['default_gene_panels'] = [panel.strip() for panel in\n                                              config_data['default_gene_panels']]\n\n    ##################### Add information from peddy if existing #####################\n    config_data['peddy_ped'] = peddy_ped or config_data.get('peddy_ped')\n    config_data['peddy_sex_check'] = peddy_sex or config_data.get('peddy_sex')\n    config_data['peddy_ped_check'] = peddy_check or config_data.get('peddy_check')\n\n    # This will add information from peddy to the individuals\n    add_peddy_information(config_data)\n\n    ##################### Add multiqc information #####################\n    config_data['multiqc'] = multiqc or config_data.get('multiqc')\n\n    config_data['vcf_snv'] = vcf_snv if vcf_snv else config_data.get('vcf_snv')\n    config_data['vcf_sv'] = vcf_sv if vcf_sv else config_data.get('vcf_sv')\n    config_data['vcf_str'] = vcf_str if vcf_str else config_data.get('vcf_str')\n    log.debug(\"Config vcf_str set to {0}\".format(config_data['vcf_str']))\n\n    config_data['vcf_cancer'] = vcf_cancer if vcf_cancer else config_data.get('vcf_cancer')\n\n    config_data['delivery_report'] = delivery_report if delivery_report else config_data.get('delivery_report')\n\n    config_data['rank_model_version'] = config_data.get('rank_model_version')\n    config_data['rank_score_threshold'] = config_data.get('rank_score_threshold', 0)\n\n    config_data['track'] = config_data.get('track', 'rare')\n    if config_data['vcf_cancer']:\n        config_data['track'] = 'cancer'\n\n    return config_data", "response": "Parse all the data needed for loading a case into a Scout object"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_peddy_information(config_data):\n    ped_info = {}\n    ped_check = {}\n    sex_check = {}\n    relations = []\n\n    if config_data.get('peddy_ped'):\n        file_handle = open(config_data['peddy_ped'], 'r')\n        for ind_info in parse_peddy_ped(file_handle):\n            ped_info[ind_info['sample_id']] = ind_info\n\n    if config_data.get('peddy_ped_check'):\n        file_handle = open(config_data['peddy_ped_check'], 'r')\n        for pair_info in parse_peddy_ped_check(file_handle):\n            ped_check[(pair_info['sample_a'], pair_info['sample_b'])] = pair_info\n\n    if config_data.get('peddy_sex_check'):\n        file_handle = open(config_data['peddy_sex_check'], 'r')\n        for ind_info in parse_peddy_sex_check(file_handle):\n            sex_check[ind_info['sample_id']] = ind_info\n\n    if not ped_info:\n        return\n\n    analysis_inds = {}\n    for ind in config_data['samples']:\n        ind_id = ind['sample_id']\n        analysis_inds[ind_id] = ind\n\n    for ind_id in analysis_inds:\n        ind = analysis_inds[ind_id]\n        # Check if peddy has inferred the ancestry\n        if ind_id in ped_info:\n            ind['predicted_ancestry'] = ped_info[ind_id].get('ancestry-prediction', 'UNKNOWN')\n        # Check if peddy has inferred the sex\n        if ind_id in sex_check:\n            if sex_check[ind_id]['error']:\n                ind['confirmed_sex'] = False\n            else:\n                ind['confirmed_sex'] = True\n\n        # Check if peddy har confirmed parental relations\n        for parent in ['mother', 'father']:\n            # If we are looking at individual with parents\n            if ind[parent] != '0':\n                # Check if the child/parent pair is in peddy data\n                for pair in ped_check:\n                    if (ind_id in pair and ind[parent] in pair):\n                        # If there is a parent error we mark that\n                        if ped_check[pair]['parent_error']:\n                            analysis_inds[ind[parent]]['confirmed_parent'] = False\n                        else:\n                            # Else if parent confirmation has not been done\n                            if 'confirmed_parent' not in analysis_inds[ind[parent]]:\n                                # Set confirmatio to True\n                                analysis_inds[ind[parent]]['confirmed_parent'] = True", "response": "Add information from PEDY outfiles to the individuals"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_individual(sample):\n    ind_info = {}\n    if 'sample_id' not in sample:\n        raise PedigreeError(\"One sample is missing 'sample_id'\")\n    sample_id = sample['sample_id']\n    # Check the sex\n    if 'sex' not in sample:\n        raise PedigreeError(\"Sample %s is missing 'sex'\" % sample_id)\n    sex = sample['sex']\n    if sex not in REV_SEX_MAP:\n        log.warning(\"'sex' is only allowed to have values from {}\"\n                    .format(', '.join(list(REV_SEX_MAP.keys()))))\n        raise PedigreeError(\"Individual %s has wrong formated sex\" % sample_id)\n\n    # Check the phenotype\n    if 'phenotype' not in sample:\n        raise PedigreeError(\"Sample %s is missing 'phenotype'\"\n                            % sample_id)\n    phenotype = sample['phenotype']\n    if phenotype not in REV_PHENOTYPE_MAP:\n        log.warning(\"'phenotype' is only allowed to have values from {}\"\n                    .format(', '.join(list(REV_PHENOTYPE_MAP.keys()))))\n        raise PedigreeError(\"Individual %s has wrong formated phenotype\" % sample_id)\n\n    ind_info['individual_id'] = sample_id\n    ind_info['display_name'] = sample.get('sample_name', sample['sample_id'])\n\n    ind_info['sex'] = sex\n    ind_info['phenotype'] = phenotype\n\n    ind_info['father'] = sample.get('father')\n    ind_info['mother'] = sample.get('mother')\n\n    ind_info['confirmed_parent'] = sample.get('confirmed_parent')\n    ind_info['confirmed_sex'] = sample.get('confirmed_sex')\n    ind_info['predicted_ancestry'] = sample.get('predicted_ancestry')\n\n    bam_file = sample.get('bam_path')\n    if bam_file:\n        ind_info['bam_file'] = bam_file\n\n    mt_bam = sample.get('mt_bam')\n    if mt_bam:\n        ind_info['mt_bam'] = mt_bam\n\n    analysis_type = sample.get('analysis_type')\n    if analysis_type:\n        ind_info['analysis_type'] = analysis_type\n\n    ind_info['capture_kits'] = ([sample.get('capture_kit')]\n                                if 'capture_kit' in sample else [])\n\n    # Path to downloadable vcf2cytosure file\n    vcf2cytosure = sample.get('vcf2cytosure')\n    if vcf2cytosure:\n        ind_info['vcf2cytosure'] = vcf2cytosure\n\n    # Cancer specific values\n    tumor_type = sample.get('tumor_type')\n    if tumor_type:\n        ind_info['tumor_type'] = tumor_type\n\n    tumor_mutational_burden = sample.get('tmb')\n    if tumor_mutational_burden:\n        ind_info['tmb'] = tumor_mutational_burden\n\n    msi = sample.get('msi')\n    if msi:\n        ind_info['msi'] = msi\n\n    tumor_purity = sample.get('tumor_purity')\n    if tumor_purity:\n        ind_info['tumor_purity'] = tumor_purity\n\n    return ind_info", "response": "Parse the individual information from a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_individuals(samples):\n    individuals = []\n    if len(samples) == 0:\n        raise PedigreeError(\"No samples could be found\")\n\n    ind_ids = set()\n    for sample_info in samples:\n        parsed_ind = parse_individual(sample_info)\n        individuals.append(parsed_ind)\n        ind_ids.add(parsed_ind['individual_id'])\n\n    # Check if relations are correct\n    for parsed_ind in individuals:\n        father = parsed_ind['father']\n        if (father and father != '0'):\n            if father not in ind_ids:\n                raise PedigreeError('father %s does not exist in family' % father)\n        mother = parsed_ind['mother']\n        if (mother and mother != '0'):\n            if mother not in ind_ids:\n                raise PedigreeError('mother %s does not exist in family' % mother)\n\n    return individuals", "response": "Parse the individual information to proper individuals\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_case(config):\n    if 'owner' not in config:\n        raise ConfigError(\"A case has to have a owner\")\n\n    if 'family' not in config:\n        raise ConfigError(\"A case has to have a 'family'\")\n\n    individuals = parse_individuals(config['samples'])\n    case_data = {\n        'owner': config['owner'],\n        'collaborators': [config['owner']],\n        'case_id': config['family'],\n        'display_name': config.get('family_name', config['family']),\n        'genome_build': config.get('human_genome_build'),\n        'rank_model_version': config.get('rank_model_version'),\n        'rank_score_threshold': config.get('rank_score_threshold', 0),\n        'analysis_date': config['analysis_date'],\n        'individuals': individuals,\n        'vcf_files': {\n            'vcf_snv': config.get('vcf_snv'),\n            'vcf_sv': config.get('vcf_sv'),\n            'vcf_str': config.get('vcf_str'),\n            'vcf_cancer': config.get('vcf_cancer'),\n            'vcf_snv_research': config.get('vcf_snv_research'),\n            'vcf_sv_research': config.get('vcf_sv_research'),\n            'vcf_cancer_research': config.get('vcf_cancer_research'),\n        },\n        'default_panels': config.get('default_gene_panels', []),\n        'gene_panels': config.get('gene_panels', []),\n        'assignee': config.get('assignee'),\n        'peddy_ped': config.get('peddy_ped'),\n        'peddy_sex': config.get('peddy_sex'),\n        'peddy_check': config.get('peddy_check'),\n        'delivery_report': config.get('delivery_report'),\n        'multiqc': config.get('multiqc'),\n        'track': config.get('track', 'rare'),\n    }\n\n    # add the pedigree figure, this is a xml file which is dumped in the db\n    if 'madeline' in config:\n        mad_path = Path(config['madeline'])\n        if not mad_path.exists():\n            raise ValueError(\"madeline path not found: {}\".format(mad_path))\n        with mad_path.open('r') as in_handle:\n            case_data['madeline_info'] = in_handle.read()\n    \n    if (case_data['vcf_files']['vcf_cancer'] or case_data['vcf_files']['vcf_cancer_research']):\n        case_data['track'] = 'cancer'\n    \n    return case_data", "response": "Parse a single PED case from a config dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_ped(ped_stream, family_type='ped'):\n    pedigree = FamilyParser(ped_stream, family_type=family_type)\n\n    if len(pedigree.families) != 1:\n        raise PedigreeError(\"Only one case per ped file is allowed\")\n\n    family_id = list(pedigree.families.keys())[0]\n    family = pedigree.families[family_id]\n\n    samples = [{\n        'sample_id': ind_id,\n        'father': individual.father,\n        'mother': individual.mother,\n        # Convert sex to human readable\n        'sex': SEX_MAP[individual.sex],\n        'phenotype': PHENOTYPE_MAP[int(individual.phenotype)],\n    } for ind_id, individual in family.individuals.items()]\n\n    return family_id, samples", "response": "Parse out minimal family information from a PED file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds an ACMG evaluation object ready to be inserted into the database.", "response": "def build_evaluation(variant_specific, variant_id, user_id, user_name,\n                     institute_id, case_id, classification, criteria):\n    \"\"\"Build a evaluation object ready to be inserted to database\n\n    Args:\n        variant_specific(str): md5 string for the specific variant\n        variant_id(str): md5 string for the common variant\n        user_id(str)\n        user_name(str)\n        institute_id(str)\n        case_id(str)\n        classification(str): The ACMG classification\n        criteria(list(dict)): A list of dictionaries with ACMG criterias\n\n    Returns:\n        evaluation_obj(dict): Correctly formatted evaluation object\n\n    \"\"\"\n    criteria = criteria or []\n    evaluation_obj = dict(\n        variant_specific = variant_specific,\n        variant_id = variant_id,\n        institute_id = institute_id,\n        case_id = case_id,\n        classification = classification,\n        user_id = user_id,\n        user_name = user_name,\n        created_at = datetime.datetime.now(),\n    )\n    criteria_objs = []\n    for info in criteria:\n        criteria_obj = {}\n        # This allways has to exist\n        # We might want to check if the term is valid here...\n        criteria_obj['term'] = info['term']\n        if 'comment' in info:\n            criteria_obj['comment'] = info['comment']\n        if 'links' in info:\n            criteria_obj['links'] = info['links']\n        criteria_objs.append(criteria_obj)\n\n    evaluation_obj['criteria'] = criteria_objs\n\n    return evaluation_obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mt_report(context, case_id, test, outpath=None):\n    LOG.info('exporting mitochondrial variants for case \"{}\"'.format(case_id))\n\n    adapter = context.obj['adapter']\n    query = {'chrom':'MT'}\n\n    case_obj = adapter.case(case_id=case_id)\n\n    if not case_obj:\n        LOG.warning('Could not find a scout case with id \"{}\". No report was created.'.format(case_id))\n        context.abort()\n\n    samples = case_obj.get('individuals')\n    mt_variants = list(adapter.variants(case_id=case_id, query=query, nr_of_variants= -1, sort_key='position'))\n    if not mt_variants:\n        LOG.warning('There are no MT variants associated to case {} in database!'.format(case_id))\n        context.abort()\n\n    today = datetime.datetime.now().strftime('%Y-%m-%d')\n\n    # set up outfolder\n    if not outpath:\n        outpath = str(os.getcwd())\n\n    # get document lines for each of the cases's individuals\n    # Write excel document for each sample in case\n    written_files = 0\n\n    for sample in samples:\n        sample_id = sample['individual_id']\n        sample_lines = export_mt_variants(variants=mt_variants, sample_id=sample_id)\n\n        # set up document name\n        document_name = '.'.join([case_obj['display_name'], sample_id, today]) + '.xlsx'\n        workbook = Workbook(os.path.join(outpath,document_name))\n        Report_Sheet = workbook.add_worksheet()\n\n        if test and sample_lines and workbook:\n            written_files +=1\n            continue\n\n        # Write the column header\n        row = 0\n        for col,field in enumerate(MT_EXPORT_HEADER):\n            Report_Sheet.write(row,col,field)\n\n        # Write variant lines, after header (start at line 1)\n        for row, line in enumerate(sample_lines,1): # each line becomes a row in the document\n            for col, field in enumerate(line): # each field in line becomes a cell\n                Report_Sheet.write(row,col,field)\n        workbook.close()\n\n        if os.path.exists(os.path.join(outpath,document_name)):\n            written_files += 1\n\n    if test:\n        LOG.info(\"Number of excel files that can be written to folder {0}: {1}\".format(outpath, written_files))\n    else:\n        LOG.info(\"Number of excel files written to folder {0}: {1}\".format(outpath, written_files))\n    return written_files", "response": "Export all Mitochondrial variants for each sample of a case and write them to an Excel file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build_genotype(gt_call):\n    gt_obj = dict(\n        sample_id = gt_call['individual_id'],\n        display_name = gt_call['display_name'],\n        genotype_call = gt_call['genotype_call'],\n        allele_depths = [gt_call['ref_depth'], gt_call['alt_depth']],\n        read_depth = gt_call['read_depth'],\n        genotype_quality = gt_call['genotype_quality']\n    )\n    \n    return gt_obj", "response": "Builds a genotype call\n    \n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if the criterias for Pathogenic are fullfilled and returns True if the criterias for Pathogenic are fullfilled.", "response": "def is_pathogenic(pvs, ps_terms, pm_terms, pp_terms):\n    \"\"\"Check if the criterias for Pathogenic is fullfilled\n\n    The following are descriptions of Pathogenic clasification from ACMG paper:\n\n    Pathogenic\n      (i) 1 Very strong (PVS1) AND\n        (a) \u22651 Strong (PS1\u2013PS4) OR\n        (b) \u22652 Moderate (PM1\u2013PM6) OR\n        (c) 1 Moderate (PM1\u2013PM6) and 1 supporting (PP1\u2013PP5) OR\n        (d) \u22652 Supporting (PP1\u2013PP5)\n      (ii) \u22652 Strong (PS1\u2013PS4) OR\n      (iii) 1 Strong (PS1\u2013PS4) AND\n        (a)\u22653 Moderate (PM1\u2013PM6) OR\n        (b)2 Moderate (PM1\u2013PM6) AND \u22652 Supporting (PP1\u2013PP5) OR\n        (c)1 Moderate (PM1\u2013PM6) AND \u22654 supporting (PP1\u2013PP5)\n\n    Args:\n        pvs(bool): Pathogenic Very Strong\n        ps_terms(list(str)): Pathogenic Strong terms\n        pm_terms(list(str)): Pathogenic Moderate terms\n        pp_terms(list(str)): Pathogenic Supporting terms\n\n    Returns:\n        bool: if classification indicates Pathogenic level\n    \"\"\"\n    if pvs:\n        # Pathogenic (i)(a):\n        if ps_terms:\n            return True\n        if pm_terms:\n            # Pathogenic (i)(c):\n            if pp_terms:\n                return True\n            # Pathogenic (i)(b):\n            if len(pm_terms) >= 2:\n                return True\n        # Pathogenic (i)(d):\n        if len(pp_terms) >= 2:\n            return True\n    if ps_terms:\n        # Pathogenic (ii):\n        if len(ps_terms) >= 2:\n            return True\n        # Pathogenic (iii)(a):\n        if pm_terms:\n            if len(pm_terms) >= 3:\n                return True\n            elif len(pm_terms) >= 2:\n                if len(pp_terms) >= 2:\n                    return True\n            elif len(pp_terms) >= 4:\n                return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_likely_pathogenic(pvs, ps_terms, pm_terms, pp_terms):\n    if pvs:\n        # Likely Pathogenic (i):\n        if pm_terms:\n            return True\n\n    if ps_terms:\n        # Likely Pathogenic (ii):\n        if pm_terms:\n            return True\n        # Likely Pathogenic (iii):\n        if len(pp_terms) >= 2:\n            return True\n\n    if pm_terms:\n        # Likely Pathogenic (iv):\n        if len(pm_terms) >= 3:\n            return True\n        # Likely Pathogenic (v):\n        elif len(pm_terms) >= 2:\n            if len(pp_terms) >= 2:\n                return True\n        # Likely Pathogenic (vi):\n        elif len(pp_terms) >= 4:\n            return True\n\n    return False", "response": "Checks if the criterias for Likely Pathogenic are fullfilled and returns True if the criterias for Likely Pathogenic are present."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if the criterias for Likely Benign are fullfilled and returns True if it is", "response": "def is_likely_benign(bs_terms, bp_terms):\n    \"\"\"Check if criterias for Likely Benign are fullfilled\n\n    The following are descriptions of Likely Benign clasification from ACMG paper:\n\n    Likely Benign\n      (i) 1 Strong (BS1\u2013BS4) and 1 supporting (BP1\u2013 BP7) OR\n      (ii) \u22652 Supporting (BP1\u2013BP7)\n\n    Args:\n        bs_terms(list(str)): Terms that indicate strong evidence for benign variant\n        bp_terms(list(str)): Terms that indicate supporting evidence for benign variant\n\n    Returns:\n        bool: if classification indicates Benign level\n    \"\"\"\n    if bs_terms:\n        # Likely Benign (i)\n        if bp_terms:\n            return True\n    # Likely Benign (ii)\n    if len(bp_terms) >= 2:\n        return True\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_acmg(acmg_terms):\n    prediction = 'uncertain_significance'\n    # This variable indicates if Pathogenecity Very Strong exists\n    pvs = False\n    # Collection of terms with Pathogenecity Strong\n    ps_terms = []\n    # Collection of terms with Pathogenecity moderate\n    pm_terms = []\n    # Collection of terms with Pathogenecity supporting\n    pp_terms = []\n    # This variable indicates if Benign impact stand-alone exists\n    ba = False\n    # Collection of terms with Benign evidence Strong\n    bs_terms = []\n    # Collection of terms with supporting Benign evidence\n    bp_terms = []\n    for term in acmg_terms:\n        if term.startswith('PVS'):\n            pvs = True\n        elif term.startswith('PS'):\n            ps_terms.append(term)\n        elif term.startswith('PM'):\n            pm_terms.append(term)\n        elif term.startswith('PP'):\n            pp_terms.append(term)\n        elif term.startswith('BA'):\n            ba = True\n        elif term.startswith('BS'):\n            bs_terms.append(term)\n        elif term.startswith('BP'):\n            bp_terms.append(term)\n\n    # We need to start by checking for Pathogenecity\n    pathogenic = is_pathogenic(pvs, ps_terms, pm_terms, pp_terms)\n    likely_pathogenic = is_likely_pathogenic(pvs, ps_terms, pm_terms, pp_terms)\n    benign = is_benign(ba, bs_terms)\n    likely_benign = is_likely_benign(bs_terms, bp_terms)\n\n    if (pathogenic or likely_pathogenic):\n        if (benign or likely_benign):\n            prediction = 'uncertain_significance'\n        elif pathogenic:\n            prediction = 'pathogenic'\n        else:\n            prediction = 'likely_pathogenic'\n    else:\n        if benign:\n            prediction = 'benign'\n        if likely_benign:\n            prediction = 'likely_benign'\n\n    return prediction", "response": "This function returns the ACMG calssification for the given set of acmg terms."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds extra information about genes from the database to the variant object.", "response": "def add_gene_info(self, variant_obj, gene_panels=None):\n        \"\"\"Add extra information about genes from gene panels\n\n        Args:\n            variant_obj(dict): A variant from the database\n            gene_panels(list(dict)): List of panels from database\n        \"\"\"\n        gene_panels = gene_panels or []\n\n        # Add a variable that checks if there are any refseq transcripts\n        variant_obj['has_refseq'] = False\n\n        # We need to check if there are any additional information in the gene panels\n\n        # extra_info will hold information from gene panels\n        # Collect all extra info from the panels in a dictionary with hgnc_id as keys\n        extra_info = {}\n        for panel_obj in gene_panels:\n            for gene_info in panel_obj['genes']:\n                hgnc_id = gene_info['hgnc_id']\n                if hgnc_id not in extra_info:\n                    extra_info[hgnc_id] = []\n\n                extra_info[hgnc_id].append(gene_info)\n\n        # Loop over the genes in the variant object to add information\n        # from hgnc_genes and panel genes to the variant object\n        for variant_gene in variant_obj.get('genes', []):\n            hgnc_id = variant_gene['hgnc_id']\n            # Get the hgnc_gene\n            hgnc_gene = self.hgnc_gene(hgnc_id)\n\n            if not hgnc_gene:\n                continue\n\n            # Create a dictionary with transcripts information\n            # Use ensembl transcript id as keys\n            transcripts_dict = {}\n            # Add transcript information from the hgnc gene\n            for transcript in hgnc_gene.get('transcripts', []):\n                tx_id = transcript['ensembl_transcript_id']\n                transcripts_dict[tx_id] = transcript\n\n            # Add the transcripts to the gene object\n            hgnc_gene['transcripts_dict'] = transcripts_dict\n\n            if hgnc_gene.get('incomplete_penetrance'):\n                variant_gene['omim_penetrance'] = True\n\n            ############# PANEL SPECIFIC INFORMATION #############\n            # Panels can have extra information about genes and transcripts\n            panel_info = extra_info.get(hgnc_id, [])\n\n            # Manually annotated disease associated transcripts\n            disease_associated = set()\n            # We need to strip the version to compare against others\n            disease_associated_no_version = set()\n            manual_penetrance = False\n            mosaicism = False\n            manual_inheritance = set()\n\n            # We need to loop since there can be information from multiple panels\n            for gene_info in panel_info:\n                # Check if there are manually annotated disease transcripts\n                for tx in gene_info.get('disease_associated_transcripts', []):\n                    # We remove the version of transcript at this stage\n                    stripped = re.sub(r'\\.[0-9]', '', tx)\n                    disease_associated_no_version.add(stripped)\n                    disease_associated.add(tx)\n\n                if gene_info.get('reduced_penetrance'):\n                    manual_penetrance = True\n\n                if gene_info.get('mosaicism'):\n                    mosaicism = True\n\n                manual_inheritance.update(gene_info.get('inheritance_models', []))\n\n            variant_gene['disease_associated_transcripts'] = list(disease_associated)\n            variant_gene['manual_penetrance'] = manual_penetrance\n            variant_gene['mosaicism'] = mosaicism\n            variant_gene['manual_inheritance'] = list(manual_inheritance)\n\n            # Now add the information from hgnc and panels\n            # to the transcripts on the variant\n\n            # First loop over the variants transcripts\n            for transcript in variant_gene.get('transcripts', []):\n                tx_id = transcript['transcript_id']\n                if not tx_id in transcripts_dict:\n                    continue\n\n                # This is the common information about the transcript\n                hgnc_transcript = transcripts_dict[tx_id]\n\n                # Check in the common information if it is a primary transcript\n                if hgnc_transcript.get('is_primary'):\n                    transcript['is_primary'] = True\n                # If the transcript has a ref seq identifier we add that\n                # to the variants transcript\n                if not hgnc_transcript.get('refseq_id'):\n                    continue\n\n                refseq_id = hgnc_transcript['refseq_id']\n                transcript['refseq_id'] = refseq_id\n                variant_obj['has_refseq'] = True\n                # Check if the refseq id are disease associated\n                if refseq_id in disease_associated_no_version:\n                    transcript['is_disease_associated'] = True\n\n                # Since a ensemble transcript can have multiple refseq identifiers we add all of\n                # those\n                transcript['refseq_identifiers'] = hgnc_transcript.get('refseq_identifiers',[])\n\n            variant_gene['common'] = hgnc_gene\n            # Add the associated disease terms\n            variant_gene['disease_terms'] = self.disease_terms(hgnc_id)\n\n        return variant_obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef variants(self, case_id, query=None, variant_ids=None, category='snv',\n                 nr_of_variants=10, skip=0, sort_key='variant_rank'):\n        \"\"\"Returns variants specified in question for a specific case.\n\n        If skip not equal to 0 skip the first n variants.\n\n        Arguments:\n            case_id(str): A string that represents the case\n            query(dict): A dictionary with querys for the database\n            variant_ids(List[str])\n            category(str): 'sv', 'str', 'snv' or 'cancer'\n            nr_of_variants(int): if -1 return all variants\n            skip(int): How many variants to skip\n            sort_key: ['variant_rank', 'rank_score', 'position']\n\n        Yields:\n            result(Iterable[Variant])\n        \"\"\"\n        LOG.debug(\"Fetching variants from {0}\".format(case_id))\n\n        if variant_ids:\n            nr_of_variants = len(variant_ids)\n\n        elif nr_of_variants == -1:\n            nr_of_variants = 0 # This will return all variants\n\n        else:\n            nr_of_variants = skip + nr_of_variants\n\n        mongo_query = self.build_query(case_id, query=query,\n                                       variant_ids=variant_ids,\n                                       category=category)\n        sorting = []\n        if sort_key == 'variant_rank':\n            sorting = [('variant_rank', pymongo.ASCENDING)]\n        if sort_key == 'rank_score':\n            sorting = [('rank_score', pymongo.DESCENDING)]\n        if sort_key == 'position':\n            sorting = [('position', pymongo.ASCENDING)]\n\n        result = self.variant_collection.find(\n            mongo_query,\n            skip=skip,\n            limit=nr_of_variants\n        ).sort(sorting)\n\n        return result", "response": "Returns a list of variants in question for a specific case."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sanger_variants(self, institute_id=None, case_id=None):\n        query = {'validation': {'$exists': True}}\n        if institute_id:\n            query['institute_id'] = institute_id\n        if case_id:\n            query['case_id'] = case_id\n\n        return self.variant_collection.find(query)", "response": "Returns all variants with sanger information"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef variant(self, document_id, gene_panels=None, case_id=None):\n        query = {}\n        if case_id:\n            # search for a variant in a case\n            query['case_id'] = case_id\n            query['variant_id'] = document_id\n        else:\n            # search with a unique id\n            query['_id'] = document_id\n\n        variant_obj = self.variant_collection.find_one(query)\n        if variant_obj:\n            variant_obj = self.add_gene_info(variant_obj, gene_panels)\n            if variant_obj['chromosome'] in ['X', 'Y']:\n                ## TODO add the build here\n                variant_obj['is_par'] = is_par(variant_obj['chromosome'],\n                                               variant_obj['position'])\n        return variant_obj", "response": "Returns the specified variant."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns all variants seen in a given gene.", "response": "def gene_variants(self, query=None,\n                   category='snv', variant_type=['clinical'],\n                   nr_of_variants=50, skip=0):\n        \"\"\"Return all variants seen in a given gene.\n\n        If skip not equal to 0 skip the first n variants.\n\n        Arguments:\n            query(dict): A dictionary with querys for the database, including\n            variant_type: 'clinical', 'research'\n            category(str): 'sv', 'str', 'snv' or 'cancer'\n            nr_of_variants(int): if -1 return all variants\n            skip(int): How many variants to skip\n        \"\"\"\n\n        mongo_variant_query = self.build_variant_query(query=query,\n                                   category=category, variant_type=variant_type)\n\n        sorting = [('rank_score', pymongo.DESCENDING)]\n\n        if nr_of_variants == -1:\n            nr_of_variants = 0 # This will return all variants\n        else:\n            nr_of_variants = skip + nr_of_variants\n\n        result = self.variant_collection.find(\n            mongo_variant_query\n            ).sort(sorting).skip(skip).limit(nr_of_variants)\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn all verified variants for a given institute id", "response": "def verified(self, institute_id):\n        \"\"\"Return all verified variants for a given institute\n\n        Args:\n            institute_id(str): institute id\n\n        Returns:\n            res(list): a list with validated variants\n        \"\"\"\n        query = {\n            'verb' : 'validate',\n            'institute' : institute_id,\n        }\n        res = []\n        validate_events = self.event_collection.find(query)\n        for validated in list(validate_events):\n            case_id = validated['case']\n            var_obj = self.variant(case_id=case_id, document_id=validated['variant_id'])\n            case_obj = self.case(case_id=case_id)\n            if not case_obj or not var_obj:\n                continue # Take into account that stuff might have been removed from database\n            var_obj['case_obj'] = {\n                'display_name' : case_obj['display_name'],\n                'individuals' : case_obj['individuals']\n            }\n            res.append(var_obj)\n\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_causatives(self, institute_id, case_id=None):\n\n        causatives = []\n\n        if case_id:\n\n            case_obj = self.case_collection.find_one(\n                    {\"_id\": case_id}\n                )\n            causatives = [causative for causative in case_obj['causatives']]\n\n        elif institute_id:\n\n            query = self.case_collection.aggregate([\n                {'$match': {'collaborators': institute_id, 'causatives': {'$exists': True}}},\n                {'$unwind': '$causatives'},\n                {'$group': {'_id': '$causatives'}}\n            ])\n            causatives = [item['_id'] for item in query]\n\n        return causatives", "response": "Returns all causative variants for an institute and case."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_causatives(self, case_obj=None, institute_obj=None):\n        institute_id = case_obj['owner'] if case_obj else institute_obj['_id']\n        institute_causative_variant_ids = self.get_causatives(institute_id)\n        if len(institute_causative_variant_ids) == 0:\n            return []\n\n        if case_obj:\n            # exclude variants that are marked causative in \"case_obj\"\n            case_causative_ids = set(case_obj.get('causatives', []))\n            institute_causative_variant_ids = list(\n                set(institute_causative_variant_ids).difference(case_causative_ids)\n            )\n\n        # convert from unique ids to general \"variant_id\"\n        query = self.variant_collection.find(\n            {'_id': {'$in': institute_causative_variant_ids}},\n            {'variant_id': 1}\n        )\n        positional_variant_ids = [item['variant_id'] for item in query]\n\n        filters = {'variant_id': {'$in': positional_variant_ids}}\n        if case_obj:\n            filters['case_id'] = case_obj['_id']\n        else:\n            filters['institute'] = institute_obj['_id']\n        return self.variant_collection.find(filters)", "response": "Checks if there are any causatives in the current case."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns an iterator over the same variant in other cases marked causative.", "response": "def other_causatives(self, case_obj, variant_obj):\n        \"\"\"Find the same variant in other cases marked causative.\n\n        Args:\n            case_obj(dict)\n            variant_obj(dict)\n\n        Yields:\n            other_variant(dict)\n        \"\"\"\n        # variant id without \"*_[variant_type]\"\n        variant_id = variant_obj['display_name'].rsplit('_', 1)[0]\n\n        institute_causatives = self.get_causatives(variant_obj['institute'])\n        for causative_id in institute_causatives:\n            other_variant = self.variant(causative_id)\n            if not other_variant:\n                continue\n            not_same_case = other_variant['case_id'] != case_obj['_id']\n            same_variant = other_variant['display_name'].startswith(variant_id)\n            if not_same_case and same_variant:\n                yield other_variant"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete_variants(self, case_id, variant_type, category=None):\n        category = category or ''\n        LOG.info(\"Deleting old {0} {1} variants for case {2}\".format(\n                    variant_type, category, case_id))\n        query = {'case_id': case_id, 'variant_type': variant_type}\n        if category:\n            query['category'] = category\n        result = self.variant_collection.delete_many(query)\n        LOG.info(\"{0} variants deleted\".format(result.deleted_count))", "response": "Delete all variants of one type for a case"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns all variants that overlap the given variant_obj.", "response": "def overlapping(self, variant_obj):\n        \"\"\"Return overlapping variants.\n\n        Look at the genes that a variant overlaps to.\n        Then return all variants that overlap these genes.\n\n        If variant_obj is sv it will return the overlapping snvs and oposite\n        There is a problem when SVs are huge since there are to many overlapping variants.\n\n        Args:\n            variant_obj(dict)\n\n        Returns:\n            variants(iterable(dict))\n        \"\"\"\n        #This is the category of the variants that we want to collect\n        category = 'snv' if variant_obj['category'] == 'sv' else 'sv'\n\n        query = {\n            '$and': [\n                {'case_id': variant_obj['case_id']},\n                {'category': category},\n                {'hgnc_ids' : { '$in' : variant_obj['hgnc_ids']}}\n            ]\n        }\n\n        sort_key = [('rank_score', pymongo.DESCENDING)]\n        # We collect the 30 most severe overlapping variants\n        variants = self.variant_collection.find(query).sort(sort_key).limit(30)\n\n        return variants"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef evaluated_variants(self, case_id):\n        # Get all variants that have been evaluated in some way for a case\n        query = {\n            '$and': [\n                {'case_id': case_id},\n                {\n                    '$or': [\n                        {'acmg_classification': {'$exists': True}},\n                        {'manual_rank': {'$exists': True}},\n                        {'dismiss_variant': {'$exists': True}},\n                    ]\n                }\n            ],\n        }\n\n        # Collect the result in a dictionary\n        variants = {}\n        for var in self.variant_collection.find(query):\n            variants[var['variant_id']] = self.add_gene_info(var)\n\n        # Collect all variant comments from the case\n        event_query = {\n            '$and': [\n                {'case': case_id},\n                {'category': 'variant'},\n                {'verb': 'comment'},\n            ]\n        }\n\n        # Get all variantids for commented variants\n        comment_variants = {event['variant_id'] for event in self.event_collection.find(event_query)}\n\n        # Get the variant objects for commented variants, if they exist\n        for var_id in comment_variants:\n            # Skip if we already added the variant\n            if var_id in variants:\n                continue\n            # Get the variant with variant_id (not _id!)\n            variant_obj = self.variant(var_id, case_id=case_id)\n\n            # There could be cases with comments that refers to non existing variants\n            # if a case has been reanalysed\n            if not variant_obj:\n                continue\n\n            variant_obj['is_commented'] = True\n            variants[var_id] = variant_obj\n\n        # Return a list with the variant objects\n        return variants.values()", "response": "Returns all variants that have been evaluated in some way for a case."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_region_vcf(self, case_obj, chrom=None, start=None, end=None,\n                       gene_obj=None, variant_type='clinical', category='snv',\n                       rank_threshold=None):\n        \"\"\"Produce a reduced vcf with variants from the specified coordinates\n           This is used for the alignment viewer.\n\n        Args:\n            case_obj(dict): A case from the scout database\n            variant_type(str): 'clinical' or 'research'. Default: 'clinical'\n            category(str): 'snv' or 'sv'. Default: 'snv'\n            rank_threshold(float): Only load variants above this score. Default: 5\n            chrom(str): Load variants from a certain chromosome\n            start(int): Specify the start position\n            end(int): Specify the end position\n            gene_obj(dict): A gene object from the database\n\n        Returns:\n            file_name(str): Path to the temporary file\n        \"\"\"\n        rank_threshold = rank_threshold or -100\n\n        variant_file = None\n        if variant_type == 'clinical':\n            if category == 'snv':\n                variant_file = case_obj['vcf_files'].get('vcf_snv')\n            elif category == 'sv':\n                variant_file = case_obj['vcf_files'].get('vcf_sv')\n            elif category == 'str':\n                variant_file = case_obj['vcf_files'].get('vcf_str')\n        elif variant_type == 'research':\n            if category == 'snv':\n                variant_file = case_obj['vcf_files'].get('vcf_snv_research')\n            elif category == 'sv':\n                variant_file = case_obj['vcf_files'].get('vcf_sv_research')\n\n        if not variant_file:\n            raise SyntaxError(\"Vcf file does not seem to exist\")\n\n        vcf_obj = VCF(variant_file)\n        region = \"\"\n\n        if gene_obj:\n            chrom = gene_obj['chromosome']\n            start = gene_obj['start']\n            end = gene_obj['end']\n\n        if chrom:\n            if (start and end):\n                region = \"{0}:{1}-{2}\".format(chrom, start, end)\n            else:\n                region = \"{0}\".format(chrom)\n\n        else:\n            rank_threshold = rank_threshold or 5\n\n        with tempfile.NamedTemporaryFile(mode='w', delete=False) as temp:\n            file_name = str(pathlib.Path(temp.name))\n            for header_line in vcf_obj.raw_header.split('\\n'):\n                if len(header_line) > 3:\n                    temp.write(header_line + '\\n')\n            for variant in vcf_obj(region):\n                temp.write(str(variant))\n\n        return file_name", "response": "This function returns a reduced vcf with variants from the specified coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sample_variants(self, variants, sample_name, category = 'snv'):\n        LOG.info('Retrieving variants for subject : {0}'.format(sample_name))\n        has_allele = re.compile('1|2') # a non wild-type allele is called at least once in this sample\n\n        query = {\n            '$and': [\n                {'_id' : { '$in' : variants}},\n                {'category' : category},\n                {'samples': {\n                    '$elemMatch': { 'display_name' : sample_name, 'genotype_call': { '$regex' : has_allele } }\n                }}\n            ]\n        }\n\n        result = self.variant_collection.find(query)\n        return result", "response": "Given a list of variants get a list of Variant objects that are found in a specific patient"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a connection to the mongo database", "response": "def get_connection(host='localhost', port=27017, username=None, password=None,\n                   uri=None, mongodb=None, authdb=None, timeout=20, *args, **kwargs):\n    \"\"\"Get a client to the mongo database\n\n        host(str): Host of database\n        port(int): Port of database\n        username(str)\n        password(str)\n        uri(str)\n        authdb (str): database to use for authentication\n        timeout(int): How long should the client try to connect\n\n    \"\"\"\n    authdb = authdb or mongodb\n    if uri is None:\n        if username and password:\n            uri = (\"mongodb://{}:{}@{}:{}/{}\"\n                   .format(quote_plus(username), quote_plus(password), host, port, authdb))\n            log_uri = (\"mongodb://{}:****@{}:{}/{}\"\n                   .format(quote_plus(username), host, port, authdb))\n        else:\n            log_uri = uri = \"mongodb://%s:%s\" % (host, port)\n            \n\n    LOG.info(\"Try to connect to %s\" % log_uri)\n    try:\n        client = MongoClient(uri, serverSelectionTimeoutMS=timeout)\n    except ServerSelectionTimeoutError as err:\n        LOG.warning(\"Connection Refused\")\n        raise ConnectionFailure\n\n    LOG.info(\"Connection established\")\n    return client"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_submission_objects(form_fields):\n    variant_ids = get_submission_variants(form_fields) # A list of variant IDs present in the submitted form\n\n    # Extract list of variant objects to be submitted\n    variant_objs = get_objects_from_form(variant_ids, form_fields, 'variant')\n\n    # Extract list of casedata objects to be submitted\n    casedata_objs = get_objects_from_form(variant_ids, form_fields, 'casedata')\n\n    return (variant_objs, casedata_objs)", "response": "Creates a list of variant and case - data objects from the clinvar submission form in blueprints / variants. html."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_objects_from_form(variant_ids, form_fields, object_type):\n    submission_fields = []\n    if object_type == 'variant':\n        submission_fields = CLINVAR_HEADER\n    else: #collect casedata objects\n        submission_fields = CASEDATA_HEADER\n\n    # A list of objects (variants of casedata info) to be saved into clinvar database collection\n    submission_objects = []\n\n    # Loop over the form fields and collect the data:\n    for variant_id in variant_ids: # loop over the variants\n\n        subm_obj = {} # A new submission object for each\n\n        # Don't included casedata for a variant unless specified by user\n        if object_type == 'casedata' and 'casedata_'+variant_id not in form_fields:\n            continue\n\n        subm_obj['csv_type'] = object_type\n        subm_obj['case_id'] = form_fields.get('case_id')\n        subm_obj['category'] = form_fields.get('category@'+variant_id)\n\n        for key, values in submission_fields.items(): # loop over the form info fields\n            field_value = form_fields.get(key+'@'+variant_id)\n            if field_value and not field_value == '-':\n                if key == 'ref_seq': # split this field into\n                    refseq_raw = field_value.split('|')\n                    subm_obj['ref_seq'] = refseq_raw[0]\n                    subm_obj['hgvs'] = refseq_raw[1]\n                else:\n                    subm_obj[key] = field_value\n\n        # Create a unique ID for the database\n        # For casedata : = caseID_sampleID_variantID\n        # For variants : ID = caseID_variantID\n        if object_type == 'casedata':\n            subm_obj['_id'] = str(subm_obj['case_id']) + '_' + variant_id + '_' + str(subm_obj['individual_id'])\n        else:\n            subm_obj['_id'] = str(subm_obj['case_id']) + '_' + variant_id\n\n        submission_objects.append(subm_obj)\n\n    return submission_objects", "response": "Extract the objects to be saved in the clinvar database collection."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract a list of variant ids from the clinvar submission form in blueprints / variants / clinvar. html.", "response": "def get_submission_variants(form_fields):\n    \"\"\"Extracts a list of variant ids from the clinvar submission form in blueprints/variants/clinvar.html (creation of a new clinvar submission).\n\n        Args:\n             form_fields(dict): it's the submission form dictionary. Keys have the same names as CLINVAR_HEADER and CASEDATA_HEADER\n\n        Returns:\n             clinvars: A list of variant IDs\n    \"\"\"\n\n    clinvars = []\n\n    # if the html checkbox named 'all_vars' is checked in the html form, then all pinned variants from a case should be included in the clinvar submission file,\n    # otherwise just the selected one.\n    if 'all_vars' in form_fields:\n        for field, value in form_fields.items():\n            if field.startswith('local_id'):\n                clinvars.append(form_fields[field].replace('local_id@',''))\n    else:\n        clinvars = [form_fields['main_var']] #also a list, but has one element\n\n    return clinvars"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndetermines which fields to include in a csv file containing the real data included in the CLINVAR_HEADER and CASEDATA_HEADER.", "response": "def clinvar_submission_header(submission_objs, csv_type):\n    \"\"\"Determine which fields to include in csv header by checking a list of submission objects\n\n        Args:\n            submission_objs(list): a list of objects (variants or casedata) to include in a csv file\n            csv_type(str) : 'variant_data' or 'case_data'\n\n        Returns:\n            custom_header(dict): A dictionary with the fields required in the csv header. Keys and values are specified in CLINVAR_HEADER and CASEDATA_HEADER\n\n    \"\"\"\n\n    complete_header = {} # header containing all available fields\n    custom_header = {}   # header reflecting the real data included in the submission objects\n    if csv_type == 'variant_data' :\n        complete_header = CLINVAR_HEADER\n    else:\n        complete_header = CASEDATA_HEADER\n\n    for header_key, header_value in complete_header.items(): # loop over the info fields provided in each submission object\n        for clinvar_obj in submission_objs: # loop over the submission objects\n            for key, value in clinvar_obj.items(): # loop over the keys and values of the clinvar objects\n\n                if not header_key in custom_header and header_key == key: # add to custom header if missing and specified in submission object\n                    custom_header[header_key] = header_value\n\n    return custom_header"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating the lines to include in a Clinvar submission csv file from a list of submission objects and a custom document header", "response": "def clinvar_submission_lines(submission_objs, submission_header):\n    \"\"\"Create the lines to include in a Clinvar submission csv file from a list of submission objects and a custom document header\n\n        Args:\n            submission_objs(list): a list of objects (variants or casedata) to include in a csv file\n            submission_header(dict) : as in constants CLINVAR_HEADER and CASEDATA_HEADER, but with required fields only\n\n        Returns:\n            submission_lines(list) a list of strings, each string represents a line of the clinvar csv file to be doenloaded\n    \"\"\"\n    submission_lines = []\n    for submission_obj in submission_objs: # Loop over the submission objects. Each of these is a line\n        csv_line = []\n        for header_key, header_value in submission_header.items(): # header_keys are the same keys as in submission_objs\n            if header_key in submission_obj: # The field is filled in for this variant/casedata object\n                csv_line.append('\"'+submission_obj.get(header_key)+'\"')\n            else: # Empty field for this this variant/casedata object\n                csv_line.append('\"\"')\n\n        submission_lines.append(','.join(csv_line))\n\n    return submission_lines"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_transcripts(adapter, transcripts_lines=None, build='37', ensembl_genes=None):\n    # Fetch all genes with ensemblid as keys\n    ensembl_genes = ensembl_genes or adapter.ensembl_genes(build)\n\n    if transcripts_lines is None:\n        transcripts_lines = fetch_ensembl_transcripts(build=build)\n\n    # Map with all transcripts enstid -> parsed transcript\n    transcripts_dict = parse_transcripts(transcripts_lines)\n    for ens_tx_id in list(transcripts_dict):\n        parsed_tx = transcripts_dict[ens_tx_id]\n        # Get the ens gene id\n        ens_gene_id = parsed_tx['ensembl_gene_id']\n\n        # Fetch the internal gene object to find out the correct hgnc id\n        gene_obj = ensembl_genes.get(ens_gene_id)\n        # If the gene is non existing in scout we skip the transcript\n        if not gene_obj:\n            transcripts_dict.pop(ens_tx_id)\n            LOG.debug(\"Gene %s does not exist in build %s\", ens_gene_id, build)\n            continue\n\n        # Add the correct hgnc id\n        parsed_tx['hgnc_id'] = gene_obj['hgnc_id']\n        # Primary transcript information is collected from HGNC\n        parsed_tx['primary_transcripts'] = set(gene_obj.get('primary_transcripts', []))\n\n\n    ref_seq_transcripts = 0\n    nr_primary_transcripts = 0\n    nr_transcripts = len(transcripts_dict)\n\n    transcript_objs = []\n\n    with progressbar(transcripts_dict.values(), label=\"Building transcripts\", length=nr_transcripts) as bar:\n        for tx_data in bar:\n\n            #################### Get the correct refseq identifier ####################\n            # We need to decide one refseq identifier for each transcript, if there are any to \n            # choose from. The algorithm is as follows:\n            # If there is ONE mrna this is choosen\n            # If there are several mrna the one that is in 'primary_transcripts' is choosen\n            # Else one is choosen at random\n            # The same follows for the other categories where nc_rna has precedense over mrna_predicted\n            # We will store all refseq identifiers in a \"refseq_identifiers\" list as well\n            tx_data['is_primary'] = False\n            primary_transcripts = tx_data['primary_transcripts']\n            refseq_identifier = None\n            refseq_identifiers = []\n            for category in TRANSCRIPT_CATEGORIES:\n                identifiers = tx_data[category]\n                if not identifiers:\n                    continue\n\n                for refseq_id in identifiers:\n                    # Add all refseq identifiers to refseq_identifiers\n                    refseq_identifiers.append(refseq_id)\n                    ref_seq_transcripts += 1\n\n                    if refseq_id in primary_transcripts:\n                        refseq_identifier = refseq_id\n                        tx_data['is_primary'] = True\n                        nr_primary_transcripts += 1\n                    \n                    if not refseq_identifier:\n                        refseq_identifier = refseq_id\n\n            if refseq_identifier:\n                tx_data['refseq_id'] = refseq_identifier\n            if refseq_identifiers:\n                tx_data['refseq_identifiers'] = refseq_identifiers\n\n            ####################  ####################  ####################\n            # Build the transcript object\n            tx_obj = build_transcript(tx_data, build)\n            transcript_objs.append(tx_obj)\n\n    # Load all transcripts\n    LOG.info(\"Loading transcripts...\")\n    if len(transcript_objs) > 0:\n        adapter.load_transcript_bulk(transcript_objs)\n\n    LOG.info('Number of transcripts in build %s: %s', build, nr_transcripts)\n    LOG.info('Number of transcripts with refseq identifier: %s', ref_seq_transcripts)\n    LOG.info('Number of primary transcripts: %s', nr_primary_transcripts)\n\n    return transcript_objs", "response": "Load all the transcripts from the ensembl database into a list of HGNC objects."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a gene panel to the database.", "response": "def panel(context, path, date, display_name, version, panel_type, panel_id, institute, omim, api_key, panel_app):\n    \"\"\"Add a gene panel to the database.\"\"\"\n\n    adapter = context.obj['adapter']\n    institute = institute or 'cust000'\n\n    if omim:\n        api_key = api_key or context.obj.get('omim_api_key')\n        if not api_key:\n            LOG.warning(\"Please provide a omim api key to load the omim gene panel\")\n            context.abort()\n        #Check if OMIM-AUTO exists\n        if adapter.gene_panel(panel_id='OMIM-AUTO'):\n            LOG.warning(\"OMIM-AUTO already exists in database\")\n            LOG.info(\"To create a new version use scout update omim\")\n            return\n\n        # Here we know that there is no panel loaded\n        try:\n            adapter.load_omim_panel(api_key, institute=institute)\n        except Exception as err:\n            LOG.error(err)\n            context.abort()\n    \n    if panel_app:\n        # try:\n        load_panel_app(adapter, panel_id, institute=institute)\n        # except Exception as err:\n        #     LOG.warning(err)\n        #     context.abort()\n\n    if (omim or panel_app):\n        return\n\n    if path is None:\n        LOG.info(\"Please provide a panel\")\n        return\n    try:\n        load_panel(path, adapter, date, display_name, version, panel_type, panel_id, institute)\n    except Exception as err:\n        LOG.warning(err)\n        context.abort()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbuild a new object of the same type as exon_obj.", "response": "def build_exon(exon_info, build='37'):\n    \"\"\"Build a Exon object object\n\n        Args:\n            exon_info(dict): Exon information\n\n        Returns:\n            exon_obj(Exon)\n        \n        \"exon_id\": str, # str(chrom-start-end)\n        \"chrom\": str, \n        \"start\": int, \n        \"end\": int,     \n        \"transcript\": str, # ENST ID\n        \"hgnc_id\": int,      # HGNC_id\n        \"rank\": int, # Order of exon in transcript\n        \"build\": str, # Genome build\n    \"\"\"\n\n    try:\n        chrom = exon_info['chrom']\n    except KeyError:\n        raise KeyError(\"Exons has to have a chromosome\")\n\n    try:\n        start = int(exon_info['start'])\n    except KeyError:\n        raise KeyError(\"Exon has to have a start\")\n    except TypeError:\n        raise TypeError(\"Exon start has to be integer\")\n    \n    try:\n        end = int(exon_info['end'])\n    except KeyError:\n        raise KeyError(\"Exon has to have a end\")\n    except TypeError:\n        raise TypeError(\"Exon end has to be integer\")\n\n    try:\n        rank = int(exon_info['rank'])\n    except KeyError:\n        raise KeyError(\"Exon has to have a rank\")\n    except TypeError:\n        raise TypeError(\"Exon rank has to be integer\")\n\n    try:\n        exon_id = exon_info['exon_id']\n    except KeyError:\n        raise KeyError(\"Exons has to have a id\")\n\n    try:\n        transcript = exon_info['transcript']\n    except KeyError:\n        raise KeyError(\"Exons has to have a transcript\")\n\n    try:\n        hgnc_id = int(exon_info['hgnc_id'])\n    except KeyError:\n        raise KeyError(\"Exons has to have a hgnc_id\")\n    except TypeError:\n        raise TypeError(\"hgnc_id has to be integer\")\n\n    exon_obj = Exon(\n        exon_id = exon_id,\n        chrom = chrom,\n        start = start,\n        end = end,\n        rank = rank,\n        transcript = transcript,\n        hgnc_id = hgnc_id,\n        build = build,\n    )\n\n    return exon_obj"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting a version of a gene panel", "response": "def panel(context, panel_id, version):\n    \"\"\"Delete a version of a gene panel or all versions of a gene panel\"\"\"\n    LOG.info(\"Running scout delete panel\")\n    adapter = context.obj['adapter']\n\n    panel_objs = adapter.gene_panels(panel_id=panel_id, version=version)\n    if panel_objs.count() == 0:\n        LOG.info(\"No panels found\")\n        \n    for panel_obj in panel_objs:\n        adapter.delete_panel(panel_obj)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete all indexes in the database", "response": "def index(context):\n    \"\"\"Delete all indexes in the database\"\"\"\n    LOG.info(\"Running scout delete index\")\n    adapter = context.obj['adapter']\n    \n    for collection in adapter.db.collection_names():\n        adapter.db[collection].drop_indexes()\n    LOG.info(\"All indexes deleted\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeleting a user from the database", "response": "def user(context, mail):\n    \"\"\"Delete a user from the database\"\"\"\n    LOG.info(\"Running scout delete user\")\n    adapter = context.obj['adapter']\n    user_obj = adapter.user(mail)\n    if not user_obj:\n        LOG.warning(\"User {0} could not be found in database\".format(mail))\n    else:\n        adapter.delete_user(mail)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef genes(context, build):\n    LOG.info(\"Running scout delete genes\")\n    adapter = context.obj['adapter']\n\n    if build:\n        LOG.info(\"Dropping genes collection for build: %s\", build)\n    else:\n        LOG.info(\"Dropping genes collection\")\n        adapter.drop_genes()", "response": "Delete all genes in the database"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef exons(context, build):\n    LOG.info(\"Running scout delete exons\")\n    adapter = context.obj['adapter']\n\n    adapter.drop_exons(build)", "response": "Delete all exons in the database"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef case(context, institute, case_id, display_name):\n    adapter = context.obj['adapter']\n    if not (case_id or display_name):\n        click.echo(\"Please specify what case to delete\")\n        context.abort()\n\n    if display_name:\n        if not institute:\n            click.echo(\"Please specify the owner of the case that should be \"\n                       \"deleted with flag '-i/--institute'.\")\n            context.abort()\n        case_id = \"{0}-{1}\".format(institute, display_name)\n\n    LOG.info(\"Running deleting case {0}\".format(case_id))\n    case = adapter.delete_case(\n        case_id=case_id,\n        institute_id=institute,\n        display_name=display_name\n    )\n\n    if case.deleted_count == 1:\n        adapter.delete_variants(case_id=case_id, variant_type='clinical')\n        adapter.delete_variants(case_id=case_id, variant_type='research')\n    else:\n        LOG.warning(\"Case does not exist in database\")\n        context.abort()", "response": "Delete a case and its variants from the database"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nshows all individuals from all cases in the database", "response": "def individuals(context, institute, causatives, case_id):\n    \"\"\"Show all individuals from all cases in the database\"\"\"\n    LOG.info(\"Running scout view individuals\")\n    adapter = context.obj['adapter']\n    individuals = []\n\n    if case_id:\n        case = adapter.case(case_id=case_id)\n        if case:\n            cases = [case]\n        else:\n            LOG.info(\"Could not find case %s\", case_id)\n            return\n    else:\n        cases = [case_obj for case_obj in\n                 adapter.cases(\n                     collaborator=institute,\n                     has_causatives=causatives)]\n        if len(cases) == 0:\n            LOG.info(\"Could not find cases that match criteria\")\n            return\n        individuals = (ind_obj for case_obj in cases for ind_obj in case_obj['individuals'])\n\n    click.echo(\"#case_id\\tind_id\\tdisplay_name\\tsex\\tphenotype\\tmother\\tfather\")\n\n    for case in cases:\n        for ind_obj in case['individuals']:\n            ind_info = [\n                case['_id'], ind_obj['individual_id'],\n                ind_obj['display_name'], SEX_MAP[int(ind_obj['sex'])],\n                PHENOTYPE_MAP[ind_obj['phenotype']], ind_obj['mother'],\n                ind_obj['father']\n            ]\n            click.echo('\\t'.join(ind_info))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nextracts all phenotype - associated terms for a case. Drawback of this method is that it returns the same phenotype terms for each affected individual .", "response": "def hpo_terms(case_obj):\n    \"\"\"Extract all phenotype-associated terms for a case. Drawback of this method is that\n        it returns the same phenotype terms for each affected individual\n        of the case.\n        Args:\n            case_obj(dict): a scout case object\n        Returns:\n            features(list): a list of phenotype objects that looks like this:\n            [\n              {\n                \"id\": \"HP:0001644\",\n                \"label\": \"Dilated cardiomyopathy\",\n                \"observed\": \"yes\"\n              },\n              ...\n            ]\n    \"\"\"\n    LOG.info('Collecting phenotype terms for case {}'.format(case_obj.get('display_name')))\n    features = []\n    case_features = case_obj.get('phenotype_terms')\n    if case_features:\n        # re-structure case features to mirror matchmaker feature fields:\n        for feature in case_features:\n            feature_obj = {\n                \"id\" : feature.get('phenotype_id'),\n                \"label\" : feature.get('feature'),\n                \"observed\" : \"yes\"\n            }\n            features.append(feature_obj)\n    return features"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef omim_terms(case_obj):\n    LOG.info(\"Collecting OMIM disorders for case {}\".format(case_obj.get('display_name')))\n    disorders = []\n\n    case_disorders = case_obj.get('diagnosis_phenotypes') # array of OMIM terms\n    if case_disorders:\n        for disorder in case_disorders:\n            disorder_obj = {\n                \"id\" : ':'.join([ 'MIM', str(disorder)])\n            }\n            disorders.append(disorder_obj)\n    return disorders", "response": "Extract all OMIM phenotypes available for the case"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef genomic_features(store, case_obj, sample_name, genes_only):\n    g_features = []\n    # genome build is required\n    build = case_obj['genome_build']\n    if not build in ['37', '38']:\n        build = 'GRCh37'\n    else:\n        build = 'GRCh'+build\n\n    individual_pinned_snvs = list(store.sample_variants( variants=case_obj.get('suspects'),\n        sample_name=sample_name))\n\n    # if genes_only is True don't add duplicated genes\n    gene_set = set()\n    for var in individual_pinned_snvs:\n        # a variant could hit one or several genes so create a genomic feature for each of these genes\n        hgnc_genes = var.get('hgnc_ids')\n        # Looks like MatchMaker Exchange API accepts only variants that hit genes :(\n        if not hgnc_genes:\n            continue\n        for hgnc_id in hgnc_genes:\n            gene_obj = store.hgnc_gene(hgnc_id)\n            if not gene_obj:\n                continue\n            g_feature = {\n                'gene': {'id': gene_obj.get('hgnc_symbol')}\n            }\n            if genes_only and not hgnc_id in gene_set: # if only gene names should be shared\n                gene_set.add(hgnc_id)\n                g_features.append(g_feature)\n                continue\n\n            # if also variants should be shared:\n            g_feature['variant'] = {\n                'referenceName' : var['chromosome'],\n                'start' : var['position'],\n                'end' : var['end'],\n                'assembly' : build,\n                'referenceBases' :var['reference'],\n                'alternateBases' : var['alternative'],\n                'shareVariantLevelData' : True\n            }\n            zygosity = None\n            # collect zygosity for the given sample\n            zygosities = var['samples'] # it's a list with zygosity situation for each sample of the case\n            for zyg in zygosities:\n                if zyg.get('display_name') == sample_name: # sample of interest\n                    zygosity = zyg['genotype_call'].count('1') + zyg['genotype_call'].count('2')\n            g_feature['zygosity'] = zygosity\n            g_features.append(g_feature)\n\n    return g_features", "response": "Extract and parse matchmaker - like genomic features from pinned variants\n        and sample_name."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_matches(patient_id, match_objs):\n    LOG.info('Parsing MatchMaker matches for patient {}'.format(patient_id))\n    parsed_matches = []\n\n    for match_obj in match_objs:\n\n        # convert match date from millisecond to readable date\n        milliseconds_date = match_obj['created']['$date']\n        mdate = datetime.datetime.fromtimestamp(milliseconds_date/1000.0)\n        match_type = 'external'\n        matching_patients = []\n\n        parsed_match = {\n            'match_oid' : match_obj['_id']['$oid'],# save match object ID\n            'match_date' : mdate\n        }\n\n        # if patient was used as query patient:\n        if match_obj['data']['patient']['id'] == patient_id:\n            match_results = match_obj['results'] # List of matching patients\n            for node_result in match_results:\n                if match_obj['match_type'] == 'internal':\n                    match_type = 'internal'\n\n                for patient in node_result['patients']:\n                    match_patient = {\n                        'patient_id' : patient['patient']['id'],\n                        'score' : patient['score'],\n                        'patient' : patient['patient'],\n                        'node' : node_result['node']\n                    }\n                    matching_patients.append(match_patient)\n\n        else: # else if patient was returned as a match result for another patient\n            m_patient = match_obj['data']['patient']\n            contact_institution = m_patient['contact'].get('institution')\n            if contact_institution and 'Scout software user' in contact_institution:\n                match_type = 'internal'\n\n            # loop over match results to capture score for matching\n            score = None\n            for res in match_obj['results']:\n                for patient in res['patients']:\n                    LOG.info('Looping in else, patient:{}'.format(patient['patient']['id']))\n                    if patient['patient']['id'] == patient_id:\n\n                        score = patient['score']\n                        match_patient = {\n                            'patient_id' : m_patient['id'],\n                            'score' : score,\n                            'patient' : m_patient,\n                            'node' : res['node']\n                        }\n                        matching_patients.append(match_patient)\n\n        parsed_match['match_type'] = match_type\n        parsed_match['patients'] = matching_patients\n        parsed_matches.append(parsed_match)\n\n    # sort results by descending score\n    parsed_matches = sorted(parsed_matches, key=lambda k: k['match_date'], reverse=True)\n    return parsed_matches", "response": "Parse a list of matchmaker matches objects and returns\nATTRIBS a readable list of matches to display in matchmaker matches view."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cases(context, institute, display_name, case_id, nr_variants, variants_treshold):\n    LOG.info(\"Running scout view institutes\")\n    adapter = context.obj['adapter']\n\n    models = []\n    if case_id:\n        case_obj = adapter.case(case_id=case_id)\n        if case_obj:\n            models.append(case_obj)\n\n    else:\n        models = adapter.cases(collaborator=institute, name_query=display_name)\n        models = [case_obj for case_obj in models]\n\n    if not models:\n        LOG.info(\"No cases could be found\")\n        return\n    \n    header = ['case_id', 'display_name', 'institute']\n    \n    if variants_treshold:\n        LOG.info(\"Only show cases with more than %s variants\", variants_treshold)\n        nr_variants = True\n    \n    if nr_variants:\n        LOG.info(\"Displaying number of variants for each case\")\n        header.append('clinical')\n        header.append('research')\n    \n    \n    click.echo(\"#\"+'\\t'.join(header))\n    for model in models:\n        output_str = \"{:<12}\\t{:<12}\\t{:<12}\"\n        output_values = [model['_id'],model['display_name'],model['owner']]\n        if nr_variants:\n            output_str += \"\\t{:<12}\\t{:<12}\"\n            nr_clinical = 0\n            nr_research = 0\n            variants = adapter.variant_collection.find({'case_id':model['_id']})\n            i = 0\n            for i, var in enumerate(variants, 1):\n                if var['variant_type'] == 'clinical':\n                    nr_clinical += 1\n                else:\n                    nr_research += 1\n            output_values.extend([nr_clinical, nr_research])\n            \n            if variants_treshold and i < variants_treshold:\n                LOG.debug(\"Case %s had to few variants, skipping\", model['_id'])\n                continue\n            \n        click.echo(output_str.format(*output_values))", "response": "Display the cases from the database"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the currently active user as an object.", "response": "def load_user(user_email):\n    \"\"\"Returns the currently active user as an object.\"\"\"\n    user_obj = store.user(user_email)\n    user_inst = LoginUser(user_obj) if user_obj else None\n    return user_inst"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlogs-in a user if they have access.", "response": "def login():\n    \"\"\"Login a user if they have access.\"\"\"\n    # store potential next param URL in the session\n    if 'next' in request.args:\n        session['next_url'] = request.args['next']\n\n    if current_app.config.get('GOOGLE'):\n        callback_url = url_for('.authorized', _external=True)\n        return google.authorize(callback=callback_url)\n\n    user_email = request.args.get('email')\n    user_obj = store.user(user_email)\n\n    if user_obj is None:\n        flash(\"email not whitelisted: {}\".format(user_email), 'warning')\n        return redirect(url_for('public.index'))\n\n    return perform_login(user_obj)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef case_mme_update(self, case_obj, user_obj, mme_subm_obj):\n        created = None\n        patient_ids = []\n        updated = datetime.now()\n        if 'mme_submission' in case_obj and case_obj['mme_submission']:\n            created = case_obj['mme_submission']['created_at']\n        else:\n            created = updated\n        patients = [ resp['patient'] for resp in mme_subm_obj.get('server_responses')]\n\n        subm_obj = {\n            'created_at' : created,\n            'updated_at' : updated,\n            'patients' : patients, # list of submitted patient data\n            'subm_user' : user_obj['_id'], # submitting user\n            'sex' : mme_subm_obj['sex'],\n            'features' : mme_subm_obj['features'],\n            'disorders' : mme_subm_obj['disorders'],\n            'genes_only' : mme_subm_obj['genes_only']\n        }\n        case_obj['mme_submission'] = subm_obj\n        updated_case = self.update_case(case_obj)\n\n        # create events for subjects add in MatchMaker for this case\n        institute_obj = self.institute(case_obj['owner'])\n        for individual in case_obj['individuals']:\n            if individual['phenotype'] == 2: # affected\n                # create event for patient\n                self.create_event(institute=institute_obj, case=case_obj, user=user_obj,\n                    link='', category='case', verb='mme_add', subject=individual['display_name'],\n                    level='specific')\n\n        return updated_case", "response": "Updates a case after a submission to MatchMaker Exchange"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef case_mme_delete(self, case_obj, user_obj):\n        institute_obj = self.institute(case_obj['owner'])\n        # create events for subjects removal from Matchmaker this cas\n        for individual in case_obj['individuals']:\n            if individual['phenotype'] == 2: # affected\n                # create event for patient removal\n                self.create_event(institute=institute_obj, case=case_obj, user=user_obj,\n                    link='', category='case', verb='mme_remove', subject=individual['display_name'],\n                    level='specific')\n\n        # Reset mme_submission field for this case\n        case_obj['mme_submission'] = None\n        updated_case = self.update_case(case_obj)\n        return updated_case", "response": "Delete a MatchMaker submission from a case and creates the related event."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_institute(internal_id, display_name, sanger_recipients=None,\n                    coverage_cutoff=None, frequency_cutoff=None):\n    \"\"\"Build a institute object\n\n    Args:\n        internal_id(str)\n        display_name(str)\n        sanger_recipients(list(str)): List with email addresses\n\n    Returns:\n        institute_obj(scout.models.Institute)\n\n    \"\"\"\n\n    LOG.info(\"Building institute %s with display name %s\", internal_id,display_name)\n\n    institute_obj = Institute(\n        internal_id=internal_id, \n        display_name=display_name, \n        sanger_recipients=sanger_recipients,\n        coverage_cutoff = coverage_cutoff,\n        frequency_cutoff = frequency_cutoff\n    )\n    \n    for key in list(institute_obj):\n        if institute_obj[key] is None:\n            institute_obj.pop(key)\n\n    return institute_obj", "response": "Build a institute object with the given internal_id and display_name."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes an event from the database.", "response": "def delete_event(self, event_id):\n        \"\"\"Delete a event\n\n            Arguments:\n                event_id (str): The database key for the event\n        \"\"\"\n        LOG.info(\"Deleting event{0}\".format(event_id))\n        if not isinstance(event_id, ObjectId):\n            event_id = ObjectId(event_id)\n        self.event_collection.delete_one({'_id': event_id})\n        LOG.debug(\"Event {0} deleted\".format(event_id))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_event(self, institute, case, user, link, category, verb,\n                     subject, level='specific', variant=None, content=None,\n                     panel=None):\n        \"\"\"Create a Event with the parameters given.\n\n        Arguments:\n            institute (dict): A institute\n            case (dict): A case\n            user (dict): A User\n            link (str): The url to be used in the event\n            category (str): case or variant\n            verb (str): What type of event\n            subject (str): What is operated on\n            level (str): 'specific' or 'global'. Default is 'specific'\n            variant (dict): A variant\n            content (str): The content of the comment\n\n        Returns:\n            event(dict): The inserted event\n        \"\"\"\n        variant = variant or {}\n        event = dict(\n            institute=institute['_id'],\n            case=case['_id'],\n            user_id=user['_id'],\n            user_name=user['name'],\n            link=link,\n            category=category,\n            verb=verb,\n            subject=subject,\n            level=level,\n            variant_id=variant.get('variant_id'),\n            content=content,\n            panel=panel,\n            created_at=datetime.now(),\n            updated_at=datetime.now(),\n        )\n\n        LOG.debug(\"Saving Event\")\n        self.event_collection.insert_one(event)\n        LOG.debug(\"Event Saved\")\n        return event", "response": "Create an event in the event collection with the parameters given."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef events(self, institute, case=None, variant_id=None, level=None,\n               comments=False, panel=None):\n        \"\"\"Fetch events from the database.\n\n          Args:\n            institute (dict): A institute\n            case (dict): A case\n            variant_id (str, optional): global variant id\n            level (str, optional): restrict comments to 'specific' or 'global'\n            comments (bool, optional): restrict events to include only comments\n            panel (str): A panel name\n\n          Returns:\n              pymongo.Cursor: Query result\n        \"\"\"\n\n        query = {}\n\n        if variant_id:\n            if comments:\n                # If it's comment-related event collect global and variant-specific comment events\n                LOG.debug(\"Fetching all comments for institute {0} case {1} variant {2}\".format(\n                          institute['_id'], case['_id'], variant_id))\n                query = {\n                    '$or': [\n                        {\n                            'category' : 'variant',\n                            'variant_id' : variant_id,\n                            'verb' : 'comment',\n                            'level' : 'global'\n                        },\n                        {\n                            'category' : 'variant',\n                            'variant_id' : variant_id,\n                            'institute' : institute['_id'],\n                            'case' : case['_id'],\n                            'verb' : 'comment',\n                            'level' : 'specific'\n                        }\n                    ]\n                }\n            else: # Collect other variant-specific events which are not comments\n                query['institute'] = institute['_id']\n                query['category'] = 'variant'\n                query['variant_id'] = variant_id\n                query['case'] = case['_id']\n        else:\n            query['institute'] = institute['_id']\n            if panel:\n                query['panel'] = panel\n            # If no variant_id or panel we know that it is a case level comment\n            else:\n                query['category'] = 'case'\n\n                if case:\n                    query['case'] = case['_id']\n\n                if comments:\n                    query['verb'] = 'comment'\n\n\n        return self.event_collection.find(query).sort('created_at', pymongo.DESCENDING)", "response": "Fetch events from the database."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfetching all events by a specific user.", "response": "def user_events(self, user_obj=None):\n        \"\"\"Fetch all events by a specific user.\"\"\"\n        query = dict(user_id=user_obj['_id']) if user_obj else dict()\n        return self.event_collection.find(query)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_phenotype(self, institute, case, user, link, hpo_term=None,\n                      omim_term=None, is_group=False):\n        \"\"\"Add a new phenotype term to a case\n\n            Create a phenotype term and event with the given information\n\n            Args:\n                institute (Institute): A Institute object\n                case (Case): Case object\n                user (User): A User object\n                link (str): The url to be used in the event\n                hpo_term (str): A hpo id\n                omim_term (str): A omim id\n                is_group (bool): is phenotype term a group?\n\n        \"\"\"\n        hpo_results = []\n        try:\n            if hpo_term:\n                hpo_results = [hpo_term]\n            elif omim_term:\n                LOG.debug(\"Fetching info for mim term {0}\".format(omim_term))\n                disease_obj = self.disease_term(omim_term)\n                if disease_obj:\n                    for hpo_term in disease_obj.get('hpo_terms', []):\n                        hpo_results.append(hpo_term)\n            else:\n                raise ValueError('Must supply either hpo or omim term')\n        except ValueError as e:\n            ## TODO Should ve raise a more proper exception here?\n            raise e\n\n        existing_terms = set(term['phenotype_id'] for term in\n                             case.get('phenotype_terms', []))\n\n        updated_case = case\n        phenotype_terms = []\n        for hpo_term in hpo_results:\n            LOG.debug(\"Fetching info for hpo term {0}\".format(hpo_term))\n            hpo_obj = self.hpo_term(hpo_term)\n            if hpo_obj is None:\n                raise ValueError(\"Hpo term: %s does not exist in database\" % hpo_term)\n\n            phenotype_id = hpo_obj['_id']\n            description = hpo_obj['description']\n            if phenotype_id not in existing_terms:\n                phenotype_term = dict(phenotype_id=phenotype_id, feature=description)\n                phenotype_terms.append(phenotype_term)\n\n                LOG.info(\"Creating event for adding phenotype term for case\"\n                            \" {0}\".format(case['display_name']))\n\n                self.create_event(\n                    institute=institute,\n                    case=case,\n                    user=user,\n                    link=link,\n                    category='case',\n                    verb='add_phenotype',\n                    subject=case['display_name'],\n                    content=phenotype_id\n                )\n\n            if is_group:\n                updated_case = self.case_collection.find_one_and_update(\n                    {'_id': case['_id']},\n                    {\n                        '$addToSet': {\n                            'phenotype_terms': {'$each': phenotype_terms},\n                            'phenotype_groups': {'$each': phenotype_terms},\n                        },\n                    },\n                    return_document=pymongo.ReturnDocument.AFTER\n                )\n            else:\n                updated_case = self.case_collection.find_one_and_update(\n                    {'_id': case['_id']},\n                    {\n                        '$addToSet': {\n                            'phenotype_terms': {'$each': phenotype_terms},\n                        },\n                    },\n                    return_document=pymongo.ReturnDocument.AFTER\n                )\n\n        LOG.debug(\"Case updated\")\n        return updated_case", "response": "Adds a new phenotype term to a case"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving an existing phenotype from a case", "response": "def remove_phenotype(self, institute, case, user, link, phenotype_id,\n                         is_group=False):\n        \"\"\"Remove an existing phenotype from a case\n\n        Args:\n            institute (dict): A Institute object\n            case (dict): Case object\n            user (dict): A User object\n            link (dict): The url to be used in the event\n            phenotype_id (str): A phenotype id\n\n        Returns:\n            updated_case(dict)\n        \"\"\"\n        LOG.info(\"Removing HPO term from case {0}\".format(case['display_name']))\n\n        if is_group:\n            updated_case = self.case_collection.find_one_and_update(\n                {'_id': case['_id']},\n                {\n                    '$pull': {\n                        'phenotype_terms': {'phenotype_id': phenotype_id},\n                        'phenotype_groups': {'phenotype_id': phenotype_id},\n                    },\n                },\n                return_document=pymongo.ReturnDocument.AFTER\n            )\n\n        else:\n            updated_case = self.case_collection.find_one_and_update(\n                {'_id': case['_id']},\n                {\n                    '$pull': {\n                        'phenotype_terms': {'phenotype_id': phenotype_id},\n                    },\n                },\n                return_document=pymongo.ReturnDocument.AFTER\n            )\n\n        LOG.info(\"Creating event for removing phenotype term {0}\" \\\n                    \" from case {1}\".format(phenotype_id, case['display_name']))\n\n        self.create_event(\n            institute=institute,\n            case=case,\n            user=user,\n            link=link,\n            category='case',\n            verb='remove_phenotype',\n            subject=case['display_name']\n        )\n\n        LOG.debug(\"Case updated\")\n        return updated_case"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef comment(self, institute, case, user, link, variant=None,\n                content=\"\", comment_level=\"specific\"):\n        \"\"\"Add a comment to a variant or a case.\n\n        This function will create an Event to log that a user have commented on\n        a variant. If a variant id is given it will be a variant comment.\n        A variant comment can be 'global' or specific. The global comments will\n        be shown for this variation in all cases while the specific comments\n        will only be shown for a specific case.\n\n        Arguments:\n            institute (dict): A Institute object\n            case (dict): A Case object\n            user (dict): A User object\n            link (str): The url to be used in the event\n            variant (dict): A variant object\n            content (str): The content of the comment\n            comment_level (str): Any one of 'specific' or 'global'.\n                                 Default is 'specific'\n\n        Return:\n            comment(dict): The comment event that was inserted\n        \"\"\"\n        if not comment_level in COMMENT_LEVELS:\n            raise SyntaxError(\"Comment levels can only be in {}\".format(','.join(COMMENT_LEVELS)))\n\n        if variant:\n            LOG.info(\"Creating event for a {0} comment on variant {1}\".format(\n                comment_level, variant['display_name']))\n\n            comment = self.create_event(\n                institute=institute,\n                case=case,\n                user=user,\n                link=link,\n                category='variant',\n                verb='comment',\n                level=comment_level,\n                variant=variant,\n                subject=variant['display_name'],\n                content=content\n            )\n\n        else:\n            LOG.info(\"Creating event for a comment on case {0}\".format(\n                case['display_name']))\n\n            comment = self.create_event(\n                institute=institute,\n                case=case,\n                user=user,\n                link=link,\n                category='case',\n                verb='comment',\n                subject=case['display_name'],\n                content=content\n            )\n        return comment", "response": "This function creates an event that will log that a user has commented on a specific variant or a specific case."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_genotypes(variant, individuals, individual_positions):\n    genotypes = []\n    for ind in individuals:\n        pos = individual_positions[ind['individual_id']]\n        genotypes.append(parse_genotype(variant, ind, pos))\n    return genotypes", "response": "Parse the genotype calls for a variant\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_genotype(variant, ind, pos):\n    gt_call = {}\n    ind_id = ind['individual_id']\n\n    gt_call['individual_id'] = ind_id\n    gt_call['display_name'] = ind['display_name']\n\n    # Fill the object with the relevant information:\n    genotype = variant.genotypes[pos]\n\n    ref_call = genotype[0]\n    alt_call = genotype[1]\n\n    gt_call['genotype_call'] = '/'.join([GENOTYPE_MAP[ref_call],\n                                         GENOTYPE_MAP[alt_call]])\n\n    paired_end_alt = None\n    paired_end_ref = None\n    split_read_alt = None\n    split_read_ref = None\n\n    # Check if PE is annotated\n    # This is the number of paired end reads that supports the variant\n    if 'PE' in variant.FORMAT:\n        try:\n            value = int(variant.format('PE')[pos])\n            if not value < 0:\n                paired_end_alt = value\n        except ValueError as e:\n            pass\n\n    # Check if PR is annotated\n    # Number of paired end reads that supports ref and alt\n    if 'PR' in variant.FORMAT:\n        values = variant.format('PR')[pos]\n        try:\n            alt_value = int(values[1])\n            ref_value = int(values[0])\n            if not alt_value < 0:\n                paired_end_alt = alt_value\n            if not ref_value < 0:\n                paired_end_ref = ref_value\n        except ValueError as r:\n            pass\n\n    # Check if 'SR' is annotated\n    if 'SR' in variant.FORMAT:\n        values = variant.format('SR')[pos]\n        alt_value = 0\n        ref_value = 0\n        if len(values) == 1:\n            alt_value = int(values[0])\n        elif len(values) == 2:\n            alt_value = int(values[1])\n            ref_value = int(values[0])\n        if not alt_value < 0:\n            split_read_alt = alt_value\n        if not ref_value < 0:\n            split_read_ref = ref_value\n\n    # Number of paired ends that supports the event\n    if 'DV' in variant.FORMAT:\n        values = variant.format('DV')[pos]\n        alt_value = int(values[0])\n        if not alt_value < 0:\n            paired_end_alt = alt_value\n\n    # Number of paired ends that supports the reference\n    if 'DR' in variant.FORMAT:\n        values = variant.format('DR')[pos]\n        ref_value = int(values[0])\n        if not alt_value < 0:\n            paired_end_ref = ref_value\n\n    # Number of split reads that supports the event\n    if 'RV' in variant.FORMAT:\n        values = variant.format('RV')[pos]\n        alt_value = int(values[0])\n        if not alt_value < 0:\n            split_read_alt = alt_value\n\n    # Number of split reads that supports the reference\n    if 'RR' in variant.FORMAT:\n        values = variant.format('RR')[pos]\n        ref_value = int(values[0])\n        if not ref_value < 0:\n            split_read_ref = ref_value\n\n\n    alt_depth = int(variant.gt_alt_depths[pos])\n    if alt_depth == -1:\n        if 'VD' in variant.FORMAT:\n            alt_depth = int(variant.format('VD')[pos][0])\n\n        if (paired_end_alt != None or split_read_alt != None):\n            alt_depth = 0\n            if paired_end_alt:\n                alt_depth += paired_end_alt\n            if split_read_alt:\n                alt_depth += split_read_alt\n\n    gt_call['alt_depth'] = alt_depth\n\n    ref_depth = int(variant.gt_ref_depths[pos])\n    if ref_depth == -1:\n        if (paired_end_ref != None or split_read_ref != None):\n            ref_depth = 0\n            if paired_end_ref:\n                ref_depth += paired_end_ref\n            if split_read_ref:\n                ref_depth += split_read_ref\n\n    gt_call['ref_depth'] = ref_depth\n\n    alt_frequency = float(variant.gt_alt_freqs[pos])\n    if alt_frequency == -1:\n        if 'AF' in variant.FORMAT:\n            alt_frequency = float(variant.format('AF')[pos][0])\n\n    read_depth = int(variant.gt_depths[pos])\n    if read_depth == -1:\n        # If read depth could not be parsed by cyvcf2, try to get it manually\n        if 'DP' in variant.FORMAT:\n            read_depth = int(variant.format('DP')[pos][0])\n        elif (alt_depth != -1 or ref_depth != -1):\n            read_depth = 0\n            if alt_depth != -1:\n                read_depth += alt_depth\n            if ref_depth != -1:\n                read_depth += alt_depth\n\n    gt_call['read_depth'] = read_depth\n\n    gt_call['alt_frequency'] = alt_frequency\n\n    gt_call['genotype_quality'] = int(variant.gt_quals[pos])\n\n    return gt_call", "response": "Parse the genotype information in the proper format"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if a variant is in the Pseudo Autosomal Region or not", "response": "def is_par(chromosome, position, build='37'):\n    \"\"\"Check if a variant is in the Pseudo Autosomal Region or not\n    \n    Args:\n        chromosome(str)\n        position(int)\n        build(str): The genome build\n    \n    Returns:\n        bool\n    \"\"\"\n    chrom_match = CHR_PATTERN.match(chromosome)\n    chrom = chrom_match.group(2)\n    \n    # PAR regions are only on X and Y\n    if not chrom in ['X','Y']:\n        return False\n    # Check if variant is in first PAR region\n    if PAR_COORDINATES[build][chrom].search(position):\n        return True\n    \n    return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if the variant is in the interval given by the coordinates", "response": "def check_coordinates(chromosome, pos, coordinates):\n    \"\"\"Check if the variant is in the interval given by the coordinates\n\n        Args:\n            chromosome(str): Variant chromosome\n            pos(int): Variant position\n            coordinates(dict): Dictionary with the region of interest\n    \"\"\"\n    chrom_match = CHR_PATTERN.match(chromosome)\n    chrom = chrom_match.group(2)\n\n    if chrom != coordinates['chrom']:\n        return False\n    \n    if (pos >= coordinates['start'] and pos <= coordinates['end']):\n        return True\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexports all genes in one or several gene panels to a bed formatted format with coordinates.", "response": "def export_panels(adapter, panels, versions=None, build='37'):\n    \"\"\"Export all genes in gene panels\n    \n    Exports the union of genes in one or several gene panels to a bed like format with coordinates.\n    \n    Args:\n        adapter(scout.adapter.MongoAdapter)\n        panels(iterable(str)): Iterable with panel ids\n        bed(bool): If lines should be bed formated\n    \"\"\"\n    if versions and (len(versions) != len(panels)):\n        raise SyntaxError(\"If version specify for each panel\")\n\n    headers = []\n    build_string = (\"##genome_build={}\")\n    \n    headers.append(build_string.format(build))\n    header_string = (\"##gene_panel={0},version={1},updated_at={2},display_name={3}\")\n    contig_string = (\"##contig={0}\")\n    bed_string = (\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\")\n\n    # Save all gene ids found in the collection if panels\n    panel_geneids = set()\n    # Save all chromosomes found in the collection if panels\n    chromosomes_found = set()\n    # Store all hgnc geneobjs\n    hgnc_geneobjs = []\n\n    # Loop over the panels\n    for i,panel_id in enumerate(panels):\n        version = None\n        if versions:\n            version = versions[i]\n            \n        panel_obj = adapter.gene_panel(panel_id, version=version)\n        if not panel_obj:\n            LOG.warning(\"Panel {0} version {1} could not be found\".format(panel_id, version))\n            continue\n\n        headers.append(header_string.format(\n            panel_obj['panel_name'],\n            panel_obj['version'],\n            panel_obj['date'].date(),\n            panel_obj['display_name'],\n        ))\n        # Collect the hgnc ids from all genes found\n        for gene_obj in panel_obj['genes']:\n            panel_geneids.add(gene_obj['hgnc_id'])\n\n    \n    gene_objs = adapter.hgncid_to_gene(build=build)\n    \n    for hgnc_id in panel_geneids:\n        hgnc_geneobj = gene_objs.get(hgnc_id)\n        if hgnc_geneobj is None:\n            LOG.warn(\"missing HGNC gene: %s\", hgnc_id)\n            continue\n        chrom = hgnc_geneobj['chromosome']\n        start = hgnc_geneobj['start']\n        chrom_int = CHROMOSOME_INTEGERS.get(chrom)\n        if not chrom_int:\n            LOG.warn(\"Chromosome %s out of scope\", chrom)\n            continue\n            \n        hgnc_geneobjs.append((chrom_int, start, hgnc_geneobj))\n        chromosomes_found.add(chrom)\n    \n    # Sort the genes:\n    hgnc_geneobjs.sort(key=lambda tup: (tup[0], tup[1]))\n    \n    for chrom in CHROMOSOMES:\n        if chrom in chromosomes_found:\n            headers.append(contig_string.format(chrom))\n\n    headers.append(\"#chromosome\\tgene_start\\tgene_stop\\thgnc_id\\thgnc_symbol\")\n\n    for header in headers:\n        yield header\n\n    for hgnc_gene in hgnc_geneobjs:\n        gene_obj = hgnc_gene[-1]\n        gene_line = bed_string.format(gene_obj['chromosome'], gene_obj['start'],\n                                      gene_obj['end'], gene_obj['hgnc_id'],\n                                      gene_obj['hgnc_symbol'])\n        yield gene_line"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef export_gene_panels(adapter, panels, version=None):\n    if version and len(panels) > 1:\n        raise SyntaxError(\"Version only possible with one panel\")\n\n    bed_string = (\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\")\n    \n    headers = []\n\n    # Dictionary with hgnc ids as keys and panel gene information as value.\n    panel_geneobjs = dict()\n\n    for panel_id in panels:\n        panel_obj = adapter.gene_panel(panel_id, version=version)\n        if not panel_obj:\n            LOG.warning(\"Panel %s could not be found\", panel_id)\n            continue\n\n        for gene_obj in panel_obj['genes']:\n            panel_geneobjs[gene_obj['hgnc_id']] = gene_obj\n\n    if len(panel_geneobjs) == 0:\n        return\n\n    headers.append('#hgnc_id\\thgnc_symbol\\tdisease_associated_transcripts\\t'\n                    'reduced_penetrance\\tmosaicism\\tdatabase_entry_version')\n\n    for header in headers:\n        yield header\n\n    for hgnc_id in panel_geneobjs:\n        gene_obj = panel_geneobjs[hgnc_id]\n        gene_line = bed_string.format(\n            gene_obj['hgnc_id'], gene_obj['symbol'],\n            ','.join(gene_obj.get('disease_associated_transcripts', [])),\n            gene_obj.get('reduced_penetrance', ''),\n            gene_obj.get('mosaicism', ''),\n            gene_obj.get('database_entry_version', ''),\n        )\n        yield gene_line", "response": "Export the genes of a single panel and return the lines of the panels."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrender search box and view for HPO phenotype terms", "response": "def hpo_terms():\n    \"\"\"Render search box and view for HPO phenotype terms\"\"\"\n    if request.method == 'GET':\n        data = controllers.hpo_terms(store= store, limit=100)\n        return data\n    else: # POST. user is searching for a specific term or phenotype\n        search_term = request.form.get('hpo_term')\n        limit = request.form.get('limit')\n        data = controllers.hpo_terms(store= store,  query = search_term, limit=limit)\n        return dict(data, query=search_term, limit=limit)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef transcripts(context, build):\n    LOG.info(\"Running scout export transcripts\")\n    adapter = context.obj['adapter']\n    \n    header = [\"#Chrom\\tStart\\tEnd\\tTranscript\\tRefSeq\\tHgncID\"]\n\n    for line in header:\n        click.echo(line)\n\n    transcript_string = (\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\")\n\n    for tx_obj in export_transcripts(adapter):\n        click.echo(transcript_string.format(\n            tx_obj['chrom'],\n            tx_obj['start'],\n            tx_obj['end'],\n            tx_obj['ensembl_transcript_id'],\n            tx_obj.get('refseq_id',''),\n            tx_obj['hgnc_id'],\n            )\n        )", "response": "Export all transcripts to. bed like format"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef exons(context, build):\n    \n    adapter = context.obj['adapter']\n    \n    start = datetime.now()\n    # Test if there are any exons loaded\n    \n    nr_exons = adapter.exons(build=build).count()\n    if nr_exons:\n        LOG.warning(\"Dropping all exons \")\n        adapter.drop_exons(build=build)\n        LOG.info(\"Exons dropped\")\n    \n    # Load the exons\n    ensembl_exons = fetch_ensembl_exons(build=build)\n    load_exons(adapter, ensembl_exons, build)\n\n    adapter.update_indexes()\n    \n    LOG.info(\"Time to load exons: {0}\".format(datetime.now() - start))", "response": "Load the exons from the ensembl database into the scout database"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nshows all indexes in the database", "response": "def intervals(context, build):\n    \"\"\"Show all indexes in the database\"\"\"\n    LOG.info(\"Running scout view index\")\n    adapter = context.obj['adapter']\n\n    intervals = adapter.get_coding_intervals(build)\n    nr_intervals = 0\n    longest = 0\n    for chrom in CHROMOSOMES:\n        for iv in intervals[chrom]:\n            iv_len = iv.end - iv.begin\n            if iv_len > longest:\n                longest = iv_len\n        int_nr = len(intervals.get(chrom, []))\n        click.echo(\"{0}\\t{1}\".format(chrom, int_nr))\n        nr_intervals += int_nr\n\n    LOG.info(\"Total nr intervals:%s\", nr_intervals)\n    LOG.info(\"Total nr genes:%s\", adapter.all_genes(build).count())\n    LOG.info(\"Longest interval:%s\", longest)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef region(context, hgnc_id, case_id, chromosome, start, end):\n    adapter = context.obj['adapter']\n    load_region(\n        adapter=adapter, case_id=case_id, hgnc_id=hgnc_id, chrom=chromosome, start=start, end=end\n    )", "response": "Load all variants in a region to a existing case"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning two datetimes first day and last day of given year & month", "response": "def get_first_and_last(year, month):\n        \"\"\"Returns two datetimes: first day and last day of given year&month\"\"\"\n        ym_first = make_aware(\n            datetime.datetime(year, month, 1),\n            get_default_timezone()\n        )\n        ym_last = make_aware(\n            datetime.datetime(year, month, monthrange(year, month)[1], 23, 59, 59, 1000000-1),\n            get_default_timezone()\n        )\n        return ym_first, ym_last"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef all_month_events(self, year, month, category=None, tag=None,\n                         loc=False, cncl=False):\n        \"\"\"\n        Returns all events that have an occurrence within the given\n        month & year.\n        \"\"\"\n        kwargs = self._get_kwargs(category, tag)\n        ym_first, ym_last = self.get_first_and_last(year, month)\n\n        pref = []\n        if loc:\n            pref.append(\"location\")\n        if cncl:\n            pref.append(\"cancellations\")\n\n        # for yearly repeat, we need to check the start and end date months\n        # b/c yearly events should occur every year in the same month\n        r = Q(repeat=\"YEARLY\")\n        dstart_mo = Q(start_date__month=month)\n        dend_mo = Q(end_date__month=month)\n\n        dstart_yr = Q(start_date__year=year)\n        dend_yr = Q(end_date__year=year)\n\n        return self.model.objects.filter(\n            # only events that are still repeating\n            r & (dstart_mo | dend_mo) |  # yearly repeat\n            (~Q(repeat=\"NEVER\")) |  # all other repeats\n            ((dstart_yr | dend_yr) & (dstart_mo | dend_yr)),  # non-repeating\n            Q(end_repeat=None) | Q(end_repeat__gte=ym_first),\n            start_date__lte=ym_last  # no events that haven't started yet\n        ).filter(**kwargs).prefetch_related(*pref).order_by('start_date').distinct()", "response": "Returns all events that occur within the given month."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef live(self, now):\n        return self.model.objects.filter(\n            Q(end_repeat=None) | Q(end_repeat__gte=now) |\n            Q(start_date__gte=now) | Q(end_date__gte=now)\n        ).exclude(  # exclude single day events that won't occur again\n            start_date__lt=now, end_date__lt=now,\n            repeat=\"NEVER\", end_repeat=None,\n        ).prefetch_related('cancellations')", "response": "Returns a queryset of events that will occur again after now."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef build_user(user_info):\n    try:\n        email = user_info['email']\n    except KeyError as err:\n        raise KeyError(\"A user has to have a email\")\n    \n    try:\n        name = user_info['name']\n    except KeyError as err:\n        raise KeyError(\"A user has to have a name\")\n    \n    user_obj = User(email=email, name=name)\n    \n    ##TODO check that these are on the correct format\n    if 'roles' in user_info:\n        user_obj['roles'] = user_info['roles']\n    \n    if 'location' in user_info:\n        user_obj['location'] = user_info['location']\n    \n    if 'institutes' in user_info:\n        user_obj['institutes'] = user_info['institutes']\n    \n    return user_obj", "response": "Build a user object from a user info dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef existing_gene(store, panel_obj, hgnc_id):\n    existing_genes = {gene['hgnc_id']: gene for gene in panel_obj['genes']}\n    return existing_genes.get(hgnc_id)", "response": "Check if a gene is already added to a panel."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_panel(store, panel_name, csv_lines, option):\n    new_genes= []\n    panel_obj = store.gene_panel(panel_name)\n    if panel_obj is None:\n        return None\n    try:\n        new_genes = parse_genes(csv_lines) # a list of gene dictionaries containing gene info\n    except SyntaxError as error:\n        flash(error.args[0], 'danger')\n        return None\n\n    # if existing genes are to be replaced by those in csv_lines\n    if option == 'replace':\n        # all existing genes should be deleted\n        for gene in panel_obj['genes']:\n            #create extra key to use in pending actions:\n            gene['hgnc_symbol'] = gene['symbol']\n            store.add_pending(panel_obj, gene, action='delete', info=None)\n\n    for new_gene in new_genes:\n        if not new_gene['hgnc_id']:\n            flash(\"gene missing hgnc id: {}\".format(new_gene['hgnc_symbol']),'danger')\n            continue\n        gene_obj = store.hgnc_gene(new_gene['hgnc_id'])\n        if gene_obj is None:\n            flash(\"gene not found: {} - {}\".format(new_gene['hgnc_id'], new_gene['hgnc_symbol']),'danger')\n            continue\n        if new_gene['hgnc_symbol'] and gene_obj['hgnc_symbol'] != new_gene['hgnc_symbol']:\n            flash(\"symbol mis-match: {0} | {1}\".format(\n                gene_obj['hgnc_symbol'], new_gene['hgnc_symbol']), 'warning')\n\n        info_data = {\n            'disease_associated_transcripts': new_gene['transcripts'],\n            'reduced_penetrance': new_gene['reduced_penetrance'],\n            'mosaicism': new_gene['mosaicism'],\n            'inheritance_models': new_gene['inheritance_models'],\n            'database_entry_version': new_gene['database_entry_version'],\n        }\n        if option == 'replace': # there will be no existing genes for sure, because we're replacing them all\n            action = 'add'\n        else: # add option. Add if genes is not existing. otherwise edit it\n            existing_genes = {gene['hgnc_id'] for gene in panel_obj['genes']}\n            action = 'edit' if gene_obj['hgnc_id'] in existing_genes else 'add'\n        store.add_pending(panel_obj, gene_obj, action=action, info=info_data)\n\n    return panel_obj", "response": "Update an existing gene panel with genes."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new gene panel.", "response": "def new_panel(store, institute_id, panel_name, display_name, csv_lines):\n    \"\"\"Create a new gene panel.\n\n    Args:\n        store(scout.adapter.MongoAdapter)\n        institute_id(str)\n        panel_name(str)\n        display_name(str)\n        csv_lines(iterable(str)): Stream with genes\n\n    Returns:\n        panel_id: the ID of the new panel document created or None\n\n    \"\"\"\n    institute_obj = store.institute(institute_id)\n    if institute_obj is None:\n        flash(\"{}: institute not found\".format(institute_id))\n        return None\n\n    panel_obj = store.gene_panel(panel_name)\n    if panel_obj:\n        flash(\"panel already exists: {} - {}\".format(panel_obj['panel_name'],\n                                                     panel_obj['display_name']))\n        return None\n\n    log.debug(\"parse genes from CSV input\")\n    try:\n        new_genes = parse_genes(csv_lines)\n    except SyntaxError as error:\n        flash(error.args[0], 'danger')\n        return None\n\n    log.debug(\"build new gene panel\")\n\n    panel_id = None\n    try:\n        panel_data = build_panel(dict(\n            panel_name=panel_name,\n            institute=institute_obj['_id'],\n            version=1.0,\n            date=dt.datetime.now(),\n            display_name=display_name,\n            genes=new_genes,\n        ), store)\n        panel_id= store.add_gene_panel(panel_data)\n\n    except Exception as err:\n        log.error('An error occurred while adding the gene panel {}'.format(err))\n\n    return panel_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets information about a case from archive.", "response": "def archive_info(database: Database, archive_case: dict) -> dict:\n    \"\"\"Get information about a case from archive.\"\"\"\n    data = {\n        'collaborators': archive_case['collaborators'],\n        'synopsis': archive_case.get('synopsis'),\n        'assignees': [],\n        'suspects': [],\n        'causatives': [],\n        'phenotype_terms': [],\n        'phenotype_groups': [],\n    }\n    if archive_case.get('assignee'):\n        archive_user = database.user.find_one({'_id': archive_case['assignee']})\n        data['assignee'].append(archive_user['email'])\n\n    for key in ['suspects', 'causatives']:\n        for variant_id in archive_case.get(key, []):\n            archive_variant = database.variant.find_one({'_id': variant_id})\n            data[key].append({\n                'chromosome': archive_variant['chromosome'],\n                'position': archive_variant['position'],\n                'reference': archive_variant['reference'],\n                'alternative': archive_variant['alternative'],\n                'variant_type': archive_variant['variant_type'],\n            })\n\n    for key in ['phenotype_terms', 'phenotype_groups']:\n        for archive_term in archive_case.get(key, []):\n            data[key].append({\n                'phenotype_id': archive_term['phenotype_id'],\n                'feature': archive_term['feature'],\n            })\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmigrates information from archive to scout_case.", "response": "def migrate_case(adapter: MongoAdapter, scout_case: dict, archive_data: dict):\n    \"\"\"Migrate case information from archive.\"\"\"\n    # update collaborators\n    collaborators = list(set(scout_case['collaborators'] + archive_data['collaborators']))\n    if collaborators != scout_case['collaborators']:\n        LOG.info(f\"set collaborators: {', '.join(collaborators)}\")\n        scout_case['collaborators'] = collaborators\n\n    # update assignees\n    if len(scout_case.get('assignees', [])) == 0:\n        scout_user = adapter.user(archive_data['assignee'])\n        if scout_user:\n            scout_case['assignees'] = [archive_data['assignee']]\n        else:\n            LOG.warning(f\"{archive_data['assignee']}: unable to find assigned user\")\n\n    # add/update suspected/causative variants\n    for key in ['suspects', 'causatives']:\n        scout_case[key] = scout_case.get(key, [])\n        for archive_variant in archive_data[key]:\n            variant_id = get_variantid(archive_variant, scout_case['_id'])\n            scout_variant = adapter.variant(variant_id)\n            if scout_variant:\n                if scout_variant['_id'] in scout_case[key]:\n                    LOG.info(f\"{scout_variant['_id']}: variant already in {key}\")\n                else:\n                    LOG.info(f\"{scout_variant['_id']}: add to {key}\")\n                    scout_variant[key].append(scout_variant['_id'])\n            else:\n                LOG.warning(f\"{scout_variant['_id']}: unable to find variant ({key})\")\n                scout_variant[key].append(variant_id)\n\n    if not scout_case.get('synopsis'):\n        # update synopsis\n        scout_case['synopsis'] = archive_data['synopsis']\n\n    scout_case['is_migrated'] = True\n    adapter.case_collection.find_one_and_replace(\n        {'_id': scout_case['_id']},\n        scout_case,\n    )\n\n    # add/update phenotype groups/terms\n    scout_institute = adapter.institute(scout_case['owner'])\n    scout_user = adapter.user('mans.magnusson@scilifelab.se')\n    for key in ['phenotype_terms', 'phenotype_groups']:\n        for archive_term in archive_data[key]:\n            adapter.add_phenotype(\n                institute=scout_institute,\n                case=scout_case,\n                user=scout_user,\n                link=f\"/{scout_case['owner']}/{scout_case['display_name']}\",\n                hpo_term=archive_term['phenotype_id'],\n                is_group=key == 'phenotype_groups',\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating all information that was manually annotated from a old instance.", "response": "def migrate(uri: str, archive_uri: str, case_id: str, dry: bool, force: bool):\n    \"\"\"Update all information that was manually annotated from a old instance.\"\"\"\n    scout_client = MongoClient(uri)\n    scout_database = scout_client[uri.rsplit('/', 1)[-1]]\n    scout_adapter = MongoAdapter(database=scout_database)\n    scout_case = scout_adapter.case(case_id)\n    if not force and scout_case.get('is_migrated'):\n        print(\"case already migrated\")\n        return\n\n    archive_client = MongoClient(archive_uri)\n    archive_database = archive_client[archive_uri.rsplit('/', 1)[-1]]\n    archive_case = archive_database.case.find_one({\n        'owner': scout_case['owner'],\n        'display_name': scout_case['display_name']\n    })\n\n    archive_data = archive_info(archive_database, archive_case)\n\n    if dry:\n        print(ruamel.yaml.safe_dump(archive_data))\n    else:\n        #migrate_case(scout_adapter, scout_case, archive_data)\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nuploading research variants to the specified case", "response": "def research(context, case_id, institute, force):\n    \"\"\"Upload research variants to cases\n\n        If a case is specified, all variants found for that case will be\n        uploaded.\n\n        If no cases are specified then all cases that have 'research_requested'\n        will have there research variants uploaded\n    \"\"\"\n    LOG.info(\"Running scout load research\")\n    adapter = context.obj['adapter']\n\n    if case_id:\n        if not institute:\n            # There was an old way to create case ids so we need a special case to handle this\n            # Assume institute-case combo\n            splitted_case = case_id.split('-')\n            # Check if first part is institute, then we know it is the old format\n            if len(splitted_case) > 1:\n                institute_obj = adapter.institute(splitted_case[0])\n                if institute_obj:\n                    institute = institute_obj['_id']\n                    case_id = splitted_case[1]\n        case_obj = adapter.case(institute_id=institute, case_id=case_id)\n        if case_obj is None:\n            LOG.warning(\"No matching case found\")\n            context.abort()\n        else:\n            case_objs = [case_obj]\n    else:\n        # Fetch all cases that have requested research\n        case_objs = adapter.cases(research_requested=True)\n\n    default_threshold = 8\n    files = False\n    for case_obj in case_objs:\n        if force or case_obj['research_requested']:\n            # Test to upload research snvs\n            if case_obj['vcf_files'].get('vcf_snv_research'):\n                files = True\n                adapter.delete_variants(case_id=case_obj['_id'],\n                                        variant_type='research',\n                                        category='snv')\n\n                LOG.info(\"Load research SNV for: %s\", case_obj['_id'])\n                adapter.load_variants(\n                    case_obj=case_obj,\n                    variant_type='research',\n                    category='snv',\n                    rank_threshold=default_threshold,\n                    )\n\n            # Test to upload research svs\n            if case_obj['vcf_files'].get('vcf_sv_research'):\n                files = True\n                adapter.delete_variants(case_id=case_obj['_id'],\n                                        variant_type='research',\n                                        category='sv')\n                LOG.info(\"Load research SV for: %s\", case_obj['_id'])\n                adapter.load_variants(\n                    case_obj=case_obj,\n                    variant_type='research',\n                    category='sv',\n                    rank_threshold=default_threshold,\n                    )\n\n            # Test to upload research cancer variants\n            if case_obj['vcf_files'].get('vcf_cancer_research'):\n                files = True\n                adapter.delete_variants(case_id=case_obj['_id'],\n                                        variant_type='research',\n                                        category='cancer')\n\n                LOG.info(\"Load research cancer for: %s\", case_obj['_id'])\n                adapter.load_variants(\n                    case_obj=case_obj,\n                    variant_type='research',\n                    category='cancer',\n                    rank_threshold=default_threshold,\n                    )\n            if not files:\n                LOG.warning(\"No research files found for case %s\", case_id)\n                context.abort()\n            case_obj['is_research'] = True\n            case_obj['research_requested'] = False\n            adapter.update_case(case_obj)\n        else:\n            LOG.warn(\"research not requested, use '--force'\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_hgnc(adapter, genes=None, ensembl_lines=None, hgnc_lines=None, exac_lines=None, mim2gene_lines=None,\n              genemap_lines=None, hpo_lines=None, transcripts_lines=None, build='37', omim_api_key=''):\n    \"\"\"Load Genes and transcripts into the database\n              \n    If no resources are provided the correct ones will be fetched.\n    \n    Args:\n        adapter(scout.adapter.MongoAdapter)\n        genes(dict): If genes are already parsed\n        ensembl_lines(iterable(str)): Lines formated with ensembl gene information\n        hgnc_lines(iterable(str)): Lines with gene information from genenames.org\n        exac_lines(iterable(str)): Lines with information pLi-scores from ExAC\n        mim2gene(iterable(str)): Lines with map from omim id to gene symbol\n        genemap_lines(iterable(str)): Lines with information of omim entries\n        hpo_lines(iterable(str)): Lines information about map from hpo terms to genes\n        transcripts_lines(iterable): iterable with ensembl transcript lines\n        build(str): What build to use. Defaults to '37'\n\n    \"\"\"\n    gene_objs = load_hgnc_genes(\n        adapter=adapter, \n        genes = genes, \n        ensembl_lines=ensembl_lines, \n        hgnc_lines=hgnc_lines, \n        exac_lines=exac_lines, \n        mim2gene_lines=mim2gene_lines,\n        genemap_lines=genemap_lines, \n        hpo_lines=hpo_lines, \n        build=build, \n        omim_api_key=omim_api_key,\n    )\n    \n    ensembl_genes = {}\n    for gene_obj in gene_objs:\n        ensembl_genes[gene_obj['ensembl_id']] = gene_obj\n    \n    transcript_objs = load_transcripts(\n        adapter=adapter, \n        transcripts_lines=transcripts_lines, \n        build=build, \n        ensembl_genes=ensembl_genes)", "response": "Load HGNC gene and transcript objects into the database."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_hgnc_genes(adapter, genes = None, ensembl_lines=None, hgnc_lines=None, exac_lines=None, mim2gene_lines=None,\n                    genemap_lines=None, hpo_lines=None, build='37', omim_api_key=''):\n    \"\"\"Load genes into the database\n        \n    link_genes will collect information from all the different sources and \n    merge it into a dictionary with hgnc_id as key and gene information as values.\n\n    Args:\n        adapter(scout.adapter.MongoAdapter)\n        genes(dict): If genes are already parsed\n        ensembl_lines(iterable(str)): Lines formated with ensembl gene information\n        hgnc_lines(iterable(str)): Lines with gene information from genenames.org\n        exac_lines(iterable(str)): Lines with information pLi-scores from ExAC\n        mim2gene(iterable(str)): Lines with map from omim id to gene symbol\n        genemap_lines(iterable(str)): Lines with information of omim entries\n        hpo_lines(iterable(str)): Lines information about map from hpo terms to genes\n        build(str): What build to use. Defaults to '37'\n\n    Returns:\n        gene_objects(list): A list with all gene_objects that was loaded into database\n    \"\"\"\n    gene_objects = list()\n    \n    if not genes:\n        # Fetch the resources if not provided\n        if ensembl_lines is None:\n            ensembl_lines = fetch_ensembl_genes(build=build)\n        hgnc_lines = hgnc_lines or fetch_hgnc()\n        exac_lines = exac_lines or fetch_exac_constraint()\n        if not (mim2gene_lines and genemap_lines):\n            if not omim_api_key:\n                raise SyntaxError(\"Need to provide omim api key\")\n            mim_files = fetch_mim_files(omim_api_key, mim2genes=True, genemap2=True)\n            mim2gene_lines = mim_files['mim2genes']\n            genemap_lines = mim_files['genemap2']\n        if not hpo_lines:\n            hpo_files = fetch_hpo_files(hpogenes=True)\n            hpo_lines = hpo_files['hpogenes']\n        \n        \n        # Link the resources\n        genes = link_genes(\n            ensembl_lines=ensembl_lines,\n            hgnc_lines=hgnc_lines,\n            exac_lines=exac_lines,\n            mim2gene_lines=mim2gene_lines,\n            genemap_lines=genemap_lines,\n            hpo_lines=hpo_lines\n        )\n\n    non_existing = 0\n    nr_genes = len(genes)\n    \n    with progressbar(genes.values(), label=\"Building genes\", length=nr_genes) as bar:\n        for gene_data in bar:\n            if not gene_data.get('chromosome'):\n                LOG.debug(\"skipping gene: %s. No coordinates found\", gene_data.get('hgnc_symbol', '?'))\n                non_existing += 1\n                continue\n        \n            gene_obj = build_hgnc_gene(gene_data, build=build)\n            gene_objects.append(gene_obj)\n\n    LOG.info(\"Loading genes build %s\", build)\n    adapter.load_hgnc_bulk(gene_objects)\n\n    LOG.info(\"Loading done. %s genes loaded\", len(gene_objects))\n    LOG.info(\"Nr of genes without coordinates in build %s: %s\", build,non_existing)\n    \n    return gene_objects", "response": "Load the genes from the HGNC file into the database and merge them into a single link_genes."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nshows all hpo terms in the database", "response": "def hpo(context, term, description):\n    \"\"\"Show all hpo terms in the database\"\"\"\n    LOG.info(\"Running scout view hpo\")\n    adapter = context.obj['adapter']\n    if term:\n        term = term.upper()\n        if not term.startswith('HP:'):\n            while len(term) < 7:\n                term = '0' + term\n            term = 'HP:' + term\n        LOG.info(\"Searching for term %s\", term)\n        hpo_terms = adapter.hpo_terms(hpo_term=term)\n    elif description:\n        sorted_terms = sorted(adapter.hpo_terms(query=description), key=itemgetter('hpo_number'))\n        for term in sorted_terms:\n            term.pop('genes')\n            print(\"name: {} | {} | {}\".format(term['_id'], term['description'], term['hpo_number']))\n        # pp(hpo_terms)\n        context.abort()\n    else:\n        hpo_terms = adapter.hpo_terms()\n    if hpo_terms.count() == 0:\n        LOG.warning(\"No matching terms found\")\n        return\n\n    click.echo(\"hpo_id\\tdescription\\tnr_genes\")\n    for hpo_obj in hpo_terms:\n        click.echo(\"{0}\\t{1}\\t{2}\".format(\n            hpo_obj['hpo_id'],\n            hpo_obj['description'],\n            len(hpo_obj.get('genes',[]))\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding a gene object for the given gene", "response": "def build_gene(gene, hgncid_to_gene=None):\n    \"\"\"Build a gene object\n        \n        Has to build the transcripts for the genes to\n    \n    Args:\n        gene(dict): Parsed information from the VCF\n        hgncid_to_gene(dict): A map from hgnc_id  -> hgnc_gene objects \n\n    Returns:\n        gene_obj(dict)\n    \n    gene = dict(\n        # The hgnc gene id\n        hgnc_id = int, # required\n        hgnc_symbol = str, \n        ensembl_gene_id = str, \n        # A list of Transcript objects\n        transcripts = list, # list of <transcript>\n        # This is the worst functional impact of all transcripts\n        functional_annotation = str, # choices=SO_TERM_KEYS\n        # This is the region of the most severe functional impact\n        region_annotation = str, # choices=FEATURE_TYPES\n        # This is most severe sift prediction of all transcripts\n        sift_prediction = str, # choices=CONSEQUENCE\n        # This is most severe polyphen prediction of all transcripts\n        polyphen_prediction = str, # choices=CONSEQUENCE\n    )\n    \n    \"\"\"\n    hgncid_to_gene = hgncid_to_gene or {}\n    gene_obj = dict()\n \n    # This id is collected from the VCF\n    # Typically annotated by VEP or snpEFF\n    hgnc_id = int(gene['hgnc_id'])\n    gene_obj['hgnc_id'] = hgnc_id\n\n    # Get the gene information from database\n    hgnc_gene = hgncid_to_gene.get(hgnc_id)\n    \n    inheritance = set()\n    hgnc_transcripts = []\n    if hgnc_gene:\n        # Set the hgnc symbol etc to the one internally in Scout\n        gene_obj['hgnc_symbol'] = hgnc_gene['hgnc_symbol']\n        gene_obj['ensembl_id'] = hgnc_gene['ensembl_id']\n        gene_obj['description'] = hgnc_gene['description']\n\n        if hgnc_gene.get('inheritance_models'):\n            gene_obj['inheritance'] = hgnc_gene['inheritance_models']\n    \n    transcripts = []\n    for transcript in gene['transcripts']:\n        transcript_obj = build_transcript(transcript)\n        transcripts.append(transcript_obj)\n    gene_obj['transcripts'] = transcripts\n\n    functional_annotation = gene.get('most_severe_consequence')\n    if functional_annotation:\n        if not functional_annotation in SO_TERM_KEYS:\n            LOG.warning(\"Invalid functional annotation %s\", functional_annotation)\n        else:\n            gene_obj['functional_annotation'] = functional_annotation\n    \n    region_annotation = gene.get('region_annotation')\n    if region_annotation:\n        if not region_annotation in FEATURE_TYPES:\n            LOG.warning(\"Invalid region annotation %s\", region_annotation)\n        else:\n            gene_obj['region_annotation'] = region_annotation\n \n    sift_prediction = gene.get('most_severe_sift')\n    if sift_prediction:\n        if not sift_prediction in CONSEQUENCE:\n            LOG.warning(\"Invalid sift prediction %s\", sift_prediction)\n        else:\n            gene_obj['sift_prediction'] = sift_prediction\n\n    polyphen_prediction = gene.get('most_severe_polyphen')\n    if polyphen_prediction:\n        if not polyphen_prediction in CONSEQUENCE:\n            LOG.warning(\"Invalid polyphen prediction %s\", polyphen_prediction)\n        else:\n            gene_obj['polyphen_prediction'] = polyphen_prediction\n    \n    gene_obj['hgvs_identifier'] = gene['hgvs_identifier']\n    gene_obj['canonical_transcript'] = gene['canonical_transcript']\n    gene_obj['exon'] = gene['exon']\n    \n    return gene_obj"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_app(config_file=None, config=None):\n    app = Flask(__name__)\n    app.config.from_pyfile('config.py')\n    app.jinja_env.add_extension('jinja2.ext.do')\n    if config:\n        app.config.update(config)\n    if config_file:\n        app.config.from_pyfile(config_file)\n\n    # If there is a MatchMaker Exchange server\n    # collect the connected external nodes\n    app.mme_nodes = mme_nodes(app.config.get('MME_URL'), app.config.get('MME_TOKEN'))\n\n\n    app.config[\"JSON_SORT_KEYS\"] = False\n    current_log_level = logger.getEffectiveLevel()\n    coloredlogs.install(level='DEBUG' if app.debug else current_log_level)\n    configure_extensions(app)\n    register_blueprints(app)\n    register_filters(app)\n\n    if not (app.debug or app.testing) and app.config.get('MAIL_USERNAME'):\n        # setup email logging of errors\n        configure_email_logging(app)\n\n    @app.before_request\n    def check_user():\n        if not app.config.get('LOGIN_DISABLED') and request.endpoint:\n            # check if the endpoint requires authentication\n            static_endpoint = 'static' in request.endpoint or 'report' in request.endpoint\n            public_endpoint = getattr(app.view_functions[request.endpoint],\n                                      'is_public', False)\n            relevant_endpoint = not (static_endpoint or public_endpoint)\n            # if endpoint requires auth, check if user is authenticated\n            if relevant_endpoint and not current_user.is_authenticated:\n                # combine visited URL (convert byte string query string to unicode!)\n                next_url = \"{}?{}\".format(request.path, request.query_string.decode())\n                login_url = url_for('login.login', next=next_url)\n                return redirect(login_url)\n\n    return app", "response": "Create Flask application factory function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef configure_email_logging(app):\n    import logging\n    from scout.log import TlsSMTPHandler\n\n    mail_handler = TlsSMTPHandler(\n        mailhost=app.config['MAIL_SERVER'],\n        fromaddr=app.config['MAIL_USERNAME'],\n        toaddrs=app.config['ADMINS'],\n        subject=\"O_ops... {} failed!\".format(app.name),\n        credentials=(app.config['MAIL_USERNAME'], app.config['MAIL_PASSWORD'])\n    )\n    mail_handler.setLevel(logging.ERROR)\n    mail_handler.setFormatter(logging.Formatter(\n        '%(asctime)s - %(name)s - %(levelname)s: %(message)s '\n        '[in %(pathname)s:%(lineno)d]')\n    )\n    app.logger.addHandler(mail_handler)", "response": "Configure logging of error and exceptions to email."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef configure_coverage(app):\n    # setup chanjo report\n    app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = True if app.debug else False\n    if chanjo_api:\n        chanjo_api.init_app(app)\n        configure_template_filters(app)\n        # register chanjo report blueprint\n        app.register_blueprint(report_bp, url_prefix='/reports')\n\n    babel = Babel(app)\n\n    @babel.localeselector\n    def get_locale():\n        \"\"\"Determine locale to use for translations.\"\"\"\n        accept_languages = current_app.config.get('ACCEPT_LANGUAGES', ['en'])\n\n        # first check request args\n        session_language = request.args.get('lang')\n        if session_language in accept_languages:\n            current_app.logger.info(\"using session language: %s\", session_language)\n            return session_language\n\n        # language can be forced in config\n        user_language = current_app.config.get('REPORT_LANGUAGE')\n        if user_language:\n            return user_language\n\n        # try to guess the language from the user accept header that\n        # the browser transmits.  We support de/fr/en in this example.\n        # The best match wins.\n        return request.accept_languages.best_match(accept_languages)", "response": "Configure coverage related extensions."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef aliases(context, build, symbol):\n    LOG.info(\"Running scout view aliases\")\n    adapter = context.obj['adapter']\n    \n    if symbol:\n        alias_genes = {}\n        res = adapter.gene_by_alias(symbol, build=build)\n        for gene_obj in res:\n            hgnc_id = gene_obj['hgnc_id']\n            # Collect the true symbol given by hgnc\n            hgnc_symbol = gene_obj['hgnc_symbol']\n            # Loop aver all aliases\n            for alias in gene_obj['aliases']:\n                true_id = None\n                # If the alias is the same as hgnc symbol we know the true id\n                if alias == hgnc_symbol:\n                    true_id = hgnc_id\n                # If the alias is already in the list we add the id\n                if alias in alias_genes:\n                    alias_genes[alias]['ids'].add(hgnc_id)\n                    if true_id:\n                        alias_genes[alias]['true'] = hgnc_id\n                else:\n                    alias_genes[alias] = {\n                        'true': hgnc_id,\n                        'ids': set([hgnc_id])\n                    }\n\n    else:\n        alias_genes = adapter.genes_by_alias(build=build)\n    \n    if len(alias_genes) == 0:\n        LOG.info(\"No gene found for build %s\", build)\n        return\n\n    click.echo(\"#hgnc_symbol\\ttrue_id\\thgnc_ids\")\n    for alias_symbol in alias_genes:\n        info = alias_genes[alias_symbol]\n        # pp(info)\n        click.echo(\"{0}\\t{1}\\t{2}\\t\".format(\n            alias_symbol,\n            (alias_genes[alias_symbol]['true'] or 'None'),\n            ', '.join([str(gene_id) for gene_id in alias_genes[alias_symbol]['ids']])\n        )\n        )", "response": "Show all aliases and how they map to ids"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild a panel_gene object from a gene info dictionary.", "response": "def build_gene(gene_info, adapter):\n    \"\"\"Build a panel_gene object\n    \n    Args:\n        gene_info(dict)\n    \n    Returns:\n        gene_obj(dict)\n    \n        panel_gene = dict(\n            hgnc_id = int, # required\n            symbol = str, \n\n            disease_associated_transcripts = list, # list of strings that represent refseq transcripts\n            reduced_penetrance = bool, \n            mosaicism = bool, \n            database_entry_version = str,\n\n            ar = bool,\n            ad = bool,\n            mt = bool,\n            xr = bool,\n            xd = bool,\n            x = bool,\n            y = bool,\n        )\n\n    \"\"\"\n    symbol = gene_info.get('hgnc_symbol')\n    try:\n        # A gene has to have a hgnc id\n        hgnc_id = gene_info['hgnc_id']\n        if not hgnc_id:\n            raise KeyError()\n        gene_obj = dict(hgnc_id=hgnc_id)\n    except KeyError as err:\n        raise KeyError(\"Gene {0} is missing hgnc id. Panel genes has to have hgnc_id\".format(symbol))\n    \n    hgnc_gene = adapter.hgnc_gene(hgnc_id)\n    if hgnc_gene is None:\n        raise IntegrityError(\"hgnc_id {0} is not in the gene database!\".format(hgnc_id))\n    \n    gene_obj['symbol'] = hgnc_gene['hgnc_symbol']\n    if symbol != gene_obj['symbol']:\n        LOG.warning(\"Symbol in database does not correspond to symbol in panel file for gene %s\", hgnc_id)\n        LOG.warning(\"Using symbol %s for gene %s, instead of %s\" % \n                    (hgnc_gene['hgnc_symbol'], hgnc_id, symbol))\n\n    if gene_info.get('transcripts'):\n        gene_obj['disease_associated_transcripts'] = gene_info['transcripts']\n    \n    if gene_info.get('reduced_penetrance'):\n        gene_obj['reduced_penetrance'] = True\n    \n    if gene_info.get('mosaicism'):\n        gene_obj['mosaicism'] = True\n    \n    if gene_info.get('database_entry_version'):\n        gene_obj['database_entry_version'] = gene_info['database_entry_version']\n    \n    if gene_info.get('inheritance_models'):\n        for model in gene_info['inheritance_models']:\n            if model == 'AR':\n                gene_obj['ar'] = True\n            if model == 'AD':\n                gene_obj['ad'] = True\n            if model == 'MT':\n                gene_obj['mt'] = True\n            if model == 'XR':\n                gene_obj['xr'] = True\n            if model == 'XD':\n                gene_obj['xd'] = True\n            if model == 'X':\n                gene_obj['x'] = True\n            if model == 'Y':\n                gene_obj['y'] = True\n\n    return gene_obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding a panel object from a panel info dictionary.", "response": "def build_panel(panel_info, adapter):\n    \"\"\"Build a gene_panel object\n\n        Args:\n            panel_info(dict): A dictionary with panel information\n            adapter (scout.adapter.MongoAdapter)\n\n        Returns:\n            panel_obj(dict)\n\n    gene_panel = dict(\n        panel_id = str, # required\n        institute = str, # institute_id, required\n        version = float, # required\n        date = datetime, # required\n        display_name = str, # default is panel_name\n        genes = list, # list of panel genes, sorted on panel_gene['symbol']\n    )\n\n    \"\"\"\n    \n    panel_name = panel_info.get('panel_id', panel_info.get('panel_name'))\n    if not panel_name:\n        raise KeyError(\"Panel has to have a id\")\n    \n    panel_obj = dict(panel_name = panel_name)\n    LOG.info(\"Building panel with name: {0}\".format(panel_name))\n\n    try:\n        institute_id = panel_info['institute']\n    except KeyError as err:\n        raise KeyError(\"Panel has to have a institute\")\n\n    # Check if institute exists in database\n    if adapter.institute(institute_id) is None:\n        raise IntegrityError(\"Institute %s could not be found\" % institute_id)\n    \n    panel_obj['institute'] = panel_info['institute']\n    \n    panel_obj['version'] = float(panel_info['version'])\n    \n    try:\n        panel_obj['date'] = panel_info['date']\n    except KeyError as err:\n        raise KeyError(\"Panel has to have a date\")\n\n\n    panel_obj['display_name'] = panel_info.get('display_name', panel_obj['panel_name'])\n    \n    gene_objs = []\n    fail = False\n    for gene_info in panel_info.get('genes', []):\n        try:\n            gene_obj = build_gene(gene_info, adapter)\n            gene_objs.append(gene_obj)\n        except IntegrityError as err:\n            LOG.warning(err)\n            fail=True\n    \n    if fail:\n        raise IntegrityError(\"Some genes did not exist in database. Please see log messages.\")\n\n    panel_obj['genes'] = gene_objs\n\n    return panel_obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexports variants which have been verified for an institute id and write them to an Excel file.", "response": "def verified(context, collaborator, test, outpath=None):\n    \"\"\"Export variants which have been verified for an institute\n        and write them to an excel file.\n\n    Args:\n        collaborator(str): institute id\n        test(bool): True if the function is called for testing purposes\n        outpath(str): path to output file\n\n    Returns:\n        written_files(int): number of written or simulated files\n    \"\"\"\n    written_files = 0\n    collaborator = collaborator or 'cust000'\n    LOG.info('Exporting verified variants for cust {}'.format(collaborator))\n\n    adapter = context.obj['adapter']\n    verified_vars = adapter.verified(institute_id=collaborator)\n    LOG.info('FOUND {} verified variants for institute {}'.format(len(verified_vars), collaborator))\n\n\n    if not verified_vars:\n        LOG.warning('There are no verified variants for institute {} in database!'.format(collaborator))\n        return None\n\n    document_lines = export_verified_variants(verified_vars)\n\n    today = datetime.datetime.now().strftime('%Y-%m-%d')\n    document_name = '.'.join(['verified_variants', collaborator, today]) + '.xlsx'\n\n    # If this was a test and lines are created return success\n    if test and document_lines:\n        written_files +=1\n        LOG.info('Success. Verified variants file contains {} lines'.format(len(document_lines)))\n        return written_files\n\n    # create workbook and new sheet\n    # set up outfolder\n    if not outpath:\n        outpath = str(os.getcwd())\n    workbook = Workbook(os.path.join(outpath,document_name))\n    Report_Sheet = workbook.add_worksheet()\n\n    # Write the column header\n    row = 0\n    for col,field in enumerate(VERIFIED_VARIANTS_HEADER):\n        Report_Sheet.write(row,col,field)\n\n    # Write variant lines, after header (start at line 1)\n    for row, line in enumerate(document_lines,1): # each line becomes a row in the document\n        for col, field in enumerate(line): # each field in line becomes a cell\n            Report_Sheet.write(row,col,field)\n    workbook.close()\n\n    if os.path.exists(os.path.join(outpath,document_name)):\n        LOG.info('Success. Verified variants file of {} lines was written to disk'. format(len(document_lines)))\n        written_files += 1\n\n    return written_files"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexport causatives for a collaborator in. vcf format", "response": "def variants(context, collaborator, document_id, case_id, json):\n    \"\"\"Export causatives for a collaborator in .vcf format\"\"\"\n    LOG.info(\"Running scout export variants\")\n    adapter = context.obj['adapter']\n    collaborator = collaborator or 'cust000'\n\n    variants = export_variants(\n        adapter,\n        collaborator,\n        document_id=document_id,\n        case_id=case_id\n    )\n\n    if json:\n        click.echo(dumps([var for var in variants]))\n        return\n\n    vcf_header = VCF_HEADER\n\n    #If case_id is given, print more complete vcf entries, with INFO,\n    #and genotypes\n    if case_id:\n        vcf_header[-1] = vcf_header[-1] + \"\\tFORMAT\"\n        case_obj = adapter.case(case_id=case_id)\n        for individual in case_obj['individuals']:\n            vcf_header[-1] = vcf_header[-1] + \"\\t\" + individual['individual_id']\n\n    #print header\n    for line in vcf_header:\n        click.echo(line)\n\n    for variant_obj in variants:\n        variant_string = get_vcf_entry(variant_obj, case_id=case_id)\n        click.echo(variant_string)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets vcf entry from a variant object", "response": "def get_vcf_entry(variant_obj, case_id=None):\n    \"\"\"\n        Get vcf entry from variant object\n\n        Args:\n            variant_obj(dict)\n        Returns:\n            variant_string(str): string representing variant in vcf format\n    \"\"\"\n    if variant_obj['category'] == 'snv':\n        var_type = 'TYPE'\n    else:\n        var_type = 'SVTYPE'\n\n    info_field = ';'.join(\n            [\n                'END='+str(variant_obj['end']),\n                var_type+'='+variant_obj['sub_category'].upper()\n            ]\n        )\n\n    variant_string = \"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\".format(\n        variant_obj['chromosome'],\n        variant_obj['position'],\n        variant_obj['dbsnp_id'],\n        variant_obj['reference'],\n        variant_obj['alternative'],\n        variant_obj['quality'],\n        ';'.join(variant_obj['filters']),\n        info_field\n    )\n\n    if case_id:\n        variant_string += \"\\tGT\"\n        for sample in variant_obj['samples']:\n            variant_string += \"\\t\" + sample['genotype_call']\n\n    return variant_string"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef serve(context, config, host, port, debug, livereload):\n    pymongo_config = dict(\n        MONGO_HOST=context.obj['host'],\n        MONGO_PORT=context.obj['port'],\n        MONGO_DBNAME=context.obj['mongodb'],\n        MONGO_USERNAME=context.obj['username'],\n        MONGO_PASSWORD=context.obj['password'],\n    )\n    \n    valid_connection = check_connection(\n        host=pymongo_config['MONGO_HOST'], \n        port=pymongo_config['MONGO_PORT'], \n        username=pymongo_config['MONGO_USERNAME'], \n        password=pymongo_config['MONGO_PASSWORD'],\n        authdb=context.obj['authdb'],\n        )\n\n    log.info(\"Test if mongod is running\")\n    if not valid_connection:\n        log.warning(\"Connection could not be established\")\n        log.info(\"Is mongod running?\")\n        context.abort()\n    \n\n    config = os.path.abspath(config) if config else None\n    app = create_app(config=pymongo_config, config_file=config)\n    if livereload:\n        server = Server(app.wsgi_app)\n        server.serve(host=host, port=port, debug=debug)\n    else:\n        app.run(host=host, port=port, debug=debug)", "response": "Start the web server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_md5_key(list_of_arguments):\n    for arg in list_of_arguments:\n        if not isinstance(arg, string_types):\n            raise SyntaxError(\"Error in generate_md5_key: \"\n                              \"Argument: {0} is a {1}\".format(arg, type(arg)))\n\n    hash = hashlib.md5()\n    hash.update(' '.join(list_of_arguments).encode('utf-8'))\n    return hash.hexdigest()", "response": "Generate an md5 - key from a list of strings."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate indexes for the database", "response": "def index(context, update):\n    \"\"\"Create indexes for the database\"\"\"\n    LOG.info(\"Running scout index\")\n    adapter = context.obj['adapter']\n    \n    if update:\n        adapter.update_indexes()\n    else:\n        adapter.load_indexes()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef database(context, institute_name, user_name, user_mail, api_key):\n    LOG.info(\"Running scout setup database\")\n\n    # Fetch the omim information\n    api_key = api_key or context.obj.get('omim_api_key')\n    if not api_key:\n        LOG.warning(\"Please provide a omim api key with --api-key\")\n        context.abort()\n\n    institute_name = institute_name or context.obj['institute_name']\n    user_name = user_name or context.obj['user_name']\n    user_mail = user_mail or context.obj['user_mail']\n\n    adapter = context.obj['adapter']\n\n    LOG.info(\"Setting up database %s\", context.obj['mongodb'])\n    \n    setup_scout(\n        adapter=adapter,\n        institute_id=institute_name, \n        user_name=user_name, \n        user_mail = user_mail, \n        api_key=api_key\n    )", "response": "Setup a scout database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nshow all institutes in the database", "response": "def institutes(context, institute_id, json):\n    \"\"\"Show all institutes in the database\"\"\"\n    LOG.info(\"Running scout view institutes\")\n    adapter = context.obj['adapter']\n\n    if institute_id:\n        institute_objs = []\n        institute_obj = adapter.institute(institute_id)\n        if not institute_obj:\n            LOG.info(\"Institute %s does not exost\", institute_id)\n            return\n        institute_objs.append(institute_obj)\n    else:\n        institute_objs = [ins_obj for ins_obj in adapter.institutes()]\n    \n    if len(institute_objs) == 0:\n        click.echo(\"No institutes found\")\n        context.abort()\n\n    header = ''\n    if not json:\n        for key in institute_objs[0].keys():\n            header = header + \"{0}\\t\".format(key)\n        \n        click.echo(header)\n\n    for institute_obj in institute_objs:\n        if json:\n            click.echo(institute_obj)\n            continue\n\n        row = ''\n        for value in institute_obj.values():\n            row = row + \"{0}\\t\".format(value)\n\n        click.echo(row)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the genetic models entry of a vcf node", "response": "def parse_genetic_models(models_info, case_id):\n    \"\"\"Parse the genetic models entry of a vcf\n\n    Args:\n        models_info(str): The raw vcf information\n        case_id(str)\n\n    Returns:\n        genetic_models(list)\n\n    \"\"\"\n    genetic_models = []\n    if models_info:\n        for family_info in models_info.split(','):\n            splitted_info = family_info.split(':')\n            if splitted_info[0] == case_id:\n                genetic_models = splitted_info[1].split('|')\n\n    return genetic_models"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef panels(context, institute):\n    LOG.info(\"Running scout view panels\")\n    adapter = context.obj['adapter']\n\n    panel_objs = adapter.gene_panels(institute_id=institute)\n    if panel_objs.count() == 0:\n        LOG.info(\"No panels found\")\n        context.abort()\n    click.echo(\"#panel_name\\tversion\\tnr_genes\\tdate\")\n\n    for panel_obj in panel_objs:\n        click.echo(\"{0}\\t{1}\\t{2}\\t{3}\".format(\n            panel_obj['panel_name'],\n            str(panel_obj['version']),\n            len(panel_obj['genes']),\n            str(panel_obj['date'].strftime('%Y-%m-%d'))\n        ))", "response": "Show all gene panels in the database"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_institute(self, institute_obj):\n        internal_id = institute_obj['internal_id']\n        display_name = institute_obj['internal_id']\n\n        # Check if institute already exists\n        if self.institute(institute_id=internal_id):\n            raise IntegrityError(\"Institute {0} already exists in database\"\n                                 .format(display_name))\n\n        LOG.info(\"Adding institute with internal_id: {0} and \"\n                    \"display_name: {1}\".format(internal_id,\n                                               display_name))\n\n        insert_info = self.institute_collection.insert_one(institute_obj)\n        ##TODO check if insert info was ok\n        LOG.info(\"Institute saved\")", "response": "Adds a institute to the database."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_institute(self, internal_id, sanger_recipient=None, coverage_cutoff=None, \n                         frequency_cutoff=None, display_name=None, remove_sanger=None,\n                         phenotype_groups=None, group_abbreviations=None, add_groups=None):\n        \"\"\"Update the information for an institute\n\n        Args:\n            internal_id(str): The internal institute id\n            sanger_recipient(str): Email adress to add for sanger order\n            coverage_cutoff(int): Update coverage cutoff\n            frequency_cutoff(float): New frequency cutoff\n            display_name(str): New display name\n            remove_sanger(str): Email adress for sanger user to be removed\n            phenotype_groups(iterable(str)): New phenotype groups\n            group_abbreviations(iterable(str))\n            add_groups: If groups should be added. If False replace groups\n        \n        Returns:\n            updated_institute(dict)\n                     \n        \"\"\"\n        add_groups = add_groups or False\n        institute_obj = self.institute(internal_id)\n        if not institute_obj:\n            raise IntegrityError(\"Institute {} does not exist in database\".format(internal_id))\n        \n        updates = {}\n        updated_institute = institute_obj\n\n        if sanger_recipient:\n            user_obj = self.user(sanger_recipient)\n            if not user_obj:\n                raise IntegrityError(\"user {} does not exist in database\".format(sanger_recipient))\n                \n            LOG.info(\"Updating sanger recipients for institute: {0} with {1}\".format(\n                     internal_id, sanger_recipient))\n            updates['$push'] = {'sanger_recipients':remove_sanger}\n\n        if remove_sanger:\n            LOG.info(\"Removing sanger recipient {0} from institute: {1}\".format(\n                     remove_sanger, internal_id))\n            updates['$pull'] = {'sanger_recipients':remove_sanger}\n\n        if coverage_cutoff:\n            LOG.info(\"Updating coverage cutoff for institute: {0} to {1}\".format(\n                            internal_id, coverage_cutoff))\n            updates['$set'] = {'coverage_cutoff': coverage_cutoff}\n        \n        if frequency_cutoff:\n            LOG.info(\"Updating frequency cutoff for institute: {0} to {1}\".format(\n                            internal_id, frequency_cutoff))\n            if not '$set' in updates:\n                updates['$set'] = {}\n            updates['$set'] = {'frequency_cutoff': frequency_cutoff}\n\n        if display_name:\n            LOG.info(\"Updating display name for institute: {0} to {1}\".format(\n                            internal_id, display_name))\n            if not '$set' in updates:\n                updates['$set'] = {}\n            updates['$set'] = {'display_name': display_name}\n\n        if phenotype_groups:\n            if group_abbreviations:\n                group_abbreviations = list(group_abbreviations)\n            existing_groups = {}\n            if add_groups:\n                existing_groups = institute_obj.get('phenotype_groups', PHENOTYPE_GROUPS)\n\n            for i,hpo_term in enumerate(phenotype_groups):\n                hpo_obj = self.hpo_term(hpo_term)\n                if not hpo_obj:\n                    raise IntegrityError(\"Term {} does not exist\".format(hpo_term))\n                hpo_id = hpo_obj['hpo_id']\n                description = hpo_obj['description']\n                abbreviation = None\n                if group_abbreviations:\n                    abbreviation = group_abbreviations[i]\n                existing_groups[hpo_term] = {'name': description, 'abbr':abbreviation}\n            updates['$set'] = {'phenotype_groups': existing_groups}\n        \n        if updates:\n            if not '$set' in updates:\n                updates['$set'] = {}\n            \n            updates['$set']['updated_at'] = datetime.now()\n            \n            updated_institute = self.institute_collection.find_one_and_update(\n                {'_id':internal_id}, updates, return_document = pymongo.ReturnDocument.AFTER)\n            \n            LOG.info(\"Institute updated\")\n        \n        return updated_institute", "response": "Updates the information for an institute with the given id and sanger recipient."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef institutes(self, institute_ids=None):\n        query = {}\n        if institute_ids:\n            query['_id'] = {'$in': institute_ids}\n        LOG.debug(\"Fetching all institutes\")\n        return self.institute_collection.find(query)", "response": "Fetch all institutes.\n        \n        Args:\n            institute_ids(list(str))\n        \n        Returns:\n            res(pymongo.Cursor)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if a string is a valid date in the current language", "response": "def match_date(date):\n    \"\"\"Check if a string is a valid date\n\n        Args:\n            date(str)\n\n        Returns:\n            bool\n    \"\"\"\n    date_pattern = re.compile(\"^(19|20)\\d\\d[- /.](0[1-9]|1[012])[- /.](0[1-9]|[12][0-9]|3[01])\")\n    if re.match(date_pattern, date):\n        return True\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_date(date, date_format = None):\n    date_obj = datetime.datetime.now()\n    if date:\n        if date_format:\n            date_obj = datetime.datetime.strptime(date, date_format)\n        else:\n            if match_date(date):\n                if len(date.split('-')) == 3:\n                    date = date.split('-')\n                elif len(date.split(' ')) == 3:\n                    date = date.split(' ')\n                elif len(date.split('.')) == 3:\n                    date = date.split('.')\n                else:\n                    date = date.split('/')\n                date_obj = datetime.datetime(*(int(number) for number in date))\n            else:\n                raise ValueError(\"Date %s is invalid\" % date)\n\n    return date_obj", "response": "Return a datetime object if there is a valid date"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hpo_genes(context, hpo_term):\n    LOG.info(\"Running scout export hpo_genes\")\n    adapter = context.obj['adapter']\n    \n    header = [\"#Gene_id\\tCount\"]\n\n    if not hpo_term:\n        LOG.warning(\"Please use at least one hpo term\")\n        context.abort()\n\n    for line in header:\n        click.echo(line)\n\n    for term in adapter.generate_hpo_gene_list(*hpo_term):\n        click.echo(\"{0}\\t{1}\".format(term[0], term[1]))", "response": "Export a list of genes based on hpo terms"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the transcript information and get the gene information from there.", "response": "def parse_genes(transcripts):\n    \"\"\"Parse transcript information and get the gene information from there.\n    \n    Use hgnc_id as identifier for genes and ensembl transcript id to identify transcripts\n    \n    Args:\n        transcripts(iterable(dict))\n\n    Returns:\n      genes (list(dict)): A list with dictionaries that represents genes\n    \n    \"\"\"\n    # Dictionary to group the transcripts by hgnc_id\n    genes_to_transcripts = {}\n    \n    # List with all genes and there transcripts\n    genes = []\n\n    hgvs_identifier = None\n    canonical_transcript = None\n    exon = None\n    # Group all transcripts by gene\n    for transcript in transcripts:\n        # Check what hgnc_id a transcript belongs to\n        hgnc_id = transcript['hgnc_id']\n        hgnc_symbol = transcript['hgnc_symbol']\n\n        if (transcript['is_canonical'] and transcript.get('coding_sequence_name')):\n            hgvs_identifier = transcript.get('coding_sequence_name')\n            canonical_transcript = transcript['transcript_id']\n            exon = transcript['exon']\n\n        # If there is a identifier we group the transcripts under gene\n        if hgnc_id:\n            if hgnc_id in genes_to_transcripts:\n                genes_to_transcripts[hgnc_id].append(transcript)\n            else:\n                genes_to_transcripts[hgnc_id] = [transcript]\n        else:\n            if hgnc_symbol:\n                if hgnc_symbol in genes_to_transcripts:\n                    genes_to_transcripts[hgnc_symbol].append(transcript)\n                else:\n                    genes_to_transcripts[hgnc_symbol] = [transcript]\n\n    # We need to find out the most severe consequence in all transcripts\n    # and save in what transcript we found it\n    \n    # Loop over all genes\n    for gene_id in genes_to_transcripts:\n        # Get the transcripts for a gene\n        gene_transcripts = genes_to_transcripts[gene_id]\n        # This will be a consequece from SO_TERMS\n        most_severe_consequence = None\n        # Set the most severe score to infinity\n        most_severe_rank = float('inf')\n        # The most_severe_transcript is a dict\n        most_severe_transcript = None\n        \n        most_severe_region = None\n        \n        most_severe_sift = None\n        most_severe_polyphen = None\n        \n        # Loop over all transcripts for a gene to check which is most severe\n        for transcript in gene_transcripts:\n            hgnc_id = transcript['hgnc_id']\n            hgnc_symbol = transcript['hgnc_symbol']\n            # Loop over the consequences for a transcript\n            for consequence in transcript['functional_annotations']:\n                # Get the rank based on SO_TERM\n                # Lower rank is worse\n                new_rank = SO_TERMS[consequence]['rank']\n                \n                if new_rank < most_severe_rank:\n                    # If a worse consequence is found, update the parameters\n                    most_severe_rank = new_rank\n                    most_severe_consequence = consequence\n                    most_severe_transcript = transcript\n                    most_severe_sift = transcript['sift_prediction']\n                    most_severe_polyphen = transcript['polyphen_prediction']\n                    most_severe_region = SO_TERMS[consequence]['region']\n\n        gene = {\n            'transcripts': gene_transcripts,\n            'most_severe_transcript': most_severe_transcript,\n            'most_severe_consequence': most_severe_consequence,\n            'most_severe_sift': most_severe_sift,\n            'most_severe_polyphen': most_severe_polyphen,\n            'hgnc_id': hgnc_id,\n            'hgnc_symbol': hgnc_symbol,\n            'region_annotation': most_severe_region,\n            'hgvs_identifier': transcript['coding_sequence_name'],\n            'canonical_transcript': transcript['transcript_id'],\n            'exon': transcript['exon'],\n        }\n        genes.append(gene)    \n\n    return genes"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_rank_score(rank_score_entry, case_id):\n    rank_score = None\n    if rank_score_entry:\n        for family_info in rank_score_entry.split(','):\n            splitted_info = family_info.split(':')\n            if case_id == splitted_info[0]:\n                rank_score = float(splitted_info[1])\n    return rank_score", "response": "Parse the rank score entry into a single node."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a user to the database.", "response": "def user(context, institute_id, user_name, user_mail, admin):\n    \"\"\"Add a user to the database.\"\"\"\n    adapter = context.obj['adapter']\n\n    institutes = []\n    for institute in institute_id:\n        institute_obj = adapter.institute(institute_id=institute)\n\n        if not institute_obj:\n            LOG.warning(\"Institute % does not exist\", institute)\n            context.abort()\n\n        institutes.append(institute)\n\n    roles = []\n    if admin:\n        LOG.info(\"User is admin\")\n        roles.append('admin')\n\n    user_info = dict(email=user_mail.lower(), name=user_name, roles=roles, institutes=institutes)\n\n    user_obj = build_user(user_info)\n\n    try:\n        adapter.add_user(user_obj)\n    except Exception as err:\n        LOG.warning(err)\n        context.abort()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the transcript information from a list of variants into a single dictionary with transcript information", "response": "def parse_transcripts(raw_transcripts, allele=None):\n    \"\"\"Parse transcript information from VCF variants\n\n    Args:\n        raw_transcripts(iterable(dict)): An iterable with raw transcript \n                                         information\n\n    Yields:\n        transcript(dict) A dictionary with transcript information\n    \"\"\"\n    for entry in raw_transcripts:\n        transcript = {}\n        # There can be several functional annotations for one variant\n        functional_annotations = entry.get('CONSEQUENCE', '').split('&')\n        transcript['functional_annotations'] = functional_annotations\n        # Get the transcript id (ensembl gene id)\n        transcript_id = entry.get('FEATURE', '').split(':')[0]\n        transcript['transcript_id'] = transcript_id\n\n        # Add the hgnc gene identifiers\n        # The HGNC ID is prefered and will be used if it exists\n        hgnc_id = entry.get('HGNC_ID')\n        if hgnc_id:\n            hgnc_id = hgnc_id.split(':')[-1]\n            transcript['hgnc_id'] = int(hgnc_id)\n        else:\n            transcript['hgnc_id'] = None\n\n        hgnc_symbol = entry.get('SYMBOL')\n        if hgnc_symbol:\n            transcript['hgnc_symbol'] = hgnc_symbol\n        else:\n            transcript['hgnc_symbol'] = None\n\n        ########### Fill it with the available information ###########\n\n        ### Protein specific annotations ###\n\n        ## Protein ID ##\n        transcript['protein_id'] = entry.get('ENSP')\n        \n        ## Polyphen prediction ##\n        polyphen_prediction = entry.get('POLYPHEN')\n        # Default is 'unknown'\n        prediction_term = 'unknown'\n        if polyphen_prediction:\n            prediction_term = polyphen_prediction.split('(')[0]\n        transcript['polyphen_prediction'] = prediction_term\n\n        ## Sift prediction ##\n        # Check with other key if it does not exist\n        sift_prediction = entry.get('SIFT')\n        # Default is 'unknown'\n        prediction_term = 'unknown'\n        \n        if not sift_prediction:\n            sift_prediction = entry.get('SIFT_PRED')\n            \n        if sift_prediction:\n            prediction_term = sift_prediction.split('(')[0]\n\n        transcript['sift_prediction'] = prediction_term\n        \n        transcript['swiss_prot'] = entry.get('SWISSPROT') or 'unknown'\n\n        if entry.get('DOMAINS', None):\n            pfam_domains = entry['DOMAINS'].split('&')\n\n            for annotation in pfam_domains:\n                annotation = annotation.split(':')\n                domain_name = annotation[0]\n                domain_id = annotation[1]\n                if domain_name == 'Pfam_domain':\n                    transcript['pfam_domain'] = domain_id\n                elif domain_name == 'PROSITE_profiles':\n                    transcript['prosite_profile'] = domain_id\n                elif domain_name == 'SMART_domains':\n                    transcript['smart_domain'] = domain_id\n\n        coding_sequence_entry = entry.get('HGVSC', '').split(':')\n        protein_sequence_entry = entry.get('HGVSP', '').split(':')\n\n        coding_sequence_name = None\n        if len(coding_sequence_entry) > 1:\n            coding_sequence_name = coding_sequence_entry[-1]\n\n        transcript['coding_sequence_name'] = coding_sequence_name\n\n        protein_sequence_name = None\n        if len(protein_sequence_entry) > 1:\n            protein_sequence_name = protein_sequence_entry[-1]\n\n        transcript['protein_sequence_name'] = protein_sequence_name\n\n        transcript['biotype'] = entry.get('BIOTYPE')\n\n        transcript['exon'] = entry.get('EXON')\n        transcript['intron'] = entry.get('INTRON')\n\n        if entry.get('STRAND'):\n            if entry['STRAND'] == '1':\n                transcript['strand'] = '+'\n            elif entry['STRAND'] == '-1':\n                transcript['strand'] = '-'\n        else:\n            transcript['strand'] = None\n\n        functional = []\n        regional = []\n        for annotation in functional_annotations:\n            functional.append(annotation)\n            regional.append(SO_TERMS[annotation]['region'])\n\n        transcript['functional_annotations'] = functional\n        transcript['region_annotations'] = regional\n\n        # Check if the transcript is marked cannonical by vep\n        transcript['is_canonical'] = (entry.get('CANONICAL') == 'YES')\n        \n        # Check if the CADD score is available on transcript level\n        cadd_phred = entry.get('CADD_PHRED')\n        if cadd_phred:\n            transcript['cadd'] = float(cadd_phred)\n\n        # Check frequencies\n        # There are different keys for different versions of VEP\n        # We only support version 90+\n        thousandg_freqs = []\n        gnomad_freqs = []\n        try:\n            # The keys for VEP v90+:\n            # 'AF' or '1000GAF' - 1000G all populations combined\n            # 'xxx_AF' - 1000G (or NHLBI-ESP) individual populations\n            # 'gnomAD_AF' - gnomAD exomes, all populations combined\n            # 'gnomAD_xxx_AF' - gnomAD exomes, individual populations\n            # 'MAX_AF' - Max of all populations (1000G, gnomAD exomes, ESP)\n            # https://www.ensembl.org/info/docs/tools/vep/vep_formats.html\n            \n            # Loop over all keys to find frequency entries\n            for key in entry:\n                #All frequencies endswith AF\n                if not key.endswith('AF'):\n                    continue\n                \n                value = entry[key]\n                if not value:\n                    continue\n                \n                # This is the 1000G max af information\n                if (key == 'AF' or key == '1000GAF'):\n                    transcript['thousand_g_maf'] = float(value)\n                    continue\n                \n                if key == 'GNOMAD_AF':\n                    transcript['gnomad_maf'] = float(value)\n                    continue\n\n                if key == 'EXAC_MAX_AF':\n                    transcript['exac_max'] = float(value)\n                    transcript['exac_maf'] = float(value)\n                    continue\n                \n                if 'GNOMAD' in key:\n                    gnomad_freqs.append(float(value))\n                \n                else:\n                    thousandg_freqs.append(float(value))\n                \n            if thousandg_freqs:\n                transcript['thousandg_max'] = max(thousandg_freqs)\n\n            if gnomad_freqs:\n                transcript['gnomad_max'] = max(gnomad_freqs)\n            \n        except Exception as err:\n            LOG.debug(\"Something went wrong when parsing frequencies\")\n            LOG.debug(\"Only splitted and normalised VEP v90+ is supported\")\n\n        clinsig = entry.get('CLIN_SIG')\n        if clinsig:\n            transcript['clinsig'] = clinsig.split('&')\n\n        transcript['dbsnp'] = []\n        transcript['cosmic'] = []\n        variant_ids = entry.get('EXISTING_VARIATION')\n        if variant_ids:\n            for variant_id in variant_ids.split('&'):\n                if variant_id.startswith('rs'):\n                    transcript['dbsnp'].append(variant_id)\n                elif variant_id.startswith('COSM'):\n                    transcript['cosmic'].append(int(variant_id[4:]))\n\n        yield transcript"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if a connection to the mongo process specified", "response": "def check_connection(host='localhost', port=27017, username=None, password=None,\n                     authdb=None, max_delay=1):\n    \"\"\"Check if a connection could be made to the mongo process specified\n\n    Args:\n        host(str)\n        port(int)\n        username(str)\n        password(str)\n        authdb (str): database to to for authentication\n        max_delay(int): Number of milliseconds to wait for connection\n\n    Returns:\n        bool: If connection could be established\n    \"\"\"\n    #uri looks like:\n    #mongodb://[username:password@]host1[:port1][,host2[:port2],...[,hostN[:portN]]][/[database][?options]]\n    if username and password:\n        uri = (\"mongodb://{}:{}@{}:{}/{}\"\n               .format(quote_plus(username), quote_plus(password), host, port, authdb))\n        log_uri = (\"mongodb://{}:****@{}:{}/{}\"\n               .format(quote_plus(username), host, port, authdb))\n    else:\n        log_uri = uri = \"mongodb://%s:%s\" % (host, port)\n    \n    LOG.info(\"Test connection with uri: %s\", log_uri)\n    client = MongoClient(uri, serverSelectionTimeoutMS=max_delay)\n    try:\n        client.server_info()\n    except (ServerSelectionTimeoutError,OperationFailure) as err:\n        LOG.warning(err)\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninitializes from flask app", "response": "def init_app(self, app):\n        \"\"\"Initialize from flask\"\"\"\n        uri = app.config.get(\"MONGO_URI\", None)\n        \n        db_name = app.config.get(\"MONGO_DBNAME\", 'scout')\n        \n        try:\n            client = get_connection(\n                host = app.config.get(\"MONGO_HOST\", 'localhost'),\n                port=app.config.get(\"MONGO_PORT\", 27017), \n                username=app.config.get(\"MONGO_USERNAME\", None), \n                password=app.config.get(\"MONGO_PASSWORD\", None),\n                uri=uri, \n                mongodb= db_name\n            )\n        except ConnectionFailure:\n            context.abort()\n\n        app.config[\"MONGO_DATABASE\"] = client[db_name]\n\n        app.config['MONGO_CLIENT'] = client"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef institutes():\n    institute_objs = user_institutes(store, current_user)\n    institutes = []\n    for ins_obj in institute_objs:\n        sanger_recipients = []\n        for user_mail in ins_obj.get('sanger_recipients',[]):\n            user_obj = store.user(user_mail)\n            if not user_obj:\n                continue\n            sanger_recipients.append(user_obj['name'])\n        institutes.append(\n            {\n                'display_name': ins_obj['display_name'],\n                'internal_id': ins_obj['_id'],\n                'coverage_cutoff': ins_obj.get('coverage_cutoff', 'None'),\n                'sanger_recipients': sanger_recipients,\n                'frequency_cutoff': ins_obj.get('frequency_cutoff', 'None'),\n                'phenotype_groups': ins_obj.get('phenotype_groups', PHENOTYPE_GROUPS)\n            }\n        )\n\n    data = dict(institutes=institutes)\n    return render_template(\n        'overview/institutes.html', **data)", "response": "Display a list of all user institutes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload a delivery report into a case in the database.", "response": "def load_delivery_report(adapter: MongoAdapter,\n                         report_path: str,\n                         case_id: str,\n                         update: bool = False):\n    \"\"\" Load a delivery report into a case in the database\n\n    If the report already exists the function will exit.\n    If the user want to load a report that is already in the database\n    'update' has to be 'True'\n\n    Args:\n        adapter     (MongoAdapter): Connection to the database\n        report_path (string):       Path to delivery report\n        case_id     (string):       Optional case identifier\n        update      (bool):         If an existing report should be replaced\n        \n    Returns:\n        updated_case(dict)\n\n    \"\"\"\n\n    case_obj = adapter.case(\n        case_id=case_id,\n    )\n\n    if case_obj is None:\n        raise DataNotFoundError(\"no case found\")\n\n    if not case_obj.get('delivery_report'):\n        _put_report_in_case_root(case_obj, report_path)\n    else:\n        if update:\n            _put_report_in_case_root(case_obj, report_path)\n        else:\n            raise IntegrityError('Existing delivery report found, use update = True to '\n                                 'overwrite')\n\n    logger.info('Saving report for case {} in database'.format(case_obj['_id']))\n    return adapter.replace_case(case_obj)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild a new transcript object from a parsed transcript information.", "response": "def build_transcript(transcript, build='37'):\n    \"\"\"Build a transcript object\n    \n    These represents the transcripts that are parsed from the VCF, not\n    the transcript definitions that are collected from ensembl.\n    \n        Args:\n            transcript(dict): Parsed transcript information\n    \n        Returns:\n            transcript_obj(dict)\n    \"\"\"\n    \n    # Transcripts has to have an id\n    transcript_id = transcript['transcript_id']\n    transcript_obj = dict(\n        transcript_id = transcript_id\n    )\n    \n    # Transcripts has to belong to a gene\n    transcript_obj['hgnc_id'] = transcript['hgnc_id']\n\n    if transcript.get('protein_id'):\n        transcript_obj['protein_id'] = transcript['protein_id']\n    \n    if transcript.get('sift_prediction'):\n        transcript_obj['sift_prediction'] = transcript['sift_prediction']\n    \n    if transcript.get('polyphen_prediction'):\n        transcript_obj['polyphen_prediction'] = transcript['polyphen_prediction']\n\n\n    if transcript.get('swiss_prot'):\n        transcript_obj['swiss_prot'] = transcript['swiss_prot']\n    \n    if transcript.get('pfam_domain'):\n        transcript_obj['pfam_domain'] = transcript.get('pfam_domain')\n    \n    if transcript.get('prosite_profile'):\n        transcript_obj['prosite_profile'] = transcript.get('prosite_profile')\n    \n    if transcript.get('smart_domain'):\n        transcript_obj['smart_domain'] = transcript.get('smart_domain')\n\n    if transcript.get('biotype'):\n        transcript_obj['biotype'] = transcript.get('biotype')\n\n    if transcript.get('functional_annotations'):\n        transcript_obj['functional_annotations'] = transcript['functional_annotations']\n    \n    if transcript.get('region_annotations'):\n        transcript_obj['region_annotations'] = transcript['region_annotations']\n\n    if transcript.get('exon'):\n        transcript_obj['exon'] = transcript.get('exon')\n    \n    if transcript.get('intron'):\n        transcript_obj['intron'] = transcript.get('intron')\n    \n    if transcript.get('strand'):\n        transcript_obj['strand'] = transcript.get('strand')\n\n    if transcript.get('coding_sequence_name'):\n        transcript_obj['coding_sequence_name'] = transcript['coding_sequence_name']\n    \n    if transcript.get('protein_sequence_name'):\n        transcript_obj['protein_sequence_name'] = transcript['protein_sequence_name']\n\n    transcript_obj['is_canonical'] = transcript.get('is_canonical', False)\n\n    return transcript_obj"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating an existing user.", "response": "def update_user(self, user_obj):\n        \"\"\"Update an existing user.\n        \n        \n            Args:\n                user_obj(dict)\n        \n            Returns:\n                updated_user(dict)\n        \"\"\"\n        LOG.info(\"Updating user %s\", user_obj['_id'])\n        updated_user = self.user_collection.find_one_and_replace(\n            {'_id': user_obj['_id']},\n            user_obj,\n            return_document=pymongo.ReturnDocument.AFTER\n        )\n        return updated_user"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a user object to the database.", "response": "def add_user(self, user_obj):\n        \"\"\"Add a user object to the database\n\n            Args:\n                user_obj(scout.models.User): A dictionary with user information\n        \n            Returns:\n                user_info(dict): a copy of what was inserted\n        \"\"\"\n        LOG.info(\"Adding user %s to the database\", user_obj['email'])\n        if not '_id' in user_obj:\n            user_obj['_id'] = user_obj['email']\n    \n        try:\n            self.user_collection.insert_one(user_obj)\n            LOG.debug(\"User inserted\")\n        except DuplicateKeyError as err:\n            raise IntegrityError(\"User {} already exists in database\".format(user_obj['email']))\n\n        return user_obj"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef users(self, institute=None):\n        query = {}\n        if institute:\n            LOG.info(\"Fetching all users from institute %s\", institute)\n            query = {'institutes': {'$in': [institute]}}\n        else:\n            LOG.info(\"Fetching all users\")\n            \n        res = self.user_collection.find(query)\n        return res", "response": "Returns all users from the database"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef user(self, email):\n        LOG.info(\"Fetching user %s\", email)\n        user_obj = self.user_collection.find_one({'_id': email})\n\n        return user_obj", "response": "Fetch a user from the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes a user from the database", "response": "def delete_user(self, email):\n        \"\"\"Delete a user from the database\n        \n        Args:\n            email(str)\n    \n        Returns:\n            user_obj(dict)\n        \n        \"\"\"\n        LOG.info(\"Deleting user %s\", email)\n        user_obj = self.user_collection.delete_one({'_id': email})\n        \n        return user_obj"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_compound(compound):\n    \n    compound_obj = dict(\n        variant = compound['variant'],\n        display_name = compound['display_name'],\n        combined_score = float(compound['score'])\n    )\n    \n    return compound_obj", "response": "Builds a compound object from a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remote_static():\n    file_path = request.args.get('file')\n\n    range_header = request.headers.get('Range', None)\n    if not range_header and file_path.endswith('.bam'):\n        return abort(500)\n\n    new_resp = send_file_partial(file_path)\n    return new_resp", "response": "Stream large static files with special requirements."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef igv():\n    chrom = request.args.get('contig')\n    if chrom == 'MT':\n        chrom = 'M'\n\n    start = request.args.get('start')\n    stop = request.args.get('stop')\n\n    locus = \"chr{0}:{1}-{2}\".format(chrom,start,stop)\n    LOG.debug('Displaying locus %s', locus)\n\n    chromosome_build = request.args.get('build')\n    LOG.debug('Chromosome build is %s', chromosome_build)\n\n    samples = request.args.getlist('sample')\n    bam_files = request.args.getlist('bam')\n    bai_files = request.args.getlist('bai')\n    LOG.debug('loading the following tracks: %s', bam_files)\n\n    display_obj={}\n\n    # Add chromosome build info to the track object\n    fastaURL = ''\n    indexURL = ''\n    cytobandURL = ''\n    gene_track_format = ''\n    gene_track_URL = ''\n    gene_track_indexURL = ''\n\n    if chromosome_build == \"GRCh38\" or chrom == 'M':\n        fastaURL = 'https://s3.amazonaws.com/igv.broadinstitute.org/genomes/seq/hg38/hg38.fa'\n        indexURL = 'https://s3.amazonaws.com/igv.broadinstitute.org/genomes/seq/hg38/hg38.fa.fai'\n        cytobandURL = 'https://s3.amazonaws.com/igv.broadinstitute.org/annotations/hg38/cytoBandIdeo.txt'\n        gene_track_format = 'gtf'\n        gene_track_URL = 'https://s3.amazonaws.com/igv.broadinstitute.org/annotations/hg38/genes/Homo_sapiens.GRCh38.80.sorted.gtf.gz'\n        gene_track_indexURL = 'https://s3.amazonaws.com/igv.broadinstitute.org/annotations/hg38/genes/Homo_sapiens.GRCh38.80.sorted.gtf.gz.tbi'\n\n    else:\n        fastaURL = 'https://s3.amazonaws.com/igv.broadinstitute.org/genomes/seq/hg19/hg19.fasta'\n        indexURL = 'https://s3.amazonaws.com/igv.broadinstitute.org/genomes/seq/hg19/hg19.fasta.fai'\n        cytobandURL = 'https://s3.amazonaws.com/igv.broadinstitute.org/genomes/seq/hg19/cytoBand.txt'\n        gene_track_format = 'bed'\n        gene_track_URL = 'https://s3.amazonaws.com/igv.broadinstitute.org/annotations/hg19/genes/refGene.hg19.bed.gz'\n        gene_track_indexURL = 'https://s3.amazonaws.com/igv.broadinstitute.org/annotations/hg19/genes/refGene.hg19.bed.gz.tbi'\n\n    display_obj['reference_track'] = {\n        'fastaURL' : fastaURL,\n        'indexURL' : indexURL,\n        'cytobandURL' : cytobandURL\n    }\n\n    display_obj['genes_track'] = {\n        'name' : 'Genes',\n        'type' : 'annotation',\n        'format': gene_track_format,\n        'sourceType': 'file',\n        'url' : gene_track_URL,\n        'indexURL' : gene_track_indexURL,\n        'displayMode' : 'EXPANDED'\n    }\n\n    sample_tracks = []\n    counter = 0\n    for sample in samples:\n        # some samples might not have an associated bam file, take care if this\n        if bam_files[counter]:\n            sample_tracks.append({ 'name' : sample, 'url' : bam_files[counter],\n                                   'indexURL' : bai_files[counter],\n                                   'height' : 700, 'maxHeight' : 2000})\n        counter += 1\n\n    display_obj['sample_tracks'] = sample_tracks\n\n    if request.args.get('center_guide'):\n        display_obj['display_center_guide'] = True\n    else:\n        display_obj['display_center_guide'] = False\n\n    return render_template('alignviewers/igv_viewer.html', locus=locus, **display_obj )", "response": "Visualize BAM alignments using igv. js."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild a disease term object from a dictionary with phenotype information.", "response": "def build_disease_term(disease_info, alias_genes={}):\n    \"\"\"Build a disease phenotype object\n    \n    Args:\n        disease_info(dict): Dictionary with phenotype information\n        alias_genes(dict): {\n                    <alias_symbol>: {\n                                        'true': hgnc_id or None,\n                                        'ids': [<hgnc_id>, ...]}}\n    \n    Returns:\n        disease_obj(dict): Formated for mongodb\n        \n        disease_term = dict(\n            _id = str, # Same as disease_id\n            disease_id = str, # required, like OMIM:600233\n            disase_nr = int, # The disease nr, required\n            description = str, # required\n            source = str, # required\n            inheritance = list, # list of strings\n            genes = list, # List with integers that are hgnc_ids \n            hpo_terms = list, # List with str that are hpo_terms \n        )\n        \n    \"\"\"\n    \n    try:\n        disease_nr = int(disease_info['mim_number'])\n    except KeyError:\n        raise KeyError(\"Diseases has to have a disease number\")\n    except ValueError:\n        raise KeyError(\"Diseases nr has to be integer\")\n\n    disease_id = \"{0}:{1}\".format('OMIM', disease_nr)\n\n    LOG.debug(\"Building disease term %s\", disease_id)\n\n    try:\n        description = disease_info['description']\n    except KeyError:\n        raise KeyError(\"Diseases has to have a description\")\n    \n    disease_obj = DiseaseTerm(\n        disease_id=disease_id, \n        disease_nr=disease_nr, \n        description=description, \n        source='OMIM'\n    )\n\n    # Check if there where any inheritance information\n    inheritance_models = disease_info.get('inheritance')\n    if inheritance_models:\n        disease_obj['inheritance'] = list(inheritance_models)\n\n    hgnc_ids = set()\n    for hgnc_symbol in disease_info.get('hgnc_symbols', []):\n        ## TODO need to consider genome build here?\n        if hgnc_symbol in alias_genes:\n            # If the symbol identifies a unique gene we add that\n            if alias_genes[hgnc_symbol]['true']:\n                hgnc_ids.add(alias_genes[hgnc_symbol]['true'])\n            else:\n                for hgnc_id in alias_genes[hgnc_symbol]['ids']:\n                    hgnc_ids.add(hgnc_id)\n        else:\n            LOG.debug(\"Gene symbol %s could not be found in database\", hgnc_symbol)\n\n    disease_obj['genes'] = list(hgnc_ids)\n    \n    if 'hpo_terms' in disease_info:\n        disease_obj['hpo_terms'] = list(disease_info['hpo_terms'])\n    \n    return disease_obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_exons(adapter, exon_lines, build='37', ensembl_genes=None):\n    # Fetch all genes with ensemblid as keys\n    ensembl_genes = ensembl_genes or adapter.ensembl_genes(build)\n    hgnc_id_transcripts = adapter.id_transcripts_by_gene(build=build)\n    \n    if isinstance(exon_lines, DataFrame):\n        exons = parse_ensembl_exon_request(exon_lines)\n        nr_exons = exon_lines.shape[0]\n    else:\n        exons = parse_ensembl_exons(exon_lines)\n        nr_exons = 1000000\n    \n    start_insertion = datetime.now()\n    loaded_exons = 0\n    LOG.info(\"Loading exons...\")\n    with progressbar(exons, label=\"Loading exons\", length=nr_exons) as bar:\n        for exon in bar:\n            ensg_id = exon['gene']\n            enst_id = exon['transcript']\n            gene_obj = ensembl_genes.get(ensg_id)\n            \n            if not gene_obj:\n                continue\n            \n            hgnc_id = gene_obj['hgnc_id']\n\n            if not enst_id in hgnc_id_transcripts[hgnc_id]:\n                continue\n\n            exon['hgnc_id'] = hgnc_id\n\n            exon_obj = build_exon(exon, build)\n            adapter.load_exon(exon_obj)\n            loaded_exons += 1\n\n    LOG.info('Number of exons in build {0}: {1}'.format(build, nr_exons))\n    LOG.info('Number loaded: {0}'.format(loaded_exons))\n    LOG.info('Time to load exons: {0}'.format(datetime.now() - start_insertion))", "response": "Load all the exons from the given list of lines into the database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a single variant and return a dictionary with the parsed variant information.", "response": "def parse_variant(variant, case, variant_type='clinical',\n                 rank_results_header=None, vep_header=None,\n                 individual_positions=None, category=None):\n    \"\"\"Return a parsed variant\n\n        Get all the necessary information to build a variant object\n\n    Args:\n        variant(cyvcf2.Variant)\n        case(dict)\n        variant_type(str): 'clinical' or 'research'\n        rank_results_header(list)\n        vep_header(list)\n        individual_positions(dict): Explain what position each individual has\n                                    in vcf\n        category(str): 'snv', 'sv', 'str' or 'cancer'\n\n    Returns:\n        parsed_variant(dict): Parsed variant\n    \"\"\"\n    # These are to display how the rank score is built\n    rank_results_header = rank_results_header or []\n    # Vep information\n    vep_header = vep_header or []\n\n    parsed_variant = {}\n\n    # Create the ID for the variant\n    case_id = case['_id']\n    if '-' in case_id:\n        logger.debug('internal case id detected')\n        genmod_key = case['display_name']\n    else:\n        genmod_key = case['_id']\n\n    chrom_match = CHR_PATTERN.match(variant.CHROM)\n    chrom = chrom_match.group(2)\n\n    # Builds a dictionary with the different ids that are used\n\n    if variant.ALT:\n        alt=variant.ALT[0]\n    elif not variant.ALT and category == \"str\":\n        alt='.'\n\n    parsed_variant['ids'] = parse_ids(\n        chrom=chrom,\n        pos=variant.POS,\n        ref=variant.REF,\n        alt=alt,\n        case_id=case_id,\n        variant_type=variant_type,\n    )\n    parsed_variant['case_id'] = case_id\n    # type can be 'clinical' or 'research'\n    parsed_variant['variant_type'] = variant_type\n    # category is sv or snv\n    # cyvcf2 knows if it is a sv, indel or snv variant\n    if not category:\n        category = variant.var_type\n        if category == 'indel':\n            category = 'snv'\n        if category == 'snp':\n            category = 'snv'\n\n    parsed_variant['category'] = category\n\n    ################# General information #################\n\n    parsed_variant['reference'] = variant.REF\n\n    ### We allways assume splitted and normalized vcfs!!!\n    if len(variant.ALT) > 1:\n        raise VcfError(\"Variants are only allowed to have one alternative\")\n    parsed_variant['alternative'] = alt\n\n    # cyvcf2 will set QUAL to None if '.' in vcf\n    parsed_variant['quality'] = variant.QUAL\n\n    if variant.FILTER:\n        parsed_variant['filters'] = variant.FILTER.split(';')\n    else:\n        parsed_variant['filters'] = ['PASS']\n\n    # Add the dbsnp ids\n    parsed_variant['dbsnp_id'] = variant.ID\n\n    # This is the id of other position in translocations\n    # (only for specific svs)\n    parsed_variant['mate_id'] = None\n\n    ################# Position specific #################\n    parsed_variant['chromosome'] = chrom\n\n    coordinates = parse_coordinates(variant, category)\n\n    parsed_variant['position'] = coordinates['position']\n    parsed_variant['sub_category'] = coordinates['sub_category']\n    parsed_variant['mate_id'] = coordinates['mate_id']\n    parsed_variant['end'] = coordinates['end']\n    parsed_variant['length'] = coordinates['length']\n    parsed_variant['end_chrom'] = coordinates['end_chrom']\n    parsed_variant['cytoband_start'] = coordinates['cytoband_start']\n    parsed_variant['cytoband_end'] = coordinates['cytoband_end']\n\n    ################# Add rank score #################\n    # The rank score is central for displaying variants in scout.\n\n    rank_score = parse_rank_score(variant.INFO.get('RankScore', ''), genmod_key)\n    parsed_variant['rank_score'] = rank_score or 0\n\n\n    ################# Add gt calls #################\n    if individual_positions and case['individuals']:\n        parsed_variant['samples'] = parse_genotypes(variant, case['individuals'],\n                                                    individual_positions)\n    else:\n        parsed_variant['samples'] = []\n\n    ################# Add compound information #################\n    compounds = parse_compounds(compound_info=variant.INFO.get('Compounds'),\n                                case_id=genmod_key,\n                                variant_type=variant_type)\n    if compounds:\n        parsed_variant['compounds'] = compounds\n\n    ################# Add inheritance patterns #################\n\n    genetic_models = parse_genetic_models(variant.INFO.get('GeneticModels'), genmod_key)\n    if genetic_models:\n        parsed_variant['genetic_models'] = genetic_models\n\n    ################# Add autozygosity calls if present #################\n\n    azlength = variant.INFO.get('AZLENGTH')\n    if azlength:\n        parsed_variant['azlength'] = int(azlength)\n\n    azqual = variant.INFO.get('AZQUAL')\n    if azqual:\n        parsed_variant['azqual'] = float(azqual)\n\n    ################ Add STR info if present ################\n\n    # repeat id generally corresponds to gene symbol\n    repeat_id = variant.INFO.get('REPID')\n    if repeat_id:\n        parsed_variant['str_repid'] = str(repeat_id)\n\n    # repeat unit - used e g in PanelApp naming of STRs\n    repeat_unit = variant.INFO.get('RU')\n    if repeat_unit:\n        parsed_variant['str_ru'] = str(repeat_unit)\n\n    # repeat ref - reference copy number\n    repeat_ref = variant.INFO.get('REF')\n    if repeat_ref:\n        parsed_variant['str_ref'] = int(repeat_ref)\n\n    # repeat len - number of repeats found in case\n    repeat_len = variant.INFO.get('RL')\n    if repeat_len:\n        parsed_variant['str_len'] = int(repeat_len)\n\n    # str status - this indicates the severity of the expansion level\n    str_status = variant.INFO.get('STR_STATUS')\n    if str_status:\n        parsed_variant['str_status'] = str(str_status)\n\n    ################# Add gene and transcript information #################\n    raw_transcripts = []\n    if vep_header:\n        vep_info = variant.INFO.get('CSQ')\n        if vep_info:\n            raw_transcripts = (dict(zip(vep_header, transcript_info.split('|')))\n                               for transcript_info in vep_info.split(','))\n\n    parsed_transcripts = []\n    dbsnp_ids = set()\n    cosmic_ids = set()\n    for parsed_transcript in parse_transcripts(raw_transcripts, parsed_variant['alternative']):\n        parsed_transcripts.append(parsed_transcript)\n        for dbsnp in parsed_transcript.get('dbsnp', []):\n            dbsnp_ids.add(dbsnp)\n        for cosmic in parsed_transcript.get('cosmic', []):\n            cosmic_ids.add(cosmic)\n\n    # The COSMIC tag in INFO is added via VEP and/or bcftools annotate\n\n    cosmic_tag = variant.INFO.get('COSMIC')\n    if cosmic_tag:\n        cosmic_ids.add(cosmic_tag[4:])\n\n    if (dbsnp_ids and not parsed_variant['dbsnp_id']):\n        parsed_variant['dbsnp_id'] = ';'.join(dbsnp_ids)\n\n    if cosmic_ids:\n        parsed_variant['cosmic_ids'] = list(cosmic_ids)\n\n    gene_info = parse_genes(parsed_transcripts)\n\n    parsed_variant['genes'] = gene_info\n\n    hgnc_ids = set([])\n\n    for gene in parsed_variant['genes']:\n        hgnc_ids.add(gene['hgnc_id'])\n\n    parsed_variant['hgnc_ids'] = list(hgnc_ids)\n\n    ################# Add clinsig prediction #################\n    if variant.INFO.get('CLNACC'):\n        acc = variant.INFO.get('CLNACC')\n    else:\n        acc = variant.INFO.get('CLNVID')\n    clnsig_predictions = parse_clnsig(\n        acc=acc,\n        sig=variant.INFO.get('CLNSIG'),\n        revstat=variant.INFO.get('CLNREVSTAT'),\n        transcripts=parsed_transcripts\n        )\n\n    if clnsig_predictions:\n        parsed_variant['clnsig'] = clnsig_predictions\n\n    ################# Add the frequencies #################\n    frequencies = parse_frequencies(variant, parsed_transcripts)\n\n    parsed_variant['frequencies'] = frequencies\n\n    # parse out old local observation count\n    local_obs_old = variant.INFO.get('Obs')\n    if local_obs_old:\n        parsed_variant['local_obs_old'] = int(local_obs_old)\n\n    local_obs_hom_old = variant.INFO.get('Hom')\n    if local_obs_hom_old:\n        parsed_variant['local_obs_hom_old'] = int(local_obs_hom_old)\n\n    ###################### Add severity predictions ######################\n    cadd = parse_cadd(variant, parsed_transcripts)\n    if cadd:\n        parsed_variant['cadd_score'] = cadd\n\n    spidex = variant.INFO.get('SPIDEX')\n    if spidex:\n        parsed_variant['spidex'] = float(spidex)\n\n    ###################### Add conservation ######################\n\n    parsed_variant['conservation'] = parse_conservations(variant)\n\n    parsed_variant['callers'] = parse_callers(variant, category=category)\n\n    rank_result = variant.INFO.get('RankResult')\n    if rank_result:\n        results = [int(i) for i in rank_result.split('|')]\n        parsed_variant['rank_result'] = dict(zip(rank_results_header, results))\n\n    ###################### Add SV specific annotations ######################\n    sv_frequencies = parse_sv_frequencies(variant)\n    for key in sv_frequencies:\n        parsed_variant['frequencies'][key] = sv_frequencies[key]\n\n    ###################### Add Cancer specific annotations ######################\n    # MSK_MVL indicates if variants are in the MSK managed variant list\n    # https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5437632/\n    mvl_tag = variant.INFO.get('MSK_MVL')\n    if mvl_tag:\n        parsed_variant['mvl_tag'] = True\n\n    return parsed_variant"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate all compounds for a case", "response": "def compounds(context, case_id):\n    \"\"\"\n    Update all compounds for a case\n    \"\"\"\n    adapter = context.obj['adapter']\n    LOG.info(\"Running scout update compounds\")\n    # Check if the case exists\n    case_obj = adapter.case(case_id)\n    \n    if not case_obj:\n        LOG.warning(\"Case %s could not be found\", case_id)\n        context.abort()\n    \n    try:\n        adapter.update_case_compounds(case_obj)\n    except Exception as err:\n        LOG.warning(err)\n        context.abort()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate a gene object with many links", "response": "def add_gene_links(gene_obj, build=37):\n    \"\"\"Update a gene object with links\n\n    Args:\n        gene_obj(dict)\n        build(int)\n\n    Returns:\n        gene_obj(dict): gene_obj updated with many links\n    \"\"\"\n    try:\n        build = int(build)\n    except ValueError:\n        build = 37\n    # Add links that use the hgnc_id\n    hgnc_id = gene_obj['hgnc_id']\n\n    gene_obj['hgnc_link'] = genenames(hgnc_id)\n    gene_obj['omim_link'] = omim(hgnc_id)\n    # Add links that use ensembl_id\n    if not 'ensembl_id' in gene_obj:\n        ensembl_id = gene_obj.get('common',{}).get('ensembl_id')\n    else:\n        ensembl_id = gene_obj['ensembl_id']\n    ensembl_37_link = ensembl(ensembl_id, build=37)\n    ensembl_38_link = ensembl(ensembl_id, build=38)\n    gene_obj['ensembl_37_link'] = ensembl_37_link\n    gene_obj['ensembl_38_link'] = ensembl_38_link\n    gene_obj['ensembl_link'] = ensembl_37_link\n    if build == 38:\n        gene_obj['ensembl_link'] = ensembl_38_link\n    gene_obj['hpa_link'] = hpa(ensembl_id)\n    gene_obj['string_link'] = string(ensembl_id)\n    gene_obj['reactome_link'] = reactome(ensembl_id)\n    gene_obj['clingen_link'] = clingen(hgnc_id)\n    gene_obj['expression_atlas_link'] = expression_atlas(ensembl_id)\n    gene_obj['exac_link'] = exac(ensembl_id)\n    # Add links that use entrez_id\n    gene_obj['entrez_link'] = entrez(gene_obj.get('entrez_id'))\n    # Add links that use omim id\n    gene_obj['omim_link'] = omim(gene_obj.get('omim_id'))\n    # Add links that use hgnc_symbol\n    gene_obj['ppaint_link'] = ppaint(gene_obj['hgnc_symbol'])\n    # Add links that use vega_id\n    gene_obj['vega_link'] = vega(gene_obj.get('vega_id'))\n    # Add links that use ucsc link\n    gene_obj['ucsc_link'] = ucsc(gene_obj.get('ucsc_id'))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nquerying the aliases and transcripts of a given hgnc symbol", "response": "def hgnc(ctx, hgnc_symbol, hgnc_id, build):\n    \"\"\"\n    Query the hgnc aliases\n    \"\"\"\n    adapter = ctx.obj['adapter']\n    \n    if not (hgnc_symbol or hgnc_id):\n        log.warning(\"Please provide a hgnc symbol or hgnc id\")\n        ctx.abort()\n\n    if hgnc_id:\n        result = adapter.hgnc_gene(hgnc_id, build=build)\n        if result:\n            hgnc_symbol = result['hgnc_symbol']\n        else:\n            log.warning(\"Gene with id %s could not be found\", hgnc_id)\n            ctx.abort()\n    \n    result = adapter.hgnc_genes(hgnc_symbol, build=build)\n    \n    if result.count() == 0:\n        log.info(\"No results found\")\n    \n    else:\n        click.echo(\"#hgnc_id\\thgnc_symbol\\taliases\\ttranscripts\")\n        for gene in result:\n            click.echo(\"{0}\\t{1}\\t{2}\\t{3}\".format(\n                gene['hgnc_id'],\n                gene['hgnc_symbol'],\n                ', '.join(gene['aliases']),\n                ', '.join(tx['ensembl_transcript_id'] for tx in gene['transcripts']),\n            ))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse an hgnc formated line into a dict with the relevant info", "response": "def parse_hgnc_line(line, header):\n    \"\"\"Parse an hgnc formated line\n\n        Args:\n            line(list): A list with hgnc gene info\n            header(list): A list with the header info\n\n        Returns:\n            hgnc_info(dict): A dictionary with the relevant info\n    \"\"\"\n    hgnc_gene = {}\n    line = line.rstrip().split('\\t')\n    raw_info = dict(zip(header, line))\n    # Skip all genes that have status withdrawn\n    if 'Withdrawn' in raw_info['status']:\n        return hgnc_gene\n    \n    hgnc_symbol = raw_info['symbol']\n    hgnc_gene['hgnc_symbol'] = hgnc_symbol\n    hgnc_gene['hgnc_id'] = int(raw_info['hgnc_id'].split(':')[-1])\n    hgnc_gene['description'] = raw_info['name']\n    \n    # We want to have the current symbol as an alias\n    aliases = set([hgnc_symbol, hgnc_symbol.upper()])\n    # We then need to add both the previous symbols and\n    # alias symbols\n    previous_names = raw_info['prev_symbol']\n    if previous_names:\n        for alias in previous_names.strip('\"').split('|'):\n            aliases.add(alias)\n\n    alias_symbols = raw_info['alias_symbol']\n    if alias_symbols:\n        for alias in alias_symbols.strip('\"').split('|'):\n            aliases.add(alias)\n\n    hgnc_gene['previous_symbols'] = list(aliases)\n\n    # We need the ensembl_gene_id to link the genes with ensembl\n    hgnc_gene['ensembl_gene_id'] = raw_info.get('ensembl_gene_id')\n\n    omim_id = raw_info.get('omim_id')\n    if omim_id:\n        hgnc_gene['omim_id'] = int(omim_id.strip('\"').split('|')[0])\n    else:\n        hgnc_gene['omim_id'] = None\n\n    entrez_id = hgnc_gene['entrez_id'] = raw_info.get('entrez_id')\n    if entrez_id:\n        hgnc_gene['entrez_id'] = int(entrez_id)\n    else:\n        hgnc_gene['entrez_id'] = None\n\n    # These are the primary transcripts according to HGNC\n    ref_seq = raw_info.get('refseq_accession')\n    if ref_seq:\n        hgnc_gene['ref_seq'] = ref_seq.strip('\"').split('|')\n    else:\n        hgnc_gene['ref_seq'] = []\n\n    uniprot_ids = raw_info.get('uniprot_ids')\n    if uniprot_ids:\n        hgnc_gene['uniprot_ids'] = uniprot_ids.strip('\"\"').split('|')\n    else:\n        hgnc_gene['uniprot_ids'] = []\n\n    ucsc_id = raw_info.get('ucsc_id')\n    if ucsc_id:\n        hgnc_gene['ucsc_id'] = ucsc_id\n    else:\n        hgnc_gene['ucsc_id'] = None\n\n    vega_id = raw_info.get('vega_id')\n    if vega_id:\n        hgnc_gene['vega_id'] = vega_id\n    else:\n        hgnc_gene['vega_id'] = None\n\n    return hgnc_gene"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_hgnc_genes(lines):\n    header = []\n    logger.info(\"Parsing hgnc genes...\")\n    for index, line in enumerate(lines):\n        if index == 0:\n            header = line.split('\\t')\n        elif len(line) > 1:\n            hgnc_gene = parse_hgnc_line(line=line, header=header)\n            if hgnc_gene:\n                yield hgnc_gene", "response": "Parse lines with HGNC formated genes\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_submission(self, user_id, institute_id):\n\n        submission_obj = {\n            'status' : 'open',\n            'created_at' : datetime.now(),\n            'user_id' : user_id,\n            'institute_id' : institute_id\n        }\n        LOG.info(\"Creating a new clinvar submission for user '%s' and institute %s\", user_id, institute_id)\n        result = self.clinvar_submission_collection.insert_one(submission_obj)\n        return result.inserted_id", "response": "Create an open clinvar submission for a user and an institute ID."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeletes a submission object along with all associated clinvar objects and all associated casedata objects.", "response": "def delete_submission(self, submission_id):\n        \"\"\"Deletes a Clinvar submission object, along with all associated clinvar objects (variants and casedata)\n\n            Args:\n                submission_id(str): the ID of the submission to be deleted\n\n            Returns:\n                deleted_objects(int): the number of associated objects removed (variants and/or casedata)\n                deleted_submissions(int): 1 if it's deleted, 0 if something went wrong\n        \"\"\"\n        LOG.info(\"Deleting clinvar submission %s\", submission_id)\n        submission_obj = self.clinvar_submission_collection.find_one({ '_id' : ObjectId(submission_id)})\n\n        submission_variants = submission_obj.get('variant_data')\n        submission_casedata = submission_obj.get('case_data')\n\n        submission_objects = []\n\n        if submission_variants and submission_casedata:\n            submission_objects = submission_variants + submission_casedata\n        elif submission_variants:\n            submission_objects = submission_variants\n        elif submission_casedata:\n            submission_objects = submission_casedata\n\n        # Delete all variants and casedata objects associated with this submission\n        result = self.clinvar_collection.delete_many({'_id': { \"$in\": submission_objects} })\n        deleted_objects = result.deleted_count\n\n        # Delete the submission itself\n        result = self.clinvar_submission_collection.delete_one({'_id': ObjectId(submission_id)})\n        deleted_submissions = result.deleted_count\n\n        #return deleted_count, deleted_submissions\n        return deleted_objects,deleted_submissions"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve the database id of an open clinvar submission for a user and institute. If none is available create one and return it.", "response": "def get_open_clinvar_submission(self, user_id, institute_id):\n        \"\"\"Retrieve the database id of an open clinvar submission for a user and institute,\n           if none is available then create a new submission and return it\n\n           Args:\n                user_id(str): a user ID\n                institute_id(str): an institute ID\n\n           Returns:\n                submission(obj) : an open clinvar submission object\n        \"\"\"\n\n        LOG.info(\"Retrieving an open clinvar submission for user '%s' and institute %s\", user_id, institute_id)\n        query = dict(user_id=user_id, institute_id=institute_id, status='open')\n        submission = self.clinvar_submission_collection.find_one(query)\n\n        # If there is no open submission for this user and institute, create one\n        if submission is None:\n            submission_id = self.create_submission(user_id, institute_id)\n            submission = self.clinvar_submission_collection.find_one({'_id':submission_id})\n\n        return submission"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_clinvar_id(self, clinvar_id, submission_id ):\n        updated_submission = self.clinvar_submission_collection.find_one_and_update( {'_id': ObjectId(submission_id)}, { '$set' : {'clinvar_subm_id' : clinvar_id, 'updated_at': datetime.now()} }, upsert=True, return_document=pymongo.ReturnDocument.AFTER )\n        return updated_submission", "response": "updates the submission ID in a clinvar submission object"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the official Clinvar submission ID for a submission object", "response": "def get_clinvar_id(self, submission_id):\n        \"\"\"Returns the official Clinvar submission ID for a submission object\n\n            Args:\n                submission_id(str): submission_id(str) : id of the submission\n\n            Returns:\n                clinvar_subm_id(str): a string with a format: SUB[0-9]. It is obtained from clinvar portal when starting a new submission\n\n        \"\"\"\n        submission_obj = self.clinvar_submission_collection.find_one({'_id': ObjectId(submission_id)})\n        clinvar_subm_id = submission_obj.get('clinvar_subm_id') # This key does not exist if it was not previously provided by user\n        return clinvar_subm_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds the given objects to the submission and updates the coresponding submission object with their id ArcATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATATAT", "response": "def add_to_submission(self, submission_id, submission_objects):\n        \"\"\"Adds submission_objects to clinvar collection and update the coresponding submission object with their id\n\n            Args:\n                submission_id(str) : id of the submission to be updated\n                submission_objects(tuple): a tuple of 2 elements coresponding to a list of variants and a list of case data objects to add to submission\n\n            Returns:\n                updated_submission(obj): an open clinvar submission object, updated\n        \"\"\"\n\n        LOG.info(\"Adding new variants and case data to clinvar submission '%s'\", submission_id)\n\n        # Insert variant submission_objects into clinvar collection\n        # Loop over the objects\n        for var_obj in submission_objects[0]:\n            try:\n                result = self.clinvar_collection.insert_one(var_obj)\n                self.clinvar_submission_collection.update_one({'_id':submission_id}, {'$push': { 'variant_data' : str(result.inserted_id) }}, upsert=True)\n            except pymongo.errors.DuplicateKeyError:\n                LOG.error(\"Attepted to insert a clinvar variant which is already in DB!\")\n\n        # Insert casedata submission_objects into clinvar collection\n        if submission_objects[1]:\n            # Loop over the objects\n            for case_obj in submission_objects[1]:\n                try:\n                    result = self.clinvar_collection.insert_one(case_obj)\n                    self.clinvar_submission_collection.update_one({'_id':submission_id}, {'$push': { 'case_data': str(result.inserted_id)}}, upsert=True)\n                except pymongo.errors.DuplicateKeyError:\n                    LOG.error(\"One or more casedata object is already present in clinvar collection!\")\n\n        updated_submission = self.clinvar_submission_collection.find_one_and_update( {'_id':submission_id}, { '$set' : {'updated_at': datetime.now()} }, return_document=pymongo.ReturnDocument.AFTER )\n        return updated_submission"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the status of a clinvar submission", "response": "def update_clinvar_submission_status(self, user_id, submission_id, status):\n        \"\"\"Set a clinvar submission ID to 'closed'\n\n            Args:\n                submission_id(str): the ID of the clinvar submission to close\n\n            Return\n                updated_submission(obj): the submission object with a 'closed' status\n\n        \"\"\"\n        LOG.info('closing clinvar submission \"%s\"', submission_id)\n\n        if status == 'open': # just close the submission its status does not affect the other submissions for this user\n            # Close all other submissions for this user and then open the desired one\n            self.clinvar_submission_collection.update_many(\n                {'user_id' : user_id},\n                {'$set' :\n                    {'status' : 'closed', 'updated_at' : datetime.now()}\n                }\n            )\n        updated_submission = self.clinvar_submission_collection.find_one_and_update(\n            {'_id'  : ObjectId(submission_id)},\n            {'$set' :\n                {'status' : status, 'updated_at' : datetime.now()}\n            },\n            return_document=pymongo.ReturnDocument.AFTER\n        )\n\n        return updated_submission"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting all open and closed clinvar submissions created by a user for an institute.", "response": "def clinvar_submissions(self, user_id, institute_id):\n        \"\"\"Collect all open and closed clinvar submission created by a user for an institute\n\n            Args:\n                user_id(str): a user ID\n                institute_id(str): an institute ID\n\n            Returns:\n                submissions(list): a list of clinvar submission objects\n        \"\"\"\n        LOG.info(\"Retrieving all clinvar submissions for user '%s', institute '%s'\", user_id, institute_id)\n        # get first all submission objects\n        query = dict(user_id=user_id, institute_id=institute_id)\n        results = list(self.clinvar_submission_collection.find(query))\n\n        submissions = []\n        for result in results:\n            submission = {}\n            submission['_id'] =  result.get('_id')\n            submission['status'] = result.get('status')\n            submission['user_id'] = result.get('user_id')\n            submission['institute_id'] = result.get('institute_id')\n            submission['created_at'] = result.get('created_at')\n            submission['updated_at'] = result.get('updated_at')\n\n            if 'clinvar_subm_id' in result:\n                submission['clinvar_subm_id'] = result['clinvar_subm_id']\n\n            if result.get('variant_data'):\n                submission['variant_data'] = self.clinvar_collection.find({'_id': { \"$in\": result['variant_data'] } })\n            if result.get('case_data'):\n                submission['case_data'] = self.clinvar_collection.find({'_id' : { \"$in\": result['case_data'] } })\n\n            submissions.append(submission)\n\n        return submissions"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncollect a list of objects from the clinvar collection as specified by the key_id in the clinvar submission object.", "response": "def clinvar_objs(self, submission_id, key_id):\n        \"\"\"Collects a list of objects from the clinvar collection (variants of case data) as specified by the key_id in the clinvar submission\n\n            Args:\n                submission_id(str): the _id key of a clinvar submission\n                key_id(str) : either 'variant_data' or 'case_data'. It's a key in a clinvar_submission object.\n                              Its value is a list of ids of clinvar objects (either variants of casedata objects)\n\n            Returns:\n                clinvar_objects(list) : a list of clinvar objects (either variants of casedata)\n\n        \"\"\"\n        # Get a submission object\n        submission = self.clinvar_submission_collection.find_one({'_id': ObjectId(submission_id)})\n\n        # a list of clinvar object ids, they can be of csv_type 'variant' or 'casedata'\n        if submission.get(key_id):\n            clinvar_obj_ids = list(submission.get(key_id))\n            clinvar_objects = self.clinvar_collection.find({'_id' : { \"$in\": clinvar_obj_ids }})\n            return list(clinvar_objects)\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving a variant object from the clinvar database and update the relative submission object", "response": "def delete_clinvar_object(self, object_id, object_type, submission_id):\n        \"\"\"Remove a variant object from clinvar database and update the relative submission object\n\n            Args:\n                object_id(str) : the id of an object to remove from clinvar_collection database collection (a variant of a case)\n                object_type(str) : either 'variant_data' or 'case_data'. It's a key in the clinvar_submission object.\n                submission_id(str): the _id key of a clinvar submission\n\n            Returns:\n                updated_submission(obj): an updated clinvar submission\n        \"\"\"\n\n        LOG.info(\"Deleting clinvar object %s (%s)\", object_id, object_type)\n\n        # If it's a variant object to be removed:\n        #   remove reference to it in the submission object 'variant_data' list field\n        #   remove the variant object from clinvar collection\n        #   remove casedata object from clinvar collection\n        #   remove reference to it in the submission object 'caset_data' list field\n\n        # if it's a casedata object to be removed:\n        #   remove reference to it in the submission object 'caset_data' list field\n        #   remove casedata object from clinvar collection\n\n        result = ''\n\n        if object_type == 'variant_data':\n            # pull out a variant from submission object\n            self.clinvar_submission_collection.find_one_and_update( {'_id': ObjectId(submission_id)}, {'$pull': {'variant_data': object_id} })\n\n            variant_object = self.clinvar_collection.find_one( {'_id': object_id} )\n            linking_id = variant_object.get(\"linking_id\") #it's the original ID of the variant in scout, it's linking clinvar variants and casedata objects together\n\n            # remove any object with that linking_id from clinvar_collection. This removes variant and casedata\n            result = self.clinvar_collection.delete_many( {'linking_id': linking_id } )\n\n        else: # remove case_data but keep variant in submission\n            # delete the object itself from clinvar_collection\n             result = self.clinvar_collection.delete_one( {'_id': object_id } )\n\n        # in any case remove reference to it in the submission object 'caset_data' list field\n        self.clinvar_submission_collection.find_one_and_update( {'_id': ObjectId(submission_id)}, {'$pull': {'case_data': object_id} })\n\n        updated_submission = self.clinvar_submission_collection.find_one_and_update( {'_id':submission_id}, { '$set' : {'updated_at': datetime.now()} }, return_document=pymongo.ReturnDocument.AFTER )\n\n        return updated_submission"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets all variants included in clinvar submissions for a case", "response": "def case_to_clinVars(self, case_id):\n        \"\"\"Get all variants included in clinvar submissions for a case\n\n        Args:\n            case_id(str): a case _id\n\n        Returns:\n            submission_variants(dict): keys are variant ids and values are variant submission objects\n\n        \"\"\"\n        query = dict(case_id=case_id, csv_type='variant')\n        clinvar_objs = list(self.clinvar_collection.find(query))\n        submitted_vars = {}\n        for clinvar in clinvar_objs:\n            submitted_vars[clinvar.get('local_id')] = clinvar\n\n        return submitted_vars"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a single hpo phenotype line into a dictionary.", "response": "def parse_hpo_phenotype(hpo_line):\n    \"\"\"Parse hpo phenotype\n    \n        Args:\n            hpo_line(str): A iterable with hpo phenotype lines\n    \n        Yields:\n            hpo_info(dict)\n    \"\"\"\n    hpo_line = hpo_line.rstrip().split('\\t')\n    hpo_info = {}\n    hpo_info['hpo_id'] = hpo_line[0]\n    hpo_info['description'] = hpo_line[1]\n    hpo_info['hgnc_symbol'] = hpo_line[3]\n    \n    return hpo_info"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_hpo_gene(hpo_line):\n    if not len(hpo_line) > 3:\n        return {}\n    hpo_line = hpo_line.rstrip().split('\\t')\n    hpo_info = {}\n    hpo_info['hgnc_symbol'] = hpo_line[1]\n    hpo_info['description'] = hpo_line[2]\n    hpo_info['hpo_id'] = hpo_line[3]\n    \n    return hpo_info", "response": "Parse the HPO gene information from a line of text."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing HPO disease line into dict.", "response": "def parse_hpo_disease(hpo_line):\n    \"\"\"Parse hpo disease line\n    \n        Args:\n            hpo_line(str)\n    \"\"\"\n    hpo_line = hpo_line.rstrip().split('\\t')\n    hpo_info = {}\n    disease = hpo_line[0].split(':')\n    \n    hpo_info['source'] = disease[0]\n    hpo_info['disease_nr'] = int(disease[1])\n    hpo_info['hgnc_symbol'] = None\n    hpo_info['hpo_term'] = None\n    \n    if len(hpo_line) >= 3:\n        hpo_info['hgnc_symbol'] = hpo_line[2]\n\n        if len(hpo_line) >= 4:\n            hpo_info['hpo_term'] = hpo_line[3]\n    \n    \n    return hpo_info"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_hpo_phenotypes(hpo_lines):\n    hpo_terms = {}\n    LOG.info(\"Parsing hpo phenotypes...\")\n    for index, line in enumerate(hpo_lines):\n        if index > 0 and len(line) > 0:\n            hpo_info = parse_hpo_phenotype(line)\n            hpo_term = hpo_info['hpo_id']\n            hgnc_symbol = hpo_info['hgnc_symbol']\n            if hpo_term in hpo_terms:\n                hpo_terms[hpo_term]['hgnc_symbols'].append(hgnc_symbol)\n            else:\n                hpo_terms[hpo_term] = {\n                    'hpo_id':hpo_term,\n                    'description': hpo_info['description'],\n                    'hgnc_symbols': [hgnc_symbol]\n                }\n    LOG.info(\"Parsing done.\")\n    return hpo_terms", "response": "Parse the lines in the hpo phenotype file and return a dictionary with the keys and terms as keys and values as values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_hpo_diseases(hpo_lines):\n    diseases = {}\n    LOG.info(\"Parsing hpo diseases...\")\n    for index, line in enumerate(hpo_lines):\n        # First line is a header\n        if index == 0:\n            continue\n        # Skip empty lines\n        if not len(line) > 3:\n            continue\n        # Parse the info\n        disease_info = parse_hpo_disease(line)\n        # Skip the line if there where no info\n        if not disease_info:\n            continue\n        disease_nr = disease_info['disease_nr']\n        hgnc_symbol = disease_info['hgnc_symbol']\n        hpo_term = disease_info['hpo_term']\n        source = disease_info['source']\n        disease_id = \"{0}:{1}\".format(source, disease_nr)\n        \n        if disease_id not in diseases:\n            diseases[disease_id] = {\n                'disease_nr': disease_nr,\n                'source': source,\n                'hgnc_symbols': set(),\n                'hpo_terms': set(),\n            }\n\n        if hgnc_symbol:\n            diseases[disease_id]['hgnc_symbols'].add(hgnc_symbol)\n        if hpo_term:\n            diseases[disease_id]['hpo_terms'].add(hpo_term)\n\n    LOG.info(\"Parsing done.\")\n    return diseases", "response": "Parse the HPO disease phenotypes\n    \n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_hpo_to_genes(hpo_lines):\n    for line in hpo_lines:\n        if line.startswith('#') or len(line) < 1:\n            continue\n        line = line.rstrip().split('\\t')\n        hpo_id = line[0]\n        hgnc_symbol = line[3]\n        \n        yield {\n            'hpo_id': hpo_id,\n            'hgnc_symbol': hgnc_symbol\n        }", "response": "Parse the map from a HPO term to a HGNC symbol."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_hpo_genes(hpo_lines):\n    LOG.info(\"Parsing HPO genes ...\")\n    genes = {}\n    for index, line in enumerate(hpo_lines):\n        # First line is header\n        if index == 0:\n            continue\n        if len(line) < 5:\n            continue\n        gene_info = parse_hpo_gene(line)\n        hgnc_symbol = gene_info['hgnc_symbol']\n        description = gene_info['description']\n        \n        if hgnc_symbol not in genes:\n            genes[hgnc_symbol] = {\n                'hgnc_symbol': hgnc_symbol\n            }\n        \n        gene = genes[hgnc_symbol]\n        if description == 'Incomplete penetrance':\n            gene['incomplete_penetrance'] = True\n        if description == 'Autosomal dominant inheritance':\n            gene['ad'] = True\n        if description == 'Autosomal recessive inheritance':\n            gene['ar'] = True\n        if description == 'Mithochondrial inheritance':\n            gene['mt'] = True\n        if description == 'X-linked dominant inheritance':\n            gene['xd'] = True\n        if description == 'X-linked recessive inheritance':\n            gene['xr'] = True\n        if description == 'Y-linked inheritance':\n            gene['x'] = True\n        if description == 'X-linked inheritance':\n            gene['y'] = True\n    LOG.info(\"Parsing done.\")\n    return genes", "response": "Parse HPO gene information into a dictionary with keys as keys\n           "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a set with all genes that have incomplete penetrance according to HPO", "response": "def get_incomplete_penetrance_genes(hpo_lines):\n    \"\"\"Get a set with all genes that have incomplete penetrance according to HPO\n    \n    Args:\n        hpo_lines(iterable(str))\n    \n    Returns:\n        incomplete_penetrance_genes(set): A set with the hgnc symbols of all \n                                          genes with incomplete penetrance\n        \n    \"\"\"\n    genes = parse_hpo_genes(hpo_lines)\n    incomplete_penetrance_genes = set()\n    for hgnc_symbol in genes:\n        if genes[hgnc_symbol].get('incomplete_penetrance'):\n            incomplete_penetrance_genes.add(hgnc_symbol)\n    return incomplete_penetrance_genes"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a. obo formated hpo line and yield a list of dicts", "response": "def parse_hpo_obo(hpo_lines):\n    \"\"\"Parse a .obo formated hpo line\"\"\"\n    term = {}\n    for line in hpo_lines:\n        if len(line) == 0:\n            continue\n        line = line.rstrip()\n        # New term starts with [Term]\n        if line == '[Term]':\n            if term:\n                yield term\n            term = {}\n        \n        elif line.startswith('id'):\n            term['hpo_id'] = line[4:]\n\n        elif line.startswith('name'):\n            term['description'] = line[6:]\n\n        elif line.startswith('alt_id'):\n            if 'aliases' not in term:\n                term['aliases'] = []\n            term['aliases'].append(line[8:])\n        \n        elif line.startswith('is_a'):\n            if 'ancestors' not in term:\n                term['ancestors'] = []\n            term['ancestors'].append(line[6:16])\n\n    if term:\n        yield term"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrendering seach box for genes.", "response": "def genes():\n    \"\"\"Render seach box for genes.\"\"\"\n    query = request.args.get('query', '')\n    if '|' in query:\n        hgnc_id = int(query.split(' | ', 1)[0])\n        return redirect(url_for('.gene', hgnc_id=hgnc_id))\n    gene_q = store.all_genes().limit(20)\n    return dict(genes=gene_q)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef gene(hgnc_id=None, hgnc_symbol=None):\n    if hgnc_symbol:\n        query = store.hgnc_genes(hgnc_symbol)\n        if query.count() == 1:\n            hgnc_id = query.first()['hgnc_id']\n        else:\n            return redirect(url_for('.genes', query=hgnc_symbol))\n    try:\n        genes = controllers.gene(store, hgnc_id)\n    except ValueError as error:\n        return abort(404)\n\n    return genes", "response": "Render information about a gene."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn JSON data about genes.", "response": "def api_genes():\n    \"\"\"Return JSON data about genes.\"\"\"\n    query = request.args.get('query')\n    json_out = controllers.genes_to_json(store, query)\n    return jsonify(json_out)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if the panels exist in the database", "response": "def check_panels(adapter, panels, default_panels=None):\n    \"\"\"Make sure that the gene panels exist in the database\n        Also check if the default panels are defined in gene panels\n\n        Args:\n            adapter(MongoAdapter)\n            panels(list(str)): A list with panel names\n\n        Returns:\n            panels_exists(bool)\n    \"\"\"\n    default_panels = default_panels or []\n    panels_exist = True\n    for panel in default_panels:\n        if panel not in panels:\n            log.warning(\"Default panels have to be defined in panels\")\n            panels_exist = False\n    for panel in panels:\n        if not adapter.gene_panel(panel):\n            log.warning(\"Panel {} does not exist in database\".format(panel))\n            panels_exist = False\n    return panels_exist"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload all variants in a region defined by a HGNC id", "response": "def load_region(adapter, case_id, hgnc_id=None, chrom=None, start=None, end=None):\n    \"\"\"Load all variants in a region defined by a HGNC id\n\n    Args:\n        adapter (MongoAdapter)\n        case_id (str): Case id\n        hgnc_id (int): If all variants from a gene should be uploaded\n        chrom (str): If variants from coordinates should be uploaded\n        start (int): Start position for region\n        end (int): Stop position for region\n    \"\"\"\n    if hgnc_id:\n        gene_obj = adapter.hgnc_gene(hgnc_id)\n        if not gene_obj:\n            ValueError(\"Gene {} does not exist in database\".format(hgnc_id))\n        chrom = gene_obj['chromosome']\n        start = gene_obj['start']\n        end = gene_obj['end']\n\n    case_obj = adapter.case(case_id=case_id)\n    if not case_obj:\n        raise ValueError(\"Case {} does not exist in database\".format(case_id))\n\n    log.info(\"Load clinical SNV variants for case: {0} region: chr {1}, start\"\n             \" {2}, end {3}\".format(case_obj['_id'], chrom, start, end))\n\n    adapter.load_variants(case_obj=case_obj, variant_type='clinical',\n                          category='snv', chrom=chrom, start=start, end=end)\n\n    vcf_sv_file = case_obj['vcf_files'].get('vcf_sv')\n    if vcf_sv_file:\n        log.info(\"Load clinical SV variants for case: {0} region: chr {1}, \"\n                 \"start {2}, end {3}\".format(case_obj['_id'], chrom, start, end))\n        adapter.load_variants(case_obj=case_obj, variant_type='clinical',\n                              category='sv', chrom=chrom, start=start, end=end)\n\n    vcf_str_file = case_obj['vcf_files'].get('vcf_str')\n    if vcf_str_file: \n        log.info(\"Load clinical STR variants for case: {0} region: chr {1}, \"\n                 \"start {2}, end {3}\".format(case_obj['_id'], chrom, start, end))\n        adapter.load_variants(case_obj=case_obj, variant_type='clinical',\n                              category='str', chrom=chrom, start=start, end=end)\n\n    if case_obj['is_research']:\n        log.info(\"Load research SNV variants for case: {0} region: chr {1}, \"\n                 \"start {2}, end {3}\".format(case_obj['_id'], chrom, start, end))\n        adapter.load_variants(case_obj=case_obj, variant_type='research',\n                              category='snv', chrom=chrom, start=start, end=end)\n\n        vcf_sv_research = case_obj['vcf_files'].get('vcf_sv_research')\n        if vcf_sv_research:\n            log.info(\"Load research SV variants for case: {0} region: chr {1},\"\n                     \" start {2}, end {3}\".format(case_obj['_id'], chrom, start, end))\n            adapter.load_variants(case_obj=case_obj, variant_type='research',\n                                  category='sv', chrom=chrom, start=start, end=end)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_scout(adapter, config, ped=None, update=False):\n    log.info(\"Check that the panels exists\")\n    if not check_panels(adapter, config.get('gene_panels', []),\n                        config.get('default_gene_panels')):\n        raise ConfigError(\"Some panel(s) does not exist in the database\")\n    case_obj = adapter.load_case(config, update=update)\n    return case_obj", "response": "Load a new case from a Scout config."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef templated(template=None):\n    def decorator(f):\n        @wraps(f)\n        def decorated_function(*args, **kwargs):\n            template_name = template\n            if template_name is None:\n                template_name = request.endpoint.replace('.', '/') + '.html'\n            context = f(*args, **kwargs)\n            if context is None:\n                context = {}\n            elif not isinstance(context, dict):\n                return context\n            return render_template(template_name, **context)\n        return decorated_function\n    return decorator", "response": "Decorator that returns a function that renders the template with the specified name."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef institute_and_case(store, institute_id, case_name=None):\n    institute_obj = store.institute(institute_id)\n    if institute_obj is None and institute_id != 'favicon.ico':\n        flash(\"Can't find institute: {}\".format(institute_id), 'warning')\n        return abort(404)\n\n    if case_name:\n        if case_name:\n            case_obj = store.case(institute_id=institute_id, display_name=case_name)\n            if case_obj is None:\n                return abort(404)\n\n    # validate that user has access to the institute\n    if not current_user.is_admin:\n        if institute_id not in current_user.institutes:\n            if not case_name or not any(inst_id in case_obj['collaborators'] for inst_id in\n                                        current_user.institutes):\n                # you don't have access!!\n                flash(\"You don't have acccess to: {}\".format(institute_id),'danger')\n                return abort(403)\n\n    # you have access!\n    if case_name:\n        return institute_obj, case_obj\n    else:\n        return institute_obj", "response": "Fetch institute and case objects."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the hgnc id for a gene", "response": "def get_hgnc_id(gene_info, adapter):\n    \"\"\"Get the hgnc id for a gene\n\n        The proprity order will be\n        1. if there is a hgnc id this one will be choosen\n        2. if the hgnc symbol matches a genes proper hgnc symbol\n        3. if the symbol ony matches aliases on several genes one will be\n           choosen at random\n\n        Args:\n            gene_info(dict)\n            adapter\n\n        Returns:\n            true_id(int)\n    \"\"\"\n    hgnc_id = gene_info.get('hgnc_id')\n    hgnc_symbol = gene_info.get('hgnc_symbol')\n\n    true_id = None\n\n    if hgnc_id:\n        true_id = int(hgnc_id)\n    else:\n        gene_result = adapter.hgnc_genes(hgnc_symbol)\n        if gene_result.count() == 0:\n            raise Exception(\"No gene could be found for {}\".format(hgnc_symbol))\n        for gene in gene_result:\n            if hgnc_symbol.upper() == gene.hgnc_symbol.upper():\n                true_id = gene.hgnc_id\n        if not gene_info['hgnc_id']:\n            true_id = gene.hgnc_id\n\n    return true_id"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates a panel in the database", "response": "def panel(context, panel, version, update_date, update_version):\n    \"\"\"\n    Update a panel in the database\n    \"\"\"\n    adapter = context.obj['adapter']\n\n    # Check that the panel exists\n    panel_obj = adapter.gene_panel(panel, version=version)\n\n    if not panel_obj:\n        LOG.warning(\"Panel %s (version %s) could not be found\" % (panel, version))\n        context.abort()\n\n    date_obj = None\n    if update_date:\n        try:\n            date_obj = get_date(update_date)\n        except Exception as err:\n            LOG.warning(err)\n            context.abort()\n\n    update_panel(\n        adapter,\n        panel,\n        panel_version=panel_obj['version'],\n        new_version=update_version,\n        new_date=date_obj\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates the disease terms in mongo database.", "response": "def diseases(context, api_key):\n    \"\"\"\n    Update disease terms in mongo database.\n    \"\"\"\n    adapter = context.obj['adapter']\n    \n    # Fetch the omim information\n    api_key = api_key or context.obj.get('omim_api_key')\n    if not api_key:\n        LOG.warning(\"Please provide a omim api key to load the omim gene panel\")\n        context.abort()\n\n    try:\n        mim_files = fetch_mim_files(api_key, genemap2=True)\n    except Exception as err:\n        LOG.warning(err)\n        context.abort()\n    \n    LOG.info(\"Dropping DiseaseTerms\")\n    adapter.disease_term_collection.drop()\n    LOG.debug(\"DiseaseTerms dropped\")\n\n    load_disease_terms(\n        adapter=adapter,\n        genemap_lines=mim_files['genemap2'], \n    )\n\n    LOG.info(\"Successfully loaded all disease terms\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload the hpo terms and diseases into the database.", "response": "def load_hpo(adapter, disease_lines, hpo_disease_lines=None, hpo_lines=None, hpo_gene_lines=None):\n    \"\"\"Load the hpo terms and hpo diseases into database\n    \n    Args:\n        adapter(MongoAdapter)\n        disease_lines(iterable(str)): These are the omim genemap2 information\n        hpo_lines(iterable(str))\n        disease_lines(iterable(str))\n        hpo_gene_lines(iterable(str))\n    \"\"\"\n    # Create a map from gene aliases to gene objects\n    alias_genes = adapter.genes_by_alias()\n    \n    # Fetch the hpo terms if no file\n    if not hpo_lines:\n        hpo_lines = fetch_hpo_terms()\n\n    # Fetch the hpo gene information if no file\n    if not hpo_gene_lines:\n        hpo_gene_lines = fetch_hpo_to_genes()\n    \n    # Fetch the hpo phenotype information if no file\n    if not hpo_disease_lines:\n        hpo_disease_lines = fetch_hpo_phenotype_to_terms()\n    \n    load_hpo_terms(adapter, hpo_lines, hpo_gene_lines, alias_genes)\n    \n    load_disease_terms(adapter, disease_lines, alias_genes, hpo_disease_lines)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload the hpo terms into the database and add them to the database", "response": "def load_hpo_terms(adapter, hpo_lines=None, hpo_gene_lines=None, alias_genes=None):\n    \"\"\"Load the hpo terms into the database\n    \n    Parse the hpo lines, build the objects and add them to the database\n    \n    Args:\n        adapter(MongoAdapter)\n        hpo_lines(iterable(str))\n        hpo_gene_lines(iterable(str))\n    \"\"\"\n    \n    # Store the hpo terms\n    hpo_terms = {}\n    \n    # Fetch the hpo terms if no file\n    if not hpo_lines:\n        hpo_lines = fetch_hpo_terms()\n    \n    # Fetch the hpo gene information if no file\n    if not hpo_gene_lines:\n        hpo_gene_lines = fetch_hpo_to_genes()\n\n    # Parse the terms\n    # This will yield dictionaries with information about the terms\n    LOG.info(\"Parsing hpo terms\")\n    for term in parse_hpo_obo(hpo_lines):\n        hpo_terms[term['hpo_id']] = term\n    \n    # Get a map with hgnc symbols to hgnc ids from scout\n    if not alias_genes:\n        alias_genes = adapter.genes_by_alias()\n\n    LOG.info(\"Adding gene information to hpo terms ...\")\n    for hpo_to_symbol in parse_hpo_to_genes(hpo_gene_lines):\n        hgnc_symbol = hpo_to_symbol['hgnc_symbol']\n        hpo_id = hpo_to_symbol['hpo_id']\n        \n        # Fetch gene info to get correct hgnc id\n        gene_info = alias_genes.get(hgnc_symbol)\n        if not gene_info:\n            continue\n\n        hgnc_id = gene_info['true']\n\n        if hpo_id not in hpo_terms:\n            continue\n\n        hpo_term = hpo_terms[hpo_id]\n\n        if not 'genes' in hpo_term:\n            hpo_term['genes'] = set()\n\n        hpo_term['genes'].add(hgnc_id)\n\n    start_time = datetime.now()\n\n    LOG.info(\"Loading the hpo terms...\")\n    nr_terms = len(hpo_terms)\n    hpo_bulk = []\n    with progressbar(hpo_terms.values(), label=\"Loading hpo terms\", length=nr_terms) as bar:\n        \n        for hpo_info in bar:\n            hpo_bulk.append(build_hpo_term(hpo_info))\n        \n        if len(hpo_bulk) > 10000:\n            adapter.load_hpo_bulk(hpo_bulk)\n            hpo_bulk = []\n    \n    if hpo_bulk:\n        adapter.load_hpo_bulk(hpo_bulk)\n    \n    LOG.info(\"Loading done. Nr of terms loaded {0}\".format(nr_terms))\n    LOG.info(\"Time to load terms: {0}\".format(datetime.now() - start_time))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading the omim phenotypes into the database", "response": "def load_disease_terms(adapter, genemap_lines, genes=None, hpo_disease_lines=None):\n    \"\"\"Load the omim phenotypes into the database\n    \n    Parse the phenotypes from genemap2.txt and find the associated hpo terms\n    from ALL_SOURCES_ALL_FREQUENCIES_diseases_to_genes_to_phenotypes.txt.\n\n    Args:\n        adapter(MongoAdapter)\n        genemap_lines(iterable(str))\n        genes(dict): Dictionary with all genes found in database\n        hpo_disease_lines(iterable(str))\n\n    \"\"\"\n    # Get a map with hgnc symbols to hgnc ids from scout\n    if not genes:\n        genes = adapter.genes_by_alias()\n\n    # Fetch the disease terms from omim\n    disease_terms = get_mim_phenotypes(genemap_lines=genemap_lines)\n\n    if not hpo_disease_lines:\n        hpo_disease_lines = fetch_hpo_phenotype_to_terms()\n    hpo_diseases = parse_hpo_diseases(hpo_disease_lines)\n\n    start_time = datetime.now()\n    nr_diseases = None\n\n    LOG.info(\"Loading the hpo disease...\")\n    for nr_diseases, disease_number in enumerate(disease_terms):\n        disease_info = disease_terms[disease_number]\n        disease_id = \"OMIM:{0}\".format(disease_number)\n        \n        if disease_id in hpo_diseases:\n            hpo_terms = hpo_diseases[disease_id]['hpo_terms']\n            if hpo_terms:\n                disease_info['hpo_terms'] = hpo_terms\n        disease_obj = build_disease_term(disease_info, genes)\n\n        adapter.load_disease_term(disease_obj)\n\n    LOG.info(\"Loading done. Nr of diseases loaded {0}\".format(nr_diseases))\n    LOG.info(\"Time to load diseases: {0}\".format(datetime.now() - start_time))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_frequencies(variant, transcripts):\n    frequencies = {}\n    # These lists could be extended...\n    thousand_genomes_keys = ['1000GAF']\n    thousand_genomes_max_keys = ['1000G_MAX_AF']\n\n    exac_keys = ['EXACAF']\n    exac_max_keys = ['ExAC_MAX_AF', 'EXAC_MAX_AF']\n\n    gnomad_keys = ['GNOMADAF', 'GNOMAD_AF']\n    gnomad_max_keys = ['GNOMADAF_POPMAX', 'GNOMADAF_MAX']\n\n    for test_key in thousand_genomes_keys:\n        thousand_g = parse_frequency(variant, test_key)\n        if thousand_g:\n            frequencies['thousand_g'] = thousand_g\n            break\n\n    for test_key in thousand_genomes_max_keys:\n        thousand_g_max = parse_frequency(variant, test_key)\n        if thousand_g_max:\n            frequencies['thousand_g_max'] = thousand_g_max\n            break\n\n    for test_key in exac_keys:\n        exac = parse_frequency(variant, test_key)\n        if exac:\n            frequencies['exac'] = exac\n            break\n\n    for test_key in exac_max_keys:\n        exac_max = parse_frequency(variant, test_key)\n        if exac_max:\n            frequencies['exac_max'] = exac_max\n            break\n\n    for test_key in gnomad_keys:\n        gnomad = parse_frequency(variant, test_key)\n        if gnomad:\n            frequencies['gnomad'] = gnomad\n            break\n\n    for test_key in gnomad_max_keys:\n        gnomad_max = parse_frequency(variant, test_key)\n        if gnomad_max:\n            frequencies['gnomad_max'] = gnomad_max\n            break\n\n    # Search transcripts if not found in VCF\n    if not frequencies:\n        for transcript in transcripts:\n            exac = transcript.get('exac_maf')\n            exac_max = transcript.get('exac_max')\n\n            thousand_g = transcript.get('thousand_g_maf')\n            thousandg_max = transcript.get('thousandg_max')\n\n            gnomad = transcript.get('gnomad_maf')\n            gnomad_max = transcript.get('gnomad_max')\n            if exac:\n                frequencies['exac'] = exac\n            if exac_max:\n                frequencies['exac_max'] = exac_max\n            if thousand_g:\n                frequencies['thousand_g'] = thousand_g\n            if thousandg_max:\n                frequencies['thousand_g_max'] = thousandg_max\n            if gnomad:\n                frequencies['gnomad'] = gnomad\n            if gnomad_max:\n                frequencies['gnomad_max'] = gnomad_max\n\n    #These are SV-specific frequencies\n    thousand_g_left = parse_frequency(variant, 'left_1000GAF')\n    if thousand_g_left:\n        frequencies['thousand_g_left'] = thousand_g_left\n\n    thousand_g_right = parse_frequency(variant, 'right_1000GAF')\n    if thousand_g_right:\n        frequencies['thousand_g_right'] = thousand_g_right\n\n    return frequencies", "response": "Add the frequencies to the variant\nTaxonomy"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse any frequency from the info dict", "response": "def parse_frequency(variant, info_key):\n    \"\"\"Parse any frequency from the info dict\n\n    Args:\n        variant(cyvcf2.Variant)\n        info_key(str)\n\n    Returns:\n        frequency(float): or None if frequency does not exist\n    \"\"\"\n    raw_annotation = variant.INFO.get(info_key)\n    raw_annotation = None if raw_annotation == '.' else raw_annotation\n    frequency = float(raw_annotation) if raw_annotation else None\n    return frequency"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the custom sv frequencies for a given structural variant.", "response": "def parse_sv_frequencies(variant):\n    \"\"\"Parsing of some custom sv frequencies\n    \n    These are very specific at the moment, this will hopefully get better over time when the\n    field of structural variants is more developed.\n\n    Args:\n        variant(cyvcf2.Variant)\n\n    Returns:\n        sv_frequencies(dict)\n    \"\"\"\n    frequency_keys = [\n        'clingen_cgh_benignAF',\n        'clingen_cgh_benign',\n        'clingen_cgh_pathogenicAF',\n        'clingen_cgh_pathogenic',\n        'clingen_ngi',\n        'clingen_ngiAF',\n        'swegen',\n        'swegenAF',\n        'decipherAF',\n        'decipher'\n    ]\n    sv_frequencies = {}\n\n    for key in frequency_keys:\n        value = variant.INFO.get(key, 0)\n        if 'AF' in key:\n            value = float(value)\n        else:\n            value = int(value)\n        if value > 0:\n            sv_frequencies[key] = value\n    return sv_frequencies"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nshow all users in the database", "response": "def users(context):\n    \"\"\"Show all users in the database\"\"\"\n    LOG.info(\"Running scout view users\")\n    adapter = context.obj['adapter']\n\n    user_objs = adapter.users()\n    if user_objs.count() == 0:\n        LOG.info(\"No users found\")\n        context.abort()\n\n    click.echo(\"#name\\temail\\troles\\tinstitutes\")\n    for user_obj in user_objs:\n        click.echo(\"{0}\\t{1}\\t{2}\\t{3}\\t\".format(\n            user_obj['name'],\n            user_obj.get('mail', user_obj['_id']),\n            ', '.join(user_obj.get('roles', [])),\n            ', '.join(user_obj.get('institutes', [])),\n        )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_case(adapter, case_obj, update=False):\n    logger.info('Loading case {} into database'.format(case_obj['display_name']))\n\n    # Check if case exists in database\n    existing_case = adapter.case(case_obj['_id'])\n\n    if existing_case:\n        if update:\n            adapter.update_case(case_obj)\n        else:\n            raise IntegrityError(\"Case {0} already exists in database\".format(case_obj['_id']))\n    else:\n        adapter.add_case(case_obj)\n    return case_obj", "response": "Load a case into the database"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding a HGNC gene object from a dictionary of gene information.", "response": "def build_hgnc_gene(gene_info, build='37'):\n    \"\"\"Build a hgnc_gene object\n\n        Args:\n            gene_info(dict): Gene information\n\n        Returns:\n            gene_obj(dict)\n    \n            {\n                '_id': ObjectId(),\n                # This is the hgnc id, required:\n                'hgnc_id': int, \n                # The primary symbol, required \n                'hgnc_symbol': str,\n                'ensembl_id': str, # required\n                'build': str, # '37' or '38', defaults to '37', required\n                \n                'chromosome': str, # required\n                'start': int, # required\n                'end': int, # required\n                \n                'description': str, # Gene description\n                'aliases': list(), # Gene symbol aliases, includes hgnc_symbol, str\n                'entrez_id': int,\n                'omim_id': int,\n                'pli_score': float,\n                'primary_transcripts': list(), # List of refseq transcripts (str)\n                'ucsc_id': str,\n                'uniprot_ids': list(), # List of str\n                'vega_id': str,\n                'transcripts': list(), # List of hgnc_transcript\n                \n                # Inheritance information\n                'inheritance_models': list(), # List of model names\n                'incomplete_penetrance': bool, # Acquired from HPO\n                \n                # Phenotype information\n                'phenotypes': list(), # List of dictionaries with phenotype information\n            }\n    \"\"\"\n    try:\n        hgnc_id = int(gene_info['hgnc_id'])\n    except KeyError as err:\n        raise KeyError(\"Gene has to have a hgnc_id\")\n    except ValueError as err:\n        raise ValueError(\"hgnc_id has to be integer\")\n    \n    try:\n        hgnc_symbol = gene_info['hgnc_symbol']\n    except KeyError as err:\n        raise KeyError(\"Gene has to have a hgnc_symbol\")\n\n    try:\n        ensembl_id = gene_info['ensembl_gene_id']\n    except KeyError as err:\n        raise KeyError(\"Gene has to have a ensembl_id\")\n\n    try:\n        chromosome = gene_info['chromosome']\n    except KeyError as err:\n        raise KeyError(\"Gene has to have a chromosome\")\n    \n    try:\n        start = int(gene_info['start'])\n    except KeyError as err:\n        raise KeyError(\"Gene has to have a start position\")\n    except TypeError as err:\n        raise TypeError(\"Gene start has to be a integer\")\n\n    try:\n        end = int(gene_info['end'])\n    except KeyError as err:\n        raise KeyError(\"Gene has to have a end position\")\n    except TypeError as err:\n        raise TypeError(\"Gene end has to be a integer\")\n\n    gene_obj = HgncGene(\n        hgnc_id=hgnc_id, \n        hgnc_symbol=hgnc_symbol, \n        ensembl_id=ensembl_id, \n        chrom=chromosome, \n        start=start, \n        end=end,\n        build=build,\n    )\n    \n    if gene_info.get('description'):\n        gene_obj['description'] = gene_info['description']\n        # LOG.debug(\"Adding info %s\", gene_info['description'])\n    \n    if gene_info.get('previous_symbols'):\n        gene_obj['aliases'] = gene_info['previous_symbols']\n\n    if gene_info.get('entrez_id'):\n        gene_obj['entrez_id'] = int(gene_info['entrez_id'])\n    \n    if gene_info.get('omim_id'):\n        gene_obj['omim_id'] = int(gene_info['omim_id'])\n\n    if gene_info.get('pli_score'):\n        gene_obj['pli_score'] = float(gene_info['pli_score'])\n    \n    if gene_info.get('ref_seq'):\n        gene_obj['primary_transcripts'] = gene_info['ref_seq']\n    \n    if gene_info.get('ucsc_id'):\n        gene_obj['ucsc_id'] = gene_info['ucsc_id']\n    \n    if gene_info.get('uniprot_ids'):\n        gene_obj['uniprot_ids'] = gene_info['uniprot_ids']\n\n    if gene_info.get('vega_id'):\n        gene_obj['vega_id'] = gene_info['vega_id']\n    \n    if gene_info.get('incomplete_penetrance'):\n        gene_obj['incomplete_penetrance'] = True\n\n    if gene_info.get('inheritance_models'):\n        gene_obj['inheritance_models'] = gene_info['inheritance_models']\n\n    phenotype_objs = []\n    for phenotype_info in gene_info.get('phenotypes', []):\n        phenotype_objs.append(build_phenotype(phenotype_info))\n    \n    if phenotype_objs:\n        gene_obj['phenotypes'] = phenotype_objs\n\n    for key in list(gene_obj):\n        if gene_obj[key] is None:\n            gene_obj.pop(key)\n\n    return gene_obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading a gene panel based on the info sent", "response": "def load_panel(self, parsed_panel):\n        \"\"\"Load a gene panel based on the info sent\n        A panel object is built and integrity checks are made.\n        The panel object is then loaded into the database.\n\n        Args:\n            path(str): Path to panel file\n            institute(str): Name of institute that owns the panel\n            panel_id(str): Panel id\n            date(datetime.datetime): Date of creation\n            version(float)\n            full_name(str): Option to have a long name\n\n            panel_info(dict): {\n                'file': <path to panel file>(str),\n                'institute': <institute>(str),\n                'type': <panel type>(str),\n                'date': date,\n                'version': version,\n                'panel_name': panel_id,\n                'full_name': name,\n            }\n        \"\"\"\n        panel_obj = build_panel(parsed_panel, self)\n\n        self.add_gene_panel(panel_obj)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate and load the OMIM - AUTO panel", "response": "def load_omim_panel(self, api_key, institute=None):\n        \"\"\"Create and load the OMIM-AUTO panel\"\"\"\n        existing_panel = self.gene_panel(panel_id='OMIM-AUTO')\n        if not existing_panel:\n            LOG.warning(\"OMIM-AUTO does not exists in database\")\n            LOG.info('Creating a first version')\n            version = 1.0\n\n        if existing_panel:\n            version = float(math.floor(existing_panel['version']) + 1)\n\n        LOG.info(\"Setting version to %s\", version)\n\n        try:\n            mim_files = fetch_mim_files(api_key=api_key, genemap2=True, mim2genes=True)\n        except Exception as err:\n            raise err\n\n        date_string = None\n        # Get the correct date when omim files where released\n        for line in mim_files['genemap2']:\n            if 'Generated' in line:\n                date_string = line.split(':')[-1].lstrip().rstrip()\n        date_obj = get_date(date_string)\n\n        if existing_panel:\n            if existing_panel['date'] == date_obj:\n                LOG.warning(\"There is no new version of OMIM\")\n                return\n\n        panel_data = {}\n        panel_data['path'] = None\n        panel_data['type'] = 'clinical'\n        panel_data['date'] = date_obj\n        panel_data['panel_id'] = 'OMIM-AUTO'\n        panel_data['institute'] = institute or 'cust002'\n        panel_data['version'] = version\n        panel_data['display_name'] = 'OMIM-AUTO'\n        panel_data['genes'] = []\n\n        alias_genes = self.genes_by_alias()\n\n        genes = get_omim_panel_genes(\n            genemap2_lines = mim_files['genemap2'],\n            mim2gene_lines = mim_files['mim2genes'],\n            alias_genes = alias_genes,\n        )\n\n        for gene in genes:\n            panel_data['genes'].append(gene)\n\n        panel_obj = build_panel(panel_data, self)\n\n        if existing_panel:\n\n            new_genes = self.compare_mim_panels(existing_panel, panel_obj)\n            if new_genes:\n                self.update_mim_version(new_genes, panel_obj, old_version=existing_panel['version'])\n            else:\n                LOG.info(\"The new version of omim does not differ from the old one\")\n                LOG.info(\"No update is added\")\n                return\n\n        self.add_gene_panel(panel_obj)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if the latest version of OMIM differs from the most recent in database Return all genes that where not in the previous version", "response": "def compare_mim_panels(self, existing_panel, new_panel):\n        \"\"\"Check if the latest version of OMIM differs from the most recent in database\n           Return all genes that where not in the previous version.\n\n        Args:\n            existing_panel(dict)\n            new_panel(dict)\n\n        Returns:\n            new_genes(set(str))\n        \"\"\"\n        existing_genes = set([gene['hgnc_id'] for gene in existing_panel['genes']])\n        new_genes = set([gene['hgnc_id'] for gene in new_panel['genes']])\n\n        return new_genes.difference(existing_genes)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the correct version for each gene in the new panel with the new version.", "response": "def update_mim_version(self, new_genes, new_panel, old_version):\n        \"\"\"Set the correct version for each gene\n        Loop over the genes in the new panel\n\n        Args:\n            new_genes(set(str)): Set with the new gene symbols\n            new_panel(dict)\n\n        \"\"\"\n        LOG.info('Updating versions for new genes')\n        version = new_panel['version']\n        for gene in new_panel['genes']:\n            gene_symbol = gene['hgnc_id']\n            # If the gene is new we add the version\n            if gene_symbol in new_genes:\n                gene['database_entry_version'] = version\n                continue\n            # If the gene is old it will have the previous version\n            gene['database_entry_version'] = old_version\n\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_gene_panel(self, panel_obj):\n        panel_name = panel_obj['panel_name']\n        panel_version = panel_obj['version']\n        display_name = panel_obj.get('display_name', panel_name)\n\n        if self.gene_panel(panel_name, panel_version):\n            raise IntegrityError(\"Panel {0} with version {1} already\"\n                                 \" exist in database\".format(panel_name, panel_version))\n        LOG.info(\"loading panel {0}, version {1} to database\".format(\n            display_name, panel_version\n        ))\n        result = self.panel_collection.insert_one(panel_obj)\n        LOG.debug(\"Panel saved\")\n        return result.inserted_id", "response": "Add a gene panel to the database. Returns the ID of the newly added gene panel."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef panel(self, panel_id):\n        if not isinstance(panel_id, ObjectId):\n            panel_id = ObjectId(panel_id)\n        panel_obj = self.panel_collection.find_one({'_id': panel_id})\n        return panel_obj", "response": "Fetch a gene panel by _id."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndelete a panel by _id.", "response": "def delete_panel(self, panel_obj):\n        \"\"\"Delete a panel by '_id'.\n\n        Args:\n            panel_obj(dict)\n\n        Returns:\n            res(pymongo.DeleteResult)\n        \"\"\"\n        res = self.panel_collection.delete_one({'_id': panel_obj['_id']})\n        LOG.warning(\"Deleting panel %s, version %s\" % (panel_obj['panel_name'], panel_obj['version']))\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfetch a gene panel object from the database.", "response": "def gene_panel(self, panel_id, version=None):\n        \"\"\"Fetch a gene panel.\n\n        If no panel is sent return all panels\n\n        Args:\n            panel_id (str): unique id for the panel\n            version (str): version of the panel. If 'None' latest version will be returned\n\n        Returns:\n            gene_panel: gene panel object\n        \"\"\"\n        query = {'panel_name': panel_id}\n        if version:\n            LOG.info(\"Fetch gene panel {0}, version {1} from database\".format(\n                panel_id, version\n            ))\n            query['version'] = version\n            return self.panel_collection.find_one(query)\n        else:\n            LOG.info(\"Fetching gene panels %s from database\", panel_id)\n            res = self.panel_collection.find(query).sort('version', -1)\n            if res.count() > 0:\n                return res[0]\n            else:\n                LOG.info(\"No gene panel found\")\n                return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns all gene panels by that panel name. If panel_id return all versions of panels by that panel name. If institute_id return all versions of panels by that panel name. If version return all versions of panels by that panel name. If institute_id return all versions of panels by that panel name.", "response": "def gene_panels(self, panel_id=None, institute_id=None, version=None):\n        \"\"\"Return all gene panels\n\n        If panel_id return all versions of panels by that panel name\n\n        Args:\n            panel_id(str)\n\n        Returns:\n            cursor(pymongo.cursor)\n        \"\"\"\n        query = {}\n        if panel_id:\n            query['panel_name'] = panel_id\n            if version:\n                query['version'] = version\n        if institute_id:\n            query['institute'] = institute_id\n\n        return self.panel_collection.find(query)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef gene_to_panels(self, case_obj):\n        LOG.info(\"Building gene to panels\")\n        gene_dict = {}\n\n        for panel_info in case_obj.get('panels', []):\n            panel_name = panel_info['panel_name']\n            panel_version = panel_info['version']\n            panel_obj = self.gene_panel(panel_name, version=panel_version)\n            if not panel_obj:\n                ## Raise exception here???\n                LOG.warning(\"Panel: {0}, version {1} does not exist in database\".format(panel_name, panel_version))\n\n            for gene in panel_obj['genes']:\n                hgnc_id = gene['hgnc_id']\n\n                if hgnc_id not in gene_dict:\n                    gene_dict[hgnc_id] = set([panel_name])\n                    continue\n\n                gene_dict[hgnc_id].add(panel_name)\n\n        LOG.info(\"Gene to panels done\")\n\n        return gene_dict", "response": "Fetch all gene panels and group them by gene"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_panel(self, panel_obj, version=None, date_obj=None):\n        LOG.info(\"Updating panel %s\", panel_obj['panel_name'])\n        # update date of panel to \"today\"\n        date = panel_obj['date']\n        if version:\n            LOG.info(\"Updating version from {0} to version {1}\".format(\n                panel_obj['version'], version))\n            panel_obj['version'] = version\n            # Updating version should not update date\n            if date_obj:\n                date = date_obj\n        else:\n            date = date_obj or dt.datetime.now()\n        panel_obj['date'] = date\n\n        updated_panel = self.panel_collection.find_one_and_replace(\n            {'_id': panel_obj['_id']},\n            panel_obj,\n            return_document=pymongo.ReturnDocument.AFTER\n        )\n\n        return updated_panel", "response": "Update a panel with a new one."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a pending action to a panel", "response": "def add_pending(self, panel_obj, hgnc_gene, action, info=None):\n        \"\"\"Add a pending action to a gene panel\n\n        Store the pending actions in panel.pending\n\n        Args:\n            panel_obj(dict): The panel that is about to be updated\n            hgnc_gene(dict)\n            action(str): choices=['add','delete','edit']\n            info(dict): additional gene info (disease_associated_transcripts,\n                        reduced_penetrance, mosaicism, database_entry_version ,\n                        inheritance_models, comment)\n\n        Returns:\n            updated_panel(dict):\n\n        \"\"\"\n\n        valid_actions = ['add', 'delete', 'edit']\n        if action not in valid_actions:\n            raise ValueError(\"Invalid action {0}\".format(action))\n\n        info = info or {}\n        pending_action = {\n            'hgnc_id': hgnc_gene['hgnc_id'],\n            'action': action,\n            'info': info,\n            'symbol': hgnc_gene['hgnc_symbol'],\n        }\n\n        updated_panel = self.panel_collection.find_one_and_update(\n            {'_id': panel_obj['_id']},\n            {\n                '$addToSet': {\n                    'pending': pending_action\n                }\n            },\n            return_document=pymongo.ReturnDocument.AFTER\n        )\n\n        return updated_panel"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\napplying the pending changes to an existing gene panel or create a new version of the same panel.", "response": "def apply_pending(self, panel_obj, version):\n        \"\"\"Apply the pending changes to an existing gene panel or create a new version of the same panel.\n\n        Args:\n            panel_obj(dict): panel in database to update\n            version(double): panel version to update\n\n        Returns:\n            inserted_id(str): id of updated panel or the new one\n        \"\"\"\n\n        updates = {}\n        new_panel = deepcopy(panel_obj)\n        new_panel['pending'] = []\n        new_panel['date'] = dt.datetime.now()\n        info_fields = ['disease_associated_transcripts', 'inheritance_models', 'reduced_penetrance',\n            'mosaicism', 'database_entry_version', 'comment']\n        new_genes = []\n\n        for update in panel_obj.get('pending', []):\n            hgnc_id = update['hgnc_id']\n\n            # If action is add we create a new gene object\n            if update['action'] != 'add':\n                updates[hgnc_id] = update\n                continue\n            info = update.get('info', {})\n            gene_obj = {\n                'hgnc_id': hgnc_id,\n                'symbol': update['symbol']\n            }\n\n            for field in info_fields:\n                if field in info:\n                    gene_obj[field] = info[field]\n            new_genes.append(gene_obj)\n\n        for gene in panel_obj['genes']:\n            hgnc_id = gene['hgnc_id']\n\n            if hgnc_id not in updates:\n                new_genes.append(gene)\n                continue\n\n            current_update = updates[hgnc_id]\n            action = current_update['action']\n            info = current_update['info']\n\n            # If action is delete we do not add the gene to new genes\n            if action == 'delete':\n                continue\n\n            elif action == 'edit':\n                for field in info_fields:\n                    if field in info:\n                        gene[field] = info[field]\n                new_genes.append(gene)\n\n        new_panel['genes'] = new_genes\n        new_panel['version'] = float(version)\n\n        inserted_id = None\n        # if the same version of the panel should be updated\n        if new_panel['version'] == panel_obj['version']:\n            # replace panel_obj with new_panel\n            result = self.panel_collection.find_one_and_replace(\n                {'_id':panel_obj['_id']},\n                new_panel,\n                return_document=pymongo.ReturnDocument.AFTER\n            )\n            inserted_id = result['_id']\n        else: # create a new version of the same panel\n            new_panel.pop('_id')\n\n            # archive the old panel\n            panel_obj['is_archived'] = True\n            self.update_panel(panel_obj=panel_obj, date_obj=panel_obj['date'])\n\n            # insert the new panel\n            inserted_id = self.panel_collection.insert_one(new_panel).inserted_id\n\n        return inserted_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clinical_symbols(self, case_obj):\n        panel_ids = [panel['panel_id'] for panel in case_obj['panels']]\n        query = self.panel_collection.aggregate([\n            {'$match': {'_id': {'$in': panel_ids}}},\n            {'$unwind': '$genes'},\n            {'$group': {'_id': '$genes.symbol'}}\n        ])\n        return set(item['_id'] for item in query)", "response": "Return all the clinical gene symbols for a case."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cases(context, case_id, institute, reruns, finished, causatives, research_requested,\n          is_research, status, json):\n    \"\"\"Interact with cases existing in the database.\"\"\"\n    adapter = context.obj['adapter']\n\n    models = []\n    if case_id:\n        case_obj = adapter.case(case_id=case_id)\n        if case_obj:\n            models.append(case_obj)\n        else:\n            LOG.info(\"No case with id {}\".format(case_id))\n\n    else:\n        models = adapter.cases(collaborator=institute, reruns=reruns,\n                           finished=finished, has_causatives=causatives,\n                           research_requested=research_requested,\n                           is_research=is_research, status=status)\n        models = [case_obj for case_obj in models]\n        if len(models) == 0:\n            LOG.info(\"No cases could be found\")\n\n    if json:\n        click.echo(dumps(models))\n        return\n\n    for model in models:\n        pp(model)", "response": "Interact with cases existing in the database."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef emit(self, record):\n        try:\n            import smtplib\n            try:\n                from email.utils import formatdate\n            except ImportError:\n                formatdate = self.date_time\n\n            port = self.mailport\n            if not port:\n                port = smtplib.SMTP_PORT\n\n            smtp = smtplib.SMTP(self.mailhost, port)\n            msg = self.format(record)\n            msg = \"From: %s\\r\\nTo: %s\\r\\nSubject: %s\\r\\nDate: %s\\r\\n\\r\\n%s\" % (\n                self.fromaddr,\n                ','.join(self.toaddrs),\n                self.getSubject(record),\n                formatdate(), msg\n            )\n\n            if self.username:\n                smtp.ehlo()  # For 'tls', add this line\n                smtp.starttls()  # For 'tls', add this line\n                smtp.ehlo()  # For 'tls', add this line\n                smtp.login(self.username, self.password)\n\n            smtp.sendmail(self.fromaddr, self.toaddrs, msg)\n            smtp.quit()\n\n        except (KeyboardInterrupt, SystemExit):\n            raise\n\n        except:\n            self.handleError(record)", "response": "Emit a record and send it to the specified addressees."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list with the current indexes", "response": "def indexes(self, collection=None):\n        \"\"\"Return a list with the current indexes\n        \n        Skip the mandatory _id_ indexes\n        \n        Args:\n            collection(str)\n\n        Returns:\n            indexes(list)\n        \"\"\"\n        \n        indexes = []\n\n        for collection_name in self.collections():\n            if collection and collection != collection_name:\n                continue\n            for index_name in self.db[collection_name].index_information():\n                if index_name != '_id_':\n                    indexes.append(index_name)\n        return indexes"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads the proper indexes for the current scout instance.", "response": "def load_indexes(self):\n        \"\"\"Add the proper indexes to the scout instance.\n\n        All indexes are specified in scout/constants/indexes.py\n\n        If this method is utilised when new indexes are defined those should be added\n\n        \"\"\"\n        for collection_name in INDEXES:\n            existing_indexes = self.indexes(collection_name)\n            indexes = INDEXES[collection_name]\n            for index in indexes:\n                index_name = index.document.get('name')\n                if index_name in existing_indexes:\n                    LOG.info(\"Deleting old index: %s\" % index_name)\n                    self.db[collection_name].drop_index(index_name)\n            LOG.info(\"creating indexes for {0} collection: {1}\".format(\n                collection_name,\n                ', '.join([index.document.get('name') for index in indexes])\n                ))\n            self.db[collection_name].create_indexes(indexes)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the indexes in the database.", "response": "def update_indexes(self):\n        \"\"\"Update the indexes\n        \n        If there are any indexes that are not added to the database, add those.\n\n        \"\"\"\n        LOG.info(\"Updating indexes...\")\n        nr_updated = 0\n        for collection_name in INDEXES:\n            existing_indexes = self.indexes(collection_name)\n            indexes = INDEXES[collection_name]\n            for index in indexes:\n                index_name = index.document.get('name')\n                if index_name not in existing_indexes:\n                    nr_updated += 1\n                    LOG.info(\"Adding index : %s\" % index_name)\n                    self.db[collection_name].create_indexes(indexes)\n        if nr_updated == 0:\n            LOG.info(\"All indexes in place\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef drop_indexes(self):\n        LOG.warning(\"Dropping all indexe\")\n        for collection_name in INDEXES:\n            LOG.warning(\"Dropping all indexes for collection name %s\", collection_name)\n            self.db[collection_name].drop_indexes()", "response": "Delete all indexes for the database"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef build_variant_query(self, query=None, category='snv', variant_type=['clinical']):\n\n        query = query or {}\n        mongo_variant_query = {}\n\n        LOG.debug(\"Building a mongo query for %s\" % query)\n\n        if query.get('hgnc_symbols'):\n            mongo_variant_query['hgnc_symbols'] = {'$in': query['hgnc_symbols']}\n\n        mongo_variant_query['variant_type'] = {'$in': variant_type}\n\n        mongo_variant_query['category'] = category\n\n        rank_score = query.get('rank_score') or 15\n\n        mongo_variant_query['rank_score'] = {'$gte': rank_score}\n        LOG.debug(\"Querying %s\" % mongo_variant_query)\n\n        return mongo_variant_query", "response": "Build a mongo query across multiple cases."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_query(self, case_id, query=None, variant_ids=None, category='snv'):\n        query = query or {}\n        mongo_query = {}\n        gene_query = None\n\n        ##### Base query params\n\n        # set up the fundamental query params: case_id, category, type and\n        # restrict to list of variants (if var list is provided)\n        for criterion in FUNDAMENTAL_CRITERIA:\n            if criterion == 'case_id':\n                LOG.debug(\"Building a mongo query for %s\" % case_id)\n                mongo_query['case_id'] = case_id\n\n            elif criterion == 'variant_ids' and variant_ids:\n                LOG.debug(\"Adding variant_ids %s to query\" % ', '.join(variant_ids))\n                mongo_query['variant_id'] = {'$in': variant_ids}\n\n            elif criterion == 'category':\n                LOG.debug(\"Querying category %s\" % category)\n                mongo_query['category'] = category\n\n            elif criterion == 'variant_type':\n                mongo_query['variant_type'] = query.get('variant_type', 'clinical')\n                LOG.debug(\"Set variant type to %s\", mongo_query['variant_type'])\n\n            # Requests to filter based on gene panels, hgnc_symbols or\n            # coordinate ranges must always be honored. They are always added to\n            # query as top level, implicit '$and'. When both hgnc_symbols and a\n            # panel is used, addition of this is delayed until after the rest of\n            # the query content is clear.\n\n            elif criterion in ['hgnc_symbols', 'gene_panels'] and gene_query is None:\n                gene_query = self.gene_filter(query, mongo_query)\n\n            elif criterion == 'chrom' and query.get('chrom'): # filter by coordinates\n                self.coordinate_filter(query, mongo_query)\n\n            elif criterion == 'variant_ids' and variant_ids:\n                LOG.debug(\"Adding variant_ids %s to query\" % ', '.join(variant_ids))\n                mongo_query['variant_id'] = {'$in': variant_ids}\n\n            ##### end of fundamental query params\n\n        ##### start of the custom query params\n        # there is only 'clinsig' criterion among the primary terms right now\n        primary_terms = False\n\n        # gnomad_frequency, local_obs, clingen_ngi, swegen, spidex_human, cadd_score, genetic_models, mvl_tag\n        # functional_annotations, region_annotations, size, svtype, decipher, depth, alt_count, control_frequency\n        secondary_terms = False\n\n        # check if any of the primary criteria was specified in the query\n        for term in PRIMARY_CRITERIA:\n            if query.get(term):\n                primary_terms = True\n\n        # check if any of the secondary criteria was specified in the query:\n        for term in SECONDARY_CRITERIA:\n            if query.get(term):\n                secondary_terms = True\n\n        if primary_terms is True:\n            clinsign_filter = self.clinsig_query(query, mongo_query)\n\n        # Secondary, excluding filter criteria will hide variants in general,\n        # but can be overridden by an including, major filter criteria\n        # such as a Pathogenic ClinSig.\n        if secondary_terms is True:\n            secondary_filter = self.secondary_query(query, mongo_query)\n\n            # If there are no primary criteria given, all secondary criteria are added as a\n            # top level '$and' to the query.\n            if primary_terms is False:\n                if gene_query:\n                    mongo_query['$and'] = [ {'$or': gene_query}, {'$and': secondary_filter}]\n                else:\n                    mongo_query['$and'] = secondary_filter\n\n            # If there is only one primary criterion given without any secondary, it will also be\n            # added as a top level '$and'.\n            # Otherwise, primary criteria are added as a high level '$or' and all secondary criteria\n            # are joined together with them as a single lower level '$and'.\n            if primary_terms is True: # clinsig is specified\n\n                # Given a request to always return confident clinical variants,\n                # add the clnsig query as a major criteria, but only\n                # trust clnsig entries with trusted revstat levels.\n                if query.get('clinsig_confident_always_returned') == True:\n                    if gene_query:\n                        mongo_query['$and'] = [\n                            {'$or': gene_query},\n                            {\n                                '$or': [\n                                    {'$and': secondary_filter}, clinsign_filter\n                                ]\n                            }\n                        ]\n                    else:\n                        mongo_query['$or'] = [ {'$and': secondary_filter}, clinsign_filter ]\n                else: # clisig terms are provided but no need for trusted revstat levels\n                    secondary_filter.append(clinsign_filter)\n                    if gene_query:\n                        mongo_query['$and'] = [ {'$or': gene_query}, {'$and': secondary_filter}]\n                    else:\n                        mongo_query['$and'] = secondary_filter\n\n        elif primary_terms is True: # clisig is provided without secondary terms query\n            # use implicit and\n            mongo_query['clnsig'] = clinsign_filter['clnsig']\n            if gene_query:\n                mongo_query['$and'] = [{ '$or': gene_query }]\n\n        elif gene_query: # no primary or secondary filters provided\n            mongo_query['$and'] = [{ '$or': gene_query }]\n\n        LOG.info(\"mongo query: %s\", mongo_query)\n\n        return mongo_query", "response": "Build a mongo query for the users of the md5 variant ids of the users of the md5 variant ids."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clinsig_query(self, query, mongo_query):\n        LOG.debug('clinsig is a query parameter')\n        trusted_revision_level = ['mult', 'single', 'exp', 'guideline']\n        rank = []\n        str_rank = []\n        clnsig_query = {}\n\n        for item in query['clinsig']:\n            rank.append(int(item))\n            # search for human readable clinsig values in newer cases\n            rank.append(CLINSIG_MAP[int(item)])\n            str_rank.append(CLINSIG_MAP[int(item)])\n\n        if query.get('clinsig_confident_always_returned') == True:\n            LOG.debug(\"add CLINSIG filter with trusted_revision_level\")\n\n            clnsig_query = { \"clnsig\":\n                        {\n                            '$elemMatch': {\n                                '$or' : [\n                                    {\n                                        '$and' : [\n                                             {'value' : { '$in': rank }},\n                                             {'revstat': { '$in': trusted_revision_level }}\n                                        ]\n                                    },\n                                    {\n                                        '$and': [\n                                            {'value' : re.compile('|'.join(str_rank))},\n                                            {'revstat' : re.compile('|'.join(trusted_revision_level))}\n                                        ]\n                                    }\n                                ]\n                            }\n                         }\n                    }\n        else:\n            LOG.debug(\"add CLINSIG filter for rank: %s\" %\n                             ', '.join(str(query['clinsig'])))\n\n            clnsig_query = {\n                    \"clnsig\":\n                        {\n                            '$elemMatch': {\n                                '$or' : [\n                                    { 'value' : { '$in': rank }},\n                                    { 'value' : re.compile('|'.join(str_rank)) }\n                                ]\n                            }\n                        }\n                }\n\n        return clnsig_query", "response": "Add clinsig filter values to the mongo query object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding genomic coordinated - related filters to the query", "response": "def coordinate_filter(self, query, mongo_query):\n        \"\"\" Adds genomic coordinated-related filters to the query object\n\n        Args:\n            query(dict): a dictionary of query filters specified by the users\n            mongo_query(dict): the query that is going to be submitted to the database\n\n        Returns:\n            mongo_query(dict): returned object contains coordinate filters\n\n        \"\"\"\n        LOG.debug('Adding genomic coordinates to the query')\n        chromosome = query['chrom']\n        mongo_query['chromosome'] = chromosome\n\n        if (query.get('start') and query.get('end')):\n            mongo_query['position'] = {'$lte': int(query['end'])}\n            mongo_query['end'] = {'$gte': int(query['start'])}\n\n        return mongo_query"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef gene_filter(self, query, mongo_query):\n        LOG.debug('Adding panel and genes-related parameters to the query')\n\n        gene_query = []\n\n        if query.get('hgnc_symbols') and query.get('gene_panels'):\n            gene_query.append({'hgnc_symbols': {'$in': query['hgnc_symbols']}})\n            gene_query.append({'panels': {'$in': query['gene_panels']}})\n            mongo_query['$or']=gene_query\n        else:\n            if query.get('hgnc_symbols'):\n                hgnc_symbols = query['hgnc_symbols']\n                mongo_query['hgnc_symbols'] = {'$in': hgnc_symbols}\n                LOG.debug(\"Adding hgnc_symbols: %s to query\" %\n                             ', '.join(hgnc_symbols))\n\n            if query.get('gene_panels'):\n                gene_panels = query['gene_panels']\n                mongo_query['panels'] = {'$in': gene_panels}\n\n        return gene_query", "response": "Adds gene - related filters to the query that is going to be submitted to the database"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a secondary query object based on the secondary parameters of the user.", "response": "def secondary_query(self, query, mongo_query, secondary_filter=None):\n        \"\"\"Creates a secondary query object based on secondary parameters specified by user\n\n            Args:\n                query(dict): a dictionary of query filters specified by the users\n                mongo_query(dict): the query that is going to be submitted to the database\n\n            Returns:\n                mongo_secondary_query(list): a dictionary with secondary query parameters\n\n        \"\"\"\n        LOG.debug('Creating a query object with secondary parameters')\n\n        mongo_secondary_query = []\n\n        # loop over secondary query criteria\n        for criterion in SECONDARY_CRITERIA:\n            if not query.get(criterion):\n                continue\n\n            if criterion == 'gnomad_frequency':\n                gnomad = query.get('gnomad_frequency')\n                if gnomad == '-1':\n                    # -1 means to exclude all variants that exists in gnomad\n                    mongo_query['gnomad_frequency'] = {'$exists': False}\n                else:\n                    # Replace comma with dot\n                    mongo_secondary_query.append(\n                        {\n                            '$or': [\n                                {\n                                    'gnomad_frequency': {'$lt': float(gnomad)}\n                                },\n                                {\n                                    'gnomad_frequency': {'$exists': False}\n                                }\n                            ]\n                        }\n                    )\n                LOG.debug(\"Adding gnomad_frequency to query\")\n\n            if criterion == 'local_obs':\n                local_obs = query.get('local_obs')\n                mongo_secondary_query.append({\n                    '$or': [\n                        {'local_obs_old': None},\n                        {'local_obs_old': {'$lt': local_obs + 1}},\n                    ]\n                })\n\n            if criterion in ['clingen_ngi', 'swegen']:\n                mongo_secondary_query.append({\n                    '$or': [\n                        { criterion : {'$exists': False}},\n                        { criterion : {'$lt': query[criterion] + 1}},\n                    ]\n                })\n\n            if criterion == 'spidex_human':\n                # construct spidex query. Build the or part starting with empty SPIDEX values\n                spidex_human = query['spidex_human']\n\n                spidex_query_or_part = []\n                if ( 'not_reported' in spidex_human):\n                    spidex_query_or_part.append({'spidex': {'$exists': False}})\n\n                for spidex_level in SPIDEX_HUMAN:\n                    if ( spidex_level in spidex_human ):\n                        spidex_query_or_part.append({'$or': [\n                                    {'$and': [{'spidex': {'$gt': SPIDEX_HUMAN[spidex_level]['neg'][0]}},\n                                              {'spidex': {'$lt': SPIDEX_HUMAN[spidex_level]['neg'][1]}}]},\n                                    {'$and': [{'spidex': {'$gt': SPIDEX_HUMAN[spidex_level]['pos'][0]}},\n                                              {'spidex': {'$lt': SPIDEX_HUMAN[spidex_level]['pos'][1]}} ]} ]})\n\n                mongo_secondary_query.append({'$or': spidex_query_or_part })\n\n            if criterion == 'cadd_score':\n                cadd = query['cadd_score']\n                cadd_query = {'cadd_score': {'$gt': float(cadd)}}\n                LOG.debug(\"Adding cadd_score: %s to query\", cadd)\n\n                if query.get('cadd_inclusive') is True:\n                    cadd_query = {\n                        '$or': [\n                            cadd_query,\n                            {'cadd_score': {'$exists': False}}\n                            ]}\n                    LOG.debug(\"Adding cadd inclusive to query\")\n\n                mongo_secondary_query.append(cadd_query)\n\n            if criterion in ['genetic_models', 'functional_annotations', 'region_annotations']:\n                criterion_values = query[criterion]\n                if criterion == 'genetic_models':\n                    mongo_secondary_query.append({criterion: {'$in': criterion_values}})\n                else:\n                    # filter key will be genes.[criterion (minus final char)]\n                    mongo_secondary_query.append({ '.'.join(['genes', criterion[:-1]]) : {'$in': criterion_values}})\n\n                LOG.debug(\"Adding {0}: {1} to query\".format(criterion, ', '.join(criterion_values)))\n\n            if criterion == 'size':\n                size = query['size']\n                size_query = {'length': {'$gt': int(size)}}\n                LOG.debug(\"Adding length: %s to query\" % size)\n\n                if query.get('size_shorter'):\n                    size_query = {\n                        '$or': [\n                            {'length': {'$lt': int(size)}},\n                            {'length': {'$exists': False}}\n                        ]}\n                    LOG.debug(\"Adding size less than, undef inclusive to query.\")\n\n                mongo_secondary_query.append(size_query)\n\n            if criterion == 'svtype':\n                svtype = query['svtype']\n                mongo_secondary_query.append({'sub_category': {'$in': svtype}})\n                LOG.debug(\"Adding SV_type %s to query\" %\n                             ', '.join(svtype))\n\n            if criterion == 'decipher':\n                mongo_query['decipher'] = {'$exists': True}\n                LOG.debug(\"Adding decipher to query\")\n\n            if criterion == 'depth':\n                LOG.debug(\"add depth filter\")\n                mongo_secondary_query.append({\n                    'tumor.read_depth': {\n                        '$gt': query.get('depth'),\n                    }\n                })\n\n            if criterion == 'alt_count':\n                LOG.debug(\"add min alt count filter\")\n                mongo_secondary_query.append({\n                    'tumor.alt_depth': {\n                        '$gt': query.get('alt_count'),\n                    }\n                })\n\n            if criterion == 'control_frequency':\n                LOG.debug(\"add minimum control frequency filter\")\n                mongo_secondary_query.append({\n                    'normal.alt_freq': {\n                        '$lt': float(query.get('control_frequency')),\n                    }\n                })\n\n            if criterion == 'mvl_tag':\n                LOG.debug(\"add managed variant list filter\")\n                mongo_secondary_query.append({\n                    'mvl_tag': {\n                        '$exists': True,\n                    }\n                })\n\n        return mongo_secondary_query"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef wipe(ctx):\n    LOG.info(\"Running scout wipe\")\n    db_name = ctx.obj['mongodb']\n    LOG.info(\"Dropping database %s\", db_name)\n    try:\n        ctx.obj['client'].drop_database(db_name)\n    except Exception as err:\n        LOG.warning(err)\n        ctx.abort()\n    LOG.info(\"Dropped whole database\")", "response": "Drop the mongo database given."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing user submitted panel.", "response": "def parse_panel(csv_stream):\n    \"\"\"Parse user submitted panel.\"\"\"\n    reader = csv.DictReader(csv_stream, delimiter=';', quoting=csv.QUOTE_NONE)\n    genes = []\n    for gene_row in reader:\n        if not gene_row['HGNC_IDnumber'].strip().isdigit():\n            continue\n        transcripts_raw = gene_row.get('Disease_associated_transcript')\n        if transcripts_raw:\n            transcripts_list = [tx.split(':', 1)[-1].strip() for tx in transcripts_raw.split(',')]\n        else:\n            transcripts_list = []\n\n        models_raw = gene_row.get('Genetic_disease_model')\n        models_list = [model.strip() for model in models_raw.split(',')] if models_raw else []\n        panel_gene = dict(\n            symbol=gene_row['HGNC_symbol'].strip() if gene_row.get('HGNC_symbol') else None,\n            hgnc_id=int(gene_row['HGNC_IDnumber'].strip()),\n            disease_associated_transcripts=transcripts_list,\n            reduced_penetrance=True if gene_row.get('Reduced_penetrance') else None,\n            mosaicism=True if gene_row.get('Mosaicism') else None,\n            inheritance_models=models_list,\n            database_entry_version=gene_row.get('Database_entry_version'),\n        )\n        genes.append(panel_gene)\n\n    return genes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_hgnc_bulk(self, gene_objs):\n\n        LOG.info(\"Loading gene bulk with length %s\", len(gene_objs))\n        try:\n            result = self.hgnc_collection.insert_many(gene_objs)\n        except (DuplicateKeyError, BulkWriteError) as err:\n            raise IntegrityError(err)\n\n        return result", "response": "Load a bulk of hgnc gene objects into the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads a bulk of transcript objects to the database", "response": "def load_transcript_bulk(self, transcript_objs):\n        \"\"\"Load a bulk of transcript objects to the database\n\n        Arguments:\n            transcript_objs(iterable(scout.models.hgnc_transcript))\n\n        \"\"\"\n        LOG.info(\"Loading transcript bulk\")\n        try:\n            result = self.transcript_collection.insert_many(transcript_objs)\n        except (DuplicateKeyError, BulkWriteError) as err:\n            raise IntegrityError(err)\n        \n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload a bulk of exon objects to the database", "response": "def load_exon_bulk(self, exon_objs):\n        \"\"\"Load a bulk of exon objects to the database\n\n        Arguments:\n            exon_objs(iterable(scout.models.hgnc_exon))\n\n        \"\"\"\n        try:\n            result = self.exon_collection.insert_many(transcript_objs)\n        except (DuplicateKeyError, BulkWriteError) as err:\n            raise IntegrityError(err)\n        \n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfetch a HGNC gene from the database", "response": "def hgnc_gene(self, hgnc_identifier, build='37'):\n        \"\"\"Fetch a hgnc gene\n\n            Args:\n                hgnc_identifier(int)\n\n            Returns:\n                gene_obj(HgncGene)\n        \"\"\"\n        if not build in ['37', '38']:\n            build = '37'\n        query = {}\n        try:\n            # If the identifier is a integer we search for hgnc_id\n            hgnc_identifier = int(hgnc_identifier)\n            query['hgnc_id'] = hgnc_identifier\n        except ValueError:\n            # Else we seach for a hgnc_symbol\n            query['hgnc_symbol'] = hgnc_identifier\n\n        query['build'] = build\n        LOG.debug(\"Fetching gene %s\" % hgnc_identifier)\n        gene_obj = self.hgnc_collection.find_one(query)\n        if not gene_obj:\n            return None\n        \n        # Add the transcripts:\n        transcripts = []\n        tx_objs = self.transcripts(build=build, hgnc_id=gene_obj['hgnc_id'])\n        if tx_objs.count() > 0:\n            for tx in tx_objs:\n                transcripts.append(tx)\n        gene_obj['transcripts'] = transcripts\n        \n        return gene_obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef hgnc_id(self, hgnc_symbol, build='37'):\n        #LOG.debug(\"Fetching gene %s\", hgnc_symbol)\n        query = {'hgnc_symbol':hgnc_symbol, 'build':build}\n        projection = {'hgnc_id':1, '_id':0}\n        res = self.hgnc_collection.find(query, projection)\n\n        if res.count() > 0:\n            return res[0]['hgnc_id']\n        else:\n            return None", "response": "Query the genes with a hgnc symbol and return the hgnc id"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef hgnc_genes(self, hgnc_symbol, build='37', search=False):\n        LOG.debug(\"Fetching genes with symbol %s\" % hgnc_symbol)\n        if search:\n            # first search for a full match\n            full_query = self.hgnc_collection.find({\n                '$or': [\n                    {'aliases': hgnc_symbol},\n                    {'hgnc_id': int(hgnc_symbol) if hgnc_symbol.isdigit() else None},\n                ],\n                'build': build\n            })\n            if full_query.count() != 0:\n                return full_query\n\n            return self.hgnc_collection.find({\n                'aliases': {'$regex': hgnc_symbol, '$options': 'i'},\n                'build': build\n            })\n\n        return self.hgnc_collection.find({'build': build, 'aliases': hgnc_symbol})", "response": "Fetch all hgnc genes that match a hgnc symbol"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfetch all hgnc genes in a given build", "response": "def all_genes(self, build='37'):\n        \"\"\"Fetch all hgnc genes\n\n            Returns:\n                result()\n        \"\"\"\n        LOG.info(\"Fetching all genes\")\n        return self.hgnc_collection.find({'build': build}).sort('chromosome', 1)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the number of genes in collection", "response": "def nr_genes(self, build=None):\n        \"\"\"Return the number of hgnc genes in collection\n\n        If build is used, return the number of genes of a certain build\n\n            Returns:\n                result()\n        \"\"\"\n        if build:\n            LOG.info(\"Fetching all genes from build %s\",  build)\n        else:\n            LOG.info(\"Fetching all genes\")\n\n        return self.hgnc_collection.find({'build':build}).count()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef drop_genes(self, build=None):\n        if build:\n            LOG.info(\"Dropping the hgnc_gene collection, build %s\", build)\n            self.hgnc_collection.delete_many({'build': build})\n        else:\n            LOG.info(\"Dropping the hgnc_gene collection\")\n            self.hgnc_collection.drop()", "response": "Drop the genes collection"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndropping the transcripts collection", "response": "def drop_transcripts(self, build=None):\n        \"\"\"Delete the transcripts collection\"\"\"\n        if build:\n            LOG.info(\"Dropping the transcripts collection, build %s\", build)\n            self.transcript_collection.delete_many({'build': build})\n        else:\n            LOG.info(\"Dropping the transcripts collection\")\n            self.transcript_collection.drop()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting the exons collection", "response": "def drop_exons(self, build=None):\n        \"\"\"Delete the exons collection\"\"\"\n        if build:\n            LOG.info(\"Dropping the exons collection, build %s\", build)\n            self.exon_collection.delete_many({'build': build})\n        else:\n            LOG.info(\"Dropping the exons collection\")\n            self.exon_collection.drop()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a dictionary with ensembl ids as keys and transcripts as values.", "response": "def ensembl_transcripts(self, build='37'):\n        \"\"\"Return a dictionary with ensembl ids as keys and transcripts as value.\n\n        Args:\n            build(str)\n        \n        Returns:\n            ensembl_transcripts(dict): {<enst_id>: transcripts_obj, ...}\n        \"\"\"\n        ensembl_transcripts = {}\n        LOG.info(\"Fetching all transcripts\")\n        for transcript_obj in self.transcript_collection.find({'build':build}):\n            enst_id = transcript_obj['transcript_id']\n            ensembl_transcripts[enst_id] = transcript_obj\n        LOG.info(\"Ensembl transcripts fetched\")\n\n        return ensembl_transcripts"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef hgncsymbol_to_gene(self, build='37', genes=None):\n        hgnc_dict = {}\n        LOG.info(\"Building hgncsymbol_to_gene\")\n        if not genes:\n            genes = self.hgnc_collection.find({'build':build})\n\n        for gene_obj in genes:\n            hgnc_dict[gene_obj['hgnc_symbol']] = gene_obj\n        LOG.info(\"All genes fetched\")\n        return hgnc_dict", "response": "Return a dictionary with hgnc_symbol as key and gene_obj as value"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef gene_by_alias(self, symbol, build='37'):\n        res = self.hgnc_collection.find({'hgnc_symbol': symbol, 'build':build})\n        if res.count() == 0:\n            res = self.hgnc_collection.find({'aliases': symbol, 'build':build})\n\n        return res", "response": "Return a iterable with hgnc_genes.\n            whose symbol is the given alias."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef genes_by_alias(self, build='37', genes=None):\n        LOG.info(\"Fetching all genes by alias\")\n        # Collect one entry for each alias symbol that exists\n        alias_genes = {}\n        # Loop over all genes\n        if not genes:\n            genes = self.hgnc_collection.find({'build':build})\n\n        for gene in genes:\n            # Collect the hgnc_id\n            hgnc_id = gene['hgnc_id']\n            # Collect the true symbol given by hgnc\n            hgnc_symbol = gene['hgnc_symbol']\n            # Loop aver all aliases\n            for alias in gene['aliases']:\n                true_id = None\n                # If the alias is the same as hgnc symbol we know the true id\n                if alias == hgnc_symbol:\n                    true_id = hgnc_id\n                # If the alias is already in the list we add the id\n                if alias in alias_genes:\n                    alias_genes[alias]['ids'].add(hgnc_id)\n                    if true_id:\n                        alias_genes[alias]['true'] = hgnc_id\n                else:\n                    alias_genes[alias] = {\n                        'true': hgnc_id,\n                        'ids': set([hgnc_id])\n                    }\n\n        return alias_genes", "response": "Return a dictionary with hgnc symbols as keys and a list of hgnc ids as values and a list of hgnc ids as values."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a set with identifier transcripts for the given HGNC ID and build.", "response": "def get_id_transcripts(self, hgnc_id, build='37'):\n        \"\"\"Return a set with identifier transcript(s)\n\n        Choose all refseq transcripts with NM symbols, if none where found choose ONE with NR,\n        if no NR choose ONE with XM. If there are no RefSeq transcripts identifiers choose the \n        longest ensembl transcript.\n\n        Args:\n            hgnc_id(int)\n            build(str)\n\n        Returns:\n            identifier_transcripts(set)\n\n        \"\"\"\n        transcripts = self.transcripts(build=build, hgnc_id=hgnc_id)\n\n        identifier_transcripts = set()\n        longest = None\n        nr = []\n        xm = []\n        for tx in transcripts:\n            enst_id = tx['transcript_id']\n            # Should we not check if it is longest?\n            if not longest:\n                longest = enst_id\n            refseq_id = tx.get('refseq_id')\n            if not refseq_id:\n                continue\n            \n            if 'NM' in refseq_id:\n                identifier_transcripts.add(enst_id)\n            elif 'NR' in refseq_id:\n                nr.append(enst_id)\n            elif 'XM' in refseq_id:\n                xm.append(enst_id)\n        \n        if identifier_transcripts:\n            return identifier_transcripts\n        \n        if nr:\n            return set([nr[0]])\n\n        if xm:\n            return set([xm[0]])\n        \n        return set([longest])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dictionary with hgnc_id as keys and a list of transcripts as value", "response": "def transcripts_by_gene(self, build='37'):\n        \"\"\"Return a dictionary with hgnc_id as keys and a list of transcripts as value\n        \n        Args:\n            build(str)\n        \n        Returns:\n            hgnc_transcripts(dict)\n                \n        \"\"\"\n        hgnc_transcripts = {}\n        LOG.info(\"Fetching all transcripts\")\n        for transcript in self.transcript_collection.find({'build':build}):\n            hgnc_id = transcript['hgnc_id']\n            if not hgnc_id in hgnc_transcripts:\n                hgnc_transcripts[hgnc_id] = []\n            \n            hgnc_transcripts[hgnc_id].append(transcript)\n        \n        return hgnc_transcripts"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef id_transcripts_by_gene(self, build='37'):\n        hgnc_id_transcripts = {}\n        LOG.info(\"Fetching all id transcripts\")\n        for gene_obj in self.hgnc_collection.find({'build': build}):\n            hgnc_id = gene_obj['hgnc_id']\n            id_transcripts = self.get_id_transcripts(hgnc_id=hgnc_id, build=build)\n            hgnc_id_transcripts[hgnc_id] = id_transcripts\n\n        return hgnc_id_transcripts", "response": "Return a dictionary with hgnc_id as keys and a set of id transcripts as values."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ensembl_genes(self, build='37'):\n        genes = {}\n        \n        LOG.info(\"Fetching all genes\")\n        for gene_obj in self.hgnc_collection.find({'build':build}):\n            ensg_id = gene_obj['ensembl_id']\n            hgnc_id = gene_obj['hgnc_id']\n            \n            genes[ensg_id] = gene_obj\n        \n        LOG.info(\"Ensembl genes fetched\")\n\n        return genes", "response": "Return a dictionary with ensembl ids as keys and gene objects as values."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef transcripts(self, build='37', hgnc_id=None):\n        \n        query = {'build': build}\n        if hgnc_id:\n            query['hgnc_id'] = hgnc_id\n        \n        return self.transcript_collection.find(query)", "response": "Return all transcripts for a given build"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_hgnc(self, hgnc_alias, build='37'):\n        result = self.hgnc_genes(hgnc_symbol=hgnc_alias, build=build)\n        if result:\n            for gene in result:\n                return gene['hgnc_symbol']\n        else:\n            return None", "response": "Check if a hgnc symbol is an alias\n            Return the correct hgnc symbol"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd the correct hgnc id to a set of genes with hgnc symbols only", "response": "def add_hgnc_id(self, genes):\n        \"\"\"Add the correct hgnc id to a set of genes with hgnc symbols\n\n        Args:\n            genes(list(dict)): A set of genes with hgnc symbols only\n\n        \"\"\"\n        genes_by_alias = self.genes_by_alias()\n\n        for gene in genes:\n            id_info = genes_by_alias.get(gene['hgnc_symbol'])\n            if not id_info:\n                LOG.warning(\"Gene %s does not exist in scout\", gene['hgnc_symbol'])\n                continue\n            gene['hgnc_id'] = id_info['true']\n            if not id_info['true']:\n                if len(id_info['ids']) > 1:\n                    LOG.warning(\"Gene %s has ambiguous value, please choose one hgnc id in result\", gene['hgnc_symbol'])\n                gene['hgnc_id'] = ','.join([str(hgnc_id) for hgnc_id in id_info['ids']])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dictionary with chromosomes as keys and intervals as values. Each interval represents a coding region of overlapping genes.", "response": "def get_coding_intervals(self, build='37', genes=None):\n        \"\"\"Return a dictionary with chromosomes as keys and interval trees as values\n\n        Each interval represents a coding region of overlapping genes.\n\n        Args:\n            build(str): The genome build\n            genes(iterable(scout.models.HgncGene)):\n\n        Returns:\n            intervals(dict): A dictionary with chromosomes as keys and overlapping genomic intervals as values\n        \"\"\"\n        intervals = {}\n        if not genes:\n            genes = self.all_genes(build=build)\n        LOG.info(\"Building interval trees...\")\n        for i,hgnc_obj in enumerate(genes):\n            chrom = hgnc_obj['chromosome']\n            start = max((hgnc_obj['start'] - 5000), 1)\n            end = hgnc_obj['end'] + 5000\n\n            # If this is the first time a chromosome is seen we create a new\n            # interval tree with current interval\n            if chrom not in intervals:\n                intervals[chrom] = intervaltree.IntervalTree()\n                intervals[chrom].addi(start, end, i)\n                continue\n\n            res = intervals[chrom].search(start, end)\n\n            # If the interval did not overlap any other intervals we insert it and continue\n            if not res:\n                intervals[chrom].addi(start, end, i)\n                continue\n\n            # Loop over the overlapping intervals\n            for interval in res:\n                # Update the positions to new max and mins\n                if interval.begin < start:\n                    start = interval.begin\n\n                if interval.end > end:\n                    end = interval.end\n\n                # Delete the old interval\n                intervals[chrom].remove(interval)\n\n            # Add the new interval consisting och the overlapping ones\n            intervals[chrom].addi(start, end, i)\n\n        return intervals"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload the exons into the database.", "response": "def load_exons(self, exons, genes=None, build='37'):\n        \"\"\"Create exon objects and insert them into the database\n        \n        Args:\n            exons(iterable(dict))\n        \"\"\"\n        genes = genes or self.ensembl_genes(build)\n        for exon in exons:\n            exon_obj = build_exon(exon, genes)\n            if not exon_obj:\n                continue\n            \n            res = self.exon_collection.insert_one(exon_obj)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef exons(self, hgnc_id=None, transcript_id=None,  build=None):\n        query = {}\n        if build:\n            query['build'] = build\n        if hgnc_id:\n            query['hgnc_id'] = hgnc_id\n        if transcript_id:\n            query['transcript_id'] = transcript_id\n        \n        return self.exon_collection.find(query)", "response": "Return all exons in the database"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the omim gene panel in the database.", "response": "def omim(context, api_key, institute):\n    \"\"\"\n    Update the automate generated omim gene panel in the database.\n    \"\"\"\n    LOG.info(\"Running scout update omim\")\n    adapter = context.obj['adapter']\n    \n    api_key = api_key or context.obj.get('omim_api_key')\n    if not api_key:\n        LOG.warning(\"Please provide a omim api key to load the omim gene panel\")\n        context.abort()\n    \n    institute_obj = adapter.institute(institute)\n    if not institute_obj:\n        LOG.info(\"Institute %s could not be found in database\", institute)\n        LOG.warning(\"Please specify an existing institute\")\n        context.abort()\n\n    try:\n        adapter.load_omim_panel(api_key, institute=institute)\n    except Exception as err:\n        LOG.error(err)\n        context.abort()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef index():\n    institute_objs = user_institutes(store, current_user)\n    institutes_count = ((institute_obj, store.cases(collaborator=institute_obj['_id']).count())\n                        for institute_obj in institute_objs if institute_obj)\n    return dict(institutes=institutes_count)", "response": "Display a list of all user institutes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndisplaying a list of cases for an institute.", "response": "def cases(institute_id):\n    \"\"\"Display a list of cases for an institute.\"\"\"\n    institute_obj = institute_and_case(store, institute_id)\n    query = request.args.get('query')\n\n    limit = 100\n    if request.args.get('limit'):\n        limit = int(request.args.get('limit'))\n\n    skip_assigned = request.args.get('skip_assigned')\n    is_research = request.args.get('is_research')\n    all_cases = store.cases(collaborator=institute_id, name_query=query,\n                        skip_assigned=skip_assigned, is_research=is_research)\n    data = controllers.cases(store, all_cases, limit)\n\n    sanger_unevaluated = controllers.get_sanger_unevaluated(store, institute_id, current_user.email)\n    if len(sanger_unevaluated)> 0:\n        data['sanger_unevaluated'] = sanger_unevaluated\n\n    return dict(institute=institute_obj, skip_assigned=skip_assigned,\n                is_research=is_research, query=query, **data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nshow all MatchMaker matches for a given institute and case", "response": "def matchmaker_matches(institute_id, case_name):\n    \"\"\"Show all MatchMaker matches for a given case\"\"\"\n    # check that only authorized users can access MME patients matches\n    user_obj = store.user(current_user.email)\n    if 'mme_submitter' not in user_obj['roles']:\n        flash('unauthorized request', 'warning')\n        return redirect(request.referrer)\n    # Required params for getting matches from MME server:\n    mme_base_url = current_app.config.get('MME_URL')\n    mme_token = current_app.config.get('MME_TOKEN')\n    if not mme_base_url or not mme_token:\n        flash('An error occurred reading matchmaker connection parameters. Please check config file!', 'danger')\n        return redirect(request.referrer)\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    data = controllers.mme_matches(case_obj, institute_obj, mme_base_url, mme_token)\n    if data and data.get('server_errors'):\n        flash('MatchMaker server returned error:{}'.format(data['server_errors']), 'danger')\n        return redirect(request.referrer)\n    elif not data:\n        data = {\n            'institute' : institute_obj,\n            'case' : case_obj\n        }\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstarting an internal match or a match against one or all external nodes", "response": "def matchmaker_match(institute_id, case_name, target):\n    \"\"\"Starts an internal match or a match against one or all MME external nodes\"\"\"\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n\n    # check that only authorized users can run matches\n    user_obj = store.user(current_user.email)\n    if 'mme_submitter' not in user_obj['roles']:\n        flash('unauthorized request', 'warning')\n        return redirect(request.referrer)\n\n    # Required params for sending an add request to MME:\n    mme_base_url = current_app.config.get('MME_URL')\n    mme_accepts = current_app.config.get('MME_ACCEPTS')\n    mme_token = current_app.config.get('MME_TOKEN')\n    nodes = current_app.mme_nodes\n\n    if not mme_base_url or not mme_token or not mme_accepts:\n        flash('An error occurred reading matchmaker connection parameters. Please check config file!', 'danger')\n        return redirect(request.referrer)\n\n    match_results = controllers.mme_match(case_obj, target, mme_base_url, mme_token, nodes, mme_accepts)\n    ok_responses = 0\n    for match_results in match_results:\n        match_results['status_code'] == 200\n        ok_responses +=1\n    if ok_responses:\n        flash(\"Match request sent. Look for eventual matches in 'Matches' page.\", 'info')\n    else:\n        flash('An error occurred while sending match request.', 'danger')\n\n    return redirect(request.referrer)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef matchmaker_add(institute_id, case_name):\n\n    # check that only authorized users can add patients to MME\n    user_obj = store.user(current_user.email)\n    if 'mme_submitter' not in user_obj['roles']:\n        flash('unauthorized request', 'warning')\n        return redirect(request.referrer)\n\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    causatives = False\n    features = False\n    if case_obj.get('suspects') and len(case_obj.get('suspects'))>3:\n        flash('At the moment it is not possible to save to MatchMaker more than 3 pinned variants', 'warning')\n        return redirect(request.referrer)\n    elif case_obj.get('suspects'):\n        causatives = True\n    if case_obj.get('phenotype_terms'):\n        features = True\n\n    mme_save_options = ['sex', 'features', 'disorders']\n    for index, item in enumerate(mme_save_options):\n        if item in request.form:\n            log.info('item {} is in request form'.format(item))\n            mme_save_options[index] = True\n        else:\n            mme_save_options[index] = False\n    genomic_features = request.form.get('genomicfeatures')\n    genes_only = True # upload to matchmaker only gene names\n    if genomic_features == 'variants':\n        genes_only = False # upload to matchmaker both variants and gene names\n\n    # If there are no genomic features nor HPO terms to share for this case, abort\n    if (not case_obj.get('suspects') and not mme_save_options[1]) or (causatives is False and features is False):\n        flash('In order to upload a case to MatchMaker you need to pin a variant or at least assign a phenotype (HPO term)', 'danger')\n        return redirect(request.referrer)\n\n    user_obj = store.user(current_user.email)\n\n    # Required params for sending an add request to MME:\n    mme_base_url = current_app.config.get('MME_URL')\n    mme_accepts = current_app.config.get('MME_ACCEPTS')\n    mme_token = current_app.config.get('MME_TOKEN')\n\n    if not mme_base_url or not mme_accepts or not mme_token:\n        flash('An error occurred reading matchmaker connection parameters. Please check config file!', 'danger')\n        return redirect(request.referrer)\n\n    add_result = controllers.mme_add(store=store, user_obj=user_obj, case_obj=case_obj,\n        add_gender=mme_save_options[0], add_features=mme_save_options[1],\n            add_disorders=mme_save_options[2], genes_only=genes_only,\n            mme_base_url = mme_base_url, mme_accepts=mme_accepts, mme_token=mme_token)\n\n    # flash MME responses (one for each patient posted)\n    n_succes_response = 0\n    n_inserted = 0\n    n_updated = 0\n    category = 'warning'\n\n    for resp in add_result['server_responses']:\n        message = resp.get('message')\n        if resp.get('status_code') == 200:\n            n_succes_response += 1\n        else:\n            flash('an error occurred while adding patient to matchmaker: {}'.format(message), 'warning')\n        if message == 'Patient was successfully updated.':\n            n_updated +=1\n        elif message == 'Patient was successfully inserted into database.':\n            n_inserted +=1\n\n    # if at least one patient was inserted or updated into matchmaker, save submission at the case level:\n    if n_inserted or n_updated:\n        category = 'success'\n        store.case_mme_update(case_obj=case_obj, user_obj=user_obj, mme_subm_obj=add_result)\n    flash('Number of new patients in matchmaker:{0}, number of updated records:{1}, number of failed requests:{2}'.format(\n            n_inserted, n_updated, len(add_result.get('server_responses')) - n_succes_response), category)\n\n    return redirect(request.referrer)", "response": "Add or update a case in MatchMaker"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef matchmaker_delete(institute_id, case_name):\n\n    # check that only authorized users can delete patients from MME\n    user_obj = store.user(current_user.email)\n    if 'mme_submitter' not in user_obj['roles']:\n        flash('unauthorized request', 'warning')\n        return redirect(request.referrer)\n\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    # Required params for sending a delete request to MME:\n    mme_base_url = current_app.config.get('MME_URL')\n    mme_token = current_app.config.get('MME_TOKEN')\n    if not mme_base_url or not mme_token:\n        flash('An error occurred reading matchmaker connection parameters. Please check config file!', 'danger')\n        return redirect(request.referrer)\n\n    delete_result = controllers.mme_delete(case_obj, mme_base_url, mme_token)\n\n    n_deleted = 0\n    category = 'warning'\n\n    for resp in delete_result:\n        if resp['status_code'] == 200:\n            n_deleted += 1\n        else:\n            flash(resp['message'], category)\n    if n_deleted:\n        category = 'success'\n        # update case by removing mme submission\n        # and create events for patients deletion from MME\n        user_obj = store.user(current_user.email)\n        store.case_mme_delete(case_obj=case_obj, user_obj=user_obj)\n    flash('Number of patients deleted from Matchmaker: {} out of {}'.format(n_deleted, len(delete_result)), category)\n    return redirect(request.referrer)", "response": "Remove a case from Matchmaker"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef gene_variants(institute_id):\n    page = int(request.form.get('page', 1))\n\n    institute_obj = institute_and_case(store, institute_id)\n\n    # populate form, conditional on request method\n    if(request.method == \"POST\"):\n            form = GeneVariantFiltersForm(request.form)\n    else:\n        form = GeneVariantFiltersForm(request.args)\n\n    variant_type = form.data.get('variant_type', 'clinical')\n\n    # check if supplied gene symbols exist\n    hgnc_symbols = []\n    non_clinical_symbols = []\n    not_found_symbols = []\n    not_found_ids = []\n    data = {}\n    if (form.hgnc_symbols.data) and len(form.hgnc_symbols.data) > 0:\n        is_clinical = form.data.get('variant_type', 'clinical') == 'clinical'\n        clinical_symbols = store.clinical_symbols(case_obj) if is_clinical else None\n        for hgnc_symbol in form.hgnc_symbols.data:\n            if hgnc_symbol.isdigit():\n                hgnc_gene = store.hgnc_gene(int(hgnc_symbol))\n                if hgnc_gene is None:\n                    not_found_ids.append(hgnc_symbol)\n                else:\n                    hgnc_symbols.append(hgnc_gene['hgnc_symbol'])\n            elif store.hgnc_genes(hgnc_symbol).count() == 0:\n                not_found_symbols.append(hgnc_symbol)\n            elif is_clinical and (hgnc_symbol not in clinical_symbols):\n                non_clinical_symbols.append(hgnc_symbol)\n            else:\n                hgnc_symbols.append(hgnc_symbol)\n\n        if (not_found_ids):\n            flash(\"HGNC id not found: {}\".format(\", \".join(not_found_ids)), 'warning')\n        if (not_found_symbols):\n            flash(\"HGNC symbol not found: {}\".format(\", \".join(not_found_symbols)), 'warning')\n        if (non_clinical_symbols):\n            flash(\"Gene not included in clinical list: {}\".format(\", \".join(non_clinical_symbols)), 'warning')\n        form.hgnc_symbols.data = hgnc_symbols\n\n        log.debug(\"query {}\".format(form.data))\n\n        variants_query = store.gene_variants(query=form.data, category='snv',\n                            variant_type=variant_type)\n\n        data = controllers.gene_variants(store, variants_query, page)\n\n    return dict(institute=institute_obj, form=form, page=page, **data)", "response": "Display a list of SNV variants."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef case_synopsis(institute_id, case_name):\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    user_obj = store.user(current_user.email)\n    new_synopsis = request.form.get('synopsis')\n    controllers.update_synopsis(store, institute_obj, case_obj, user_obj, new_synopsis)\n    return redirect(request.referrer)", "response": "Update ( PUT ) synopsis of a specific case."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pdf_case_report(institute_id, case_name):\n\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    data = controllers.case_report_content(store, institute_obj, case_obj)\n\n    # add coverage report on the bottom of this report\n    if current_app.config.get('SQLALCHEMY_DATABASE_URI'):\n        data['coverage_report'] = controllers.coverage_report_contents(store, institute_obj, case_obj, request.url_root)\n\n    # workaround to be able to print the case pedigree to pdf\n    if case_obj.get('madeline_info') is not None:\n        with open(os.path.join(cases_bp.static_folder, 'madeline.svg'), 'w') as temp_madeline:\n            temp_madeline.write(case_obj['madeline_info'])\n\n    html_report = render_template('cases/case_report.html', institute=institute_obj, case=case_obj, format='pdf', **data)\n    return render_pdf(HTML(string=html_report), download_filename=case_obj['display_name']+'_'+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'_scout.pdf')", "response": "Download a pdf report for a case"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef case_diagnosis(institute_id, case_name):\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    user_obj = store.user(current_user.email)\n    link = url_for('.case', institute_id=institute_id, case_name=case_name)\n    level = 'phenotype' if 'phenotype' in request.form else 'gene'\n    omim_id = request.form['omim_id']\n    remove = True if request.args.get('remove') == 'yes' else False\n    store.diagnose(institute_obj, case_obj, user_obj, link, level=level,\n                   omim_id=omim_id, remove=remove)\n    return redirect(request.referrer)", "response": "Add or remove a diagnosis for a case."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef phenotypes_actions(institute_id, case_name):\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    case_url = url_for('.case', institute_id=institute_id, case_name=case_name)\n    action = request.form['action']\n    hpo_ids = request.form.getlist('hpo_id')\n    user_obj = store.user(current_user.email)\n\n    if action == 'DELETE':\n        for hpo_id in hpo_ids:\n            # DELETE a phenotype from the list\n            store.remove_phenotype(institute_obj, case_obj, user_obj, case_url, hpo_id)\n    elif action == 'PHENOMIZER':\n        if len(hpo_ids) == 0:\n            hpo_ids = [term['phenotype_id'] for term in case_obj.get('phenotype_terms', [])]\n\n        username = current_app.config['PHENOMIZER_USERNAME']\n        password = current_app.config['PHENOMIZER_PASSWORD']\n        diseases = controllers.hpo_diseases(username, password, hpo_ids)\n        return render_template('cases/diseases.html', diseases=diseases,\n                               institute=institute_obj, case=case_obj)\n\n    elif action == 'GENES':\n        hgnc_symbols = set()\n        for raw_symbols in request.form.getlist('genes'):\n            # avoid empty lists\n            if raw_symbols:\n                hgnc_symbols.update(raw_symbol.split(' ', 1)[0] for raw_symbol in\n                                    raw_symbols.split('|'))\n        store.update_dynamic_gene_list(case_obj, hgnc_symbols=hgnc_symbols)\n\n    elif action == 'GENERATE':\n        if len(hpo_ids) == 0:\n            hpo_ids = [term['phenotype_id'] for term in case_obj.get('phenotype_terms', [])]\n        results = store.generate_hpo_gene_list(*hpo_ids)\n        # determine how many HPO terms each gene must match\n        hpo_count = int(request.form.get('min_match') or 1)\n        hgnc_ids = [result[0] for result in results if result[1] >= hpo_count]\n        store.update_dynamic_gene_list(case_obj, hgnc_ids=hgnc_ids, phenotype_ids=hpo_ids)\n\n    return redirect(case_url)", "response": "Perform actions on multiple phenotypes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef status(institute_id, case_name):\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    user_obj = store.user(current_user.email)\n\n    status = request.form.get('status', case_obj['status'])\n    link = url_for('.case', institute_id=institute_id, case_name=case_name)\n\n    if status == 'archive':\n        store.archive_case(institute_obj, case_obj, user_obj, status, link)\n    else:\n        store.update_status(institute_obj, case_obj, user_obj, status, link)\n\n    return redirect(request.referrer)", "response": "Update status of a specific case."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nassign a user from a case.", "response": "def assign(institute_id, case_name, user_id=None):\n    \"\"\"Assign and unassign a user from a case.\"\"\"\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    link = url_for('.case', institute_id=institute_id, case_name=case_name)\n    if user_id:\n        user_obj = store.user(user_id)\n    else:\n        user_obj = store.user(current_user.email)\n    if request.form.get('action') == 'DELETE':\n        store.unassign(institute_obj, case_obj, user_obj, link)\n    else:\n        store.assign(institute_obj, case_obj, user_obj, link)\n    return redirect(request.referrer)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsearching for HPO terms.", "response": "def hpoterms():\n    \"\"\"Search for HPO terms.\"\"\"\n    query = request.args.get('query')\n    if query is None:\n        return abort(500)\n    terms = sorted(store.hpo_terms(query=query), key=itemgetter('hpo_number'))\n    json_terms = [\n        {'name': '{} | {}'.format(term['_id'], term['description']),\n         'id': term['_id']\n        } for term in terms[:7]]\n\n    return jsonify(json_terms)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pin_variant(institute_id, case_name, variant_id):\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    variant_obj = store.variant(variant_id)\n    user_obj = store.user(current_user.email)\n    link = url_for('variants.variant', institute_id=institute_id, case_name=case_name,\n                   variant_id=variant_id)\n    if request.form['action'] == 'ADD':\n        store.pin_variant(institute_obj, case_obj, user_obj, link, variant_obj)\n    elif request.form['action'] == 'DELETE':\n        store.unpin_variant(institute_obj, case_obj, user_obj, link, variant_obj)\n    return redirect(request.referrer or link)", "response": "Pin and unpin a single variant."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmarks a variant as sanger validated.", "response": "def mark_validation(institute_id, case_name, variant_id):\n    \"\"\"Mark a variant as sanger validated.\"\"\"\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    variant_obj = store.variant(variant_id)\n    user_obj = store.user(current_user.email)\n    validate_type = request.form['type'] or None\n    link = url_for('variants.variant', institute_id=institute_id, case_name=case_name,\n                   variant_id=variant_id)\n    store.validate(institute_obj, case_obj, user_obj, link, variant_obj, validate_type)\n    return redirect(request.referrer or link)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mark_causative(institute_id, case_name, variant_id):\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    variant_obj = store.variant(variant_id)\n    user_obj = store.user(current_user.email)\n    link = url_for('variants.variant', institute_id=institute_id, case_name=case_name,\n                   variant_id=variant_id)\n    if request.form['action'] == 'ADD':\n        store.mark_causative(institute_obj, case_obj, user_obj, link, variant_obj)\n    elif request.form['action'] == 'DELETE':\n        store.unmark_causative(institute_obj, case_obj, user_obj, link, variant_obj)\n\n    # send the user back to the case that was marked as solved\n    case_url = url_for('.case', institute_id=institute_id, case_name=case_name)\n    return redirect(case_url)", "response": "Mark a variant as confirmed causative."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_case(institute_id, case_name):\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    store.case_collection.find_one_and_update({'_id':case_obj['_id']}, {'$set': {'needs_check': False}})\n    return redirect(request.referrer)", "response": "Mark a case that is being checked."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsharing a case with a different institute.", "response": "def share(institute_id, case_name):\n    \"\"\"Share a case with a different institute.\"\"\"\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    user_obj = store.user(current_user.email)\n    collaborator_id = request.form['collaborator']\n    revoke_access = 'revoke' in request.form\n    link = url_for('.case', institute_id=institute_id, case_name=case_name)\n\n    if revoke_access:\n        store.unshare(institute_obj, case_obj, collaborator_id, user_obj, link)\n    else:\n        store.share(institute_obj, case_obj, collaborator_id, user_obj, link)\n\n    return redirect(request.referrer)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrequests a case to be rerun.", "response": "def rerun(institute_id, case_name):\n    \"\"\"Request a case to be rerun.\"\"\"\n    sender = current_app.config['MAIL_USERNAME']\n    recipient = current_app.config['TICKET_SYSTEM_EMAIL']\n    controllers.rerun(store, mail, current_user, institute_id, case_name, sender,\n                      recipient)\n    return redirect(request.referrer)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef research(institute_id, case_name):\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    user_obj = store.user(current_user.email)\n    link = url_for('.case', institute_id=institute_id, case_name=case_name)\n    store.open_research(institute_obj, case_obj, user_obj, link)\n    return redirect(request.referrer)", "response": "Open the research list for a case."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds or remove cohorts.", "response": "def cohorts(institute_id, case_name):\n    \"\"\"Add/remove institute tags.\"\"\"\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    user_obj = store.user(current_user.email)\n    link = url_for('.case', institute_id=institute_id, case_name=case_name)\n    cohort_tag = request.form['cohort_tag']\n    if request.args.get('remove') == 'yes':\n        store.remove_cohort(institute_obj, case_obj, user_obj, link, cohort_tag)\n    else:\n        store.add_cohort(institute_obj, case_obj, user_obj, link, cohort_tag)\n    return redirect(request.referrer)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate default panels for a case.", "response": "def default_panels(institute_id, case_name):\n    \"\"\"Update default panels for a case.\"\"\"\n    panel_ids = request.form.getlist('panel_ids')\n    controllers.update_default_panels(store, current_user, institute_id, case_name, panel_ids)\n    return redirect(request.referrer)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndownloading vcf2cytosure file for individual.", "response": "def vcf2cytosure(institute_id, case_name, individual_id):\n    \"\"\"Download vcf2cytosure file for individual.\"\"\"\n\n    (display_name, vcf2cytosure) = controllers.vcf2cytosure(store,\n        institute_id, case_name, individual_id)\n\n    outdir = os.path.abspath(os.path.dirname(vcf2cytosure))\n    filename = os.path.basename(vcf2cytosure)\n\n    log.debug(\"Attempt to deliver file {0} from dir {1}\".format(filename, outdir))\n\n    attachment_filename = display_name + \".vcf2cytosure.cgh\"\n\n    return send_from_directory(outdir, filename,\n        attachment_filename=attachment_filename,\n        as_attachment=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads multiqc report for the case.", "response": "def multiqc(institute_id, case_name):\n    \"\"\"Load multiqc report for the case.\"\"\"\n    data = controllers.multiqc(store, institute_id, case_name)\n    if data['case'].get('multiqc') is None:\n        return abort(404)\n    out_dir = os.path.abspath(os.path.dirname(data['case']['multiqc']))\n    filename = os.path.basename(data['case']['multiqc'])\n    return send_from_directory(out_dir, filename)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_panels(context, mongodb, username, password, authdb, host, port, loglevel, config):\n    coloredlogs.install(level=loglevel)\n    \n    LOG.info(\"Running scout version %s\", __version__)\n    LOG.debug(\"Debug logging enabled.\")\n\n    mongo_config = {}\n    cli_config = {}\n    if config:\n        LOG.debug(\"Use config file %s\", config)\n        with open(config, 'r') as in_handle:\n            cli_config = yaml.load(in_handle)\n\n    mongo_config['mongodb'] = (mongodb or cli_config.get('mongodb') or 'scout')\n\n    mongo_config['host'] = (host or cli_config.get('host') or 'localhost')\n    mongo_config['port'] = (port or cli_config.get('port') or 27017)\n    mongo_config['username'] = username or cli_config.get('username')\n    mongo_config['password'] = password or cli_config.get('password')\n    mongo_config['authdb'] = authdb or cli_config.get('authdb') or mongo_config['mongodb']\n    mongo_config['omim_api_key'] = cli_config.get('omim_api_key')\n\n    LOG.info(\"Setting database name to %s\", mongo_config['mongodb'])\n    LOG.debug(\"Setting host to %s\", mongo_config['host'])\n    LOG.debug(\"Setting port to %s\", mongo_config['port'])\n\n    valid_connection = check_connection(\n        host=mongo_config['host'],\n        port=mongo_config['port'],\n        username=mongo_config['username'],\n        password=mongo_config['password'],\n        authdb=mongo_config['authdb'],\n    )\n\n    LOG.info(\"Test if mongod is running\")\n    if not valid_connection:\n        LOG.warning(\"Connection could not be established\")\n        context.abort()\n\n    try:\n        client = get_connection(**mongo_config)\n    except ConnectionFailure:\n        context.abort()\n\n    database = client[mongo_config['mongodb']]\n\n    LOG.info(\"Setting up a mongo adapter\")\n    mongo_config['client'] = client\n    adapter = MongoAdapter(database)\n    \n    requests = []\n    \n    for case_obj in adapter.case_collection.find():\n        # pp(case_obj)\n        \n        gene_to_panels = adapter.gene_to_panels(case_obj)\n        \n        variants = adapter.variant_collection.find({\n            'case_id': case_obj['_id'],\n            'category': 'snv',\n            'variant_type': 'clinical',\n        })\n        \n        for variant_obj in variants:\n        \n            panel_names = set()\n            for hgnc_id in variant_obj['hgnc_ids']:\n                gene_panels = gene_to_panels.get(hgnc_id, set())\n                panel_names = panel_names.union(gene_panels)\n            \n            if panel_names:\n                operation = pymongo.UpdateOne(\n                    {'_id': variant_obj['_id']},\n                    {\n                        '$set': {\n                            'panels': list(panel_names)\n                        }\n                    })\n                requests.append(operation)\n            \n            if len(requests) > 5000:\n                adapter.variant_collection.bulk_write(requests, ordered=False)\n                requests = []\n        \n        if requests:\n            adapter.variant_collection.bulk_write(requests, ordered=False)\n            requests = []", "response": "Update the panels with a scout instance."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cases(store, case_query, limit=100):\n\n    case_groups = {status: [] for status in CASE_STATUSES}\n    for case_obj in case_query.limit(limit):\n        analysis_types = set(ind['analysis_type'] for ind in case_obj['individuals'])\n\n        case_obj['analysis_types'] = list(analysis_types)\n        case_obj['assignees'] = [store.user(user_email) for user_email in\n                                 case_obj.get('assignees', [])]\n        case_groups[case_obj['status']].append(case_obj)\n        case_obj['is_rerun'] = len(case_obj.get('analyses', [])) > 0\n        case_obj['clinvar_variants'] = store.case_to_clinVars(case_obj['_id'])\n        case_obj['display_track'] = TRACKS[case_obj.get('track', 'rare')]\n\n    data = {\n        'cases': [(status, case_groups[status]) for status in CASE_STATUSES],\n        'found_cases': case_query.count(),\n        'limit': limit,\n    }\n    return data", "response": "Return a dict of all cases in the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a single case view.", "response": "def case(store, institute_obj, case_obj):\n    \"\"\"Preprocess a single case.\n\n    Prepare the case to be displayed in the case view.\n\n    Args:\n        store(adapter.MongoAdapter)\n        institute_obj(models.Institute)\n        case_obj(models.Case)\n\n    Returns:\n        data(dict): includes the cases, how many there are and the limit.\n\n    \"\"\"\n    # Convert individual information to more readable format\n    case_obj['individual_ids'] = []\n    for individual in case_obj['individuals']:\n        try:\n            sex = int(individual.get('sex', 0))\n        except ValueError as err:\n            sex = 0\n        individual['sex_human'] = SEX_MAP[sex]\n\n        pheno_map = PHENOTYPE_MAP\n        if case_obj.get('track', 'rare') == 'cancer':\n            pheno_map = CANCER_PHENOTYPE_MAP\n\n        individual['phenotype_human'] = pheno_map.get(individual['phenotype'])\n        case_obj['individual_ids'].append(individual['individual_id'])\n\n    case_obj['assignees'] = [store.user(user_email) for user_email in\n                             case_obj.get('assignees', [])]\n\n    # Fetch the variant objects for suspects and causatives\n    suspects = [store.variant(variant_id) or variant_id for variant_id in\n                case_obj.get('suspects', [])]\n    causatives = [store.variant(variant_id) or variant_id for variant_id in\n                  case_obj.get('causatives', [])]\n\n    # Set of all unique genes in the default gene panels\n    distinct_genes = set()\n    case_obj['panel_names'] = []\n    for panel_info in case_obj.get('panels', []):\n        if not panel_info.get('is_default'):\n            continue\n        panel_obj = store.gene_panel(panel_info['panel_name'], version=panel_info.get('version'))\n        distinct_genes.update([gene['hgnc_id'] for gene in panel_obj.get('genes', [])])\n        full_name = \"{} ({})\".format(panel_obj['display_name'], panel_obj['version'])\n        case_obj['panel_names'].append(full_name)\n    case_obj['default_genes'] = list(distinct_genes)\n    for hpo_term in itertools.chain(case_obj.get('phenotype_groups', []),\n                                    case_obj.get('phenotype_terms', [])):\n        hpo_term['hpo_link'] = (\"http://compbio.charite.de/hpoweb/showterm?id={}\"\n                                .format(hpo_term['phenotype_id']))\n\n    # other collaborators than the owner of the case\n    o_collaborators = []\n    for collab_id in case_obj['collaborators']:\n        if collab_id != case_obj['owner'] and store.institute(collab_id):\n            o_collaborators.append(store.institute(collab_id))\n\n    case_obj['o_collaborators'] = [(collab_obj['_id'], collab_obj['display_name']) for\n                                   collab_obj in o_collaborators]\n\n    irrelevant_ids = ('cust000', institute_obj['_id'])\n    collab_ids = [(collab['_id'], collab['display_name']) for collab in store.institutes() if\n                  (collab['_id'] not in irrelevant_ids) and\n                  (collab['_id'] not in case_obj['collaborators'])]\n\n    events = list(store.events(institute_obj, case=case_obj))\n    for event in events:\n        event['verb'] = VERBS_MAP[event['verb']]\n\n    case_obj['clinvar_variants'] = store.case_to_clinVars(case_obj['_id'])\n\n    # Phenotype groups can be specific for an institute, there are some default groups\n    pheno_groups = institute_obj.get('phenotype_groups') or PHENOTYPE_GROUPS\n\n    data = {\n        'status_class': STATUS_MAP.get(case_obj['status']),\n        'other_causatives': store.check_causatives(case_obj=case_obj),\n        'comments': store.events(institute_obj, case=case_obj, comments=True),\n        'hpo_groups': pheno_groups,\n        'events': events,\n        'suspects': suspects,\n        'causatives': causatives,\n        'collaborators': collab_ids,\n        'cohort_tags': COHORT_TAGS,\n        'mme_nodes': current_app.mme_nodes, # Get available MatchMaker nodes for matching case\n    }\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef case_report_content(store, institute_obj, case_obj):\n    variant_types = {\n        'causatives_detailed': 'causatives',\n        'suspects_detailed': 'suspects',\n        'classified_detailed': 'acmg_classification',\n        'tagged_detailed': 'manual_rank',\n        'dismissed_detailed': 'dismiss_variant',\n        'commented_detailed': 'is_commented',\n    }\n    data = case_obj\n\n    for individual in data['individuals']:\n        try:\n            sex = int(individual.get('sex', 0))\n        except ValueError as err:\n            sex = 0\n        individual['sex_human'] = SEX_MAP[sex]\n        individual['phenotype_human'] = PHENOTYPE_MAP.get(individual['phenotype'])\n\n    # Add the case comments\n    data['comments'] = store.events(institute_obj, case=case_obj, comments=True)\n\n    data['manual_rank_options'] = MANUAL_RANK_OPTIONS\n    data['dismissed_options'] = DISMISS_VARIANT_OPTIONS\n    data['genetic_models'] = dict(GENETIC_MODELS)\n    data['report_created_at'] = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n\n    evaluated_variants = {}\n    for vt in variant_types:\n        evaluated_variants[vt] = []\n    # We collect all causatives and suspected variants\n    # These are handeled in separate since they are on case level\n    for var_type in ['causatives', 'suspects']:\n        #These include references to variants\n        vt = '_'.join([var_type, 'detailed'])\n        for var_id in case_obj.get(var_type,[]):\n            variant_obj = store.variant(var_id)\n            if not variant_obj:\n                continue\n            # If the variant exists we add it to the evaluated variants\n            evaluated_variants[vt].append(variant_obj)\n\n    ## get variants for this case that are either classified, commented, tagged or dismissed.\n    for var_obj in store.evaluated_variants(case_id=case_obj['_id']):\n        # Check which category it belongs to\n        for vt in variant_types:\n            keyword = variant_types[vt]\n            # When found we add it to the categpry\n            # Eac variant can belong to multiple categories\n            if keyword in var_obj:\n                evaluated_variants[vt].append(var_obj)\n\n    for var_type in evaluated_variants:\n        decorated_variants = []\n        for var_obj in evaluated_variants[var_type]:\n        # We decorate the variant with some extra information\n            if var_obj['category'] == 'snv':\n                decorated_info = variant_decorator(\n                        store=store,\n                        institute_obj=institute_obj,\n                        case_obj=case_obj,\n                        variant_id=None,\n                        variant_obj=var_obj,\n                        add_case=False,\n                        add_other=False,\n                        get_overlapping=False\n                    )\n            else:\n                decorated_info = sv_variant(\n                    store=store,\n                    institute_id=institute_obj['_id'],\n                    case_name=case_obj['display_name'],\n                    variant_obj=var_obj,\n                    add_case=False,\n                    get_overlapping=False\n                    )\n            decorated_variants.append(decorated_info['variant'])\n        # Add the decorated variants to the case\n        data[var_type] = decorated_variants\n\n    return data", "response": "Gather contents to be visualized in a case report"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef coverage_report_contents(store, institute_obj, case_obj, base_url):\n\n    request_data = {}\n    # extract sample ids from case_obj and add them to the post request object:\n    request_data['sample_id'] = [ ind['individual_id'] for ind in case_obj['individuals'] ]\n\n    # extract default panel names and default genes from case_obj and add them to the post request object\n    distinct_genes = set()\n    panel_names = []\n    for panel_info in case_obj.get('panels', []):\n        if not panel_info.get('is_default'):\n            continue\n        panel_obj = store.gene_panel(panel_info['panel_name'], version=panel_info.get('version'))\n        full_name = \"{} ({})\".format(panel_obj['display_name'], panel_obj['version'])\n        panel_names.append(full_name)\n    panel_names = ' ,'.join(panel_names)\n    request_data['panel_name'] = panel_names\n\n    # add institute-specific cutoff level to the post request object\n    request_data['level'] = institute_obj.get('coverage_cutoff', 15)\n\n    #send get request to chanjo report\n    resp = requests.get(base_url+'reports/report', params=request_data)\n\n    #read response content\n    soup = BeautifulSoup(resp.text)\n\n    # remove links in the printed version of coverage report\n    for tag in soup.find_all('a'):\n        tag.replaceWith('')\n\n    #extract body content using BeautifulSoup\n    coverage_data = ''.join(['%s' % x for x in soup.body.contents])\n\n    return coverage_data", "response": "Get the contents of a coverage report"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef clinvar_submissions(store, user_id, institute_id):\n    submissions = list(store.clinvar_submissions(user_id, institute_id))\n    return submissions", "response": "Get all Clinvar submissions for a user and institute"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncollect MT variants and format line of a MT variant report to be exported in excel format otope", "response": "def mt_excel_files(store, case_obj, temp_excel_dir):\n    \"\"\"Collect MT variants and format line of a MT variant report\n    to be exported in excel format\n\n    Args:\n        store(adapter.MongoAdapter)\n        case_obj(models.Case)\n        temp_excel_dir(os.Path): folder where the temp excel files are written to\n\n    Returns:\n        written_files(int): the number of files written to temp_excel_dir\n\n    \"\"\"\n    today = datetime.datetime.now().strftime('%Y-%m-%d')\n    samples = case_obj.get('individuals')\n\n    query = {'chrom':'MT'}\n    mt_variants = list(store.variants(case_id=case_obj['_id'], query=query, nr_of_variants= -1, sort_key='position'))\n\n    written_files = 0\n    for sample in samples:\n        sample_id = sample['individual_id']\n        sample_lines = export_mt_variants(variants=mt_variants, sample_id=sample_id)\n\n        # set up document name\n        document_name = '.'.join([case_obj['display_name'], sample_id, today]) + '.xlsx'\n        workbook = Workbook(os.path.join(temp_excel_dir,document_name))\n        Report_Sheet = workbook.add_worksheet()\n\n        # Write the column header\n        row = 0\n        for col,field in enumerate(MT_EXPORT_HEADER):\n            Report_Sheet.write(row,col,field)\n\n        # Write variant lines, after header (start at line 1)\n        for row, line in enumerate(sample_lines,1): # each line becomes a row in the document\n            for col, field in enumerate(line): # each field in line becomes a cell\n                Report_Sheet.write(row,col,field)\n        workbook.close()\n\n        if os.path.exists(os.path.join(temp_excel_dir,document_name)):\n            written_files += 1\n\n    return written_files"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef hpo_diseases(username, password, hpo_ids, p_value_treshold=1):\n    # skip querying Phenomizer unless at least one HPO terms exists\n    try:\n        results = query_phenomizer.query(username, password, *hpo_ids)\n        diseases = [result for result in results\n                    if result['p_value'] <= p_value_treshold]\n        return diseases\n    except SystemExit:\n        return None", "response": "Return the list of HGNC symbols that match annotated HPO terms."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rerun(store, mail, current_user, institute_id, case_name, sender, recipient):\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    user_obj = store.user(current_user.email)\n    link = url_for('cases.case', institute_id=institute_id, case_name=case_name)\n    store.request_rerun(institute_obj, case_obj, user_obj, link)\n\n    # this should send a JSON document to the SuSy API in the future\n    html = \"\"\"\n        <p>{institute}: {case} ({case_id})</p>\n        <p>Re-run requested by: {name}</p>\n    \"\"\".format(institute=institute_obj['display_name'],\n               case=case_obj['display_name'], case_id=case_obj['_id'],\n               name=user_obj['name'].encode())\n\n    # compose and send the email message\n    msg = Message(subject=(\"SCOUT: request RERUN for {}\"\n                           .format(case_obj['display_name'])),\n                  html=html, sender=sender, recipients=[recipient],\n                  # cc the sender of the email for confirmation\n                  cc=[user_obj['email']])\n    mail.send(msg)", "response": "Request a rerun by email."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_default_panels(store, current_user, institute_id, case_name, panel_ids):\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    user_obj = store.user(current_user.email)\n    link = url_for('cases.case', institute_id=institute_id, case_name=case_name)\n    panel_objs = [store.panel(panel_id) for panel_id in panel_ids]\n    store.update_default_panels(institute_obj, case_obj, user_obj, link, panel_objs)", "response": "Update default panels for a case."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the vcf2cytosure CGH file for the given inidividual.", "response": "def vcf2cytosure(store, institute_id, case_name, individual_id):\n    \"\"\"vcf2cytosure CGH file for inidividual.\"\"\"\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n\n    for individual in case_obj['individuals']:\n        if individual['individual_id'] == individual_id:\n            individual_obj = individual\n\n    return (individual_obj['display_name'], individual_obj['vcf2cytosure'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef gene_variants(store, variants_query, page=1, per_page=50):\n    variant_count = variants_query.count()\n    skip_count = per_page * max(page - 1, 0)\n    more_variants = True if variant_count > (skip_count + per_page) else False\n    variant_res = variants_query.skip(skip_count).limit(per_page)\n\n    my_institutes = list(inst['_id'] for inst in user_institutes(store, current_user))\n\n    variants = []\n    for variant_obj in variant_res:\n        # hide other institutes for now\n        if variant_obj['institute'] not in my_institutes:\n            LOG.warning(\"Institute {} not allowed.\".format(variant_obj['institute']))\n            continue\n\n        # Populate variant case_display_name\n        variant_case_obj = store.case(case_id=variant_obj['case_id'])\n        if not variant_case_obj:\n            # A variant with missing case was encountered\n            continue\n        case_display_name = variant_case_obj.get('display_name')\n        variant_obj['case_display_name'] = case_display_name\n\n        genome_build = variant_case_obj.get('genome_build', '37')\n        if genome_build not in ['37','38']:\n            genome_build = '37'\n\n        # Update the HGNC symbols if they are not set\n        variant_genes = variant_obj.get('genes')\n        if variant_genes is not None:\n            for gene_obj in variant_genes:\n                # If there is no hgnc id there is nothin we can do\n                if not gene_obj['hgnc_id']:\n                    continue\n                # Else we collect the gene object and check the id\n                if gene_obj.get('hgnc_symbol') is None or gene_obj.get('description') is None:\n                    hgnc_gene = store.hgnc_gene(gene_obj['hgnc_id'], build=genome_build)\n                    if not hgnc_gene:\n                        continue\n                    gene_obj['hgnc_symbol'] = hgnc_gene['hgnc_symbol']\n                    gene_obj['description'] = hgnc_gene['description']\n\n        # Populate variant HGVS and predictions\n        gene_ids = []\n        gene_symbols = []\n        hgvs_c = []\n        hgvs_p = []\n        variant_genes = variant_obj.get('genes')\n\n        if variant_genes is not None:\n            functional_annotation = ''\n\n            for gene_obj in variant_genes:\n                hgnc_id = gene_obj['hgnc_id']\n                gene_symbol = gene(store, hgnc_id)['symbol']\n                gene_ids.append(hgnc_id)\n                gene_symbols.append(gene_symbol)\n\n                hgvs_nucleotide = '-'\n                # gather HGVS info from gene transcripts\n                transcripts_list = gene_obj.get('transcripts')\n                for transcript_obj in transcripts_list:\n                    if transcript_obj.get('is_canonical') and transcript_obj.get('is_canonical') is True:\n                        hgvs_nucleotide = str(transcript_obj.get('coding_sequence_name'))\n                        hgvs_protein = str(transcript_obj.get('protein_sequence_name'))\n                hgvs_c.append(hgvs_nucleotide)\n                hgvs_p.append(hgvs_protein)\n\n            if len(gene_symbols) == 1:\n                if(hgvs_p[0] != \"None\"):\n                    hgvs = hgvs_p[0]\n                elif(hgvs_c[0] != \"None\"):\n                    hgvs = hgvs_c[0]\n                else:\n                    hgvs = \"-\"\n                variant_obj['hgvs'] = hgvs\n\n            # populate variant predictions for display\n            variant_obj.update(get_predictions(variant_genes))\n\n        variants.append(variant_obj)\n\n    return {\n        'variants': variants,\n        'more_variants': more_variants,\n    }", "response": "Return a list of variants."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef multiqc(store, institute_id, case_name):\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    return dict(\n        institute=institute_obj,\n        case=case_obj,\n    )", "response": "Find MultiQC report for the case."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget all unevaluated variants for an institute with Sanger validation ordered but still not evaluated", "response": "def get_sanger_unevaluated(store, institute_id, user_id):\n    \"\"\"Get all variants for an institute having Sanger validations ordered but still not evaluated\n\n        Args:\n            store(scout.adapter.MongoAdapter)\n            institute_id(str)\n\n        Returns:\n            unevaluated: a list that looks like this: [ {'case1': [varID_1, varID_2, .., varID_n]}, {'case2' : [varID_1, varID_2, .., varID_n]} ],\n                         where the keys are case_ids and the values are lists of variants with Sanger ordered but not yet validated\n\n    \"\"\"\n\n    # Retrieve a list of ids for variants with Sanger ordered grouped by case from the 'event' collection\n    # This way is much faster than querying over all variants in all cases of an institute\n    sanger_ordered_by_case = store.sanger_ordered(institute_id, user_id)\n    unevaluated = []\n\n    # for each object where key==case and value==[variant_id with Sanger ordered]\n    for item in sanger_ordered_by_case:\n        case_id = item['_id']\n        # Get the case to collect display name\n        case_obj = store.case(case_id=case_id)\n\n        if not case_obj: # the case might have been removed\n            continue\n\n        case_display_name = case_obj.get('display_name')\n\n        # List of variant document ids\n        varid_list = item['vars']\n\n        unevaluated_by_case = {}\n        unevaluated_by_case[case_display_name] = []\n\n        for var_id in varid_list:\n            # For each variant with sanger validation ordered\n            variant_obj = store.variant(document_id=var_id, case_id=case_id)\n\n            # Double check that Sanger was ordered (and not canceled) for the variant\n            if variant_obj is None or variant_obj.get('sanger_ordered') is None or variant_obj.get('sanger_ordered') is False:\n                continue\n\n            validation = variant_obj.get('validation', 'not_evaluated')\n\n            # Check that the variant is not evaluated\n            if validation in ['True positive', 'False positive']:\n                continue\n\n            unevaluated_by_case[case_display_name].append(variant_obj['_id'])\n\n        # If for a case there is at least one Sanger validation to evaluate add the object to the unevaluated objects list\n        if len(unevaluated_by_case[case_display_name]) > 0:\n            unevaluated.append(unevaluated_by_case)\n\n    return unevaluated"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a patient to MatchMaker server Args: store(adapter.MongoAdapter) user_obj(dict) a scout user object (to be added as matchmaker contact) case_obj(dict) a scout case object add_gender(bool) if True case gender will be included in matchmaker add_features(bool) if True HPO features will be included in matchmaker add_disorders(bool) if True OMIM diagnoses will be included in matchmaker genes_only(bool) if True only genes and not variants will be shared mme_base_url(str) base url of the MME server mme_accepts(str) request content accepted by MME server mme_token(str) auth token of the MME server Returns: submitted_info(dict) info submitted to MatchMaker and its responses", "response": "def mme_add(store, user_obj, case_obj, add_gender, add_features, add_disorders, genes_only,\n    mme_base_url, mme_accepts, mme_token):\n    \"\"\"Add a patient to MatchMaker server\n\n    Args:\n        store(adapter.MongoAdapter)\n        user_obj(dict) a scout user object (to be added as matchmaker contact)\n        case_obj(dict) a scout case object\n        add_gender(bool) if True case gender will be included in matchmaker\n        add_features(bool) if True HPO features will be included in matchmaker\n        add_disorders(bool) if True OMIM diagnoses will be included in matchmaker\n        genes_only(bool) if True only genes and not variants will be shared\n        mme_base_url(str) base url of the MME server\n        mme_accepts(str) request content accepted by MME server\n        mme_token(str) auth token of the MME server\n\n    Returns:\n        submitted_info(dict) info submitted to MatchMaker and its responses\n    \"\"\"\n\n    if not mme_base_url or not mme_accepts or not mme_token:\n        return 'Please check that Matchmaker connection parameters are valid'\n    url = ''.join([mme_base_url, '/patient/add'])\n    features = []   # this is the list of HPO terms\n    disorders = []  # this is the list of OMIM diagnoses\n    g_features = []\n\n     # create contact dictionary\n    contact_info = {\n        'name' : user_obj['name'],\n        'href' : ''.join( ['mailto:',user_obj['email']] ),\n        'institution' : 'Scout software user, Science For Life Laboratory, Stockholm, Sweden'\n    }\n    if add_features: # create features dictionaries\n        features = hpo_terms(case_obj)\n\n    if add_disorders: # create OMIM disorders dictionaries\n        disorders = omim_terms(case_obj)\n\n    # send a POST request and collect response for each affected individual in case\n    server_responses = []\n\n    submitted_info = {\n        'contact' : contact_info,\n        'sex' : add_gender,\n        'features' : features,\n        'disorders' : disorders,\n        'genes_only' : genes_only,\n        'patient_id' : []\n    }\n    for individual in case_obj.get('individuals'):\n        if not individual['phenotype'] in  [2, 'affected']: # include only affected individuals\n            continue\n\n        patient = {\n            'contact' : contact_info,\n            'id' : '.'.join([case_obj['_id'], individual.get('individual_id')]), # This is a required field form MME\n            'label' : '.'.join([case_obj['display_name'], individual.get('display_name')]),\n            'features' : features,\n            'disorders' : disorders\n        }\n        if add_gender:\n            if individual['sex'] == '1':\n                patient['sex'] = 'MALE'\n            else:\n                patient['sex'] = 'FEMALE'\n\n        if case_obj.get('suspects'):\n            g_features = genomic_features(store, case_obj, individual.get('display_name'), genes_only)\n            patient['genomicFeatures'] = g_features\n\n        # send add request to server and capture response\n        resp = matchmaker_request(url=url, token=mme_token, method='POST', content_type=mme_accepts,\n            accept='application/json', data={'patient':patient})\n\n        server_responses.append({\n                'patient': patient,\n                'message': resp.get('message'),\n                'status_code' : resp.get('status_code')\n            })\n    submitted_info['server_responses'] = server_responses\n    return submitted_info"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting all affected samples for a case from Matchmaker", "response": "def mme_delete(case_obj, mme_base_url, mme_token):\n    \"\"\"Delete all affected samples for a case from MatchMaker\n\n    Args:\n        case_obj(dict) a scout case object\n        mme_base_url(str) base url of the MME server\n        mme_token(str) auth token of the MME server\n\n    Returns:\n         server_responses(list): a list of object of this type:\n                    {\n                        'patient_id': patient_id\n                        'message': server_message,\n                        'status_code': server_status_code\n                    }\n    \"\"\"\n    server_responses = []\n\n    if not mme_base_url or not mme_token:\n        return 'Please check that Matchmaker connection parameters are valid'\n\n    # for each patient of the case in matchmaker\n    for patient in case_obj['mme_submission']['patients']:\n\n        # send delete request to server and capture server's response\n        patient_id = patient['id']\n        url = ''.join([mme_base_url, '/patient/delete/', patient_id])\n        resp = matchmaker_request(url=url, token=mme_token, method='DELETE', )\n\n        server_responses.append({\n            'patient_id': patient_id,\n            'message': resp.get('message'),\n            'status_code': resp.get('status_code')\n        })\n\n    return server_responses"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mme_matches(case_obj, institute_obj, mme_base_url, mme_token):\n    data = {\n        'institute' : institute_obj,\n        'case' : case_obj,\n        'server_errors' : []\n    }\n    matches = {}\n    # loop over the submitted samples and get matches from the MatchMaker server\n    if not case_obj.get('mme_submission'):\n        return None\n\n    for patient in case_obj['mme_submission']['patients']:\n        patient_id = patient['id']\n        matches[patient_id] = None\n        url = ''.join([ mme_base_url, '/matches/', patient_id])\n        server_resp = matchmaker_request(url=url, token=mme_token, method='GET')\n        if 'status_code' in server_resp: # the server returned a valid response\n            # and this will be a list of match objects sorted by desc date\n            pat_matches = []\n            if server_resp.get('matches'):\n                pat_matches = parse_matches(patient_id, server_resp['matches'])\n            matches[patient_id] = pat_matches\n        else:\n            LOG.warning('Server returned error message: {}'.format(server_resp['message']))\n            data['server_errors'].append(server_resp['message'])\n\n    data['matches'] = matches\n\n    return data", "response": "Show Matchmaker submission data for a sample and eventual matches."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mme_match(case_obj, match_type, mme_base_url, mme_token, nodes=None, mme_accepts=None):\n    query_patients = []\n    server_responses = []\n    url = None\n    # list of patient dictionaries is required for internal matching\n    query_patients = case_obj['mme_submission']['patients']\n    if match_type=='internal':\n        url = ''.join([mme_base_url,'/match'])\n        for patient in query_patients:\n            json_resp = matchmaker_request(url=url, token=mme_token, method='POST',\n                content_type=mme_accepts, accept=mme_accepts, data={'patient':patient})\n            resp_obj = {\n                'server' : 'Local MatchMaker node',\n                'patient_id' : patient['id'],\n                'results' : json_resp.get('results'),\n                'status_code' : json_resp.get('status_code'),\n                'message' : json_resp.get('message') # None if request was successful\n            }\n            server_responses.append(resp_obj)\n    else: # external matching\n        # external matching requires only patient ID\n        query_patients = [ patient['id'] for patient in query_patients]\n        node_ids = [ node['id'] for node in nodes ]\n        if match_type in node_ids: # match is against a specific external node\n            node_ids = [match_type]\n\n        # Match every affected patient\n        for patient in query_patients:\n            # Against every node\n            for node in node_ids:\n                url = ''.join([mme_base_url,'/match/external/', patient, '?node=', node])\n                json_resp = matchmaker_request(url=url, token=mme_token, method='POST')\n                resp_obj = {\n                    'server' : node,\n                    'patient_id' : patient,\n                    'results' : json_resp.get('results'),\n                    'status_code' : json_resp.get('status_code'),\n                    'message' : json_resp.get('message') # None if request was successful\n                }\n                server_responses.append(resp_obj)\n    return server_responses", "response": "Initiate a MatchMaker match against either other Scout patients or external nodes"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_variant(variant, institute_id, gene_to_panels = None,\n                  hgncid_to_gene=None, sample_info=None):\n    \"\"\"Build a variant object based on parsed information\n\n        Args:\n            variant(dict)\n            institute_id(str)\n            gene_to_panels(dict): A dictionary with\n                {<hgnc_id>: {\n                    'panel_names': [<panel_name>, ..],\n                    'disease_associated_transcripts': [<transcript_id>, ..]\n                    }\n                    .\n                    .\n                }\n\n            hgncid_to_gene(dict): A dictionary with\n                {<hgnc_id>: <hgnc_gene info>\n                    .\n                    .\n                }\n            sample_info(dict): A dictionary with info about samples.\n                               Strictly for cancer to tell which is tumor\n\n\n        Returns:\n            variant_obj(dict)\n\n        variant = dict(\n            # document_id is a md5 string created by institute_genelist_caseid_variantid:\n            _id = str, # required, same as document_id\n            document_id = str, # required\n            # variant_id is a md5 string created by chrom_pos_ref_alt (simple_id)\n            variant_id = str, # required\n            # display name is variant_id (no md5)\n            display_name = str, # required\n\n            # chrom_pos_ref_alt\n            simple_id = str,\n            # The variant can be either research or clinical.\n            # For research variants we display all the available information while\n            # the clinical variants have limited annotation fields.\n            variant_type = str, # required, choices=('research', 'clinical'))\n\n            category = str, # choices=('sv', 'snv', 'str')\n            sub_category = str, # choices=('snv', 'indel', 'del', 'ins', 'dup', 'inv', 'cnv', 'bnd')\n            mate_id = str, # For SVs this identifies the other end\n\n            case_id = str, # case_id is a string like owner_caseid\n            chromosome = str, # required\n            position = int, # required\n            end = int, # required\n            length = int, # required\n            reference = str, # required\n            alternative = str, # required\n\n            rank_score = float, # required\n            variant_rank = int, # required\n            rank_score_results = list, # List if dictionaries\n            variant_rank = int, # required\n\n            institute = str, # institute_id, required\n\n            sanger_ordered = bool,\n            validation = str, # Sanger validation, choices=('True positive', 'False positive')\n\n            quality = float,\n            filters = list, # list of strings\n            samples = list, # list of dictionaries that are <gt_calls>\n            genetic_models = list, # list of strings choices=GENETIC_MODELS\n            compounds = list, # sorted list of <compound> ordering='combined_score'\n\n            genes = list, # list with <gene>\n            dbsnp_id = str,\n\n            # Gene ids:\n            hgnc_ids = list, # list of hgnc ids (int)\n            hgnc_symbols = list, # list of hgnc symbols (str)\n            panels = list, # list of panel names that the variant ovelapps\n\n            # Frequencies:\n            thousand_genomes_frequency = float,\n            thousand_genomes_frequency_left = float,\n            thousand_genomes_frequency_right = float,\n            exac_frequency = float,\n            max_thousand_genomes_frequency = float,\n            max_exac_frequency = float,\n            local_frequency = float,\n            local_obs_old = int,\n            local_obs_hom_old = int,\n            local_obs_total_old = int, # default=638\n\n            # Predicted deleteriousness:\n            cadd_score = float,\n            clnsig = list, # list of <clinsig>\n            spidex = float,\n\n            missing_data = bool, # default False\n            \n            # STR specific information\n            str_repid = str, repeat id generally corresponds to gene symbol\n            str_ru = str, used e g in PanelApp naming of STRs\n            str_ref = int, reference copy number\n            str_len = int, number of repeats found in case\n            str_status = str, this indicates the severity of the expansion level\n\n            # Callers\n            gatk = str, # choices=VARIANT_CALL, default='Not Used'\n            samtools = str, # choices=VARIANT_CALL, default='Not Used'\n            freebayes = str, # choices=VARIANT_CALL, default='Not Used'\n\n            # Conservation:\n            phast_conservation = list, # list of str, choices=CONSERVATION\n            gerp_conservation = list, # list of str, choices=CONSERVATION\n            phylop_conservation = list, # list of str, choices=CONSERVATION\n            # Database options:\n            gene_lists = list,\n            manual_rank = int, # choices=[0, 1, 2, 3, 4, 5]\n            dismiss_variant = list,\n\n            acmg_evaluation = str, # choices=ACMG_TERMS\n        )\n\n    \"\"\"\n    gene_to_panels = gene_to_panels or {}\n    hgncid_to_gene = hgncid_to_gene or {}\n    sample_info = sample_info or {}\n\n    #LOG.debug(\"Building variant %s\", variant['ids']['document_id'])\n    variant_obj = dict(\n        _id = variant['ids']['document_id'],\n        document_id=variant['ids']['document_id'],\n        variant_id=variant['ids']['variant_id'],\n        display_name=variant['ids']['display_name'],\n        variant_type=variant['variant_type'],\n        case_id=variant['case_id'],\n        chromosome=variant['chromosome'],\n        reference=variant['reference'],\n        alternative=variant['alternative'],\n        institute=institute_id,\n    )\n\n    variant_obj['missing_data'] = False\n    variant_obj['position'] = int(variant['position'])\n    variant_obj['rank_score'] = float(variant['rank_score'])\n\n    end = variant.get('end')\n    if end:\n        variant_obj['end'] = int(end)\n\n    length = variant.get('length')\n    if length:\n        variant_obj['length'] = int(length)\n\n    variant_obj['simple_id'] = variant['ids'].get('simple_id')\n\n    variant_obj['quality'] = float(variant['quality']) if variant['quality'] else None\n    variant_obj['filters'] = variant['filters']\n\n    variant_obj['dbsnp_id'] = variant.get('dbsnp_id')\n    variant_obj['cosmic_ids'] = variant.get('cosmic_ids')\n\n    variant_obj['category'] = variant['category']\n    variant_obj['sub_category'] = variant.get('sub_category')\n\n    if 'mate_id' in variant:\n        variant_obj['mate_id'] = variant['mate_id']\n\n    if 'cytoband_start' in variant:\n        variant_obj['cytoband_start'] = variant['cytoband_start']\n\n    if 'cytoband_end' in variant:\n        variant_obj['cytoband_end'] = variant['cytoband_end']\n\n    if 'end_chrom' in variant:\n        variant_obj['end_chrom'] = variant['end_chrom']\n\n    ############ Str specific ############\n    if 'str_ru' in variant:\n        variant_obj['str_ru'] = variant['str_ru']\n\n    if 'str_repid' in variant:\n        variant_obj['str_repid'] = variant['str_repid']\n\n    if 'str_ref' in variant:\n        variant_obj['str_ref'] = variant['str_ref']\n\n    if 'str_len' in variant:\n        variant_obj['str_len'] = variant['str_len']\n\n    if 'str_status' in variant:\n        variant_obj['str_status'] = variant['str_status']\n\n    gt_types = []\n    for sample in variant.get('samples', []):\n        gt_call = build_genotype(sample)\n        gt_types.append(gt_call)\n\n        if sample_info:\n            sample_id = sample['individual_id']\n            if sample_info[sample_id] == 'case':\n                key = 'tumor'\n            else:\n                key = 'normal'\n            variant_obj[key] = {\n                    'alt_depth': sample['alt_depth'],\n                    'ref_depth': sample['ref_depth'],\n                    'read_depth': sample['read_depth'],\n                    'alt_freq': sample['alt_frequency'],\n                    'ind_id': sample_id\n                }\n\n\n    variant_obj['samples'] = gt_types\n\n    if 'genetic_models' in variant:\n        variant_obj['genetic_models'] = variant['genetic_models']\n\n    # Add the compounds\n    compounds = []\n    for compound in variant.get('compounds', []):\n        compound_obj = build_compound(compound)\n        compounds.append(compound_obj)\n\n    if compounds:\n        variant_obj['compounds'] = compounds\n\n    # Add the genes with transcripts\n    genes = []\n    for index, gene in enumerate(variant.get('genes', [])):\n        if gene.get('hgnc_id'):\n            gene_obj = build_gene(gene, hgncid_to_gene)\n            genes.append(gene_obj)\n            if index > 30:\n                # avoid uploading too much data (specifically for SV variants)\n                # mark variant as missing data\n                variant_obj['missing_data'] = True\n                break\n\n    if genes:\n        variant_obj['genes'] = genes\n\n    # To make gene searches more effective\n    if 'hgnc_ids' in variant:\n        variant_obj['hgnc_ids'] = [hgnc_id for hgnc_id in variant['hgnc_ids'] if hgnc_id]\n\n    # Add the hgnc symbols from the database genes\n    hgnc_symbols = []\n    for hgnc_id in variant_obj['hgnc_ids']:\n        gene_obj = hgncid_to_gene.get(hgnc_id)\n        if gene_obj:\n            hgnc_symbols.append(gene_obj['hgnc_symbol'])\n        # else:\n            # LOG.warn(\"missing HGNC symbol for: %s\", hgnc_id)\n\n    if hgnc_symbols:\n        variant_obj['hgnc_symbols'] = hgnc_symbols\n\n    # link gene panels\n    panel_names = set()\n    for hgnc_id in variant_obj['hgnc_ids']:\n        gene_panels = gene_to_panels.get(hgnc_id, set())\n        panel_names = panel_names.union(gene_panels)\n\n    if panel_names:\n        variant_obj['panels'] = list(panel_names)\n\n    # Add the clnsig ocbjects\n    clnsig_objects = []\n    for entry in variant.get('clnsig', []):\n        clnsig_obj = build_clnsig(entry)\n        clnsig_objects.append(clnsig_obj)\n\n    if clnsig_objects:\n        variant_obj['clnsig'] = clnsig_objects\n\n    # Add the callers\n    call_info = variant.get('callers', {})\n\n    for caller in call_info:\n        if call_info[caller]:\n            variant_obj[caller] = call_info[caller]\n\n    # Add the conservation\n    conservation_info = variant.get('conservation', {})\n    if conservation_info.get('phast'):\n        variant_obj['phast_conservation'] = conservation_info['phast']\n\n    if conservation_info.get('gerp'):\n        variant_obj['gerp_conservation'] = conservation_info['gerp']\n\n    if conservation_info.get('phylop'):\n        variant_obj['phylop_conservation'] = conservation_info['phylop']\n\n    # Add autozygosity calls\n    if variant.get('azlength'):\n        variant_obj['azlength'] = variant['azlength']\n\n    if variant.get('azqual'):\n        variant_obj['azqual'] = variant['azqual']\n\n    # Add the frequencies\n    frequencies = variant.get('frequencies', {})\n    if frequencies.get('thousand_g'):\n        variant_obj['thousand_genomes_frequency'] = float(frequencies['thousand_g'])\n\n    if frequencies.get('thousand_g_max'):\n        variant_obj['max_thousand_genomes_frequency'] = float(frequencies['thousand_g_max'])\n\n    if frequencies.get('exac'):\n        variant_obj['exac_frequency'] = float(frequencies['exac'])\n\n    if frequencies.get('exac_max'):\n        variant_obj['max_exac_frequency'] = float(frequencies['exac_max'])\n\n    if frequencies.get('gnomad'):\n        variant_obj['gnomad_frequency'] = float(frequencies['gnomad'])\n\n    if frequencies.get('gnomad_max'):\n        variant_obj['max_gnomad_frequency'] = float(frequencies['gnomad_max'])\n\n    if frequencies.get('thousand_g_left'):\n        variant_obj['thousand_genomes_frequency_left'] = float(frequencies['thousand_g_left'])\n\n    if frequencies.get('thousand_g_right'):\n        variant_obj['thousand_genomes_frequency_right'] = float(frequencies['thousand_g_right'])\n\n    # add the local observation counts from the old archive\n    if variant.get('local_obs_old'):\n        variant_obj['local_obs_old'] =  variant['local_obs_old']\n\n    if variant.get('local_obs_hom_old'):\n        variant_obj['local_obs_hom_old'] = variant['local_obs_hom_old']\n\n    # Add the sv counts:\n    if frequencies.get('clingen_cgh_benign'):\n        variant_obj['clingen_cgh_benign'] = frequencies['clingen_cgh_benign']\n    if frequencies.get('clingen_cgh_pathogenic'):\n        variant_obj['clingen_cgh_pathogenic'] = frequencies['clingen_cgh_pathogenic']\n    if frequencies.get('clingen_ngi'):\n        variant_obj['clingen_ngi'] = frequencies['clingen_ngi']\n    if frequencies.get('swegen'):\n            variant_obj['swegen'] = frequencies['swegen']\n    # Decipher is never a frequency, it will ony give 1 if variant exists in decipher\n    # Check if decipher exists\n    if frequencies.get('decipher'):\n        variant_obj['decipher'] = frequencies['decipher']\n    # If not check if field decipherAF exists\n    elif frequencies.get('decipherAF'):\n        variant_obj['decipher'] = frequencies['decipherAF']\n\n    # Add the severity predictors\n\n    if variant.get('cadd_score'):\n        variant_obj['cadd_score'] = variant['cadd_score']\n\n    if variant.get('spidex'):\n        variant_obj['spidex'] = variant['spidex']\n\n    # Add the rank score results\n    rank_results = []\n    for category in variant.get('rank_result', []):\n        rank_result = {\n            'category': category,\n            'score': variant['rank_result'][category]\n        }\n        rank_results.append(rank_result)\n\n    if rank_results:\n        variant_obj['rank_score_results'] = rank_results\n\n    # Cancer specific\n    if variant.get('mvl_tag'):\n        variant_obj['mvl_tag'] = True\n\n    return variant_obj", "response": "Build a variant object based on parsed information."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload the hgnc and transcript aliases for a given build.", "response": "def genes(context, build, api_key):\n    \"\"\"\n    Load the hgnc aliases to the mongo database.\n    \"\"\"\n    LOG.info(\"Running scout update genes\")\n    adapter = context.obj['adapter']\n\n    # Fetch the omim information\n    api_key = api_key or context.obj.get('omim_api_key')\n    if not api_key:\n        LOG.warning(\"Please provide a omim api key to load the omim gene panel\")\n        context.abort()\n\n    try:\n        mim_files = fetch_mim_files(api_key, mim2genes=True, morbidmap=True, genemap2=True)\n    except Exception as err:\n        LOG.warning(err)\n        context.abort()\n\n    LOG.warning(\"Dropping all gene information\")\n    adapter.drop_genes(build)\n    LOG.info(\"Genes dropped\")\n    LOG.warning(\"Dropping all transcript information\")\n    adapter.drop_transcripts(build)\n    LOG.info(\"transcripts dropped\")\n\n    hpo_genes = fetch_hpo_genes()\n    \n    if build:\n        builds = [build]\n    else:\n        builds = ['37', '38']\n    \n    hgnc_lines = fetch_hgnc()\n    exac_lines = fetch_exac_constraint()\n    \n    \n    for build in builds:\n        ensembl_genes = fetch_ensembl_genes(build=build)\n        \n        # load the genes\n        hgnc_genes = load_hgnc_genes(\n            adapter=adapter,\n            ensembl_lines=ensembl_genes,\n            hgnc_lines=hgnc_lines,\n            exac_lines=exac_lines,\n            mim2gene_lines=mim_files['mim2genes'],\n            genemap_lines=mim_files['genemap2'],\n            hpo_lines=hpo_genes,\n            build=build,\n        )\n\n        ensembl_genes = {}\n        for gene_obj in hgnc_genes:\n            ensembl_id = gene_obj['ensembl_id']\n            ensembl_genes[ensembl_id] = gene_obj\n\n        # Fetch the transcripts from ensembl\n        ensembl_transcripts = fetch_ensembl_transcripts(build=build)\n        \n        transcripts = load_transcripts(adapter, ensembl_transcripts, build, ensembl_genes)\n\n    adapter.update_indexes()\n        \n    LOG.info(\"Genes, transcripts and Exons loaded\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_callers(variant, category='snv'):\n    relevant_callers = CALLERS[category]\n    callers = {caller['id']: None for caller in relevant_callers}\n    raw_info = variant.INFO.get('set')\n    if raw_info:\n        info = raw_info.split('-')\n        for call in info:\n            if call == 'FilteredInAll':\n                for caller in callers:\n                    callers[caller] = 'Filtered'\n            elif call == 'Intersection':\n                for caller in callers:\n                    callers[caller] = 'Pass'\n            elif 'filterIn' in call:\n                for caller in callers:\n                    if caller in call:\n                        callers[caller] = 'Filtered'\n\n            elif call in set(callers.keys()):\n                callers[call] = 'Pass'\n    # The following is parsing of a custom made merge\n    other_info = variant.INFO.get('FOUND_IN')\n    if other_info:\n        for info in other_info.split(','):\n            called_by = info.split('|')[0]\n            callers[called_by] = 'Pass'\n\n    return callers", "response": "Parse how the different variant callers have performed\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the format from a vcf header line description", "response": "def parse_header_format(description):\n    \"\"\"Get the format from a vcf header line description\n    \n    If format begins with white space it will be stripped\n    \n    Args:\n        description(str): Description from a vcf header line\n    \n    Return:\n        format(str): The format information from description\n    \"\"\"\n    description = description.strip('\"')\n    keyword = 'Format:'\n    before_keyword, keyword, after_keyword = description.partition(keyword)\n    return after_keyword.strip()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_vep_header(vcf_obj):\n    vep_header = []\n    \n    if 'CSQ' in vcf_obj:\n        # This is a dictionary\n        csq_info = vcf_obj['CSQ']\n        format_info = parse_header_format(csq_info['Description'])\n        vep_header = [key.upper() for key in format_info.split('|')]\n    \n    return vep_header", "response": "Return a list with the VEP header collected from CSQ in the vcf file\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_transcript(transcript_info, build='37'):\n    try:\n        transcript_id = transcript_info['ensembl_transcript_id']\n    except KeyError:\n        raise KeyError(\"Transcript has to have ensembl id\")\n    \n    build = build\n    is_primary = transcript_info.get('is_primary', False)\n    \n    refseq_id = transcript_info.get('refseq_id')\n    refseq_identifiers = transcript_info.get('refseq_identifiers')\n\n    try:\n        chrom = transcript_info['chrom']\n    except KeyError:\n        raise KeyError(\"Transcript has to have a chromosome\")\n    \n    try:\n        start = int(transcript_info['transcript_start'])\n    except KeyError:\n        raise KeyError(\"Transcript has to have start\")\n    except TypeError:\n        raise TypeError(\"Transcript start has to be integer\")\n\n    try:\n        end = int(transcript_info['transcript_end'])\n    except KeyError:\n        raise KeyError(\"Transcript has to have end\")\n    except TypeError:\n        raise TypeError(\"Transcript end has to be integer\")\n\n    try:\n        hgnc_id = int(transcript_info['hgnc_id'])\n    except KeyError:\n        raise KeyError(\"Transcript has to have a hgnc id\")\n    except TypeError:\n        raise TypeError(\"hgnc id has to be integer\")\n\n    transcript_obj = HgncTranscript(\n        transcript_id=transcript_id, \n        hgnc_id=hgnc_id, \n        chrom=chrom, \n        start=start, \n        end=end, \n        is_primary=is_primary, \n        refseq_id=refseq_id,\n        refseq_identifiers=refseq_identifiers,\n        build=build\n    )\n    # Remove unnessesary keys\n    for key in list(transcript_obj):\n        if transcript_obj[key] is None:\n            transcript_obj.pop(key)\n\n    return transcript_obj", "response": "Build a HGNC_Transcript object from a dictionary of transcript information."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_institute(adapter, internal_id, display_name, sanger_recipients=None):\n\n    institute_obj = build_institute(\n        internal_id=internal_id,\n        display_name=display_name,\n        sanger_recipients=sanger_recipients\n    )\n    log.info(\"Loading institute {0} with display name {1}\" \\\n             \" into database\".format(internal_id, display_name))\n\n    adapter.add_institute(institute_obj)", "response": "Load a institute into the database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if the cadd phred score is annotated", "response": "def parse_cadd(variant, transcripts):\n    \"\"\"Check if the cadd phred score is annotated\"\"\"\n    cadd = 0\n    cadd_keys = ['CADD', 'CADD_PHRED']\n    for key in cadd_keys:\n        cadd = variant.INFO.get(key, 0)\n        if cadd:\n            return float(cadd)\n    \n    for transcript in transcripts:\n        cadd_entry = transcript.get('cadd')\n        if (cadd_entry and cadd_entry > cadd):\n            cadd = cadd_entry\n    \n    return cadd"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef case(context, vcf, vcf_sv, vcf_cancer, vcf_str, owner, ped, update, config,\n         no_variants, peddy_ped, peddy_sex, peddy_check):\n    \"\"\"Load a case into the database.\n\n    A case can be loaded without specifying vcf files and/or bam files\n    \"\"\"\n    adapter = context.obj['adapter']\n\n    if config is None and ped is None:\n        LOG.warning(\"Please provide either scout config or ped file\")\n        context.abort()\n\n    # Scout needs a config object with the neccessary information\n    # If no config is used create a dictionary\n    config_raw = yaml.load(config) if config else {}\n    \n    try:\n        config_data = parse_case_data(\n            config=config_raw,\n            ped=ped,\n            owner=owner,\n            vcf_snv=vcf,\n            vcf_sv=vcf_sv,\n            vcf_str=vcf_str,\n            vcf_cancer=vcf_cancer,\n            peddy_ped=peddy_ped,\n            peddy_sex=peddy_sex,\n            peddy_check=peddy_check\n        )\n    except SyntaxError as err:\n        LOG.warning(err)\n        context.abort()\n\n    LOG.info(\"Use family %s\" % config_data['family'])\n    \n    try:\n        case_obj = adapter.load_case(config_data, update)\n    except Exception as err:\n        LOG.error(\"Something went wrong during loading\")\n        LOG.warning(err)\n        context.abort()", "response": "Load a single case into the database."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_variant(self, variant_obj):\n        LOG.debug('Updating variant %s', variant_obj.get('simple_id'))\n\n        new_variant = self.variant_collection.find_one_and_replace(\n            {'_id': variant_obj['_id']},\n            variant_obj,\n            return_document=pymongo.ReturnDocument.AFTER\n        )\n        return new_variant", "response": "Update one variant document in the database."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_variant_rank(self, case_obj, variant_type='clinical', category='snv'):\n        # Get all variants sorted by rank score\n        variants = self.variant_collection.find({\n            'case_id': case_obj['_id'],\n            'category': category,\n            'variant_type': variant_type,\n        }).sort('rank_score', pymongo.DESCENDING)\n\n        LOG.info(\"Updating variant_rank for all variants\")\n\n        requests = []\n\n        for index, var_obj in enumerate(variants):\n            if len(requests) > 5000:\n                try:\n                    self.variant_collection.bulk_write(requests, ordered=False)\n                    requests = []\n                except BulkWriteError as err:\n                    LOG.warning(\"Updating variant rank failed\")\n                    raise err\n\n            operation = pymongo.UpdateOne(\n                {'_id': var_obj['_id']},\n                {\n                    '$set': {\n                        'variant_rank': index + 1,\n                    }\n                })\n            requests.append(operation)\n\n        #Update the final bulk\n        try:\n            self.variant_collection.bulk_write(requests, ordered=False)\n        except BulkWriteError as err:\n            LOG.warning(\"Updating variant rank failed\")\n            raise err\n\n        LOG.info(\"Updating variant_rank done\")", "response": "Updates the manual rank for all variants in a case"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate compounds for a variant. This will add all the necessary information of a variant on a compound object. Args: variant(scout.models.Variant) variant_objs(dict): A dictionary with _ids as keys and variant objs as values. Returns: compound_objs(list(dict)): A dictionary with updated compound objects.", "response": "def update_variant_compounds(self, variant, variant_objs = None):\n        \"\"\"Update compounds for a variant.\n\n        This will add all the necessary information of a variant on a compound object.\n\n        Args:\n            variant(scout.models.Variant)\n            variant_objs(dict): A dictionary with _ids as keys and variant objs as values.\n\n        Returns:\n            compound_objs(list(dict)): A dictionary with updated compound objects.\n\n        \"\"\"\n        compound_objs = []\n        for compound in variant.get('compounds', []):\n            not_loaded = True\n            gene_objs = []\n            # Check if the compound variant exists\n            if variant_objs:\n                variant_obj = variant_objs.get(compound['variant'])\n            else:\n                variant_obj = self.variant_collection.find_one({'_id': compound['variant']})\n            if variant_obj:\n                # If the variant exosts we try to collect as much info as possible\n                not_loaded = False\n                compound['rank_score'] = variant_obj['rank_score']\n                for gene in variant_obj.get('genes', []):\n                    gene_obj = {\n                        'hgnc_id': gene['hgnc_id'],\n                        'hgnc_symbol': gene.get('hgnc_symbol'),\n                        'region_annotation': gene.get('region_annotation'),\n                        'functional_annotation': gene.get('functional_annotation'),\n                    }\n                    gene_objs.append(gene_obj)\n                    compound['genes'] = gene_objs\n\n            compound['not_loaded'] = not_loaded\n            compound_objs.append(compound)\n\n        return compound_objs"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_compounds(self, variants):\n        LOG.debug(\"Updating compound objects\")\n\n        for var_id in variants:\n            variant_obj = variants[var_id]\n            if not variant_obj.get('compounds'):\n                continue\n\n            updated_compounds = self.update_variant_compounds(variant_obj, variants)\n            variant_obj['compounds'] = updated_compounds\n\n        LOG.debug(\"Compounds updated\")\n\n        return variants", "response": "Updates the compounds of a set of variants."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_mongo_compound_variants(self, bulk):\n        requests = []\n        for var_id in bulk:\n            var_obj = bulk[var_id]\n            if not var_obj.get('compounds'):\n                continue\n            # Add a request to update compounds\n            operation = pymongo.UpdateOne(\n                {'_id': var_obj['_id']},\n                {\n                    '$set': {\n                        'compounds': var_obj['compounds']\n                    }\n                })\n            requests.append(operation)\n\n        if not requests:\n            return\n\n        try:\n            self.variant_collection.bulk_write(requests, ordered=False)\n        except BulkWriteError as err:\n            LOG.warning(\"Updating compounds failed\")\n            raise err", "response": "Update the compound information for a bulk of variants in the database"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_case_compounds(self, case_obj, build='37'):\n\n        case_id = case_obj['_id']\n        # Possible categories 'snv', 'sv', 'str', 'cancer':\n        categories = set()\n        # Possible variant types 'clinical', 'research':\n        variant_types = set()\n\n        for file_type in FILE_TYPE_MAP:\n            if case_obj.get('vcf_files',{}).get(file_type):\n                categories.add(FILE_TYPE_MAP[file_type]['category'])\n                variant_types.add(FILE_TYPE_MAP[file_type]['variant_type'])\n\n        coding_intervals = self.get_coding_intervals(build=build)\n        # Loop over all intervals\n        for chrom in CHROMOSOMES:\n            intervals = coding_intervals.get(chrom, IntervalTree())\n            for var_type in variant_types:\n                for category in categories:\n                    LOG.info(\"Updating compounds on chromosome:{0}, type:{1}, category:{2} for case:{3}\".format(\n                             chrom, var_type, category, case_id))\n\n                    # Fetch all variants from a chromosome\n                    query  = {\n                        'variant_type': var_type,\n                        'chrom': chrom,\n                    }\n\n                    # Get all variants from the database of the specific type\n                    variant_objs = self.variants(\n                        case_id=case_id,\n                        query=query,\n                        category=category,\n                        nr_of_variants=-1,\n                        sort_key='position'\n                    )\n\n                    # Initiate a bulk\n                    bulk = {}\n                    current_region = None\n                    special = False\n\n                    # Loop over the variants and check if they are in a coding region\n                    for var_obj in variant_objs:\n                        var_id = var_obj['_id']\n                        var_chrom = var_obj['chromosome']\n                        var_start = var_obj['position']\n                        var_end = var_obj['end'] + 1\n\n                        update_bulk = True\n                        new_region = None\n\n                        # Check if the variant is in a coding region\n                        genomic_regions = coding_intervals.get(var_chrom, IntervalTree()).search(var_start, var_end)\n\n\n                        # If the variant is in a coding region\n                        if genomic_regions:\n                            # We know there is data here so get the interval id\n                            new_region = genomic_regions.pop().data\n\n                        if new_region and (new_region == current_region):\n                            # If the variant is in the same region as previous\n                            # we add it to the same bulk\n                            update_bulk = False\n\n                        current_region = new_region\n\n                        # If the variant is not in a current region we update the compounds\n                        # from the previous region, if any. Otherwise continue\n                        if update_bulk and bulk:\n                            self.update_compounds(bulk)\n                            self.update_mongo_compound_variants(bulk)\n                            bulk = {}\n\n                        if new_region:\n                            bulk[var_id] = var_obj\n\n                    if not bulk:\n                        continue\n\n                    self.update_compounds(bulk)\n                    self.update_mongo_compound_variants(bulk)\n\n        LOG.info(\"All compounds updated\")\n        return", "response": "Update the compounds for a case."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads a variant object into the database.", "response": "def load_variant(self, variant_obj):\n        \"\"\"Load a variant object\n\n        Args:\n            variant_obj(dict)\n\n        Returns:\n            inserted_id\n        \"\"\"\n        # LOG.debug(\"Loading variant %s\", variant_obj['_id'])\n        try:\n            result = self.variant_collection.insert_one(variant_obj)\n        except DuplicateKeyError as err:\n            raise IntegrityError(\"Variant %s already exists in database\", variant_obj['_id'])\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef upsert_variant(self, variant_obj):\n        LOG.debug(\"Upserting variant %s\", variant_obj['_id'])\n        try:\n            result = self.variant_collection.insert_one(variant_obj)\n        except DuplicateKeyError as err:\n            LOG.debug(\"Variant %s already exists in database\", variant_obj['_id'])\n            result = self.variant_collection.find_one_and_update(\n                {'_id': variant_obj['_id']},\n                {\n                    '$set': {\n                        'compounds': variant_obj.get('compounds',[])\n                    }\n                }\n            )\n            variant = self.variant_collection.find_one({'_id': variant_obj['_id']})\n        return result", "response": "Load a variant object into the database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload a bulk of variants into the database.", "response": "def load_variant_bulk(self, variants):\n        \"\"\"Load a bulk of variants\n\n        Args:\n            variants(iterable(scout.models.Variant))\n\n        Returns:\n            object_ids\n        \"\"\"\n        if not len(variants) > 0:\n            return\n\n        LOG.debug(\"Loading variant bulk\")\n        try:\n            result = self.variant_collection.insert_many(variants)\n        except (DuplicateKeyError, BulkWriteError) as err:\n            # If the bulk write is wrong there are probably some variants already existing\n            # In the database. So insert each variant\n            for var_obj in variants:\n                try:\n                    self.upsert_variant(var_obj)\n                except IntegrityError as err:\n                    pass\n\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload the variants into the database.", "response": "def _load_variants(self, variants, variant_type, case_obj, individual_positions, rank_threshold,\n                       institute_id, build=None, rank_results_header=None, vep_header=None,\n                       category='snv', sample_info = None):\n        \"\"\"Perform the loading of variants\n\n        This is the function that loops over the variants, parse them and build the variant\n        objects so they are ready to be inserted into the database.\n\n        \"\"\"\n        build = build or '37'\n        genes = [gene_obj for gene_obj in self.all_genes(build=build)]\n        gene_to_panels = self.gene_to_panels(case_obj)\n        hgncid_to_gene = self.hgncid_to_gene(genes=genes)\n        genomic_intervals = self.get_coding_intervals(genes=genes)\n\n        LOG.info(\"Start inserting {0} {1} variants into database\".format(variant_type, category))\n        start_insertion = datetime.now()\n        start_five_thousand = datetime.now()\n        # These are the number of parsed varaints\n        nr_variants = 0\n        # These are the number of variants that meet the criteria and gets inserted\n        nr_inserted = 0\n        # This is to keep track of blocks of inserted variants\n        inserted = 1\n\n        nr_bulks = 0\n\n        # We want to load batches of variants to reduce the number of network round trips\n        bulk = {}\n        current_region = None\n\n        for nr_variants, variant in enumerate(variants):\n            # All MT variants are loaded\n            mt_variant = 'MT' in variant.CHROM\n            rank_score = parse_rank_score(variant.INFO.get('RankScore'), case_obj['_id'])\n\n            # Check if the variant should be loaded at all\n            # if rank score is None means there are no rank scores annotated, all variants will be loaded\n            # Otherwise we load all variants above a rank score treshold\n            # Except for MT variants where we load all variants\n            if (rank_score is None) or (rank_score > rank_threshold) or mt_variant:\n                nr_inserted += 1\n                # Parse the vcf variant\n                parsed_variant = parse_variant(\n                    variant=variant,\n                    case=case_obj,\n                    variant_type=variant_type,\n                    rank_results_header=rank_results_header,\n                    vep_header=vep_header,\n                    individual_positions=individual_positions,\n                    category=category,\n                )\n\n                # Build the variant object\n                variant_obj = build_variant(\n                    variant=parsed_variant,\n                    institute_id=institute_id,\n                    gene_to_panels=gene_to_panels,\n                    hgncid_to_gene=hgncid_to_gene,\n                    sample_info=sample_info\n                )\n\n                # Check if the variant is in a genomic region\n                var_chrom = variant_obj['chromosome']\n                var_start = variant_obj['position']\n                # We need to make sure that the interval has a length > 0\n                var_end = variant_obj['end'] + 1\n                var_id = variant_obj['_id']\n                # If the bulk should be loaded or not\n                load = True\n                new_region = None\n\n                genomic_regions = genomic_intervals.get(var_chrom, IntervalTree()).search(var_start, var_end)\n\n                # If the variant is in a coding region\n                if genomic_regions:\n                    # We know there is data here so get the interval id\n                    new_region = genomic_regions.pop().data\n                    # If the variant is in the same region as previous\n                    # we add it to the same bulk\n                    if new_region == current_region:\n                        load = False\n\n                # This is the case where the variant is intergenic\n                else:\n                    # If the previous variant was also intergenic we add the variant to the bulk\n                    if not current_region:\n                        load = False\n                    # We need to have a max size of the bulk\n                    if len(bulk) > 10000:\n                        load = True\n                # Load the variant object\n                if load:\n                    # If the variant bulk contains coding variants we want to update the compounds\n                    if current_region:\n                        self.update_compounds(bulk)\n                    try:\n                        # Load the variants\n                        self.load_variant_bulk(list(bulk.values()))\n                        nr_bulks += 1\n                    except IntegrityError as error:\n                        pass\n                    bulk = {}\n\n                current_region = new_region\n                bulk[var_id] = variant_obj\n\n                if (nr_variants != 0 and nr_variants % 5000 == 0):\n                    LOG.info(\"%s variants parsed\", str(nr_variants))\n                    LOG.info(\"Time to parse variants: %s\",\n                                (datetime.now() - start_five_thousand))\n                    start_five_thousand = datetime.now()\n\n                if (nr_inserted != 0 and (nr_inserted * inserted) % (1000 * inserted) == 0):\n                    LOG.info(\"%s variants inserted\", nr_inserted)\n                    inserted += 1\n        # If the variants are in a coding region we update the compounds\n        if current_region:\n            self.update_compounds(bulk)\n\n        # Load the final variant bulk\n        self.load_variant_bulk(list(bulk.values()))\n        nr_bulks += 1\n        LOG.info(\"All variants inserted, time to insert variants: {0}\".format(\n            datetime.now() - start_insertion))\n\n        if nr_variants:\n            nr_variants += 1\n        LOG.info(\"Nr variants parsed: %s\", nr_variants)\n        LOG.info(\"Nr variants inserted: %s\", nr_inserted)\n        LOG.debug(\"Nr bulks inserted: %s\", nr_bulks)\n\n        return nr_inserted"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading the variants for a specific analysis type and category into the database.", "response": "def load_variants(self, case_obj, variant_type='clinical', category='snv',\n                      rank_threshold=None, chrom=None, start=None, end=None,\n                      gene_obj=None, build='37'):\n        \"\"\"Load variants for a case into scout.\n\n        Load the variants for a specific analysis type and category into scout.\n        If no region is specified, load all variants above rank score threshold\n        If region or gene is specified, load all variants from that region\n        disregarding variant rank(if not specified)\n\n        Args:\n            case_obj(dict): A case from the scout database\n            variant_type(str): 'clinical' or 'research'. Default: 'clinical'\n            category(str): 'snv', 'str' or 'sv'. Default: 'snv'\n            rank_threshold(float): Only load variants above this score. Default: 0\n            chrom(str): Load variants from a certain chromosome\n            start(int): Specify the start position\n            end(int): Specify the end position\n            gene_obj(dict): A gene object from the database\n\n        Returns:\n            nr_inserted(int)\n        \"\"\"\n        # We need the institute object\n        institute_id = self.institute(institute_id=case_obj['owner'])['_id']\n        nr_inserted = 0\n\n        variant_file = None\n        if variant_type == 'clinical':\n            if category == 'snv':\n                variant_file = case_obj['vcf_files'].get('vcf_snv')\n            elif category == 'sv':\n                variant_file = case_obj['vcf_files'].get('vcf_sv')\n            elif category == 'str':\n                LOG.debug('Attempt to load STR VCF.')\n                variant_file = case_obj['vcf_files'].get('vcf_str')\n            elif category == 'cancer':\n                # Currently this implies a paired tumor normal\n                variant_file = case_obj['vcf_files'].get('vcf_cancer')\n        elif variant_type == 'research':\n            if category == 'snv':\n                variant_file = case_obj['vcf_files'].get('vcf_snv_research')\n            elif category == 'sv':\n                variant_file = case_obj['vcf_files'].get('vcf_sv_research')\n            elif category == 'cancer':\n                variant_file = case_obj['vcf_files'].get('vcf_cancer_research')\n\n        if not variant_file:\n            raise SyntaxError(\"Vcf file does not seem to exist\")\n\n        # Check if there are any variants in file\n        try:\n            vcf_obj = VCF(variant_file)\n            var = next(vcf_obj)\n        except StopIteration as err:\n            LOG.warning(\"Variant file %s does not include any variants\", variant_file)\n            return nr_inserted\n        # We need to reload the file\n        vcf_obj = VCF(variant_file)\n\n        # Parse the neccessary headers from vcf file\n        rank_results_header = parse_rank_results_header(vcf_obj)\n        vep_header = parse_vep_header(vcf_obj)\n\n        # This is a dictionary to tell where ind are in vcf\n        individual_positions = {}\n        for i, ind in enumerate(vcf_obj.samples):\n            individual_positions[ind] = i\n        \n        # Dictionary for cancer analysis\n        sample_info = {}\n        if category == 'cancer':\n            for ind in case_obj['individuals']:\n                if ind['phenotype'] == 2:\n                    sample_info[ind['individual_id']] = 'case'\n                else:\n                    sample_info[ind['individual_id']] = 'control'\n\n        # Check if a region scould be uploaded\n        region = \"\"\n        if gene_obj:\n            chrom = gene_obj['chromosome']\n            # Add same padding as VEP\n            start = max(gene_obj['start'] - 5000, 0)\n            end = gene_obj['end'] + 5000\n        if chrom:\n            # We want to load all variants in the region regardless of rank score\n            rank_threshold = rank_threshold or -1000\n            if not (start and end):\n                raise SyntaxError(\"Specify chrom start and end\")\n            region = \"{0}:{1}-{2}\".format(chrom, start, end)\n        else:\n            rank_threshold = rank_threshold or 0\n\n        variants = vcf_obj(region)\n\n        try:\n            nr_inserted = self._load_variants(\n                variants=variants,\n                variant_type=variant_type,\n                case_obj=case_obj,\n                individual_positions=individual_positions,\n                rank_threshold=rank_threshold,\n                institute_id=institute_id,\n                build=build,\n                rank_results_header=rank_results_header,\n                vep_header=vep_header,\n                category=category,\n                sample_info = sample_info\n            )\n        except Exception as error:\n            LOG.exception('unexpected error')\n            LOG.warning(\"Deleting inserted variants\")\n            self.delete_variants(case_obj['_id'], variant_type)\n            raise error\n\n        self.update_variant_rank(case_obj, variant_type, category=category)\n\n        return nr_inserted"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nassigning a user to a case. This function will create an Event to log that a person has been assigned to a case. Also the user will be added to case \"assignees\". Arguments: institute (dict): A institute case (dict): A case user (dict): A User object link (str): The url to be used in the event Returns: updated_case(dict)", "response": "def assign(self, institute, case, user, link):\n        \"\"\"Assign a user to a case.\n\n        This function will create an Event to log that a person has been assigned\n        to a case. Also the user will be added to case \"assignees\".\n\n        Arguments:\n            institute (dict): A institute\n            case (dict): A case\n            user (dict): A User object\n            link (str): The url to be used in the event\n\n        Returns:\n            updated_case(dict)\n        \"\"\"\n        LOG.info(\"Creating event for assigning {0} to {1}\"\n                    .format(user['name'].encode('utf-8'), case['display_name']))\n\n        self.create_event(\n            institute=institute,\n            case=case,\n            user=user,\n            link=link,\n            category='case',\n            verb='assign',\n            subject=case['display_name']\n        )\n        LOG.info(\"Updating {0} to be assigned with {1}\"\n                    .format(case['display_name'], user['name']))\n\n        updated_case = self.case_collection.find_one_and_update(\n            {'_id': case['_id']},\n            {'$addToSet': {'assignees': user['_id']}},\n            return_document=pymongo.ReturnDocument.AFTER\n        )\n        return updated_case"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nshare a case with a new institute.", "response": "def share(self, institute, case, collaborator_id, user, link):\n        \"\"\"Share a case with a new institute.\n\n        Arguments:\n            institute (dict): A Institute object\n            case (dict): Case object\n            collaborator_id (str): A instute id\n            user (dict): A User object\n            link (str): The url to be used in the event\n\n        Return:\n            updated_case\n        \"\"\"\n        if collaborator_id in case.get('collaborators', []):\n            raise ValueError('new customer is already a collaborator')\n\n        self.create_event(\n            institute=institute,\n            case=case,\n            user=user,\n            link=link,\n            category='case',\n            verb='share',\n            subject=collaborator_id\n        )\n\n        updated_case = self.case_collection.find_one_and_update(\n            {'_id': case['_id']},\n            {\n                '$push': {'collaborators': collaborator_id}\n            },\n            return_document=pymongo.ReturnDocument.AFTER\n        )\n        LOG.debug(\"Case updated\")\n        return updated_case"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndiagnose a case using OMIM ids.", "response": "def diagnose(self, institute, case, user, link, level, omim_id, remove=False):\n        \"\"\"Diagnose a case using OMIM ids.\n\n        Arguments:\n            institute (dict): A Institute object\n            case (dict): Case object\n            user (dict): A User object\n            link (str): The url to be used in the event\n            level (str): choices=('phenotype','gene')\n\n        Return:\n            updated_case\n\n        \"\"\"\n        if level == 'phenotype':\n            case_key = 'diagnosis_phenotypes'\n        elif level == 'gene':\n            case_key = 'diagnosis_genes'\n        else:\n            raise TypeError('wrong level')\n\n        diagnosis_list = case.get(case_key, [])\n        omim_number = int(omim_id.split(':')[-1])\n\n        updated_case = None\n        if remove and omim_number in diagnosis_list:\n            updated_case = self.case_collection.find_one_and_update(\n                {'_id': case['_id']},\n                {'$pull': {case_key: omim_number}},\n                return_document=pymongo.ReturnDocument.AFTER\n            )\n\n        elif omim_number not in diagnosis_list:\n            updated_case = self.case_collection.find_one_and_update(\n                {'_id': case['_id']},\n                {'$push': {case_key: omim_number}},\n                return_document=pymongo.ReturnDocument.AFTER\n            )\n\n        if updated_case:\n            self.create_event(\n                institute=institute,\n                case=case,\n                user=user,\n                link=link,\n                category='case',\n                verb='update_diagnosis',\n                subject=case['display_name'],\n                content=omim_id\n            )\n\n        return updated_case"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mark_checked(self, institute, case, user, link,\n                     unmark=False):\n        \"\"\"Mark a case as checked from an analysis point of view.\n\n        Arguments:\n            institute (dict): A Institute object\n            case (dict): Case object\n            user (dict): A User object\n            link (str): The url to be used in the event\n            unmark (bool): If case should ve unmarked\n\n        Return:\n            updated_case\n        \"\"\"\n\n        LOG.info(\"Updating checked status of {}\"\n                    .format(case['display_name']))\n\n        status = 'not checked' if unmark else 'checked'\n        self.create_event(\n            institute=institute,\n            case=case,\n            user=user,\n            link=link,\n            category='case',\n            verb='check_case',\n            subject=status\n        )\n\n        LOG.info(\"Updating {0}'s checked status {1}\"\n                    .format(case['display_name'], status))\n        analysis_checked = False if unmark else True\n        updated_case = self.case_collection.find_one_and_update(\n            {'_id': case['_id']},\n            {\n                '$set': {'analysis_checked': analysis_checked}\n            },\n            return_document=pymongo.ReturnDocument.AFTER\n        )\n        LOG.debug(\"Case updated\")\n        return updated_case", "response": "Mark a case as checked from an analysis point of view."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_default_panels(self, institute_obj, case_obj, user_obj, link, panel_objs):\n        self.create_event(\n            institute=institute_obj,\n            case=case_obj,\n            user=user_obj,\n            link=link,\n            category='case',\n            verb='update_default_panels',\n            subject=case_obj['display_name'],\n        )\n\n        LOG.info(\"Update default panels for {}\".format(case_obj['display_name']))\n\n        panel_ids = [panel['_id'] for panel in panel_objs]\n\n        for existing_panel in case_obj['panels']:\n            if existing_panel['panel_id'] in panel_ids:\n                existing_panel['is_default'] = True\n            else:\n                existing_panel['is_default'] = False\n\n        updated_case = self.case_collection.find_one_and_update(\n            {'_id': case_obj['_id']},\n            {\n                '$set': {'panels': case_obj['panels']}\n            },\n            return_document=pymongo.ReturnDocument.AFTER\n        )\n        LOG.debug(\"Case updated\")\n\n        return updated_case", "response": "Update default panels for a case."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating an event for a variant verification for a specific object.", "response": "def order_verification(self, institute, case, user, link, variant):\n        \"\"\"Create an event for a variant verification for a variant\n        and an event for a variant verification for a case\n\n        Arguments:\n            institute (dict): A Institute object\n            case (dict): Case object\n            user (dict): A User object\n            link (str): The url to be used in the event\n            variant (dict): A variant object\n\n        Returns:\n            updated_variant(dict)\n        \"\"\"\n        LOG.info(\"Creating event for ordering validation for variant\" \\\n                    \" {0}\".format(variant['display_name']))\n\n        updated_variant = self.variant_collection.find_one_and_update(\n            {'_id': variant['_id']},\n            {'$set': {'sanger_ordered': True}},\n            return_document=pymongo.ReturnDocument.AFTER\n        )\n\n        self.create_event(\n            institute=institute,\n            case=case,\n            user=user,\n            link=link,\n            category='variant',\n            verb='sanger',\n            variant=variant,\n            subject=variant['display_name'],\n        )\n\n        LOG.info(\"Creating event for ordering sanger for case\" \\\n                    \" {0}\".format(case['display_name']))\n\n        self.create_event(\n            institute=institute,\n            case=case,\n            user=user,\n            link=link,\n            category='case',\n            verb='sanger',\n            variant=variant,\n            subject=variant['display_name'],\n        )\n        return updated_variant"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget all variants with validations ever ordered.", "response": "def sanger_ordered(self, institute_id=None, user_id=None):\n        \"\"\"Get all variants with validations ever ordered.\n\n        Args:\n            institute_id(str) : The id of an institute\n            user_id(str) : The id of an user\n\n        Returns:\n            sanger_ordered(list) : a list of dictionaries, each with \"case_id\" as keys and list of variant ids as values\n        \"\"\"\n        query = {'$match': {\n                '$and': [\n                    {'verb': 'sanger'},\n                ],\n            }}\n\n        if institute_id:\n            query['$match']['$and'].append({'institute': institute_id})\n        if user_id:\n            query['$match']['$and'].append({'user_id': user_id})\n\n        # Get all sanger ordered variants grouped by case_id\n        results = self.event_collection.aggregate([\n            query,\n            {'$group': {\n                '_id': \"$case\",\n                'vars': {'$addToSet' : '$variant_id'}\n            }}\n        ])\n\n        sanger_ordered =  [item for item in results]\n        return sanger_ordered"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmarks validation status for a variant.", "response": "def validate(self, institute, case, user, link, variant, validate_type):\n        \"\"\"Mark validation status for a variant.\n\n        Arguments:\n            institute (dict): A Institute object\n            case (dict): Case object\n            user (dict): A User object\n            link (str): The url to be used in the event\n            variant (dict): A variant object\n            validate_type(str): The outcome of validation.\n                                choices=('True positive', 'False positive')\n\n        Returns:\n            updated_variant(dict)\n        \"\"\"\n        if not validate_type in SANGER_OPTIONS:\n            LOG.warning(\"Invalid validation string: %s\", validate_type)\n            LOG.info(\"Validation options: %s\", ', '.join(SANGER_OPTIONS))\n            return\n\n        updated_variant = self.variant_collection.find_one_and_update(\n            {'_id': variant['_id']},\n            {'$set': {'validation': validate_type}},\n            return_document=pymongo.ReturnDocument.AFTER\n        )\n\n        self.create_event(\n            institute=institute,\n            case=case,\n            user=user,\n            link=link,\n            category='variant',\n            verb='validate',\n            variant=variant,\n            subject=variant['display_name'],\n        )\n        return updated_variant"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmarking a variant as causative in a case.", "response": "def mark_causative(self, institute, case, user, link, variant):\n        \"\"\"Create an event for marking a variant causative.\n\n        Arguments:\n          institute (dict): A Institute object\n          case (dict): Case object\n          user (dict): A User object\n          link (str): The url to be used in the event\n          variant (variant): A variant object\n\n        Returns:\n            updated_case(dict)\n        \"\"\"\n        display_name = variant['display_name']\n        LOG.info(\"Mark variant {0} as causative in the case {1}\".format(\n            display_name, case['display_name']))\n\n        LOG.info(\"Adding variant to causatives in case {0}\".format(\n            case['display_name']))\n\n        LOG.info(\"Marking case {0} as solved\".format(\n            case['display_name']))\n\n        updated_case = self.case_collection.find_one_and_update(\n            {'_id': case['_id']},\n            {\n                '$push': {'causatives': variant['_id']},\n                '$set': {'status': 'solved'}\n            },\n            return_document=pymongo.ReturnDocument.AFTER\n        )\n\n        LOG.info(\"Creating case event for marking {0}\" \\\n                    \" causative\".format(variant['display_name']))\n\n        self.create_event(\n            institute=institute,\n            case=case,\n            user=user,\n            link=link,\n            category='case',\n            verb='mark_causative',\n            variant=variant,\n            subject=variant['display_name'],\n        )\n\n        LOG.info(\"Creating variant event for marking {0}\" \\\n                    \" causative\".format(case['display_name']))\n\n        self.create_event(\n            institute=institute,\n            case=case,\n            user=user,\n            link=link,\n            category='variant',\n            verb='mark_causative',\n            variant=variant,\n            subject=variant['display_name'],\n        )\n        return updated_case"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the manual dismiss variant entry in the database.", "response": "def update_dismiss_variant(self, institute, case, user, link, variant,\n                               dismiss_variant):\n        \"\"\"Create an event for updating the manual dismiss variant entry\n\n          This function will create a event and update the dismiss variant\n          field of the variant.\n\n        Arguments:\n            institute (dict): A Institute object\n            case (dict): Case object\n            user (dict): A User object\n            link (str): The url to be used in the event\n            variant (dict): A variant object\n            dismiss_variant (list): The new dismiss variant list\n\n        Return:\n            updated_variant\n\n        \"\"\"\n        LOG.info(\"Creating event for updating dismiss variant for \"\n                    \"variant {0}\".format(variant['display_name']))\n\n        self.create_event(\n            institute=institute,\n            case=case,\n            user=user,\n            link=link,\n            category='variant',\n            verb='dismiss_variant',\n            variant=variant,\n            subject=variant['display_name'],\n        )\n\n        if dismiss_variant:\n            LOG.info(\"Setting dismiss variant to {0} for variant {1}\"\n                        .format(dismiss_variant, variant['display_name']))\n            action = '$set'\n        else:\n            LOG.info(\"Reset dismiss variant from {0} for variant {1}\"\n                        .format(variant['dismiss_variant'], variant['display_name']))\n            action = '$unset'\n\n        updated_variant = self.variant_collection.find_one_and_update(\n            {'_id': variant['_id']},\n            {action: {'dismiss_variant': dismiss_variant}},\n            return_document=pymongo.ReturnDocument.AFTER\n        )\n        LOG.debug(\"Variant updated\")\n        return updated_variant"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate an event for updating the ACMG classification of a variant.", "response": "def update_acmg(self, institute_obj, case_obj, user_obj, link, variant_obj, acmg_str):\n        \"\"\"Create an event for updating the ACMG classification of a variant.\n\n        Arguments:\n            institute_obj (dict): A Institute object\n            case_obj (dict): Case object\n            user_obj (dict): A User object\n            link (str): The url to be used in the event\n            variant_obj (dict): A variant object\n            acmg_str (str): The new ACMG classification string\n\n        Returns:\n            updated_variant\n        \"\"\"\n        self.create_event(\n            institute=institute_obj,\n            case=case_obj,\n            user=user_obj,\n            link=link,\n            category='variant',\n            verb='acmg',\n            variant=variant_obj,\n            subject=variant_obj['display_name'],\n        )\n        LOG.info(\"Setting ACMG to {} for: {}\".format(acmg_str, variant_obj['display_name']))\n\n        if acmg_str is None:\n            updated_variant = self.variant_collection.find_one_and_update(\n                {'_id': variant_obj['_id']},\n                {'$unset': {'acmg_classification': 1}},\n                return_document=pymongo.ReturnDocument.AFTER\n            )\n        else:\n            updated_variant = self.variant_collection.find_one_and_update(\n                {'_id': variant_obj['_id']},\n                {'$set': {'acmg_classification': REV_ACMG_MAP[acmg_str]}},\n                return_document=pymongo.ReturnDocument.AFTER\n            )\n\n        LOG.debug(\"Variant updated\")\n        return updated_variant"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconstruct the relevant ids for a variant in the cache", "response": "def parse_ids(chrom, pos, ref, alt, case_id, variant_type):\n    \"\"\"Construct the necessary ids for a variant\n\n    Args:\n        chrom(str): Variant chromosome\n        pos(int): Variant position\n        ref(str): Variant reference\n        alt(str): Variant alternative\n        case_id(str): Unique case id\n        variant_type(str): 'clinical' or 'research'\n\n    Returns:\n        ids(dict): Dictionary with the relevant ids\n    \"\"\"\n    ids = {}\n    pos = str(pos)\n\n    ids['simple_id'] = parse_simple_id(chrom, pos, ref, alt)\n    ids['variant_id'] = parse_variant_id(chrom, pos, ref, alt, variant_type)\n    ids['display_name'] = parse_display_name(chrom, pos, ref, alt, variant_type)\n    ids['document_id'] = parse_document_id(chrom, pos, ref, alt, variant_type, case_id)\n\n    return ids"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the simple id for a variant in a non - unique variant set.", "response": "def parse_simple_id(chrom, pos, ref, alt):\n    \"\"\"Parse the simple id for a variant\n\n    Simple id is used as a human readable reference for a position, it is\n    in no way unique.\n\n    Args:\n        chrom(str)\n        pos(str)\n        ref(str)\n        alt(str)\n\n    Returns:\n        simple_id(str): The simple human readable variant id\n    \"\"\"\n    return '_'.join([chrom, pos, ref, alt])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_variant_id(chrom, pos, ref, alt, variant_type):\n    return generate_md5_key([chrom, pos, ref, alt, variant_type])", "response": "Parse the variant id for a variant in a specific type of base class"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_display_name(chrom, pos, ref, alt, variant_type):\n    return '_'.join([chrom, pos, ref, alt, variant_type])", "response": "Parse the display name for a variant in the scout log"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing the unique document id for a variant.", "response": "def parse_document_id(chrom, pos, ref, alt, variant_type, case_id):\n    \"\"\"Parse the unique document id for a variant.\n\n    This will always be unique in the database.\n\n    Args:\n        chrom(str)\n        pos(str)\n        ref(str)\n        alt(str)\n        variant_type(str): 'clinical' or 'research'\n        case_id(str): unqiue family id\n\n    Returns:\n        document_id(str): The unique document id in an md5 string\n    \"\"\"\n    return generate_md5_key([chrom, pos, ref, alt, variant_type, case_id])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef convert(context, panel):\n    adapter = context.obj['adapter']\n    new_header = [\"hgnc_id\",\"hgnc_symbol\",\"disease_associated_transcripts\",\n                 \"reduced_penetrance\", \"genetic_disease_models\", \"mosaicism\",\n                 \"database_entry_version\"]\n    \n    genes = parse_genes(panel)\n    \n    adapter.add_hgnc_id(genes)\n    \n    click.echo(\"#{0}\".format('\\t'.join(new_header)))\n    for gene in genes:\n        if gene.get('hgnc_id'):\n            print_info = []\n            for head in new_header:\n                print_info.append(str(gene[head]) if gene.get(head) else '')\n            \n            click.echo('\\t'.join(print_info))", "response": "Convert a gene panel with hgnc symbols to a new one with hgnc ids."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_variantid(variant_obj, family_id):\n    new_id = parse_document_id(\n        chrom=variant_obj['chromosome'],\n        pos=str(variant_obj['position']),\n        ref=variant_obj['reference'],\n        alt=variant_obj['alternative'],\n        variant_type=variant_obj['variant_type'],\n        case_id=family_id,\n    )\n    return new_id", "response": "Create a new variant id."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfetches all cases from the backend.", "response": "def cases(self, owner=None, collaborator=None, query=None, skip_assigned=False,\n              has_causatives=False, reruns=False, finished=False,\n              research_requested=False, is_research=False, status=None,\n              phenotype_terms=False, pinned=False, cohort=False, name_query=None,\n              yield_query=False):\n        \"\"\"Fetches all cases from the backend.\n\n        Args:\n            collaborator(str): If collaborator should be considered\n            owner(str): Query cases for specified case owner only\n            query(dict): If a specific query is used\n            skip_assigned(bool)\n            has_causatives(bool)\n            reruns(bool)\n            finished(bool)\n            research_requested(bool)\n            is_research(bool)\n            status(str)\n            phenotype_terms(bool): Fetch all cases with phenotype terms\n            pinned(bool): Fetch all cases with pinned variants\n            name_query(str): Could be hpo term, HPO-group, user, part of display name,\n                             part of inds or part of synopsis\n            yield_query(bool): If true, only return mongo query dict for use in\n                                compound querying.\n\n        Returns:\n            Cases ordered by date.\n            If yield_query is True, does not pose query to db;\n                instead returns corresponding query dict\n                that can be reused in compound queries or for testing.\n        \"\"\"\n        LOG.debug(\"Fetch all cases\")\n        query = query or {}\n\n        # Prioritize when both owner and collaborator params are present\n        if collaborator and owner:\n            collaborator = None\n\n        if collaborator:\n            LOG.debug(\"Use collaborator {0}\".format(collaborator))\n            query['collaborators'] = collaborator\n\n        if owner:\n            LOG.debug(\"Use owner {0}\".format(owner))\n            query['owner'] = owner\n\n        if skip_assigned:\n            query['assignees'] = {'$exists': False}\n\n        if has_causatives:\n            query['causatives'] = {'$exists': True, '$ne': []}\n\n        if reruns:\n            query['rerun_requested'] = True\n\n        if status:\n            query['status'] = status\n\n        elif finished:\n            query['status'] = {'$in': ['solved', 'archived']}\n\n        if research_requested:\n            query['research_requested'] = True\n\n        if is_research:\n            query['is_research'] = {'$exists': True, '$eq': True}\n\n        if phenotype_terms:\n            query['phenotype_terms'] = {'$exists': True, '$ne': []}\n\n        if pinned:\n            query['suspects'] = {'$exists': True, '$ne': []}\n\n        if cohort:\n            query['cohorts'] = {'$exists': True, '$ne': []}\n\n        if name_query:\n            name_value = name_query.split(':')[-1] # capture ant value provided after query descriptor\n            users = self.user_collection.find({'name': {'$regex': name_query, '$options': 'i'}})\n            if users.count() > 0:\n                query['assignees'] = {'$in': [user['email'] for user in users]}\n            elif name_query.startswith('HP:'):\n                LOG.debug(\"HPO case query\")\n                if name_value:\n                    query['phenotype_terms.phenotype_id'] = name_query\n                else: # query for cases with no HPO terms\n                    query['$or'] = [ {'phenotype_terms' : {'$size' : 0}}, {'phenotype_terms' : {'$exists' : False}} ]\n            elif name_query.startswith('PG:'):\n                LOG.debug(\"PG case query\")\n                if name_value:\n                    phenotype_group_query = name_query.replace('PG:', 'HP:')\n                    query['phenotype_groups.phenotype_id'] = phenotype_group_query\n                else: # query for cases with no phenotype groups\n                    query['$or'] = [ {'phenotype_groups' : {'$size' : 0}}, {'phenotype_groups' : {'$exists' : False}} ]\n            elif name_query.startswith('synopsis:'):\n                if name_value:\n                    query['$text']={'$search':name_value}\n                else: # query for cases with missing synopsis\n                    query['synopsis'] = ''\n            elif name_query.startswith('cohort:'):\n                query['cohorts'] = name_value\n            elif name_query.startswith('panel:'):\n                query['panels'] = {'$elemMatch': {'panel_name': name_value,\n                                    'is_default': True }}\n            elif name_query.startswith('status:'):\n                status_query = name_query.replace('status:','')\n                query['status'] = status_query\n            elif name_query.startswith('is_research'):\n                query['is_research'] = {'$exists': True, '$eq': True}\n            else:\n                query['$or'] = [\n                    {'display_name': {'$regex': name_query}},\n                    {'individuals.display_name': {'$regex': name_query}},\n                ]\n\n        if yield_query:\n            return query\n\n        LOG.info(\"Get cases with query {0}\".format(query))\n        return self.case_collection.find(query).sort('updated_at', -1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the number of cases in the database", "response": "def nr_cases(self, institute_id=None):\n        \"\"\"Return the number of cases\n\n        This function will change when we migrate to 3.7.1\n\n        Args:\n            collaborator(str): Institute id\n\n        Returns:\n            nr_cases(int)\n        \"\"\"\n        query = {}\n\n        if institute_id:\n            query['collaborators'] = institute_id\n\n        LOG.debug(\"Fetch all cases with query {0}\".format(query))\n        nr_cases = self.case_collection.find(query).count()\n\n        return nr_cases"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates the dynamic gene list for a case", "response": "def update_dynamic_gene_list(self, case, hgnc_symbols=None, hgnc_ids=None,\n                                 phenotype_ids=None, build='37'):\n        \"\"\"Update the dynamic gene list for a case\n\n        Adds a list of dictionaries to case['dynamic_gene_list'] that looks like\n\n        {\n            hgnc_symbol: str,\n            hgnc_id: int,\n            description: str\n        }\n\n        Arguments:\n            case (dict): The case that should be updated\n            hgnc_symbols (iterable): A list of hgnc_symbols\n            hgnc_ids (iterable): A list of hgnc_ids\n\n        Returns:\n            updated_case(dict)\n        \"\"\"\n        dynamic_gene_list = []\n        res = []\n        if hgnc_ids:\n            LOG.info(\"Fetching genes by hgnc id\")\n            res = self.hgnc_collection.find({'hgnc_id': {'$in': hgnc_ids}, 'build': build})\n        elif hgnc_symbols:\n            LOG.info(\"Fetching genes by hgnc symbols\")\n            res = []\n            for symbol in hgnc_symbols:\n                for gene_obj in self.gene_by_alias(symbol=symbol, build=build):\n                    res.append(gene_obj)\n\n        for gene_obj in res:\n            dynamic_gene_list.append(\n                {\n                    'hgnc_symbol': gene_obj['hgnc_symbol'],\n                    'hgnc_id': gene_obj['hgnc_id'],\n                    'description': gene_obj['description'],\n                }\n            )\n\n        LOG.info(\"Update dynamic gene panel for: %s\", case['display_name'])\n        updated_case = self.case_collection.find_one_and_update(\n            {'_id': case['_id']},\n            {'$set': {'dynamic_gene_list': dynamic_gene_list,\n                      'dynamic_panel_phenotypes': phenotype_ids or []}},\n            return_document=pymongo.ReturnDocument.AFTER\n        )\n        LOG.debug(\"Case updated\")\n        return updated_case"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfetches a single case from database. Use either the _id or combination of institute_id and display_name.", "response": "def case(self, case_id=None, institute_id=None, display_name=None):\n        \"\"\"Fetches a single case from database\n\n        Use either the _id or combination of institute_id and display_name\n\n        Args:\n            case_id(str): _id for a caes\n            institute_id(str):\n            display_name(str)\n\n        Yields:\n            A single Case\n        \"\"\"\n        query = {}\n        if case_id:\n            query['_id'] = case_id\n            LOG.info(\"Fetching case %s\", case_id)\n        else:\n            if not (institute_id and display_name):\n                raise ValueError(\"Have to provide both institute_id and display_name\")\n            LOG.info(\"Fetching case %s institute %s\", display_name, institute_id)\n            query['owner'] = institute_id\n            query['display_name'] = display_name\n\n        return self.case_collection.find_one(query)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes a single case from database.", "response": "def delete_case(self, case_id=None, institute_id=None, display_name=None):\n        \"\"\"Delete a single case from database\n\n        Args:\n            institute_id(str)\n            case_id(str)\n\n        Returns:\n            case_obj(dict): The case that was deleted\n        \"\"\"\n        query = {}\n        if case_id:\n            query['_id'] = case_id\n            LOG.info(\"Deleting case %s\", case_id)\n        else:\n            if not (institute_id and display_name):\n                raise ValueError(\"Have to provide both institute_id and display_name\")\n            LOG.info(\"Deleting case %s institute %s\", display_name, institute_id)\n            query['owner'] = institute_id\n            query['display_name'] = display_name\n\n        result = self.case_collection.delete_one(query)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads a case into the database.", "response": "def load_case(self, config_data, update=False):\n        \"\"\"Load a case into the database\n\n        Check if the owner and the institute exists.\n\n        Args:\n            config_data(dict): A dictionary with all the necessary information\n            update(bool): If existing case should be updated\n\n        Returns:\n            case_obj(dict)\n        \"\"\"\n        # Check that the owner exists in the database\n        institute_obj = self.institute(config_data['owner'])\n        if not institute_obj:\n            raise IntegrityError(\"Institute '%s' does not exist in database\" % config_data['owner'])\n\n        # Parse the case information\n        parsed_case = parse_case(config=config_data)\n        # Build the case object\n        case_obj = build_case(parsed_case, self)\n        # Check if case exists with old case id\n        old_caseid = '-'.join([case_obj['owner'], case_obj['display_name']])\n        old_case = self.case(old_caseid)\n        if old_case:\n            LOG.info(\"Update case id for existing case: %s -> %s\", old_caseid, case_obj['_id'])\n            self.update_caseid(old_case, case_obj['_id'])\n            update = True\n\n        # Check if case exists in database\n        existing_case = self.case(case_obj['_id'])\n        if existing_case and not update:\n            raise IntegrityError(\"Case %s already exists in database\" % case_obj['_id'])\n\n        files = [\n            {'file_name': 'vcf_snv', 'variant_type': 'clinical', 'category': 'snv'},\n            {'file_name': 'vcf_sv', 'variant_type': 'clinical', 'category': 'sv'},\n            {'file_name': 'vcf_cancer', 'variant_type': 'clinical', 'category': 'cancer'},\n            {'file_name': 'vcf_str', 'variant_type': 'clinical', 'category': 'str'}\n        ]\n\n        try:\n            for vcf_file in files:\n                # Check if file exists\n                if not case_obj['vcf_files'].get(vcf_file['file_name']):\n                    LOG.debug(\"didn't find {}, skipping\".format(vcf_file['file_name']))\n                    continue\n\n                variant_type = vcf_file['variant_type']\n                category = vcf_file['category']\n                if update:\n                    self.delete_variants(\n                        case_id=case_obj['_id'],\n                        variant_type=variant_type,\n                        category=category\n                    )\n                self.load_variants(\n                    case_obj=case_obj,\n                    variant_type=variant_type,\n                    category=category,\n                    rank_threshold=case_obj.get('rank_score_threshold', 0),\n                )\n\n        except (IntegrityError, ValueError, ConfigError, KeyError) as error:\n            LOG.warning(error)\n\n        if existing_case and update:\n            self.update_case(case_obj)\n        else:\n            LOG.info('Loading case %s into database', case_obj['display_name'])\n            self._add_case(case_obj)\n\n        return case_obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a case to the database.", "response": "def _add_case(self, case_obj):\n        \"\"\"Add a case to the database\n           If the case already exists exception is raised\n\n            Args:\n                case_obj(Case)\n        \"\"\"\n        if self.case(case_obj['_id']):\n            raise IntegrityError(\"Case %s already exists in database\" % case_obj['_id'])\n\n        return self.case_collection.insert_one(case_obj)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_case(self, case_obj):\n        # Todo: rename to match the intended purpose\n\n        LOG.info(\"Updating case {0}\".format(case_obj['_id']))\n        old_case = self.case_collection.find_one(\n            {'_id': case_obj['_id']}\n        )\n        updated_case = self.case_collection.find_one_and_update(\n            {'_id': case_obj['_id']},\n            {\n                '$addToSet': {\n                    'collaborators': {'$each': case_obj['collaborators']},\n                    'analyses': {\n                        'date': old_case['analysis_date'],\n                        'delivery_report': old_case.get('delivery_report')\n                    }\n                },\n                '$set': {\n                    'analysis_date': case_obj['analysis_date'],\n                    'delivery_report': case_obj.get('delivery_report'),\n                    'individuals': case_obj['individuals'],\n                    'updated_at': datetime.datetime.now(),\n                    'rerun_requested': False,\n                    'panels': case_obj.get('panels', []),\n                    'genome_build': case_obj.get('genome_build', '37'),\n                    'genome_version': case_obj.get('genome_version'),\n                    'rank_model_version': case_obj.get('rank_model_version'),\n                    'madeline_info': case_obj.get('madeline_info'),\n                    'vcf_files': case_obj.get('vcf_files'),\n                    'has_svvariants': case_obj.get('has_svvariants'),\n                    'has_strvariants': case_obj.get('has_strvariants'),\n                    'is_research': case_obj.get('is_research', False),\n                    'research_requested': case_obj.get('research_requested', False),\n                    'multiqc': case_obj.get('multiqc'),\n                    'mme_submission': case_obj.get('mme_submission'),\n                }\n            },\n            return_document=pymongo.ReturnDocument.AFTER\n        )\n\n        LOG.info(\"Case updated\")\n        return updated_case", "response": "Update a case in the database"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef replace_case(self, case_obj):\n        # Todo: Figure out and describe when this method destroys a case if invoked instead of\n        # update_case\n        LOG.info(\"Saving case %s\", case_obj['_id'])\n        # update updated_at of case to \"today\"\n\n        case_obj['updated_at'] = datetime.datetime.now(),\n\n        updated_case = self.case_collection.find_one_and_replace(\n            {'_id': case_obj['_id']},\n            case_obj,\n            return_document=pymongo.ReturnDocument.AFTER\n        )\n\n        return updated_case", "response": "Replace a existing case with a new one."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the case id for a given case across the database.", "response": "def update_caseid(self, case_obj, family_id):\n        \"\"\"Update case id for a case across the database.\n\n        This function is used when a case is a rerun or updated for another reason.\n\n        Args:\n            case_obj(dict)\n            family_id(str): The new family id\n\n        Returns:\n            new_case(dict): The updated case object\n\n        \"\"\"\n        new_case = deepcopy(case_obj)\n        new_case['_id'] = family_id\n\n        # update suspects and causatives\n        for case_variants in ['suspects', 'causatives']:\n            new_variantids = []\n            for variant_id in case_obj.get(case_variants, []):\n                case_variant = self.variant(variant_id)\n                if not case_variant:\n                    continue\n                new_variantid = get_variantid(case_variant, family_id)\n                new_variantids.append(new_variantid)\n            new_case[case_variants] = new_variantids\n\n        # update ACMG\n        for acmg_obj in self.acmg_collection.find({'case_id': case_obj['_id']}):\n            LOG.info(\"update ACMG classification: %s\", acmg_obj['classification'])\n            acmg_variant = self.variant(acmg_obj['variant_specific'])\n            new_specific_id = get_variantid(acmg_variant, family_id)\n            self.acmg_collection.find_one_and_update(\n                {'_id': acmg_obj['_id']},\n                {'$set': {'case_id': family_id, 'variant_specific': new_specific_id}},\n            )\n\n        # update events\n        institute_obj = self.institute(case_obj['owner'])\n        for event_obj in self.events(institute_obj, case=case_obj):\n            LOG.info(\"update event: %s\", event_obj['verb'])\n            self.event_collection.find_one_and_update(\n                {'_id': event_obj['_id']},\n                {'$set': {'case': family_id}},\n            )\n\n        # insert the updated case\n        self.case_collection.insert_one(new_case)\n        # delete the old case\n        self.case_collection.find_one_and_delete({'_id': case_obj['_id']})\n        return new_case"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef submit_evaluation(self, variant_obj, user_obj, institute_obj, case_obj, link, criteria):\n        variant_specific = variant_obj['_id']\n        variant_id = variant_obj['variant_id']\n        user_id = user_obj['_id']\n        user_name = user_obj.get('name', user_obj['_id'])\n        institute_id = institute_obj['_id']\n        case_id = case_obj['_id']\n\n        evaluation_terms = [evluation_info['term'] for evluation_info in criteria]\n\n        classification = get_acmg(evaluation_terms)\n\n        evaluation_obj = build_evaluation(\n            variant_specific=variant_specific,\n            variant_id=variant_id,\n            user_id=user_id,\n            user_name=user_name,\n            institute_id=institute_id,\n            case_id=case_id,\n            classification=classification,\n            criteria=criteria\n        )\n\n        self._load_evaluation(evaluation_obj)\n\n        # Update the acmg classification for the variant:\n        self.update_acmg(institute_obj, case_obj, user_obj, link, variant_obj, classification)\n        return classification", "response": "Submit an evaluation to the database."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_evaluations(self, variant_obj):\n        query = dict(variant_id=variant_obj['variant_id'])\n        res = self.acmg_collection.find(query).sort([('created_at', pymongo.DESCENDING)])\n        return res", "response": "Returns all evaluations for a certain variant."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_transcripts(transcript_lines):\n    LOG.info(\"Parsing transcripts\")\n    # Parse the transcripts, we need to check if it is a request or a file handle\n    if isinstance(transcript_lines, DataFrame):\n        transcripts = parse_ensembl_transcript_request(transcript_lines)\n    else:\n        transcripts = parse_ensembl_transcripts(transcript_lines)\n\n    # Since there can be multiple lines with information about the same transcript\n    # we store transcript information in a dictionary for now\n    parsed_transcripts = {}\n    # Loop over the parsed transcripts\n    for tx in transcripts:\n        tx_id = tx['ensembl_transcript_id']\n        ens_gene_id = tx['ensembl_gene_id']\n\n        # Check if the transcript has been added\n        # If not, create a new transcript\n        if not tx_id in parsed_transcripts:\n            tx_info = {\n                'chrom': tx['chrom'],\n                'transcript_start': tx['transcript_start'],\n                'transcript_end': tx['transcript_end'],\n                'mrna': set(),\n                'mrna_predicted': set(),\n                'nc_rna': set(),\n                'ensembl_gene_id': ens_gene_id,\n                'ensembl_transcript_id': tx_id,\n            }\n            parsed_transcripts[tx_id] = tx_info\n        \n        tx_info = parsed_transcripts[tx_id]\n        # Add the ref seq information\n        if tx.get('refseq_mrna_predicted'):\n            tx_info['mrna_predicted'].add(tx['refseq_mrna_predicted'])\n        if tx.get('refseq_mrna'):\n            tx_info['mrna'].add(tx['refseq_mrna'])\n        if tx.get('refseq_ncrna'):\n            tx_info['nc_rna'].add(tx['refseq_ncrna'])\n\n    return parsed_transcripts", "response": "Parse and massage the transcript information of the ensembl transcripts into a dictionary of the ensembl transcript - level information."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_ensembl_gene_request(result):\n    LOG.info(\"Parsing genes from request\")\n\n    for index, row in result.iterrows():\n        # print(index, row)\n        ensembl_info = {}\n\n        # Pandas represents missing data with nan which is a float\n        if type(row['hgnc_symbol']) is float:\n            # Skip genes without hgnc information\n            continue\n\n        ensembl_info['chrom'] = row['chromosome_name']\n        ensembl_info['gene_start'] = int(row['start_position'])\n        ensembl_info['gene_end'] = int(row['end_position'])\n        ensembl_info['ensembl_gene_id'] = row['ensembl_gene_id']\n        ensembl_info['hgnc_symbol'] = row['hgnc_symbol']\n\n        hgnc_id = row['hgnc_id']\n\n        if type(hgnc_id) is float:\n            hgnc_id = int(hgnc_id)\n        else:\n            hgnc_id = int(hgnc_id.split(':')[-1])\n\n        ensembl_info['hgnc_id'] = hgnc_id\n\n        yield ensembl_info", "response": "Parse a dataframe with ensembl gene information and return a list of gene_info dicts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a dataframe with ensembl transcript information", "response": "def parse_ensembl_transcript_request(result):\n    \"\"\"Parse a dataframe with ensembl transcript information\n\n    Args:\n        res(pandas.DataFrame)\n\n    Yields:\n        transcript_info(dict)\n    \"\"\"\n    LOG.info(\"Parsing transcripts from request\")\n\n    keys = [\n        'chrom',\n        'ensembl_gene_id',\n        'ensembl_transcript_id',\n        'transcript_start',\n        'transcript_end',\n        'refseq_mrna',\n        'refseq_mrna_predicted',\n        'refseq_ncrna',\n    ]\n    # for res in result.itertuples():\n    for index, row in result.iterrows():\n        ensembl_info = {}\n\n        ensembl_info['chrom'] = str(row['chromosome_name'])\n        ensembl_info['ensembl_gene_id'] = row['ensembl_gene_id']\n        ensembl_info['ensembl_transcript_id'] = row['ensembl_transcript_id']\n\n        ensembl_info['transcript_start'] = int(row['transcript_start'])\n        ensembl_info['transcript_end'] = int(row['transcript_end'])\n\n        # Check if refseq data is annotated\n        # Pandas represent missing data with nan\n        for key in keys[-3:]:\n            if type(row[key]) is float:\n                ensembl_info[key] = None\n            else:\n                ensembl_info[key] = row[key]\n        yield ensembl_info"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses an ensembl line into a dictionary with the relevant info", "response": "def parse_ensembl_line(line, header):\n    \"\"\"Parse an ensembl formated line\n\n        Args:\n            line(list): A list with ensembl gene info\n            header(list): A list with the header info\n\n        Returns:\n            ensembl_info(dict): A dictionary with the relevant info\n    \"\"\"\n    line = line.rstrip().split('\\t')\n    header = [head.lower() for head in header]\n    raw_info = dict(zip(header, line))\n\n    ensembl_info = {}\n\n    for word in raw_info:\n        value = raw_info[word]\n\n        if not value:\n            continue\n\n        if 'chromosome' in word:\n            ensembl_info['chrom'] = value\n\n        if 'gene' in word:\n            if 'id' in word:\n                ensembl_info['ensembl_gene_id'] = value\n            elif 'start' in word:\n                ensembl_info['gene_start'] = int(value)\n            elif 'end' in word:\n                ensembl_info['gene_end'] = int(value)\n\n        if 'hgnc symbol' in word:\n            ensembl_info['hgnc_symbol'] = value\n        if \"gene name\" in word:\n            ensembl_info['hgnc_symbol'] = value\n\n        if 'hgnc id' in word:\n            ensembl_info['hgnc_id'] = int(value.split(':')[-1])\n\n        if 'transcript' in word:\n            if 'id' in word:\n                ensembl_info['ensembl_transcript_id'] = value\n            elif 'start' in word:\n                ensembl_info['transcript_start'] = int(value)\n            elif 'end' in word:\n                ensembl_info['transcript_end'] = int(value)\n\n        if 'exon' in word:\n            if 'start' in word:\n                ensembl_info['exon_start'] = int(value)\n            elif 'end' in word:\n                ensembl_info['exon_end'] = int(value)\n            elif 'rank' in word:\n                ensembl_info['exon_rank'] = int(value)\n\n        if 'utr' in word:\n\n            if 'start' in word:\n                if '5' in word:\n                    ensembl_info['utr_5_start'] = int(value)\n                elif '3' in word:\n                    ensembl_info['utr_3_start'] = int(value)\n            elif 'end' in word:\n                if '5' in word:\n                    ensembl_info['utr_5_end'] = int(value)\n                elif '3' in word:\n                    ensembl_info['utr_3_end'] = int(value)\n\n        if 'strand' in word:\n            ensembl_info['strand'] = int(value)\n\n        if 'refseq' in word:\n            if 'mrna' in word:\n                if 'predicted' in word:\n                    ensembl_info['refseq_mrna_predicted'] = value\n                else:\n                    ensembl_info['refseq_mrna'] = value\n\n            if 'ncrna' in word:\n                ensembl_info['refseq_ncrna'] = value\n\n    return ensembl_info"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_ensembl_genes(lines):\n    LOG.info(\"Parsing ensembl genes from file\")\n    header = []\n    for index, line in enumerate(lines):\n\n        # File allways start with a header line\n        if index == 0:\n            header = line.rstrip().split('\\t')\n            continue\n        # After that each line represents a gene\n\n        yield parse_ensembl_line(line, header)", "response": "Parse a list of lines with ensembl formated genes into a dictionary with the relevant information."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a list of lines with ensembl formated exons into a dictionary with the relevant information", "response": "def parse_ensembl_exons(lines):\n    \"\"\"Parse lines with ensembl formated exons\n\n        This is designed to take a biomart dump with exons from ensembl.\n        Check documentation for spec for download\n\n        Args:\n            lines(iterable(str)): An iterable with ensembl formated exons\n        Yields:\n            ensembl_gene(dict): A dictionary with the relevant information\n    \"\"\"\n    header = []\n    LOG.debug(\"Parsing ensembl exons...\")\n    for index, line in enumerate(lines):\n\n        # File allways start with a header line\n        if index == 0:\n            header = line.rstrip().split('\\t')\n            continue\n\n        exon_info = parse_ensembl_line(line, header)\n        chrom = exon_info['chrom']\n        start = exon_info['exon_start']\n        end = exon_info['exon_end']\n        transcript = exon_info['ensembl_transcript_id']\n        gene = exon_info['ensembl_gene_id']\n\n        rank = exon_info['exon_rank']\n        strand = exon_info['strand']\n\n        # Recalculate start and stop (taking UTR regions into account for end exons)\n        if strand == 1:\n            # highest position: start of exon or end of 5' UTR\n            # If no 5' UTR make sure exon_start is allways choosen\n            start = max(start, exon_info.get('utr_5_end') or -1)\n            # lowest position: end of exon or start of 3' UTR\n            end = min(end, exon_info.get('utr_3_start') or float('inf'))\n        elif strand == -1:\n            # highest position: start of exon or end of 3' UTR\n            start = max(start, exon_info.get('utr_3_end') or -1)\n            # lowest position: end of exon or start of 5' UTR\n            end = min(end, exon_info.get('utr_5_start') or float('inf'))\n\n        exon_id = \"-\".join([chrom, str(start), str(end)])\n\n        if start > end:\n            raise ValueError(\"ERROR: %s\" % exon_id)\n        data = {\n            \"exon_id\": exon_id,\n            \"chrom\": chrom,\n            \"start\": start,\n            \"end\": end,\n            \"transcript\": transcript,\n            \"gene\": gene,\n            \"rank\": rank,\n        }\n\n        yield data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_ensembl_exon_request(result):\n    keys = [\n        'chrom',\n        'gene',\n        'transcript',\n        'exon_id',\n        'exon_chrom_start',\n        'exon_chrom_end',\n        '5_utr_start',\n        '5_utr_end',\n        '3_utr_start',\n        '3_utr_end',\n        'strand',\n        'rank'\n    ]\n\n    # for res in result.itertuples():\n    for res in zip(result['Chromosome/scaffold name'],\n                   result['Gene stable ID'],\n                   result['Transcript stable ID'],\n                   result['Exon stable ID'],\n                   result['Exon region start (bp)'],\n                   result['Exon region end (bp)'],\n                   result[\"5' UTR start\"],\n                   result[\"5' UTR end\"],\n                   result[\"3' UTR start\"],\n                   result[\"3' UTR end\"],\n                   result[\"Strand\"],\n                   result[\"Exon rank in transcript\"]):\n        ensembl_info = dict(zip(keys, res))\n\n        # Recalculate start and stop (taking UTR regions into account for end exons)\n        if ensembl_info['strand'] == 1:\n            # highest position: start of exon or end of 5' UTR\n            # If no 5' UTR make sure exon_start is allways choosen\n            start = max(ensembl_info['exon_chrom_start'], ensembl_info['5_utr_end'] or -1)\n            # lowest position: end of exon or start of 3' UTR\n            end = min(ensembl_info['exon_chrom_end'], ensembl_info['3_utr_start'] or float('inf'))\n        elif ensembl_info['strand'] == -1:\n            # highest position: start of exon or end of 3' UTR\n            start = max(ensembl_info['exon_chrom_start'], ensembl_info['3_utr_end'] or -1)\n            # lowest position: end of exon or start of 5' UTR\n            end = min(ensembl_info['exon_chrom_end'], ensembl_info['5_utr_start'] or float('inf'))\n\n        ensembl_info['start'] = start\n        ensembl_info['end'] = end\n\n        yield ensembl_info", "response": "Parse a dataframe with ensembl exon information"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef init_log(logger, filename=None, loglevel=None):\n    template = '[%(asctime)s] %(levelname)-8s: %(name)-25s: %(message)s'\n    formatter = logging.Formatter(template)\n\n    if loglevel:\n        logger.setLevel(getattr(logging, loglevel))\n\n    # We will always print warnings and higher to stderr\n    console = logging.StreamHandler()\n    console.setLevel('WARNING')\n    console.setFormatter(formatter)\n\n    if filename:\n        file_handler = logging.FileHandler(filename, encoding='utf-8')\n        if loglevel:\n            file_handler.setLevel(getattr(logging, loglevel))\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n    # If no logfile is provided we print all log messages that the user has\n    # defined to stderr\n    else:\n        if loglevel:\n            console.setLevel(getattr(logging, loglevel))\n\n    logger.addHandler(console)", "response": "Initializes the log file in the proper format."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the file called mim2gene returning a list of dictionaries each containing the entry type and gene ID.", "response": "def parse_mim2gene(lines):\n    \"\"\"Parse the file called mim2gene\n    \n    This file describes what type(s) the different mim numbers have.\n    The different entry types are: 'gene', 'gene/phenotype', 'moved/removed',\n    'phenotype', 'predominantly phenotypes'\n    Where:\n        gene: Is a gene entry\n        gene/phenotype: This entry describes both a phenotype and a gene\n        moved/removed: No explanation needed\n        phenotype: Describes a phenotype\n        predominantly phenotype: Not clearly established (probably phenotype)\n    \n    Args:\n        lines(iterable(str)): The mim2gene lines\n    \n    Yields:\n        parsed_entry(dict)\n    \n        {\n            \"mim_number\": int, \n            \"entry_type\": str, \n            \"entrez_gene_id\": int, \n            \"hgnc_symbol\": str, \n            \"ensembl_gene_id\": str,\n            \"ensembl_transcript_id\": str,\n        }\n    \n    \"\"\"\n    LOG.info(\"Parsing mim2gene\")\n    header = [\"mim_number\", \"entry_type\", \"entrez_gene_id\", \"hgnc_symbol\", \"ensembl_gene_id\"]\n    for i, line in enumerate(lines):\n        if line.startswith('#'):\n            continue\n        \n        if not len(line) > 0:\n            continue\n\n        line = line.rstrip()\n        parsed_entry = parse_omim_line(line, header)\n        parsed_entry['mim_number'] = int(parsed_entry['mim_number'])\n        parsed_entry['raw'] = line\n        \n        if 'hgnc_symbol' in parsed_entry:\n            parsed_entry['hgnc_symbol'] = parsed_entry['hgnc_symbol']\n        \n        if parsed_entry.get('entrez_gene_id'):\n            parsed_entry['entrez_gene_id'] = int(parsed_entry['entrez_gene_id'])\n        \n        if parsed_entry.get('ensembl_gene_id'):\n            ensembl_info = parsed_entry['ensembl_gene_id'].split(',')\n            parsed_entry['ensembl_gene_id'] = ensembl_info[0].strip()\n            if len(ensembl_info) > 1:\n                parsed_entry['ensembl_transcript_id'] = ensembl_info[1].strip()\n        \n        yield parsed_entry"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_mim_titles(lines):\n    header = ['prefix', 'mim_number', 'preferred_title', 'alternative_title', 'included_title']\n    for i,line in enumerate(lines):\n        line = line.rstrip()\n        if not line.startswith('#'):\n            parsed_entry = parse_omim_line(line, header)\n            parsed_entry['mim_number'] = int(parsed_entry['mim_number'])\n            parsed_entry['preferred_title'] = parsed_entry['preferred_title'].split(';')[0]\n            yield parsed_entry", "response": "Parse the mimTitles. txt file\n    \n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a dictionary with genes and their omim information.", "response": "def get_mim_genes(genemap_lines, mim2gene_lines):\n    \"\"\"Get a dictionary with genes and their omim information\n    \n    Args:\n        genemap_lines(iterable(str))\n        mim2gene_lines(iterable(str))\n    \n    Returns.\n        hgnc_genes(dict): A dictionary with hgnc_symbol as keys\n    \n    \"\"\"\n    LOG.info(\"Get the mim genes\")\n    \n    genes = {}\n    hgnc_genes = {}\n    \n    gene_nr = 0\n    no_hgnc = 0\n    \n    for entry in parse_mim2gene(mim2gene_lines):\n        if 'gene' in entry['entry_type']:\n            mim_nr = entry['mim_number']\n            gene_nr += 1\n            if not 'hgnc_symbol' in entry:\n                no_hgnc += 1\n            else:\n                genes[mim_nr] = entry\n    \n    LOG.info(\"Number of genes without hgnc symbol %s\", str(no_hgnc))\n    \n    for entry in parse_genemap2(genemap_lines):\n        mim_number = entry['mim_number']\n        inheritance = entry['inheritance']\n        phenotype_info = entry['phenotypes']\n        hgnc_symbol = entry['hgnc_symbol']\n        hgnc_symbols = entry['hgnc_symbols']\n        if mim_number in genes:\n            genes[mim_number]['inheritance'] = inheritance\n            genes[mim_number]['phenotypes'] = phenotype_info\n            genes[mim_number]['hgnc_symbols'] = hgnc_symbols\n\n    for mim_nr in genes:\n        gene_info = genes[mim_nr]\n        hgnc_symbol = gene_info['hgnc_symbol']\n        \n        if hgnc_symbol in hgnc_genes:\n            existing_info = hgnc_genes[hgnc_symbol]\n            if not existing_info['phenotypes']:\n                hgnc_genes[hgnc_symbol] = gene_info\n            \n        else:\n            hgnc_genes[hgnc_symbol] = gene_info\n    \n    return hgnc_genes"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a dictionary with phenotype numbers as keys and phenotype information as values.", "response": "def get_mim_phenotypes(genemap_lines):\n    \"\"\"Get a dictionary with phenotypes\n    \n    Use the mim numbers for phenotypes as keys and phenotype information as \n    values.\n\n    Args:\n        genemap_lines(iterable(str))\n    \n    Returns:\n        phenotypes_found(dict): A dictionary with mim_numbers as keys and \n        dictionaries with phenotype information as values.\n    \n        {\n             'description': str, # Description of the phenotype\n             'hgnc_symbols': set(), # Associated hgnc symbols\n             'inheritance': set(),  # Associated phenotypes\n             'mim_number': int, # mim number of phenotype\n        }\n    \"\"\"\n    # Set with all omim numbers that are phenotypes\n    # Parsed from mim2gene.txt\n    phenotype_mims = set()\n    \n    phenotypes_found = {}\n\n    # Genemap is a file with one entry per gene.\n    # Each line hold a lot of information and in specific it\n    # has information about the phenotypes that a gene is associated with\n    # From this source we collect inheritane patterns and what hgnc symbols\n    # a phenotype is associated with\n    for entry in parse_genemap2(genemap_lines):\n        hgnc_symbol = entry['hgnc_symbol']\n        for phenotype in entry['phenotypes']:\n            mim_nr = phenotype['mim_number']\n            if mim_nr in phenotypes_found:\n                phenotype_entry = phenotypes_found[mim_nr]\n                phenotype_entry['inheritance'] = phenotype_entry['inheritance'].union(phenotype['inheritance'])\n                phenotype_entry['hgnc_symbols'].add(hgnc_symbol)\n            else:\n                phenotype['hgnc_symbols'] = set([hgnc_symbol])\n                phenotypes_found[mim_nr] = phenotype\n\n    return phenotypes_found"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the omim files", "response": "def cli(context, morbid, genemap, mim2gene, mim_titles, phenotypes):\n    \"\"\"Parse the omim files\"\"\"\n    # if not (morbid and genemap and mim2gene, mim_titles):\n    #     print(\"Please provide all files\")\n    #     context.abort()\n\n    from scout.utils.handle import get_file_handle\n    from pprint import pprint as pp\n    \n    print(\"Morbid file: %s\" % morbid)\n    print(\"Genemap file: %s\" % genemap)\n    print(\"mim2gene file: %s\" % mim2gene)\n    print(\"MimTitles file: %s\" % mim_titles)\n    \n    \n    if morbid:\n        morbid_handle = get_file_handle(morbid)\n    if genemap:\n        genemap_handle = get_file_handle(genemap)\n    if mim2gene:\n        mim2gene_handle = get_file_handle(mim2gene)\n    if mim_titles:\n        mimtitles_handle = get_file_handle(mim_titles)\n\n    \n    mim_genes = get_mim_genes(genemap_handle, mim2gene_handle)\n    for entry in mim_genes: \n        if entry == 'C10orf11':\n            pp(mim_genes[entry])\n    \n    context.abort()\n    if phenotypes:\n        if not genemap:\n            click.echo(\"Please provide the genemap file\")\n            context.abort()\n        phenotypes = get_mim_phenotypes(genemap_handle)\n        for i,mim_term in enumerate(phenotypes):\n            # pp(phenotypes[mim_term])\n            pass\n    \n    print(\"Number of phenotypes found: %s\" % i)\n    \n    context.abort()\n    # hgnc_genes = get_mim_genes(genemap_handle, mim2gene_handle)\n    # for hgnc_symbol in hgnc_genes:\n    #     pp(hgnc_genes[hgnc_symbol])\n    # phenotypes = get_mim_phenotypes(genemap_handle, mim2gene_handle, mimtitles_handle)\n    # for mim_nr in phenotypes:\n    #     pp(phenotypes[mim_nr])\n    \n    genes = get_mim_genes(genemap_handle, mim2gene_handle)\n    for hgnc_symbol in genes:\n        if hgnc_symbol == 'OPA1':\n            print(genes[hgnc_symbol])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convert_number(string):\n    res = None\n    if isint(string):\n        res = int(string)\n    elif isfloat(string):\n        res = float(string) \n    return res", "response": "Convert a string to number if int convert to int otherwise float"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates a single case in the database.", "response": "def case(context, case_id, case_name, institute, collaborator, vcf, vcf_sv,\n         vcf_cancer, vcf_research, vcf_sv_research, vcf_cancer_research, peddy_ped,\n         reupload_sv, rankscore_treshold, rankmodel_version):\n    \"\"\"\n    Update a case in the database\n    \"\"\"\n    adapter = context.obj['adapter']\n    if not case_id:\n        if not (case_name and institute):\n            LOG.info(\"Please specify which case to update.\")\n            context.abort\n        case_id = \"{0}-{1}\".format(institute, case_name)\n    # Check if the case exists\n    case_obj = adapter.case(case_id)\n    \n    if not case_obj:\n        LOG.warning(\"Case %s could not be found\", case_id)\n        context.abort()\n    \n    case_changed = False\n    if collaborator:\n        if not adapter.institute(collaborator):\n            LOG.warning(\"Institute %s could not be found\", collaborator)\n            context.abort()\n        if not collaborator in case_obj['collaborators']:\n            case_changed = True\n            case_obj['collaborators'].append(collaborator)\n            LOG.info(\"Adding collaborator %s\", collaborator)\n    \n    if vcf:\n        LOG.info(\"Updating 'vcf_snv' to %s\", vcf)\n        case_obj['vcf_files']['vcf_snv'] = vcf\n        case_changed = True\n    if vcf_sv:\n        LOG.info(\"Updating 'vcf_sv' to %s\", vcf_sv)\n        case_obj['vcf_files']['vcf_sv'] = vcf_sv\n        case_changed = True\n    if vcf_cancer:\n        LOG.info(\"Updating 'vcf_cancer' to %s\", vcf_cancer)\n        case_obj['vcf_files']['vcf_cancer'] = vcf_cancer\n        case_changed = True\n    if vcf_research:\n        LOG.info(\"Updating 'vcf_research' to %s\", vcf_research)\n        case_obj['vcf_files']['vcf_research'] = vcf_research\n        case_changed = True\n    if vcf_sv_research:\n        LOG.info(\"Updating 'vcf_sv_research' to %s\", vcf_sv_research)\n        case_obj['vcf_files']['vcf_sv_research'] = vcf_sv_research\n        case_changed = True\n    if vcf_cancer_research:\n        LOG.info(\"Updating 'vcf_cancer_research' to %s\", vcf_cancer_research)\n        case_obj['vcf_files']['vcf_cancer_research'] = vcf_cancer_research\n        case_changed = True\n    \n    if case_changed:\n        adapter.update_case(case_obj)\n    \n    if reupload_sv:\n        LOG.info(\"Set needs_check to True for case %s\", case_id)\n        updates = {'needs_check': True}\n        if rankscore_treshold:\n            updates['sv_rank_model_version'] = rankmodel_version\n        if vcf_sv:\n            updates['vcf_files.vcf_sv'] = vcf_sv\n        if vcf_sv:\n            updates['vcf_files.vcf_sv_research'] = vcf_sv_research\n            \n        updated_case = adapter.case_collection.find_one_and_update(\n            {'_id':case_id}, \n            {'$set': updates\n            },\n            return_document=pymongo.ReturnDocument.AFTER\n        )\n        rankscore_treshold = rankscore_treshold or updated_case.get(\"rank_score_threshold\", 5)\n        # Delete and reload the clinical SV variants\n        if updated_case['vcf_files'].get('vcf_sv'):\n            adapter.delete_variants(case_id, variant_type='clinical', category='sv')\n            adapter.load_variants(updated_case, variant_type='clinical', \n                                  category='sv', rank_threshold=rankscore_treshold)\n        # Delete and reload research SV variants\n        if updated_case['vcf_files'].get('vcf_sv_research'):\n            adapter.delete_variants(case_id, variant_type='research', category='sv')\n            if updated_case.get('is_research'):\n                adapter.load_variants(updated_case, variant_type='research', \n                                      category='sv', rank_threshold=rankscore_treshold)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nexporting all transcripts from the database", "response": "def export_transcripts(adapter, build='37'):\n    \"\"\"Export all transcripts from the database\n    \n    Args:\n        adapter(scout.adapter.MongoAdapter)\n        build(str)\n    \n    Yields:\n        transcript(scout.models.Transcript)\n    \"\"\"\n    LOG.info(\"Exporting all transcripts\")\n\n    for tx_obj in adapter.transcripts(build=build):\n        yield tx_obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a formatted month as a table.", "response": "def formatmonth(self, theyear, themonth, withyear=True, net=None, qs=None, template='happenings/partials/calendar/month_table.html'):\n        \"\"\"Return a formatted month as a table.\"\"\"\n        context = self.get_context()\n        context['month_start_date'] = date(self.yr, self.mo, 1)\n        context['week_rows'] = []\n        for week in self.monthdays2calendar(theyear, themonth):\n            week_row = []\n            for day, weekday in week:\n                week_row.append(self.formatday(day, weekday))\n            context['week_rows'].append(week_row)\n\n        nxt, prev = get_next_and_prev(net)\n        extra_qs = ('&' + '&'.join(qs)) if qs else ''\n        context['prev_qs'] = mark_safe('?cal_prev=%d%s' % (prev, extra_qs))\n        context['next_qs'] = mark_safe('?cal_next=%d%s' % (nxt, extra_qs))\n        context['withyear'] = withyear\n        return render_to_string(template, context)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a day as a table cell.", "response": "def formatday(\n            self, day, weekday,\n            day_template='happenings/partials/calendar/day_cell.html',\n            noday_template='happenings/partials/calendar/day_noday_cell.html',\n            popover_template='happenings/partials/calendar/popover.html',\n            ):\n        \"\"\"Return a day as a table cell.\"\"\"\n        super(EventCalendar, self).formatday(day, weekday)\n        now = get_now()\n        context = self.get_context()\n        context['events'] = []\n        context['day'] = day\n        context['day_url'] = self.get_day_url(day)\n        context['month_start_date'] = date(self.yr, self.mo, 1)\n        context['weekday'] = weekday\n        context['cssclass'] = self.cssclasses[weekday]\n        context['popover_template'] = popover_template\n        context['num_events'] = len(self.count.get(day, [])),\n        try:\n            processed_date = date(self.yr, self.mo, day)\n        except ValueError:\n            # day is out of range for month\n            processed_date = None\n\n        context['month_start_date'] = date(self.yr, self.mo, 1)\n\n        if day == 0:\n            template = noday_template\n        else:\n            template = day_template\n\n        if now.date() == processed_date:\n            context['is_current_day'] = True\n\n        if processed_date and (day in self.count):\n            for item in self.count[day]:\n                self.pk = item[1]\n                self.title = item[0]\n                for event in self.events:\n                    if event.pk == self.pk:\n                        event.check_if_cancelled(processed_date)\n                        # allow to use event.last_check_if_cancelled and populate event.title.extra\n\n                        context['events'].append(event)\n\n        return render_to_string(template, context)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a day as a table cell.", "response": "def formatday(self, day, weekday):\n        \"\"\"Return a day as a table cell.\"\"\"\n        return super(MiniEventCalendar, self).formatday(\n            day, weekday,\n            day_template='happenings/partials/calendar/mini_day_cell.html',\n            popover_template='happenings/partials/calendar/mini_popover.html',\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset some commonly used variables.", "response": "def formatday(self, day, weekday):\n        \"\"\"Set some commonly used variables.\"\"\"\n        self.wkday_not_today = '<td class=\"%s\"><div class=\"td-inner\">' % (\n            self.cssclasses[weekday])\n\n        self.wkday_today = (\n            '<td class=\"%s calendar-today\"><div class=\"td-inner\">' % (\n                self.cssclasses[weekday])\n        )\n        if URLS_NAMESPACE:\n            url_name = '%s:day_list' % (URLS_NAMESPACE)\n        else:\n            url_name = 'day_list'\n\n        self.day_url = reverse(url_name, args=(self.yr, self.mo, day))\n        self.day = day\n\n        self.anch = '<a href=\"%s\">%d</a>' % (\n            self.day_url, day\n        )\n\n        self.end = '</div></td>'"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef formatmonthname(self, theyear, themonth, withyear=True):\n        display_month = month_name[themonth]\n\n        if isinstance(display_month, six.binary_type) and self.encoding:\n            display_month = display_month.decode(self.encoding)\n\n        if withyear:\n            s = u'%s %s' % (display_month, theyear)\n        else:\n            s = u'%s' % display_month\n        return ('<tr><th colspan=\"5\" class=\"month\">'\n                '<button id=\"cal-today-btn\" class=\"btn btn-small\">'\n                'Today</button> %s</th></tr>' % s)", "response": "Return a month name as a table row."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npopulating variables used to build popovers.", "response": "def popover_helper(self):\n        \"\"\"Populate variables used to build popovers.\"\"\"\n        # when\n        display_month = month_name[self.mo]\n\n        if isinstance(display_month, six.binary_type) and self.encoding:\n            display_month = display_month.decode('utf-8')\n\n        self.when = ('<p><b>When:</b> ' + display_month + ' ' +\n                     str(self.day) + ', ' + self.event.l_start_date.strftime(\n                         LEGACY_CALENDAR_TIME_FORMAT).lstrip('0') + ' - ' +\n                     self.event.l_end_date.strftime(LEGACY_CALENDAR_TIME_FORMAT).lstrip('0') +\n                     '</p>')\n\n        if self.event.location.exists():  # where\n            self.where = '<p><b>Where:</b> '\n            for l in self.event.location.all():\n                self.where += l.name\n            self.where += '</p>'\n        else:\n            self.where = ''\n\n        # description\n        self.desc = '<p><b>Description:</b> ' + self.event.description[:100]\n        self.desc += ('...</p>' if len(self.event.description) > 100\n                      else '</p>')\n\n        self.event_url = self.event.get_absolute_url()  # url\n        t = LEGACY_CALENDAR_TIME_FORMAT if self.event.l_start_date.minute else LEGACY_CALENDAR_HOUR_FORMAT\n        self.title2 = (self.event.l_start_date.strftime(t).lstrip('0') +\n                       ' ' + self.title)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef formatday(self, day, weekday):\n        super(EventCalendar, self).formatday(day, weekday)\n        now = get_now()\n        self.day = day\n        out = ''\n\n        if day == 0:\n            return '<td class=\"noday\">&nbsp;</td>'  # day outside month\n        elif now.month == self.mo and now.year == self.yr and day == now.day:\n            if day in self.count:\n                # don't return just yet\n                out = self.wkday_today + self.anch\n            else:\n                return self.wkday_today + self.anch + self.end\n        elif day in self.count:\n            # don't return just yet\n            out = self.wkday_not_today + self.anch\n        else:\n            return self.wkday_not_today + self.anch + self.end\n\n        detail = \"%s%s%s<br><a href='%s'>View details</a>\"\n        extras = ('<div title=\"%s\" data-content=\"%s\" data-container=\"body\"'\n                  ' data-toggle=\"popover\" class=\"calendar-event\"%s>')\n        common = ' style=background:%s;color:%s;'\n\n        # inject style and extras into calendar html\n        for item in self.count[day]:\n            self.pk = item[1]\n            self.title = item[0]\n            for event in self.events:\n                if event.pk == self.pk:\n                    self.event = event\n                    self.check_if_cancelled()\n                    # self.add_occurrence\n                    self.popover_helper()\n                    bg, fnt = self.event.get_colors()\n            out += ('<a class=\"event-anch\" href=\"' + self.event_url + '\">' +\n                    extras % (\n                        self.title,\n                        detail % (\n                            self.when, self.where, self.desc, self.event_url\n                        ),\n                        common % (bg, fnt)\n                    ) +\n                    self.title2 + '</div></a>')\n\n        return out + self.end", "response": "Return a day as a table cell."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a day as a table cell.", "response": "def formatday(self, day, weekday):\n        \"\"\"Return a day as a table cell.\"\"\"\n        super(MiniEventCalendar, self).formatday(day, weekday)\n        now = get_now()\n        self.day = day\n\n        if day == 0:\n            return '<td class=\"noday\">&nbsp;</td>'  # day outside month\n        elif now.month == self.mo and now.year == self.yr and day == now.day:\n            if day in self.count:\n                self.popover_helper()\n                return self.wkday_today + self.anch + self.cal_event + self.end\n            else:\n                return self.wkday_today + self.anch + self.end\n        elif day in self.count:\n            self.popover_helper()\n            return self.wkday_not_today + self.anch + self.cal_event + self.end\n        else:\n            return self.wkday_not_today + self.anch + self.end"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_panel_info(panel_lines=None, panel_id=None, institute=None, version=None, date=None,\n                   display_name=None):\n    \"\"\"Parse metadata for a gene panel\n\n    For historical reasons it is possible to include all information about a gene panel in the\n    header of a panel file. This function parses the header.\n\n    Args:\n        panel_lines(iterable(str))\n\n    Returns:\n        panel_info(dict): Dictionary with panel information\n    \"\"\"\n    panel_info = {\n        'panel_id': panel_id,\n        'institute': institute,\n        'version': version,\n        'date': date,\n        'display_name': display_name,\n    }\n\n    if panel_lines:\n        for line in panel_lines:\n            line = line.rstrip()\n            if not line.startswith('##'):\n                break\n        \n            info = line[2:].split('=')\n            field = info[0]\n            value = info[1]\n        \n        \n            if not panel_info.get(field):\n                panel_info[field] = value\n\n    panel_info['date'] = get_date(panel_info['date'])\n\n    return panel_info", "response": "Parse the panel metadata for a gene panel."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a gene line with information from a panel file containing a set of related objects.", "response": "def parse_gene(gene_info):\n    \"\"\"Parse a gene line with information from a panel file\n\n        Args:\n            gene_info(dict): dictionary with gene info\n\n        Returns:\n            gene(dict): A dictionary with the gene information\n                {\n                'hgnc_id': int,\n                'hgnc_symbol': str,\n                'disease_associated_transcripts': list(str),\n                'inheritance_models': list(str),\n                'mosaicism': bool,\n                'reduced_penetrance': bool,\n                'database_entry_version': str,\n                }\n\n    \"\"\"\n    gene = {}\n    # This is either hgnc id or hgnc symbol\n    identifier = None\n\n    hgnc_id = None\n    try:\n        if 'hgnc_id' in gene_info:\n            hgnc_id = int(gene_info['hgnc_id'])\n        elif 'hgnc_idnumber' in gene_info:\n            hgnc_id = int(gene_info['hgnc_idnumber'])\n        elif 'hgncid' in gene_info:\n            hgnc_id = int(gene_info['hgncid'])\n    except ValueError as e:\n        raise SyntaxError(\"Invalid hgnc id: {0}\".format(hgnc_id))\n\n    gene['hgnc_id'] = hgnc_id\n    identifier = hgnc_id\n\n    hgnc_symbol = None\n    if 'hgnc_symbol' in gene_info:\n        hgnc_symbol = gene_info['hgnc_symbol']\n    elif 'hgncsymbol' in gene_info:\n        hgnc_symbol = gene_info['hgncsymbol']\n    elif 'symbol' in gene_info:\n        hgnc_symbol = gene_info['symbol']\n\n    gene['hgnc_symbol'] = hgnc_symbol\n\n    if not identifier:\n        if hgnc_symbol:\n            identifier = hgnc_symbol\n        else:\n            raise SyntaxError(\"No gene identifier could be found\")\n    gene['identifier'] = identifier\n    # Disease associated transcripts is a ','-separated list of\n    # manually curated transcripts\n    transcripts = \"\"\n    if 'disease_associated_transcripts' in gene_info:\n        transcripts = gene_info['disease_associated_transcripts']\n    elif 'disease_associated_transcript' in gene_info:\n        transcripts = gene_info['disease_associated_transcript']\n    elif 'transcripts' in gene_info:\n        transcripts = gene_info['transcripts']\n\n    gene['transcripts'] = [\n            transcript.strip() for transcript in\n            transcripts.split(',') if transcript\n        ]\n\n    # Genetic disease models is a ','-separated list of manually curated\n    # inheritance patterns that are followed for a gene\n    models = \"\"\n    if 'genetic_disease_models' in gene_info:\n        models = gene_info['genetic_disease_models']\n    elif 'genetic_disease_model' in gene_info:\n        models = gene_info['genetic_disease_model']\n    elif 'inheritance_models' in gene_info:\n        models = gene_info['inheritance_models']\n    elif 'genetic_inheritance_models' in gene_info:\n        models = gene_info['genetic_inheritance_models']\n\n    gene['inheritance_models'] = [\n        model.strip() for model in models.split(',')\n        if model.strip() in VALID_MODELS\n    ]\n\n    # If a gene is known to be associated with mosaicism this is annotated\n    gene['mosaicism'] = True if gene_info.get('mosaicism') else False\n\n    # If a gene is known to have reduced penetrance this is annotated\n    gene['reduced_penetrance'] = True if gene_info.get('reduced_penetrance') else False\n\n    # The database entry version is a way to track when a a gene was added or\n    # modified, optional\n    gene['database_entry_version'] = gene_info.get('database_entry_version')\n\n    return gene"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_genes(gene_lines):\n    genes = []\n    header = []\n    hgnc_identifiers = set()\n    delimiter = '\\t'\n    # This can be '\\t' or ';'\n    delimiters = ['\\t', ' ', ';']\n\n    # There are files that have '#' to indicate headers\n    # There are some files that start with a header line without\n    # any special symbol\n    for i,line in enumerate(gene_lines):\n        line = line.rstrip()\n        if not len(line) > 0:\n            continue\n        if line.startswith('#'):\n            if not line.startswith('##'):\n                # We need to try delimiters\n                # We prefer ';' or '\\t' byt should accept ' '\n                line_length = 0\n                delimiter = None\n                for alt in delimiters:\n                    head_line = line.split(alt)\n                    if len(head_line) > line_length:\n                        line_length = len(head_line)\n                        delimiter = alt\n\n                header = [word.lower() for word in line[1:].split(delimiter)]\n        else:\n            # If no header symbol(#) assume first line is header\n            if i == 0:\n                line_length = 0\n                for alt in delimiters:\n                    head_line = line.split(alt)\n                    if len(head_line) > line_length:\n                        line_length = len(head_line)\n                        delimiter = alt\n                \n                if ('hgnc' in line or 'HGNC' in line):\n                    header = [word.lower() for word in line.split(delimiter)]\n                    continue\n                # If first line is not a header try to sniff what the first\n                # columns holds\n                if line.split(delimiter)[0].isdigit():\n                    header = ['hgnc_id']\n                else:\n                    header = ['hgnc_symbol']\n\n            splitted_line = line.split(delimiter)\n            gene_info = dict(zip(header, splitted_line))\n\n            # There are cases when excel exports empty lines that looks like\n            # ;;;;;;;. This is a exception to handle these\n            info_found = False\n            for key in gene_info:\n                if gene_info[key]:\n                    info_found = True\n                    break\n            # If no info was found we skip that line\n            if not info_found:\n                continue\n\n            try:\n                gene = parse_gene(gene_info)\n            except Exception as e:\n                LOG.warning(e)\n                raise SyntaxError(\"Line {0} is malformed\".format(i + 1))\n\n            identifier = gene.pop('identifier')\n\n            if not identifier in hgnc_identifiers:\n                hgnc_identifiers.add(identifier)\n                genes.append(gene)\n\n    return genes", "response": "Parse a file with genes and return the hgnc ids"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the panel info and return a dictionary of the correct format", "response": "def parse_gene_panel(path, institute='cust000', panel_id='test', panel_type='clinical', date=datetime.now(), \n                     version=1.0, display_name=None, genes = None):\n    \"\"\"Parse the panel info and return a gene panel\n\n        Args:\n            path(str): Path to panel file\n            institute(str): Name of institute that owns the panel\n            panel_id(str): Panel id\n            date(datetime.datetime): Date of creation\n            version(float)\n            full_name(str): Option to have a long name\n\n        Returns:\n            gene_panel(dict)\n    \"\"\"\n    LOG.info(\"Parsing gene panel %s\", panel_id)\n    gene_panel = {}\n\n    gene_panel['path'] = path\n    gene_panel['type'] = panel_type\n    gene_panel['date'] = date\n    gene_panel['panel_id'] = panel_id\n    gene_panel['institute'] = institute\n    version = version or 1.0\n    gene_panel['version'] = float(version)\n    gene_panel['display_name'] = display_name or panel_id\n\n    if not path:\n        panel_handle = genes\n    else:\n        panel_handle = get_file_handle(gene_panel['path'])\n    gene_panel['genes'] = parse_genes(gene_lines=panel_handle)\n\n    return gene_panel"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a panel app gene into a set of dicts.", "response": "def parse_panel_app_gene(app_gene, hgnc_map):\n    \"\"\"Parse a panel app formated gene\n    \n    Args:\n        app_gene(dict): Dict with panel app info\n        hgnc_map(dict): Map from hgnc_symbol to hgnc_id\n    \n    Returns:\n        gene_info(dict): Scout infromation\n    \"\"\"\n    gene_info = {}\n    confidence_level = app_gene['LevelOfConfidence']\n    # Return empty gene if not confident gene\n    if not confidence_level == 'HighEvidence':\n        return gene_info\n    \n    hgnc_symbol = app_gene['GeneSymbol']\n    # Returns a set of hgnc ids\n    hgnc_ids = get_correct_ids(hgnc_symbol, hgnc_map)\n    if not hgnc_ids:\n        LOG.warning(\"Gene %s does not exist in database. Skipping gene...\", hgnc_symbol)\n        return gene_info\n    \n    if len(hgnc_ids) > 1:\n        LOG.warning(\"Gene %s has unclear identifier. Choose random id\", hgnc_symbol)\n\n    gene_info['hgnc_symbol'] = hgnc_symbol\n    for hgnc_id in hgnc_ids:\n        gene_info['hgnc_id'] = hgnc_id\n\n    gene_info['reduced_penetrance'] = INCOMPLETE_PENETRANCE_MAP.get(app_gene['Penetrance'])\n\n    inheritance_models = []\n    for model in MODELS_MAP.get(app_gene['ModeOfInheritance'],[]):\n        inheritance_models.append(model)\n    \n    gene_info['inheritance_models'] = inheritance_models\n    \n    return gene_info"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_panel_app_panel(panel_info, hgnc_map, institute='cust000', panel_type='clinical'):\n    date_format = \"%Y-%m-%dT%H:%M:%S.%f\"\n    \n    gene_panel = {}\n    gene_panel['version'] = float(panel_info['version'])\n    gene_panel['date'] = get_date(panel_info['Created'][:-1], date_format=date_format)\n    gene_panel['display_name'] = panel_info['SpecificDiseaseName']\n    gene_panel['institute'] = institute\n    gene_panel['panel_type'] = panel_type\n    \n    LOG.info(\"Parsing panel %s\", gene_panel['display_name'])\n    \n    gene_panel['genes'] = []\n    \n    nr_low_confidence = 1\n    nr_genes = 0\n    for nr_genes, gene in enumerate(panel_info['Genes'],1):\n        gene_info = parse_panel_app_gene(gene, hgnc_map)\n        if not gene_info:\n            nr_low_confidence += 1\n            continue\n        gene_panel['genes'].append(gene_info)\n    \n    LOG.info(\"Number of genes in panel %s\", nr_genes)\n    LOG.info(\"Number of low confidence genes in panel %s\", nr_low_confidence)\n    \n    return gene_panel", "response": "Parse a PanelApp panel into a dict."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn all genes that should be included in the OMIM - AUTO panel", "response": "def get_omim_panel_genes(genemap2_lines, mim2gene_lines, alias_genes):\n    \"\"\"Return all genes that should be included in the OMIM-AUTO panel\n    Return the hgnc symbols\n    \n    Genes that have at least one 'established' or 'provisional' phenotype connection\n    are included in the gene panel\n    \n    Args:\n        genemap2_lines(iterable)\n        mim2gene_lines(iterable)\n        alias_genes(dict): A dictionary that maps hgnc_symbol to hgnc_id\n    \n    Yields:\n        hgnc_symbol(str)\n    \"\"\"\n    parsed_genes = get_mim_genes(genemap2_lines, mim2gene_lines)\n    \n    STATUS_TO_ADD = set(['established', 'provisional'])\n    \n    for hgnc_symbol in parsed_genes:\n        try:\n            gene = parsed_genes[hgnc_symbol]\n            keep = False\n            for phenotype_info in gene.get('phenotypes',[]):\n                if phenotype_info['status'] in STATUS_TO_ADD:\n                    keep = True\n                    break\n            if keep:\n                hgnc_id_info = alias_genes.get(hgnc_symbol)\n                if not hgnc_id_info:\n                    for symbol in gene.get('hgnc_symbols', []):\n                        if symbol in alias_genes:\n                            hgnc_id_info = alias_genes[symbol]\n                            break\n\n                if hgnc_id_info:\n                    yield {\n                    'hgnc_id': hgnc_id_info['true'],\n                    'hgnc_symbol': hgnc_symbol,\n                    }\n                else:\n                    LOG.warning(\"Gene symbol %s does not exist\", hgnc_symbol)\n                    \n        except KeyError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nshowing all diseases in the database", "response": "def diseases(context):\n    \"\"\"Show all diseases in the database\"\"\"\n    LOG.info(\"Running scout view diseases\")\n    adapter = context.obj['adapter']\n\n    disease_objs = adapter.disease_terms()\n\n    nr_diseases = disease_objs.count()\n    if nr_diseases == 0:\n        click.echo(\"No diseases found\")\n    else:\n        click.echo(\"Disease\")\n        for disease_obj in adapter.disease_terms():\n            click.echo(\"{0}\".format(disease_obj['_id']))\n        LOG.info(\"{0} diseases found\".format(nr_diseases))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hpo(context):\n    LOG.info(\"Running scout update hpo\")\n    adapter = context.obj['adapter']\n\n    LOG.info(\"Dropping HPO terms\")\n    adapter.hpo_term_collection.drop()\n    LOG.debug(\"HPO terms dropped\")\n\n    load_hpo_terms(adapter)", "response": "Update the HPO terms in the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_upcoming_events(self):\n        if self.event.repeats('NEVER'):\n            has_ended = False\n            now_gt_start = self.now > self.event.l_start_date\n            now_gt_end = self.now > self.event.end_date\n            if now_gt_end or now_gt_start:\n                has_ended = True\n            has_not_started = self.event.l_start_date > self.finish\n            if has_ended or has_not_started:\n                return self.events\n            self.events.append((self.event.l_start_date, self.event))\n            return self.events\n        if self.event.repeats('WEEKDAY'):\n            self._weekday()\n        elif self.event.repeats('MONTHLY'):\n            self._monthly()\n        elif self.event.repeats('YEARLY'):\n            self._yearly()\n        else:\n            self._others()\n        return self.events", "response": "Repeats an event and returns num or fewer upcoming events from now."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck start to see if we should stop collecting upcoming events.", "response": "def we_should_stop(self, start, start_):\n        \"\"\"\n        Checks 'start' to see if we should stop collecting upcoming events.\n        'start' should be a datetime.datetime, 'start_' should be the same\n        as 'start', but it should be a datetime.date to allow comparison\n        w/ end_repeat.\n        \"\"\"\n        if start > self.finish or \\\n                self.event.end_repeat is not None and \\\n                start_ > self.event.end_repeat:\n            return True\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndisplaying a list of all users and which institutes they belong to.", "response": "def users(store):\n    \"\"\"Display a list of all users and which institutes they belong to.\"\"\"\n    user_objs = list(store.users())\n    total_events = store.user_events().count()\n    for user_obj in user_objs:\n        if user_obj.get('institutes'):\n            user_obj['institutes'] = [store.institute(inst_id) for inst_id in user_obj.get('institutes')]\n        else:\n            user_obj['institutes'] = []\n        user_obj['events'] = store.user_events(user_obj).count()\n        user_obj['events_rank'] = event_rank(user_obj['events'])\n    return dict(\n        users=sorted(user_objs, key=lambda user: -user['events']),\n        total_events=total_events,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the conservation predictors for a single variant.", "response": "def parse_conservations(variant):\n    \"\"\"Parse the conservation predictors\n\n        Args:\n            variant(dict): A variant dictionary\n\n        Returns:\n            conservations(dict): A dictionary with the conservations\n    \"\"\"\n    conservations = {}\n\n    conservations['gerp'] = parse_conservation(\n                                            variant,\n                                            'dbNSFP_GERP___RS'\n                                        )\n    conservations['phast'] = parse_conservation(\n                                            variant,\n                                            'dbNSFP_phastCons100way_vertebrate'\n                                        )\n    conservations['phylop'] = parse_conservation(\n                                            variant,\n                                            'dbNSFP_phyloP100way_vertebrate'\n                                        )\n    return conservations"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_conservation(variant, info_key):\n    raw_score = variant.INFO.get(info_key)\n    conservations = []\n\n    if raw_score:\n        if isinstance(raw_score, numbers.Number):\n            raw_score = (raw_score,)\n\n        for score in raw_score:\n            if score >= CONSERVATION[info_key]['conserved_min']:\n                conservations.append('Conserved')\n            else:\n                conservations.append('NotConserved')\n\n    return conservations", "response": "Get the conservation prediction\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_next_event(event, now):\n    year = now.year\n    month = now.month\n    day = now.day\n    e_day = event[0].l_start_date.day\n    e_end_day = event[0].l_end_date.day\n    good_today = True if event[0].l_start_date.time() >= now.time() else False\n    if event[0].starts_same_year_month_as(year, month) and \\\n            e_day <= now.day <= e_end_day:\n        occurrences = CountHandler(year, month, event).get_count()\n        future_dates = (x for x in occurrences if x >= now.day)\n        day = min(future_dates, key=lambda x: abs(x - now.day))\n    else:\n        e_year = event[0].l_start_date.year\n        e_month = event[0].l_start_date.month\n        # convert to datetime.date() to be sure we can make a comparison\n        if date(e_year, e_month, e_day) > date(now.year, now.month, now.day):\n            # if the event hasn't started yet, then its next occurrence will\n            # be on its start date, so return that.\n            year = e_year\n            month = e_month\n            day = e_day\n        else:\n            occurrences = CountHandler(year, month, event).get_count()\n            future_dates = [x for x in occurrences if x >= now.day]\n            e_end_month = event[0].l_end_date.month\n            if future_dates and future_dates[0] is day and not good_today:\n                future_dates.pop(0)\n            while not future_dates:\n                month, year = inc_month(month, year)\n                if event[0].repeats('YEARLY') and \\\n                        (month != e_month or month != e_end_month):\n                    continue\n                occurrences = CountHandler(year, month, event).get_count()\n                # we don't check for now.day here, b/c we're in a month past\n                # whatever now is. As an example, if we checked for now.day\n                # we'd get stuck in an infinite loop if this were a\n                # monthly repeating event and our 'now' was on a day after the\n                # event's l_end_date.day\n                future_dates = [x for x in occurrences]\n            day = min(future_dates)\n\n    if event[0].repeats('WEEKDAY'):\n        return check_weekday(year, month, day)\n\n    return year, month, day", "response": "Returns the next occurrence of a given event relative to now."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the informations for a single instance of a base class based on the phenotype of the cases.", "response": "def get_dashboard_info(adapter, institute_id=None, slice_query=None):\n    \"\"\"Returns cases with phenotype\n\n        If phenotypes are provided search for only those\n\n    Args:\n        adapter(adapter.MongoAdapter)\n        institute_id(str): an institute _id\n        slice_query(str):  query to filter cases to obtain statistics for.\n\n    Returns:\n        data(dict): Dictionary with relevant information\n    \"\"\"\n\n    LOG.debug(\"General query with institute_id {}.\".format(institute_id))\n\n    # if institute_id == 'None' or None, all cases and general stats will be returned\n    if institute_id == 'None':\n        institute_id = None\n\n    # If a slice_query is present then numbers in \"General statistics\" and \"Case statistics\" will\n    # reflect the data available for the query\n    general_sliced_info = get_general_case_info(adapter, institute_id=institute_id,\n                                                    slice_query=slice_query)\n    total_sliced_cases = general_sliced_info['total_cases']\n\n    data = {'total_cases': total_sliced_cases}\n    if total_sliced_cases == 0:\n        return data\n\n    data['pedigree'] = []\n    for ped_info in general_sliced_info['pedigree'].values():\n        ped_info['percent'] = ped_info['count'] / total_sliced_cases\n        data['pedigree'].append(ped_info)\n\n    data['cases'] = get_case_groups(adapter, total_sliced_cases,\n                        institute_id=institute_id, slice_query=slice_query)\n\n    data['analysis_types'] = get_analysis_types(adapter, total_sliced_cases,\n                                institute_id=institute_id, slice_query=slice_query)\n\n    overview = [\n        {\n            'title': 'Phenotype terms',\n            'count': general_sliced_info['phenotype_cases'],\n            'percent': general_sliced_info['phenotype_cases'] / total_sliced_cases,\n        },\n        {\n            'title': 'Causative variants',\n            'count': general_sliced_info['causative_cases'],\n            'percent': general_sliced_info['causative_cases'] / total_sliced_cases,\n        },\n        {\n            'title': 'Pinned variants',\n            'count': general_sliced_info['pinned_cases'],\n            'percent': general_sliced_info['pinned_cases'] / total_sliced_cases,\n        },\n        {\n            'title': 'Cohort tag',\n            'count': general_sliced_info['cohort_cases'],\n            'percent': general_sliced_info['cohort_cases'] / total_sliced_cases,\n        }\n    ]\n\n    # Data from \"Variant statistics tab\" is not filtered by slice_query and numbers will\n    # reflect verified variants in all available cases for an institute\n    general_info = get_general_case_info(adapter, institute_id=institute_id)\n    total_cases = general_info['total_cases']\n    sliced_case_ids = general_sliced_info['case_ids']\n    verified_query = {\n        'verb' : 'validate',\n    }\n    if institute_id: # filter by institute if users wishes so\n        verified_query['institute'] =  institute_id\n\n    # Case level information\n    sliced_validation_cases = set()\n    sliced_validated_cases = set()\n\n    # Variant level information\n    validated_tp = set()\n    validated_fp = set()\n    var_valid_orders = 0 # use this counter to count 'True Positive', 'False positive' and 'Not validated' vars\n\n    validate_events = adapter.event_collection.find(verified_query)\n    for validate_event in list(validate_events):\n        case_id = validate_event.get('case')\n        var_obj = adapter.variant(case_id=case_id, document_id=validate_event['variant_id'])\n\n        if var_obj: # Don't take into account variants which have been removed from db\n            var_valid_orders += 1\n            if case_id in sliced_case_ids:\n                sliced_validation_cases.add(case_id) # add to the set. Can't add same id twice since it'a a set\n\n            validation = var_obj.get('validation')\n            if validation and validation in ['True positive', 'False positive']:\n                if case_id in sliced_case_ids:\n                    sliced_validated_cases.add(case_id)\n                if validation == 'True positive':\n                    validated_tp.add(var_obj['_id'])\n                elif validation == 'False positive':\n                    validated_fp.add(var_obj['_id'])\n\n    n_validation_cases = len(sliced_validation_cases)\n    n_validated_cases = len(sliced_validated_cases)\n\n    # append\n    overview.append(\n    {\n        'title': 'Validation ordered',\n        'count': n_validation_cases,\n        'percent': n_validation_cases / total_sliced_cases,\n    })\n    overview.append(\n    {\n        'title': 'Validated cases (TP + FP)',\n        'count': n_validated_cases,\n        'percent': n_validated_cases / total_sliced_cases,\n    })\n\n    data['overview'] = overview\n\n    variants = []\n    nr_validated = len(validated_tp) + len(validated_fp)\n    variants.append(\n        {\n            'title': 'Validation ordered',\n            'count': var_valid_orders,\n            'percent': 1\n        }\n    )\n\n    # taking into account that var_valid_orders might be 0:\n    percent_validated_tp = 0\n    percent_validated_fp = 0\n\n    if var_valid_orders:\n        percent_validated_tp = len(validated_tp) / var_valid_orders\n        percent_validated_fp = len(validated_fp) / var_valid_orders\n\n    variants.append(\n        {\n            'title': 'Validated True Positive',\n            'count': len(validated_tp),\n            'percent': percent_validated_tp,\n        }\n    )\n\n    variants.append(\n        {\n            'title': 'Validated False Positive',\n            'count': len(validated_fp),\n            'percent': percent_validated_fp,\n        }\n    )\n\n    data['variants'] = variants\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a dictionary of general information about the cases.", "response": "def get_general_case_info(adapter, institute_id=None, slice_query=None):\n    \"\"\"Return general information about cases\n\n    Args:\n        adapter(adapter.MongoAdapter)\n        institute_id(str)\n        slice_query(str):   Query to filter cases to obtain statistics for.\n\n\n    Returns:\n        general(dict)\n    \"\"\"\n    general = {}\n\n    # Potentially sensitive slice queries are assumed allowed if we have got this far\n    name_query = slice_query\n\n    cases = adapter.cases(owner=institute_id, name_query=name_query)\n\n    phenotype_cases = 0\n    causative_cases = 0\n    pinned_cases = 0\n    cohort_cases = 0\n\n    pedigree = {\n        1: {\n            'title': 'Single',\n            'count': 0\n        },\n        2: {\n            'title': 'Duo',\n            'count': 0\n        },\n        3: {\n            'title': 'Trio',\n            'count': 0\n        },\n        'many': {\n            'title': 'Many',\n            'count': 0\n        },\n    }\n\n    case_ids = set()\n\n    total_cases = 0\n    for total_cases,case in enumerate(cases,1):\n        # If only looking at one institute we need to save the case ids\n        if institute_id:\n            case_ids.add(case['_id'])\n        if case.get('phenotype_terms'):\n            phenotype_cases += 1\n        if case.get('causatives'):\n            causative_cases += 1\n        if case.get('suspects'):\n            pinned_cases += 1\n        if case.get('cohorts'):\n            cohort_cases += 1\n\n        nr_individuals = len(case.get('individuals',[]))\n        if nr_individuals == 0:\n            continue\n        if nr_individuals > 3:\n            pedigree['many']['count'] += 1\n        else:\n            pedigree[nr_individuals]['count'] += 1\n\n    general['total_cases'] = total_cases\n    general['phenotype_cases'] = phenotype_cases\n    general['causative_cases'] = causative_cases\n    general['pinned_cases'] = pinned_cases\n    general['cohort_cases'] = cohort_cases\n    general['pedigree'] = pedigree\n    general['case_ids'] = case_ids\n\n    return general"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_case_groups(adapter, total_cases, institute_id=None, slice_query=None):\n    # Create a group with all cases in the database\n    cases = [{'status': 'all', 'count': total_cases, 'percent': 1}]\n    # Group the cases based on their status\n    pipeline = []\n    group = {'$group' : {'_id': '$status', 'count': {'$sum': 1}}}\n\n    subquery = {}\n    if institute_id and slice_query:\n        subquery = adapter.cases(owner=institute_id, name_query=slice_query,\n                              yield_query=True)\n    elif institute_id:\n        subquery = adapter.cases(owner=institute_id, yield_query=True)\n    elif slice_query:\n        subquery = adapter.cases(name_query=slice_query, yield_query=True)\n\n    query = {'$match': subquery} if subquery else {}\n\n    if query:\n        pipeline.append(query)\n\n    pipeline.append(group)\n    res = adapter.case_collection.aggregate(pipeline)\n\n    for status_group in res:\n        cases.append({'status': status_group['_id'],\n                      'count': status_group['count'],\n                      'percent': status_group['count'] / total_cases})\n\n    return cases", "response": "Return the information about the cases in the cluster."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn information about analysis types.", "response": "def get_analysis_types(adapter, total_cases, institute_id=None, slice_query=None):\n    \"\"\" Return information about analysis types.\n        Group cases based on analysis type for the individuals.\n    Args:\n        adapter(adapter.MongoAdapter)\n        total_cases(int): Total number of cases\n        institute_id(str)\n        slice_query(str): Query to filter cases to obtain statistics for.\n    Returns:\n        analysis_types array of hashes with name: analysis_type(str), count: count(int)\n\n    \"\"\"\n    # Group cases based on analysis type of the individuals\n    query = {}\n\n    subquery = {}\n    if institute_id and slice_query:\n        subquery = adapter.cases(owner=institute_id, name_query=slice_query,\n                              yield_query=True)\n    elif institute_id:\n        subquery = adapter.cases(owner=institute_id, yield_query=True)\n    elif slice_query:\n        subquery = adapter.cases(name_query=slice_query, yield_query=True)\n\n    query = {'$match': subquery}\n\n    pipeline = []\n    if query:\n        pipeline.append(query)\n\n    pipeline.append({'$unwind': '$individuals'})\n    pipeline.append({'$group': {'_id': '$individuals.analysis_type', 'count': {'$sum': 1}}})\n    analysis_query = adapter.case_collection.aggregate(pipeline)\n    analysis_types = [{'name': group['_id'], 'count': group['count']} for group in analysis_query]\n\n    return analysis_types"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef render_to_json_response(self, context, **kwargs):\n        return HttpResponse(\n            self.convert_context_to_json(context),\n            content_type='application/json',\n            **kwargs\n        )", "response": "Returns a JSON response transforming the context to make the payload."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts the context dict to a JSON object.", "response": "def convert_context_to_json(self, context):\n        \"\"\"\n        Get what we want out of the context dict and convert that to a JSON\n        object. Note that this does no object serialization b/c we're\n        not sending any objects.\n        \"\"\"\n        if 'month/shift' in self.request.path:  # month calendar\n            return dumps(self.get_month_calendar_dict(context))\n        elif 'event-list/shift' in self.request.path:  # month event list\n            return dumps(self.get_month_event_list_dict(context))\n        elif 'cal-and-list/shift' in self.request.path:\n            cal = self.get_month_calendar_dict(context)\n            l = self.get_month_event_list_dict(context)\n            cal.update(l)\n            return dumps(cal)\n        else:  # day list view\n            for key, val in context.items():\n                if isinstance(val, Promise):\n                    context[key] = force_text(val)\n\n            return dumps(self.get_day_context_dict(context))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the year and month from the net and the querystring.", "response": "def get_year_and_month(self, net, qs, **kwargs):\n        \"\"\"\n        Get the year and month. First tries from kwargs, then from\n        querystrings. If none, or if cal_ignore qs is specified,\n        sets year and month to this year and this month.\n        \"\"\"\n        now = c.get_now()\n        year = now.year\n        month = now.month + net\n        month_orig = None\n\n        if 'cal_ignore=true' not in qs:\n            if 'year' and 'month' in self.kwargs:  # try kwargs\n                year, month_orig = map(\n                    int, (self.kwargs['year'], self.kwargs['month'])\n                )\n                month = month_orig + net\n            else:\n                try:  # try querystring\n                    year = int(self.request.GET['cal_year'])\n                    month_orig = int(self.request.GET['cal_month'])\n                    month = month_orig + net\n                except Exception:\n                    pass\n        # return the year and month, and any errors that may have occurred do\n        # to an invalid month/year being given.\n        return c.clean_year_month(year, month, month_orig)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_for_cancelled_events(self, d):\n        for event in self.events:\n            for cn in event.cancellations.all():\n                if cn.date == d:\n                    event.title += ' (CANCELLED)'", "response": "Check if any events are cancelled on the given date d."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading a HPO term into the database.", "response": "def load_hpo_term(self, hpo_obj):\n        \"\"\"Add a hpo object\n\n        Arguments:\n            hpo_obj(dict)\n\n        \"\"\"\n        LOG.debug(\"Loading hpo term %s into database\", hpo_obj['_id'])\n        try:\n            self.hpo_term_collection.insert_one(hpo_obj)\n        except DuplicateKeyError as err:\n            raise IntegrityError(\"Hpo term %s already exists in database\".format(hpo_obj['_id']))\n        LOG.debug(\"Hpo term saved\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads a list of HPO terms into the bulkwrite database", "response": "def load_hpo_bulk(self, hpo_bulk):\n        \"\"\"Add a hpo object\n\n        Arguments:\n            hpo_bulk(list(scout.models.HpoTerm))\n\n        Returns:\n            result: pymongo bulkwrite result\n\n        \"\"\"\n        LOG.debug(\"Loading hpo bulk\")\n\n        try:\n            result = self.hpo_term_collection.insert_many(hpo_bulk)\n        except (DuplicateKeyError, BulkWriteError) as err:\n            raise IntegrityError(err)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfetches a hpo term", "response": "def hpo_term(self, hpo_id):\n        \"\"\"Fetch a hpo term\n\n        Args:\n            hpo_id(str)\n\n        Returns:\n            hpo_obj(dict)\n        \"\"\"\n        LOG.debug(\"Fetching hpo term %s\", hpo_id)\n\n        return self.hpo_term_collection.find_one({'_id': hpo_id})"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsearches for all HPO terms in the database.", "response": "def hpo_terms(self, query=None, hpo_term=None, text=None, limit=None):\n        \"\"\"Return all HPO terms\n\n        If a query is sent hpo_terms will try to match with regex on term or\n        description.\n\n        Args:\n            query(str): Part of a hpoterm or description\n            hpo_term(str): Search for a specific hpo term\n            limit(int): the number of desired results\n\n        Returns:\n            result(pymongo.Cursor): A cursor with hpo terms\n        \"\"\"\n        query_dict = {}\n        search_term = None\n        if query:\n            query_dict = {'$or':\n                [\n                    {'hpo_id': {'$regex': query, '$options':'i'}},\n                    {'description': {'$regex': query, '$options':'i'}},\n                ]   \n            }\n            search_term = query\n        elif text:\n            new_string = ''\n            for i,word in enumerate(text.split(' ')):\n                if i == 0:\n                    new_string += word\n                else:\n                    new_string += ' \\\"{0}\\\"'.format(word)\n            LOG.info(\"Search HPO terms with %s\", new_string)\n            query_dict['$text'] = {'$search': new_string}\n            search_term = text\n        elif hpo_term:\n            query_dict['hpo_id'] = hpo_term\n            search_term = hpo_term\n\n        limit = limit or int(10e10)\n        res = self.hpo_term_collection.find(query_dict).limit(limit).sort('hpo_number',ASCENDING)\n        \n\n        LOG.info(\"Found {0} terms with search word {1}\".format(res.count(), search_term))\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a disease term object based on the identifier", "response": "def disease_term(self, disease_identifier):\n        \"\"\"Return a disease term\n\n        Checks if the identifier is a disease number or a id\n\n        Args:\n            disease_identifier(str)\n\n        Returns:\n            disease_obj(dict)\n        \"\"\"\n        query = {}\n        try:\n            disease_identifier = int(disease_identifier)\n            query['disease_nr'] = disease_identifier\n        except ValueError:\n            query['_id'] = disease_identifier\n\n        return self.disease_term_collection.find_one(query)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning all disease terms that overlaps a gene", "response": "def disease_terms(self, hgnc_id=None):\n        \"\"\"Return all disease terms that overlaps a gene\n\n            If no gene, return all disease terms\n\n        Args:\n            hgnc_id(int)\n\n        Returns:\n            iterable(dict): A list with all disease terms that match\n        \"\"\"\n        query = {}\n        if hgnc_id:\n            LOG.debug(\"Fetching all diseases for gene %s\", hgnc_id)\n            query['genes'] = hgnc_id\n        else:\n            LOG.info(\"Fetching all disease terms\")\n\n        return list(self.disease_term_collection.find(query))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading a disease term into the database.", "response": "def load_disease_term(self, disease_obj):\n        \"\"\"Load a disease term into the database\n\n        Args:\n            disease_obj(dict)\n        \"\"\"\n        LOG.debug(\"Loading disease term %s into database\", disease_obj['_id'])\n        try:\n            self.disease_term_collection.insert_one(disease_obj)\n        except DuplicateKeyError as err:\n            raise IntegrityError(\"Disease term %s already exists in database\".format(disease_obj['_id']))\n\n        LOG.debug(\"Disease term saved\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate_hpo_gene_list(self, *hpo_terms):\n        genes = {}\n        for term in hpo_terms:\n            hpo_obj = self.hpo_term(term)\n            if hpo_obj:\n                for hgnc_id in hpo_obj['genes']:\n                    if hgnc_id in genes:\n                        genes[hgnc_id] += 1\n                    else:\n                        genes[hgnc_id] = 1\n            else:\n                LOG.warning(\"Term %s could not be found\", term)\n\n        sorted_genes = sorted(genes.items(), key=operator.itemgetter(1), reverse=True)\n        return sorted_genes", "response": "Generate a sorted list of namedtuples of hpogenes\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cmd_tool(args=None):\n\n    from argparse import ArgumentParser\n\n    parser = ArgumentParser(description=\"Command line utility for reading and plotting filterbank files.\")\n\n    parser.add_argument('-p', action='store',  default='ank', dest='what_to_plot', type=str,\n                        help='Show: \"w\" waterfall (freq vs. time) plot; \"s\" integrated spectrum plot; \\\n                        \"t\" for time series; \"mm\" for spectrum including min max; \"k\" for kurtosis; \\\n                        \"a\" for all available plots and information; and \"ank\" for all but kurtosis.')\n    parser.add_argument('filename', type=str,\n                        help='Name of file to read')\n    parser.add_argument('-b', action='store', default=None, dest='f_start', type=float,\n                        help='Start frequency (begin), in MHz')\n    parser.add_argument('-e', action='store', default=None, dest='f_stop', type=float,\n                        help='Stop frequency (end), in MHz')\n    parser.add_argument('-B', action='store', default=None, dest='t_start', type=int,\n                        help='Start integration (begin) ID')\n    parser.add_argument('-E', action='store', default=None, dest='t_stop', type=int,\n                        help='Stop integration (end) ID')\n    parser.add_argument('-i', action='store_true', default=False, dest='info_only',\n                        help='Show info only')\n    parser.add_argument('-a', action='store_true', default=False, dest='average',\n                       help='average along time axis (plot spectrum only)')\n    parser.add_argument('-s', action='store', default='', dest='plt_filename', type=str,\n                       help='save plot graphic to file (give filename as argument)')\n    parser.add_argument('-S', action='store_true', default=False, dest='save_only',\n                       help='Turn off plotting of data and only save to file.')\n    parser.add_argument('-D', action='store_false', default=True, dest='blank_dc',\n                       help='Use to not blank DC bin.')\n    parser.add_argument('-c', action='store_true', default=False, dest='calibrate_band_pass',\n                        help='Calibrate band pass.')\n    args = parser.parse_args()\n\n    # Open blimpy data\n    filename = args.filename\n    load_data = not args.info_only\n\n    # only load one integration if looking at spectrum\n    wtp = args.what_to_plot\n    if not wtp or 's' in wtp:\n        if args.t_start == None:\n            t_start = 0\n        else:\n            t_start = args.t_start\n        t_stop  = t_start + 1\n\n        if args.average:\n            t_start = None\n            t_stop  = None\n    else:\n        t_start = args.t_start\n        t_stop  = args.t_stop\n\n    if args.info_only:\n        args.blank_dc = False\n        args.calibrate_band_pass = False\n\n    fil = Filterbank(filename, f_start=args.f_start, f_stop=args.f_stop,\n                     t_start=t_start, t_stop=t_stop,\n                     load_data=load_data,blank_dc=args.blank_dc,\n                     cal_band_pass=args.calibrate_band_pass)\n    fil.info()\n\n    # And if we want to plot data, then plot data.\n    if not args.info_only:\n        # check start & stop frequencies make sense\n        #try:\n        #    if args.f_start:\n        #        print \"Start freq: %2.2f\" % args.f_start\n        #        assert args.f_start >= fil.freqs[0] or np.isclose(args.f_start, fil.freqs[0])\n        #\n        #    if args.f_stop:\n        #        print \"Stop freq: %2.2f\" % args.f_stop\n        #        assert args.f_stop <= fil.freqs[-1] or np.isclose(args.f_stop, fil.freqs[-1])\n        #except AssertionError:\n        #    print \"Error: Start and stop frequencies must lie inside file's frequency range.\"\n        #    print \"i.e. between %2.2f-%2.2f MHz.\" % (fil.freqs[0], fil.freqs[-1])\n        #    exit()\n\n        if args.what_to_plot == \"w\":\n            plt.figure(\"waterfall\", figsize=(8, 6))\n            fil.plot_waterfall(f_start=args.f_start, f_stop=args.f_stop)\n        elif args.what_to_plot == \"s\":\n            plt.figure(\"Spectrum\", figsize=(8, 6))\n            fil.plot_spectrum(logged=True, f_start=args.f_start, f_stop=args.f_stop, t='all')\n        elif args.what_to_plot == \"mm\":\n            plt.figure(\"min max\", figsize=(8, 6))\n            fil.plot_spectrum_min_max(logged=True, f_start=args.f_start, f_stop=args.f_stop, t='all')\n        elif args.what_to_plot == \"k\":\n            plt.figure(\"kurtosis\", figsize=(8, 6))\n            fil.plot_kurtosis(f_start=args.f_start, f_stop=args.f_stop)\n        elif args.what_to_plot == \"t\":\n            plt.figure(\"Time Series\", figsize=(8, 6))\n            fil.plot_time_series(f_start=args.f_start, f_stop=args.f_stop,orientation='h')\n        elif args.what_to_plot == \"a\":\n            plt.figure(\"Multiple diagnostic plots\", figsize=(12, 9),facecolor='white')\n            fil.plot_all(logged=True, f_start=args.f_start, f_stop=args.f_stop, t='all')\n        elif args.what_to_plot == \"ank\":\n            plt.figure(\"Multiple diagnostic plots\", figsize=(12, 9),facecolor='white')\n            fil.plot_all(logged=True, f_start=args.f_start, f_stop=args.f_stop, t='all',kurtosis=False)\n\n        if args.plt_filename != '':\n            plt.savefig(args.plt_filename)\n\n        if not args.save_only:\n            if 'DISPLAY' in os.environ.keys():\n                plt.show()\n            else:\n                print(\"No $DISPLAY available.\")", "response": "Command line tool for reading and plotting filterbank files."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_hdf5(self, filename, f_start=None, f_stop=None,\n                        t_start=None, t_stop=None, load_data=True):\n        \"\"\" Populate Filterbank instance with data from HDF5 file\n\n        Note:\n            This is to be deprecated in future, please use Waterfall() to open files.\n        \"\"\"\n        print(\"Warning: this function will be deprecated in the future. Please use Waterfall to open HDF5 files.\")\n#        raise DeprecationWarning('Please use Waterfall to open HDF5 files.')\n\n        self.header = {}\n        self.filename = filename\n        self.h5 = h5py.File(filename)\n        for key, val in self.h5[b'data'].attrs.items():\n            if six.PY3:\n                key = bytes(key, 'ascii')\n            if key == b'src_raj':\n                self.header[key] = Angle(val, unit='hr')\n            elif key == b'src_dej':\n                self.header[key] = Angle(val, unit='deg')\n            else:\n                self.header[key] = val\n\n        self.n_ints_in_file = self.h5[b\"data\"].shape[0]\n        i_start, i_stop, chan_start_idx, chan_stop_idx = self._setup_freqs(f_start=f_start, f_stop=f_stop)\n        ii_start, ii_stop, n_ints = self._setup_time_axis(t_start=t_start, t_stop=t_stop)\n\n        if load_data:\n            self.data = self.h5[b\"data\"][ii_start:ii_stop, :, chan_start_idx:chan_stop_idx]\n\n            self.file_size_bytes = os.path.getsize(self.filename)\n\n#         if self.header[b'foff'] < 0:\n#             self.data = self.data[..., ::-1] # Reverse data\n\n        else:\n            print(\"Skipping data load...\")\n            self.data = np.array([0])\n            self.n_ints_in_file  = 0\n            self.file_size_bytes = os.path.getsize(self.filename)", "response": "Read filterbank from HDF5 file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_filterbank(self, filename=None, f_start=None, f_stop=None,\n                        t_start=None, t_stop=None, load_data=True):\n        \"\"\" Populate Filterbank instance with data from Filterbank file\n\n        Note:\n            This is to be deprecated in future, please use Waterfall() to open files.\n        \"\"\"\n        if filename is None:\n            filename = self.filename\n        else:\n            self.filename = filename\n\n        self.header = read_header(filename)\n\n        #convert input frequencies into what their corresponding index would be\n        i_start, i_stop, chan_start_idx, chan_stop_idx = self._setup_freqs(f_start=f_start, f_stop=f_stop)\n\n        n_bits  = self.header[b'nbits']\n        n_bytes  = int(self.header[b'nbits'] / 8)\n        n_chans = self.header[b'nchans']\n        n_chans_selected = self.freqs.shape[0]\n        n_ifs   = self.header[b'nifs']\n\n        # Load binary data\n        self.idx_data = len_header(filename)\n        f = open(filename, 'rb')\n        f.seek(self.idx_data)\n        filesize = os.path.getsize(self.filename)\n        n_bytes_data = filesize - self.idx_data\n\n        # Finally add some other info to the class as objects\n        self.n_ints_in_file  = calc_n_ints_in_file(self.filename)\n        self.file_size_bytes = filesize\n\n        ## Setup time axis\n        ii_start, ii_stop, n_ints = self._setup_time_axis(t_start=t_start, t_stop=t_stop)\n\n        # Seek to first integration\n        f.seek(int(ii_start * n_bits * n_ifs * n_chans / 8), 1)\n\n        # Set up indexes used in file read (taken out of loop for speed)\n        i0 = np.min((chan_start_idx, chan_stop_idx))\n        i1 = np.max((chan_start_idx, chan_stop_idx))\n\n        #Set up the data type (taken out of loop for speed)\n        if n_bits == 2:\n            dd_type = b'uint8'\n            n_chans_selected = int(n_chans_selected/4)\n        elif n_bytes == 4:\n            dd_type = b'float32'\n        elif n_bytes == 2:\n            dd_type = b'uint16'\n        elif n_bytes == 1:\n            dd_type = b'uint8'\n\n        if load_data:\n\n            if n_ints * n_ifs * n_chans_selected > MAX_DATA_ARRAY_SIZE:\n                print(\"[Filterbank]  Error: data array is too large to load. Either select fewer points or manually increase MAX_DATA_ARRAY_SIZE. Large files are now handle with Waterfall .\")\n                sys.exit()\n\n            if n_bits == 2:\n                self.data = np.zeros((n_ints, n_ifs, n_chans_selected*4), dtype=dd_type)\n            else:\n                self.data = np.zeros((n_ints, n_ifs, n_chans_selected), dtype=dd_type)\n\n            for ii in range(n_ints):\n                \"\"\"d = f.read(n_bytes * n_chans * n_ifs)\n                \"\"\"\n\n                for jj in range(n_ifs):\n\n                    f.seek(n_bytes * i0, 1) # 1 = from current location\n                    #d = f.read(n_bytes * n_chans_selected)\n                    #bytes_to_read = n_bytes * n_chans_selected\n\n                    dd = np.fromfile(f, count=n_chans_selected, dtype=dd_type)\n\n                    # Reverse array if frequency axis is flipped\n#                     if f_delt < 0:\n#                         dd = dd[::-1]\n\n                    if n_bits == 2:\n                        dd = unpack_2to8(dd)\n                    self.data[ii, jj] = dd\n\n                    f.seek(n_bytes * (n_chans - i1), 1)  # Seek to start of next block\n        else:\n            print(\"Skipping data load...\")\n            self.data = np.array([0], dtype=dd_type)", "response": "Read a filterbank file and populate the object with data from it."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes LST for observation", "response": "def compute_lst(self):\n        \"\"\" Compute LST for observation \"\"\"\n        if self.header[b'telescope_id'] == 6:\n            self.coords = gbt_coords\n        elif self.header[b'telescope_id'] == 4:\n            self.coords = parkes_coords\n        else:\n            raise RuntimeError(\"Currently only Parkes and GBT supported\")\n        if HAS_SLALIB:\n            # dut1 = (0.2 /3600.0) * np.pi/12.0\n            dut1 = 0.0\n            mjd = self.header[b'tstart']\n            tellong = np.deg2rad(self.coords[1])\n            last = s.sla_gmst(mjd) - tellong + s.sla_eqeqx(mjd) + dut1\n            # lmst = s.sla_gmst(mjd) - tellong\n            if last < 0.0 : last = last + 2.0*np.pi\n            return last\n        else:\n            raise RuntimeError(\"This method requires pySLALIB\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the LSR in km and s .", "response": "def compute_lsrk(self):\n        \"\"\" Computes the LSR in km/s\n\n        uses the MJD, RA and DEC of observation to compute\n        along with the telescope location. Requires pyslalib\n        \"\"\"\n        ra = Angle(self.header[b'src_raj'], unit='hourangle')\n        dec = Angle(self.header[b'src_dej'], unit='degree')\n        mjdd = self.header[b'tstart']\n        rarad = ra.to('radian').value\n        dcrad = dec.to('radian').value\n        last = self.compute_lst()\n        tellat  = np.deg2rad(self.coords[0])\n        tellong = np.deg2rad(self.coords[1])\n\n        # convert star position to vector\n        starvect = s.sla_dcs2c(rarad, dcrad)\n\n        # velocity component in ra,dec due to Earth rotation\n        Rgeo = s.sla_rverot( tellat, rarad, dcrad, last)\n\n        # get Barycentric and heliocentric velocity and position of the Earth.\n        evp = s.sla_evp(mjdd, 2000.0)\n        dvb = evp[0]   # barycentric velocity vector, in AU/sec\n        dpb = evp[1]   # barycentric position vector, in AU\n        dvh = evp[2]   # heliocentric velocity vector, in AU/sec\n        dph = evp[3]   # heliocentric position vector, in AU\n\n        # dot product of vector to object and heliocentric velocity\n        # convert AU/sec to km/sec\n        vcorhelio = -s.sla_dvdv( starvect, dvh) *149.597870e6\n        vcorbary  = -s.sla_dvdv( starvect, dvb) *149.597870e6\n\n        # rvlsrd is velocity component in ra,dec direction due to the Sun's\n        # motion with respect to the \"dynamical\" local standard of rest\n        rvlsrd = s.sla_rvlsrd( rarad,dcrad)\n\n        # rvlsrk is velocity component in ra,dec direction due to i\n        # the Sun's motion w.r.t the \"kinematic\" local standard of rest\n        rvlsrk = s.sla_rvlsrk( rarad,dcrad)\n\n        # rvgalc is velocity component in ra,dec direction due to\n        #the rotation of the Galaxy.\n        rvgalc = s.sla_rvgalc( rarad,dcrad)\n        totalhelio = Rgeo + vcorhelio\n        totalbary  = Rgeo + vcorbary\n        totallsrk = totalhelio + rvlsrk\n        totalgal  = totalbary  + rvlsrd + rvgalc\n\n        return totallsrk"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nblank DC bins in coarse channels.", "response": "def blank_dc(self, n_coarse_chan):\n        \"\"\" Blank DC bins in coarse channels.\n\n        Note: currently only works if entire file is read\n        \"\"\"\n\n        if n_coarse_chan < 1:\n            logger.warning('Coarse channel number < 1, unable to blank DC bin.')\n            return None\n\n        if not n_coarse_chan % int(n_coarse_chan) == 0:\n            logger.warning('Selection does not contain an interger number of coarse channels, unable to blank DC bin.')\n            return None\n\n        n_coarse_chan = int(n_coarse_chan)\n\n        n_chan = self.data.shape[-1]\n        n_chan_per_coarse = int(n_chan / n_coarse_chan)\n\n        mid_chan = int(n_chan_per_coarse / 2)\n\n        for ii in range(n_coarse_chan):\n            ss = ii*n_chan_per_coarse\n            self.data[..., ss+mid_chan] = np.median(self.data[..., ss+mid_chan+5:ss+mid_chan+10])"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprints information about the current object.", "response": "def info(self):\n        \"\"\" Print header information \"\"\"\n\n        for key, val in self.header.items():\n            if key == b'src_raj':\n                val = val.to_string(unit=u.hour, sep=':')\n            if key == b'src_dej':\n                val = val.to_string(unit=u.deg, sep=':')\n            if key == b'tsamp':\n                val *= u.second\n            if key in ('foff', 'fch1'):\n                val *= u.MHz\n            if key == b'tstart':\n                print(\"%16s : %32s\" % (\"tstart (ISOT)\", Time(val, format='mjd').isot))\n                key = \"tstart (MJD)\"\n            print(\"%16s : %32s\" % (key, val))\n\n        print(\"\\n%16s : %32s\" % (\"Num ints in file\", self.n_ints_in_file))\n        print(\"%16s : %32s\" % (\"Data shape\", self.data.shape))\n        print(\"%16s : %32s\" % (\"Start freq (MHz)\", self.freqs[0]))\n        print(\"%16s : %32s\" % (\"Stop freq (MHz)\", self.freqs[-1]))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_freqs(self, f_start, f_stop):\n\n        fch1 = self.header[b'fch1']\n        foff = self.header[b'foff']\n\n        #convert input frequencies into what their corresponding index would be\n        i_start = int((f_start - fch1) / foff)\n        i_stop  = int((f_stop - fch1)  / foff)\n\n        #calculate closest true index value\n        chan_start_idx = np.int(i_start)\n        chan_stop_idx  = np.int(i_stop)\n\n        #create freq array\n        i_vals = np.arange(chan_stop_idx, chan_start_idx, 1)\n\n        freqs = foff * i_vals + fch1\n\n        return freqs", "response": "Generate the frequency array for the given start and stop frequencies."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the extent of the log entry.", "response": "def _calc_extent(self,plot_f=None,plot_t=None,MJD_time=False):\n        \"\"\" Setup ploting edges.\n        \"\"\"\n\n        plot_f_begin = plot_f[0]\n        plot_f_end = plot_f[-1] + (plot_f[1]-plot_f[0])\n\n        plot_t_begin = self.timestamps[0]\n        plot_t_end  = self.timestamps[-1] + (self.timestamps[1] - self.timestamps[0])\n\n        if MJD_time:\n            extent=(plot_f_begin, plot_f_begin_end, plot_t_begin, plot_t_end)\n        else:\n            extent=(plot_f_begin, plot_f_end, 0.0,(plot_t_end-plot_t_begin)*24.*60.*60)\n\n        return extent"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plot_spectrum(self, t=0, f_start=None, f_stop=None, logged=False, if_id=0, c=None, **kwargs):\n        if self.header[b'nbits'] <=2:\n            logged = False\n            t='all'\n        ax = plt.gca()\n\n        plot_f, plot_data = self.grab_data(f_start, f_stop, if_id)\n\n        #Using accending frequency for all plots.\n        if self.header[b'foff'] < 0:\n            plot_data = plot_data[..., ::-1] # Reverse data\n            plot_f = plot_f[::-1]\n\n        if isinstance(t, int):\n            print(\"extracting integration %i...\" % t)\n            plot_data = plot_data[t]\n        elif t == b'all':\n            print(\"averaging along time axis...\")\n            #Since the data has been squeezed, the axis for time goes away if only one bin, causing a bug with axis=1\n            if len(plot_data.shape) > 1:\n                plot_data = plot_data.mean(axis=0)\n            else:\n                plot_data = plot_data.mean()\n        else:\n            raise RuntimeError(\"Unknown integration %s\" % t)\n\n        # Rebin to max number of points\n        dec_fac_x = 1\n        if plot_data.shape[0] > MAX_PLT_POINTS:\n            dec_fac_x = int(plot_data.shape[0] / MAX_PLT_POINTS)\n\n        plot_data = rebin(plot_data, dec_fac_x, 1)\n        plot_f    = rebin(plot_f, dec_fac_x, 1)\n\n        if not c:\n            kwargs['c'] = '#333333'\n\n        if logged:\n            plt.plot(plot_f, db(plot_data),label='Stokes I', **kwargs)\n            plt.ylabel(\"Power [dB]\")\n        else:\n\n            plt.plot(plot_f, plot_data,label='Stokes I', **kwargs)\n            plt.ylabel(\"Power [counts]\")\n        plt.xlabel(\"Frequency [MHz]\")\n        plt.legend()\n\n        try:\n            plt.title(self.header[b'source_name'])\n        except KeyError:\n            plt.title(self.filename)\n\n        plt.xlim(plot_f[0], plot_f[-1])", "response": "Plot frequency spectrum of a given file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nplotting the frequency spectrum of a given file.", "response": "def plot_spectrum_min_max(self, t=0, f_start=None, f_stop=None, logged=False, if_id=0, c=None, **kwargs):\n        \"\"\" Plot frequency spectrum of a given file\n\n        Args:\n            logged (bool): Plot in linear (False) or dB units (True)\n            if_id (int): IF identification (if multiple IF signals in file)\n            c: color for line\n            kwargs: keyword args to be passed to matplotlib plot()\n        \"\"\"\n        ax = plt.gca()\n\n        plot_f, plot_data = self.grab_data(f_start, f_stop, if_id)\n\n        #Using accending frequency for all plots.\n        if self.header[b'foff'] < 0:\n            plot_data = plot_data[..., ::-1] # Reverse data\n            plot_f = plot_f[::-1]\n\n        fig_max = plot_data[0].max()\n        fig_min = plot_data[0].min()\n\n        print(\"averaging along time axis...\")\n\n        #Since the data has been squeezed, the axis for time goes away if only one bin, causing a bug with axis=1\n        if len(plot_data.shape) > 1:\n            plot_max = plot_data.max(axis=0)\n            plot_min = plot_data.min(axis=0)\n            plot_data = plot_data.mean(axis=0)\n        else:\n            plot_max = plot_data.max()\n            plot_min = plot_data.min()\n            plot_data = plot_data.mean()\n\n        # Rebin to max number of points\n        dec_fac_x = 1\n        MAX_PLT_POINTS = 8*64  # Low resoluition to see the difference.\n        if plot_data.shape[0] > MAX_PLT_POINTS:\n            dec_fac_x = int(plot_data.shape[0] / MAX_PLT_POINTS)\n\n        plot_data = rebin(plot_data, dec_fac_x, 1)\n        plot_min = rebin(plot_min, dec_fac_x, 1)\n        plot_max = rebin(plot_max, dec_fac_x, 1)\n        plot_f    = rebin(plot_f, dec_fac_x, 1)\n\n        if logged:\n            plt.plot(plot_f, db(plot_data), \"#333333\", label='mean', **kwargs)\n            plt.plot(plot_f, db(plot_max),  \"#e74c3c\", label='max', **kwargs)\n            plt.plot(plot_f, db(plot_min),  '#3b5b92', label='min', **kwargs)\n            plt.ylabel(\"Power [dB]\")\n        else:\n            plt.plot(plot_f, plot_data,  \"#333333\", label='mean', **kwargs)\n            plt.plot(plot_f, plot_max,   \"#e74c3c\", label='max', **kwargs)\n            plt.plot(plot_f, plot_min,   '#3b5b92', label='min', **kwargs)\n            plt.ylabel(\"Power [counts]\")\n        plt.xlabel(\"Frequency [MHz]\")\n        plt.legend()\n\n        try:\n            plt.title(self.header[b'source_name'])\n        except KeyError:\n            plt.title(self.filename)\n\n        plt.xlim(plot_f[0], plot_f[-1])\n        if logged:\n            plt.ylim(db(fig_min),db(fig_max))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nplots the waterfall of the data in the specified frequency range.", "response": "def plot_waterfall(self, f_start=None, f_stop=None, if_id=0, logged=True, cb=True, MJD_time=False, **kwargs):\n        \"\"\" Plot waterfall of data\n\n        Args:\n            f_start (float): start frequency, in MHz\n            f_stop (float): stop frequency, in MHz\n            logged (bool): Plot in linear (False) or dB units (True),\n            cb (bool): for plotting the colorbar\n            kwargs: keyword args to be passed to matplotlib imshow()\n        \"\"\"\n\n        plot_f, plot_data = self.grab_data(f_start, f_stop, if_id)\n\n        #Using accending frequency for all plots.\n        if self.header[b'foff'] < 0:\n            plot_data = plot_data[..., ::-1] # Reverse data\n            plot_f = plot_f[::-1]\n\n        if logged:\n            plot_data = db(plot_data)\n\n        # Make sure waterfall plot is under 4k*4k\n        dec_fac_x, dec_fac_y = 1, 1\n        if plot_data.shape[0] > MAX_IMSHOW_POINTS[0]:\n            dec_fac_x = int(plot_data.shape[0] / MAX_IMSHOW_POINTS[0])\n\n        if plot_data.shape[1] > MAX_IMSHOW_POINTS[1]:\n            dec_fac_y =  int(plot_data.shape[1] /  MAX_IMSHOW_POINTS[1])\n\n        plot_data = rebin(plot_data, dec_fac_x, dec_fac_y)\n\n        try:\n            plt.title(self.header[b'source_name'])\n        except KeyError:\n            plt.title(self.filename)\n\n        extent = self._calc_extent(plot_f=plot_f,plot_t=self.timestamps,MJD_time=MJD_time)\n\n        plt.imshow(plot_data,\n            aspect='auto',\n            origin='lower',\n            rasterized=True,\n            interpolation='nearest',\n            extent=extent,\n            cmap='viridis',\n            **kwargs\n        )\n        if cb:\n            plt.colorbar()\n        plt.xlabel(\"Frequency [MHz]\")\n        if MJD_time:\n            plt.ylabel(\"Time [MJD]\")\n        else:\n            plt.ylabel(\"Time [s]\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_time_series(self, f_start=None, f_stop=None, if_id=0, logged=True, orientation='h', MJD_time=False, **kwargs):\n\n        ax = plt.gca()\n        plot_f, plot_data = self.grab_data(f_start, f_stop, if_id)\n\n        if logged and self.header[b'nbits'] >= 8:\n            plot_data = db(plot_data)\n\n        #Since the data has been squeezed, the axis for time goes away if only one bin, causing a bug with axis=1\n        if len(plot_data.shape) > 1:\n            plot_data = plot_data.mean(axis=1)\n        else:\n            plot_data = plot_data.mean()\n\n        #Make proper time axis for plotting (but only for plotting!). Note that this makes the values inclusive.\n        extent = self._calc_extent(plot_f=plot_f,plot_t=self.timestamps,MJD_time=MJD_time)\n        plot_t = np.linspace(extent[2],extent[3],len(self.timestamps))\n\n        if MJD_time:\n            tlabel = \"Time [MJD]\"\n        else:\n            tlabel = \"Time [s]\"\n\n        if logged:\n            plabel = \"Power [dB]\"\n        else:\n            plabel = \"Power [counts]\"\n\n        # Reverse oder if vertical orientation.\n        if 'v' in orientation:\n            plt.plot(plot_data, plot_t, **kwargs)\n            plt.xlabel(plabel)\n\n        else:\n            plt.plot(plot_t, plot_data, **kwargs)\n            plt.xlabel(tlabel)\n            plt.ylabel(plabel)\n\n        ax.autoscale(axis='both',tight=True)", "response": "Plot the time series."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot_kurtosis(self, f_start=None, f_stop=None, if_id=0, **kwargs):\n        ax = plt.gca()\n\n        plot_f, plot_data = self.grab_data(f_start, f_stop, if_id)\n\n        #Using accending frequency for all plots.\n        if self.header[b'foff'] < 0:\n            plot_data = plot_data[..., ::-1] # Reverse data\n            plot_f = plot_f[::-1]\n\n        try:\n            plot_kurtosis = scipy.stats.kurtosis(plot_data, axis=0, nan_policy='omit')\n        except:\n            plot_kurtosis = plot_data*0.0\n\n        plt.plot(plot_f, plot_kurtosis, **kwargs)\n        plt.ylabel(\"Kurtosis\")\n        plt.xlabel(\"Frequency [MHz]\")\n\n        plt.xlim(plot_f[0], plot_f[-1])", "response": "Plots the kurtosis of the current entry in the system."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nplotting all the data for a specific time series and kurtosis.", "response": "def plot_all(self, t=0, f_start=None, f_stop=None, logged=False, if_id=0, kurtosis=True, **kwargs):\n        \"\"\" Plot waterfall of data as well as spectrum; also, placeholder to make even more complicated plots in the future.\n\n        Args:\n            f_start (float): start frequency, in MHz\n            f_stop (float): stop frequency, in MHz\n            logged (bool): Plot in linear (False) or dB units (True),\n            t (int): integration number to plot (0 -> len(data))\n            logged (bool): Plot in linear (False) or dB units (True)\n            if_id (int): IF identification (if multiple IF signals in file)\n            kwargs: keyword args to be passed to matplotlib plot() and imshow()\n        \"\"\"\n        if self.header[b'nbits'] <=2:\n            logged = False\n\n        nullfmt = NullFormatter()  # no labels\n\n        # definitions for the axes\n        left, width = 0.35, 0.5\n        bottom, height = 0.45, 0.5\n        width2, height2 = 0.1125, 0.15\n        bottom2, left2 = bottom - height2 - .025, left - width2 - .02\n        bottom3, left3 = bottom2 - height2 - .025, 0.075\n\n        rect_waterfall = [left, bottom, width, height]\n        rect_colorbar = [left + width, bottom, .025, height]\n        rect_spectrum = [left, bottom2, width, height2]\n        rect_min_max = [left, bottom3, width, height2]\n        rect_timeseries = [left + width, bottom, width2, height]\n        rect_kurtosis = [left3, bottom3, 0.25, height2]\n        rect_header = [left3 - .05, bottom, 0.2, height]\n\n        # --------\n        #         axColorbar = plt.axes(rect_colorbar)\n        #         print 'Ploting Colorbar'\n        #         print plot_data.max()\n        #         print plot_data.min()\n        #\n        #         plot_colorbar = range(plot_data.min(),plot_data.max(),int((plot_data.max()-plot_data.min())/plot_data.shape[0]))\n        #         plot_colorbar = np.array([[plot_colorbar],[plot_colorbar]])\n        #\n        #         plt.imshow(plot_colorbar,aspect='auto', rasterized=True, interpolation='nearest',)\n\n        #         axColorbar.xaxis.set_major_formatter(nullfmt)\n        #         axColorbar.yaxis.set_major_formatter(nullfmt)\n\n        #         heatmap = axColorbar.pcolor(plot_data, edgecolors = 'none', picker=True)\n        #         plt.colorbar(heatmap, cax = axColorbar)\n\n\n        # --------\n        axMinMax = plt.axes(rect_min_max)\n        print('Plotting Min Max')\n        self.plot_spectrum_min_max(logged=logged, f_start=f_start, f_stop=f_stop, t=t)\n        plt.title('')\n        axMinMax.yaxis.tick_right()\n        axMinMax.yaxis.set_label_position(\"right\")\n\n        # --------\n        axSpectrum = plt.axes(rect_spectrum,sharex=axMinMax)\n        print('Plotting Spectrum')\n        self.plot_spectrum(logged=logged, f_start=f_start, f_stop=f_stop, t=t)\n        plt.title('')\n        axSpectrum.yaxis.tick_right()\n        axSpectrum.yaxis.set_label_position(\"right\")\n        plt.xlabel('')\n#        axSpectrum.xaxis.set_major_formatter(nullfmt)\n        plt.setp(axSpectrum.get_xticklabels(), visible=False)\n\n        # --------\n        axWaterfall = plt.axes(rect_waterfall,sharex=axMinMax)\n        print('Plotting Waterfall')\n        self.plot_waterfall(f_start=f_start, f_stop=f_stop, logged=logged, cb=False)\n        plt.xlabel('')\n\n        # no labels\n#        axWaterfall.xaxis.set_major_formatter(nullfmt)\n        plt.setp(axWaterfall.get_xticklabels(), visible=False)\n\n        # --------\n        axTimeseries = plt.axes(rect_timeseries)\n        print('Plotting Timeseries')\n        self.plot_time_series(f_start=f_start, f_stop=f_stop, orientation='v')\n        axTimeseries.yaxis.set_major_formatter(nullfmt)\n#        axTimeseries.xaxis.set_major_formatter(nullfmt)\n\n        # --------\n        # Could exclude since it takes much longer to run than the other plots.\n        if kurtosis:\n            axKurtosis = plt.axes(rect_kurtosis)\n            print('Plotting Kurtosis')\n            self.plot_kurtosis(f_start=f_start, f_stop=f_stop)\n\n\n        # --------\n        axHeader = plt.axes(rect_header)\n        print('Plotting Header')\n        # Generate nicer header\n        telescopes = {0: 'Fake data',\n                      1: 'Arecibo',\n                      2: 'Ooty',\n                      3: 'Nancay',\n                      4: 'Parkes',\n                      5: 'Jodrell',\n                      6: 'GBT',\n                      8: 'Effelsberg',\n                      10: 'SRT',\n                      64: 'MeerKAT',\n                      65: 'KAT7'\n                      }\n\n        telescope = telescopes.get(self.header[b\"telescope_id\"], self.header[b\"telescope_id\"])\n\n        plot_header = \"%14s: %s\\n\" % (\"TELESCOPE_ID\", telescope)\n        for key in (b'SRC_RAJ', b'SRC_DEJ', b'TSTART', b'NCHANS', b'NBEAMS', b'NIFS', b'NBITS'):\n            try:\n                plot_header += \"%14s: %s\\n\" % (key, self.header[key.lower()])\n            except KeyError:\n                pass\n        fch1 = \"%6.6f MHz\" % self.header[b'fch1']\n\n        foff = (self.header[b'foff'] * 1e6 * u.Hz)\n        if np.abs(foff) > 1e6 * u.Hz:\n            foff = str(foff.to('MHz'))\n        elif np.abs(foff) > 1e3 * u.Hz:\n            foff = str(foff.to('kHz'))\n        else:\n            foff = str(foff.to('Hz'))\n\n        plot_header += \"%14s: %s\\n\" % (\"FCH1\", fch1)\n        plot_header += \"%14s: %s\\n\" % (\"FOFF\", foff)\n\n        plt.text(0.05, .95, plot_header, ha='left', va='top', wrap=True)\n\n        axHeader.set_facecolor('white')\n        axHeader.xaxis.set_major_formatter(nullfmt)\n        axHeader.yaxis.set_major_formatter(nullfmt)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_to_filterbank(self, filename_out):\n\n        print(\"[Filterbank] Warning: Non-standard function to write in filterbank (.fil) format. Please use Waterfall.\")\n\n        n_bytes  = int(self.header[b'nbits'] / 8)\n        with open(filename_out, \"wb\") as fileh:\n            fileh.write(generate_sigproc_header(self))\n            j = self.data\n            if n_bytes == 4:\n                np.float32(j.ravel()).tofile(fileh)\n            elif n_bytes == 2:\n                np.int16(j.ravel()).tofile(fileh)\n            elif n_bytes == 1:\n                np.int8(j.ravel()).tofile(fileh)", "response": "Write data to blimpy file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite the filterbank data to an HDF5 file.", "response": "def write_to_hdf5(self, filename_out, *args, **kwargs):\n        \"\"\" Write data to HDF5 file.\n\n        Args:\n            filename_out (str): Name of output file\n        \"\"\"\n\n        print(\"[Filterbank] Warning: Non-standard function to write in HDF5 (.h5) format. Please use Waterfall.\")\n\n        if not HAS_HDF5:\n            raise RuntimeError(\"h5py package required for HDF5 output.\")\n\n        with h5py.File(filename_out, 'w') as h5:\n\n            dset = h5.create_dataset(b'data',\n                              data=self.data,\n                              compression='lzf')\n\n            dset_mask = h5.create_dataset(b'mask',\n                                     shape=self.data.shape,\n                                     compression='lzf',\n                                     dtype='uint8')\n\n            dset.dims[0].label = b\"frequency\"\n            dset.dims[1].label = b\"feed_id\"\n            dset.dims[2].label = b\"time\"\n\n            dset_mask.dims[0].label = b\"frequency\"\n            dset_mask.dims[1].label = b\"feed_id\"\n            dset_mask.dims[2].label = b\"time\"\n\n            # Copy over header information as attributes\n            for key, value in self.header.items():\n                dset.attrs[key] = value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef calibrate_band_pass_N1(self):\n\n        band_pass = np.median(self.data.squeeze(),axis=0)\n        self.data = self.data/band_pass", "response": "Calibration for the band pass of the log - level data."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\noutputs stokes parameters I Q U V for a rawspec cross polarization filterbank file", "response": "def get_stokes(cross_dat, feedtype='l'):\n    '''Output stokes parameters (I,Q,U,V) for a rawspec\n    cross polarization filterbank file'''\n\n    #Compute Stokes Parameters\n    if feedtype=='l':\n        #I = XX+YY\n        I = cross_dat[:,0,:]+cross_dat[:,1,:]\n        #Q = XX-YY\n        Q = cross_dat[:,0,:]-cross_dat[:,1,:]\n        #U = 2*Re(XY)\n        U = 2*cross_dat[:,2,:]\n        #V = -2*Im(XY)\n        V = -2*cross_dat[:,3,:]\n\n    elif feedtype=='c':\n        #I = LL+RR\n        I = cross_dat[:,0,:]+cross_dat[:,1,:]\n        #Q = 2*Re(RL)\n        Q = 2*cross_dat[:,2,:]\n        #U = 2*Im(RL)\n        U = -2*cross_dat[:,3,:]\n        #V = RR-LL\n        V = cross_dat[:,1,:]-cross_dat[:,0,:]\n    else:\n        raise ValueError('feedtype must be \\'l\\' (linear) or \\'c\\' (circular)')\n\n    #Add middle dimension to match Filterbank format\n    I = np.expand_dims(I,axis=1)\n    Q = np.expand_dims(Q,axis=1)\n    U = np.expand_dims(U,axis=1)\n    V = np.expand_dims(V,axis=1)\n\n    #Compute linear polarization\n    #L=np.sqrt(np.square(Q)+np.square(U))\n\n    return I,Q,U,V"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a data array with length n_chans to an array of length n_coarse_chans by averaging over the coarse channels", "response": "def convert_to_coarse(data,chan_per_coarse):\n    '''\n    Converts a data array with length n_chans to an array of length n_coarse_chans\n    by averaging over the coarse channels\n    '''\n    #find number of coarse channels and reshape array\n    num_coarse = data.size/chan_per_coarse\n    data_shaped = np.array(np.reshape(data,(num_coarse,chan_per_coarse)))\n\n    #Return the average over each coarse channel\n    return np.mean(data_shaped[:,2:-1],axis=1)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef phase_offsets(Idat,Qdat,Udat,Vdat,tsamp,chan_per_coarse,feedtype='l',**kwargs):\n    '''\n    Calculates phase difference between X and Y feeds given U and V (U and Q for circular basis)\n    data from a noise diode measurement on the target\n    '''\n    #Fold noise diode data and calculate ON OFF diferences for U and V\n    if feedtype=='l':\n        U_OFF,U_ON = foldcal(Udat,tsamp,**kwargs)\n        V_OFF,V_ON = foldcal(Vdat,tsamp,**kwargs)\n        Udiff = U_ON-U_OFF\n        Vdiff = V_ON-V_OFF\n        poffset = np.arctan2(-1*Vdiff,Udiff)\n\n    if feedtype=='c':\n        U_OFF,U_ON = foldcal(Udat,tsamp,**kwargs)\n        Q_OFF,Q_ON = foldcal(Qdat,tsamp,**kwargs)\n        Udiff = U_ON-U_OFF\n        Qdiff = Q_ON-Q_OFF\n        poffset = np.arctan2(Udiff,Qdiff)\n\n    coarse_p =  convert_to_coarse(poffset,chan_per_coarse)\n\n    #Correct for problems created by discontinuity in arctan\n    #Find whether phase offsets have increasing or decreasing slope\n    y = coarse_p[:6]\n    x = np.arange(y.size)\n    m = np.polyfit(x,y,1)[0]\n\n    for i in range(coarse_p.size-3):\n        if (m>0 and coarse_p[i+1]<coarse_p[i]) or (m<0 and coarse_p[i+1]>coarse_p[i]):\n            coarse_p[i+1] = 2*coarse_p[i+2]-coarse_p[i+3]    #Move problem point near the next\n\n    return coarse_p", "response": "Calculates the phase difference between X and Y feeds given U and V and a frequency series of frequency intervals."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef gain_offsets(Idat,Qdat,Udat,Vdat,tsamp,chan_per_coarse,feedtype='l',**kwargs):\n    '''\n    Determines relative gain error in the X and Y feeds for an\n    observation given I and Q (I and V for circular basis) noise diode data.\n    '''\n    if feedtype=='l':\n        #Fold noise diode data and calculate ON OFF differences for I and Q\n        I_OFF,I_ON = foldcal(Idat,tsamp,**kwargs)\n        Q_OFF,Q_ON = foldcal(Qdat,tsamp,**kwargs)\n\n        #Calculate power in each feed for noise diode ON and OFF\n        XX_ON = (I_ON+Q_ON)/2\n        XX_OFF = (I_OFF+Q_OFF)/2\n        YY_ON = (I_ON-Q_ON)/2\n        YY_OFF = (I_OFF-Q_OFF)/2\n\n        #Calculate gain offset (divided by 2) as defined in Heiles (2001)\n        G = (XX_OFF-YY_OFF)/(XX_OFF+YY_OFF)\n\n    if feedtype=='c':\n        #Fold noise diode data and calculate ON OFF differences for I and Q\n        I_OFF,I_ON = foldcal(Idat,tsamp,**kwargs)\n        V_OFF,V_ON = foldcal(Vdat,tsamp,**kwargs)\n\n        #Calculate power in each feed for noise diode ON and OFF\n        RR_ON = (I_ON+V_ON)/2\n        RR_OFF = (I_OFF+V_OFF)/2\n        LL_ON = (I_ON-V_ON)/2\n        LL_OFF = (I_OFF-V_OFF)/2\n\n        #Calculate gain offset (divided by 2) as defined in Heiles (2001)\n        G = (RR_OFF-LL_OFF)/(RR_OFF+LL_OFF)\n\n    return convert_to_coarse(G,chan_per_coarse)", "response": "Determines relative gain error in the X and Y feeds for an anon_noise_diode observation given I Q and V for circular basis."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef apply_Mueller(I,Q,U,V, gain_offsets, phase_offsets, chan_per_coarse, feedtype='l'):\n    '''\n    Returns calibrated Stokes parameters for an observation given an array\n    of differential gains and phase differences.\n    '''\n\n    #Find shape of data arrays and calculate number of coarse channels\n    shape = I.shape\n    ax0 = I.shape[0]\n    ax1 = I.shape[1]\n    nchans = I.shape[2]\n    ncoarse = nchans/chan_per_coarse\n\n    #Reshape data arrays to separate coarse channels\n    I = np.reshape(I,(ax0,ax1,ncoarse,chan_per_coarse))\n    Q = np.reshape(Q,(ax0,ax1,ncoarse,chan_per_coarse))\n    U = np.reshape(U,(ax0,ax1,ncoarse,chan_per_coarse))\n    V = np.reshape(V,(ax0,ax1,ncoarse,chan_per_coarse))\n\n    #Swap axes 2 and 3 to in order for broadcasting to work correctly\n    I = np.swapaxes(I,2,3)\n    Q = np.swapaxes(Q,2,3)\n    U = np.swapaxes(U,2,3)\n    V = np.swapaxes(V,2,3)\n\n    #Apply top left corner of electronics chain inverse Mueller matrix\n    a = 1/(1-gain_offsets**2)\n    if feedtype=='l':\n        Icorr = a*(I-gain_offsets*Q)\n        Qcorr = a*(-1*gain_offsets*I+Q)\n        I = None\n        Q = None\n    if feedtype=='c':\n        Icorr = a*(I-gain_offsets*V)\n        Vcorr = a*(-1*gain_offsets*I+V)\n        I = None\n        V = None\n\n    #Apply bottom right corner of electronics chain inverse Mueller matrix\n    if feedtype=='l':\n        Ucorr = U*np.cos(phase_offsets)-V*np.sin(phase_offsets)\n        Vcorr = U*np.sin(phase_offsets)+V*np.cos(phase_offsets)\n        U = None\n        V = None\n    if feedtype=='c':\n        Qcorr = Q*np.cos(phase_offsets)+U*np.sin(phase_offsets)\n        Ucorr = -1*Q*np.sin(phase_offsets)+U*np.cos(phase_offsets)\n        Q = None\n        U = None\n\n    #Reshape arrays to original shape\n    Icorr = np.reshape(np.swapaxes(Icorr,2,3),shape)\n    Qcorr = np.reshape(np.swapaxes(Qcorr,2,3),shape)\n    Ucorr = np.reshape(np.swapaxes(Ucorr,2,3),shape)\n    Vcorr = np.reshape(np.swapaxes(Vcorr,2,3),shape)\n\n    #Return corrected data arrays\n    return Icorr,Qcorr,Ucorr,Vcorr", "response": "Applies the Mueller matrix to an array of differential gains and phase differences."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef calibrate_pols(cross_pols,diode_cross,obsI=None,onefile=True,feedtype='l',**kwargs):\n    '''\n    Write Stokes-calibrated filterbank file for a given observation\n    with a calibrator noise diode measurement on the source\n\n    Parameters\n    ----------\n    cross_pols : string\n        Path to cross polarization filterbank file (rawspec output) for observation to be calibrated\n    diode_cross : string\n        Path to cross polarization filterbank file of noise diode measurement ON the target\n    obsI : string\n        Path to Stokes I filterbank file of main observation (only needed if onefile=False)\n    onefile : boolean\n        True writes all calibrated Stokes parameters to a single filterbank file,\n        False writes four separate files\n    feedtype : 'l' or 'c'\n        Basis of antenna dipoles. 'c' for circular, 'l' for linear\n    '''\n    #Obtain time sample length, frequencies, and noise diode data\n    obs = Waterfall(diode_cross,max_load=150)\n    cross_dat = obs.data\n    tsamp = obs.header['tsamp']\n\n    #Calculate number of coarse channels in the noise diode measurement (usually 8)\n    dio_ncoarse = obs.calc_n_coarse_chan()\n    dio_nchans = obs.header['nchans']\n    dio_chan_per_coarse = dio_nchans/dio_ncoarse\n    obs = None\n    Idat,Qdat,Udat,Vdat = get_stokes(cross_dat,feedtype)\n    cross_dat = None\n    #Calculate differential gain and phase from noise diode measurements\n    print('Calculating Mueller Matrix variables')\n    gams = gain_offsets(Idat,Qdat,Udat,Vdat,tsamp,dio_chan_per_coarse,feedtype,**kwargs)\n    psis = phase_offsets(Idat,Qdat,Udat,Vdat,tsamp,dio_chan_per_coarse,feedtype,**kwargs)\n\n    #Clear data arrays to save memory\n    Idat = None\n    Qdat = None\n    Udat = None\n    Vdat = None\n\n    #Get corrected Stokes parameters\n    print('Opening '+cross_pols)\n    cross_obs = Waterfall(cross_pols,max_load=150)\n    obs_ncoarse = cross_obs.calc_n_coarse_chan()\n    obs_nchans = cross_obs.header['nchans']\n    obs_chan_per_coarse = obs_nchans/obs_ncoarse\n\n    print('Grabbing Stokes parameters')\n    I,Q,U,V = get_stokes(cross_obs.data,feedtype)\n\n    print('Applying Mueller Matrix')\n    I,Q,U,V = apply_Mueller(I,Q,U,V,gams,psis,obs_chan_per_coarse,feedtype)\n\n    #Use onefile (default) to produce one filterbank file containing all Stokes information\n    if onefile==True:\n        cross_obs.data[:,0,:] = np.squeeze(I)\n        cross_obs.data[:,1,:] = np.squeeze(Q)\n        cross_obs.data[:,2,:] = np.squeeze(U)\n        cross_obs.data[:,3,:] = np.squeeze(V)\n        cross_obs.write_to_fil(cross_pols[:-15]+'.SIQUV.polcal.fil')\n        print('Calibrated Stokes parameters written to '+cross_pols[:-15]+'.SIQUV.polcal.fil')\n        return\n\n    #Write corrected Stokes parameters to four filterbank files if onefile==False\n    obs = Waterfall(obs_I,max_load=150)\n    obs.data = I\n    obs.write_to_fil(cross_pols[:-15]+'.SI.polcal.fil')   #assuming file is named *.cross_pols.fil\n    print('Calibrated Stokes I written to '+cross_pols[:-15]+'.SI.polcal.fil')\n\n    obs.data = Q\n    obs.write_to_fil(cross_pols[:-15]+'.Q.polcal.fil')   #assuming file is named *.cross_pols.fil\n    print('Calibrated Stokes Q written to '+cross_pols[:-15]+'.Q.polcal.fil')\n\n    obs.data = U\n    obs.write_to_fil(cross_pols[:-15]+'.U.polcal.fil')   #assuming file is named *.cross_pols.fil\n    print('Calibrated Stokes U written to '+cross_pols[:-15]+'.U.polcal.fil')\n\n    obs.data = V\n    obs.write_to_fil(cross_pols[:-15]+'.V.polcal.fil')   #assuming file is named *.cross_pols.fil\n    print('Calibrated Stokes V written to '+cross_pols[:-15]+'.V.polcal.fil')", "response": "Calibrate the given cross polarization filterbanks for a given observation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fracpols(str, **kwargs):\n    '''Output fractional linear and circular polarizations for a\n    rawspec cross polarization .fil file. NOT STANDARD USE'''\n\n    I,Q,U,V,L=get_stokes(str, **kwargs)\n    return L/I,V/I", "response": "Output fractional linear and circular polarizations for a\n    rawspec cross polarization. NOT STANDARD USE"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_stokefils(str, str_I, Ifil=False, Qfil=False, Ufil=False, Vfil=False, Lfil=False, **kwargs):\n    '''Writes up to 5 new filterbank files corresponding to each Stokes\n    parameter (and total linear polarization L) for a given cross polarization .fil file'''\n\n    I,Q,U,V,L=get_stokes(str, **kwargs)\n    obs = Waterfall(str_I, max_load=150) #Load filterbank file to write stokes data to\n    if Ifil:\n        obs.data = I\n        obs.write_to_fil(str[:-15]+'.I.fil')   #assuming file is named *.cross_pols.fil\n\n    if Qfil:\n        obs.data = Q\n        obs.write_to_fil(str[:-15]+'.Q.fil')   #assuming file is named *.cross_pols.fil\n\n    if Ufil:\n        obs.data = U\n        obs.write_to_fil(str[:-15]+'.U.fil')   #assuming file is named *.cross_pols.fil\n\n    if Vfil:\n        obs.data = V\n        obs.write_to_fil(str[:-15]+'.V.fil')   #assuming file is named *.cross_pols.fil\n\n    if Lfil:\n        obs.data = L\n        obs.write_to_fil(str[:-15]+'.L.fil')", "response": "Writes up to 5 new filterbank files corresponding to each StokesTaxonomy parameter and total linear polarization L for a given cross polarization. fil file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting two new filterbank files containing fractional linear and circular polarization data", "response": "def write_polfils(str, str_I, **kwargs):\n    '''Writes two new filterbank files containing fractional linear and\n    circular polarization data'''\n\n    lin,circ=fracpols(str, **kwargs)\n    obs = Waterfall(str_I, max_load=150)\n\n    obs.data = lin\n    obs.write_to_fil(str[:-15]+'.linpol.fil')   #assuming file is named *.cross_pols.fil\n\n    obs.data = circ\n    obs.write_to_fil(str[:-15]+'.circpol.fil')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the index of the closest in xarr to value val", "response": "def closest(xarr, val):\n    \"\"\" Return the index of the closest in xarr to value val \"\"\"\n    idx_closest = np.argmin(np.abs(np.array(xarr) - val))\n    return idx_closest"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupgrade data from nbits to 8bits", "response": "def unpack(data, nbit):\n    \"\"\"upgrade data from nbits to 8bits\n\n    Notes: Pretty sure this function is a little broken!\n    \"\"\"\n    if nbit > 8:\n        raise ValueError(\"unpack: nbit must be <= 8\")\n    if 8 % nbit != 0:\n        raise ValueError(\"unpack: nbit must divide into 8\")\n    if data.dtype not in (np.uint8, np.int8):\n        raise TypeError(\"unpack: dtype must be 8-bit\")\n    if nbit == 8:\n        return data\n    elif nbit == 4:\n        data = unpack_4to8(data)\n        return data\n    elif nbit == 2:\n        data = unpack_2to8(data)\n        return data\n    elif nbit == 1:\n        data = unpack_1to8(data)\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unpack_2to8(data):\n    two_eight_lookup = {0: 40,\n                        1: 12,\n                        2: -12,\n                        3: -40}\n\n    tmp = data.astype(np.uint32)\n    tmp = (tmp | (tmp << 12)) & 0xF000F\n    tmp = (tmp | (tmp << 6)) & 0x3030303\n    tmp = tmp.byteswap()\n    tmp = tmp.view('uint8')\n    mapped = np.array(tmp, dtype=np.int8)\n    for k, v in two_eight_lookup.items():\n        mapped[tmp == k] = v\n    return mapped", "response": "Promote 2 - bit unisgned data into 8 - bit unsigned data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npromote 2 - bit unisgned data into 8 - bit unsigned data.", "response": "def unpack_4to8(data):\n    \"\"\" Promote 2-bit unisgned data into 8-bit unsigned data.\n\n    Args:\n        data: Numpy array with dtype == uint8\n\n    Notes:\n        # The process is this:\n        # ABCDEFGH [Bits of one 4+4-bit value]\n        # 00000000ABCDEFGH [astype(uint16)]\n        # 0000ABCDEFGH0000 [<< 4]\n        # 0000ABCDXXXXEFGH [bitwise 'or' of previous two lines]\n        # 0000111100001111 [0x0F0F]\n        # 0000ABCD0000EFGH [bitwise 'and' of previous two lines]\n        # ABCD0000EFGH0000 [<< 4]\n        # which effectively pads the two 4-bit values with zeros on the right\n        # Note: This technique assumes LSB-first ordering\n    \"\"\"\n\n    tmpdata = data.astype(np.int16)  # np.empty(upshape, dtype=np.int16)\n    tmpdata = (tmpdata | (tmpdata << 4)) & 0x0F0F\n    # tmpdata = tmpdata << 4 # Shift into high bits to avoid needing to sign extend\n    updata = tmpdata.byteswap()\n    return updata.view(data.dtype)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning ON - OFF for all Stokes parameters given a cross_pols noise diode measurement", "response": "def get_diff(dio_cross,feedtype,**kwargs):\n    '''\n    Returns ON-OFF for all Stokes parameters given a cross_pols noise diode measurement\n    '''\n    #Get Stokes parameters, frequencies, and time sample length\n    obs = Waterfall(dio_cross,max_load=150)\n    freqs = obs.populate_freqs()\n    tsamp = obs.header['tsamp']\n    data = obs.data\n    obs = None\n\n    I,Q,U,V = get_stokes(data,feedtype)\n\n    #Fold noise diode data\n    I_OFF,I_ON = foldcal(I,tsamp,**kwargs)\n    Q_OFF,Q_ON = foldcal(Q,tsamp,**kwargs)\n    U_OFF,U_ON = foldcal(U,tsamp,**kwargs)\n    V_OFF,V_ON = foldcal(V,tsamp,**kwargs)\n\n    #Do ON-OFF subtraction\n    Idiff = I_ON-I_OFF\n    Qdiff = Q_ON-Q_OFF\n    Udiff = U_ON-U_OFF\n    Vdiff = V_ON-V_OFF\n\n    return Idiff,Qdiff,Udiff,Vdiff,freqs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plot_Stokes_diode(dio_cross,diff=True,feedtype='l',**kwargs):\n    '''\n    Plots the uncalibrated full stokes spectrum of the noise diode.\n    Use diff=False to plot both ON and OFF, or diff=True for ON-OFF\n    '''\n\n    #If diff=True, get ON-OFF. If not get ON and OFF separately\n    if diff==True:\n        Idiff,Qdiff,Udiff,Vdiff,freqs = get_diff(dio_cross,feedtype,**kwargs)\n    else:\n        obs = Waterfall(dio_cross,max_load=150)\n        freqs = obs.populate_freqs()\n        tsamp = obs.header['tsamp']\n        data = obs.data\n        I,Q,U,V = get_stokes(data,feedtype)\n\n        I_OFF,I_ON = foldcal(I,tsamp,**kwargs)\n        Q_OFF,Q_ON = foldcal(Q,tsamp,**kwargs)\n        U_OFF,U_ON = foldcal(U,tsamp,**kwargs)\n        V_OFF,V_ON = foldcal(V,tsamp,**kwargs)\n\n    #Plot spectra\n    if diff==True:\n        plt.plot(freqs,Idiff,'k-',label='I')\n        plt.plot(freqs,Qdiff,'r-',label='Q')\n        plt.plot(freqs,Udiff,'g-',label='U')\n        plt.plot(freqs,Vdiff,'m-',label='V')\n    else:\n        plt.plot(freqs,I_ON,'k-',label='I ON')\n        plt.plot(freqs,I_OFF,'k--',label='I OFF')\n        plt.plot(freqs,Q_ON,'r-',label='Q ON')\n        plt.plot(freqs,Q_OFF,'r--',label='Q OFF')\n        plt.plot(freqs,U_ON,'g-',label='U ON')\n        plt.plot(freqs,U_OFF,'g--',label='U OFF')\n        plt.plot(freqs,V_ON,'m-',label='V ON')\n        plt.plot(freqs,V_OFF,'m--',label='V OFF')\n\n    plt.legend()\n    plt.xlabel('Frequency (MHz)')\n    plt.title('Uncalibrated Full Stokes Noise Diode Spectrum')\n    plt.ylabel('Power (Counts)')", "response": "Plot the uncalibrated full stokes spectrum of the noise diode."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plot_calibrated_diode(dio_cross,chan_per_coarse=8,feedtype='l',**kwargs):\n    '''\n    Plots the corrected noise diode spectrum for a given noise diode measurement\n    after application of the inverse Mueller matrix for the electronics chain.\n    '''\n    #Get full stokes data for the ND observation\n    obs = Waterfall(dio_cross,max_load=150)\n    freqs = obs.populate_freqs()\n    tsamp = obs.header['tsamp']\n    data = obs.data\n    obs = None\n    I,Q,U,V = get_stokes(data,feedtype)\n    data = None\n\n    #Calculate Mueller Matrix variables for each coarse channel\n    psis = phase_offsets(I,Q,U,V,tsamp,chan_per_coarse,feedtype,**kwargs)\n    G = gain_offsets(I,Q,U,V,tsamp,chan_per_coarse,feedtype,**kwargs)\n\n    #Apply the Mueller matrix to original noise diode data and refold\n    I,Q,U,V = apply_Mueller(I,Q,U,V,G,psis,chan_per_coarse,feedtype)\n    I_OFF,I_ON = foldcal(I,tsamp,**kwargs)\n    Q_OFF,Q_ON = foldcal(Q,tsamp,**kwargs)\n    U_OFF,U_ON = foldcal(U,tsamp,**kwargs)\n    V_OFF,V_ON = foldcal(V,tsamp,**kwargs)\n\n    #Delete data arrays for space\n    I = None\n    Q = None\n    U = None\n    V = None\n\n    #Plot new ON-OFF spectra\n    plt.plot(freqs,I_ON-I_OFF,'k-',label='I')\n    plt.plot(freqs,Q_ON-Q_OFF,'r-',label='Q')\n    plt.plot(freqs,U_ON-U_OFF,'g-',label='U')\n    plt.plot(freqs,V_ON-V_OFF,'m-',label='V')\n\n    plt.legend()\n    plt.xlabel('Frequency (MHz)')\n    plt.title('Calibrated Full Stokes Noise Diode Spectrum')\n    plt.ylabel('Power (Counts)')", "response": "Plots the corrected noise diode spectrum for a given noise diode measurement."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_phase_offsets(dio_cross,chan_per_coarse=8,feedtype='l',ax1=None,ax2=None,legend=True,**kwargs):\n    '''\n    Plots the calculated phase offsets of each coarse channel along with\n    the UV (or QU) noise diode spectrum for comparison\n    '''\n    #Get ON-OFF ND spectra\n    Idiff,Qdiff,Udiff,Vdiff,freqs = get_diff(dio_cross,feedtype,**kwargs)\n    obs = Waterfall(dio_cross,max_load=150)\n    tsamp = obs.header['tsamp']\n    data = obs.data\n    obs = None\n    I,Q,U,V = get_stokes(data,feedtype)\n\n    #Get phase offsets and convert to degrees\n    coarse_psis = phase_offsets(I,Q,U,V,tsamp,chan_per_coarse,feedtype,**kwargs)\n    coarse_freqs = convert_to_coarse(freqs,chan_per_coarse)\n    coarse_degs = np.degrees(coarse_psis)\n\n    #Plot phase offsets\n    if ax2==None:\n        plt.subplot(211)\n    else:\n        axPsi = plt.axes(ax2)\n        plt.setp(axPsi.get_xticklabels(),visible=False)\n    plt.plot(coarse_freqs,coarse_degs,'ko',markersize=2,label='Coarse Channel $\\psi$')\n    plt.ylabel('Degrees')\n    plt.grid(True)\n    plt.title('Phase Offsets')\n    if legend==True:\n        plt.legend()\n\n    #Plot U and V spectra\n    if ax1==None:\n        plt.subplot(212)\n    else:\n        axUV = plt.axes(ax1)\n    plt.plot(freqs,Udiff,'g-',label='U')\n    if feedtype=='l':\n        plt.plot(freqs,Vdiff,'m-',label='V')\n    if feedtype=='c':\n        plt.plot(freqs,Qdiff,'r-',label='Q')\n    plt.xlabel('Frequency (MHz)')\n    plt.ylabel('Power (Counts)')\n    plt.grid(True)\n    if legend==True:\n        plt.legend()", "response": "Plots the phase offsets of each coarse channel along with the UV or QU of the noise diode spectrum for comparison."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plot_gain_offsets(dio_cross,dio_chan_per_coarse=8,feedtype='l',ax1=None,ax2=None,legend=True,**kwargs):\n    '''\n    Plots the calculated gain offsets of each coarse channel along with\n    the time averaged power spectra of the X and Y feeds\n    '''\n    #Get ON-OFF ND spectra\n    Idiff,Qdiff,Udiff,Vdiff,freqs = get_diff(dio_cross,feedtype,**kwargs)\n    obs = Waterfall(dio_cross,max_load=150)\n    tsamp = obs.header['tsamp']\n    data = obs.data\n    obs = None\n    I,Q,U,V = get_stokes(data,feedtype)\n\n    #Get phase offsets and convert to degrees\n    coarse_G = gain_offsets(I,Q,U,V,tsamp,dio_chan_per_coarse,feedtype,**kwargs)\n    coarse_freqs = convert_to_coarse(freqs,dio_chan_per_coarse)\n\n    #Get X and Y spectra for the noise diode ON and OFF\n    #If using circular feeds these correspond to LL and RR\n    XX_OFF,XX_ON = foldcal(np.expand_dims(data[:,0,:],axis=1),tsamp,**kwargs)\n    YY_OFF,YY_ON = foldcal(np.expand_dims(data[:,1,:],axis=1),tsamp,**kwargs)\n\n    if ax1==None:\n        plt.subplot(211)\n    else:\n        axG = plt.axes(ax1)\n        plt.setp(axG.get_xticklabels(),visible=False)\n    plt.plot(coarse_freqs,coarse_G,'ko',markersize=2)\n    plt.ylabel(r'$\\frac{\\Delta G}{2}$',rotation=90)\n    if feedtype=='l':\n        plt.title('XY Gain Difference')\n    if feedtype=='c':\n        plt.title('LR Gain Difference')\n    plt.grid(True)\n\n    if ax2==None:\n        plt.subplot(212)\n    else:\n        axXY = plt.axes(ax2,sharex=axG)\n    if feedtype=='l':\n        plt.plot(freqs,XX_OFF,'b-',label='XX')\n        plt.plot(freqs,YY_OFF,'r-',label='YY')\n    if feedtype=='c':\n        plt.plot(freqs,XX_OFF,'b-',label='LL')\n        plt.plot(freqs,YY_OFF,'r-',label='RR')\n    plt.xlabel('Frequency (MHz)')\n    plt.ylabel('Power (Counts)')\n    if legend==True:\n        plt.legend()", "response": "Plots the gain offsets of each coarse channel along with the time averaged power spectra of the X and Y feeds."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nplotting the diode measurement over the observation time series.", "response": "def plot_diode_fold(dio_cross,bothfeeds=True,feedtype='l',min_samp=-500,max_samp=7000,legend=True,**kwargs):\n    '''\n    Plots the calculated average power and time sampling of ON (red) and\n    OFF (blue) for a noise diode measurement over the observation time series\n    '''\n    #Get full stokes data of ND measurement\n    obs = Waterfall(dio_cross,max_load=150)\n    tsamp = obs.header['tsamp']\n    data = obs.data\n    obs = None\n    I,Q,U,V = get_stokes(data,feedtype)\n\n    #Calculate time series, OFF and ON averages, and time samples for each\n    tseriesI = np.squeeze(np.mean(I,axis=2))\n    I_OFF,I_ON,OFFints,ONints = foldcal(I,tsamp,inds=True,**kwargs)\n    if bothfeeds==True:\n        if feedtype=='l':\n            tseriesQ = np.squeeze(np.mean(Q,axis=2))\n            tseriesX = (tseriesI+tseriesQ)/2\n            tseriesY = (tseriesI-tseriesQ)/2\n        if feedtype=='c':\n            tseriesV = np.squeeze(np.mean(V,axis=2))\n            tseriesR = (tseriesI+tseriesV)/2\n            tseriesL = (tseriesI-tseriesV)/2\n\n    stop = ONints[-1,1]\n\n    #Plot time series and calculated average for ON and OFF\n    if bothfeeds==False:\n        plt.plot(tseriesI[0:stop],'k-',label='Total Power')\n        for i in ONints:\n            plt.plot(np.arange(i[0],i[1]),np.full((i[1]-i[0]),np.mean(I_ON)),'r-')\n        for i in OFFints:\n            plt.plot(np.arange(i[0],i[1]),np.full((i[1]-i[0]),np.mean(I_OFF)),'b-')\n    else:\n        if feedtype=='l':\n            diff = np.mean(tseriesX)-np.mean(tseriesY)\n            plt.plot(tseriesX[0:stop],'b-',label='XX')\n            plt.plot(tseriesY[0:stop]+diff,'r-',label='YY (shifted)')\n        if feedtype=='c':\n            diff = np.mean(tseriesL)-np.mean(tseriesR)\n            plt.plot(tseriesL[0:stop],'b-',label='LL')\n            plt.plot(tseriesR[0:stop]+diff,'r-',label='RR (shifted)')\n\n    #Calculate plotting limits\n    if bothfeeds==False:\n        lowlim = np.mean(I_OFF)-(np.mean(I_ON)-np.mean(I_OFF))/2\n        hilim = np.mean(I_ON)+(np.mean(I_ON)-np.mean(I_OFF))/2\n        plt.ylim((lowlim,hilim))\n\n    plt.xlim((min_samp,max_samp))\n    plt.xlabel('Time Sample Number')\n    plt.ylabel('Power (Counts)')\n    plt.title('Noise Diode Fold')\n    if legend==True:\n        plt.legend()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate and shows five plots of full calibration on the diode.", "response": "def plot_fullcalib(dio_cross,feedtype='l',**kwargs):\n    '''\n    Generates and shows five plots: Uncalibrated diode, calibrated diode, fold information,\n    phase offsets, and gain offsets for a noise diode measurement. Most useful diagnostic plot to\n    make sure calibration proceeds correctly.\n    '''\n\n    plt.figure(\"Multiple Calibration Plots\", figsize=(12,9))\n    left, width = 0.075,0.435\n    bottom, height = 0.45,0.5\n    width2 = 0.232\n    bottom2, height2 = 0.115,0.0975\n\n    rect_uncal = [left,bottom,width,height]\n    rect_cal = [left+width+0.025,bottom,width,height]\n    rect_fold = [left,bottom2,width2,0.22]\n    rect_gain1 = [left+width2+0.1,bottom2,width2,height2]\n    rect_phase1 = [left+width2*2+0.1*2,bottom2,width2,height2]\n    rect_gain2 = [left+width2+0.1,bottom2+height2+0.025,width2,height2]\n    rect_phase2 = [left+width2*2+0.1*2,bottom2+height2+0.025,width2,height2]\n\n    #--------\n    axFold = plt.axes(rect_fold)\n    print('Plotting Diode Fold')\n    plot_diode_fold(dio_cross,bothfeeds=False,feedtype=feedtype,min_samp=2000,max_samp=5500,legend=False,**kwargs)\n\n    #--------\n    print('Plotting Gain Offsets')\n    plot_gain_offsets(dio_cross,feedtype=feedtype,ax1=rect_gain2,ax2=rect_gain1,legend=False,**kwargs)\n\n    #--------\n    print('Plotting Phase Offsets')\n    plot_phase_offsets(dio_cross,feedtype=feedtype,ax1=rect_phase1,ax2=rect_phase2,legend=False,**kwargs)\n    plt.ylabel('')\n\n    #--------\n    ax_uncal = plt.axes(rect_uncal)\n    print('Plotting Uncalibrated Diode')\n    plot_Stokes_diode(dio_cross,feedtype=feedtype,**kwargs)\n\n    #--------\n    ax_cal = plt.axes(rect_cal,sharey=ax_uncal)\n    print('Plotting Calibrated Diode')\n    plot_calibrated_diode(dio_cross,feedtype=feedtype,**kwargs)\n    plt.ylabel('')\n    plt.setp(ax_cal.get_yticklabels(),visible=False)\n\n    plt.savefig(dio_cross[:-4]+'.stokescalib.png',dpi=2000)\n    plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nplots the full - band Stokes I spectrum of the noise diode.", "response": "def plot_diodespec(ON_obs,OFF_obs,calflux,calfreq,spec_in,units='mJy',**kwargs):\n    '''\n    Plots the full-band Stokes I spectrum of the noise diode (ON-OFF)\n    '''\n\n    dspec = diode_spec(ON_obs,OFF_obs,calflux,calfreq,spec_in,**kwargs)\n    obs = Waterfall(ON_obs,max_load=150)\n    freqs = obs.populate_freqs()\n    chan_per_coarse = obs.header['nchans']/obs.calc_n_coarse_chan()\n    coarse_freqs = convert_to_coarse(freqs,chan_per_coarse)\n    plt.ion()\n    plt.figure()\n    plt.plot(coarse_freqs,dspec)\n    plt.xlabel('Frequency (MHz)')\n    plt.ylabel('Flux Density ('+units+')')\n    plt.title('Noise Diode Spectrum')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cmd_tool():\r\n    '''Read input and output frequency, and output file name\r\n    '''\r\n\r\n    parser = argparse.ArgumentParser(description='Dices hdf5 or fil files and writes to hdf5 or fil.')\r\n    parser.add_argument('-f', '--input_filename', action='store', default=None, dest='in_fname', type=str, help='Name of file to write from (HDF5 or FIL)')\r\n    parser.add_argument('-b', action='store', default=None, dest='f_start', type=float, help='Start frequency in MHz')\r\n    parser.add_argument('-e', action='store', default=None, dest='f_stop', type=float, help='Stop frequency in MHz')\r\n    parser.add_argument('-x', '--output_file', action='store', default=None, dest='out_format', type=str, help='Output file format [.h5 or .fil].')\r\n    parser.add_argument('-o', '--output_filename', action='store', default=None, dest='out_fname', type=str, help='Ouput file name to write (to HDF5 or FIL).')\r\n    parser.add_argument('-l', action='store', default=None, dest='max_load', type=float,help='Maximum data limit to load. Default:1GB')\r\n\r\n    args = parser.parse_args()\r\n\r\n    if len(sys.argv) == 1:\r\n            logger.error('Indicate file name and start and stop frequencies')\r\n            sys.exit()\r\n\r\n    if args.in_fname == None:\r\n            logger.error('Need to indicate input file name')\r\n            sys.exit()\r\n\r\n    if args.out_fname == None:\r\n        if (args.out_format == None) or (args.out_format == 'h5'):\r\n            if args.in_fname[len(args.in_fname)-4:] == '.fil':\r\n                args.out_fname = args.in_fname\r\n                args.out_fname = args.out_fname.replace('.fil','_diced.h5')\r\n            elif args.in_fname[len(args.in_fname)-3:] == '.h5':\r\n                args.out_fname = args.in_fname\r\n                args.out_fname = args.out_fname.replace('.h5','_diced.h5')\r\n            else:\r\n                logger.error('Input file not recognized')\r\n                sys.exit()\r\n        elif args.out_format == 'fil':\r\n            if args.in_fname[len(args.in_fname)-4:] == '.fil':\r\n                args.out_fname = args.in_fname\r\n                args.out_fname = args.out_fname.replace('.fil','_diced.fil')\r\n            elif args.in_fname[len(args.in_fname)-3:] == '.h5':\r\n                args.out_fname = args.in_fname\r\n                args.out_fname = args.out_fname.replace('.h5','_diced.fil')\r\n            else:\r\n                logger.error('input file not recognized.')\r\n                sys.exit()\r\n\r\n        else:\r\n            logger.error('Must indicate either output file name or valid output file extension.')\r\n            sys.exit()\r\n\r\n    elif (args.out_fname[len(args.out_fname)-4:] == '.fil') and (args.out_format == 'h5'):\r\n        logger.error('Output file extension does not match output file name')\r\n        sys.exit()\r\n\r\n    elif (args.out_fname[len(args.out_fname)-3:] == '.h5') and (args.out_format == 'fil'):\r\n        logger.error('Output file extension does not match output file name.')\r\n        sys.exit()\r\n\r\n    if (args.out_fname[len(args.out_fname)-3:] != '.h5') and (args.out_fname[len(args.out_fname)-4:] != '.fil'):\r\n        logger.error('Indicate output file name with extension, or simply output file extension.')\r\n        sys.exit()\r\n\r\n    if args.f_start == None and args.f_stop == None:\r\n        logger.error('Please give either start and/or end frequencies. Otherwise use fil2h5 or h52fil functions.')\r\n        sys.exit()\r\n\r\n    if args.f_start == None:\r\n        logger.warning('Lower frequency not given, setting to ' + str(f_min_file) + ' MHz to match file.')\r\n\r\n    if args.f_stop == None:\r\n        logger.warning('Higher frequency not given, setting to ' + str(f_max_file) + ' MHz to match file.')\r\n\r\n    #Read start frequency and bandwidth from data set\r\n    file_big = Waterfall(args.in_fname, max_load = args.max_load)\r\n    f_min_file = file_big.header['fch1']\r\n    f_max_file = file_big.header['fch1'] + file_big.header['nchans']*file_big.header['foff']\r\n\r\n    if f_max_file < f_min_file:\r\n            f_max_file,f_min_file = f_min_file,f_max_file\r\n\r\n    FreqBWFile = f_max_file-f_min_file\r\n    stdDF = FreqBWFile / float(file_big.calc_n_coarse_chan())   #stdDF = 2.9296875\r\n\r\n    if args.f_stop < args.f_start:\r\n            args.f_stop,args.f_start = args.f_start,args.f_stop\r\n\r\n    if args.f_start < f_max_file and args.f_start > f_min_file and args.f_stop > f_max_file:\r\n            args.f_stop = f_max_file\r\n            logger.warning('Higher frequency set to ' + str(f_max_file) + ' MHz to match file.')\r\n\r\n    if args.f_stop < f_max_file and args.f_stop > f_min_file and args.f_start < f_min_file:\r\n            args.f_start = f_min_file\r\n            logger.warning('Lower frequency set to ' + str(f_min_file) + ' MHz to match file.')\r\n\r\n    if args.f_start < f_min_file and args.f_stop > f_max_file:\r\n            args.f_start = f_min_file\r\n            args.f_stop = f_max_file\r\n            logger.warning('Lower frequency set to ' + str(f_min_file) + ' MHz and higher frequency set to ' + str(f_max_file) + ' MHz to match file.')\r\n            # print '\\nindicated frequencies include file frequency span - no need to dice\\n'\r\n            # sys.exit()\r\n\r\n    if min(args.f_start,args.f_stop) < f_min_file or max(args.f_start,args.f_stop) > f_max_file:\r\n            logger.error('Bandwidth to extract must be within ' + str(f_min_file) + ' MHz and ' + str(f_max_file) + ' MHz.')\r\n            sys.exit()\r\n\r\n    # calculate real coarse channel begin and end freqs\r\n    f_start_real = math.floor((min(args.f_start,args.f_stop) - f_min_file)/stdDF)*stdDF + f_min_file\r\n    f_stop_real = f_max_file - math.floor((f_max_file - max(args.f_start,args.f_stop))/stdDF)*stdDF\r\n\r\n    # print\r\n    # print \"true start frequency is \" + str(f_start_real)\r\n    # print \"true stop frequency is \" + str(f_stop_real)\r\n\r\n    logger.info('Writing to ' + args.out_fname)\r\n    logger.info('Extacting from ' + str(f_start_real) + ' MHz to ' + str(f_stop_real) + ' MHz.')\r\n\r\n    # create waterfall object\r\n    file_small = Waterfall(args.in_fname, f_start = f_start_real, f_stop = f_stop_real, max_load = args.max_load)\r\n\r\n    # write waterfall object\r\n    if args.out_fname[len(args.out_fname)-4:] == '.fil':\r\n        file_small.write_to_fil(args.out_fname)\r\n    elif args.out_fname[len(args.out_fname)-3:] == '.h5':\r\n        file_small.write_to_hdf5(args.out_fname)\r\n    else:\r\n        logger.error('Error in output file creation : verify output file name and extension.')\r\n        sys.exit()", "response": "Command line interface for the\r\nTaxonomy command line interface."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncommands line utility for creating HDF5 Filterbank files.", "response": "def cmd_tool(args=None):\n    \"\"\" Command line utility for creating HDF5 blimpy files. \"\"\"\n    from argparse import ArgumentParser\n    parser = ArgumentParser(description=\"Command line utility for creating HDF5 Filterbank files.\")\n    parser.add_argument('dirname', type=str, help='Name of directory to read')\n    args = parser.parse_args()\n    \n    if not HAS_BITSHUFFLE:\n        print(\"Error: the bitshuffle library is required to run this script.\")\n        exit()\n\n    filelist = glob.glob(os.path.join(args.dirname, '*.fil'))\n\n    for filename in filelist:\n        if not os.path.exists(filename + '.h5'):\n            t0 = time.time()\n            print(\"\\nReading %s header...\" % filename)\n            fb = Filterbank(filename, load_data=False)\n\n            data_shape = (fb.n_ints_in_file, fb.header['nifs'], fb.header['nchans'])\n            data_dtype = fb.data.dtype\n            print(data_dtype)\n\n\n            print(\"Creating new dataset, %s\" % str(data_shape))\n            block_size = 0\n            h5 = h5py.File(filename + '.h5', 'w')\n\n            h5.attrs['CLASS'] = 'FILTERBANK'\n\n            dset = h5.create_dataset('data',\n                              shape=data_shape,\n                              compression=bitshuffle.h5.H5FILTER,\n                              compression_opts=(block_size, bitshuffle.h5.H5_COMPRESS_LZ4),\n                              dtype=data_dtype)\n\n            dset_mask = h5.create_dataset('mask',\n                                     shape=data_shape,\n                                     compression=bitshuffle.h5.H5FILTER,\n                                     compression_opts=(block_size, bitshuffle.h5.H5_COMPRESS_LZ4),\n                                     dtype='uint8')\n\n            dset.dims[0].label = \"frequency\"\n            dset.dims[1].label = \"feed_id\"\n            dset.dims[2].label = \"time\"\n\n            dset_mask.dims[0].label = \"frequency\"\n            dset_mask.dims[1].label = \"feed_id\"\n            dset_mask.dims[2].label = \"time\"\n\n            # Copy over header information as attributes\n            for key, value in fb.header.items():\n                dset.attrs[key] = value\n\n            filesize = os.path.getsize(filename)\n\n            if filesize >= MAX_SIZE:\n                n_int_per_read = int(filesize / MAX_SIZE / 2)\n                print(\"Filling in with data over %i reads...\" % n_int_per_read)\n                for ii in range(0, n_int_per_read):\n                    print(\"Reading %i of %i\" % (ii + 1, n_int_per_read))\n                    #print  ii*n_int_per_read, (ii+1)*n_int_per_read\n                    fb = Filterbank(filename, t_start=ii*n_int_per_read, t_stop=(ii+1) * n_int_per_read)\n                    dset[ii*n_int_per_read:(ii+1)*n_int_per_read] = fb.data[:]\n            else:\n                fb = Filterbank(filename)\n                print(dset.shape, \" -> \", fb.data.shape)\n                dset[:] = fb.data[:]\n\n            h5.close()\n\n            t1 = time.time()\n            print(\"Conversion time: %2.2fs\" % (t1- t0))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nopen a file in a Waterfall file.", "response": "def open_file(filename, f_start=None, f_stop=None,t_start=None, t_stop=None,load_data=True,max_load=1.):\n    \"\"\"Open a HDF5 or filterbank file\n\n    Returns instance of a Reader to read data from file.\n\n    ================== ==================================================\n    Filename extension File type\n    ================== ==================================================\n    h5, hdf5           HDF5 format\n    fil                fil format\n    *other*            Will raise NotImplementedError\n    ================== ==================================================\n\n    \"\"\"\n    if not os.path.isfile(filename):\n        type(filename)\n        print(filename)\n        raise IOError(\"No such file or directory: \" + filename)\n\n    filename = os.path.expandvars(os.path.expanduser(filename))\n    # Get file extension to determine type\n    ext = filename.split(\".\")[-1].strip().lower()\n\n    if six.PY3:\n        ext = bytes(ext, 'ascii')\n\n    if h5py.is_hdf5(filename):\n        # Open HDF5 file\n        return H5Reader(filename, f_start=f_start, f_stop=f_stop, t_start=t_start, t_stop=t_stop,\n                        load_data=load_data, max_load=max_load)\n    elif sigproc.is_filterbank(filename):\n        # Open FIL file\n        return FilReader(filename, f_start=f_start, f_stop=f_stop, t_start=t_start, t_stop=t_stop, load_data=load_data, max_load=max_load)\n    else:\n        raise NotImplementedError('Cannot open this type of file with Waterfall')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _setup_selection_range(self, f_start=None, f_stop=None, t_start=None, t_stop=None, init=False):\n\n        # This avoids resetting values\n        if init is True:\n            if t_start is None:\n                t_start = self.t_begin\n            if t_stop is None:\n                t_stop = self.t_end\n            if f_start is None:\n                f_start = self.f_begin\n            if f_stop is None:\n                f_stop = self.f_end\n        else:\n            if f_start is None:\n                f_start = self.f_start\n            if f_stop is None:\n                f_stop = self.f_stop\n            if t_start is None:\n                t_start = self.t_start\n            if t_stop is None:\n                t_stop = self.t_stop\n\n        # By now, all values start/stop are populated.\n\n        if t_stop >= 0 and t_start >= 0 and t_stop < t_start:\n            t_stop, t_start = t_start,t_stop\n            logger.warning('Given t_stop < t_start, assuming reversed values.')\n        if f_stop and f_start and f_stop < f_start:\n            f_stop, f_start = f_start,f_stop\n            logger.warning('Given f_stop < f_start, assuming reversed values.')\n\n        if t_start >= self.t_begin and t_start < self.t_end:\n            self.t_start = int(t_start)\n        else:\n            if init is False or t_start != None:\n                logger.warning('Setting t_start = %f, since t_start not given or not valid.'%self.t_begin)\n            self.t_start = self.t_begin\n\n        if t_stop <= self.t_end  and t_stop > self.t_begin:\n            self.t_stop = int(t_stop)\n        else:\n            if init is False or t_stop:\n                logger.warning('Setting t_stop = %f, since t_stop not given or not valid.'%self.t_end)\n            self.t_stop = self.t_end\n\n        if f_start >= self.f_begin and f_start < self.f_end:\n            self.f_start = f_start\n        else:\n            if init is False or f_start:\n                logger.warning('Setting f_start = %f, since f_start not given or not valid.'%self.f_begin)\n            self.f_start = self.f_begin\n\n        if f_stop <= self.f_end and f_stop > self.f_begin:\n            self.f_stop = f_stop\n        else:\n            if init is False or f_stop:\n                logger.warning('Setting f_stop = %f, since f_stop not given or not valid.'%self.f_end)\n            self.f_stop = self.f_end\n\n        # Now we have setup bounds, we can calculate shape of selection\n        self.selection_shape = self._calc_selection_shape()", "response": "Sets up the selection range for the current file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets up the data type of the object.", "response": "def _setup_dtype(self):\n        \"\"\"Calculating dtype\n        \"\"\"\n\n        #Set up the data type\n        if self._n_bytes  == 4:\n            return b'float32'\n        elif self._n_bytes  == 2:\n            return b'uint16'\n        elif self._n_bytes  == 1:\n            return b'uint8'\n        else:\n            logger.warning('Having trouble setting dtype, assuming float32.')\n            return b'float32'"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating size of data of interest.", "response": "def _calc_selection_size(self):\n        \"\"\"Calculate size of data of interest.\n        \"\"\"\n\n        #Check to see how many integrations requested\n        n_ints = self.t_stop - self.t_start\n        #Check to see how many frequency channels requested\n        n_chan = (self.f_stop - self.f_start) / abs(self.header[b'foff'])\n\n        n_bytes  = self._n_bytes\n        selection_size = int(n_ints*n_chan*n_bytes)\n\n        return selection_size"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate shape of data of interest.", "response": "def _calc_selection_shape(self):\n        \"\"\"Calculate shape of data of interest.\n        \"\"\"\n\n        #Check how many integrations requested\n        n_ints = int(self.t_stop - self.t_start)\n        #Check how many frequency channels requested\n        n_chan = int(np.round((self.f_stop - self.f_start) / abs(self.header[b'foff'])))\n\n        selection_shape = (n_ints,int(self.header[b'nifs']),n_chan)\n\n        return selection_shape"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _setup_chans(self):\n\n        if self.header[b'foff'] < 0:\n            f0 = self.f_end\n        else:\n            f0 = self.f_begin\n\n        i_start, i_stop = 0, self.n_channels_in_file\n        if self.f_start:\n            i_start = np.round((self.f_start - f0) / self.header[b'foff'])\n        if self.f_stop:\n            i_stop  = np.round((self.f_stop - f0)  / self.header[b'foff'])\n\n        #calculate closest true index value\n        chan_start_idx = np.int(i_start)\n        chan_stop_idx  = np.int(i_stop)\n\n        if chan_stop_idx < chan_start_idx:\n            chan_stop_idx, chan_start_idx = chan_start_idx,chan_stop_idx\n\n        self.chan_start_idx =  chan_start_idx\n        self.chan_stop_idx = chan_stop_idx", "response": "Setup channel borders and indices for the channel tables."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _setup_freqs(self):\n\n        if self.header[b'foff'] > 0:\n            self.f_start = self.f_begin + self.chan_start_idx*abs(self.header[b'foff'])\n            self.f_stop = self.f_begin + self.chan_stop_idx*abs(self.header[b'foff'])\n        else:\n            self.f_start = self.f_end - self.chan_stop_idx*abs(self.header[b'foff'])\n            self.f_stop = self.f_end - self.chan_start_idx*abs(self.header[b'foff'])", "response": "Sets up the frequency borders based on the channel values."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef populate_timestamps(self,update_header=False):\n\n        #Check to see how many integrations requested\n        ii_start, ii_stop = 0, self.n_ints_in_file\n        if self.t_start:\n            ii_start = self.t_start\n        if self.t_stop:\n            ii_stop = self.t_stop\n\n        ## Setup time axis\n        t0 = self.header[b'tstart']\n        t_delt = self.header[b'tsamp']\n\n        if update_header:\n            timestamps = ii_start * t_delt / 24./60./60. + t0\n        else:\n            timestamps = np.arange(ii_start, ii_stop) * t_delt / 24./60./60. + t0\n\n        return timestamps", "response": "Populate time axis.\n            timestamps."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef calc_n_blobs(self, blob_dim):\n\n        n_blobs = int(np.ceil(1.0 * np.prod(self.selection_shape) / np.prod(blob_dim)))\n\n        return n_blobs", "response": "Calculates the number of blobs in the data selection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if the current selection is too large.", "response": "def isheavy(self):\n        \"\"\" Check if the current selection is too large.\n        \"\"\"\n\n        selection_size_bytes = self._calc_selection_size()\n\n        if selection_size_bytes > self.MAX_DATA_ARRAY_SIZE:\n            return True\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading header and return a Python dictionary of key = value pairs", "response": "def read_header(self):\n        \"\"\" Read header and return a Python dictionary of key:value pairs\n        \"\"\"\n\n        self.header = {}\n\n        for key, val in self.h5['data'].attrs.items():\n            if six.PY3:\n                key = bytes(key, 'ascii')\n            if key == b'src_raj':\n                self.header[key] = Angle(val, unit='hr')\n            elif key == b'src_dej':\n                self.header[key] = Angle(val, unit='deg')\n            else:\n                self.header[key] = val\n\n        return self.header"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds first blob from selection.", "response": "def _find_blob_start(self, blob_dim, n_blob):\n        \"\"\"Find first blob from selection.\n        \"\"\"\n\n        #Convert input frequencies into what their corresponding channel number would be.\n        self._setup_chans()\n\n        #Check which is the blob time offset\n        blob_time_start = self.t_start + blob_dim[self.time_axis]*n_blob\n\n        #Check which is the blob frequency offset (in channels)\n        blob_freq_start = self.chan_start_idx + (blob_dim[self.freq_axis]*n_blob)%self.selection_shape[self.freq_axis]\n\n        blob_start = np.array([blob_time_start, 0, blob_freq_start])\n\n        return blob_start"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_data(self, f_start=None, f_stop=None,t_start=None, t_stop=None):\n\n        self._setup_selection_range(f_start=f_start, f_stop=f_stop, t_start=t_start, t_stop=t_stop)\n\n        #check if selection is small enough.\n        if self.isheavy():\n            logger.warning(\"Selection size of %.2f GB, exceeding our size limit %.2f GB. Instance created, header loaded, but data not loaded, please try another (t,v) selection.\" % (self._calc_selection_size() / (1024. ** 3), self.MAX_DATA_ARRAY_SIZE / (1024. ** 3)))\n            self.data = np.array([0],dtype=self._d_type)\n            return None\n\n        #Convert input frequencies into what their corresponding channel number would be.\n        self._setup_chans()\n        #Update frequencies ranges from channel number.\n        self._setup_freqs()\n\n        self.data = self.h5[\"data\"][self.t_start:self.t_stop,:,self.chan_start_idx:self.chan_stop_idx]", "response": "Read data from the H5 file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading a single blob from a selection.", "response": "def read_blob(self,blob_dim,n_blob=0):\n        \"\"\"Read blob from a selection.\n        \"\"\"\n\n        n_blobs = self.calc_n_blobs(blob_dim)\n        if n_blob > n_blobs or n_blob < 0:\n            raise ValueError('Please provide correct n_blob value. Given %i, but max values is %i'%(n_blob,n_blobs))\n\n        #This prevents issues when the last blob is smaller than the others in time\n        if blob_dim[self.time_axis]*(n_blob+1) > self.selection_shape[self.time_axis]:\n            updated_blob_dim = (self.selection_shape[self.time_axis] - blob_dim[self.time_axis]*n_blob, 1, blob_dim[self.freq_axis])\n        else:\n            updated_blob_dim = [int(i) for i in blob_dim]\n\n        blob_start = self._find_blob_start(blob_dim, n_blob)\n        blob_end = blob_start + np.array(updated_blob_dim)\n\n        blob = self.h5[\"data\"][int(blob_start[self.time_axis]):int(blob_end[self.time_axis]),\n                               :,\n                               int(blob_start[self.freq_axis]):int(blob_end[self.freq_axis])\n                               ]\n\n#         if self.header[b'foff'] < 0:\n#             blob = blob[:,:,::-1]\n\n        return blob"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading blimpy header and return a Python dictionary of key value pairs.", "response": "def read_header(self, return_idxs=False):\n        \"\"\" Read blimpy header and return a Python dictionary of key:value pairs\n\n        Args:\n            filename (str): name of file to open\n\n        Optional args:\n            return_idxs (bool): Default False. If true, returns the file offset indexes\n                                for values\n\n        Returns:\n            Python dict of key:value pairs, OR returns file offset indexes for values.\n\n        \"\"\"\n        self.header = sigproc.read_header(self.filename, return_idxs=return_idxs)\n        return self.header"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads data from the file and store it in self. data.", "response": "def read_data(self, f_start=None, f_stop=None,t_start=None, t_stop=None):\n        \"\"\" Read data.\n        \"\"\"\n\n        self._setup_selection_range(f_start=f_start, f_stop=f_stop, t_start=t_start, t_stop=t_stop)\n\n        #check if selection is small enough.\n        if self.isheavy():\n            logger.warning(\"Selection size of %.2f GB, exceeding our size limit %.2f GB. Instance created, \"\n                           \"header loaded, but data not loaded, please try another (t,v) selection.\" % (self._calc_selection_size() / (1024. ** 3), self.MAX_DATA_ARRAY_SIZE / (1024. ** 3)))\n            self.data = np.array([0],dtype=self._d_type)\n            return None\n\n        #Convert input frequencies into what their corresponding channel number would be.\n        self._setup_chans()\n        #Update frequencies ranges from channel number.\n        self._setup_freqs()\n\n        n_chans = self.header[b'nchans']\n        n_chans_selected = self.selection_shape[self.freq_axis]\n        n_ifs   = self.header[b'nifs']\n\n        # Load binary data\n        f = open(self.filename, 'rb')\n        f.seek(int(self.idx_data))\n\n        # now check to see how many integrations requested\n        n_ints = self.t_stop - self.t_start\n\n        # Seek to first integration\n        f.seek(int(self.t_start * self._n_bytes  * n_ifs * n_chans), 1)\n\n        #Loading  data\n        self.data = np.zeros((n_ints, n_ifs, n_chans_selected), dtype=self._d_type)\n\n        for ii in range(n_ints):\n            for jj in range(n_ifs):\n                f.seek(int(self._n_bytes  * self.chan_start_idx), 1) # 1 = from current location\n                dd = np.fromfile(f, count=n_chans_selected, dtype=self._d_type)\n\n                # Reverse array if frequency axis is flipped\n#                     if self.header[b'foff'] < 0:\n#                         dd = dd[::-1]\n\n                self.data[ii, jj] = dd\n\n                f.seek(int(self._n_bytes  * (n_chans - self.chan_stop_idx)), 1)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _find_blob_start(self):\n\n        # Convert input frequencies into what their corresponding channel number would be.\n        self._setup_chans()\n\n        # Check which is the blob time offset\n        blob_time_start = self.t_start\n\n        # Check which is the blob frequency offset (in channels)\n        blob_freq_start = self.chan_start_idx\n\n        blob_start = blob_time_start * self.n_channels_in_file + blob_freq_start\n\n        return blob_start", "response": "Find first blob from selection."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_blob(self,blob_dim,n_blob=0):\n\n        n_blobs = self.calc_n_blobs(blob_dim)\n        if n_blob > n_blobs or n_blob < 0:\n            raise ValueError('Please provide correct n_blob value. Given %i, but max values is %i'%(n_blob,n_blobs))\n\n        # This prevents issues when the last blob is smaller than the others in time.\n        if blob_dim[self.time_axis]*(n_blob+1) > self.selection_shape[self.time_axis]:\n            updated_blob_dim = (int(self.selection_shape[self.time_axis] - blob_dim[self.time_axis]*n_blob), 1, int(blob_dim[self.freq_axis]))\n        else:\n            updated_blob_dim = [int(i) for i in blob_dim]\n\n        blob_start = self._find_blob_start()\n        blob = np.zeros(updated_blob_dim, dtype=self._d_type)\n\n        # EE: For now; also assuming one polarization and one beam.\n\n        # Assuming the blob will loop over the whole frequency range.\n        if self.f_start == self.f_begin and self.f_stop == self.f_end:\n\n            blob_flat_size = np.prod(blob_dim)\n            updated_blob_flat_size = np.prod(updated_blob_dim)\n\n            # Load binary data\n            with open(self.filename, 'rb') as f:\n                f.seek(int(self.idx_data + self._n_bytes  * (blob_start + n_blob*blob_flat_size)))\n                dd = np.fromfile(f, count=updated_blob_flat_size, dtype=self._d_type)\n\n            if dd.shape[0] == updated_blob_flat_size:\n                blob = dd.reshape(updated_blob_dim)\n            else:\n                logger.info('DD shape != blob shape.')\n                blob = dd.reshape((int(dd.shape[0]/blob_dim[self.freq_axis]),blob_dim[self.beam_axis],blob_dim[self.freq_axis]))\n        else:\n\n            for blobt in range(updated_blob_dim[self.time_axis]):\n\n                #Load binary data\n                with open(self.filename, 'rb') as f:\n                    f.seek(int(self.idx_data + self._n_bytes * (blob_start + n_blob*blob_dim[self.time_axis]*self.n_channels_in_file + blobt*self.n_channels_in_file)))\n                    dd = np.fromfile(f, count=blob_dim[self.freq_axis], dtype=self._d_type)\n\n                blob[blobt] = dd\n\n#         if self.header[b'foff'] < 0:\n#             blob = blob[:,:,::-1]\n\n        return blob", "response": "Read a binary blob from a selection."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_all(self,reverse=True):\n        raise NotImplementedError('To be implemented')\n\n        # go to start of the data\n        self.filfile.seek(int(self.datastart))\n        # read data into 2-D numpy array\n#        data=np.fromfile(self.filfile,dtype=self.dtype).reshape(self.channels,self.blocksize,order='F')\n        data=np.fromfile(self.filfile,dtype=self.dtype).reshape(self.blocksize, self.channels)\n        if reverse:\n            data = data[:,::-1]\n        return data", "response": "read all the data into a 2 - D numpy array."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread a single row of data into a 2 - D numpy array.", "response": "def read_row(self,rownumber,reverse=True):\n        \"\"\" Read a block of data. The number of samples per row is set in self.channels\n            If reverse=True the x axis is flipped.\n        \"\"\"\n        raise NotImplementedError('To be implemented')\n\n        # go to start of the row\n        self.filfile.seek(int(self.datastart+self.channels*rownumber*(int(self.nbits/8))))\n        # read data into 2-D numpy array\n        data=np.fromfile(self.filfile,count=self.channels,dtype=self.dtype).reshape(1, self.channels)\n        if reverse:\n            data = data[:,::-1]\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cmd_tool(args=None):\n\n    from argparse import ArgumentParser\n\n    parser = ArgumentParser(description=\"Command line utility for reading and plotting blimpy files.\")\n\n    parser.add_argument('filename', type=str,\n                        help='Name of file to read')\n    parser.add_argument('-p', action='store',  default='a', dest='what_to_plot', type=str,\n                        help='Show: \"w\" waterfall (freq vs. time) plot; \"s\" integrated spectrum plot; \\\n                        \"t\" for time series; \"mm\" for spectrum including min max; \"k\" for kurtosis; \\\n                        \"a\" for all available plots and information; and \"ank\" for all but kurtosis.')\n    parser.add_argument('-b', action='store', default=None, dest='f_start', type=float,\n                        help='Start frequency (begin), in MHz')\n    parser.add_argument('-e', action='store', default=None, dest='f_stop', type=float,\n                        help='Stop frequency (end), in MHz')\n    parser.add_argument('-B', action='store', default=None, dest='t_start', type=int,\n                        help='Start integration (begin, inclusive) ID ')\n    parser.add_argument('-E', action='store', default=None, dest='t_stop', type=int,\n                        help='Stop integration (end, exclusive) ID')\n    parser.add_argument('-i', action='store_true', default=False, dest='info_only',\n                        help='Show info only')\n    parser.add_argument('-a', action='store_true', default=False, dest='average',\n                       help='average along time axis (plot spectrum only)')\n    parser.add_argument('-s', action='store', default='', dest='plt_filename', type=str,\n                       help='save plot graphic to file (give filename as argument)')\n    parser.add_argument('-S', action='store_true', default=False, dest='save_only',\n                       help='Turn off plotting of data and only save to file.')\n    parser.add_argument('-D', action='store_false', default=True, dest='blank_dc',\n                       help='Use to not blank DC bin.')\n    parser.add_argument('-H', action='store_true', default=False, dest='to_hdf5',\n                       help='Write file to hdf5 format.')\n    parser.add_argument('-F', action='store_true', default=False, dest='to_fil',\n                       help='Write file to .fil format.')\n    parser.add_argument('-o', action='store', default=None, dest='filename_out', type=str,\n                        help='Filename output (if not probided, the name will be the same but with apropiate extension).')\n    parser.add_argument('-l', action='store', default=None, dest='max_load', type=float,\n                        help='Maximum data limit to load. Default:1GB')\n\n    if args is None:\n        args = sys.argv[1:]\n\n    parse_args = parser.parse_args(args)\n\n    # Open blimpy data\n    filename = parse_args.filename\n    load_data = not parse_args.info_only\n    info_only = parse_args.info_only\n    filename_out = parse_args.filename_out\n\n    fil = Waterfall(filename, f_start=parse_args.f_start, f_stop=parse_args.f_stop, t_start=parse_args.t_start, t_stop=parse_args.t_stop, load_data=load_data, max_load=parse_args.max_load)\n    fil.info()\n\n    #Check the size of selection.\n    if fil.container.isheavy() or parse_args.to_hdf5 or parse_args.to_fil:\n        info_only = True\n\n    # And if we want to plot data, then plot data.\n\n    if not info_only:\n        print('')\n\n        if parse_args.blank_dc:\n            logger.info(\"Blanking DC bin\")\n            n_coarse_chan = fil.calc_n_coarse_chan()\n            fil.blank_dc(n_coarse_chan)\n\n        if parse_args.what_to_plot == \"w\":\n            plt.figure(\"waterfall\", figsize=(8, 6))\n            fil.plot_waterfall(f_start=parse_args.f_start, f_stop=parse_args.f_stop)\n        elif parse_args.what_to_plot == \"s\":\n            plt.figure(\"Spectrum\", figsize=(8, 6))\n            fil.plot_spectrum(logged=True, f_start=parse_args.f_start, f_stop=parse_args.f_stop, t='all')\n        elif parse_args.what_to_plot == \"mm\":\n            plt.figure(\"min max\", figsize=(8, 6))\n            fil.plot_spectrum_min_max(logged=True, f_start=parse_args.f_start, f_stop=parse_args.f_stop, t='all')\n        elif parse_args.what_to_plot == \"k\":\n            plt.figure(\"kurtosis\", figsize=(8, 6))\n            fil.plot_kurtosis(f_start=parse_args.f_start, f_stop=parse_args.f_stop)\n        elif parse_args.what_to_plot == \"t\":\n            plt.figure(\"Time Series\", figsize=(8, 6))\n            fil.plot_time_series(f_start=parse_args.f_start, f_stop=parse_args.f_stop,orientation='h')\n        elif parse_args.what_to_plot == \"a\":\n            plt.figure(\"Multiple diagnostic plots\", figsize=(12, 9),facecolor='white')\n            fil.plot_all(logged=True, f_start=parse_args.f_start, f_stop=parse_args.f_stop, t='all')\n        elif parse_args.what_to_plot == \"ank\":\n            plt.figure(\"Multiple diagnostic plots\", figsize=(12, 9),facecolor='white')\n            fil.plot_all(logged=True, f_start=parse_args.f_start, f_stop=parse_args.f_stop, t='all',kutosis=False)\n\n        if parse_args.plt_filename != '':\n            plt.savefig(parse_args.plt_filename)\n\n        if not parse_args.save_only:\n            if 'DISPLAY' in os.environ.keys():\n                plt.show()\n            else:\n                logger.warning(\"No $DISPLAY available.\")\n\n    else:\n\n        if parse_args.to_hdf5 and parse_args.to_fil:\n            raise Warning('Either provide to_hdf5 or to_fil, but not both.')\n\n        if parse_args.to_hdf5:\n            if not filename_out:\n                filename_out = filename.replace('.fil','.h5')\n            elif '.h5' not in filename_out:\n                filename_out = filename_out.replace('.fil','')+'.h5'\n\n            logger.info('Writing file : %s'%(filename_out))\n            fil.write_to_hdf5(filename_out)\n            logger.info('File written.')\n\n        if parse_args.to_fil:\n            if not filename_out:\n                filename_out = filename.replace('.h5','.fil')\n            elif '.fil' not in filename_out:\n                filename_out = filename_out.replace('.h5','')+'.fil'\n\n            logger.info('Writing file : %s'%(filename_out))\n            fil.write_to_fil(filename_out)\n            logger.info('File written.')", "response": "Command line tool for reading and plotting blimpy files."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_data(self, f_start=None, f_stop=None,t_start=None, t_stop=None):\n\n        self.container.read_data(f_start=f_start, f_stop=f_stop,t_start=t_start, t_stop=t_stop)\n\n        self.__load_data()", "response": "Reads data selection if small enough."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __update_header(self):\n\n        #Updating frequency of first channel from selection\n        if self.header[b'foff'] < 0:\n            self.header[b'fch1'] = self.container.f_stop\n        else:\n            self.header[b'fch1'] = self.container.f_start\n\n        #Updating number of coarse channels.\n        self.header[b'nchans'] = self.container.selection_shape[self.freq_axis]\n\n        #Updating time stamp for first time bin from selection\n        self.header[b'tstart'] = self.container.populate_timestamps(update_header=True)", "response": "Updates the header information from the original file to the selection."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprints header information and other derived information.", "response": "def info(self):\n        \"\"\" Print header information and other derived information. \"\"\"\n\n        print(\"\\n--- File Info ---\")\n\n        for key, val in self.file_header.items():\n            if key == 'src_raj':\n                val = val.to_string(unit=u.hour, sep=':')\n            if key == 'src_dej':\n                val = val.to_string(unit=u.deg, sep=':')\n            print(\"%16s : %32s\" % (key, val))\n\n        print(\"\\n%16s : %32s\" % (\"Num ints in file\", self.n_ints_in_file))\n        print(\"%16s : %32s\" % (\"File shape\", self.file_shape))\n        print(\"--- Selection Info ---\")\n        print(\"%16s : %32s\" % (\"Data selection shape\", self.selection_shape))\n        print(\"%16s : %32s\" % (\"Minimum freq (MHz)\", self.container.f_start))\n        print(\"%16s : %32s\" % (\"Maximum freq (MHz)\", self.container.f_stop))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites data to a. fil file.", "response": "def write_to_fil(self, filename_out, *args, **kwargs):\n        \"\"\" Write data to .fil file.\n            It check the file size then decides how to write the file.\n\n        Args:\n            filename_out (str): Name of output file\n        \"\"\"\n\n        #For timing how long it takes to write a file.\n        t0 = time.time()\n\n        #Update header\n        self.__update_header()\n\n        if self.container.isheavy():\n            self.__write_to_fil_heavy(filename_out)\n        else:\n            self.__write_to_fil_light(filename_out)\n\n        t1 = time.time()\n        logger.info('Conversion time: %2.2fsec' % (t1- t0))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite data to. fil file.", "response": "def __write_to_fil_heavy(self, filename_out, *args, **kwargs):\n        \"\"\" Write data to .fil file.\n\n        Args:\n            filename_out (str): Name of output file\n        \"\"\"\n\n        #Note that a chunk is not a blob!!\n        chunk_dim = self.__get_chunk_dimensions()\n        blob_dim = self.__get_blob_dimensions(chunk_dim)\n        n_blobs = self.container.calc_n_blobs(blob_dim)\n\n        #Write header of .fil file\n        n_bytes  = self.header[b'nbits'] / 8\n        with open(filename_out, \"wb\") as fileh:\n            fileh.write(generate_sigproc_header(self)) #generate_sigproc_header comes from sigproc.py\n\n        logger.info('Using %i n_blobs to write the data.'% n_blobs)\n        for ii in range(0, n_blobs):\n            logger.info('Reading %i of %i' % (ii + 1, n_blobs))\n\n            bob = self.container.read_blob(blob_dim,n_blob=ii)\n\n            #Write data of .fil file.\n            with open(filename_out, \"a\") as fileh:\n                j = bob\n                if n_bytes == 4:\n                    np.float32(j.ravel()).tofile(fileh)\n                elif n_bytes == 2:\n                    np.int16(j.ravel()).tofile(fileh)\n                elif n_bytes == 1:\n                    np.int8(j.ravel()).tofile(fileh)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite data to. fil file.", "response": "def __write_to_fil_light(self, filename_out, *args, **kwargs):\n        \"\"\" Write data to .fil file.\n\n        Args:\n            filename_out (str): Name of output file\n        \"\"\"\n\n        n_bytes  = self.header[b'nbits'] / 8\n        with open(filename_out, \"wb\") as fileh:\n            fileh.write(generate_sigproc_header(self)) #generate_sigproc_header comes from sigproc.py\n            j = self.data\n            if n_bytes == 4:\n                np.float32(j.ravel()).tofile(fileh)\n            elif n_bytes == 2:\n                np.int16(j.ravel()).tofile(fileh)\n            elif n_bytes == 1:\n                np.int8(j.ravel()).tofile(fileh)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites data to HDF5 file.", "response": "def write_to_hdf5(self, filename_out, *args, **kwargs):\n        \"\"\" Write data to HDF5 file.\n            It check the file size then decides how to write the file.\n\n        Args:\n            filename_out (str): Name of output file\n        \"\"\"\n\n        #For timing how long it takes to write a file.\n        t0 = time.time()\n\n        #Update header\n        self.__update_header()\n\n        if self.container.isheavy():\n            self.__write_to_hdf5_heavy(filename_out)\n        else:\n            self.__write_to_hdf5_light(filename_out)\n\n        t1 = time.time()\n        logger.info('Conversion time: %2.2fsec' % (t1- t0))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __write_to_hdf5_heavy(self, filename_out, *args, **kwargs):\n\n        block_size = 0\n\n        #Note that a chunk is not a blob!!\n        chunk_dim = self.__get_chunk_dimensions()\n        blob_dim = self.__get_blob_dimensions(chunk_dim)\n        n_blobs = self.container.calc_n_blobs(blob_dim)\n\n        with h5py.File(filename_out, 'w') as h5:\n\n            h5.attrs[b'CLASS'] = b'FILTERBANK'\n            h5.attrs[b'VERSION'] = b'1.0'\n\n            if HAS_BITSHUFFLE:\n                bs_compression = bitshuffle.h5.H5FILTER\n                bs_compression_opts = (block_size, bitshuffle.h5.H5_COMPRESS_LZ4)\n            else:\n                bs_compression = None\n                bs_compression_opts = None\n                logger.warning(\"Warning: bitshuffle not found. No compression applied.\")\n\n            dset = h5.create_dataset('data',\n                            shape=self.selection_shape,\n                            chunks=chunk_dim,\n                            compression=bs_compression,\n                            compression_opts=bs_compression_opts,\n                            dtype=self.data.dtype)\n\n            dset_mask = h5.create_dataset('mask',\n                            shape=self.selection_shape,\n                            chunks=chunk_dim,\n                            compression=bs_compression,\n                            compression_opts=bs_compression_opts,\n                            dtype='uint8')\n\n            dset.dims[0].label = b\"frequency\"\n            dset.dims[1].label = b\"feed_id\"\n            dset.dims[2].label = b\"time\"\n\n            dset_mask.dims[0].label = b\"frequency\"\n            dset_mask.dims[1].label = b\"feed_id\"\n            dset_mask.dims[2].label = b\"time\"\n\n            # Copy over header information as attributes\n            for key, value in self.header.items():\n                dset.attrs[key] = value\n\n            if blob_dim[self.freq_axis] < self.selection_shape[self.freq_axis]:\n\n                logger.info('Using %i n_blobs to write the data.'% n_blobs)\n                for ii in range(0, n_blobs):\n                    logger.info('Reading %i of %i' % (ii + 1, n_blobs))\n\n                    bob = self.container.read_blob(blob_dim,n_blob=ii)\n\n                    #-----\n                    #Using channels instead of frequency.\n                    c_start = self.container.chan_start_idx + ii*blob_dim[self.freq_axis]\n                    t_start = self.container.t_start + (c_start/self.selection_shape[self.freq_axis])*blob_dim[self.time_axis]\n                    t_stop = t_start + blob_dim[self.time_axis]\n\n                    # Reverse array if frequency axis is flipped\n#                     if self.header['foff'] < 0:\n#                         c_stop = self.selection_shape[self.freq_axis] - (c_start)%self.selection_shape[self.freq_axis]\n#                         c_start = c_stop - blob_dim[self.freq_axis]\n#                     else:\n                    c_start = (c_start)%self.selection_shape[self.freq_axis]\n                    c_stop = c_start + blob_dim[self.freq_axis]\n                    #-----\n\n                    logger.debug(t_start,t_stop,c_start,c_stop)\n\n                    dset[t_start:t_stop,0,c_start:c_stop] = bob[:]\n\n            else:\n\n                logger.info('Using %i n_blobs to write the data.'% n_blobs)\n                for ii in range(0, n_blobs):\n                    logger.info('Reading %i of %i' % (ii + 1, n_blobs))\n\n                    bob = self.container.read_blob(blob_dim,n_blob=ii)\n                    t_start = self.container.t_start + ii*blob_dim[self.time_axis]\n\n                    #This prevents issues when the last blob is smaller than the others in time\n                    if (ii+1)*blob_dim[self.time_axis] > self.n_ints_in_file:\n                        t_stop = self.n_ints_in_file\n                    else:\n                        t_stop = (ii+1)*blob_dim[self.time_axis]\n\n                    dset[t_start:t_stop] = bob[:]", "response": "Write data to HDF5 file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite data to HDF5 file in one go.", "response": "def __write_to_hdf5_light(self, filename_out, *args, **kwargs):\n        \"\"\" Write data to HDF5 file in one go.\n\n        Args:\n            filename_out (str): Name of output file\n        \"\"\"\n\n        block_size = 0\n\n        with h5py.File(filename_out, 'w') as h5:\n\n            h5.attrs[b'CLASS']   = b'FILTERBANK'\n            h5.attrs[b'VERSION'] = b'1.0'\n\n            if HAS_BITSHUFFLE:\n                bs_compression = bitshuffle.h5.H5FILTER\n                bs_compression_opts = (block_size, bitshuffle.h5.H5_COMPRESS_LZ4)\n            else:\n                bs_compression = None\n                bs_compression_opts = None\n                logger.warning(\"Warning: bitshuffle not found. No compression applied.\")\n\n\n            dset = h5.create_dataset('data',\n                        data=self.data,\n#                          compression='lzf')\n                        compression=bs_compression,\n                        compression_opts=bs_compression_opts)\n\n            dset_mask = h5.create_dataset('mask',\n                        shape=self.file_shape,\n#                                 compression='lzf',\n                        compression=bs_compression,\n                        compression_opts=bs_compression_opts,\n                        dtype='uint8')\n\n            dset.dims[0].label = b\"frequency\"\n            dset.dims[1].label = b\"feed_id\"\n            dset.dims[2].label = b\"time\"\n\n            dset_mask.dims[0].label = b\"frequency\"\n            dset_mask.dims[1].label = b\"feed_id\"\n            dset_mask.dims[2].label = b\"time\"\n\n            # Copy over header information as attributes\n            for key, value in self.header.items():\n                dset.attrs[key] = value"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the blob dimmentions", "response": "def __get_blob_dimensions(self, chunk_dim):\n        \"\"\" Sets the blob dimmentions, trying to read around 1024 MiB at a time.\n            This is assuming a chunk is about 1 MiB.\n        \"\"\"\n\n        #Taking the size into consideration, but avoiding having multiple blobs within a single time bin.\n        if self.selection_shape[self.freq_axis] > chunk_dim[self.freq_axis]*MAX_BLOB_MB:\n            freq_axis_size = self.selection_shape[self.freq_axis]\n#             while freq_axis_size > chunk_dim[self.freq_axis]*MAX_BLOB_MB:\n#                 freq_axis_size /= 2\n            time_axis_size = 1\n        else:\n            freq_axis_size = self.selection_shape[self.freq_axis]\n            time_axis_size = np.min([chunk_dim[self.time_axis] * MAX_BLOB_MB * chunk_dim[self.freq_axis] / freq_axis_size, self.selection_shape[self.time_axis]])\n\n        blob_dim = (int(time_axis_size), 1, freq_axis_size)\n\n        return blob_dim"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the chunking dimmentions depending on the file format.", "response": "def __get_chunk_dimensions(self):\n        \"\"\" Sets the chunking dimmentions depending on the file type.\n        \"\"\"\n\n        #Usually '.0000.' is in self.filename\n        if np.abs(self.header[b'foff']) < 1e-5:\n            logger.info('Detecting high frequency resolution data.')\n            chunk_dim = (1,1,1048576) #1048576 is the number of channels in a coarse channel.\n            return chunk_dim\n        #Usually '.0001.' is in self.filename\n        elif np.abs(self.header[b'tsamp']) < 1e-3:\n            logger.info('Detecting high time resolution data.')\n            chunk_dim = (2048,1,512) #512 is the total number of channels per single band (ie. blc00)\n            return chunk_dim\n        #Usually '.0002.' is in self.filename\n        elif np.abs(self.header[b'foff']) < 1e-2 and np.abs(self.header[b'foff'])  >= 1e-5:\n            logger.info('Detecting intermediate frequency and time resolution data.')\n            chunk_dim = (10,1,65536)  #65536 is the total number of channels per single band (ie. blc00)\n#            chunk_dim = (1,1,65536/4)\n            return chunk_dim\n        else:\n            logger.warning('File format not known. Will use minimum chunking. NOT OPTIMAL.')\n            chunk_dim = (1,1,512)\n            return chunk_dim"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngrab a portion of data from the file.", "response": "def grab_data(self, f_start=None, f_stop=None,t_start=None, t_stop=None, if_id=0):\n        \"\"\" Extract a portion of data by frequency range.\n\n        Args:\n            f_start (float): start frequency in MHz\n            f_stop (float): stop frequency in MHz\n            if_id (int): IF input identification (req. when multiple IFs in file)\n\n        Returns:\n            (freqs, data) (np.arrays): frequency axis in MHz and data subset\n        \"\"\"\n\n        self.freqs = self.populate_freqs()\n        self.timestamps = self.populate_timestamps()\n\n        if f_start is None:\n            f_start = self.freqs[0]\n        if f_stop is None:\n            f_stop = self.freqs[-1]\n\n        i0 = np.argmin(np.abs(self.freqs - f_start))\n        i1 = np.argmin(np.abs(self.freqs - f_stop))\n\n        if i0 < i1:\n            plot_f    = self.freqs[i0:i1 + 1]\n            plot_data = np.squeeze(self.data[t_start:t_stop, ..., i0:i1 + 1])\n        else:\n            plot_f    = self.freqs[i1:i0 + 1]\n            plot_data = np.squeeze(self.data[t_start:t_stop, ..., i1:i0 + 1])\n\n        return plot_f, plot_data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncommand line tool for plotting and viewing info on guppi raw files.", "response": "def cmd_tool(args=None):\n    \"\"\" Command line tool for plotting and viewing info on guppi raw files \"\"\"\n\n    from argparse import ArgumentParser\n\n    parser = ArgumentParser(description=\"Command line utility for creating spectra from GuppiRaw files.\")\n\n    parser.add_argument('filename', type=str, help='Name of file to read')\n    parser.add_argument('-o', dest='outdir', type=str, default='./', help='output directory for PNG files')\n    args = parser.parse_args()\n\n    r = GuppiRaw(args.filename)\n\n    r.print_stats()\n    bname = os.path.splitext(os.path.basename(args.filename))[0]\n    bname = os.path.join(args.outdir, bname)\n    r.plot_histogram(filename=\"%s_hist.png\" % bname)\n    r.plot_spectrum(filename=\"%s_spec.png\" % bname)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_header(self):\n        start_idx = self.file_obj.tell()\n        key, val = '', ''\n\n        header_dict = {}\n        keep_reading = True\n\n        first_line = self.file_obj\n\n        try:\n            while keep_reading:\n                if start_idx + 80 > self.filesize:\n                    keep_reading = False\n                    raise EndOfFileError(\"End Of Data File\")\n                line = self.file_obj.read(80)\n                if PYTHON3:\n                    line = line.decode(\"utf-8\")\n                # print line\n                if line.startswith('END'):\n                    keep_reading = False\n                    break\n                else:\n                    key, val = line.split('=')\n                    key, val = key.strip(), val.strip()\n\n                    if \"'\" in val:\n                        # Items in quotes are strings\n                        val = str(val.strip(\"'\").strip())\n                    elif \".\" in val:\n                        # Items with periods are floats (if not a string)\n                        val = float(val)\n                    else:\n                        # Otherwise it's an integer\n                        val = int(val)\n\n                header_dict[key] = val\n        except ValueError:\n            print(\"CURRENT LINE: \", line)\n            print(\"BLOCK START IDX: \", start_idx)\n            print(\"FILE SIZE: \", self.filesize)\n            print(\"NEXT 512 BYTES: \\n\")\n            print(self.file_obj.read(512))\n            raise\n\n        data_idx = self.file_obj.tell()\n\n        # Seek past padding if DIRECTIO is being used\n        if \"DIRECTIO\" in header_dict.keys():\n            if int(header_dict[\"DIRECTIO\"]) == 1:\n                if data_idx % 512:\n                    data_idx += (512 - data_idx % 512)\n\n        self.file_obj.seek(start_idx)\n        return header_dict, data_idx", "response": "Reads next header from the file and returns a dictionary of keyword - value header data and byte index of where the corresponding data block resides."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread the first header in file", "response": "def read_first_header(self):\n        \"\"\" Read first header in file\n\n        Returns:\n            header (dict): keyword:value pairs of header metadata\n        \"\"\"\n        self.file_obj.seek(0)\n        header_dict, pos = self.read_header()\n        self.file_obj.seek(0)\n        return header_dict"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_data(self):\n        with self as gr:\n            while True:\n                try:\n                    yield gr.read_next_data_block_int8()\n                except Exception as e:\n                    print(\"File depleted\")\n                    yield None, None, None", "response": "returns a generator object that reads data at a time"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread the next block of data and its header and return it as a tuple of header data and complex64.", "response": "def read_next_data_block_int8(self):\n        \"\"\" Read the next block of data and its header\n\n        Returns: (header, data)\n            header (dict): dictionary of header metadata\n            data (np.array): Numpy array of data, converted into to complex64.\n        \"\"\"\n        header, data_idx = self.read_header()\n        self.file_obj.seek(data_idx)\n\n        # Read data and reshape\n\n        n_chan = int(header['OBSNCHAN'])\n        n_pol = int(header['NPOL'])\n        n_bit = int(header['NBITS'])\n        n_samples = int(int(header['BLOCSIZE']) / (n_chan * n_pol * (n_bit / 8)))\n\n        d = np.fromfile(self.file_obj, count=header['BLOCSIZE'], dtype='int8')\n\n        # Handle 2-bit and 4-bit data\n        if n_bit != 8:\n            d = unpack(d, n_bit)\n\n        d = d.reshape((n_chan, n_samples, n_pol))  # Real, imag\n\n        if self._d_x.shape != d[..., 0:2].shape:\n            self._d_x = np.ascontiguousarray(np.zeros(d[..., 0:2].shape, dtype='int8'))\n            self._d_y = np.ascontiguousarray(np.zeros(d[..., 2:4].shape, dtype='int8'))\n\n        self._d_x[:] = d[..., 0:2]\n        self._d_y[:] = d[..., 2:4]\n        return header, self._d_x, self._d_y"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the next block of data and its header and data and convert it into complex64.", "response": "def read_next_data_block_int8_2x(self):\n        \"\"\" Read the next block of data and its header\n\n        Returns: (header, data)\n            header (dict): dictionary of header metadata\n            data (np.array): Numpy array of data, converted into to complex64.\n        \"\"\"\n        header, data_idx = self.read_header()\n        self.file_obj.seek(data_idx)\n\n        # Read data and reshape\n\n        n_chan = int(header['OBSNCHAN'])\n        n_pol = int(header['NPOL'])\n        n_bit = int(header['NBITS'])\n        n_samples = int(int(header['BLOCSIZE']) / (n_chan * n_pol * (n_bit / 8)))\n\n        d = np.fromfile(self.file_obj, count=header['BLOCSIZE'], dtype='int8')\n\n        header, data_idx = self.read_header()\n        self.file_obj.seek(data_idx)\n        d2 = np.fromfile(self.file_obj, count=header['BLOCSIZE'], dtype='int8')\n\n        # Handle 2-bit and 4-bit data\n        if n_bit != 8:\n            d = unpack(d, n_bit)\n\n        d = d.reshape((n_chan, n_samples, n_pol))  # Real, imag\n        d2 = d2.reshape((n_chan, n_samples, n_pol))\n        d = np.concatenate((d, d2), axis=1)\n        print(d.shape)\n\n        if self._d_x.shape != (n_chan, n_samples * 2, n_pol):\n            self._d_x = np.ascontiguousarray(np.zeros(d[..., 0:2].shape, dtype='int8'))\n            self._d_y = np.ascontiguousarray(np.zeros(d[..., 2:4].shape, dtype='int8'))\n\n        self._d_x[:] = d[..., 0:2]\n        self._d_y[:] = d[..., 2:4]\n        return header, self._d_x, self._d_y"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading the next block of data and its header and data.", "response": "def read_next_data_block(self):\n        \"\"\" Read the next block of data and its header\n\n        Returns: (header, data)\n            header (dict): dictionary of header metadata\n            data (np.array): Numpy array of data, converted into to complex64.\n        \"\"\"\n        header, data_idx = self.read_header()\n        self.file_obj.seek(data_idx)\n\n        # Read data and reshape\n\n        n_chan = int(header['OBSNCHAN'])\n        n_pol = int(header['NPOL'])\n        n_bit = int(header['NBITS'])\n        n_samples = int(int(header['BLOCSIZE']) / (n_chan * n_pol * (n_bit / 8)))\n\n        d = np.ascontiguousarray(np.fromfile(self.file_obj, count=header['BLOCSIZE'], dtype='int8'))\n\n        # Handle 2-bit and 4-bit data\n        if n_bit != 8:\n            d = unpack(d, n_bit)\n\n        dshape = self.read_next_data_block_shape()\n\n        d = d.reshape(dshape)  # Real, imag\n\n        if self._d.shape != d.shape:\n            self._d = np.zeros(d.shape, dtype='float32')\n\n        self._d[:] = d\n\n        return header, self._d[:].view('complex64')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_n_data_blocks(self):\n        self.file_obj.seek(0)\n        header0, data_idx0 = self.read_header()\n\n        self.file_obj.seek(data_idx0)\n        block_size = int(header0['BLOCSIZE'])\n        n_bits = int(header0['NBITS'])\n        self.file_obj.seek(int(header0['BLOCSIZE']), 1)\n        n_blocks = 1\n        end_found = False\n        while not end_found:\n            try:\n                header, data_idx = self.read_header()\n                self.file_obj.seek(data_idx)\n                self.file_obj.seek(header['BLOCSIZE'], 1)\n                n_blocks += 1\n            except EndOfFileError:\n                end_found = True\n                break\n\n        self.file_obj.seek(0)\n        return n_blocks", "response": "Seek through the file to find how many data blocks there are in the file\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef print_stats(self):\n\n        header, data = self.read_next_data_block()\n        data = data.view('float32')\n\n        print(\"AVG: %2.3f\" % data.mean())\n        print(\"STD: %2.3f\" % data.std())\n        print(\"MAX: %2.3f\" % data.max())\n        print(\"MIN: %2.3f\" % data.min())\n\n        import pylab as plt", "response": "Compute some basic stats on the next block of data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot_histogram(self, filename=None):\n        header, data = self.read_next_data_block()\n        data = data.view('float32')\n\n        plt.figure(\"Histogram\")\n        plt.hist(data.flatten(), 65, facecolor='#cc0000')\n        if filename:\n            plt.savefig(filename)\n        plt.show()", "response": "Plot a histogram of data values"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nplot the spectrum of the current logarithm of the data.", "response": "def plot_spectrum(self, filename=None, plot_db=True):\n        \"\"\" Do a (slow) numpy FFT and take power of data \"\"\"\n        header, data = self.read_next_data_block()\n\n        print(\"Computing FFT...\")\n        d_xx_fft = np.abs(np.fft.fft(data[..., 0]))\n        d_xx_fft = d_xx_fft.flatten()\n\n        # Rebin to max number of points\n        dec_fac_x = 1\n        if d_xx_fft.shape[0] > MAX_PLT_POINTS:\n            dec_fac_x = d_xx_fft.shape[0] / MAX_PLT_POINTS\n\n        d_xx_fft = rebin(d_xx_fft, dec_fac_x)\n\n        print(\"Plotting...\")\n        if plot_db:\n            plt.plot(10 * np.log10(d_xx_fft))\n            plt.ylabel(\"Power [dB]\")\n        else:\n            plt.plot(d_xx_fft)\n            plt.ylabel(\"Power\")\n            plt.xlabel(\"Channel\")\n        plt.title(self.filename)\n        if filename:\n            plt.savefig(filename)\n        plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate_filterbank_header(self, nchans=1, ):\n        gp_head = self.read_first_header()\n        fb_head = {}\n\n        telescope_str = gp_head.get(\"TELESCOP\", \"unknown\")\n        if telescope_str in ('GBT', 'GREENBANK'):\n            fb_head[\"telescope_id\"] = 6\n        elif telescope_str in ('PKS', 'PARKES'):\n            fb_head[\"telescop_id\"] = 7\n        else:\n            fb_head[\"telescop_id\"] = 0\n\n        # Using .get() method allows us to fill in default values if not present\n        fb_head[\"source_name\"] = gp_head.get(\"SRC_NAME\", \"unknown\")\n        fb_head[\"az_start\"] = gp_head.get(\"AZ\", 0)\n        fb_head[\"za_start\"] = gp_head.get(\"ZA\", 0)\n\n        fb_head[\"src_raj\"] = Angle(str(gp_head.get(\"RA\", 0.0)) + \"hr\")\n        fb_head[\"src_dej\"] = Angle(str(gp_head.get(\"DEC\", 0.0)) + \"deg\")\n        fb_head[\"rawdatafile\"] = self.filename\n\n        # hardcoded\n        fb_head[\"machine_id\"] = 20\n        fb_head[\"data_type\"] = 1  # blio datatype\n        fb_head[\"barycentric\"] = 0\n        fb_head[\"pulsarcentric\"] = 0\n        fb_head[\"nbits\"] = 32\n\n        # TODO - compute these values. Need to figure out the correct calcs\n        fb_head[\"tstart\"] = 0.0\n        fb_head[\"tsamp\"] = 1.0\n        fb_head[\"fch1\"] = 0.0\n        fb_head[\"foff\"] = 187.5 / nchans\n\n        # Need to be updated based on output specs\n        fb_head[\"nchans\"] = nchans\n        fb_head[\"nifs\"] = 1\n        fb_head[\"nbeams\"] = 1\n\n        return fb_head", "response": "Generate a blimpy header dictionary for the filterbank."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_header_size(filename):\n    ''' Script to find the header size of a filterbank file'''\n\n    # open datafile\n    filfile=open(filename,'rb')\n    # go to the start of the file\n    filfile.seek(0)\n    #read some region larger than the header.\n    round1 = filfile.read(1000)\n    headersize = round1.find('HEADER_END')+len('HEADER_END')\n\n    return headersize", "response": "Script to find the header size of a filterbank file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncommanding line tool to make a md5sum comparison of two. fil files.", "response": "def cmd_tool(args=None):\n    \"\"\" Command line tool to make a md5sum comparison of two .fil files. \"\"\"\n\n    if 'bl' in local_host:\n        header_loc = '/usr/local/sigproc/bin/header' #Current location of header command in GBT.\n    else:\n        raise IOError('Script only able to run in BL systems.')\n\n    p = OptionParser()\n    p.set_usage('matchfils <FIL_FILE1> <FIL_FILE2>')\n    opts, args = p.parse_args(sys.argv[1:])\n\n    file1 = args[0]\n    file2 = args[1]\n\n    #------------------------------------\n    #Create batch script\n\n    make_batch_script()\n\n    #------------------------------------\n    #First checksum\n\n    headersize1 = find_header_size(file1)\n    file_size1 = os.path.getsize(file1)\n\n    #Strip header from file, and calculate the md5sum of the rest.\n    #command=['tail','-c',str(file_size1-headersize1),file1,'|','md5sum']\n    command=['./tail_sum.sh',file1,str(file_size1-headersize1)]\n    print('[matchfils] '+' '.join(command))\n\n    proc = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    (out, err) = proc.communicate()\n\n    check_sum1 = out.split()[0]\n    print('[matchfils] Checksum is:', check_sum1)\n\n    if err:\n        raise Error('There is an error.')\n\n    #---\n    out,err = reset_outs()\n\n    command=[header_loc,file1]\n    print('[matchfils] Header information:')\n\n    proc = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    (out, err) = proc.communicate()\n\n    header1 = out\n    print(header1)\n\n    #------------------------------------\n    #Second checksum\n\n    out,err = reset_outs()\n\n    headersize2 = find_header_size(file2)\n    file_size2 = os.path.getsize(file2)\n\n    #Strip header from file, and calculate the md5sum of the rest.\n    command=['./tail_sum.sh',file2,str(file_size2-headersize2)]\n    print('[matchfils] '+' '.join(command))\n\n    proc = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    (out, err) = proc.communicate()\n\n    check_sum2 = out.split()[0]\n    print('[matchfils] Checksum is:', check_sum2)\n\n    if err:\n        raise Error('There is an error.')\n\n    #---\n    out,err = reset_outs()\n\n    command=[header_loc,file2]\n    print('[matchfils] Header information:')\n\n    proc = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    (out, err) = proc.communicate()\n\n    header2 = out\n    print(header2)\n\n    #------------------------------------\n    #check the checksums\n\n    if check_sum1 != check_sum2:\n        print('[matchfils] Booo! Checksum does not match between files.')\n    else:\n        print('[matchfils] Hooray! Checksum matches between files.')\n\n    #------------------------------------\n    #Remove batch script\n\n    os.remove('tail_sum.sh')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_h5_file(filename,out_dir='./', new_filename = None, max_load = None):\n    ''' Converts file to HDF5 (.h5) format. Default saves output in current dir.\n    '''\n\n    fil_file = Waterfall(filename, max_load = max_load)\n    if not new_filename:\n        new_filename = out_dir+filename.replace('.fil','.h5').split('/')[-1]\n\n    if '.h5' not in new_filename:\n        new_filename = new_filename+'.h5'\n\n    fil_file.write_to_hdf5(new_filename)", "response": "Converts a file to HDF5 format."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cmd_tool(args=None):\n    from argparse import ArgumentParser\n\n    if not HAS_BITSHUFFLE:\n        print(\"Error: the bitshuffle library is required to run this script.\")\n        exit()\n\n    parser = ArgumentParser(description=\"Command line utility for creating HDF5 Raw files.\")\n    parser.add_argument('filename', type=str, help='Name of filename to read')\n    args = parser.parse_args()\n\n    fileroot = args.filename.split('.0000.raw')[0]\n\n    filelist = glob.glob(fileroot + '*.raw')\n    filelist = sorted(filelist)\n\n\n    # Read first file\n    r = GuppiRaw(filelist[0])\n    header, data = r.read_next_data_block()\n    dshape = data.shape #r.read_next_data_block_shape()\n    print(dshape)\n\n    n_blocks_total = 0\n    for filename in filelist:\n        print(filename)\n        r = GuppiRaw(filename)\n        n_blocks_total += r.n_blocks\n    print(n_blocks_total)\n\n    full_dshape = np.concatenate(((n_blocks_total,), dshape))\n\n\n    # Create h5py file\n    h5 = h5py.File(fileroot + '.h5', 'w')\n    h5.attrs['CLASS'] = 'GUPPIRAW'\n    block_size = 0      # This is chunk block size\n    dset = h5.create_dataset('data',\n                  shape=full_dshape,\n                  #compression=bitshuffle.h5.H5FILTER,\n                  #compression_opts=(block_size, bitshuffle.h5.H5_COMPRESS_LZ4),\n                  dtype=data.dtype) \n\n    h5_idx = 0\n    for filename in filelist:\n        print(\"\\nReading %s header...\" % filename)\n        r = GuppiRaw(filename)\n        h5 = h5py.File(filename + '.h5', 'w')\n    \n        header, data = r.read_next_data_block()\n        \n        for ii in range(0, r.n_blocks):\n            t0 = time.time()\n            print(\"Reading block %i of %i\" % (h5_idx+1, full_dshape[0]))\n            header, data = r.read_next_data_block()\n            t1 = time.time()\n        \n            t2 = time.time()\n            print(\"Writing block %i of %i\" % (h5_idx+1, full_dshape[0]))\n            dset[h5_idx, :] = data\n            t3 = time.time()\n            print(\"Read: %2.2fs, Write %2.2fs\" % ((t1-t0), (t3-t2)))\n        \n            h5_idx += 1\n\n            # Copy over header information as attributes\n            for key, value in header.items():\n                dset.attrs[key] = value\n\n        h5.close()\n\n        t1 = time.time()\n        print(\"Conversion time: %2.2fs\" % (t1- t0))", "response": "Command line tool for converting guppi raw into HDF5 versions of guppi raw"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef foldcal(data,tsamp, diode_p=0.04,numsamps=1000,switch=False,inds=False):\n    '''\n    Returns time-averaged spectra of the ON and OFF measurements in a\n    calibrator measurement with flickering noise diode\n\n    Parameters\n    ----------\n    data : 2D Array object (float)\n        2D dynamic spectrum for data (any Stokes parameter) with flickering noise diode.\n    tsamp : float\n        Sampling time of data in seconds\n    diode_p : float\n        Period of the flickering noise diode in seconds\n    numsamps : int\n        Number of samples over which to average noise diode ON and OFF\n    switch : boolean\n        Use switch=True if the noise diode \"skips\" turning from OFF to ON once or vice versa\n    inds : boolean\n        Use inds=True to also return the indexes of the time series where the ND is ON and OFF\n    '''\n\n    halfper = diode_p/2.0\n\n    foldt = halfper/tsamp   #number of time samples per diode switch\n\n    onesec = 1/tsamp    #number of time samples in the first second\n\n    #Find diode switches in units of time samples and round down to the nearest int\n    ints = np.arange(0,numsamps)\n    t_switch = (onesec+ints*foldt)\n    t_switch = t_switch.astype('int')\n\n    ONints = np.array(np.reshape(t_switch[:],(numsamps/2,2)))\n    ONints[:,0] = ONints[:,0]+1   #Find index ranges of ON time samples\n\n    OFFints = np.array(np.reshape(t_switch[1:-1],(numsamps/2-1,2)))\n    OFFints[:,0] = OFFints[:,0]+1   #Find index ranges of OFF time samples\n\n    av_ON = []\n    av_OFF = []\n\n    #Average ON and OFF spectra separately with respect to time\n    for i in ONints:\n        if i[1]!=i[0]:\n            av_ON.append(np.sum(data[i[0]:i[1],:,:],axis=0)/(i[1]-i[0]))\n\n    for i in OFFints:\n        if i[1]!=i[0]:\n            av_OFF.append(np.sum(data[i[0]:i[1],:,:],axis=0)/(i[1]-i[0]))\n\n    #If switch=True, flip the return statement since ON is actually OFF\n    if switch==False:\n        if inds==False:\n            return np.squeeze(np.mean(av_ON,axis=0)), np.squeeze(np.mean(av_OFF,axis=0))\n        else:\n            return np.squeeze(np.mean(av_ON,axis=0)), np.squeeze(np.mean(av_OFF,axis=0)),ONints,OFFints\n    if switch==True:\n        if inds==False:\n            return np.squeeze(np.mean(av_OFF,axis=0)), np.squeeze(np.mean(av_ON,axis=0))\n        else:\n            return np.squeeze(np.mean(av_OFF,axis=0)), np.squeeze(np.mean(av_ON,axis=0)),OFFints,ONints", "response": "This function calculates the time - averaged spectra of the ON and OFF measurements in a single calendar file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nintegrates over each core channel of a given spectrum and frequencies.", "response": "def integrate_chans(spec,freqs,chan_per_coarse):\n    '''\n    Integrates over each core channel of a given spectrum.\n    Important for calibrating data with frequency/time resolution different from noise diode data\n\n    Parameters\n    ----------\n    spec : 1D Array (float)\n        Spectrum (any Stokes parameter) to be integrated\n    freqs : 1D Array (float)\n        Frequency values for each bin of the spectrum\n    chan_per_coarse: int\n        Number of frequency bins per coarse channel\n    '''\n\n    num_coarse = spec.size/chan_per_coarse     #Calculate total number of coarse channels\n\n    #Rearrange spectrum by coarse channel\n    spec_shaped = np.array(np.reshape(spec,(num_coarse,chan_per_coarse)))\n    freqs_shaped = np.array(np.reshape(freqs,(num_coarse,chan_per_coarse)))\n\n    #Average over coarse channels\n    return np.mean(spec_shaped[:,1:-1],axis=1)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nintegrating the frequency bins of a single noise diode into two different components.", "response": "def integrate_calib(name,chan_per_coarse,fullstokes=False,**kwargs):\n    '''\n    Folds Stokes I noise diode data and integrates along coarse channels\n\n    Parameters\n    ----------\n    name : str\n        Path to noise diode filterbank file\n    chan_per_coarse : int\n        Number of frequency bins per coarse channel\n    fullstokes : boolean\n        Use fullstokes=True if data is in IQUV format or just Stokes I, use fullstokes=False if\n        it is in cross_pols format\n    '''\n    #Load data\n    obs = Waterfall(name,max_load=150)\n    data = obs.data\n\n    #If the data has cross_pols format calculate Stokes I\n    if fullstokes==False and data.shape[1]>1:\n        data = data[:,0,:]+data[:,1,:]\n        data = np.expand_dims(data,axis=1)\n    #If the data has IQUV format get Stokes I\n    if fullstokes==True:\n        data = data[:,0,:]\n        data = np.expand_dims(data,axis=1)\n\n    tsamp = obs.header['tsamp']\n\n    #Calculate ON and OFF values\n    OFF,ON = foldcal(data,tsamp,**kwargs)\n\n    freqs = obs.populate_freqs()\n\n    #Find ON and OFF spectra by coarse channel\n    ON_int = integrate_chans(ON,freqs,chan_per_coarse)\n    OFF_int = integrate_chans(OFF,freqs,chan_per_coarse)\n\n    #If \"ON\" is actually \"OFF\" switch them\n    if np.sum(ON_int)<np.sum(OFF_int):\n        temp = ON_int\n        ON_int = OFF_int\n        OFF_int = temp\n\n    #Return coarse channel spectrum of OFF and ON\n    return OFF_int,ON_int"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate the fluxes of the source segment in a particular frequency range and returns the fluxes of the source segment in a particular frequency range.", "response": "def get_calfluxes(calflux,calfreq,spec_in,centerfreqs,oneflux):\n    '''\n    Given properties of the calibrator source, calculate fluxes of the source\n    in a particular frequency range\n\n    Parameters\n    ----------\n    calflux : float\n        Known flux of calibrator source at a particular frequency\n    calfreq : float\n        Frequency where calibrator source has flux calflux (see above)\n    spec_in : float\n        Known power-law spectral index of calibrator source. Use convention flux(frequency) = constant * frequency^(spec_in)\n    centerfreqs : 1D Array (float)\n        Central frequency values of each coarse channel\n    oneflux : boolean\n        Use oneflux to choose between calculating the flux for each core channel (False)\n        or using one value for the entire frequency range (True)\n    '''\n\n    const = calflux/np.power(calfreq,spec_in)\n    if oneflux==False:\n        return const*np.power(centerfreqs,spec_in)\n    else:\n        return const*np.power(np.mean(centerfreqs),spec_in)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the center frequency of each coarse channel in the frequency array.", "response": "def get_centerfreqs(freqs,chan_per_coarse):\n    '''\n    Returns central frequency of each coarse channel\n\n    Parameters\n    ----------\n    freqs : 1D Array (float)\n        Frequency values for each bin of the spectrum\n    chan_per_coarse: int\n        Number of frequency bins per coarse channel\n    '''\n\n    num_coarse = freqs.size/chan_per_coarse\n    freqs = np.reshape(freqs,(num_coarse,chan_per_coarse))\n    return np.mean(freqs,axis=1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef f_ratios(calON_obs,calOFF_obs,chan_per_coarse,**kwargs):\n    '''\n    Calculate f_ON, and f_OFF as defined in van Straten et al. 2012 equations 2 and 3\n\n    Parameters\n    ----------\n    calON_obs : str\n        Path to filterbank file (any format) for observation ON the calibrator source\n    calOFF_obs : str\n        Path to filterbank file (any format) for observation OFF the calibrator source\n    '''\n    #Calculate noise diode ON and noise diode OFF spectra (H and L) for both observations\n    L_ON,H_ON = integrate_calib(calON_obs,chan_per_coarse,**kwargs)\n    L_OFF,H_OFF = integrate_calib(calOFF_obs,chan_per_coarse,**kwargs)\n\n    f_ON = H_ON/L_ON-1\n    f_OFF = H_OFF/L_OFF-1\n\n    return f_ON, f_OFF", "response": "Calculate f_ON and f_OFF spectra for both calibration and non - calibration states."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the coarse channel spectrum and system temperature of the noise diode in Jy given two calibrator sources ON and OFF measurements.", "response": "def diode_spec(calON_obs,calOFF_obs,calflux,calfreq,spec_in,average=True,oneflux=False,**kwargs):\n    '''\n    Calculate the coarse channel spectrum and system temperature of the noise diode in Jy given two noise diode\n    measurements ON and OFF the calibrator source with the same frequency and time resolution\n\n    Parameters\n    ----------\n    calON_obs : str\n        (see f_ratios() above)\n    calOFF_obs : str\n        (see f_ratios() above)\n    calflux : float\n        Known flux of calibrator source at a particular frequency\n    calfreq : float\n        Frequency where calibrator source has flux calflux (see above)\n    spec_in : float\n        Known power-law spectral index of calibrator source. Use convention flux(frequency) = constant * frequency^(spec_in)\n    average : boolean\n        Use average=True to return noise diode and Tsys spectra averaged over frequencies\n    '''\n    #Load frequencies and calculate number of channels per coarse channel\n    obs = Waterfall(calON_obs,max_load=150)\n    freqs = obs.populate_freqs()\n    ncoarse = obs.calc_n_coarse_chan()\n    nchans = obs.header['nchans']\n    chan_per_coarse = nchans/ncoarse\n\n    f_ON, f_OFF = f_ratios(calON_obs,calOFF_obs,chan_per_coarse,**kwargs)\n\n    #Obtain spectrum of the calibrator source for the given frequency range\n    centerfreqs = get_centerfreqs(freqs,chan_per_coarse)\n    calfluxes = get_calfluxes(calflux,calfreq,spec_in,centerfreqs,oneflux)\n\n    #C_o and Tsys as defined in van Straten et al. 2012\n    C_o = calfluxes/(1/f_ON-1/f_OFF)\n    Tsys = C_o/f_OFF\n\n    #return coarse channel diode spectrum\n    if average==True:\n        return np.mean(C_o),np.mean(Tsys)\n    else:\n        return C_o,Tsys"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_Tsys(calON_obs,calOFF_obs,calflux,calfreq,spec_in,oneflux=False,**kwargs):\n    '''\n    Returns frequency dependent system temperature given observations on and off a calibrator source\n\n    Parameters\n    ----------\n    (See diode_spec())\n    '''\n    return diode_spec(calON_obs,calOFF_obs,calflux,calfreq,spec_in,average=False,oneflux=False,**kwargs)[1]", "response": "Returns frequency dependent system temperature given observations on and off a calibrator source."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nproducing calibrated Stokes I for an observation given a noise diode and a frequency spectrum.", "response": "def calibrate_fluxes(main_obs_name,dio_name,dspec,Tsys,fullstokes=False,**kwargs):\n    '''\n    Produce calibrated Stokes I for an observation given a noise diode\n    measurement on the source and a diode spectrum with the same number of\n    coarse channels\n\n    Parameters\n    ----------\n    main_obs_name : str\n        Path to filterbank file containing final data to be calibrated\n    dio_name : str\n        Path to filterbank file for observation on the target source with flickering noise diode\n    dspec : 1D Array (float) or float\n        Coarse channel spectrum (or average) of the noise diode in Jy (obtained from diode_spec())\n    Tsys : 1D Array (float) or float\n        Coarse channel spectrum (or average) of the system temperature in Jy\n    fullstokes: boolean\n        Use fullstokes=True if data is in IQUV format or just Stokes I, use fullstokes=False if\n        it is in cross_pols format\n    '''\n\n    #Find folded spectra of the target source with the noise diode ON and OFF\n    main_obs = Waterfall(main_obs_name,max_load=150)\n    ncoarse = main_obs.calc_n_coarse_chan()\n    dio_obs = Waterfall(dio_name,max_load=150)\n    dio_chan_per_coarse = dio_obs.header['nchans']/ncoarse\n    dOFF,dON = integrate_calib(dio_name,dio_chan_per_coarse,fullstokes,**kwargs)\n\n    #Find Jy/count for each coarse channel using the diode spectrum\n    main_dat = main_obs.data\n    scale_facs = dspec/(dON-dOFF)\n    print(scale_facs)\n\n    nchans = main_obs.header['nchans']\n    obs_chan_per_coarse = nchans/ncoarse\n\n    ax0_size = np.size(main_dat,0)\n    ax1_size = np.size(main_dat,1)\n\n    #Reshape data array of target observation and multiply coarse channels by the scale factors\n    main_dat = np.reshape(main_dat,(ax0_size,ax1_size,ncoarse,obs_chan_per_coarse))\n    main_dat = np.swapaxes(main_dat,2,3)\n\n    main_dat = main_dat*scale_facs\n    main_dat = main_dat-Tsys\n    main_dat = np.swapaxes(main_dat,2,3)\n    main_dat = np.reshape(main_dat,(ax0_size,ax1_size,nchans))\n\n    #Write calibrated data to a new filterbank file with \".fluxcal\" extension\n\n    main_obs.data = main_dat\n    main_obs.write_to_filterbank(main_obs_name[:-4]+'.fluxcal.fil')\n    print('Finished: calibrated product written to ' + main_obs_name[:-4]+'.fluxcal.fil')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the length of the blimpy header in bytes", "response": "def len_header(filename):\n    \"\"\" Return the length of the blimpy header, in bytes\n\n    Args:\n        filename (str): name of file to open\n\n    Returns:\n        idx_end (int): length of header, in bytes\n    \"\"\"\n    with  open(filename, 'rb') as f:\n        header_sub_count = 0\n        eoh_found = False\n        while not eoh_found:\n            header_sub = f.read(512)\n            header_sub_count += 1\n            if b'HEADER_END' in header_sub:\n                idx_end = header_sub.index(b'HEADER_END') + len(b'HEADER_END')\n                eoh_found = True\n                break\n\n        idx_end = (header_sub_count -1) * 512 + idx_end\n    return idx_end"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if the file is a filterbank file or not.", "response": "def is_filterbank(filename):\n    \"\"\" Open file and confirm if it is a filterbank file or not. \"\"\"\n    with open(filename, 'rb') as fh:\n        is_fil = True\n\n        # Check this is a blimpy file\n        try:\n            keyword, value, idx = read_next_header_keyword(fh)\n            try:\n                assert keyword == b'HEADER_START'\n            except AssertionError:\n                is_fil = False\n        except KeyError:\n            is_fil = False\n        return is_fil"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_header(filename, return_idxs=False):\n    with open(filename, 'rb') as fh:\n        header_dict = {}\n        header_idxs = {}\n\n        # Check this is a blimpy file\n        keyword, value, idx = read_next_header_keyword(fh)\n\n        try:\n            assert keyword == b'HEADER_START'\n        except AssertionError:\n            raise RuntimeError(\"Not a valid blimpy file.\")\n\n        while True:\n            keyword, value, idx = read_next_header_keyword(fh)\n            if keyword == b'HEADER_END':\n                break\n            else:\n                header_dict[keyword] = value\n                header_idxs[keyword] = idx\n\n    if return_idxs:\n        return header_idxs\n    else:\n        return header_dict", "response": "Read blimpy header and return a Python dictionary of key - value pairs."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fix_header(filename, keyword, new_value):\n\n    # Read header data and return indexes of data offsets in file\n    hd = read_header(filename)\n    hi = read_header(filename, return_idxs=True)\n    idx = hi[keyword]\n\n    # Find out the datatype for the given keyword\n    dtype = header_keyword_types[keyword]\n    dtype_to_type = {b'<l'  : np.int32,\n                     b'str' : bytes,\n                     b'<d'  : np.float64,\n                     b'angle' : to_sigproc_angle}\n    value_dtype = dtype_to_type[dtype]\n\n    # Generate the new string\n    if isinstance(value_dtype, bytes):\n        if len(hd[keyword]) == len(new_value):\n            val_str = np.int32(len(new_value)).tostring() + new_value\n        else:\n            raise RuntimeError(\"String size mismatch. Cannot update without rewriting entire file.\")\n    else:\n        val_str = value_dtype(new_value).tostring()\n\n    # Write the new string to file\n    with open(filename, 'rb+') as fh:\n        fh.seek(idx)\n        fh.write(val_str)", "response": "Apply a quick patch - up to a Filterbank header by overwriting a header value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fil_double_to_angle(angle):\n\n      negative = (angle < 0.0)\n      angle = np.abs(angle)\n\n      dd = np.floor((angle / 10000))\n      angle -= 10000 * dd\n      mm = np.floor((angle / 100))\n      ss = angle - 100 * mm\n      dd += mm/60.0 + ss/3600.0\n\n      if negative:\n          dd *= -1\n\n      return dd", "response": "Reads a little - endian double in ddmmss. s or hhmmss. s format and then converts to Float degrees or hours."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_sigproc_keyword(keyword, value=None):\n\n    keyword = bytes(keyword)\n\n    if value is None:\n        return np.int32(len(keyword)).tostring() + keyword\n    else:\n        dtype = header_keyword_types[keyword]\n\n        dtype_to_type = {b'<l'  : np.int32,\n                         b'str' : str,\n                         b'<d'  : np.float64,\n                         b'angle' : to_sigproc_angle}\n\n        value_dtype = dtype_to_type[dtype]\n\n        if value_dtype is str:\n            return np.int32(len(keyword)).tostring() + keyword + np.int32(len(value)).tostring() + value\n        else:\n            return np.int32(len(keyword)).tostring() + keyword + value_dtype(value).tostring()", "response": "Generates a serialized string for a sigproc keyword value pair\n\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate a serialzed sigproc header which can be written to disk.", "response": "def generate_sigproc_header(f):\n    \"\"\" Generate a serialzed sigproc header which can be written to disk.\n\n    Args:\n        f (Filterbank object): Filterbank object for which to generate header\n\n    Returns:\n        header_str (str): Serialized string corresponding to header\n    \"\"\"\n\n    header_string = b''\n    header_string += to_sigproc_keyword(b'HEADER_START')\n\n    for keyword in f.header.keys():\n        if keyword == b'src_raj':\n            header_string += to_sigproc_keyword(b'src_raj')  + to_sigproc_angle(f.header[b'src_raj'])\n        elif keyword == b'src_dej':\n            header_string += to_sigproc_keyword(b'src_dej')  + to_sigproc_angle(f.header[b'src_dej'])\n        elif keyword == b'az_start' or keyword == b'za_start':\n            header_string += to_sigproc_keyword(keyword)  + np.float64(f.header[keyword]).tostring()\n        elif keyword not in header_keyword_types.keys():\n            pass\n        else:\n            header_string += to_sigproc_keyword(keyword, f.header[keyword])\n\n    header_string += to_sigproc_keyword(b'HEADER_END')\n    return header_string"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts an astropy. Angle to the ridiculous sigproc angle format string.", "response": "def to_sigproc_angle(angle_val):\n    \"\"\" Convert an astropy.Angle to the ridiculous sigproc angle format string. \"\"\"\n    x = str(angle_val)\n\n    if '.' in x:\n        if 'h' in x:\n                d, m, s, ss = int(x[0:x.index('h')]), int(x[x.index('h')+1:x.index('m')]), \\\n                int(x[x.index('m')+1:x.index('.')]), float(x[x.index('.'):x.index('s')])\n        if 'd' in x:\n            d, m, s, ss = int(x[0:x.index('d')]), int(x[x.index('d')+1:x.index('m')]), \\\n            int(x[x.index('m')+1:x.index('.')]), float(x[x.index('.'):x.index('s')])\n    else:\n        if 'h' in x:\n            d, m, s = int(x[0:x.index('h')]), int(x[x.index('h')+1:x.index('m')]), \\\n            int(x[x.index('m')+1:x.index('s')])\n        if 'd' in x:\n            d, m, s = int(x[0:x.index('d')]), int(x[x.index('d')+1:x.index('m')]), \\\n            int(x[x.index('m')+1:x.index('s')])\n        ss = 0\n    num = str(d).zfill(2) + str(m).zfill(2) + str(s).zfill(2)+ '.' + str(ss).split(\".\")[-1]\n    return np.float64(num).tostring()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef calc_n_ints_in_file(filename):\n    # Load binary data\n    h = read_header(filename)\n    n_bytes  = int(h[b'nbits'] / 8)\n\n    n_chans = h[b'nchans']\n    n_ifs   = h[b'nifs']\n    idx_data = len_header(filename)\n    f = open(filename, 'rb')\n    f.seek(idx_data)\n    filesize = os.path.getsize(filename)\n    n_bytes_data = filesize - idx_data\n\n    if h[b'nbits'] == 2:\n        n_ints = int(4 * n_bytes_data / (n_chans * n_ifs))\n    else:\n        n_ints = int(n_bytes_data / (n_bytes * n_chans * n_ifs))\n\n    return n_ints", "response": "Calculate the number of integrations in a given file"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a file to Sigproc filterbank format.", "response": "def make_fil_file(filename,out_dir='./', new_filename=None, max_load = None):\n    ''' Converts file to Sigproc filterbank (.fil) format.  Default saves output in current dir.\n    '''\n\n    fil_file = Waterfall(filename, max_load = max_load)\n    if not new_filename:\n        new_filename = out_dir+filename.replace('.h5','.fil').split('/')[-1]\n\n    if '.fil' not in new_filename:\n        new_filename = new_filename+'.fil'\n\n    fil_file.write_to_fil(new_filename)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a Traceback into a dictionary representation", "response": "def to_dict(self):\n        \"\"\"Convert a Traceback into a dictionary representation\"\"\"\n        if self.tb_next is None:\n            tb_next = None\n        else:\n            tb_next = self.tb_next.to_dict()\n\n        code = {\n            'co_filename': self.tb_frame.f_code.co_filename,\n            'co_name': self.tb_frame.f_code.co_name,\n        }\n        frame = {\n            'f_globals': self.tb_frame.f_globals,\n            'f_code': code,\n        }\n        return {\n            'tb_frame': frame,\n            'tb_lineno': self.tb_lineno,\n            'tb_next': tb_next,\n        }"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes a subparser for a given type of DNS record.", "response": "def make_rr_subparser(subparsers, rec_type, args_and_types):\n    \"\"\"\n    Make a subparser for a given type of DNS record\n    \"\"\"\n    sp = subparsers.add_parser(rec_type)\n\n    sp.add_argument(\"name\", type=str)\n    sp.add_argument(\"ttl\", type=int, nargs='?')\n    sp.add_argument(rec_type, type=str)\n\n    for my_spec in args_and_types:\n        (argname, argtype) = my_spec[:2]\n        if len(my_spec) > 2:\n            nargs = my_spec[2]\n            sp.add_argument(argname, type=argtype, nargs=nargs)\n        else:\n            sp.add_argument(argname, type=argtype)\n    return sp"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_parser():\n    line_parser = ZonefileLineParser()\n    subparsers = line_parser.add_subparsers()\n\n    # parse $ORIGIN\n    sp = subparsers.add_parser(\"$ORIGIN\")\n    sp.add_argument(\"$ORIGIN\", type=str)\n\n    # parse $TTL\n    sp = subparsers.add_parser(\"$TTL\")\n    sp.add_argument(\"$TTL\", type=int)\n\n    # parse each RR\n    args_and_types = [\n        (\"mname\", str), (\"rname\", str), (\"serial\", int), (\"refresh\", int),\n        (\"retry\", int), (\"expire\", int), (\"minimum\", int)\n    ]\n    make_rr_subparser(subparsers, \"SOA\", args_and_types)\n\n    make_rr_subparser(subparsers, \"NS\", [(\"host\", str)])\n    make_rr_subparser(subparsers, \"A\", [(\"ip\", str)])\n    make_rr_subparser(subparsers, \"AAAA\", [(\"ip\", str)])\n    make_rr_subparser(subparsers, \"CNAME\", [(\"alias\", str)])\n    make_rr_subparser(subparsers, \"ALIAS\", [(\"host\", str)])\n    make_rr_subparser(subparsers, \"MX\", [(\"preference\", str), (\"host\", str)])\n    make_txt_subparser(subparsers)\n    make_rr_subparser(subparsers, \"PTR\", [(\"host\", str)])\n    make_rr_subparser(subparsers, \"SRV\", [(\"priority\", int), (\"weight\", int), (\"port\", int), (\"target\", str)])\n    make_rr_subparser(subparsers, \"SPF\", [(\"data\", str)])\n    make_rr_subparser(subparsers, \"URI\", [(\"priority\", int), (\"weight\", int), (\"target\", str)])\n\n    return line_parser", "response": "Make an ArgumentParser that accepts DNS RRs\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nserializes a list of tokens into a string.", "response": "def serialize(tokens):\n    \"\"\"\n    Serialize tokens:\n    * quote whitespace-containing tokens\n    * escape semicolons\n    \"\"\"\n    ret = []\n    for tok in tokens:\n        if \" \" in tok:\n            tok = '\"%s\"' % tok\n\n        if \";\" in tok:\n            tok = tok.replace(\";\", \"\\;\")\n\n        ret.append(tok)\n\n    return \" \".join(ret)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving comments from a zonefile", "response": "def remove_comments(text):\n    \"\"\"\n    Remove comments from a zonefile\n    \"\"\"\n    ret = []\n    lines = text.split(\"\\n\")\n    for line in lines:\n        if len(line) == 0:\n            continue \n\n        line = serialize(tokenize_line(line))\n        ret.append(line)\n\n    return \"\\n\".join(ret)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef flatten(text):\n    lines = text.split(\"\\n\")\n\n    # tokens: sequence of non-whitespace separated by '' where a newline was\n    tokens = []\n    for l in lines:\n        if len(l) == 0:\n            continue \n\n        l = l.replace(\"\\t\", \" \")\n        tokens += filter(lambda x: len(x) > 0, l.split(\" \")) + ['']\n\n    # find (...) and turn it into a single line (\"capture\" it)\n    capturing = False\n    captured = []\n\n    flattened = []\n    while len(tokens) > 0:\n        tok = tokens.pop(0)\n        if not capturing and len(tok) == 0:\n            # normal end-of-line\n            if len(captured) > 0:\n                flattened.append(\" \".join(captured))\n                captured = []\n            continue \n\n        if tok.startswith(\"(\"):\n            # begin grouping\n            tok = tok.lstrip(\"(\")\n            capturing = True\n\n        if capturing and tok.endswith(\")\"):\n            # end grouping.  next end-of-line will turn this sequence into a flat line\n            tok = tok.rstrip(\")\")\n            capturing = False \n\n        captured.append(tok)\n\n    return \"\\n\".join(flattened)", "response": "Flatten the text into a single line."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove the CLASS from each DNS record if present.", "response": "def remove_class(text):\n    \"\"\"\n    Remove the CLASS from each DNS record, if present.\n    The only class that gets used today (for all intents\n    and purposes) is 'IN'.\n    \"\"\"\n\n    # see RFC 1035 for list of classes\n    lines = text.split(\"\\n\")\n    ret = []\n    for line in lines:\n        tokens = tokenize_line(line)\n        tokens_upper = [t.upper() for t in tokens]\n\n        if \"IN\" in tokens_upper:\n            tokens.remove(\"IN\")\n        elif \"CS\" in tokens_upper:\n            tokens.remove(\"CS\")\n        elif \"CH\" in tokens_upper:\n            tokens.remove(\"CH\")\n        elif \"HS\" in tokens_upper:\n            tokens.remove(\"HS\")\n\n        ret.append(serialize(tokens))\n\n    return \"\\n\".join(ret)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a default name to the list of available users.", "response": "def add_default_name(text):\n    \"\"\"\n    Go through each line of the text and ensure that \n    a name is defined.  Use '@' if there is none.\n    \"\"\"\n    global SUPPORTED_RECORDS\n\n    lines = text.split(\"\\n\")\n    ret = []\n    for line in lines:\n        tokens = tokenize_line(line)\n        if len(tokens) == 0:\n            continue\n\n        if tokens[0] in SUPPORTED_RECORDS and not tokens[0].startswith(\"$\"):\n            # add back the name\n            tokens = ['@'] + tokens \n\n        ret.append(serialize(tokens))\n\n    return \"\\n\".join(ret)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_line(parser, record_token, parsed_records):\n\n    global SUPPORTED_RECORDS\n\n    line = \" \".join(record_token)\n\n    # match parser to record type\n    if len(record_token) >= 2 and record_token[1] in SUPPORTED_RECORDS:\n        # with no ttl\n        record_token = [record_token[1]] + record_token\n    elif len(record_token) >= 3 and record_token[2] in SUPPORTED_RECORDS:\n        # with ttl\n        record_token = [record_token[2]] + record_token\n        if record_token[0] == \"TXT\":\n            record_token = record_token[:2] + [\"--ttl\"] + record_token[2:]\n    try:\n        rr, unmatched = parser.parse_known_args(record_token)\n        assert len(unmatched) == 0, \"Unmatched fields: %s\" % unmatched\n    except (SystemExit, AssertionError, InvalidLineException):\n        # invalid argument \n        raise InvalidLineException(line)\n\n    record_dict = rr.__dict__\n    if record_token[0] == \"TXT\" and len(record_dict['txt']) == 1:\n        record_dict['txt'] = record_dict['txt'][0]\n\n    # what kind of record? including origin and ttl\n    record_type = None\n    for key in record_dict.keys():\n        if key in SUPPORTED_RECORDS and (key.startswith(\"$\") or record_dict[key] == key):\n            record_type = key\n            if record_dict[key] == key:\n                del record_dict[key]\n            break\n\n    assert record_type is not None, \"Unknown record type in %s\" % rr\n\n    # clean fields\n    for field in record_dict.keys():\n        if record_dict[field] is None:\n            del record_dict[field]\n\n    current_origin = record_dict.get('$ORIGIN', parsed_records.get('$ORIGIN', None))\n\n    # special record-specific fix-ups\n    if record_type == 'PTR':\n        record_dict['fullname'] = record_dict['name'] + '.' + current_origin\n      \n    if len(record_dict) > 0:\n        if record_type.startswith(\"$\"):\n            # put the value directly\n            record_dict_key = record_type.lower()\n            parsed_records[record_dict_key] = record_dict[record_type]\n        else:\n            record_dict_key = record_type.lower()\n            parsed_records[record_dict_key].append(record_dict)\n\n    return parsed_records", "response": "Given the parser and a list of tokens and a current set of parsed records return the current set of parsed records."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a zonefile into a dict.", "response": "def parse_lines(text, ignore_invalid=False):\n    \"\"\"\n    Parse a zonefile into a dict.\n    @text must be flattened--each record must be on one line.\n    Also, all comments must be removed.\n    \"\"\"\n    json_zone_file = defaultdict(list)\n    record_lines = text.split(\"\\n\")\n    parser = make_parser()\n\n    for record_line in record_lines:\n        record_token = tokenize_line(record_line)\n        try:\n            json_zone_file = parse_line(parser, record_token, json_zone_file)\n        except InvalidLineException:\n            if ignore_invalid:\n                continue\n            else:\n                raise\n\n    return json_zone_file"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_zone_file(text, ignore_invalid=False):\n    text = remove_comments(text)\n    text = flatten(text)\n    text = remove_class(text)\n    text = add_default_name(text)\n    json_zone_file = parse_lines(text, ignore_invalid=ignore_invalid)\n    return json_zone_file", "response": "Parse a zonefile into a dict."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a DNS zonefile from a json - encoded description of the zone file and a template.", "response": "def make_zone_file(json_zone_file_input, origin=None, ttl=None, template=None):\n    \"\"\"\n    Generate the DNS zonefile, given a json-encoded description of the\n    zone file (@json_zone_file) and the template to fill in (@template)\n\n    json_zone_file = {\n        \"$origin\": origin server,\n        \"$ttl\":    default time-to-live,\n        \"soa\":     [ soa records ],\n        \"ns\":      [ ns records ],\n        \"a\":       [ a records ],\n        \"aaaa\":    [ aaaa records ]\n        \"cname\":   [ cname records ]\n        \"alias\":   [ alias records ]\n        \"mx\":      [ mx records ]\n        \"ptr\":     [ ptr records ]\n        \"txt\":     [ txt records ]\n        \"srv\":     [ srv records ]\n        \"spf\":     [ spf records ]\n        \"uri\":     [ uri records ]\n    }\n    \"\"\"\n\n    if template is None:\n        template = DEFAULT_TEMPLATE[:]\n\n    # careful...\n    json_zone_file = copy.deepcopy(json_zone_file_input)\n    if origin is not None:\n        json_zone_file['$origin'] = origin \n\n    if ttl is not None:\n        json_zone_file['$ttl'] = ttl\n\n    soa_records = [json_zone_file.get('soa')] if json_zone_file.get('soa') else None\n\n    zone_file = template\n    zone_file = process_origin(json_zone_file.get('$origin', None), zone_file)\n    zone_file = process_ttl(json_zone_file.get('$ttl', None), zone_file)\n    zone_file = process_soa(soa_records, zone_file)\n    zone_file = process_ns(json_zone_file.get('ns', None), zone_file)\n    zone_file = process_a(json_zone_file.get('a', None), zone_file)\n    zone_file = process_aaaa(json_zone_file.get('aaaa', None), zone_file)\n    zone_file = process_cname(json_zone_file.get('cname', None), zone_file)\n    zone_file = process_alias(json_zone_file.get('alias', None), zone_file)\n    zone_file = process_mx(json_zone_file.get('mx', None), zone_file)\n    zone_file = process_ptr(json_zone_file.get('ptr', None), zone_file)\n    zone_file = process_txt(json_zone_file.get('txt', None), zone_file)\n    zone_file = process_srv(json_zone_file.get('srv', None), zone_file)\n    zone_file = process_spf(json_zone_file.get('spf', None), zone_file)\n    zone_file = process_uri(json_zone_file.get('uri', None), zone_file)\n\n    # remove newlines, but terminate with one\n    zone_file = \"\\n\".join(\n        filter(\n            lambda l: len(l.strip()) > 0, [tl.strip() for tl in zone_file.split(\"\\n\")]\n        )\n    ) + \"\\n\"\n\n    return zone_file"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process_origin(data, template):\n    record = \"\"\n    if data is not None:\n        record += \"$ORIGIN %s\" % data\n\n    return template.replace(\"{$origin}\", record)", "response": "Process the origin in a template."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprocess the TTL field in a template.", "response": "def process_ttl(data, template):\n    \"\"\"\n    Replace {$ttl} in template with a serialized $TTL record\n    \"\"\"\n    record = \"\"\n    if data is not None:\n        record += \"$TTL %s\" % data\n\n    return template.replace(\"{$ttl}\", record)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprocessing a single SOA record in a template and return a set of serialized SOA records.", "response": "def process_soa(data, template):\n    \"\"\"\n    Replace {SOA} in template with a set of serialized SOA records\n    \"\"\"\n    record = template[:]\n\n    if data is not None:\n    \n        assert len(data) == 1, \"Only support one SOA RR at this time\"\n        data = data[0]\n\n        soadat = []\n        domain_fields = ['mname', 'rname']\n        param_fields = ['serial', 'refresh', 'retry', 'expire', 'minimum']\n\n        for f in domain_fields + param_fields:\n            assert f in data.keys(), \"Missing '%s' (%s)\" % (f, data)\n\n        data_name = str(data.get('name', '@'))\n        soadat.append(data_name)\n\n        if data.get('ttl') is not None:\n            soadat.append( str(data['ttl']) )\n  \n        soadat.append(\"IN\")\n        soadat.append(\"SOA\")\n\n        for key in domain_fields:\n            value = str(data[key])\n            soadat.append(value)\n\n        soadat.append(\"(\")\n\n        for key in param_fields:\n            value = str(data[key])\n            soadat.append(value)\n\n        soadat.append(\")\")\n\n        soa_txt = \" \".join(soadat)\n        record = record.replace(\"{soa}\", soa_txt)\n\n    else:\n        # clear all SOA fields \n        record = record.replace(\"{soa}\", \"\")\n\n    return record"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nquotes a field in a list of DNS records. Return the new data records.", "response": "def quote_field(data, field):\n    \"\"\"\n    Quote a field in a list of DNS records.\n    Return the new data records.\n    \"\"\"\n    if data is None:\n        return None \n\n    data_dup = copy.deepcopy(data)\n    for i in xrange(0, len(data_dup)):\n        data_dup[i][field] = '\"%s\"' % data_dup[i][field]\n        data_dup[i][field] = data_dup[i][field].replace(\";\", \"\\;\")\n\n    return data_dup"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef process_rr(data, record_type, record_keys, field, template):\n    if data is None:\n        return template.replace(field, \"\")\n\n    if type(record_keys) == list:\n        pass\n    elif type(record_keys) == str:\n        record_keys = [record_keys]\n    else:\n        raise ValueError(\"Invalid record keys\")\n\n    assert type(data) == list, \"Data must be a list\"\n\n    record = \"\"\n    for i in xrange(0, len(data)):\n\n        for record_key in record_keys:\n            assert record_key in data[i].keys(), \"Missing '%s'\" % record_key\n\n        record_data = []\n        record_data.append( str(data[i].get('name', '@')) )\n        if data[i].get('ttl') is not None:\n            record_data.append( str(data[i]['ttl']) )\n\n        record_data.append(record_type)\n        record_data += [str(data[i][record_key]) for record_key in record_keys]\n        record += \" \".join(record_data) + \"\\n\"\n\n    return template.replace(field, record)", "response": "Process the record_type records in the template."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprocess TXT records in data with the serialized TXT records", "response": "def process_txt(data, template):\n    \"\"\"\n    Replace {txt} in template with the serialized TXT records\n    \"\"\"\n    if data is None:\n        to_process = None\n    else:\n        # quote txt\n        to_process = copy.deepcopy(data)\n        for datum in to_process:\n            if isinstance(datum[\"txt\"], list):\n                datum[\"txt\"] = \" \".join(['\"%s\"' % entry.replace(\";\", \"\\;\")\n                                         for entry in datum[\"txt\"]])\n            else:\n                datum[\"txt\"] = '\"%s\"' % datum[\"txt\"].replace(\";\", \"\\;\")\n    return process_rr(to_process, \"TXT\", \"txt\", \"{txt}\", template)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_schema_string(schema_string):\n    if isinstance(schema_string, str):\n        schema_string = schema_string.decode(\"utf8\")\n    schema_struct = json.loads(schema_string)\n\n    return AvroSchemaParser().parse_schema_struct(schema_struct)", "response": "Load and return a PySchema class from an avsc string"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _class_source(schema, indent):\n\n    def_pattern = (\n        \"class {class_name}(pyschema.Record):\\n\"\n        \"{indent}# WARNING: This class was generated by pyschema.to_python_source\\n\"\n        \"{indent}# there is a risk that any modification made to this class will be overwritten\\n\"\n        \"{optional_namespace_def}\"\n        \"{field_defs}\\n\"\n    )\n    if hasattr(schema, '_namespace'):\n        optional_namespace_def = \"{indent}_namespace = {namespace!r}\\n\".format(\n            namespace=schema._namespace, indent=indent)\n    else:\n        optional_namespace_def = \"\"\n\n    field_defs = [\n        \"{indent}{field_name} = {field!r}\".format(field_name=field_name, field=field, indent=indent)\n        for field_name, field in schema._fields.iteritems()\n    ]\n    if not field_defs:\n        field_defs = [\"{indent}pass\".format(indent=indent)]\n\n    return def_pattern.format(\n        class_name=schema._schema_name,\n        optional_namespace_def=optional_namespace_def,\n        field_defs=\"\\n\".join(field_defs),\n        indent=indent\n    )", "response": "Generate Python source code for one specific class"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndump record in json - encodable object format", "response": "def to_json_compatible(record):\n    \"Dump record in json-encodable object format\"\n    d = {}\n    for fname, f in record._fields.iteritems():\n        val = getattr(record, fname)\n        if val is not None:\n            d[fname] = f.dump(val)\n    return d"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading from json - encodable", "response": "def from_json_compatible(schema, dct):\n    \"Load from json-encodable\"\n    kwargs = {}\n\n    for key in dct:\n        field_type = schema._fields.get(key)\n        if field_type is None:\n            raise ParseError(\"Unexpected field encountered in line for record %s: %s\" % (schema.__name__, key))\n        kwargs[key] = field_type.load(dct[key])\n\n    return schema(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads a record from a json - compatible dictionary.", "response": "def load_json_dct(\n        dct,\n        record_store=None,\n        schema=None,\n        loader=from_json_compatible\n):\n    \"\"\" Create a Record instance from a json-compatible dictionary\n\n    The dictionary values should have types that are json compatible,\n    as if just loaded from a json serialized record string.\n\n    :param dct:\n        Python dictionary with key/value pairs for the record\n\n    :param record_store:\n        Record store to use for schema lookups (when $schema field is present)\n\n    :param schema:\n        PySchema Record class for the record to load.\n        This will override any $schema fields specified in `dct`\n\n    \"\"\"\n    if schema is None:\n        if record_store is None:\n            record_store = auto_store\n        try:\n            schema_name = dct.pop(SCHEMA_FIELD_NAME)\n        except KeyError:\n            raise ParseError((\n                \"Serialized record missing '{0}' \"\n                \"record identifier and no schema supplied\")\n                .format(SCHEMA_FIELD_NAME)\n            )\n        try:\n            schema = record_store.get(schema_name)\n        except KeyError:\n            raise ParseError(\n                \"Can't recognize record type %r\"\n                % (schema_name,), schema_name)\n\n    # if schema is explicit, use that instead of SCHEMA_FIELD_NAME\n    elif SCHEMA_FIELD_NAME in dct:\n        dct.pop(SCHEMA_FIELD_NAME)\n\n    record = loader(schema, dct)\n    return record"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef loads(\n        s,\n        record_store=None,\n        schema=None,\n        loader=from_json_compatible,\n        record_class=None  # deprecated in favor of schema\n):\n    \"\"\" Create a Record instance from a json serialized dictionary\n\n    :param s:\n        String with a json-serialized dictionary\n\n    :param record_store:\n        Record store to use for schema lookups (when $schema field is present)\n\n    :param loader:\n        Function called to fetch attributes from json. Typically shouldn't be used by end users\n\n    :param schema:\n        PySchema Record class for the record to load.\n        This will override any $schema fields specified in `s`\n\n    :param record_class:\n        DEPRECATED option, old name for the `schema` parameter\n\n    \"\"\"\n    if record_class is not None:\n        warnings.warn(\n            \"The record_class parameter is deprecated in favour of schema\",\n            DeprecationWarning,\n            stacklevel=2\n        )\n        schema = record_class\n    if not isinstance(s, unicode):\n        s = s.decode('utf8')\n    if s.startswith(u\"{\"):\n        json_dct = json.loads(s)\n        return load_json_dct(json_dct, record_store, schema, loader)\n    else:\n        raise ParseError(\"Not a json record\")", "response": "Load a record from a json - serialized string."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a record class to the record store for retrieval at record load time.", "response": "def add_record(self, schema, _bump_stack_level=False):\n        \"\"\" Add record class to record store for retrieval at record load time.\n\n            Can be used as a class decorator\n        \"\"\"\n        full_name = get_full_name(schema)\n        has_namespace = '.' in full_name\n        self._force_add(full_name, schema, _bump_stack_level, _raise_on_existing=has_namespace)\n        if has_namespace and schema.__name__ not in self._schema_map:\n            self._force_add(schema.__name__, schema, _bump_stack_level)\n        return schema"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(self, record_name):\n        if record_name in self._schema_map:\n            return self._schema_map[record_name]\n        else:\n            last_name = record_name.split('.')[-1]\n            return self._schema_map[last_name]", "response": "Returns a matching record for the given name."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a dictionary the field definition Should contain all fields that are required for the class", "response": "def repr_vars(self):\n        \"\"\"Return a dictionary the field definition\n\n        Should contain all fields that are required for the definition of this field in a pyschema class\"\"\"\n        d = OrderedDict()\n        d[\"nullable\"] = repr(self.nullable)\n        d[\"default\"] = repr(self.default)\n        if self.description is not None:\n            d[\"description\"] = repr(self.description)\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mixin(cls, mixin_cls):\n        for item_name in dir(mixin_cls):\n            if item_name.startswith(\"__\"):\n                # don't copy magic properties\n                continue\n            item = getattr(mixin_cls, item_name)\n\n            if isinstance(item, types.MethodType):\n                # unbound method will cause problems\n                # so get the underlying function instead\n                item = item.im_func\n\n            setattr(cls, item_name, item)\n        return mixin_cls", "response": "Decorator for mixing in additional functionality into field type\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating proper PySchema class from cls.", "response": "def from_class(metacls, cls, auto_store=True):\n        \"\"\"Create proper PySchema class from cls\n\n        Any methods and attributes will be transferred to the\n        new object\n        \"\"\"\n        if auto_store:\n            def wrap(cls):\n                return cls\n        else:\n            wrap = no_auto_store()\n\n        return wrap(metacls.__new__(\n            metacls,\n            cls.__name__,\n            (Record,),\n            dict(cls.__dict__)\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a python dict representing the jsonschema of a record.", "response": "def get_schema_dict(record, state=None):\n    \"\"\"Return a python dict representing the jsonschema of a record\n\n    Any references to sub-schemas will be URI fragments that won't be\n    resolvable without a root schema, available from get_root_schema_dict.\n    \"\"\"\n    state = state or SchemaGeneratorState()\n    schema = OrderedDict([\n        ('type', 'object'),\n        ('id', record._schema_name),\n    ])\n    fields = dict()\n    for field_name, field_type in record._fields.iteritems():\n        fields[field_name] = field_type.jsonschema_type_schema(state)\n\n    required = set(fields.keys())\n    schema['properties'] = fields\n    schema['required'] = sorted(list(required))\n    schema['additionalProperties'] = False\n\n    state.record_schemas[record._schema_name] = schema\n    return schema"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_root_schema_dict(record):\n    state = SchemaGeneratorState()\n    schema = get_schema_dict(record, state)\n    del state.record_schemas[record._schema_name]\n    if state.record_schemas:\n        schema['definitions'] = dict()\n        for name, sub_schema in state.record_schemas.iteritems():\n            schema['definitions'][name] = sub_schema\n    return schema", "response": "Return a root jsonschema for a given record."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload from json - encodable", "response": "def from_json_compatible(schema, dct):\n    \"Load from json-encodable\"\n    kwargs = {}\n\n    for key in dct:\n        field_type = schema._fields.get(key)\n        if field_type is None:\n            warnings.warn(\"Unexpected field encountered in line for record %s: %r\" % (schema.__name__, key))\n            continue\n        kwargs[key] = field_type.avro_load(dct[key])\n\n    return schema(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mr_reader(job, input_stream, loads=core.loads):\n    for line in input_stream:\n        yield loads(line),", "response": "Converts a file object with json serialised pyschema records\n        to a stream of pyschema objects"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting a stream of pyschema Records to a file object", "response": "def mr_writer(job, outputs, output_stream,\n              stderr=sys.stderr, dumps=core.dumps):\n    \"\"\" Writes a stream of json serialised pyschema Records to a file object\n\n    Can be used as job.writer in luigi.hadoop.JobTask\n    \"\"\"\n    for output in outputs:\n        try:\n            print >> output_stream, dumps(output)\n        except core.ParseError, e:\n            print >> stderr, e\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ordereddict_push_front(dct, key, value):\n    d = OrderedDict()\n    d[key] = value\n    d.update(dct)\n    return d", "response": "Set a value at the front of an OrderedDict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating a single filter expression for the given name and operator and value.", "response": "def gen_filter(name, op, value, is_or=False):\n    \"\"\"Generates a single filter expression for ``filter[]``.\"\"\"\n    if op not in OPERATORS:\n        raise ValueError('Unknown operator {}'.format(op))\n    result = u'{} {} {}'.format(name, op, escape_filter(value))\n    if is_or:\n        result = u'or ' + result\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_dict(cls, d):\n        if not d:\n            raise ValueError('Empty dictionary!')\n        items = list(d.items())\n        key, value = items.pop(0)\n        q = cls(key, u'=', value)\n        for key, value in items:\n            q = q & cls(key, u'=', value)\n        return q", "response": "Creates a query ( AND and = ) from a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nspecifies query string to use with the collection.", "response": "def query_string(self, **params):\n        \"\"\"Specify query string to use with the collection.\n\n        Returns: :py:class:`SearchResult`\n        \"\"\"\n        return SearchResult(self, self._api.get(self._href, **params))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends all filters to the API.", "response": "def raw_filter(self, filters):\n        \"\"\"Sends all filters to the API.\n\n        No fancy, just a wrapper. Any advanced functionality shall be implemented as another method.\n\n        Args:\n            filters: List of filters (strings)\n\n        Returns: :py:class:`SearchResult`\n        \"\"\"\n        return SearchResult(self, self._api.get(self._href, **{\"filter[]\": filters}))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef all_include_attributes(self, attributes):\n        self.reload(expand=True, attributes=attributes)\n        entities = [Entity(self, r, attributes=attributes) for r in self._resources]\n        self.reload()\n        return entities", "response": "Returns all entities present in the collection with attributes included."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn entity in correct collection.", "response": "def _get_entity_from_href(self, result):\n        \"\"\"Returns entity in correct collection.\n\n        If the \"href\" value in result doesn't match the current collection,\n        try to find the collection that the \"href\" refers to.\n        \"\"\"\n        href_result = result['href']\n\n        if self.collection._href.startswith(href_result):\n            return Entity(self.collection, result, incomplete=True)\n\n        href_match = re.match(r\"(https?://.+/api[^?]*)/([a-z_-]+)\", href_result)\n        if not href_match:\n            raise ValueError(\"Malformed href: {}\".format(href_result))\n        collection_name = href_match.group(2)\n        entry_point = href_match.group(1)\n        new_collection = Collection(\n            self.collection.api,\n            \"{}/{}\".format(entry_point, collection_name),\n            collection_name\n        )\n        return Entity(new_collection, result, incomplete=True)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef give_another_quote(q):\n    for qc in QUOTES:\n        if qc != q:\n            return qc\n    else:\n        raise ValueError(u'Could not find a different quote for {}'.format(q))", "response": "Returns a new quote if possible."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef escape_filter(o):\n    if o is None:\n        return u'NULL'\n    if isinstance(o, int):\n        return str(o)\n    if not isinstance(o, six.string_types):\n        raise ValueError('Filters take only None, int or a string type')\n    if not o:\n        # Empty string\n        return u\"''\"\n    # Now enforce unicode\n    o = unicode_process(o)\n    if u'\"' not in o:\n        # Simple case, just put the quote that does not exist in the string\n        return u'\"' + o + u'\"'\n    elif u\"'\" not in o:\n        # Simple case, just put the quote that does not exist in the string\n        return u\"'\" + o + u\"'\"\n    else:\n        # Both are there, so start guessing\n        # Empty strings are sorted out, so the string must contain something.\n        # String with length == 1 are sorted out because if they have a quote, they would be quoted\n        # with the another quote in preceeding branch. Therefore the string is at least 2 chars long\n        # here which allows us to NOT check the length here.\n        first_char = o[0]\n        last_char = o[-1]\n        if first_char in QUOTES and last_char in QUOTES:\n            # The first and last chars definitely are quotes\n            if first_char == last_char:\n                # Simple, just put another ones around them\n                quote = give_another_quote(first_char)\n                return quote + o + quote\n            else:\n                # I don't like this but the nature of the escape is like that ...\n                # Since now it uses both of the quotes, just pick the simple ones and surround it\n                return u\"'\" + o + u\"'\"\n        elif first_char not in QUOTES and last_char not in QUOTES:\n            # First and last chars are not quotes, so a simple solution\n            return u\"'\" + o + u\"'\"\n        else:\n            # One of the first or last chars is not a quote\n            if first_char in QUOTES:\n                quote = give_another_quote(first_char)\n            else:\n                # last_char\n                quote = give_another_quote(last_char)\n            return quote + o + quote", "response": "Tries to escape the values that are passed to filter as correctly as possible."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmake the plot with parallax performance predictions.", "response": "def makePlot(args):\n  \"\"\"\n  Make the plot with parallax performance predictions.\n\n  :argument args: command line arguments\n  \"\"\"\n  gmag=np.linspace(5.7,20.0,101)\n\n  vminiB1V=vminiFromSpt('B1V')\n  vminiG2V=vminiFromSpt('G2V')\n  vminiM6V=vminiFromSpt('M6V')\n  \n  vmagB1V=gmag-gminvFromVmini(vminiB1V)\n  vmagG2V=gmag-gminvFromVmini(vminiG2V)\n  vmagM6V=gmag-gminvFromVmini(vminiM6V)\n  \n  sigparB1V=parallaxErrorSkyAvg(gmag,vminiB1V)\n  sigparB1Vmin=parallaxMinError(gmag,vminiB1V)\n  sigparB1Vmax=parallaxMaxError(gmag,vminiB1V)\n  \n  sigparG2V=parallaxErrorSkyAvg(gmag,vminiG2V)\n  sigparG2Vmin=parallaxMinError(gmag,vminiG2V)\n  sigparG2Vmax=parallaxMaxError(gmag,vminiG2V)\n  \n  sigparM6V=parallaxErrorSkyAvg(gmag,vminiM6V)\n  sigparM6Vmin=parallaxMinError(gmag,vminiM6V)\n  sigparM6Vmax=parallaxMaxError(gmag,vminiM6V)\n  \n  fig=plt.figure(figsize=(10,6.5))\n  \n  if (args['gmagAbscissa']):\n    plt.semilogy(gmag, sigparB1V, 'b', label='B1V')\n    plt.semilogy(gmag, sigparG2V, 'g', label='G2V')\n    plt.semilogy(gmag, sigparM6V, 'r', label='M6V')\n    plt.xlim((5,20))\n    plt.ylim((4,1000))\n    plt.legend(loc=4)\n    plt.xlabel('$G$ [mag]')\n  else:\n    ax=fig.add_subplot(111)\n    plt.semilogy(vmagB1V, sigparB1V, 'b', label='B1V')\n    #plt.semilogy(vmagG2V, sigparG2V, 'g', label='G2V')\n    plt.semilogy(vmagM6V, sigparM6V, 'r', label='M6V')\n    plt.fill_between(vmagB1V, sigparB1Vmin, sigparB1Vmax, color='b', alpha=0.3)\n    plt.fill_between(vmagM6V, sigparM6Vmin, sigparM6Vmax, color='r', alpha=0.3)\n    plt.xlim((5,22.5))\n    plt.ylim((4,1000))\n    plt.text(17.2,190,'B1V',color='b')\n    plt.text(18,20,'M6V',color='r')\n    plt.xlabel('$V$ [mag]')\n    plt.text(7,17,'calibration noise floor', size=12, bbox=dict(boxstyle=\"round,pad=0.3\",\n                       ec=(0.0, 0.0, 0.0),\n                       fc=(1.0, 1.0, 1.0),\n                       ))\n    plt.text(14.75,80,'photon noise', rotation=45, size=12, bbox=dict(boxstyle=\"round,pad=0.3\",\n                       ec=(0.0, 0.0, 0.0),\n                       fc=(1.0, 1.0, 1.0),\n                       ))\n    ax.annotate('non-uniformity\\nover the sky', xy=(21.5, 320),  xycoords='data',\n                  xytext=(21.5,80), textcoords='data', ha='center', size='12',\n                  bbox=dict(boxstyle=\"round,pad=0.3\",ec=(0,0,0),fc=(1,1,1)),\n                  arrowprops=dict(facecolor='black', shrink=0.15, width=1,\n                    headwidth=6),\n                  horizontalalignment='right', verticalalignment='top',\n                  )\n    ax.annotate('', xy=(21.5, 500),  xycoords='data',\n                  xytext=(21.5,950), textcoords='data', ha='center', size='12',\n                  arrowprops=dict(facecolor='black', shrink=0.15, width=1,\n                    headwidth=6),\n                  horizontalalignment='right', verticalalignment='bottom',\n                  )\n  \n  plt.xticks(np.arange(6,24,2))\n  ax = plt.gca().yaxis \n  ax.set_major_formatter(matplotlib.ticker.ScalarFormatter())\n  plt.ticklabel_format(axis='y',style='plain')\n  plt.grid(which='both')\n  plt.ylabel('End-of-mission parallax standard error [$\\mu$as]')\n  \n  if (args['pdfOutput']):\n    plt.savefig('ParallaxErrors.pdf')\n  elif (args['pngOutput']):\n    plt.savefig('ParallaxErrors.png')\n  else:\n    plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plotBrightLimitInV(gBright, pdf=False, png=False):\n  vmini=np.linspace(0.0,6.0,1001)\n  gminv=gminvFromVmini(vmini)\n  vBright=gBright-gminv\n\n  fig=plt.figure(figsize=(10,6.5))\n  plt.plot(vmini,vBright,'b-')\n  plt.xlabel('$(V-I)$')\n  plt.ylabel('Bright limit of Gaia in $V$')\n  plt.xlim(0,6)\n  plt.ylim(5,11)\n  plt.grid(which='both')\n  plt.title(\"Bright limit in $G$: {0}\".format(gBright))\n\n  if (pdf):\n    plt.savefig('VBandBrightLimit.pdf')\n  elif (png):\n    plt.savefig('VBandBrightLimit.png')\n  else:\n    plt.show()", "response": "Plots the bright limit of Gaia in V as a function of ( V - I )."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting spherical coordinates to Cartesian coordinates.", "response": "def sphericalToCartesian(r, phi, theta):\n  \"\"\"\n  Convert spherical to Cartesian coordinates. The input can be scalars or 1-dimensional numpy arrays.\n  Note that the angle coordinates follow the astronomical convention of using elevation (declination,\n  latitude) rather than its complement (pi/2-elevation), where the latter is commonly used in the\n  mathematical treatment of spherical coordinates.\n\n  Parameters\n  ----------\n\n  r     - length of input Cartesian vector.\n  phi   - longitude-like angle (e.g., right ascension, ecliptic longitude) in radians\n  theta - latitide-like angle (e.g., declination, ecliptic latitude) in radians\n\n  Returns\n  -------\n  \n  The Cartesian vector components x, y, z\n  \"\"\"\n  ctheta=cos(theta)\n  x=r*cos(phi)*ctheta\n  y=r*sin(phi)*ctheta\n  z=r*sin(theta)\n  return x, y, z"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert cartesian coordinates to spherical coordinates.", "response": "def cartesianToSpherical(x, y, z):\n  \"\"\"\n  Convert Cartesian to spherical coordinates. The input can be scalars or 1-dimensional numpy arrays.\n  Note that the angle coordinates follow the astronomical convention of using elevation (declination,\n  latitude) rather than its complement (pi/2-elevation), which is commonly used in the mathematical\n  treatment of spherical coordinates.\n\n  Parameters\n  ----------\n  \n  x - Cartesian vector component along the X-axis\n  y - Cartesian vector component along the Y-axis\n  z - Cartesian vector component along the Z-axis\n\n  Returns\n  -------\n  \n  The spherical coordinates r=sqrt(x*x+y*y+z*z), longitude phi, latitude theta.\n  \n  NOTE THAT THE LONGITUDE ANGLE IS BETWEEN 0 AND +2PI. FOR r=0 AN EXCEPTION IS RAISED.\n  \"\"\"\n  rCylSq=x*x+y*y\n  r=sqrt(rCylSq+z*z)\n  if any(r==0.0):\n    raise Exception(\"Error: one or more of the points is at distance zero.\")\n  phi = arctan2(y,x)\n  phi = where(phi<0.0, phi+2*pi, phi)\n  return r, phi, arctan2(z,sqrt(rCylSq))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the so - called normal triad as the vectors p q r", "response": "def normalTriad(phi, theta):\n  \"\"\"\n  Calculate the so-called normal triad [p, q, r] which is associated with a spherical coordinate system .\n  The three vectors are:\n\n  p - The unit tangent vector in the direction of increasing longitudinal angle phi.\n  q - The unit tangent vector in the direction of increasing latitudinal angle theta.\n  r - The unit vector toward the point (phi, theta).\n\n  Parameters\n  ----------\n\n  phi   - longitude-like angle (e.g., right ascension, ecliptic longitude) in radians\n  theta - latitide-like angle (e.g., declination, ecliptic latitude) in radians\n  \n  Returns\n  -------\n\n  The normal triad as the vectors p, q, r\n  \"\"\"\n  sphi=sin(phi)\n  stheta=sin(theta)\n  cphi=cos(phi)\n  ctheta=cos(theta)\n  p=array([-sphi, cphi, zeros_like(phi)])\n  q=array([-stheta*cphi, -stheta*sphi, ctheta])\n  r=array([ctheta*cphi, ctheta*sphi, stheta])\n  return p, q, r"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef elementaryRotationMatrix(axis, rotationAngle):\n  if (axis==\"x\" or axis==\"X\"):\n    return array([[1.0, 0.0, 0.0], [0.0, cos(rotationAngle), sin(rotationAngle)], [0.0,\n      -sin(rotationAngle), cos(rotationAngle)]])\n  elif (axis==\"y\" or axis==\"Y\"):\n    return array([[cos(rotationAngle), 0.0, -sin(rotationAngle)], [0.0, 1.0, 0.0], [sin(rotationAngle),\n      0.0, cos(rotationAngle)]])\n  elif (axis==\"z\" or axis==\"Z\"):\n    return array([[cos(rotationAngle), sin(rotationAngle), 0.0], [-sin(rotationAngle),\n      cos(rotationAngle), 0.0], [0.0, 0.0, 1.0]])\n  else:\n    raise Exception(\"Unknown rotation axis \"+axis+\"!\")", "response": "Constructs an elementary rotation matrix describing a rotation around the x y or z - axis."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef phaseSpaceToAstrometry(x, y, z, vx, vy, vz):\n  u, phi, theta = cartesianToSpherical(x, y, z)\n  parallax = _auMasParsec/u\n  p, q, r = normalTriad(phi, theta)\n  velocitiesArray=array([vx,vy,vz])\n  if  isscalar(u):\n    muphistar=dot(p,velocitiesArray)*parallax/_auKmYearPerSec\n    mutheta=dot(q,velocitiesArray)*parallax/_auKmYearPerSec\n    vrad=dot(r,velocitiesArray)\n  else:\n    muphistar=zeros_like(parallax)\n    mutheta=zeros_like(parallax)\n    vrad=zeros_like(parallax)\n    for i in range(parallax.size):\n      muphistar[i]=dot(p[:,i],velocitiesArray[:,i])*parallax[i]/_auKmYearPerSec\n      mutheta[i]=dot(q[:,i],velocitiesArray[:,i])*parallax[i]/_auKmYearPerSec\n      vrad[i]=dot(r[:,i],velocitiesArray[:,i])\n\n  return phi, theta, parallax, muphistar, mutheta, vrad", "response": "This function converts a phase space coordinates to astrometry observables."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef astrometryToPhaseSpace(phi, theta, parallax, muphistar, mutheta, vrad):\n  if any(parallax<=0.0):\n    raise Exception(\"One or more of the input parallaxes is non-positive\")\n  x, y, z = sphericalToCartesian(_auMasParsec/parallax, phi, theta)\n  p, q, r = normalTriad(phi, theta)\n  transverseMotionArray = array([muphistar*_auKmYearPerSec/parallax, mutheta*_auKmYearPerSec/parallax,\n    vrad])\n  if isscalar(parallax):\n    velocityArray=dot(transpose(array([p, q, r])),transverseMotionArray)\n    vx = velocityArray[0]\n    vy = velocityArray[1]\n    vz = velocityArray[2]\n  else:\n    vx = zeros_like(parallax)\n    vy = zeros_like(parallax)\n    vz = zeros_like(parallax)\n    for i in range(parallax.size):\n      velocityArray = dot(transpose(array([p[:,i], q[:,i], r[:,i]])), transverseMotionArray[:,i])\n      vx[i] = velocityArray[0]\n      vy[i] = velocityArray[1]\n      vz[i] = velocityArray[2]\n  return x, y, z, vx, vy, vz", "response": "This function calculates the phase space coordinates from the input astrometric parameters."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef makePlot(args):\n  gmag=np.linspace(5.7,20.0,101)\n\n  vminiB1V=vminiFromSpt('B1V')\n  vminiG2V=vminiFromSpt('G2V')\n  vminiM6V=vminiFromSpt('M6V')\n  \n  vmagB1V=gmag-gminvFromVmini(vminiB1V)\n  vmagG2V=gmag-gminvFromVmini(vminiG2V)\n  vmagM6V=gmag-gminvFromVmini(vminiM6V)\n\n  sigmualphaB1V, sigmudeltaB1V = properMotionErrorSkyAvg(gmag,vminiB1V)\n  sigmuB1V = np.sqrt(0.5*sigmualphaB1V**2+0.5*sigmudeltaB1V**2)\n  sigmualphaB1V, sigmudeltaB1V = properMotionMinError(gmag,vminiB1V)\n  sigmuB1Vmin = np.sqrt(0.5*sigmualphaB1V**2+0.5*sigmudeltaB1V**2)\n  sigmualphaB1V, sigmudeltaB1V = properMotionMaxError(gmag,vminiB1V)\n  sigmuB1Vmax = np.sqrt(0.5*sigmualphaB1V**2+0.5*sigmudeltaB1V**2)\n  \n  sigmualphaG2V, sigmudeltaG2V = properMotionErrorSkyAvg(gmag,vminiG2V)\n  sigmuG2V = np.sqrt(0.5*sigmualphaG2V**2+0.5*sigmudeltaG2V**2)\n  sigmualphaG2V, sigmudeltaG2V = properMotionMinError(gmag,vminiG2V)\n  sigmuG2Vmin = np.sqrt(0.5*sigmualphaG2V**2+0.5*sigmudeltaG2V**2)\n  sigmualphaG2V, sigmudeltaG2V = properMotionMaxError(gmag,vminiG2V)\n  sigmuG2Vmax = np.sqrt(0.5*sigmualphaG2V**2+0.5*sigmudeltaG2V**2)\n  \n  sigmualphaM6V, sigmudeltaM6V = properMotionErrorSkyAvg(gmag,vminiM6V)\n  sigmuM6V = np.sqrt(0.5*sigmualphaM6V**2+0.5*sigmudeltaM6V**2)\n  sigmualphaM6V, sigmudeltaM6V = properMotionMinError(gmag,vminiM6V)\n  sigmuM6Vmin = np.sqrt(0.5*sigmualphaM6V**2+0.5*sigmudeltaM6V**2)\n  sigmualphaM6V, sigmudeltaM6V = properMotionMaxError(gmag,vminiM6V)\n  sigmuM6Vmax = np.sqrt(0.5*sigmualphaM6V**2+0.5*sigmudeltaM6V**2)\n  \n  fig=plt.figure(figsize=(10,6.5))\n  \n  if (args['gmagAbscissa']):\n    plt.semilogy(gmag, sigmuB1V, 'b', label='B1V')\n    plt.semilogy(gmag, sigmuG2V, 'g', label='G2V')\n    plt.semilogy(gmag, sigmuM6V, 'r', label='M6V')\n    plt.xlim((5,20))\n    plt.ylim((1,500))\n    plt.legend(loc=4)\n  else:\n    ax=fig.add_subplot(111)\n    plt.semilogy(vmagB1V, sigmuB1V, 'b', label='B1V')\n    #plt.semilogy(vmagG2V, sigmuG2V, 'g', label='G2V')\n    plt.semilogy(vmagM6V, sigmuM6V, 'r', label='M6V')\n    plt.fill_between(vmagB1V, sigmuB1Vmin, sigmuB1Vmax, color='b', alpha=0.3)\n    plt.fill_between(vmagM6V, sigmuM6Vmin, sigmuM6Vmax, color='r', alpha=0.3)\n    plt.xlim((5,22.5))\n    plt.ylim((1,500))\n    plt.text(17.5,100,'B1V',color='b')\n    plt.text(18,10,'M6V',color='r')\n    plt.text(7,11,'calibration noise floor', size=12, bbox=dict(boxstyle=\"round,pad=0.3\",\n                       ec=(0.0, 0.0, 0.0),\n                       fc=(1.0, 1.0, 1.0),\n                       ))\n    plt.text(14.75,50,'photon noise', rotation=45, size=12, bbox=dict(boxstyle=\"round,pad=0.3\",\n                       ec=(0.0, 0.0, 0.0),\n                       fc=(1.0, 1.0, 1.0),\n                       ))\n    ax.annotate('non-uniformity\\nover the sky', xy=(21.5, 80),  xycoords='data',\n                  xytext=(21.5,30), textcoords='data', ha='center', size='12',\n                  bbox=dict(boxstyle=\"round,pad=0.3\",ec=(0,0,0),fc=(1,1,1)),\n                  arrowprops=dict(facecolor='black', shrink=0.15, width=1,\n                    headwidth=6),\n                  horizontalalignment='right', verticalalignment='top',\n                  )\n    ax.annotate('', xy=(21.5, 170),  xycoords='data',\n                  xytext=(21.5,380), textcoords='data', ha='center', size='12',\n                  arrowprops=dict(facecolor='black', shrink=0.15, width=1,\n                    headwidth=6),\n                  horizontalalignment='right', verticalalignment='bottom',\n                  )\n  \n  plt.xticks(np.arange(6,24,2))\n  ax = plt.gca().yaxis \n  ax.set_major_formatter(matplotlib.ticker.ScalarFormatter())\n  plt.ticklabel_format(axis='y',style='plain')\n  plt.grid(which='both')\n  plt.xlabel('$V$ [mag]')\n  plt.ylabel('End-of-mission $\\\\sigma_\\\\mu$ [$\\mu$as/yr]')\n  \n  basename = 'ProperMotionErrors'\n  if (args['pdfOutput']):\n    plt.savefig(basename+'.pdf')\n  elif (args['pngOutput']):\n    plt.savefig(basename+'.png')\n  else:\n    plt.show()", "response": "Make the plot with the TOTAL proper\n ective motion."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parseCommandLineArguments():\n  parser = argparse.ArgumentParser(description=\"Plot predicted Gaia sky averaged proper motion errors as a function of V\")\n  parser.add_argument(\"-p\", action=\"store_true\", dest=\"pdfOutput\", help=\"Make PDF plot\")\n  parser.add_argument(\"-b\", action=\"store_true\", dest=\"pngOutput\", help=\"Make PNG plot\")\n  parser.add_argument(\"-g\", action=\"store_true\", dest=\"gmagAbscissa\", help=\"Plot performance vs G instead of V\")\n  args=vars(parser.parse_args())\n  return args", "response": "Parse command line arguments."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef enum(typename, field_names):\n\n    if isinstance(field_names, str):\n        field_names = field_names.replace(',', ' ').split()\n    d = dict((reversed(nv) for nv in enumerate(field_names)), __slots__ = ())\n    return type(typename, (object,), d)()", "response": "Create a new enumeration type."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef construct_covariance_matrix(cvec, parallax, radial_velocity, radial_velocity_error):\n\n    if np.ndim(cvec)==1:\n        cmat = np.zeros((1,6,6))\n        nsources = 1\n        cv = np.atleast_2d(cvec)\n    else:\n        nsources = cvec.shape[0]\n        cmat = np.zeros((nsources,6,6))\n        cv = cvec\n    for k in range(nsources):\n        cmat[k,0:5,0:5] = cv[k,0:5]**2\n\n    iu = np.triu_indices(5,k=1)\n    for k in range(10):\n        i = iu[0][k]\n        j = iu[1][k]\n        cmat[:,i,j] = cv[:,i]*cv[:,j]*cv[:,k+5]\n        cmat[:,j,i] = cmat[:,i,j]\n\n    for k in range(nsources):\n        cmat[k,0:5,5] = cmat[k,0:5,2]*np.atleast_1d(radial_velocity)[k]/auKmYearPerSec\n    cmat[:,5,0:5] = cmat[:,0:5,5]\n    cmat[:,5,5] = cmat[:,2,2]*(radial_velocity**2 + radial_velocity_error**2)/auKmYearPerSec**2 + \\\n            (parallax*radial_velocity_error/auKmYearPerSec)**2\n\n    return np.squeeze(cmat)", "response": "Constructs the covariance matrix for a single species."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef makePlot(gmag, pdf=False, png=False, rvs=False):\n  vmini = np.linspace(-0.5,4.0,100)\n  if (rvs):\n    gminv = -vminGrvsFromVmini(vmini)\n  else:\n    gminv = gminvFromVmini(vmini)\n  mvlimit100pc = gmag-5.0*np.log10(100.0)+5.0-gminv\n  mvlimit1kpc = gmag-5.0*np.log10(1000.0)+5.0-gminv\n  mvlimit10kpc = gmag-5.0*np.log10(10000.0)+5.0-gminv\n\n  fig=plt.figure(figsize=(8,8))\n  plt.plot(vmini,mvlimit100pc,'b')\n  plt.text(vmini[50]-0.4,mvlimit100pc[50],\"$d=100$ pc\", horizontalalignment='right', va='top')\n  plt.plot(vmini,mvlimit1kpc,'r')\n  plt.text(vmini[50]-0.4,mvlimit1kpc[50],\"$d=1000$ pc\", horizontalalignment='right', va='top')\n  plt.plot(vmini,mvlimit10kpc,'g')\n  plt.text(vmini[50]-0.4,mvlimit10kpc[50],\"$d=10000$ pc\", horizontalalignment='right', va='top')\n  ax=plt.gca()\n  ax.set_ylim(ax.get_ylim()[::-1])\n  plt.xlabel(\"$(V-I)$\")\n  plt.ylabel(\"$M_V$\")\n  if (rvs):\n    plt.title(\"Distance limits for $G_\\\\mathrm{RVS}\"+\"={0}$\".format(gmag))\n  else:\n    plt.title(\"Distance limits for $G={0}$\".format(gmag))\n  \n  if (args['pdfOutput']):\n    plt.savefig('GaiaSurveyLimits.pdf')\n  elif (args['pngOutput']):\n    plt.savefig('GaiaSurveyLimits.png')\n  else:\n    plt.show()", "response": "Make a plot of a colour magnitude diagram of stars at G = 20."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef vradErrorSkyAvg(vmag, spt):\n  return _vradCalibrationFloor + _vradErrorBCoeff[spt]*exp(_vradErrorACoeff[spt]*(vmag-_vradMagnitudeZeroPoint))", "response": "Calculates the radial velocity error from V and the spectral type."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plotCoordinateTransformationOnSky(transformation, outfile=None, myProjection='hammer',\n    noTitle=False, noLabels=False, returnPlotObject=False):\n    \"\"\"\n    Produce a sky-plot in a given coordinate system with the meridians and paralles for another\n    coordinate system overlayed. The coordinate systems are specified through the\n    pygaia.coordinates.Transformations enum. For example for Transformations.GAL2ECL the sky plot will be\n    in Ecliptic coordinates with the Galactic coordinate grid overlayed.\n\n    Keywords\n    --------\n\n    transformation - The coordinate transformation for which to make the plot (e.g.,\n                     Transformations.GAL2ECL)\n    outfile        - Save plot to this output file (default is to plot on screen). Make sure an extension\n                     (.pdf, .png, etc) is included.\n    myProjection   - Use this map projection (default is 'hammer', see basemap documentation)\n    noTitle        - If true do not include the plot title.\n    noLabels       - If true do not include plot labels.\n    returnPlotObject - If true return the matplotlib object used for plotting. Further plot elements can\n                       then be added.\n    \"\"\"\n    ct = CoordinateTransformation(transformation)\n\n    parallels=arange(-80.0,90.0,10.0)\n    meridians=arange(0.0,375.0,15.0)\n    meridianMax=degreesToRadians(85.0)\n    parallelsMax=degreesToRadians(179.0)\n\n    fig=plt.figure(figsize=(12,6))\n    basemapInstance=Basemap(projection=myProjection,lon_0=0, celestial=True)\n    basemapInstance.drawmapboundary()\n\n    for thetaDeg in parallels:\n      phi=linspace(-pi,pi,1001)\n      theta=zeros_like(phi)+degreesToRadians(thetaDeg)\n      phirot, thetarot = ct.transformSkyCoordinates(phi, theta)\n      x ,y = basemapInstance(radiansToDegrees(phirot), radiansToDegrees(thetarot))\n      \n      indices=(phirot>=0.0)\n      xplot=x[indices]\n      yplot=y[indices]\n      if any(indices):\n        xplot, yplot = _orderGridlinePoints(xplot, yplot)\n      plt.plot(xplot,yplot,'b-')\n     \n      indices=(phirot<0.0)\n      xplot=x[indices]\n      yplot=y[indices]\n      if any(indices):\n        xplot, yplot = _orderGridlinePoints(xplot, yplot)\n      plt.plot(xplot,yplot,'b-')\n\n    for phiDeg in meridians:\n      theta=linspace(-meridianMax,meridianMax,1001)\n      phi=zeros_like(theta)+degreesToRadians(phiDeg)\n      phirot, thetarot = ct.transformSkyCoordinates(phi, theta)\n      x ,y = basemapInstance(radiansToDegrees(phirot), radiansToDegrees(thetarot))\n\n      indices=(phirot>=0.0)\n      xplot=x[indices]\n      yplot=y[indices]\n      if any(indices):\n        xplot, yplot = _orderGridlinePoints(xplot, yplot)\n      plt.plot(xplot,yplot,'b-')\n     \n      indices=(phirot<0.0)\n      xplot=x[indices]\n      yplot=y[indices]\n      if any(indices):\n        xplot, yplot = _orderGridlinePoints(xplot, yplot)\n      plt.plot(xplot,yplot,'b-')\n\n    if (not noTitle):\n      plt.title(\"Sky projection in \" + ct.transformationStrings[1] + \" coordinates with the corresponding \" + ct.transformationStrings[0] + \" grid overlayed\")\n\n    if (not noLabels):\n      for theta in arange(-60,90,30):\n        phirot, thetarot=ct.transformSkyCoordinates(0.0,degreesToRadians(theta))\n        x, y = basemapInstance(radiansToDegrees(phirot), radiansToDegrees(thetarot))\n        plt.text(x,y,\"${0}$\".format(theta),fontsize=16,va='bottom',ha='center',color='r')\n      for phi in arange(-180,0,30):\n        phirot, thetarot=ct.transformSkyCoordinates(degreesToRadians(phi),0.0)\n        x, y = basemapInstance(radiansToDegrees(phirot), radiansToDegrees(thetarot))\n        plt.text(x,y,\"${0}$\".format(phi),fontsize=16,va='bottom',ha='center',color='r')\n      for phi in arange(30,180,30):\n        phirot, thetarot=ct.transformSkyCoordinates(degreesToRadians(phi),0.0)\n        x, y = basemapInstance(radiansToDegrees(phirot), radiansToDegrees(thetarot))\n        plt.text(x,y,\"${0}$\".format(phi),fontsize=16,va='bottom',ha='center',color='r')\n\n    if (outfile != None):\n      plt.savefig(outfile)\n    elif (returnPlotObject):\n      return plt.gca(), basemapInstance\n    else:\n      plt.show()", "response": "Plots a coordinate transformation on a sky - plot."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef calcParallaxError(args):\n  gmag=float(args['gmag'])\n  vmini=float(args['vmini'])\n  sigmaPar=parallaxErrorSkyAvg(gmag, vmini)\n  gminv=gminvFromVmini(vmini)\n  print(\"G = {0}\".format(gmag))\n  print(\"V = {0}\".format(gmag-gminv))\n  print(\"(V-I) = {0}\".format(vmini))\n  print(\"(G-V) = {0}\".format(gminv))\n  print(\"standard error = {0} muas\".format(sigmaPar))", "response": "Calculates the parallax error for the given input source magnitude and colour."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing command line arguments.", "response": "def parseCommandLineArguments():\n  \"\"\"\n  Set up command line parsing.\n  \"\"\"\n  parser = argparse.ArgumentParser(description=\"Calculate parallax error for given G and (V-I)\")\n  parser.add_argument(\"gmag\", help=\"G-band magnitude of source\", type=float)\n  parser.add_argument(\"vmini\", help=\"(V-I) colour of source\", type=float)\n\n  args=vars(parser.parse_args())\n  return args"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef gMagnitudeError(G):\n  z=calcZ(G)\n  return 1.0e-3*sqrt(0.04895*z*z + 1.8633*z + 0.0001985) * _scienceMargin", "response": "Calculates the photometric standard error of a single - field - of - view transit G - band."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate the end of mission photometric standard error in the G - band.", "response": "def gMagnitudeErrorEoM(G, nobs=70):\n  \"\"\"\n  Calculate the end of mission photometric standard error in the G band as a function\n  of G. A 20% margin is included.\n\n  Parameters\n  ----------\n\n  G     - Value(s) of G-band magnitude.\n\n  Keywords\n  --------\n\n  nobs  - Number of observations collected (default 70).\n\n  Returns\n  -------\n\n  The G band photometric standard error in units of magnitude.\n  \"\"\"\n  return sqrt( (power(gMagnitudeError(G)/_scienceMargin,2) +\n      _eomCalibrationFloorG*_eomCalibrationFloorG)/nobs ) * _scienceMargin"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the photometric standard error of a single - field - of - view transit in the BP band.", "response": "def bpMagnitudeError(G, vmini):\n  \"\"\"\n  Calculate the single-field-of-view-transit photometric standard error in the BP band as a function\n  of G and (V-I). Note: this refers to the integrated flux from the BP spectrophotometer. A margin of 20%\n  is included.\n\n  Parameters\n  ----------\n\n  G     - Value(s) of G-band magnitude.\n  vmini - Value(s) of (V-I) colour.\n\n  Returns\n  -------\n\n  The BP band photometric standard error in units of magnitude.\n  \"\"\"\n  z=calcZBpRp(G)\n  a\t=\t-0.000562*power(vmini,3) + 0.044390*vmini*vmini + 0.355123*vmini + 1.043270\n  b\t=\t-0.000400*power(vmini,3) + 0.018878*vmini*vmini + 0.195768*vmini + 1.465592\n  c\t=\t+0.000262*power(vmini,3) + 0.060769*vmini*vmini - 0.205807*vmini - 1.866968\n  return 1.0e-3*sqrt(power(10.0,a)*z*z+power(10.0,b)*z+power(10.0,c))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bpMagnitudeErrorEoM(G, vmini, nobs=70):\n  return sqrt( (power(bpMagnitudeError(G, vmini)/_scienceMargin,2) +\n      _eomCalibrationFloorBP*_eomCalibrationFloorBP)/nobs ) * _scienceMargin", "response": "Calculates the end - of - mission photometric standard error in the BP band as a function of G and V - I."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the end - of - mission photometric standard error in the RP band as a function of G and V - I.", "response": "def rpMagnitudeErrorEoM(G, vmini, nobs=70):\n  \"\"\"\n  Calculate the end-of-mission photometric standard error in the RP band as a function of G and (V-I).\n  Note: this refers to the integrated flux from the RP spectrophotometer. A margin of 20% is included.\n\n  Parameters\n  ----------\n\n  G     - Value(s) of G-band magnitude.\n  vmini - Value(s) of (V-I) colour.\n\n  Keywords\n  --------\n\n  nobs  - Number of observations collected (default 70).\n\n  Returns\n  -------\n\n  The RP band photometric standard error in units of magnitude.\n  \"\"\"\n  return sqrt( (power(rpMagnitudeError(G, vmini)/_scienceMargin,2) +\n      _eomCalibrationFloorRP*_eomCalibrationFloorRP)/nobs ) * _scienceMargin"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmake the plot with photometry performance predictions.", "response": "def makePlot(args):\n  \"\"\"\n  Make the plot with photometry performance predictions.\n\n  :argument args: command line arguments\n  \"\"\"\n  gmag=np.linspace(3.0,20.0,171)\n\n  vmini = args['vmini']\n  \n  vmag=gmag-gminvFromVmini(vmini)\n  \n  if args['eom']:\n      sigmaG = gMagnitudeErrorEoM(gmag)\n      sigmaGBp = bpMagnitudeErrorEoM(gmag, vmini)\n      sigmaGRp = rpMagnitudeErrorEoM(gmag, vmini)\n      yminmax = (1.0-4,0.1)\n  else:\n      sigmaG = gMagnitudeError(gmag)\n      sigmaGBp = bpMagnitudeError(gmag, vmini)\n      sigmaGRp = rpMagnitudeError(gmag, vmini)\n      yminmax = (1.0-4,1)\n\n  fig=plt.figure(figsize=(10,6.5))\n  \n  if (args['vmagAbscissa']):\n    plt.semilogy(vmag, sigmaG, 'k', label='$\\\\sigma_G$')\n    plt.semilogy(vmag, sigmaGBp, 'b', label='$\\\\sigma_{G_\\\\mathrm{BP}}$'+' for $(V-I)={0}$'.format(vmini))\n    plt.semilogy(vmag, sigmaGRp, 'r', label='$\\\\sigma_{G_\\\\mathrm{RP}}$'+' for $(V-I)={0}$'.format(vmini))\n    plt.xlim((6,20))\n    #plt.ylim(yminmax)\n    plt.legend(loc=0)\n    plt.xlabel('$V$ [mag]')\n  else:\n    ax=fig.add_subplot(111)\n    plt.semilogy(gmag, sigmaG, 'k', label='$\\\\sigma_G$')\n    plt.semilogy(gmag, sigmaGBp, 'b', label='$\\\\sigma_{G_\\\\mathrm{BP}}$'+' for $(V-I)={0}$'.format(vmini))\n    plt.semilogy(gmag, sigmaGRp, 'r', label='$\\\\sigma_{G_\\\\mathrm{RP}}$'+' for $(V-I)={0}$'.format(vmini))\n    plt.xlim((6,20))\n    #plt.ylim(yminmax)\n    plt.legend(loc=0)\n    plt.xlabel('$G$ [mag]')\n  \n  plt.xticks(np.arange(6,20,2))\n  ax = plt.gca().yaxis \n  #ax.set_major_formatter(matplotlib.ticker.ScalarFormatter())\n  #plt.ticklabel_format(axis='y',style='plain')\n  plt.grid(which='both')\n  plt.ylabel('Photometric error [mag]')\n  if args['eom']:\n      plt.title('End-of-mission mean photometry: sky averaged errors for $(V-I)={0}$'.format(vmini), fontsize=14)\n  else:\n      plt.title('Single-FoV-transit photometry: sky averaged errors for $(V-I)={0}$'.format(vmini), fontsize=14)\n  \n  basename = 'PhotometricErrors'\n  if (args['pdfOutput']):\n    plt.savefig(basename+'.pdf')\n  elif (args['pngOutput']):\n    plt.savefig(basename+'.png')\n  else:\n    plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the value for the parameter z in the formula for parallax and G magnitude errors.", "response": "def calcZ(G):\n  \"\"\"\n  Calculate the value for the parameter z in the formula for parallax and G magnitude errors as a\n  function of G and (V-I).\n\n  Parameters\n  ----------\n  \n  G - Value of G-band magnitude.\n\n  Returns\n  -------\n  \n  Value of z.\n  \"\"\"\n  gatefloor=power(10.0,0.4*(12.0-15.0))\n  if isscalar(G):\n   result=amax((gatefloor,power(10.0,0.4*(G-15.0))))\n  else :\n    result=power(10.0,0.4*(G-15.0))\n    indices=(result<gatefloor)\n    result[indices]=gatefloor\n  return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef calcZBpRp(G):\n  gatefloor=power(10.0,0.4*(11.0-15.0))\n  if isscalar(G):\n   result=amax((gatefloor,power(10.0,0.4*(G-15.0))))\n  else :\n    result=power(10.0,0.4*(G-15.0))\n    indices=(result<gatefloor)\n    result[indices]=gatefloor\n  return result", "response": "Calculates the value for the parameter z for BP and RP magnitude errors."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef calcZAltStartGate(G):\n  gatefloor=power(10.0,0.4*(13.3-15.0))\n  if isscalar(G):\n   result=amax((gatefloor,power(10.0,0.4*(G-15.0))))\n  else :\n    result=power(10.0,0.4*(G-15.0))\n    indices=(result<gatefloor)\n    result[indices]=gatefloor\n  return result", "response": "Calculates the value of z in the formula for the parallax errors."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the average number of transits across the Gaia focal plane over ecliptic latitude.", "response": "def averageNumberOfTransits(beta):\n  \"\"\"\n  Returns the number of transits across the Gaia focal plane averaged over ecliptic longitude.\n\n  Parameters\n  ----------\n\n  beta - Value(s) of the Ecliptic latitude.\n\n  Returns\n  -------\n\n  Average number of transits for the input values of beta.\n  \"\"\"\n  indices = array(floor(abs(sin(beta))*_numStepsSinBeta), dtype=int)\n  indices[(indices==_numStepsSinBeta)] = _numStepsSinBeta-1\n  return _averageTransitNumber[indices]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef angularDistance(phi1, theta1, phi2, theta2):\n    # Formula below is more numerically stable than arccos( sin(theta1)*sin(theta2) +\n    # cos(phi2-phi1)*cos(theta1)*cos(theta2) )\n    # See: https://en.wikipedia.org/wiki/Great-circle_distance\n    return arctan( sqrt((cos(theta2)*sin(phi2-phi1))**2 +\n        (cos(theta1)*sin(theta2)-sin(theta1)*cos(theta2)*cos(phi2-phi1))**2) / (sin(theta1)*sin(theta2) +\n            cos(phi2-phi1)*cos(theta1)*cos(theta2)) )", "response": "Calculates the angular distance between two pairs of sky coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef transformCartesianCoordinates(self, x, y, z):\n        xrot, yrot, zrot = dot(self.rotationMatrix,[x,y,z])\n        return xrot, yrot, zrot", "response": "Rotates Cartesian coordinates from one reference system to another using the rotation matrix."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef transformSkyCoordinates(self, phi, theta):\n        r=ones_like(phi)\n        x, y, z = sphericalToCartesian(r, phi, theta)\n        xrot, yrot, zrot = self.transformCartesianCoordinates(x, y, z)\n        r, phirot, thetarot = cartesianToSpherical(xrot, yrot, zrot)\n        return phirot, thetarot", "response": "Converts sky coordinates from one reference system to another making use of the rotation matrix with the rotation matrix with the class initialized."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef transformProperMotions(self, phi, theta, muphistar, mutheta):\n        c, s = self._getJacobian(phi,theta)\n        return c*muphistar+s*mutheta, c*mutheta-s*muphistar", "response": "This function transforms the given longitude - like angle and proper motion values into a proper motion vector."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts the sky coordinate errors from one reference system to another, including the covariance term. Equations (1.5.4) and (1.5.20) from section 1.5 in the Hipparcos Explanatory Volume 1 are used. Parameters ---------- phi - The longitude-like angle of the position of the source (radians). theta - The latitude-like angle of the position of the source (radians). sigPhiStar - Standard error in the longitude-like angle of the position of the source (radians or sexagesimal units, including cos(latitude) term) sigTheta - Standard error in the latitude-like angle of the position of the source (radians or sexagesimal units) Keywords (optional) ------------------- rhoPhiTheta - Correlation coefficient of the position errors. Set to zero if this keyword is not provided. Retuns ------ sigPhiRotStar - The transformed standard error in the longitude-like angle (including cos(latitude) factor) sigThetaRot - The transformed standard error in the latitude-like angle. rhoPhiThetaRot - The transformed correlation coefficient.", "response": "def transformSkyCoordinateErrors(self, phi, theta, sigPhiStar, sigTheta, rhoPhiTheta=0):\n        \"\"\"\n        Converts the sky coordinate errors from one reference system to another, including the covariance\n        term. Equations (1.5.4) and (1.5.20) from section 1.5 in the Hipparcos Explanatory Volume 1 are used.\n\n        Parameters\n        ----------\n\n        phi         - The longitude-like angle of the position of the source (radians).\n        theta       - The latitude-like angle of the position of the source (radians).\n        sigPhiStar  - Standard error in the longitude-like angle of the position of the source (radians or\n                      sexagesimal units, including cos(latitude) term)\n        sigTheta    - Standard error in the latitude-like angle of the position of the source (radians or\n                      sexagesimal units)\n\n        Keywords (optional)\n        -------------------\n\n        rhoPhiTheta - Correlation coefficient of the position errors. Set to zero if this keyword is not\n                      provided.\n\n        Retuns\n        ------\n        \n        sigPhiRotStar  - The transformed standard error in the longitude-like angle (including\n                         cos(latitude) factor)\n        sigThetaRot    - The transformed standard error in the latitude-like angle.\n        rhoPhiThetaRot - The transformed correlation coefficient.\n        \"\"\"\n        if isscalar(rhoPhiTheta) and not isscalar(sigTheta):\n            rhoPhiTheta=zeros_like(sigTheta)+rhoPhiTheta\n        c, s = self._getJacobian(phi,theta)\n        cSqr = c*c\n        sSqr = s*s\n        covar = sigPhiStar*sigTheta*rhoPhiTheta\n        varPhiStar = sigPhiStar*sigPhiStar\n        varTheta = sigTheta*sigTheta\n        varPhiStarRot = cSqr*varPhiStar+sSqr*varTheta+2.0*covar*c*s\n        varThetaRot = sSqr*varPhiStar+cSqr*varTheta-2.0*covar*c*s\n        covarRot = (cSqr-sSqr)*covar+c*s*(varTheta-varPhiStar)\n        return sqrt(varPhiStarRot), sqrt(varThetaRot), covarRot/sqrt(varPhiStarRot*varThetaRot)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts the proper motion errors from one reference system to another, including the covariance term. Equations (1.5.4) and (1.5.20) from section 1.5 in the Hipparcos Explanatory Volume 1 are used. Parameters ---------- phi - The longitude-like angle of the position of the source (radians). theta - The latitude-like angle of the position of the source (radians). sigMuPhiStar - Standard error in the proper motion in the longitude-like direction (including cos(latitude) factor). sigMuTheta - Standard error in the proper motion in the latitude-like direction. Keywords (optional) ------------------- rhoMuPhiMuTheta - Correlation coefficient of the proper motion errors. Set to zero if this keyword is not provided. Retuns ------ sigMuPhiRotStar - The transformed standard error in the proper motion in the longitude direction (including cos(latitude) factor). sigMuThetaRot - The transformed standard error in the proper motion in the longitude direction. rhoMuPhiMuThetaRot - The transformed correlation coefficient.", "response": "def transformProperMotionErrors(self, phi, theta, sigMuPhiStar, sigMuTheta, rhoMuPhiMuTheta=0):\n        \"\"\"\n        Converts the proper motion errors from one reference system to another, including the covariance\n        term. Equations (1.5.4) and (1.5.20) from section 1.5 in the Hipparcos Explanatory Volume 1 are used.\n\n        Parameters\n        ----------\n\n        phi             - The longitude-like angle of the position of the source (radians).\n        theta           - The latitude-like angle of the position of the source (radians).\n        sigMuPhiStar    - Standard error in the proper motion in the longitude-like direction (including\n        cos(latitude) factor).\n        sigMuTheta      - Standard error in the proper motion in the latitude-like direction.\n\n        Keywords (optional)\n        -------------------\n\n        rhoMuPhiMuTheta - Correlation coefficient of the proper motion errors. Set to zero if this \n                          keyword is not provided.\n\n        Retuns\n        ------\n        \n        sigMuPhiRotStar    - The transformed standard error in the proper motion in the longitude direction\n        (including cos(latitude) factor).\n        sigMuThetaRot      - The transformed standard error in the proper motion in the longitude direction.\n        rhoMuPhiMuThetaRot - The transformed correlation coefficient.\n        \"\"\"\n        return self.transformSkyCoordinateErrors(phi, theta, sigMuPhiStar, sigMuTheta,\n                rhoPhiTheta=rhoMuPhiMuTheta)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntransform the astrometric covariance matrix to its representation in the new coordinate system.", "response": "def transformCovarianceMatrix(self, phi, theta, covmat):\n        \"\"\"\n        Transform the astrometric covariance matrix to its representation in the new coordinate system.\n\n        Parameters\n        ----------\n\n        phi       - The longitude-like angle of the position of the source (radians).\n        theta     - The latitude-like angle of the position of the source (radians).\n        covmat    - Covariance matrix (5x5) of the astrometric parameters.\n\n        Returns\n        -------\n\n        covmat_rot - Covariance matrix in its representation in the new coordinate system.\n        \"\"\"\n\n        c, s = self._getJacobian(phi,theta)\n        jacobian = identity(5)\n        jacobian[0][0]=c\n        jacobian[1][1]=c\n        jacobian[3][3]=c\n        jacobian[4][4]=c\n        jacobian[0][1]=s\n        jacobian[1][0]=-s\n        jacobian[3][4]=s\n        jacobian[4][3]=-s\n\n        return dot( dot(jacobian, covmat), jacobian.T )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _getJacobian(self, phi, theta):\n\n        p, q, r = normalTriad(phi, theta)\n\n        # zRot = z-axis of new coordinate system expressed in terms of old system\n        zRot = self.rotationMatrix[2,:]\n\n        if (p.ndim == 2):\n            zRotAll = tile(zRot, p.shape[1]).reshape(p.shape[1],3)\n            pRot = cross(zRotAll, r.T)\n            normPRot = norm(pRot,axis=1)\n            for i in range(pRot.shape[0]):\n                pRot[i] = pRot[i]/normPRot[i]\n            c = zeros(pRot.shape[0])\n            s = zeros(pRot.shape[0])\n            for i in range(pRot.shape[0]):\n                c[i] = dot(pRot[i], p.T[i])\n                s[i] = dot(pRot[i], q.T[i])\n            return c, s\n        else:\n            pRot = cross(zRot, r.T)\n            pRot = pRot/norm(pRot)\n            return dot(pRot,p), dot(pRot,q)", "response": "Calculates the Jacobian for the transformation of the position errors and proper motion errors and desired coordinate system."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef propagate_astrometry(self, phi, theta, parallax, muphistar, mutheta, vrad, t0, t1):\n\n        t = t1-t0\n        p0, q0, r0 = normalTriad(phi, theta)\n\n        # Convert input data to units of radians and Julian year. Use ICRS coordinate names internally to\n        # avoid errors in translating the formulae to code.\n        pmra0 = muphistar*self.mastorad\n        pmdec0 = mutheta*self.mastorad\n        plx0 = parallax*self.mastorad\n        pmr0 = vrad*parallax/auKmYearPerSec*self.mastorad\n        pmtot0sqr = (muphistar**2 + mutheta**2) * self.mastorad**2\n\n        # Proper motion vector\n        pmvec0 = pmra0*p0+pmdec0*q0\n\n        f = (1 + 2*pmr0*t + (pmtot0sqr+pmr0**2)*t**2)**(-0.5)\n        u = (r0*(1+pmr0*t) + pmvec0*t)*f\n\n        _, phi1, theta1 = cartesianToSpherical(u[0], u[1], u[2])\n        parallax1 = parallax*f\n        pmr1 = (pmr0+(pmtot0sqr + pmr0**2)*t)*f**2\n        pmvec1 = (pmvec0*(1+pmr0*t) - r0*pmr0**2*t)*f**3\n        p1, q1, r1 = normalTriad(phi1, theta1)\n        muphistar1 = sum(p1*pmvec1/self.mastorad, axis=0)\n        mutheta1 = sum(q1*pmvec1/self.mastorad, axis =0)\n        murad1 = pmr1/self.mastorad\n\n        return phi1, theta1, parallax1, muphistar1, mutheta1, murad1", "response": "Propagate the position of a source from the reference epoch t0 to the new epoch t1."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef propagate_pos(self, phi, theta, parallax, muphistar, mutheta, vrad, t0, t1):\n        phi1, theta1, parallax1, muphistar1, mutheta1, vrad1 = self.epoch_prop_astrometry(phi, theta, parallax, muphistar, mutheta, vrad, t0, t1)\n        return phi1, theta1", "response": "Propagate the position of a source from the reference epoch t0 to the new epoch t1."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef propagate_astrometry_and_covariance_matrix(self, a0, c0, t0, t1):\n\n        zero, one, two, three = 0, 1, 2, 3\n        tau = t1-t0\n\n        # Calculate the normal triad [p0 q0 r0] at t0\n        p0, q0, r0 = normalTriad(a0[0], a0[1])\n\n        # Convert to internal units (radians, Julian year)\n        par0 = a0[2]*self.mastorad\n        pma0 = a0[3]*self.mastorad\n        pmd0 = a0[4]*self.mastorad\n        pmr0 = a0[5]*a0[2]/auKmYearPerSec*self.mastorad\n\n        # Proper motion vector\n        pmvec0 = pma0*p0+pmd0*q0\n\n        # Auxiliary quantities\n        tau2 = tau*tau\n        pm02 = pma0**2 + pmd0**2\n        w = one + pmr0*tau\n        f2 = one/(one + two*pmr0*tau + (pm02+pmr0**2)*tau2)\n        f = sqrt(f2)\n        f3 = f2*f\n        f4 = f2*f2\n\n        # Position vector and parallax at t1\n        u = (r0*w + pmvec0*tau)*f\n        _, ra, dec = cartesianToSpherical(u[0], u[1], u[2])\n        par = par0*f\n\n        # Proper motion vector and radial proper motion at t1\n        pmvec = (pmvec0*(one+pmr0*tau) - r0*pmr0**2*tau)*f3\n        pmr = (pmr0+(pm02 + pmr0**2)*tau)*f2\n\n        # Normal triad at t1\n        p, q, r = normalTriad(ra, dec)\n\n        # Convert parameters at t1 to external units (mas, Julian year)\n        pma = sum(p*pmvec, axis=0)\n        pmd = sum(q*pmvec, axis =0)\n\n        a = zeros_like(a0)\n        a[0] = ra\n        a[1] = dec\n        a[2] = par/self.mastorad\n        a[3] = pma/self.mastorad\n        a[4] = pmd/self.mastorad\n        a[5] = pmr/self.mastorad\n\n        # Auxiliary quantities for the partial derivatives\n\n        pmz = pmvec0*f - three*pmvec*w\n        pp0 = sum(p*p0, axis=0)\n        pq0 = sum(p*q0, axis=0)\n        pr0 = sum(p*r0, axis=0)\n        qp0 = sum(q*p0, axis=0)\n        qq0 = sum(q*q0, axis=0)\n        qr0 = sum(q*r0, axis=0)\n        ppmz = sum(p*pmz, axis=0)\n        qpmz = sum(q*pmz, axis=0)\n\n        J = zeros_like(c0)\n        if (c0.ndim==2):\n            J = J[newaxis,:,:]\n\n        # Partial derivatives\n        J[:,0,0] = pp0*w*f - pr0*pma0*tau*f\n        J[:,0,1] = pq0*w*f - pr0*pmd0*tau*f\n        J[:,0,2] = zero\n        J[:,0,3] = pp0*tau*f\n        J[:,0,4] = pq0*tau*f\n        J[:,0,5] = -pma*tau2\n\n        J[:,1,0] = qp0*w*f - qr0*pma0*tau*f\n        J[:,1,1] = qq0*w*f - qr0*pmd0*tau*f\n        J[:,1,2] = zero\n        J[:,1,3] = qp0*tau*f\n        J[:,1,4] = qq0*tau*f\n        J[:,1,5] = -pmd*tau2\n\n        J[:,2,0] = zero\n        J[:,2,1] = zero\n        J[:,2,2] = f\n        J[:,2,3] = -par*pma0*tau2*f2\n        J[:,2,4] = -par*pmd0*tau2*f2\n        J[:,2,5] = -par*w*tau*f2\n\n        J[:,3,0] = -pp0*pm02*tau*f3 - pr0*pma0*w*f3\n        J[:,3,1] = -pq0*pm02*tau*f3 - pr0*pmd0*w*f3\n        J[:,3,2] = zero\n        J[:,3,3] = pp0*w*f3 - two*pr0*pma0*tau*f3 - three*pma*pma0*tau2*f2\n        J[:,3,4] = pq0*w*f3 - two*pr0*pmd0*tau*f3 - three*pma*pmd0*tau2*f2\n        J[:,3,5] = ppmz*tau*f2\n\n        J[:,4,0] = -qp0*pm02*tau*f3 - qr0*pma0*w*f3\n        J[:,4,1] = -qq0*pm02*tau*f3 - qr0*pmd0*w*f3\n        J[:,4,2] = zero\n        J[:,4,3] = qp0*w*f3 - two*qr0*pma0*tau*f3 - three*pmd*pma0*tau2*f2\n        J[:,4,4] = qq0*w*f3 - two*qr0*pmd0*tau*f3 - three*pmd*pmd0*tau2*f2\n        J[:,4,5] = qpmz*tau*f2\n\n        J[:,5,0] = zero\n        J[:,5,1] = zero\n        J[:,5,2] = zero\n        J[:,5,3] = two*pma0*w*tau*f4\n        J[:,5,4] = two*pmd0*w*tau*f4\n        J[:,5,5] = (w**2 - pm02*tau2)*f4\n\n        JT = zeros_like(J)\n        for i in range(J.shape[0]):\n            JT[i] = J[i].T\n\n        if (c0.ndim==2):\n            c = matmul(J,matmul(c0[newaxis,:,:],JT))\n        else:\n            c = matmul(J,matmul(c0,JT))\n\n        return a, squeeze(c)", "response": "Propagate the astrometric parameters and radial proper motion of aCOOKIE into the new epoch t0 and t1."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef makePlot(args):\n  distances = 10.0**np.linspace(1,6,10001)\n\n  spts=['B0V', 'A0V', 'F0V', 'G0V', 'K0V', 'K4V', 'K1III']\n  twokmsRV = []\n  twokmsV = []\n  vabsTwokms = []\n  fivekmsRV = []\n  fivekmsV = []\n  vabsFivekms = []\n  tenkmsRV = []\n  tenkmsV = []\n  vabsTenkms = []\n\n  fig=plt.figure(figsize=(11,7.8))\n  deltaHue = 240.0/(len(spts)-1)\n  hues = (240.0-np.arange(len(spts))*deltaHue)/360.0\n  hsv=np.zeros((1,1,3))\n  hsv[0,0,1]=1.0\n  hsv[0,0,2]=0.9\n  for hue,spt in zip(hues, spts):\n    hsv[0,0,0]=hue\n    vmags = vabsFromSpt(spt)+5.0*np.log10(distances)-5.0\n    vmini=vminiFromSpt(spt)\n    grvsmags = vmags - vminGrvsFromVmini(vmini)\n    rvError = vradErrorSkyAvg(vmags, spt)\n    observed = (grvsmags>=5.7) & (grvsmags<=16.1)\n    rvError = rvError[observed]\n    # Identify the points where the relative parallax accuracy is 0.1, 1, or 10 per cent.\n    if (rvError.min()<=2.0):\n      index = len(rvError[rvError<=2.0])-1\n      twokmsRV.append(distances[observed][index])\n      twokmsV.append(vmags[observed][index])\n      vabsTwokms.append(vabsFromSpt(spt))\n    if (rvError.min()<=5.0):\n      index = len(rvError[rvError<=5.0])-1\n      fivekmsRV.append(distances[observed][index])\n      fivekmsV.append(vmags[observed][index])\n      vabsFivekms.append(vabsFromSpt(spt))\n    if (rvError.min()<=10.0):\n      index = len(rvError[rvError<=10.0])-1\n      tenkmsRV.append(distances[observed][index])\n      tenkmsV.append(vmags[observed][index])\n      vabsTenkms.append(vabsFromSpt(spt))\n    plt.semilogx(distances[observed], vmags[observed], '-', label=spt, color=hsv_to_rgb(hsv)[0,0,:])\n    plt.text(distances[observed][-1], vmags[observed][-1], spt, horizontalalignment='center',\n        verticalalignment='bottom', fontsize=14)\n\n  # Draw the \"contours\" of constant radial velocity accuracy.\n  twokmsRV = np.array(twokmsRV)\n  twokmsV = np.array(twokmsV)\n  indices = np.argsort(vabsTwokms)\n  plt.semilogx(twokmsRV[indices],twokmsV[indices],'k--')\n  plt.text(twokmsRV[indices][-1]*0.8,twokmsV[indices][-1],\"$2$ km s$^{-1}$\", ha='right', size=16,\n      bbox=dict(boxstyle=\"round, pad=0.3\", ec=(0.0, 0.0, 0.0), fc=(1.0, 1.0, 1.0),))\n\n  fivekmsRV = np.array(fivekmsRV)\n  fivekmsV = np.array(fivekmsV)\n  indices = np.argsort(vabsFivekms)\n  plt.semilogx(fivekmsRV[indices],fivekmsV[indices],'k--')\n  plt.text(fivekmsRV[indices][-1]*0.8,fivekmsV[indices][-1],\"$5$ km s$^{-1}$\", ha='right', size=16,\n      bbox=dict(boxstyle=\"round, pad=0.3\", ec=(0.0, 0.0, 0.0), fc=(1.0, 1.0, 1.0),))\n\n  tenkmsRV = np.array(tenkmsRV)\n  tenkmsV = np.array(tenkmsV)\n  indices = np.argsort(vabsTenkms)\n  plt.semilogx(tenkmsRV[indices],tenkmsV[indices],'k--')\n  plt.text(tenkmsRV[indices][-1]*0.8,tenkmsV[indices][-1]+0.5,\"$10$ km s$^{-1}$\", ha='right', size=16,\n      bbox=dict(boxstyle=\"round, pad=0.3\", ec=(0.0, 0.0, 0.0), fc=(1.0, 1.0, 1.0),))\n\n  plt.title('Radial velocity accuracy horizons ($A_V=0$)')\n\n  plt.xlabel('Distance [pc]')\n  plt.ylabel('V')\n  plt.grid()\n  #leg=plt.legend(loc=4, fontsize=14, labelspacing=0.5)\n  plt.ylim(5,20)\n  \n  basename='RadialVelocityHorizons'\n  if (args['pdfOutput']):\n    plt.savefig(basename+'.pdf')\n  elif (args['pngOutput']):\n    plt.savefig(basename+'.png')\n  else:\n    plt.show()", "response": "A function to make a plot of the number of possible non - parallax horizons."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the numerical factors to apply to the error of the given astrometric parameter.", "response": "def errorScalingFactor(observable, beta):\n  \"\"\"\n  Look up the numerical factors to apply to the sky averaged parallax error in order to obtain error\n  values for a given astrometric parameter, taking the Ecliptic latitude and the number of transits into\n  account.\n\n  Parameters\n  ----------\n\n  observable - Name of astrometric observable (one of: alphaStar, delta, parallax, muAlphaStar, muDelta)\n  beta       - Values(s) of the Ecliptic latitude.\n\n  Returns\n  -------\n\n  Numerical factors to apply to the errors of the given observable.\n  \"\"\"\n  if isscalar(beta):\n    index=int(floor(abs(sin(beta))*_numStepsSinBeta))\n    if index == _numStepsSinBeta:\n      return _astrometricErrorFactors[observable][_numStepsSinBeta-1]\n    else:\n      return _astrometricErrorFactors[observable][index]\n  else:\n    indices = array(floor(abs(sin(beta))*_numStepsSinBeta), dtype=int)\n    indices[(indices==_numStepsSinBeta)] = _numStepsSinBeta-1\n    return _astrometricErrorFactors[observable][indices]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parallaxErrorSkyAvg(G, vmini, extension=0.0):\n  factor = errorScalingMissionLength(extension, -0.5)\n  z=calcZ(G)\n  return sqrt(-1.631 + 680.766*z + 32.732*z*z)*(0.986 + (1.0 - 0.986)*vmini)*factor", "response": "Calculate the parallax error from G and V - I."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parallaxMinError(G, vmini, extension=0.0):\n  return _astrometricErrorFactors[\"parallax\"].min()*parallaxErrorSkyAvg(G, vmini, extension=extension)", "response": "Calculates the minimum parallax error from G and V - I."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the maximum parallax error from G and V - I.", "response": "def parallaxMaxError(G, vmini, extension=0.0):\n  \"\"\"\n  Calculate the maximum parallax error from G and (V-I). This correspond to the sky regions with the\n  largest astrometric errors.  At the bright end the parallax error is at least 14 muas due to the\n  gating scheme.\n\n  Parameters\n  ----------\n\n  G     - Value(s) of G-band magnitude.\n  vmini - Value(s) of (V-I) colour.\n\n  Keywords\n  --------\n\n  extension - Add this amount of years to the mission lifetime and scale the errors accordingly.\n\n  Returns\n  -------\n\n  The maximum parallax error in micro-arcseconds.\n  \"\"\"\n  errors = _astrometricErrorFactors[\"parallax\"].max()*parallaxErrorSkyAvg(G, vmini, extension=extension)\n  indices = (errors<_parallaxErrorMaxBright)\n  errors[indices]=_parallaxErrorMaxBright\n  return errors"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parallaxErrorSkyAvgAltStartGate(G, vmini, extension=0.0):\n  factor = errorScalingMissionLength(extension, -0.5)\n  z=calcZAltStartGate(G)\n  return sqrt(-1.631 + 680.766*z + 32.732*z*z)*(0.986 + (1.0 - 0.986)*vmini)*factor", "response": "Calculate the parallax error from G and V - I."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef positionErrorSkyAvg(G, vmini, extension=0.0):\n  parallaxError = parallaxErrorSkyAvg(G, vmini, extension=extension)\n  return _scalingForPositions['AlphaStar']*parallaxError, \\\n         _scalingForPositions['Delta']*parallaxError", "response": "Calculate the sky averaged position errors from G and V - I."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the minimum position errors from G and V - I.", "response": "def positionMinError(G, vmini, extension=0.0):\n  \"\"\"\n  Calculate the minimum position errors from G and (V-I). These correspond to the sky regions with the\n  smallest astrometric errors.\n\n  NOTE! THE ERRORS ARE FOR SKY POSITIONS IN THE ICRS (I.E., RIGHT ASCENSION, DECLINATION). MAKE SURE YOUR\n  SIMULATED ASTROMETRY IS ALSO ON THE ICRS.\n\n  Parameters\n  ----------\n\n  G     - Value(s) of G-band magnitude.\n  vmini - Value(s) of (V-I) colour.\n\n  Keywords\n  --------\n\n  extension - Add this amount of years to the mission lifetime and scale the errors accordingly.\n\n  Returns\n  -------\n\n  The minimum error in alpha* and the error in delta, in that order, in micro-arcsecond.\n  \"\"\"\n  parallaxError = parallaxErrorSkyAvg(G, vmini, extension=extension)\n  return _astrometricErrorFactors['alphaStar'].min()*parallaxError, \\\n         _astrometricErrorFactors['delta'].min()*parallaxError"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the maximum position errors from G and V - I.", "response": "def positionMaxError(G, vmini, extension=0.0):\n  \"\"\"\n  Calculate the maximum position errors from G and (V-I). These correspond to the sky regions with the\n  largest astrometric errors.\n\n  NOTE! THE ERRORS ARE FOR SKY POSITIONS IN THE ICRS (I.E., RIGHT ASCENSION, DECLINATION). MAKE SURE YOUR\n  SIMULATED ASTROMETRY IS ALSO ON THE ICRS.\n\n  Parameters\n  ----------\n\n  G     - Value(s) of G-band magnitude.\n  vmini - Value(s) of (V-I) colour.\n\n  Keywords\n  --------\n\n  extension - Add this amount of years to the mission lifetime and scale the errors accordingly.\n\n  Returns\n  -------\n\n  The maximum error in alpha* and the error in delta, in that order, in micro-arcsecond.\n  \"\"\"\n  parallaxError = parallaxErrorSkyAvg(G, vmini, extension)\n  return _astrometricErrorFactors['alphaStar'].max()*parallaxError, \\\n         _astrometricErrorFactors['delta'].max()*parallaxError"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the position errors from G and V - I and the Ecliptic latitude beta.", "response": "def positionError(G, vmini, beta, extension=0.0):\n  \"\"\"\n  Calculate the position errors from G and (V-I) and the Ecliptic latitude beta of the source.\n\n  NOTE! THE ERRORS ARE FOR SKY POSITIONS IN THE ICRS (I.E., RIGHT ASCENSION, DECLINATION). MAKE SURE YOUR\n  SIMULATED ASTROMETRY IS ALSO ON THE ICRS.\n\n  Parameters\n  ----------\n\n  G     - Value(s) of G-band magnitude.\n  vmini - Value(s) of (V-I) colour.\n  beta  - Value(s) of the Ecliptic latitude.\n\n  Keywords\n  --------\n\n  extension - Add this amount of years to the mission lifetime and scale the errors accordingly.\n\n  Returns\n  -------\n\n  The error in alpha* and the error in delta, in that order, in micro-arcsecond.\n  \"\"\"\n  parallaxError = parallaxErrorSkyAvg(G, vmini, extension=extension)\n  return errorScalingFactor('alphaStar',beta)*parallaxError, \\\n         errorScalingFactor('delta',beta)*parallaxError"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the minimum error in a given minimum proper motion from G and V - I.", "response": "def properMotionMinError(G, vmini, extension=0.0):\n  \"\"\"\n  Calculate the minimum proper motion errors from G and (V-I). These correspond to the sky regions with\n  the smallest astrometric errors.\n\n  NOTE! THE ERRORS ARE FOR PROPER MOTIONS IN THE ICRS (I.E., RIGHT ASCENSION, DECLINATION). MAKE SURE\n  YOUR SIMULATED ASTROMETRY IS ALSO ON THE ICRS.\n\n  Parameters\n  ----------\n\n  G     - Value(s) of G-band magnitude.\n  vmini - Value(s) of (V-I) colour.\n\n  Keywords\n  --------\n\n  extension - Add this amount of years to the mission lifetime and scale the errors accordingly.\n\n  Returns\n  -------\n\n  The minimum error in mu_alpha* and the error in mu_delta, in that order, in micro-arcsecond/year.\n  \"\"\"\n  factor = errorScalingMissionLength(extension, -1.5)\n  parallaxError = parallaxErrorSkyAvg(G, vmini)*factor\n  return _astrometricErrorFactors['muAlphaStar'].min()*parallaxError, \\\n         _astrometricErrorFactors['muDelta'].min()*parallaxError"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef properMotionMaxError(G, vmini, extension=0.0):\n  factor = errorScalingMissionLength(extension, -1.5)\n  parallaxError = parallaxErrorSkyAvg(G, vmini)*factor\n  indices = (parallaxError<_parallaxErrorMaxBright)\n  parallaxError[indices] = _parallaxErrorMaxBright\n  return _astrometricErrorFactors['muAlphaStar'].max()*parallaxError, \\\n         _astrometricErrorFactors['muDelta'].max()*parallaxError", "response": "Calculates the maximum error in a given G and V - I."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef properMotionError(G, vmini, beta, extension=0.0):\n  factor = errorScalingMissionLength(extension, -1.5)\n  parallaxError = parallaxErrorSkyAvg(G, vmini)*factor\n  return errorScalingFactor('muAlphaStar',beta)*parallaxError, \\\n         errorScalingFactor('muDelta',beta)*parallaxError", "response": "Calculates the proper motion errors from G and V - I and the Ecliptic latitude beta."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef totalProperMotionErrorSkyAvg(G, vmini, extension=0.0):\n  factor = errorScalingMissionLength(extension, -1.5)\n  parallaxError = parallaxErrorSkyAvg(G, vmini)*factor\n  return _scalingForProperMotions['Total']*parallaxError", "response": "Calculates the total proper motion error from G and V - I."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmaking the plot of the number of possible non - parallax contours.", "response": "def makePlot(args):\n  \"\"\"\n  Make the plot with parallax horizons. The plot shows V-band magnitude vs distance for a number of\n  spectral types and over the range 5.7<G<20. In addition a set of crudely drawn contours show the points\n  where 0.1, 1, and 10 per cent relative parallax accracy are reached.\n\n  Parameters\n  ----------\n  \n  args - Command line arguments.\n  \"\"\"\n  distances = 10.0**np.linspace(1,6,10001)\n  av = args['extinction']\n  ai = 0.479*av #Cardelli et al R=3.1\n\n  spts = ['B0I', 'B1V', 'G2V', 'K4V', 'M0V', 'M6V', 'K1III', 'M0III']\n  pointOnePercD = []\n  pointOnePercV = []\n  onePercD = []\n  onePercV = []\n  tenPercD = []\n  tenPercV = []\n  vabsPointOnePerc = []\n  vabsOnePerc = []\n  vabsTenPerc = []\n\n  fig=plt.figure(figsize=(11,7.8))\n  deltaHue = 240.0/(len(spts)-1)\n  hues = (240.0-np.arange(len(spts))*deltaHue)/360.0\n  hsv=np.zeros((1,1,3))\n  hsv[0,0,1]=1.0\n  hsv[0,0,2]=0.9\n  for hue,spt in zip(hues, spts):\n    hsv[0,0,0]=hue\n    vmags = vabsFromSpt(spt)+5.0*np.log10(distances)-5.0+av\n    vmini=vminiFromSpt(spt)+av-ai\n    #gmags = gabsFromSpt(spt)+5.0*np.log10(distances)-5.0\n    gmags = vmags + gminvFromVmini(vmini)\n    relParErr = parallaxErrorSkyAvg(gmags,vmini)*distances/1.0e6\n    observed = (gmags>=5.7) & (gmags<=20.0)\n    relParErrObs = relParErr[observed]\n    # Identify the points where the relative parallax accuracy is 0.1, 1, or 10 per cent.\n    if (relParErrObs.min()<0.001):\n      index = len(relParErrObs[relParErrObs<=0.001])-1\n      pointOnePercD.append(distances[observed][index])\n      pointOnePercV.append(vmags[observed][index])\n      vabsPointOnePerc.append(vabsFromSpt(spt))\n    if (relParErrObs.min()<0.01):\n      index = len(relParErrObs[relParErrObs<=0.01])-1\n      onePercD.append(distances[observed][index])\n      onePercV.append(vmags[observed][index])\n      vabsOnePerc.append(vabsFromSpt(spt))\n    if (relParErrObs.min()<0.1):\n      index = len(relParErrObs[relParErrObs<=0.1])-1\n      tenPercD.append(distances[observed][index])\n      tenPercV.append(vmags[observed][index])\n      vabsTenPerc.append(vabsFromSpt(spt))\n    plt.semilogx(distances[observed], vmags[observed], '-', label=spt, color=hsv_to_rgb(hsv)[0,0,:])\n    if (spt=='B0I'):\n      plt.text(distances[observed][-1]-1.0e5, vmags[observed][-1], spt, horizontalalignment='right',\n          verticalalignment='bottom', fontsize=14)\n    else:\n      plt.text(distances[observed][-1], vmags[observed][-1], spt, horizontalalignment='center',\n          verticalalignment='bottom', fontsize=14)\n\n  # Draw the \"contours\" of constant relative parallax accuracy.\n  pointOnePercD = np.array(pointOnePercD)\n  pointOnePercV = np.array(pointOnePercV)\n  indices = np.argsort(vabsPointOnePerc)\n  plt.semilogx(pointOnePercD[indices],pointOnePercV[indices],'k--')\n  plt.text(pointOnePercD[indices][-1]*1.2,pointOnePercV[indices][-1]-2.5,\"$0.1$\\\\%\", ha='right', size=16,\n      bbox=dict(boxstyle=\"round, pad=0.3\", ec=(0.0, 0.0, 0.0), fc=(1.0, 1.0, 1.0),))\n\n  onePercD = np.array(onePercD)\n  onePercV = np.array(onePercV)\n  indices = np.argsort(vabsOnePerc)\n  plt.semilogx(onePercD[indices],onePercV[indices],'k--')\n  plt.text(onePercD[indices][-1]*1.2,onePercV[indices][-1]-2.5,\"$1$\\\\%\", ha='right', size=16,\n      bbox=dict(boxstyle=\"round, pad=0.3\", ec=(0.0, 0.0, 0.0), fc=(1.0, 1.0, 1.0),))\n\n  tenPercD = np.array(tenPercD)\n  tenPercV = np.array(tenPercV)\n  indices = np.argsort(vabsTenPerc)\n  plt.semilogx(tenPercD[indices],tenPercV[indices],'k--')\n  plt.text(tenPercD[indices][-1]*1.5,tenPercV[indices][-1]-2.5,\"$10$\\\\%\", ha='right', size=16,\n      bbox=dict(boxstyle=\"round, pad=0.3\", ec=(0.0, 0.0, 0.0), fc=(1.0, 1.0, 1.0),))\n\n  plt.title('Parallax relative accuracy horizons ($A_V={0}$)'.format(av))\n\n  plt.xlabel('Distance [pc]')\n  plt.ylabel('V')\n  plt.grid()\n  #leg=plt.legend(loc=4, fontsize=14, labelspacing=0.5)\n  plt.ylim(5,26)\n  \n  basename='ParallaxHorizons'\n  if (args['pdfOutput']):\n    plt.savefig(basename+'.pdf')\n  elif (args['pngOutput']):\n    plt.savefig(basename+'.png')\n  else:\n    plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the V - I for the input spectral type.", "response": "def vminiFromSpt(spt):\n  \"\"\"\n  Obtain (V-I) for the input spectral type.\n\n  Parameters\n  ----------\n  \n  spt - String representing the spectral type of the star.\n\n  Returns\n  -------\n  \n  The value of (V-I).\n  \"\"\"\n  if spt in _sptToVminiVabsDictionary:\n    return _sptToVminiVabsDictionary[spt][0]\n  else:\n    message=\"Unknown spectral type. Allowed values are: \"\n    for key in _sptToVminiVabsDictionary.keys():\n      message += key+\" \"\n    raise Exception(message)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef vabsFromSpt(spt):\n  if spt in _sptToVminiVabsDictionary:\n    return _sptToVminiVabsDictionary[spt][1]\n  else:\n    message=\"Unknown spectral type. Allowed values are: \"\n    for key in _sptToVminiVabsDictionary.keys():\n      message += key+\" \"\n    raise Exception(message)", "response": "Returns the absolute magnitude in V - band for the input spectral type."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef gabsFromSpt(spt):\n  if spt in _sptToVminiVabsDictionary:\n    return vabsFromSpt(spt) + gminvFromVmini(vminiFromSpt(spt))\n  else:\n    message=\"Unknown spectral type. Allowed values are: \"\n    for key in _sptToVminiVabsDictionary.keys():\n      message += key+\" \"\n    raise Exception(message)", "response": "Obtain M_G absolute magnitude in G - band for the input spectral type."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nplots relative parallax errors as a function of distance for stars of a given spectral type.", "response": "def makePlot(pdf=False, png=False):\n  \"\"\"\n  Plot relative parallax errors as a function of distance for stars of a given spectral type.\n\n  Parameters\n  ----------\n\n  args - command line arguments\n  \"\"\"\n  logdistancekpc = np.linspace(-1,np.log10(20.0),100)\n  sptVabsAndVmini=OrderedDict([('K0V',(5.58,0.87)), ('G5V',(4.78,0.74)), ('G0V',(4.24,0.67)),\n    ('F5V',(3.50,0.50)), ('F0V',(2.98,0.38)), ('RC',(0.8,1.0))])\n  lines={}\n\n  fig=plt.figure(figsize=(10,6.5))\n  currentAxis=plt.gca()\n\n  for spt in sptVabsAndVmini.keys():\n    vmag=sptVabsAndVmini[spt][0]+5.0*logdistancekpc+10.0\n    indices=(vmag>14) & (vmag<16)\n    gmag=vmag+gminvFromVmini(sptVabsAndVmini[spt][1])\n    parerrors=parallaxErrorSkyAvg(gmag,sptVabsAndVmini[spt][1])\n    relparerrors=parerrors*10**logdistancekpc/1000.0\n    plt.loglog(10**logdistancekpc, relparerrors,'--k',lw=1)\n    plt.loglog(10**logdistancekpc[indices], relparerrors[indices],'-',label=spt)\n  plt.xlim(0.1,20.0)\n  plt.ylim(0.001,0.5)\n  plt.text(0.9, 0.05,'Colours indicate $14<V<16$',\n     horizontalalignment='right',\n     verticalalignment='bottom',\n     transform = currentAxis.transAxes)\n  plt.legend(loc=2)\n  plt.xlabel('distance [kpc]')\n  plt.ylabel('$\\\\sigma_\\\\varpi/\\\\varpi$')\n  plt.grid(which='both')\n  \n  if (args['pdfOutput']):\n    plt.savefig('RelativeParallaxErrorsVsDist.pdf')\n  elif (args['pngOutput']):\n    plt.savefig('RelativeParallaxErrorsVsDist.png')\n  else:\n    plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmake the plot with radial velocity error.", "response": "def makePlot(args):\n  \"\"\"\n  Make the plot with radial velocity performance predictions.\n\n  :argument args: command line arguments\n  \"\"\"\n  gRvs=np.linspace(5.7,16.1,101)\n\n  spts=['B0V', 'B5V', 'A0V', 'A5V', 'F0V', 'G0V',\n        'G5V', 'K0V', 'K1IIIMP', 'K4V', 'K1III']\n\n  fig=plt.figure(figsize=(10,6.5))\n  deltaHue = 240.0/(len(spts)-1)\n  hsv=np.zeros((1,1,3))\n  hsv[0,0,1]=1.0\n  hsv[0,0,2]=0.9\n  count=0\n  for spt in spts:\n    hsv[0,0,0]=(240-count*deltaHue)/360.0\n    vmag = vminGrvsFromVmini(vminiFromSpt(spt)) + gRvs\n    vradErrors = vradErrorSkyAvg(vmag, spt)\n    plt.plot(vmag, vradErrors, '-', label=spt, color=hsv_to_rgb(hsv)[0,0,:])\n    count+=1\n  plt.grid(which='both')\n  plt.xlim(9,17.5)\n  plt.ylim(0,20)\n  plt.xticks(np.arange(9,18,1))\n  plt.yticks(np.arange(0,20.5,5))\n  plt.xlabel('$V$ [mag]')\n  plt.ylabel('End-of-mission radial velocity error [km s$^{-1}$]')\n  leg=plt.legend(loc=0,  handlelength=2.0, labelspacing=0.10)\n  for t in leg.get_texts():\n    t.set_fontsize(12)\n\n  if (args['pdfOutput']):\n    plt.savefig('RadialVelocityErrors.pdf')\n  elif (args['pngOutput']):\n    plt.savefig('RadialVelocityErrors.png')\n  else:\n    plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts to unicode and add quotes if initially a string", "response": "def _uniquote(value):\n    \"\"\"\n    Convert to unicode, and add quotes if initially a string\n    \"\"\"\n    if isinstance(value, six.binary_type):\n        try:\n            value = value.decode('utf-8')\n        except UnicodeDecodeError:  # Not utf-8. Show the repr\n            value = six.text_type(_dequote(repr(value)))  # trim quotes\n\n    result = six.text_type(value)\n\n    if isinstance(value, six.text_type):\n        result = \"'%s'\" % result\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef each(self, *funcs):\n\n        funcs = list(map(_make_callable, funcs))\n\n        if len(funcs) == 1:\n            return Collection(map(funcs[0], self._items))\n\n        tupler = lambda item: Scalar(\n            tuple(_unwrap(func(item)) for func in funcs))\n        return Collection(map(tupler, self._items))", "response": "Call each function on each element in the collection."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef exclude(self, func=None):\n        func = _make_callable(func)\n        inverse = lambda x: not func(x)\n        return self.filter(inverse)", "response": "Returns a new Collection excluding some items\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef filter(self, func=None):\n        func = _make_callable(func)\n        return Collection(filter(func, self._items))", "response": "Returns a new Collection with some items removed."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef takewhile(self, func=None):\n        func = _make_callable(func)\n        return Collection(takewhile(func, self._items))", "response": "Returns a new Collection with the last few items removed."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a new Collection with the first few items removed.", "response": "def dropwhile(self, func=None):\n        \"\"\"\n        Return a new Collection with the first few items removed.\n\n        Parameters:\n\n            func : function(Node) -> Node\n\n        Returns:\n\n            A new Collection, discarding all items\n            before the first item where bool(func(item)) == True\n        \"\"\"\n        func = _make_callable(func)\n        return Collection(dropwhile(func, self._items))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndumping a list of dicts by calling Node. dump on each item.", "response": "def dump(self, *args, **kwargs):\n        \"\"\"\n        Build a list of dicts, by calling :meth:`Node.dump`\n        on each item.\n\n        Each keyword provides a function that extracts a value\n        from a Node.\n\n        Examples:\n\n            >>> c = Collection([Scalar(1), Scalar(2)])\n            >>> c.dump(x2=Q*2, m1=Q-1).val()\n            [{'x2': 2, 'm1': 0}, {'x2': 4, 'm1': 1}]\n        \"\"\"\n        return self.each(Q.dump(*args, **kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a new collection with the items of this collection with one or more Collections and wrap the result.", "response": "def zip(self, *others):\n        \"\"\"\n        Zip the items of this collection with one or more\n        other sequences, and wrap the result.\n\n        Unlike Python's zip, all sequences must be the same length.\n\n        Parameters:\n\n            others: One or more iterables or Collections\n\n        Returns:\n\n            A new collection.\n\n        Examples:\n\n            >>> c1 = Collection([Scalar(1), Scalar(2)])\n            >>> c2 = Collection([Scalar(3), Scalar(4)])\n            >>> c1.zip(c2).val()\n            [(1, 3), (2, 4)]\n        \"\"\"\n        args = [_unwrap(item) for item in (self,) + others]\n        ct = self.count()\n        if not all(len(arg) == ct for arg in args):\n            raise ValueError(\"Arguments are not all the same length\")\n        return Collection(map(Wrapper.wrap, zip(*args)))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nturning this collection into a Scalar ( dict by zipping keys and items.", "response": "def dictzip(self, keys):\n        \"\"\"\n        Turn this collection into a Scalar(dict), by zipping keys and items.\n\n        Parameters:\n\n            keys: list or Collection of NavigableStrings\n                The keys of the dictionary\n\n        Examples:\n\n            >>> c = Collection([Scalar(1), Scalar(2)])\n            >>> c.dictzip(['a', 'b']).val() == {'a': 1, 'b': 2}\n            True\n        \"\"\"\n        return Scalar(dict(zip(_unwrap(keys), self.val())))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find(self, *args, **kwargs):\n        op = operator.methodcaller('find', *args, **kwargs)\n        return self._wrap_node(op)", "response": "Find a single Node among this Node s descendants."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nliking find but searches through next_siblings", "response": "def find_next_sibling(self, *args, **kwargs):\n        \"\"\"\n        Like :meth:`find`, but searches through :attr:`next_siblings`\n        \"\"\"\n        op = operator.methodcaller('find_next_sibling', *args, **kwargs)\n        return self._wrap_node(op)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlike find but searches through parents", "response": "def find_parent(self, *args, **kwargs):\n        \"\"\"\n        Like :meth:`find`, but searches through :attr:`parents`\n        \"\"\"\n        op = operator.methodcaller('find_parent', *args, **kwargs)\n        return self._wrap_node(op)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_previous_sibling(self, *args, **kwargs):\n        op = operator.methodcaller('find_previous_sibling', *args, **kwargs)\n        return self._wrap_node(op)", "response": "Like find but searches through previous_siblings"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_all(self, *args, **kwargs):\n        op = operator.methodcaller('find_all', *args, **kwargs)\n        return self._wrap_multi(op)", "response": "Like find but selects all matches."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_next_siblings(self, *args, **kwargs):\n        op = operator.methodcaller('find_next_siblings', *args, **kwargs)\n        return self._wrap_multi(op)", "response": "Like find_all but searches through next_siblings"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_parents(self, *args, **kwargs):\n        op = operator.methodcaller('find_parents', *args, **kwargs)\n        return self._wrap_multi(op)", "response": "Like find but searches through parents"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_previous_siblings(self, *args, **kwargs):\n        op = operator.methodcaller('find_previous_siblings', *args, **kwargs)\n        return self._wrap_multi(op)", "response": "Like find_all but searches through previous_siblings"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlikes find but takes a CSS selector string as input.", "response": "def select(self, selector):\n        \"\"\"\n        Like :meth:`find_all`, but takes a CSS selector string as input.\n        \"\"\"\n        op = operator.methodcaller('select', selector)\n        return self._wrap_multi(op)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef serach_path():\n    operating_system = get_os()\n    # 1st choice: in ~/.kerncraft/iaca-{}\n    # 2nd choice: in package directory / iaca-{}\n    return [os.path.expanduser(\"~/.kerncraft/iaca/{}/\".format(operating_system)),\n            os.path.abspath(os.path.dirname(os.path.realpath(__file__))) + '/iaca/{}/'.format(\n                operating_system)]", "response": "Return potential locations of IACA installation."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn ( hopefully ) valid installation of IACA.", "response": "def find_iaca():\n    \"\"\"Return (hopefully) valid installation of IACA.\"\"\"\n    requires = ['iaca2.2', 'iaca2.3', 'iaca3.0']\n    for path in serach_path():\n        path += 'bin/'\n        valid = True\n        for r in requires:\n            if not os.path.exists(path + r):\n                valid = False\n                break\n        if valid:\n            return path\n    raise RuntimeError(\"No IACA installation found in {}. Run iaca_get command to fix this issue.\"\n                       \"\".format(serach_path()))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nyields all groups of a simple regex - like expression.", "response": "def group_iterator(group):\n    \"\"\"\n    Yild all groups of simple regex-like expression.\n\n    The only special character is a dash (-), which take the preceding and the following chars to\n    compute a range. If the range is non-sensical (e.g., b-a) it will be empty\n\n    Example:\n    >>> list(group_iterator('a-f'))\n    ['a', 'b', 'c', 'd', 'e', 'f']\n    >>> list(group_iterator('148'))\n    ['1', '4', '8']\n    >>> list(group_iterator('7-9ab'))\n    ['7', '8', '9', 'a', 'b']\n    >>> list(group_iterator('0B-A1'))\n    ['0', '1']\n    \"\"\"\n    ordered_chars = string.ascii_letters + string.digits\n    tokenizer = ('(?P<seq>[a-zA-Z0-9]-[a-zA-Z0-9])|'\n                 '(?P<chr>.)')\n    for m in re.finditer(tokenizer, group):\n        if m.group('seq'):\n            start, sep, end = m.group('seq')\n            for i in range(ordered_chars.index(start), ordered_chars.index(end)+1):\n                yield ordered_chars[i]\n        else:\n            yield m.group('chr')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nyield all options that are supported by the given regular expression.", "response": "def register_options(regdescr):\n    \"\"\"\n    Very reduced regular expressions for describing a group of registers.\n\n    Only groups in square bracktes and unions with pipes (|) are supported.\n\n    Examples:\n    >>> list(register_options('PMC[0-3]'))\n    ['PMC0', 'PMC1', 'PMC2', 'PMC3']\n    >>> list(register_options('MBOX0C[01]'))\n    ['MBOX0C0', 'MBOX0C1']\n    >>> list(register_options('CBOX2C1'))\n    ['CBOX2C1']\n    >>> list(register_options('CBOX[0-3]C[01]'))\n    ['CBOX0C0', 'CBOX0C1', 'CBOX1C0', 'CBOX1C1', 'CBOX2C0', 'CBOX2C1', 'CBOX3C0', 'CBOX3C1']\n    >>> list(register_options('PMC[0-1]|PMC[23]'))\n    ['PMC0', 'PMC1', 'PMC2', 'PMC3']\n\n    \"\"\"\n    if not regdescr:\n        yield None\n    tokenizer = ('\\[(?P<grp>[^]]+)\\]|'\n                 '(?P<chr>.)')\n    for u in regdescr.split('|'):\n        m = re.match(tokenizer, u)\n\n        if m.group('grp'):\n            current = group_iterator(m.group('grp'))\n        else:\n            current = [m.group('chr')]\n\n        for c in current:\n            if u[m.end():]:\n                for r in register_options(u[m.end():]):\n                    yield c + r\n            else:\n                yield c"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef eventstr(event_tuple=None, event=None, register=None, parameters=None):\n    if len(event_tuple) == 3:\n        event, register, parameters = event_tuple\n    elif len(event_tuple) == 2:\n        event, register = event_tuple\n    event_dscr = [event, register]\n\n    if parameters:\n        for k, v in sorted(event_tuple[2].items()):  # sorted for reproducability\n            if type(v) is int:\n                k += \"={}\".format(hex(v))\n            event_dscr.append(k)\n    return \":\".join(event_dscr)", "response": "Return a LIKWID event string from an event tuple or keyword arguments."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_minimal_runs(events):\n    # Eliminate multiples\n    events = [e for i, e in enumerate(events) if events.index(e) == i]\n\n    # Build list of runs per register group\n    scheduled_runs = {}\n    scheduled_events = []\n    cur_run = 0\n    while len(scheduled_events) != len(events):\n        for event_tpl in events:\n            event, registers, parameters = event_tpl\n            # Skip allready scheduled events\n            if event_tpl in scheduled_events:\n                continue\n            # Compile explicit list of possible register locations\n            for possible_reg in register_options(registers):\n                # Schedule in current run, if register is not yet in use\n                s = scheduled_runs.setdefault(cur_run, {})\n                if possible_reg not in s:\n                    s[possible_reg] = (event, possible_reg, parameters)\n                    # ban from further scheduling attempts\n                    scheduled_events.append(event_tpl)\n                    break\n        cur_run += 1\n\n    # Collaps all register dicts to single runs\n    runs = [list(v.values()) for v in scheduled_runs.values()]\n\n    return runs", "response": "Compile list of minimal runs for given events."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef calculate_cache_access(self):\n        self.results = {'misses': self.predictor.get_misses(),\n                        'hits': self.predictor.get_hits(),\n                        'evicts': self.predictor.get_evicts(),\n                        'verbose infos': self.predictor.get_infos(),  # only for verbose outputs\n                        'bottleneck level': 0,\n                        'mem bottlenecks': []}\n\n        element_size = self.kernel.datatypes_size[self.kernel.datatype]\n        cacheline_size = float(self.machine['cacheline size'])\n        elements_per_cacheline = int(cacheline_size // element_size)\n\n        total_flops = sum(self.kernel._flops.values())*elements_per_cacheline\n\n        # TODO let user choose threads_per_core:\n        threads_per_core = 1\n\n        # Compile relevant information\n\n        # CPU-L1 stats (in bytes!)\n        # We compile CPU-L1 stats on our own, because cacheprediction only works on cache lines\n        read_offsets, write_offsets = zip(*list(self.kernel.compile_global_offsets(\n            iteration=range(0, elements_per_cacheline))))\n        read_offsets = set([item for sublist in read_offsets if sublist is not None\n                            for item in sublist])\n        write_offsets = set([item for sublist in write_offsets if sublist is not None\n                             for item in sublist])\n\n        write_streams = len(write_offsets)\n        read_streams = len(read_offsets) + write_streams  # write-allocate\n        total_loads = read_streams * element_size\n        # total_evicts = write_streams * element_size\n        bw, measurement_kernel = self.machine.get_bandwidth(\n            0,\n            read_streams - write_streams,  # no write-allocate in L1\n            write_streams,\n            threads_per_core,\n            cores=self.cores)\n\n        # Calculate performance (arithmetic intensity * bandwidth with\n        # arithmetic intensity = flops / bytes loaded )\n        if total_loads == 0:\n            # This happens in case of full-caching\n            arith_intens = None\n            performance = None\n        else:\n            arith_intens = float(total_flops)/total_loads\n            performance = PrefixedUnit(arith_intens * float(bw), 'FLOP/s')\n\n        self.results['mem bottlenecks'].append({\n            'performance': self.conv_perf(PrefixedUnit(performance, 'FLOP/s')),\n            'level': self.machine['memory hierarchy'][0]['level'],\n            'arithmetic intensity': arith_intens,\n            'bw kernel': measurement_kernel,\n            'bandwidth': bw,\n            'bytes transfered': total_loads})\n        self.results['bottleneck level'] = len(self.results['mem bottlenecks'])-1\n        self.results['min performance'] = self.conv_perf(performance)\n\n        # for other cache and memory levels:\n        for cache_level, cache_info in list(enumerate(self.machine['memory hierarchy']))[:-1]:\n            # Compiling stats (in bytes!)\n            total_misses = self.results['misses'][cache_level]*cacheline_size\n            total_evicts = self.results['evicts'][cache_level]*cacheline_size\n\n            # choose bw according to cache level and problem\n            # first, compile stream counts at current cache level\n            # write-allocate is allready resolved above\n            read_streams = self.results['misses'][cache_level]\n            write_streams = self.results['evicts'][cache_level]\n            # second, try to find best fitting kernel (closest to stream seen stream counts):\n            bw, measurement_kernel = self.machine.get_bandwidth(\n                cache_level+1, read_streams, write_streams, threads_per_core,\n                cores=self.cores)\n\n            # Calculate performance (arithmetic intensity * bandwidth with\n            # arithmetic intensity = flops / bytes transfered)\n            bytes_transfered = total_misses + total_evicts\n\n            if bytes_transfered == 0:\n                # This happens in case of full-caching\n                arith_intens = float('inf')\n                performance = PrefixedUnit(float('inf'), 'FLOP/s')\n            else:\n                arith_intens = float(total_flops)/bytes_transfered\n                performance = PrefixedUnit(arith_intens * float(bw), 'FLOP/s')\n\n            self.results['mem bottlenecks'].append({\n                'performance': self.conv_perf(performance),\n                'level': (self.machine['memory hierarchy'][cache_level + 1]['level']),\n                'arithmetic intensity': arith_intens,\n                'bw kernel': measurement_kernel,\n                'bandwidth': bw,\n                'bytes transfered': bytes_transfered})\n            if performance < self.results.get('min performance', {'FLOP/s': performance})['FLOP/s']:\n                self.results['bottleneck level'] = len(self.results['mem bottlenecks'])-1\n                self.results['min performance'] = self.conv_perf(performance)\n\n        return self.results", "response": "Calculate cache access behaviour."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef conv_perf(self, performance):\n        clock = self.machine['clock']\n        flops_per_it = sum(self.kernel._flops.values())\n        it_s = performance/flops_per_it\n        it_s.unit = 'It/s'\n        element_size = self.kernel.datatypes_size[self.kernel.datatype]\n        elements_per_cacheline = int(float(self.machine['cacheline size'])) / element_size\n        cy_cl = clock/it_s*elements_per_cacheline\n        cy_cl.unit = 'cy/CL'\n        cy_it = clock/it_s\n        cy_it.unit = 'cy/It'\n\n        return {'It/s': it_s,\n                'cy/CL': cy_cl,\n                'cy/It': cy_it,\n                'FLOP/s': performance}", "response": "Convert performance to other units such as It / s or cy / CL."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef report(self, output_file=sys.stdout):\n        max_perf = self.results['max_perf']\n\n        if self._args and self._args.verbose >= 3:\n            print('{}'.format(pformat(self.results)), file=output_file)\n\n        if self._args and self._args.verbose >= 1:\n            print('{}'.format(pformat(self.results['verbose infos'])), file=output_file)\n            print('Bottlenecks:', file=output_file)\n            print('  level | a. intensity |   performance   |   peak bandwidth  | peak bandwidth kernel',\n                  file=output_file)\n            print('--------+--------------+-----------------+-------------------+----------------------',\n                  file=output_file)\n            print('    CPU |              | {!s:>15} |                   |'.format(\n                max_perf[self._args.unit]),\n                  file=output_file)\n            for b in self.results['mem bottlenecks']:\n                print('{level:>7} | {arithmetic intensity:>5.2} FLOP/B | {0!s:>15} |'\n                      ' {bandwidth!s:>17} | {bw kernel:<8}'.format(\n                          b['performance'][self._args.unit], **b),\n                      file=output_file)\n            print('', file=output_file)\n\n        if self.results['min performance']['FLOP/s'] > max_perf['FLOP/s']:\n            # CPU bound\n            print('CPU bound. {!s} due to CPU max. FLOP/s'.format(max_perf), file=output_file)\n        else:\n            # Cache or mem bound\n            print('Cache or mem bound.', file=output_file)\n\n            bottleneck = self.results['mem bottlenecks'][self.results['bottleneck level']]\n            print('{!s} due to {} transfer bottleneck (with bw from {} benchmark)'.format(\n                    bottleneck['performance'][self._args.unit],\n                    bottleneck['level'],\n                    bottleneck['bw kernel']),\n                  file=output_file)\n            print('Arithmetic Intensity: {:.2f} FLOP/B'.format(bottleneck['arithmetic intensity']),\n                  file=output_file)", "response": "Report analysis outcome in human readable form."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef analyze(self):\n        self.results = self.calculate_cache_access()\n        try:\n            iaca_analysis, asm_block = self.kernel.iaca_analysis(\n                micro_architecture=self.machine['micro-architecture'],\n                asm_block=self.asm_block,\n                pointer_increment=self.pointer_increment,\n                verbose=self.verbose > 2)\n        except RuntimeError as e:\n            print(\"IACA analysis failed: \" + str(e))\n            sys.exit(1)\n\n        block_throughput = iaca_analysis['throughput']\n        uops = iaca_analysis['uops']\n        iaca_output = iaca_analysis['output']\n        port_cycles = iaca_analysis['port cycles']\n\n        # Normalize to cycles per cacheline\n        elements_per_block = abs(asm_block['pointer_increment']\n                                 / self.kernel.datatypes_size[self.kernel.datatype])\n        block_size = elements_per_block*self.kernel.datatypes_size[self.kernel.datatype]\n        try:\n            block_to_cl_ratio = float(self.machine['cacheline size'])/block_size\n        except ZeroDivisionError as e:\n            print(\"Too small block_size / pointer_increment:\", e, file=sys.stderr)\n            sys.exit(1)\n\n        port_cycles = dict([(i[0], i[1]*block_to_cl_ratio) for i in list(port_cycles.items())])\n        uops = uops*block_to_cl_ratio\n        cl_throughput = block_throughput*block_to_cl_ratio\n        flops_per_element = sum(self.kernel._flops.values())\n\n        # Overwrite CPU-L1 stats, because they are covered by IACA\n        self.results['mem bottlenecks'][0] = None\n\n        # Reevaluate mem bottleneck\n        self.results['min performance'] = self.conv_perf(PrefixedUnit(float('inf'), 'FLOP/s'))\n        self.results['bottleneck level'] = None\n        for level, bottleneck in enumerate(self.results['mem bottlenecks']):\n            if level == 0:\n                # ignoring CPU-L1\n                continue\n            if bottleneck['performance']['FLOP/s'] < self.results['min performance']['FLOP/s']:\n                self.results['bottleneck level'] = level\n                self.results['min performance'] = bottleneck['performance']\n\n        # Create result dictionary\n        self.results.update({\n            'cpu bottleneck': {\n                'port cycles': port_cycles,\n                'cl throughput': cl_throughput,\n                'uops': uops,\n                'performance throughput': self.conv_perf(PrefixedUnit(\n                    self.machine['clock']/block_throughput*elements_per_block*flops_per_element\n                    * self.cores, \"FLOP/s\")),\n                'IACA output': iaca_output}})", "response": "Run the complete analysis of the IACA and return a dictionary of results."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprinting human readable report of model.", "response": "def report(self, output_file=sys.stdout):\n        \"\"\"Print human readable report of model.\"\"\"\n        cpu_perf = self.results['cpu bottleneck']['performance throughput']\n\n        if self.verbose >= 3:\n            print('{}'.format(pformat(self.results)), file=output_file)\n\n        if self.verbose >= 1:\n            print('Bottlenecks:', file=output_file)\n            print('  level | a. intensity |   performance   |   peak bandwidth  | peak bandwidth kernel',\n                  file=output_file)\n            print('--------+--------------+-----------------+-------------------+----------------------',\n                  file=output_file)\n            print('    CPU |              | {!s:>15} |                   |'.format(\n                cpu_perf[self._args.unit]),\n                  file=output_file)\n            for b in self.results['mem bottlenecks']:\n                # Skip CPU-L1 from Roofline model\n                if b is None:\n                    continue\n                print('{level:>7} | {arithmetic intensity:>5.2} FLOP/B | {0!s:>15} |'\n                      ' {bandwidth!s:>17} | {bw kernel:<8}'.format(\n                          b['performance'][self._args.unit], **b),\n                      file=output_file)\n            print('', file=output_file)\n            print('IACA analisys:', file=output_file)\n            print('{!s}'.format(\n                {k: v\n                 for k, v in list(self.results['cpu bottleneck'].items())\n                 if k not in['IACA output']}),\n                file=output_file)\n\n        if self.results['min performance']['FLOP/s'] > cpu_perf['FLOP/s']:\n            # CPU bound\n            print('CPU bound. {!s} due to CPU bottleneck'.format(cpu_perf[self._args.unit]),\n                  file=output_file)\n        else:\n            # Cache or mem bound\n            print('Cache or mem bound.', file=output_file)\n\n            bottleneck = self.results['mem bottlenecks'][self.results['bottleneck level']]\n            print('{!s} due to {} transfer bottleneck (with bw from {} benchmark)'.format(\n                      bottleneck['performance'][self._args.unit],\n                      bottleneck['level'],\n                      bottleneck['bw kernel']),\n                  file=output_file)\n            print('Arithmetic Intensity: {:.2f} FLOP/B'.format(bottleneck['arithmetic intensity']),\n                  file=output_file)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef calculate_cache_access(self):\n        # FIXME handle multiple datatypes\n        element_size = self.kernel.datatypes_size[self.kernel.datatype]\n\n        results = {'dimensions': {}}\n\n        def sympy_compare(a, b):\n            c = 0\n            for i in range(min(len(a), len(b))):\n                s = a[i] - b[i]\n                if sympy.simplify(s > 0):\n                    c = -1\n                elif sympy.simplify(s == 0):\n                    c = 0\n                else:\n                    c = 1\n                if c != 0:\n                    break\n            return c\n\n        accesses = defaultdict(list)\n        sympy_accesses = defaultdict(list)\n        for var_name in self.kernel.variables:\n            for r in self.kernel.sources.get(var_name, []):\n                if r is None:\n                    continue\n                accesses[var_name].append(r)\n                sympy_accesses[var_name].append(self.kernel.access_to_sympy(var_name, r))\n            for w in self.kernel.destinations.get(var_name, []):\n                if w is None:\n                    continue\n                accesses[var_name].append(w)\n                sympy_accesses[var_name].append(self.kernel.access_to_sympy(var_name, w))\n            # order accesses by increasing order\n            accesses[var_name].sort(key=cmp_to_key(sympy_compare), reverse=True)\n\n        results['accesses'] = accesses\n        results['sympy_accesses'] = sympy_accesses\n\n        # For each dimension (1D, 2D, 3D ... nD)\n        for dimension in range(1, len(list(self.kernel.get_loop_stack()))+1):\n            results['dimensions'][dimension] = {}\n\n            slices = defaultdict(list)\n            slices_accesses = defaultdict(list)\n            for var_name in accesses:\n                for a in accesses[var_name]:\n                    # slices are identified by the tuple of indices of higher dimensions\n                    slice_id = tuple([var_name, tuple(a[:-dimension])])\n                    slices[slice_id].append(a)\n                    slices_accesses[slice_id].append(self.kernel.access_to_sympy(var_name, a))\n            results['dimensions'][dimension]['slices'] = slices\n            results['dimensions'][dimension]['slices_accesses'] = slices_accesses\n\n            slices_distances = defaultdict(list)\n            for k, v in slices_accesses.items():\n                for i in range(1, len(v)):\n                    slices_distances[k].append((v[i] - v[i-1]).simplify())\n            results['dimensions'][dimension]['slices_distances'] = slices_distances\n\n            # Check that distances contain only free_symbols based on constants\n            for dist in chain(*slices_distances.values()):\n                if any([s not in self.kernel.constants.keys() for s in dist.free_symbols]):\n                    raise ValueError(\"Some distances are not based on constants: \"+str(dist))\n\n            # Sum of lengths between relative distances\n            slices_sum = sum([sum(dists) for dists in slices_distances.values()])\n            results['dimensions'][dimension]['slices_sum'] = slices_sum\n\n            # Max of lengths between relative distances\n            # Work-around, the arguments with the most symbols get to stay\n            # FIXME, may not be correct in all cases. e.g., N+M vs. N*M\n            def FuckedUpMax(*args):\n                if len(args) == 1:\n                    return args[0]\n                # expand all expressions:\n                args = [a.expand() for a in args]\n                # Filter expressions with less than the maximum number of symbols\n                max_symbols = max([len(a.free_symbols) for a in args])\n                args = list(filter(lambda a: len(a.free_symbols) == max_symbols, args))\n                if max_symbols == 0:\n                    return sympy.Max(*args)\n                # Filter symbols with lower exponent\n                max_coeffs = 0\n                for a in args:\n                    for s in a.free_symbols:\n                        max_coeffs = max(max_coeffs, len(sympy.Poly(a, s).all_coeffs()))\n\n                def coeff_filter(a):\n                    return max(\n                        0, 0,\n                        *[len(sympy.Poly(a, s).all_coeffs()) for s in a.free_symbols]) == max_coeffs\n                args = list(filter(coeff_filter, args))\n\n                m = sympy.Max(*args)\n                # if m.is_Function:\n                #     raise ValueError(\"Could not resolve {} to maximum.\".format(m))\n                return m\n\n            slices_max = FuckedUpMax(sympy.Integer(0),\n                                     *[FuckedUpMax(*dists) for dists in slices_distances.values()])\n            results['dimensions'][dimension]['slices_max'] = slices_max\n\n            # Nmber of slices\n            slices_count = len(slices_accesses)\n            results['dimensions'][dimension]['slices_count'] = slices_count\n\n            # Cache requirement expression\n            cache_requirement_bytes = (slices_sum + slices_max*slices_count)*element_size\n            results['dimensions'][dimension]['cache_requirement_bytes'] = cache_requirement_bytes\n            # Apply to all cache sizes\n            csim = self.machine.get_cachesim(self._args.cores)\n            results['dimensions'][dimension]['caches'] = {}\n            for cl in csim.levels(with_mem=False):\n                cache_equation = sympy.Eq(cache_requirement_bytes, cl.size())\n                if len(self.kernel.constants.keys()) <= 1:\n                    inequality = sympy.solve(sympy.LessThan(cache_requirement_bytes, cl.size()),\n                                             *self.kernel.constants.keys())\n                else:\n                    # Sympy does not solve for multiple constants\n                    inequality = sympy.LessThan(cache_requirement_bytes, cl.size())\n                try:\n                    eq = sympy.solve(inequality, *self.kernel.constants.keys(), dict=True)\n                except NotImplementedError:\n                    eq = None\n                results['dimensions'][dimension]['caches'][cl.name] = {\n                    'cache_size': cl.size(),\n                    'equation': cache_equation,\n                    'lt': inequality,\n                    'eq': eq\n                }\n\n        return results", "response": "Apply layer condition model to calculate cache accesses."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef analyze(self):\n        # check that layer conditions can be applied on this kernel:\n        # 1. All iterations may only have a step width of 1\n        loop_stack = list(self.kernel.get_loop_stack())\n        if any([l['increment'] != 1 for l in loop_stack]):\n            raise ValueError(\"Can not apply layer condition, since not all loops are of step \"\n                             \"length 1.\")\n\n        # 2. The order of iterations must be reflected in the order of indices in all array\n        #    references containing the inner loop index. If the inner loop index is not part of the\n        #    reference, the reference is simply ignored\n        # TODO support flattend array indexes\n        for aref in list(self.kernel.index_order()):\n            # Strip left most empty sets (refereces without index)\n            while aref and len(aref[0]) == 0:\n                aref.pop(0)\n            for i, idx_names in enumerate(aref):\n                # 1. Check for that there are enough loops to handle access dimensions\n                # 2. Check that offset index matches loop index (in same order)\n                if i >= len(loop_stack) or \\\n                        any([loop_stack[i]['index'] != idx.name for idx in idx_names]):\n                    raise ValueError(\"Can not apply layer condition, order of indices in array \"\n                                     \"does not follow order of loop indices. Single-dimension is \"\n                                     \"currently not supported.\")\n\n        # 3. Indices may only increase with one\n        # TODO use a public interface, not self.kernel._*\n        for arefs in chain(chain(*self.kernel.sources.values()),\n                           chain(*self.kernel.destinations.values())):\n            if not arefs:\n                continue\n\n            # Strip left most constant offsets (refereces without index) to support things like:\n            # a[0][i+1][j][k-1] with an i, j and k loop-nest\n            while arefs and not arefs[0].free_symbols:\n                arefs = arefs[1:]\n\n            # Check that remaining indices are in orde\n            for i, expr in enumerate(arefs):\n                diff = sympy.diff(expr, sympy.Symbol(loop_stack[i]['index']))\n                if diff != 0 and diff != 1:\n                    # TODO support -1 aswell\n                    raise ValueError(\"Can not apply layer condition, array references may not \"\n                                     \"increment more then one per iteration.\")\n\n        self.results = self.calculate_cache_access()", "response": "Run complete analysis of the current array of all the array items in the kernel."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreport generated model in human readable form.", "response": "def report(self, output_file=sys.stdout):\n        \"\"\"Report generated model in human readable form.\"\"\"\n        if self._args and self._args.verbose > 2:\n            pprint(self.results)\n\n        for dimension, lc_info in self.results['dimensions'].items():\n            print(\"{}D layer condition:\".format(dimension), file=output_file)\n            for cache, lc_solution in sorted(lc_info['caches'].items()):\n                print(cache+\": \", end='', file=output_file)\n                if lc_solution['lt'] is sympy.true:\n                    print(\"unconditionally fulfilled\", file=output_file)\n                else:\n                    if lc_solution['eq'] is None:\n                        print(\"{}\".format(lc_solution['lt']), file=output_file)\n                    elif type(lc_solution['eq']) is not list:\n                        print(\"{}\".format(lc_solution['eq']), file=output_file)\n                    else:\n                        for solu in lc_solution['eq']:\n                            for s, v in solu.items():\n                                print(\"{} <= {}\".format(s, v), file=output_file)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclean a source code by removing comments and macros and pragmas from the source code.", "response": "def clean_code(code, comments=True, macros=False, pragmas=False):\n    \"\"\"\n    Naive comment and macro striping from source code\n\n    :param comments: If True, all comments are stripped from code\n    :param macros: If True, all macros are stripped from code\n    :param pragmas: If True, all pragmas are stripped from code\n\n    :return: cleaned code. Line numbers are preserved with blank lines,\n    and multiline comments and macros are supported. BUT comment-like\n    strings are (wrongfully) treated as comments.\n    \"\"\"\n    if macros or pragmas:\n        lines = code.split('\\n')\n        in_macro = False\n        in_pragma = False\n        for i in range(len(lines)):\n            l = lines[i].strip()\n\n            if macros and (l.startswith('#') and not l.startswith('#pragma') or in_macro):\n                lines[i] = ''\n                in_macro = l.endswith('\\\\')\n            if pragmas and (l.startswith('#pragma') or in_pragma):\n                lines[i] = ''\n                in_pragma = l.endswith('\\\\')\n        code = '\\n'.join(lines)\n\n    if comments:\n        idx = 0\n        comment_start = None\n        while idx < len(code) - 1:\n            if comment_start is None and code[idx:idx + 2] == '//':\n                end_idx = code.find('\\n', idx)\n                code = code[:idx] + code[end_idx:]\n                idx -= end_idx - idx\n            elif comment_start is None and code[idx:idx + 2] == '/*':\n                comment_start = idx\n            elif comment_start is not None and code[idx:idx + 2] == '*/':\n                code = (code[:comment_start] +\n                        '\\n' * code[comment_start:idx].count('\\n') +\n                        code[idx + 2:])\n                idx -= idx - comment_start\n                comment_start = None\n            idx += 1\n\n    return code"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreplace all matching ID nodes in the given AST with replacement.", "response": "def replace_id(ast, id_name, replacement):\n    \"\"\"\n    Replace all matching ID nodes in ast (in-place), with replacement.\n\n    :param id_name: name of ID node to match\n    :param replacement: single or list of node to insert in replacement for ID node.\n    \"\"\"\n    for a in ast:\n        if isinstance(a, c_ast.ID) and a.name == id_name:\n            # Check all attributes of ast\n            for attr_name in dir(ast):\n                # Exclude special and method attributes\n                if attr_name.startswith('__') or callable(getattr(ast, attr_name)):\n                    continue\n                attr = getattr(ast, attr_name)\n                # In case of direct match, just replace\n                if attr is a:\n                    setattr(ast, attr_name, replacement)\n                # If contained in list replace occurrence with replacement\n                if type(attr) is list:\n                    for i, attr_element in enumerate(attr):\n                        if attr_element is a:\n                            if type(replacement) is list:\n                                # If replacement is list, inject\n                                attr[i:i+1] = replacement\n                            else:\n                                # otherwise replace\n                                attr[i] = replacement\n        else:\n            replace_id(a, id_name, replacement)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef round_to_next(x, base):\n    # Based on: http://stackoverflow.com/a/2272174\n    return int(base * math.ceil(float(x)/base))", "response": "Round float to next multiple of base."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef blocking(indices, block_size, initial_boundary=0):\n    blocks = []\n\n    for idx in indices:\n        bl_idx = (idx-initial_boundary)//float(block_size)\n        if bl_idx not in blocks:\n            blocks.append(bl_idx)\n    blocks.sort()\n\n    return blocks", "response": "Split list of integers into blocks of block_size and return block indices."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef calculate_cache_access(self):\n        self.results.update({\n                        'cycles': [],  # will be filled by caclculate_cycles()\n                        'misses': self.predictor.get_misses(),\n                        'hits': self.predictor.get_hits(),\n                        'evicts': self.predictor.get_evicts(),\n                        'verbose infos': self.predictor.get_infos()})", "response": "Dispatch to cache predictor to get cache stats."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating performance model cycles from cache stats.", "response": "def calculate_cycles(self):\n        \"\"\"\n        Calculate performance model cycles from cache stats.\n\n        calculate_cache_access() needs to have been execute before.\n        \"\"\"\n        element_size = self.kernel.datatypes_size[self.kernel.datatype]\n        elements_per_cacheline = float(self.machine['cacheline size']) // element_size\n        iterations_per_cacheline = (sympy.Integer(self.machine['cacheline size']) /\n                                    sympy.Integer(self.kernel.bytes_per_iteration))\n        self.results['iterations per cacheline'] = iterations_per_cacheline\n        cacheline_size = float(self.machine['cacheline size'])\n\n        loads, stores = (self.predictor.get_loads(), self.predictor.get_stores())\n\n        for cache_level, cache_info in list(enumerate(self.machine['memory hierarchy']))[1:]:\n            throughput, duplexness = cache_info['non-overlap upstream throughput']\n\n            if type(throughput) is str and throughput == 'full socket memory bandwidth':\n                # Memory transfer\n                # we use bandwidth to calculate cycles and then add panalty cycles (if given)\n\n                # choose bw according to cache level and problem\n                # first, compile stream counts at current cache level\n                # write-allocate is allready resolved in cache predictor\n                read_streams = loads[cache_level]\n                write_streams = stores[cache_level]\n                # second, try to find best fitting kernel (closest to stream seen stream counts):\n                threads_per_core = 1\n                bw, measurement_kernel = self.machine.get_bandwidth(\n                    cache_level, read_streams, write_streams, threads_per_core)\n\n                # calculate cycles\n                if duplexness == 'half-duplex':\n                    cycles = float(loads[cache_level] + stores[cache_level]) * \\\n                             float(elements_per_cacheline) * float(element_size) * \\\n                             float(self.machine['clock']) / float(bw)\n                else:  # full-duplex\n                    raise NotImplementedError(\n                        \"full-duplex mode is not (yet) supported for memory transfers.\")\n                # add penalty cycles for each read stream\n                if 'penalty cycles per read stream' in cache_info:\n                    cycles += stores[cache_level] * \\\n                              cache_info['penalty cycles per read stream']\n\n                self.results.update({\n                    'memory bandwidth kernel': measurement_kernel,\n                    'memory bandwidth': bw})\n            else:\n                # since throughput is given in B/cy, and we need CL/cy:\n                throughput = float(throughput) / cacheline_size\n                # only cache cycles count\n                if duplexness == 'half-duplex':\n                    cycles = (loads[cache_level] + stores[cache_level]) / float(throughput)\n                elif duplexness == 'full-duplex':\n                    cycles = max(loads[cache_level] / float(throughput),\n                                 stores[cache_level] / float(throughput))\n                else:\n                    raise ValueError(\"Duplexness of cache throughput may only be 'half-duplex'\"\n                                     \"or 'full-duplex', found {} in {}.\".format(\n                        duplexness, cache_info['name']))\n\n            self.results['cycles'].append((cache_info['level'], cycles))\n\n            self.results[cache_info['level']] = cycles\n\n        return self.results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun complete anaylysis and return results.", "response": "def analyze(self):\n        \"\"\"Run complete anaylysis and return results.\"\"\"\n        self.calculate_cache_access()\n        self.calculate_cycles()\n        self.results['flops per iteration'] = sum(self.kernel._flops.values())\n\n        return self.results"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef report(self, output_file=sys.stdout):\n        if self.verbose > 1:\n            print('{}'.format(pprint.pformat(self.results['verbose infos'])), file=output_file)\n\n        for level, cycles in self.results['cycles']:\n            print('{} = {}'.format(\n                level, self.conv_cy(cycles)[self._args.unit]), file=output_file)\n\n        if self.verbose > 1:\n            if 'memory bandwidth kernel' in self.results:\n                print('memory cycles based on {} kernel with {}'.format(\n                          self.results['memory bandwidth kernel'],\n                          self.results['memory bandwidth']),\n                      file=output_file)\n\n        if self.verbose > 1:\n            print(file=output_file)\n            print(self.report_data_transfers(), file=output_file)", "response": "Print generated model data in human readable format."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef analyze(self):\n        try:\n            incore_analysis, asm_block =  self.kernel.iaca_analysis(\n                micro_architecture=self.machine['micro-architecture'],\n                asm_block=self.asm_block,\n                pointer_increment=self.pointer_increment,\n                verbose=self.verbose > 2)\n        except RuntimeError as e:\n            print(\"IACA analysis failed: \" + str(e))\n            sys.exit(1)\n\n        block_throughput = incore_analysis['throughput']\n        port_cycles = incore_analysis['port cycles']\n        uops = incore_analysis['uops']\n\n        # Normalize to cycles per cacheline\n        elements_per_block = abs(asm_block['pointer_increment']\n                                 // self.kernel.datatypes_size[self.kernel.datatype])\n        block_size = elements_per_block*self.kernel.datatypes_size[self.kernel.datatype]\n        try:\n            block_to_cl_ratio = float(self.machine['cacheline size'])/block_size\n        except ZeroDivisionError as e:\n            print(\"Too small block_size / pointer_increment:\", e, file=sys.stderr)\n            sys.exit(1)\n\n        port_cycles = dict([(i[0], i[1]*block_to_cl_ratio) for i in list(port_cycles.items())])\n        uops = uops*block_to_cl_ratio\n        cl_throughput = block_throughput*block_to_cl_ratio\n\n        # Compile most relevant information\n        T_OL = max([v for k, v in list(port_cycles.items())\n                    if k in self.machine['overlapping model']['ports']])\n        T_nOL = max([v for k, v in list(port_cycles.items())\n                     if k in self.machine['non-overlapping model']['ports']])\n\n        # Use IACA throughput prediction if it is slower then T_nOL\n        if T_nOL < cl_throughput:\n            T_OL = cl_throughput\n\n        # Create result dictionary\n        self.results = {\n            'port cycles': port_cycles,\n            'cl throughput': self.conv_cy(cl_throughput),\n            'uops': uops,\n            'T_nOL': T_nOL,\n            'T_OL': T_OL,\n            'IACA output': incore_analysis['output'],\n            'elements_per_block': elements_per_block,\n            'pointer_increment': asm_block['pointer_increment'],\n            'flops per iteration': sum(self.kernel._flops.values())}\n        return self.results", "response": "Run complete analysis and return results."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert cycles to other units such as FLOP or It units.", "response": "def conv_cy(self, cy_cl):\n        \"\"\"Convert cycles (cy/CL) to other units, such as FLOP/s or It/s.\"\"\"\n        if not isinstance(cy_cl, PrefixedUnit):\n            cy_cl = PrefixedUnit(cy_cl, '', 'cy/CL')\n        clock = self.machine['clock']\n        element_size = self.kernel.datatypes_size[self.kernel.datatype]\n        elements_per_cacheline = int(self.machine['cacheline size']) // element_size\n        it_s = clock/cy_cl*elements_per_cacheline\n        it_s.unit = 'It/s'\n        flops_per_it = sum(self.kernel._flops.values())\n        performance = it_s*flops_per_it\n        performance.unit = 'FLOP/s'\n        cy_it = cy_cl*elements_per_cacheline\n        cy_it.unit = 'cy/It'\n\n        return {'It/s': it_s,\n                'cy/CL': cy_cl,\n                'cy/It': cy_it,\n                'FLOP/s': performance}"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef report(self, output_file=sys.stdout):\n        if self.verbose > 2:\n            print(\"IACA Output:\", file=output_file)\n            print(self.results['IACA output'], file=output_file)\n            print('', file=output_file)\n\n        if self.verbose > 1:\n            print('Detected pointer increment: {}'.format(self.results['pointer_increment']),\n                  file=output_file)\n            print('Derived elements stored to per asm block iteration: {}'.format(\n                  self.results['elements_per_block']), file=output_file)\n            print('Ports and cycles:', str(self.results['port cycles']), file=output_file)\n            print('Uops:', str(self.results['uops']), file=output_file)\n\n            print('Throughput: {}'.format(self.results['cl throughput'][self._args.unit]),\n                  file=output_file)\n\n        print('T_nOL = {:.1f} cy/CL'.format(self.results['T_nOL']), file=output_file)\n        print('T_OL = {:.1f} cy/CL'.format(self.results['T_OL']), file=output_file)", "response": "Print generated model data in human readable format."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprint generated model data in human readable format.", "response": "def report(self, output_file=sys.stdout):\n        \"\"\"Print generated model data in human readable format.\"\"\"\n        report = ''\n        if self.verbose > 1:\n            self._CPU.report()\n            self._data.report()\n\n        report += '{{ {:.1f} || {:.1f} | {} }} cy/CL'.format(\n            self.results['T_OL'],\n            self.results['T_nOL'],\n            ' | '.join(['{:.1f}'.format(i[1]) for i in self.results['cycles']]))\n\n        if self._args.cores > 1:\n            report += \" (single core)\"\n\n        report += ' = {}'.format(self.results['total cycles'][self._args.unit])\n\n        report += '\\n{{ {:.1f} \\ {} }} cy/CL'.format(\n            max(self.results['T_OL'], self.results['T_nOL']),\n            ' \\ '.join(['{:.1f}'.format(max(sum([x[1] for x in self.results['cycles'][:i+1]]) +\n                                            self.results['T_nOL'], self.results['T_OL']))\n                        for i in range(len(self.results['cycles']))]))\n\n        if self._args.cores > 1:\n            report += \" (single core)\"\n\n        report += '\\nsaturating at {:.0f} cores'.format(self.results['scaling cores'])\n\n        if self.results['multi-core']:\n            report += \"\\nprediction for {} cores,\".format(self.results['multi-core']['cores']) + \\\n                      \" assuming static scheduling: \"\n            report += \"{} ({})\\n\".format(\n                self.results['multi-core']['performance'][self._args.unit],\n                ', '.join(self.results['multi-core']['notes']))\n\n        if self.results['scaling prediction']:\n            report += \"\\nScaling prediction, considering memory bus utilization penalty and \" \\\n                \"assuming all scalable caches:\\n\"\n            if self.machine['cores per socket'] > self.machine['cores per NUMA domain']:\n                report += \"1st NUMA dom.\" + (len(self._args.unit) - 4) * ' ' + '||' + \\\n                    '--------' * (self.machine['cores per NUMA domain']-1) + '-------|\\n'\n\n            report +=  \"cores \" + (len(self._args.unit)+2)*' ' + \" || \" + ' | '.join(\n                ['{:<5}'.format(s['cores']) for s in self.results['scaling prediction']]) + '\\n'\n            report +=  \"perf. ({}) || \".format(self._args.unit) + ' | '.join(\n                ['{:<5.1f}'.format(float(s['performance'][self._args.unit]))\n                 for s in self.results['scaling prediction']]) + '\\n'\n\n        print(report, file=output_file)\n\n        if self._args and self._args.ecm_plot:\n            assert plot_support, \"matplotlib couldn't be imported. Plotting is not supported.\"\n            fig = plt.figure(frameon=False)\n            self.plot(fig)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplot visualization of model prediction.", "response": "def plot(self, fig=None):\n        \"\"\"Plot visualization of model prediction.\"\"\"\n        if not fig:\n            fig = plt.gcf()\n\n        fig.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.15)\n        ax = fig.add_subplot(1, 1, 1)\n\n        sorted_overlapping_ports = sorted(\n            [(p, self.results['port cycles'][p]) for p in self.machine['overlapping ports']],\n            key=lambda x: x[1])\n\n        yticks_labels = []\n        yticks = []\n        xticks_labels = []\n        xticks = []\n\n        # Plot configuration\n        height = 0.9\n\n        i = 0\n        # T_OL\n        colors = ([(254. / 255, 177. / 255., 178. / 255.)] +\n                  [(255. / 255., 255. / 255., 255. / 255.)] * (len(sorted_overlapping_ports) - 1))\n        for p, c in sorted_overlapping_ports:\n            ax.barh(i, c, height, align='center', color=colors.pop(),\n                    edgecolor=(0.5, 0.5, 0.5), linestyle='dashed')\n            if i == len(sorted_overlapping_ports) - 1:\n                ax.text(c / 2.0, i, '$T_\\mathrm{OL}$', ha='center', va='center')\n            yticks_labels.append(p)\n            yticks.append(i)\n            i += 1\n        xticks.append(sorted_overlapping_ports[-1][1])\n        xticks_labels.append('{:.1f}'.format(sorted_overlapping_ports[-1][1]))\n\n        # T_nOL + memory transfers\n        y = 0\n        colors = [(187. / 255., 255 / 255., 188. / 255.)] * (len(self.results['cycles'])) + \\\n                 [(119. / 255, 194. / 255., 255. / 255.)]\n        for k, v in [('nOL', self.results['T_nOL'])] + self.results['cycles']:\n            ax.barh(i, v, height, y, align='center', color=colors.pop())\n            ax.text(y + v / 2.0, i, '$T_\\mathrm{' + k + '}$', ha='center', va='center')\n            xticks.append(y + v)\n            xticks_labels.append('{:.1f}'.format(y + v))\n            y += v\n        yticks_labels.append('LD')\n        yticks.append(i)\n\n        ax.tick_params(axis='y', which='both', left='off', right='off')\n        ax.tick_params(axis='x', which='both', top='off')\n        ax.set_xlabel('t [cy]')\n        ax.set_ylabel('execution port')\n        ax.set_yticks(yticks)\n        ax.set_yticklabels(yticks_labels)\n        ax.set_xticks(xticks)\n        ax.set_xticklabels(xticks_labels, rotation='vertical')\n        ax.xaxis.grid(alpha=0.7, linestyle='--')\n        fig.savefig(self._args.ecm_plot)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef strip_and_uncomment(asm_lines):\n    asm_stripped = []\n    for line in asm_lines:\n        # Strip comments and whitespaces\n        asm_stripped.append(line.split('#')[0].strip())\n    return asm_stripped", "response": "Strip whitespaces and comments from asm lines."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef strip_unreferenced_labels(asm_lines):\n    asm_stripped = []\n    for line in asm_lines:\n        if re.match(r'^\\S+:', line):\n            # Found label\n            label = line[0:line.find(':')]\n            # Search for references to current label\n            if not any([re.match(r'^[^#]*\\s' + re.escape(label) + '[\\s,]?.*$', l) for l in asm_lines]):\n                # Skip labels without seen reference\n                line = ''\n        asm_stripped.append(line)\n    return asm_stripped", "response": "Strip all labels which are never referenced."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_asm_blocks(asm_lines):\n    blocks = []\n\n    last_labels = OrderedDict()\n    packed_ctr = 0\n    avx_ctr = 0\n    xmm_references = []\n    ymm_references = []\n    zmm_references = []\n    gp_references = []\n    mem_references = []\n    increments = {}\n    for i, line in enumerate(asm_lines):\n        # Register access counts\n        zmm_references += re.findall('%zmm[0-9]+', line)\n        ymm_references += re.findall('%ymm[0-9]+', line)\n        xmm_references += re.findall('%xmm[0-9]+', line)\n        gp_references += re.findall('%r[a-z0-9]+', line)\n        if re.search(r'\\d*\\(%\\w+(,%\\w+)?(,\\d)?\\)', line):\n            m = re.search(r'(?P<off>[-]?\\d*)\\(%(?P<basep>\\w+)(,%(?P<idx>\\w+))?(?:,(?P<scale>\\d))?\\)'\n                          r'(?P<eol>$)?',\n                          line)\n            mem_references.append((\n                int(m.group('off')) if m.group('off') else 0,\n                m.group('basep'),\n                m.group('idx'),\n                int(m.group('scale')) if m.group('scale') else 1,\n                'load' if m.group('eol') is None else 'store'))\n\n        if re.match(r\"^[v]?(mul|add|sub|div|fmadd(132|213|231)?)[h]?p[ds]\", line):\n            if line.startswith('v'):\n                avx_ctr += 1\n            packed_ctr += 1\n        elif re.match(r'^\\S+:', line):\n            # last_labels[label_name] = line_number\n            last_labels[line[0:line.find(':')]] =i\n\n            # Reset counters\n            packed_ctr = 0\n            avx_ctr = 0\n            xmm_references = []\n            ymm_references = []\n            zmm_references = []\n            gp_references = []\n            mem_references = []\n            increments = {}\n        elif re.match(r'^inc[bwlq]?\\s+%[a-z0-9]+', line):\n            reg_start = line.find('%') + 1\n            increments[line[reg_start:]] = 1\n        elif re.match(r'^add[bwlq]?\\s+\\$[0-9]+,\\s*%[a-z0-9]+', line):\n            const_start = line.find('$') + 1\n            const_end = line[const_start + 1:].find(',') + const_start + 1\n            reg_start = line.find('%') + 1\n            increments[line[reg_start:]] = int(line[const_start:const_end])\n        elif re.match(r'^dec[bwlq]?', line):\n            reg_start = line.find('%') + 1\n            increments[line[reg_start:]] = -1\n        elif re.match(r'^sub[bwlq]?\\s+\\$[0-9]+,', line):\n            const_start = line.find('$') + 1\n            const_end = line[const_start + 1:].find(',') + const_start + 1\n            reg_start = line.find('%') + 1\n            increments[line[reg_start:]] = -int(line[const_start:const_end])\n        elif last_labels and re.match(r'^j[a-z]+\\s+\\S+\\s*', line):\n            # End of block(s) due to jump\n\n            # Check if jump target matches any previously recoded label\n            last_label = None\n            last_label_line = -1\n            for label_name, label_line in last_labels.items():\n                if re.match(r'^j[a-z]+\\s+' + re.escape(label_name) + r'\\s*', line):\n                    # matched\n                    last_label = label_name\n                    last_label_line = label_line\n\n            labels = list(last_labels.keys())\n\n            if last_label:\n                # deduce loop increment from memory index register\n                pointer_increment = None  # default -> can not decide, let user choose\n                possible_idx_regs = None\n                if mem_references:\n                    # we found memory references to work with\n\n                    # If store accesses exist, consider only those\n                    store_references = [mref for mref in mem_references\n                                        if mref[4] == 'store']\n                    refs = store_references or mem_references\n\n                    possible_idx_regs = list(set(increments.keys()).intersection(\n                        set([r[1] for r in refs if r[1] is not None] +\n                            [r[2] for r in refs if r[2] is not None])))\n                    for mref in refs:\n                        for reg in list(possible_idx_regs):\n                            # Only consider references with two registers, where one could be an index\n                            if None not in mref[1:3]:\n                                # One needs to mach, other registers will be excluded\n                                if not (reg == mref[1] or reg == mref[2]):\n                                    # reg can not be it\n                                    possible_idx_regs.remove(reg)\n\n                    idx_reg = None\n                    if len(possible_idx_regs) == 1:\n                        # good, exactly one register was found\n                        idx_reg = possible_idx_regs[0]\n                    elif possible_idx_regs and itemsEqual([increments[pidxreg]\n                                                           for pidxreg in possible_idx_regs]):\n                        # multiple were option found, but all have the same increment\n                        # use first match:\n                        idx_reg = possible_idx_regs[0]\n\n                    if idx_reg:\n                        mem_scales = [mref[3] for mref in refs\n                                      if idx_reg == mref[2] or idx_reg == mref[1]]\n\n                        if itemsEqual(mem_scales):\n                            # good, all scales are equal\n                            try:\n                                pointer_increment = mem_scales[0] * increments[idx_reg]\n                            except:\n                                print(\"labels\", pformat(labels[labels.index(last_label):]))\n                                print(\"lines\", pformat(asm_lines[last_label_line:i + 1]))\n                                print(\"increments\", increments)\n                                print(\"mem_references\", pformat(mem_references))\n                                print(\"idx_reg\", idx_reg)\n                                print(\"mem_scales\", mem_scales)\n                                raise\n\n                blocks.append({'first_line': last_label_line,\n                               'last_line': i,\n                               'ops': i - last_label_line,\n                               'labels': labels[labels.index(last_label):],\n                               'packed_instr': packed_ctr,\n                               'avx_instr': avx_ctr,\n                               'XMM': (len(xmm_references), len(set(xmm_references))),\n                               'YMM': (len(ymm_references), len(set(ymm_references))),\n                               'ZMM': (len(zmm_references), len(set(zmm_references))),\n                               'GP': (len(gp_references), len(set(gp_references))),\n                               'regs': (len(xmm_references) + len(ymm_references) +\n                                        len(zmm_references) + len(gp_references),\n                                        len(set(xmm_references)) + len(set(ymm_references)) +\n                                        len(set(zmm_references)) +\n                                        len(set(gp_references))),\n                               'pointer_increment': pointer_increment,\n                               'lines': asm_lines[last_label_line:i + 1],\n                               'possible_idx_regs': possible_idx_regs,\n                               'mem_references': mem_references,\n                               'increments': increments, })\n            # Reset counters\n            packed_ctr = 0\n            avx_ctr = 0\n            xmm_references = []\n            ymm_references = []\n            zmm_references = []\n            gp_references = []\n            mem_references = []\n            increments = {}\n            last_labels = OrderedDict()\n    return list(enumerate(blocks))", "response": "Find blocks probably corresponding to loops in assembly."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef select_best_block(blocks):\n    # TODO make this cleverer with more stats\n    if not blocks:\n        raise ValueError(\"No suitable blocks were found in assembly.\")\n    best_block = max(blocks, key=lambda b: b[1]['packed_instr'])\n    if best_block[1]['packed_instr'] == 0:\n        best_block = max(blocks,\n                         key=lambda b: (b[1]['ops'] + b[1]['packed_instr'] + b[1]['avx_instr'],\n                                        b[1]['ZMM'], b[1]['YMM'], b[1]['XMM']))\n    return best_block[0]", "response": "Select the best block based on simple heuristic."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef userselect_increment(block):\n    print(\"Selected block:\")\n    print('\\n    ' + ('\\n    '.join(block['lines'])))\n    print()\n\n    increment = None\n    while increment is None:\n        increment = input(\"Choose store pointer increment (number of bytes): \")\n        try:\n            increment = int(increment)\n        except ValueError:\n            increment = None\n\n    block['pointer_increment'] = increment\n    return increment", "response": "Let user interactively select byte increment."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlet user interactively select a single block.", "response": "def userselect_block(blocks, default=None, debug=False):\n    \"\"\"Let user interactively select block.\"\"\"\n    print(\"Blocks found in assembly file:\")\n    print(\"      block     | OPs | pck. | AVX || Registers |    ZMM   |    YMM   |    XMM   |    GP   ||ptr.inc|\\n\"\n          \"----------------+-----+------+-----++-----------+----------+----------+----------+---------++-------|\")\n    for idx, b in blocks:\n        print('{:>2} {b[labels]!r:>12} | {b[ops]:>3} | {b[packed_instr]:>4} | {b[avx_instr]:>3} |'\n              '| {b[regs][0]:>3} ({b[regs][1]:>3}) | {b[ZMM][0]:>3} ({b[ZMM][1]:>2}) | '\n              '{b[YMM][0]:>3} ({b[YMM][1]:>2}) | '\n              '{b[XMM][0]:>3} ({b[XMM][1]:>2}) | {b[GP][0]:>2} ({b[GP][1]:>2}) || '\n              '{b[pointer_increment]!s:>5} |'.format(idx, b=b))\n\n        if debug:\n            ln = b['first_line']\n            print(' '*4 + 'Code:')\n            for l in b['lines']:\n                print(' '*8 + '{:>5} | {}'.format(ln, l))\n                ln += 1\n            print(' '*4 + 'Metadata:')\n            print(textwrap.indent(\n                pformat({k: v for k,v in b.items() if k not in ['lines']}),\n                ' '*8))\n\n    # Let user select block:\n    block_idx = -1\n    while not (0 <= block_idx < len(blocks)):\n        block_idx = input(\"Choose block to be marked [\" + str(default) + \"]: \") or default\n        try:\n            block_idx = int(block_idx)\n        except ValueError:\n            block_idx = -1\n    # block = blocks[block_idx][1]\n\n    return block_idx"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef insert_markers(asm_lines, start_line, end_line):\n    asm_lines = (asm_lines[:start_line] + START_MARKER +\n                 asm_lines[start_line:end_line + 1] + END_MARKER +\n                 asm_lines[end_line + 1:])\n    return asm_lines", "response": "Insert IACA marker into list of ASM instructions at given indices."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef iaca_instrumentation(input_file, output_file,\n                         block_selection='auto',\n                         pointer_increment='auto_with_manual_fallback',\n                         debug=False):\n    \"\"\"\n    Add IACA markers to an assembly file.\n\n    If instrumentation fails because loop increment could not determined automatically, a ValueError\n    is raised.\n\n    :param input_file: file-like object to read from\n    :param output_file: file-like object to write to\n    :param block_selection: index of the assembly block to instrument, or 'auto' for automatically\n                            using block with the\n                            most vector instructions, or 'manual' to read index to prompt user\n    :param pointer_increment: number of bytes the pointer is incremented after the loop or\n                              - 'auto': automatic detection, otherwise RuntimeError is raised\n                              - 'auto_with_manual_fallback': like auto with fallback to manual input\n                              - 'manual': prompt user\n    :param debug: output additional internal analysis information. Only works with manual selection.\n    :return: the instrumented assembly block\n    \"\"\"\n    assembly_orig = input_file.readlines()\n\n    # If input and output files are the same, overwrite with output\n    if input_file is output_file:\n        output_file.seek(0)\n        output_file.truncate()\n\n    if debug:\n        block_selection = 'manual'\n\n    assembly = strip_and_uncomment(copy(assembly_orig))\n    assembly = strip_unreferenced_labels(assembly)\n    blocks = find_asm_blocks(assembly)\n    if block_selection == 'auto':\n        block_idx = select_best_block(blocks)\n    elif block_selection == 'manual':\n        block_idx = userselect_block(blocks, default=select_best_block(blocks), debug=debug)\n    elif isinstance(block_selection, int):\n        block_idx = block_selection\n    else:\n        raise ValueError(\"block_selection has to be an integer, 'auto' or 'manual' \")\n\n    block = blocks[block_idx][1]\n\n    if pointer_increment == 'auto':\n        if block['pointer_increment'] is None:\n            raise RuntimeError(\"pointer_increment could not be detected automatically. Use \"\n                               \"--pointer-increment to set manually to byte offset of store \"\n                               \"pointer address between consecutive assembly block iterations.\")\n    elif pointer_increment == 'auto_with_manual_fallback':\n        if block['pointer_increment'] is None:\n            block['pointer_increment'] = userselect_increment(block)\n    elif pointer_increment == 'manual':\n        block['pointer_increment'] = userselect_increment(block)\n    elif isinstance(pointer_increment, int):\n        block['pointer_increment'] = pointer_increment\n    else:\n        raise ValueError(\"pointer_increment has to be an integer, 'auto', 'manual' or  \"\n                         \"'auto_with_manual_fallback' \")\n\n    instrumented_asm = insert_markers(assembly_orig, block['first_line'], block['last_line'])\n    output_file.writelines(instrumented_asm)\n\n    return block", "response": "Instruments the IACA code for the current language."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef iaca_analyse_instrumented_binary(instrumented_binary_file, micro_architecture):\n    # Select IACA version and executable based on micro_architecture:\n    arch_map = {\n        # arch: (binary name, version string, required additional arguments)\n        'NHM': ('iaca2.2', 'v2.2', ['-64']),\n        'WSM': ('iaca2.2', 'v2.2', ['-64']),\n        'SNB': ('iaca2.3', 'v2.3', ['-64']),\n        'IVB': ('iaca2.3', 'v2.3', ['-64']),\n        'HSW': ('iaca3.0', 'v3.0', []),\n        'BDW': ('iaca3.0', 'v3.0', []),\n        'SKL': ('iaca3.0', 'v3.0', []),\n        'SKX': ('iaca3.0', 'v3.0', []),\n    }\n\n    if micro_architecture not in arch_map:\n        raise ValueError('Invalid micro_architecture selected ({}), valid options are {}'.format(\n            micro_architecture, ', '.join(arch_map.keys())))\n\n    iaca_path = iaca_get.find_iaca()  # Throws exception if not found\n    os.environ['PATH'] += ':' + iaca_path\n\n    iaca_exec, iaca_version, base_args = arch_map[micro_architecture]\n    if find_executable(iaca_exec) is None:\n        raise RuntimeError(\"{0} executable was not found. Make sure that {0} is found in \"\n                           \"{1}. Install using iaca_get.\".format(iaca_exec, iaca_path))\n\n    result = {}\n\n    cmd = [iaca_exec] + base_args + ['-arch', micro_architecture, instrumented_binary_file]\n    try:\n        iaca_output = subprocess.check_output(cmd).decode('utf-8')\n        result['output'] = iaca_output\n    except OSError as e:\n        raise RuntimeError(\"IACA execution failed:\" + ' '.join(cmd) + '\\n' + str(e))\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(\"IACA throughput analysis failed:\" + str(e))\n\n    # Get total cycles per loop iteration\n    match = re.search(r'^Block Throughput: ([0-9.]+) Cycles', iaca_output, re.MULTILINE)\n    assert match, \"Could not find Block Throughput in IACA output.\"\n    throughput = float(match.groups()[0])\n    result['throughput'] = throughput\n\n    # Find ports and cycles per port\n    ports = [l for l in iaca_output.split('\\n') if l.startswith('|  Port  |')]\n    cycles = [l for l in iaca_output.split('\\n') if l.startswith('| Cycles |')]\n    assert ports and cycles, \"Could not find ports/cycles lines in IACA output.\"\n    ports = [p.strip() for p in ports[0].split('|')][2:]\n    cycles = [c.strip() for c in cycles[0].split('|')][2:]\n    port_cycles = []\n    for i in range(len(ports)):\n        if '-' in ports[i] and ' ' in cycles[i]:\n            subports = [p.strip() for p in ports[i].split('-')]\n            subcycles = [c for c in cycles[i].split(' ') if bool(c)]\n            port_cycles.append((subports[0], float(subcycles[0])))\n            port_cycles.append((subports[0] + subports[1], float(subcycles[1])))\n        elif ports[i] and cycles[i]:\n            port_cycles.append((ports[i], float(cycles[i])))\n    result['port cycles'] = dict(port_cycles)\n\n    match = re.search(r'^Total Num Of Uops: ([0-9]+)', iaca_output, re.MULTILINE)\n    assert match, \"Could not find Uops in IACA output.\"\n    result['uops'] = float(match.groups()[0])\n    return result", "response": "Analyse an instrumented binary file and return a dictionary of the information that IACA can run on it."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexecuting command line interface.", "response": "def main():\n    \"\"\"Execute command line interface.\"\"\"\n    parser = argparse.ArgumentParser(\n        description='Find and analyze basic loop blocks and mark for IACA.',\n        epilog='For help, examples, documentation and bug reports go to:\\nhttps://github.com'\n               '/RRZE-HPC/kerncraft\\nLicense: AGPLv3')\n    parser.add_argument('--version', action='version', version='%(prog)s {}'.format(__version__))\n    parser.add_argument('source', type=argparse.FileType(), nargs='?', default=sys.stdin,\n                        help='assembly file to analyze (default: stdin)')\n    parser.add_argument('--outfile', '-o', type=argparse.FileType('w'), nargs='?',\n                        default=sys.stdout, help='output file location (default: stdout)')\n    parser.add_argument('--debug', action='store_true',\n                        help='Output nternal analysis information for debugging.')\n    args = parser.parse_args()\n\n    # pointer_increment is given, since it makes no difference on the command lien and requires\n    # less user input\n    iaca_instrumentation(input_file=args.source, output_file=args.outfile,\n                         block_selection='manual', pointer_increment=1, debug=args.debug)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef good_prefix(self, max_error=0.01, round_length=2, min_prefix='', max_prefix=None):\n        good_prefix = min_prefix\n        base_value = self.base_value()\n\n        for k, v in list(self.PREFIXES.items()):\n            # Ignoring to large prefixes\n            if max_prefix is not None and v > self.PREFIXES[max_prefix]:\n                continue\n\n            # Check that differences is < relative error *max_error*\n            if abs(round(base_value/v, round_length)*v - base_value) > base_value*max_error:\n                continue\n\n            # Check that resulting number is >= 0.9\n            if abs(round(base_value/v, round_length)) < 0.9:\n                continue\n\n            # Check if prefix is larger then already chosen\n            if v < self.PREFIXES[good_prefix]:\n                continue\n\n            # seems to be okay\n            good_prefix = k\n\n        return good_prefix", "response": "Returns the good prefix for this locale."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nyield a list of evenly spaced integers over an interval.", "response": "def space(start, stop, num, endpoint=True, log=False, base=10):\n    \"\"\"\n    Return list of evenly spaced integers over an interval.\n\n    Numbers can either be evenly distributed in a linear space (if *log* is False) or in a log\n    space (if *log* is True). If *log* is True, base is used to define the log space basis.\n\n    If *endpoint* is True, *stop* will be the last retruned value, as long as *num* >= 2.\n    \"\"\"\n    assert type(start) is int and type(stop) is int and type(num) is int, \\\n        \"start, stop and num need to be intergers\"\n    assert num >= 2, \"num has to be atleast 2\"\n\n    if log:\n        start = math.log(start, base)\n        stop = math.log(stop, base)\n\n    if endpoint:\n        step_length = float((stop - start)) / float(num - 1)\n    else:\n        step_length = float((stop - start)) / float(num)\n\n    i = 0\n    while i < num:\n        if log:\n            yield int(round(base ** (start + i * step_length)))\n        else:\n            yield int(round(start + i * step_length))\n        i += 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_last_modified_datetime(dir_path=os.path.dirname(__file__)):\n    max_mtime = 0\n    for root, dirs, files in os.walk(dir_path):\n        for f in files:\n            p = os.path.join(root, f)\n            try:\n                max_mtime = max(max_mtime, os.stat(p).st_mtime)\n            except FileNotFoundError:\n                pass\n    return datetime.utcfromtimestamp(max_mtime)", "response": "Return datetime object of latest change in kerncraft module directory."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate an argument parser for the command line.", "response": "def create_parser():\n    \"\"\"Return argparse parser.\"\"\"\n    parser = argparse.ArgumentParser(\n        description='Analytical performance modelling and benchmarking toolkit.',\n        epilog='For help, examples, documentation and bug reports go to:\\nhttps://github.com'\n               '/RRZE-HPC/kerncraft\\nLicense: AGPLv3')\n    parser.add_argument('--version', action='version', version='%(prog)s {}'.format(__version__))\n    parser.add_argument('--machine', '-m', type=argparse.FileType('r'), required=True,\n                        help='Path to machine description yaml file.')\n    parser.add_argument('--pmodel', '-p', choices=models.__all__, required=True, action='append',\n                        default=[], help='Performance model to apply')\n    parser.add_argument('-D', '--define', nargs=2, metavar=('KEY', 'VALUE'), default=[],\n                        action=AppendStringRange,\n                        help='Define constant to be used in C code. Values must be integer or '\n                             'match start-stop[:num[log[base]]]. If range is given, all '\n                             'permutation s will be tested. Overwrites constants from testcase '\n                             'file.')\n    parser.add_argument('--verbose', '-v', action='count', default=0,\n                        help='Increases verbosity level.')\n    parser.add_argument('code_file', metavar='FILE', type=argparse.FileType(),\n                        help='File with loop kernel C code')\n    parser.add_argument('--asm-block', metavar='BLOCK', default='auto',\n                        help='Number of ASM block to mark for IACA, \"auto\" for automatic '\n                             'selection or \"manual\" for interactiv selection.')\n    parser.add_argument('--pointer-increment', metavar='INCR', default='auto', type=int_or_str,\n                        help='Increment of store pointer within one ASM block in bytes. If \"auto\": '\n                             'automatic detection, error on failure to detect, if '\n                             '\"auto_with_manual_fallback\": fallback to manual input, or if '\n                             '\"manual\": always prompt user.')\n    parser.add_argument('--store', metavar='PICKLE', type=argparse.FileType('a+b'),\n                        help='Addes results to PICKLE file for later processing.')\n    parser.add_argument('--unit', '-u', choices=['cy/CL', 'cy/It', 'It/s', 'FLOP/s'],\n                        help='Select the output unit, defaults to model specific if not given.')\n    parser.add_argument('--cores', '-c', metavar='CORES', type=int, default=1,\n                        help='Number of cores to be used in parallel. (default: 1) '\n                             'ECM model will consider the scaling of the last level cache and '\n                             'predict the overall performance in addition to single-core behavior. '\n                             'The benchmark mode will run the code with OpenMP on as many physical '\n                             'cores.')\n    parser.add_argument('--kernel-description', action='store_true',\n                        help='Use kernel description instead of analyzing the kernel code.')\n    parser.add_argument('--clean-intermediates', action='store_true',\n                        help='If set, will delete all intermediate files after completion.')\n\n    # Needed for ECM, ECMData and Roofline model:\n    parser.add_argument('--cache-predictor', '-P', choices=['LC', 'SIM'], default='SIM',\n                        help='Change cache predictor to use, options are LC (layer conditions) and '\n                             'SIM (cache simulation with pycachesim), default is SIM.')\n\n    # Needed for ECM, RooflineIACA and Benchmark model:\n    parser.add_argument('--compiler', '-C', type=str, default=None,\n                        help='Compiler to use, default is first in machine description file.')\n    parser.add_argument('--compiler-flags', type=str, default=None,\n                        help='Compiler flags to use. If not set, flags are taken from machine '\n                             'description file (-std=c99 is always added).')\n\n    for m in models.__all__:\n        ag = parser.add_argument_group('arguments for ' + m + ' model', getattr(models, m).name)\n        getattr(models, m).configure_arggroup(ag)\n\n    return parser"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_arguments(args, parser):\n    if args.asm_block not in ['auto', 'manual']:\n        try:\n            args.asm_block = int(args.asm_block)\n        except ValueError:\n            parser.error('--asm-block can only be \"auto\", \"manual\" or an integer')\n\n    # Set default unit depending on performance model requested\n    if not args.unit:\n        if 'Roofline' in args.pmodel or 'RooflineIACA' in args.pmodel:\n            args.unit = 'FLOP/s'\n        else:\n            args.unit = 'cy/CL'", "response": "Check arguments passed by user that are not checked by argparse itself."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning command line interface.", "response": "def run(parser, args, output_file=sys.stdout):\n    \"\"\"Run command line interface.\"\"\"\n    # Try loading results file (if requested)\n    result_storage = {}\n    if args.store:\n        args.store.seek(0)\n        try:\n            result_storage = pickle.load(args.store)\n        except EOFError:\n            pass\n        args.store.close()\n\n    # machine information\n    # Read machine description\n    machine = MachineModel(args.machine.name, args=args)\n\n    # process kernel\n    if not args.kernel_description:\n        code = str(args.code_file.read())\n        code = clean_code(code)\n        kernel = KernelCode(code, filename=args.code_file.name, machine=machine,\n                            keep_intermediates=not args.clean_intermediates)\n    else:\n        description = str(args.code_file.read())\n        kernel = KernelDescription(yaml.load(description, Loader=yaml.Loader), machine=machine)\n    # if no defines were given, guess suitable defines in-mem\n    # TODO support in-cache\n    # TODO broaden cases to n-dimensions\n    # TODO make configurable (no hardcoded 512MB/1GB/min. 3 iteration ...)\n    # works only for up to 3 dimensions\n    required_consts = [v[1] for v in kernel.variables.values() if v[1] is not None]\n    required_consts += [[l['start'], l['stop']] for l in kernel.get_loop_stack()]\n    # split into individual consts\n    required_consts = [i for l in required_consts for i in l]\n    required_consts = set([i for l in required_consts for i in l.free_symbols])\n    if len(required_consts) > 0:\n        # build defines permutations\n        define_dict = {}\n        for name, values in args.define:\n            if name not in define_dict:\n                define_dict[name] = [[name, v] for v in values]\n                continue\n            for v in values:\n                if v not in define_dict[name]:\n                    define_dict[name].append([name, v])\n        define_product = list(itertools.product(*list(define_dict.values())))\n\n        # Check that all consts have been defined\n        if set(required_consts).difference(set([symbol_pos_int(k) for k in define_dict.keys()])):\n            raise ValueError(\"Not all constants have been defined. Required are: {}\".format(\n                required_consts))\n    else:\n        define_product = [{}]\n\n    for define in define_product:\n        # Reset state of kernel\n        kernel.clear_state()\n\n        # Add constants from define arguments\n        for k, v in define:\n            kernel.set_constant(k, v)\n\n        for model_name in uniquify(args.pmodel):\n            # print header\n            print('{:^80}'.format(' kerncraft '), file=output_file)\n            print('{:<40}{:>40}'.format(args.code_file.name, '-m ' + args.machine.name),\n                  file=output_file)\n            print(' '.join(['-D {} {}'.format(k, v) for k, v in define]), file=output_file)\n            print('{:-^80}'.format(' ' + model_name + ' '), file=output_file)\n\n            if args.verbose > 1:\n                if not args.kernel_description:\n                    kernel.print_kernel_code(output_file=output_file)\n                    print('', file=output_file)\n                kernel.print_variables_info(output_file=output_file)\n                kernel.print_kernel_info(output_file=output_file)\n            if args.verbose > 0:\n                kernel.print_constants_info(output_file=output_file)\n\n            model = getattr(models, model_name)(kernel, machine, args, parser)\n\n            model.analyze()\n            model.report(output_file=output_file)\n\n            # Add results to storage\n            kernel_name = os.path.split(args.code_file.name)[1]\n            if kernel_name not in result_storage:\n                result_storage[kernel_name] = {}\n            if tuple(kernel.constants.items()) not in result_storage[kernel_name]:\n                result_storage[kernel_name][tuple(kernel.constants.items())] = {}\n            result_storage[kernel_name][tuple(kernel.constants.items())][model_name] = \\\n                model.results\n\n            print('', file=output_file)\n\n        # Save storage to file (if requested)\n        if args.store:\n            temp_name = args.store.name + '.tmp'\n            with open(temp_name, 'wb+') as f:\n                pickle.dump(result_storage, f)\n            shutil.move(temp_name, args.store.name)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef main():\n    # Create and populate parser\n    parser = create_parser()\n\n    # Parse given arguments\n    args = parser.parse_args()\n\n    # Checking arguments\n    check_arguments(args, parser)\n\n    # BUSINESS LOGIC IS FOLLOWING\n    run(parser, args)", "response": "Initialize and run command line interface."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef main():\n    parser = argparse.ArgumentParser(\n        description='Recursively merges two or more pickle files. Only supports pickles consisting '\n        'of a single dictionary object.')\n    parser.add_argument('destination', type=argparse.FileType('r+b'),\n                        help='File to write to and include in resulting pickle. (WILL BE CHANGED)')\n    parser.add_argument('source', type=argparse.FileType('rb'), nargs='+',\n                        help='File to include in resulting pickle.')\n\n    args = parser.parse_args()\n\n    result = pickle.load(args.destination)\n    assert isinstance(result, collections.Mapping), \"only Mapping types can be handled.\"\n\n    for s in args.source:\n        data = pickle.load(s)\n        assert isinstance(data, collections.Mapping), \"only Mapping types can be handled.\"\n\n        update(result, data)\n\n    args.destination.seek(0)\n    args.destination.truncate()\n    pickle.dump(result, args.destination)", "response": "Comand line interface of picklemerge."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef symbol_pos_int(*args, **kwargs):\n    kwargs.update({'positive': True,\n                   'integer': True})\n    return sympy.Symbol(*args, **kwargs)", "response": "Create a sympy. Symbol with positive and integer assumptions."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef prefix_indent(prefix, textblock, later_prefix=' '):\n    textblock = textblock.split('\\n')\n    line = prefix + textblock[0] + '\\n'\n    if len(later_prefix) == 1:\n        later_prefix = ' '*len(prefix)\n    line = line + '\\n'.join([later_prefix + x for x in textblock[1:]])\n    if line[-1] != '\\n':\n        return line + '\\n'\n    else:\n        return line", "response": "Prefix and indent all lines in textblock with prefix"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntransforms ast of multidimensional declaration to a single dimension declaration.", "response": "def transform_multidim_to_1d_decl(decl):\n    \"\"\"\n    Transform ast of multidimensional declaration to a single dimension declaration.\n\n    In-place operation!\n\n    Returns name and dimensions of array (to be used with transform_multidim_to_1d_ref())\n    \"\"\"\n    dims = []\n    type_ = decl.type\n    while type(type_) is c_ast.ArrayDecl:\n        dims.append(type_.dim)\n        type_ = type_.type\n\n    if dims:\n        # Multidimensional array\n        decl.type.dim = reduce(lambda l, r: c_ast.BinaryOp('*', l, r), dims)\n        decl.type.type = type_\n\n    return decl.name, dims"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntransforms ast of multidimensional reference to a single dimension reference.", "response": "def transform_multidim_to_1d_ref(aref, dimension_dict):\n    \"\"\"\n    Transform ast of multidimensional reference to a single dimension reference.\n\n    In-place operation!\n    \"\"\"\n    dims = []\n    name = aref\n    while type(name) is c_ast.ArrayRef:\n        dims.append(name.subscript)\n        name = name.name\n\n    subscript_list = []\n    for i, d in enumerate(dims):\n        if i == 0:\n            subscript_list.append(d)\n        else:\n            subscript_list.append(c_ast.BinaryOp('*', d, reduce(\n                lambda l, r: c_ast.BinaryOp('*', l, r),\n                dimension_dict[name.name][-1:-i-1:-1])))\n\n    aref.subscript = reduce(\n        lambda l, r: c_ast.BinaryOp('+', l, r), subscript_list)\n    aref.name = name"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntransforms array declaration to malloc", "response": "def transform_array_decl_to_malloc(decl, with_init=True):\n    \"\"\"\n    Transform ast of \"type var_name[N]\" to \"type* var_name = aligned_malloc(sizeof(type)*N, 32)\"\n\n    In-place operation.\n\n    :param with_init: if False, ommit malloc\n    \"\"\"\n    if type(decl.type) is not c_ast.ArrayDecl:\n        # Not an array declaration, can be ignored\n        return\n\n    type_ = c_ast.PtrDecl([], decl.type.type)\n    if with_init:\n        decl.init = c_ast.FuncCall(\n            c_ast.ID('aligned_malloc'),\n            c_ast.ExprList([\n                c_ast.BinaryOp(\n                    '*',\n                    c_ast.UnaryOp(\n                        'sizeof',\n                        c_ast.Typename(None, [], c_ast.TypeDecl(\n                            None, [], decl.type.type.type))),\n                    decl.type.dim),\n                c_ast.Constant('int', '32')]))\n    decl.type = type_"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_node_type(ast, node_type):\n    if type(ast) is node_type:\n        return [ast]\n    elif type(ast) is list:\n        return reduce(operator.add, list(map(lambda a: find_node_type(a, node_type), ast)), [])\n    elif ast is None:\n        return []\n    else:\n        return reduce(operator.add,\n                      [find_node_type(o[1], node_type) for o in ast.children()], [])", "response": "Return list of array references in AST."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreduce absolute path to relative if shorter.", "response": "def reduce_path(path):\n    \"\"\"Reduce absolute path to relative (if shorter) for easier readability.\"\"\"\n    relative_path = os.path.relpath(path)\n    if len(relative_path) < len(path):\n        return relative_path\n    else:\n        return path"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check(self):\n        datatypes = [v[0] for v in self.variables.values()]\n        assert len(set(datatypes)) <= 1, 'mixing of datatypes within a kernel is not supported.'", "response": "Check that information about kernel makes sens and is valid."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the value of a constant in the current locale.", "response": "def set_constant(self, name, value):\n        \"\"\"\n        Set constant of name to value.\n\n        :param name: may be a str or a sympy.Symbol\n        :param value: must be an int\n        \"\"\"\n        assert isinstance(name, str) or isinstance(name, sympy.Symbol), \\\n            \"constant name needs to be of type str, unicode or a sympy.Symbol\"\n        assert type(value) is int, \"constant value needs to be of type int\"\n        if isinstance(name, sympy.Symbol):\n            self.constants[name] = value\n        else:\n            self.constants[symbol_pos_int(name)] = value"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nregisters a variable of name and type_ with a size.", "response": "def set_variable(self, name, type_, size):\n        \"\"\"\n        Register variable of name and type_, with a (multidimensional) size.\n\n        :param name: variable name as it appears in code\n        :param type_: may be any key from Kernel.datatypes_size (typically float or double)\n        :param size: either None for scalars or an n-tuple of ints for an n-dimensional array\n        \"\"\"\n        assert type_ in self.datatypes_size, 'only float and double variables are supported'\n        if self.datatype is None:\n            self.datatype = type_\n        else:\n            assert type_ == self.datatype, 'mixing of datatypes within a kernel is not supported.'\n        assert type(size) in [tuple, type(None)], 'size has to be defined as tuple or None'\n        self.variables[name] = (type_, size)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef subs_consts(self, expr):\n        if isinstance(expr, numbers.Number):\n            return expr\n        else:\n            return expr.subs(self.constants)", "response": "Substitute constants in expression unless it is already a number."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a dictionary with all arrays sizes.", "response": "def array_sizes(self, in_bytes=False, subs_consts=False):\n        \"\"\"\n        Return a dictionary with all arrays sizes.\n\n        :param in_bytes: If True, output will be in bytes, not element counts.\n        :param subs_consts: If True, output will be numbers and not symbolic.\n\n        Scalar variables are ignored.\n        \"\"\"\n        var_sizes = {}\n\n        for var_name, var_info in self.variables.items():\n            var_type, var_size = var_info\n\n            # Skiping sclars\n            if var_size is None:\n                continue\n\n            var_sizes[var_name] = reduce(operator.mul, var_size, 1)\n\n            # Multiply by bytes per element if requested\n            if in_bytes:\n                element_size = self.datatypes_size[var_type]\n                var_sizes[var_name] *= element_size\n\n        if subs_consts:\n            return {k: self.subs_consts(v) for k, v in var_sizes.items()}\n        else:\n            return var_sizes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the relative offset of the iteration center in number of elements.", "response": "def _calculate_relative_offset(self, name, access_dimensions):\n        \"\"\"\n        Return the offset from the iteration center in number of elements.\n\n        The order of indices used in access is preserved.\n        \"\"\"\n        # TODO to be replaced with compile_global_offsets\n        offset = 0\n        base_dims = self.variables[name][1]\n\n        for dim, offset_info in enumerate(access_dimensions):\n            offset_type, idx_name, dim_offset = offset_info\n            assert offset_type == 'rel', 'Only relative access to arrays is supported at the moment'\n\n            if offset_type == 'rel':\n                offset += self.subs_consts(\n                    dim_offset*reduce(operator.mul, base_dims[dim+1:], sympy.Integer(1)))\n            else:\n                # should not happen\n                pass\n\n        return offset"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove duplicate source and destination accesses", "response": "def _remove_duplicate_accesses(self):\n        \"\"\"\n        Remove duplicate source and destination accesses\n        \"\"\"\n        self.destinations = {var_name: set(acs) for var_name, acs in self.destinations.items()}\n        self.sources = {var_name: set(acs) for var_name, acs in self.sources.items()}"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntransforms a multidimensional variable access to a flattend sympy expression.", "response": "def access_to_sympy(self, var_name, access):\n        \"\"\"\n        Transform a (multidimensional) variable access to a flattend sympy expression.\n\n        Also works with flat array accesses.\n        \"\"\"\n        base_sizes = self.variables[var_name][1]\n\n        expr = sympy.Number(0)\n\n        for dimension, a in enumerate(access):\n            base_size = reduce(operator.mul, base_sizes[dimension+1:], sympy.Integer(1))\n\n            expr += base_size*a\n\n        return expr"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef iteration_length(self, dimension=None):\n        total_length = 1\n\n        if dimension is not None:\n            loops = [self._loop_stack[dimension]]\n        else:\n            loops = reversed(self._loop_stack)\n\n        for var_name, start, end, incr in loops:\n            # This unspools the iterations:\n            length = end-start\n            total_length = total_length*length\n        return self.subs_consts(total_length)", "response": "Return the length of the iteration that are performed."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_loop_stack(self, subs_consts=False):\n        for l in self._loop_stack:\n            if subs_consts:\n                yield {'index': l[0],\n                       'start': self.subs_consts(l[1]),\n                       'stop': self.subs_consts(l[2]),\n                       'increment': self.subs_consts(l[3])}\n            else:\n                yield {'index': l[0], 'start': l[1], 'stop': l[2], 'increment': l[3]}", "response": "Yield loop stack dictionaries in order from outer to inner."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef index_order(self, sources=True, destinations=True):\n        if sources:\n            arefs = chain(*self.sources.values())\n        else:\n            arefs = []\n        if destinations:\n            arefs = chain(arefs, *self.destinations.values())\n\n        ret = []\n        for a in [aref for aref in arefs if aref is not None]:\n            ref = []\n            for expr in a:\n                ref.append(expr.free_symbols)\n            ret.append(ref)\n\n        return ret", "response": "Return the order of indices in the array references."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncompiling sympy accesses for each variable in the source and destination tables.", "response": "def compile_sympy_accesses(self, sources=True, destinations=True):\n        \"\"\"\n        Return a dictionary of lists of sympy accesses, for each variable.\n\n        Use *source* and *destination* to filter output\n        \"\"\"\n        sympy_accesses = defaultdict(list)\n        # Compile sympy accesses\n        for var_name in self.variables:\n            if sources:\n                for r in self.sources.get(var_name, []):\n                    if r is None:\n                        continue\n                    sympy_accesses[var_name].append(self.access_to_sympy(var_name, r))\n            if destinations:\n                for w in self.destinations.get(var_name, []):\n                    if w is None:\n                        continue\n                    sympy_accesses[var_name].append(self.access_to_sympy(var_name, w))\n\n        return sympy_accesses"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning load and store distances between all variables in the system.", "response": "def compile_relative_distances(self, sympy_accesses=None):\n        \"\"\"\n        Return load and store distances between accesses.\n\n        :param sympy_accesses: optionally restrict accesses, default from compile_sympy_accesses()\n\n        e.g. if accesses are to [+N, +1, -1, -N], relative distances are [N-1, 2, N-1]\n\n        returned is a dict of list of sympy expressions, for each variable\n        \"\"\"\n        if sympy_accesses is None:\n            sympy_accesses = self.compile_sympy_accesses()\n\n        sympy_distances = defaultdict(list)\n        for var_name, accesses in sympy_accesses.items():\n            for i in range(1, len(accesses)):\n                sympy_distances[var_name].append((accesses[i-1]-accesses[i]).simplify())\n\n        return sympy_distances"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns sympy expressions translating global_iterator to loop indices.", "response": "def global_iterator_to_indices(self, git=None):\n        \"\"\"\n        Return sympy expressions translating global_iterator to loop indices.\n\n        If global_iterator is given, an integer is returned\n        \"\"\"\n        # unwind global iteration count into loop counters:\n        base_loop_counters = {}\n        global_iterator = symbol_pos_int('global_iterator')\n        idiv = implemented_function(sympy.Function(str('idiv')), lambda x, y: x//y)\n        total_length = 1\n        last_incr = 1\n        for var_name, start, end, incr in reversed(self._loop_stack):\n            loop_var = symbol_pos_int(var_name)\n\n            # This unspools the iterations:\n            length = end-start  # FIXME is incr handled correct here?\n            counter = start+(idiv(global_iterator*last_incr, total_length)*incr) % length\n            total_length = total_length*length\n            last_incr = incr\n\n            base_loop_counters[loop_var] = sympy.lambdify(\n                global_iterator,\n                self.subs_consts(counter), modules=[numpy, {'Mod': numpy.mod}])\n\n            if git is not None:\n                try:  # Try to resolve to integer if global_iterator was given\n                    base_loop_counters[loop_var] = sympy.Integer(self.subs_consts(counter))\n                    continue\n                except (ValueError, TypeError):\n                    base_loop_counters[loop_var] = base_loop_counters[loop_var](git)\n\n        return base_loop_counters"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the global iterator of the current set of entries in the log.", "response": "def global_iterator(self):\n        \"\"\"\n        Return global iterator sympy expression\n        \"\"\"\n        global_iterator = sympy.Integer(0)\n        total_length = sympy.Integer(1)\n        for var_name, start, end, incr in reversed(self._loop_stack):\n            loop_var = symbol_pos_int(var_name)\n            length = end - start  # FIXME is incr handled correct here?\n            global_iterator += (loop_var - start) * total_length\n            total_length *= length\n        return global_iterator"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntransform a dictionary of indices to a global iterator integer.", "response": "def indices_to_global_iterator(self, indices):\n        \"\"\"\n        Transform a dictionary of indices to a global iterator integer.\n\n        Inverse of global_iterator_to_indices().\n        \"\"\"\n        global_iterator = self.subs_consts(self.global_iterator().subs(indices))\n        return global_iterator"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef max_global_iteration(self):\n        return self.indices_to_global_iterator({\n            symbol_pos_int(var_name): end-1 for var_name, start, end, incr in self._loop_stack\n        })", "response": "Return global iterator with last iteration number"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncompiles the load and store offsets for the virtual address space.", "response": "def compile_global_offsets(self, iteration=0, spacing=0):\n        \"\"\"\n        Return load and store offsets on a virtual address space.\n\n        :param iteration: controls the inner index counter\n        :param spacing: sets a spacing between the arrays, default is 0\n\n        All array variables (non scalars) are laid out linearly starting from 0. An optional\n        spacing can be set. The accesses are based on this layout.\n\n        The iteration 0 is the first iteration. All loops are mapped to this linear iteration\n        space.\n\n        Accesses to scalars are ignored.\n\n        Returned are load and store byte-offset pairs for each iteration.\n        \"\"\"\n        global_load_offsets = []\n        global_store_offsets = []\n\n        if isinstance(iteration, range):\n            iteration = numpy.arange(iteration.start, iteration.stop, iteration.step, dtype='O')\n        else:\n            if not isinstance(iteration, collections.Sequence):\n                iteration = [iteration]\n            iteration = numpy.array(iteration, dtype='O')\n\n        # loop indices based on iteration\n        # unwind global iteration count into loop counters:\n        base_loop_counters = self.global_iterator_to_indices()\n        total_length = self.iteration_length()\n\n        assert iteration.max() < self.subs_consts(total_length), \\\n            \"Iterations go beyond what is possible in the original code ({} vs {}). \" \\\n            \"One common reason, is that the iteration length are unrealistically small.\".format(\n                iteration.max(), self.subs_consts(total_length))\n\n        # Get sizes of arrays and base offsets for each array\n        var_sizes = self.array_sizes(in_bytes=True, subs_consts=True)\n        base_offsets = {}\n        base = 0\n        # Always arrange arrays in alphabetical order in memory, for reproducibility\n        for var_name, var_size in sorted(var_sizes.items(), key=lambda v: v[0]):\n            base_offsets[var_name] = base\n            array_total_size = self.subs_consts(var_size + spacing)\n            # Add bytes to align by 64 byte (typical cacheline size):\n            array_total_size = ((int(array_total_size) + 63) & ~63)\n            base += array_total_size\n\n        # Gather all read and write accesses to the array:\n        for var_name, var_size in var_sizes.items():\n            element_size = self.datatypes_size[self.variables[var_name][0]]\n            for r in self.sources.get(var_name, []):\n                offset_expr = self.access_to_sympy(var_name, r)\n                # Ignore accesses that always go to the same location (constant offsets)\n                if not any([s in base_loop_counters.keys() for s in offset_expr.free_symbols]):\n                    continue\n                offset = force_iterable(sympy.lambdify(\n                    base_loop_counters.keys(),\n                    self.subs_consts(\n                        offset_expr*element_size\n                        + base_offsets[var_name]), numpy))\n                # TODO possibly differentiate between index order\n                global_load_offsets.append(offset)\n            for w in self.destinations.get(var_name, []):\n                offset_expr = self.access_to_sympy(var_name, w)\n                # Ignore accesses that always go to the same location (constant offsets)\n                if not any([s in base_loop_counters.keys() for s in offset_expr.free_symbols]):\n                    continue\n                offset = force_iterable(sympy.lambdify(\n                    base_loop_counters.keys(),\n                    self.subs_consts(\n                        offset_expr*element_size\n                        + base_offsets[var_name]), numpy))\n                # TODO possibly differentiate between index order\n                global_store_offsets.append(offset)\n                # TODO take element sizes into account, return in bytes\n\n        # Generate numpy.array for each counter\n        counter_per_it = [v(iteration) for v in base_loop_counters.values()]\n\n        # Old and slow - left for reference\n        ## Data access as they appear with iteration order\n        #return zip_longest(zip(*[o(*counter_per_it) for o in global_load_offsets]),\n        #                   zip(*[o(*counter_per_it) for o in global_store_offsets]),\n        #                   fillvalue=None)\n\n        # Data access as they appear with iteration order\n        load_offsets = []\n        for o in global_load_offsets:\n            load_offsets.append(o(*counter_per_it))\n        # Convert to numpy ndarray and transpose to get offsets per iterations\n        load_offsets = numpy.asarray(load_offsets).T\n\n        store_offsets = []\n        for o in global_store_offsets:\n            store_offsets.append(o(*counter_per_it))\n        store_offsets = numpy.asarray(store_offsets).T\n\n        # Combine loads and stores\n        store_width = store_offsets.shape[1] if len(store_offsets.shape) > 1 else 0\n        dtype = [('load', load_offsets.dtype, (load_offsets.shape[1],)),\n                 ('store', store_offsets.dtype, (store_width,))]\n        offsets = numpy.empty(max(load_offsets.shape[0], store_offsets.shape[0]), dtype=dtype)\n        offsets['load'] = load_offsets\n        offsets['store'] = store_offsets\n\n        return offsets"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the number of bytes written out per high - level iteration.", "response": "def bytes_per_iteration(self):\n        \"\"\"\n        Consecutive bytes written out per high-level iterations (as counted by loop stack).\n\n        Is used to compute number of iterations per cacheline.\n        \"\"\"\n        # TODO Find longst consecutive writes to any variable and use as basis\n        var_name = list(self.destinations)[0]\n        var_type = self.variables[var_name][0]\n        # FIXME this is correct most of the time, but not guaranteed:\n        # Multiplying datatype size with step increment of inner-most loop\n        return self.datatypes_size[var_type] * self._loop_stack[-1][3]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef print_kernel_info(self, output_file=sys.stdout):\n        table = ('     idx |        min        max       step\\n' +\n                 '---------+---------------------------------\\n')\n        for l in self._loop_stack:\n            table += '{:>8} | {!r:>10} {!r:>10} {!r:>10}\\n'.format(*l)\n        print(prefix_indent('loop stack:        ', table), file=output_file)\n\n        table = ('    name |  offsets   ...\\n' +\n                 '---------+------------...\\n')\n        for name, offsets in list(self.sources.items()):\n            prefix = '{:>8} | '.format(name)\n            right_side = '\\n'.join(['{!r:}'.format(o) for o in offsets])\n            table += prefix_indent(prefix, right_side, later_prefix='         | ')\n        print(prefix_indent('data sources:      ', table), file=output_file)\n\n        table = ('    name |  offsets   ...\\n' +\n                 '---------+------------...\\n')\n        for name, offsets in list(self.destinations.items()):\n            prefix = '{:>8} | '.format(name)\n            right_side = '\\n'.join(['{!r:}'.format(o) for o in offsets])\n            table += prefix_indent(prefix, right_side, later_prefix='         | ')\n        print(prefix_indent('data destinations: ', table), file=output_file)\n\n        table = (' op | count \\n' +\n                 '----+-------\\n')\n        for op, count in list(self._flops.items()):\n            table += '{:>3} | {:>4}\\n'.format(op, count)\n        table += '     =======\\n'\n        table += '      {:>4}'.format(sum(self._flops.values()))\n        print(prefix_indent('FLOPs:     ', table), file=output_file)", "response": "Print kernel information in human readble format."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef print_variables_info(self, output_file=sys.stdout):\n        table = ('    name |   type size             \\n' +\n                 '---------+-------------------------\\n')\n        for name, var_info in list(self.variables.items()):\n            table += '{:>8} | {:>6} {!s:<10}\\n'.format(name, var_info[0], var_info[1])\n        print(prefix_indent('variables: ', table), file=output_file)", "response": "Print variables information in human readble format."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef print_constants_info(self, output_file=sys.stdout):\n        table = ('    name | value     \\n' +\n                 '---------+-----------\\n')\n        for name, value in list(self.constants.items()):\n            table += '{!s:>8} | {:<10}\\n'.format(name, value)\n        print(prefix_indent('constants: ', table), file=output_file)", "response": "Print constants information in human readble format."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_intermediate_file(self, name, machine_and_compiler_dependent=True, binary=False,\n                               fp=True):\n        \"\"\"\n        Create or open intermediate file (may be used for caching).\n\n        Will replace files older than kernel file, machine file or kerncraft version.\n\n        :param machine_and_compiler_dependent: set to False if file content does not depend on\n                                               machine file or compiler settings\n        :param fp: if False, will only return file name, not file object\n        :paarm binary: if True, use binary mode for file access\n\n        :return: (file object or file name, boolean if already existent and up-to-date)\n        \"\"\"\n        if self._filename:\n            base_name = os.path.join(os.path.dirname(self._filename),\n                                     '.' + os.path.basename(self._filename) + '_kerncraft')\n        else:\n            base_name = tempfile.mkdtemp()\n\n        if not self._keep_intermediates:\n            # Remove directory and all content up on program exit\n            atexit.register(shutil.rmtree, base_name)\n\n        if machine_and_compiler_dependent:\n            compiler, compiler_args = self._machine.get_compiler()\n            compiler_args = '_'.join(compiler_args).replace('/', '')\n            base_name += '/{}/{}/{}/'.format(\n                self._machine.get_identifier(), compiler, compiler_args)\n\n        # Create dirs recursively\n        os.makedirs(base_name, exist_ok=True)\n\n        # Build actual file path\n        file_path = os.path.join(base_name, name)\n\n        already_exists = False\n\n        # Check if file exists and is still fresh\n        if os.path.exists(file_path):\n            file_modified = datetime.utcfromtimestamp(os.stat(file_path).st_mtime)\n            if (file_modified < self._machine.get_last_modified_datetime() or\n                file_modified < kerncraft.get_last_modified_datetime() or\n                (self._filename and\n                 file_modified < datetime.utcfromtimestamp(os.stat(self._filename).st_mtime))):\n                os.remove(file_path)\n            else:\n                already_exists = True\n\n        if fp:\n            if already_exists:\n                mode = 'r+'\n            else:\n                mode = 'w'\n            if binary:\n                mode += 'b'\n            f = open(file_path, mode)\n            return f, already_exists\n        else:\n            return reduce_path(file_path), already_exists", "response": "Create or open intermediate file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef print_kernel_code(self, output_file=sys.stdout):\n        print(self.kernel_code, file=output_file)", "response": "Print source code of kernel."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a mathematical expression to a sympy representation.", "response": "def conv_ast_to_sym(self, math_ast):\n        \"\"\"\n        Convert mathematical expressions to a sympy representation.\n\n        May only contain paranthesis, addition, subtraction and multiplication from AST.\n        \"\"\"\n        if type(math_ast) is c_ast.ID:\n            return symbol_pos_int(math_ast.name)\n        elif type(math_ast) is c_ast.Constant:\n            return sympy.Integer(math_ast.value)\n        else:  # elif type(dim) is c_ast.BinaryOp:\n            op = {\n                '*': operator.mul,\n                '+': operator.add,\n                '-': operator.sub\n            }\n\n            return op[math_ast.op](\n                self.conv_ast_to_sym(math_ast.left),\n                self.conv_ast_to_sym(math_ast.right))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a tuple of offsets of an array reference in all dimensions.", "response": "def _get_offsets(self, aref, dim=0):\n        \"\"\"\n        Return a tuple of offsets of an ArrayRef object in all dimensions.\n\n        The index order is right to left (c-code order).\n        e.g. c[i+1][j-2] -> (-2, +1)\n\n        If aref is actually a c_ast.ID, None will be returned.\n        \"\"\"\n        if isinstance(aref, c_ast.ID):\n            return None\n\n        # Check for restrictions\n        assert type(aref.name) in [c_ast.ArrayRef, c_ast.ID], \\\n            \"array references must only be used with variables or other array references\"\n        assert type(aref.subscript) in [c_ast.ID, c_ast.Constant, c_ast.BinaryOp], \\\n            'array subscript must only contain variables or binary operations'\n\n        # Convert subscript to sympy and append\n        idxs = [self.conv_ast_to_sym(aref.subscript)]\n\n        # Check for more indices (multi-dimensional access)\n        if type(aref.name) is c_ast.ArrayRef:\n            idxs += self._get_offsets(aref.name, dim=dim+1)\n\n        # Reverse to preserver order (the subscripts in the AST are traversed backwards)\n        if dim == 0:\n            idxs.reverse()\n\n        return tuple(idxs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn base name of ArrayRef object.", "response": "def _get_basename(cls, aref):\n        \"\"\"\n        Return base name of ArrayRef object.\n\n        e.g. c[i+1][j-2] -> 'c'\n        \"\"\"\n        if isinstance(aref.name, c_ast.ArrayRef):\n            return cls._get_basename(aref.name)\n        elif isinstance(aref.name, str):\n            return aref.name\n        else:\n            return aref.name.name"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_index_type(self, loop_nest=None):\n        if loop_nest is None:\n            loop_nest = self.get_kernel_loop_nest()\n        if type(loop_nest) is c_ast.For:\n            loop_nest = [loop_nest]\n        index_types = (None, None)\n        for s in loop_nest:\n            if type(s) is c_ast.For:\n                if type(s.stmt) in [c_ast.For, c_ast.Compound]:\n                    other = self.get_index_type(loop_nest=s.stmt)\n                else:\n                    other = None\n                index_types = (s.init.decls[0].type.type.names, other)\n                break\n        if index_types[0] == index_types[1] or index_types[1] is None:\n            return index_types[0]\n        else:\n            raise ValueError(\"Loop indices must have same type, found {}.\".format(index_types))", "response": "Get index type used in loop nest."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates constants declarations :return: list of declarations", "response": "def _build_const_declartions(self, with_init=True):\n        \"\"\"\n        Generate constants declarations\n\n        :return: list of declarations\n        \"\"\"\n        decls = []\n\n        # Use type as provided by user in loop indices\n        index_type = self.get_index_type()\n\n        i = 2  # subscript for cli input, 1 is reserved for repeat\n        for k in self.constants:\n            # const long long N = strtoul(argv[2])\n            # with increasing N and 1\n            # TODO change subscript of argv depending on constant count\n            type_decl = c_ast.TypeDecl(k.name, ['const'], c_ast.IdentifierType(index_type))\n            init = None\n            if with_init:\n                init = c_ast.FuncCall(\n                    c_ast.ID('atoi'),\n                    c_ast.ExprList([c_ast.ArrayRef(c_ast.ID('argv'),\n                                                   c_ast.Constant('int', str(i)))]))\n            i += 1\n            decls.append(c_ast.Decl(\n                k.name, ['const'], [], [],\n                type_decl, init, None))\n\n        return decls"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_kernel_loop_nest(self):\n        loop_nest = [s for s in self.kernel_ast.block_items\n                     if type(s) in [c_ast.For, c_ast.Pragma, c_ast.FuncCall]]\n        assert len(loop_nest) >= 1, \"Found to few for statements in kernel\"\n        return loop_nest", "response": "Return kernel loop nest including any preceding pragmas and following swaps."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _build_array_declarations(self, with_init=True):\n        # copy array declarations from from kernel ast\n        array_declarations = deepcopy(self.get_array_declarations())\n        array_dict = []\n        for d in array_declarations:\n            # We need to transform\n            array_dict.append(transform_multidim_to_1d_decl(d))\n            transform_array_decl_to_malloc(d, with_init=with_init)\n        return array_declarations, dict(array_dict)", "response": "Generate declaration statements for arrays."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the inner most for loop in loop nest", "response": "def _find_inner_most_loop(self, loop_nest):\n        \"\"\"Return inner most for loop in loop nest\"\"\"\n        r = None\n        for s in loop_nest:\n            if type(s) is c_ast.For:\n                return self._find_inner_most_loop(s) or s\n            else:\n                r = r or self._find_inner_most_loop(s)\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _build_array_initializations(self, array_dimensions):\n        kernel = deepcopy(deepcopy(self.get_kernel_loop_nest()))\n        # traverse to the inner most for loop:\n        inner_most = self._find_inner_most_loop(kernel)\n        orig_inner_stmt = inner_most.stmt\n        inner_most.stmt = c_ast.Compound([])\n\n        rand_float_str = str(random.uniform(1.0, 0.1))\n\n        # find all array references in original orig_inner_stmt\n        for aref in find_node_type(orig_inner_stmt, c_ast.ArrayRef):\n            # transform to 1d references\n            transform_multidim_to_1d_ref(aref, array_dimensions)\n            # build static assignments and inject into inner_most.stmt\n            inner_most.stmt.block_items.append(c_ast.Assignment(\n                '=', aref, c_ast.Constant('float', rand_float_str)))\n\n        return kernel", "response": "Generate initialization statements for arrays."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _build_dummy_calls(self):\n        # Make sure nothing gets removed by inserting dummy calls\n        dummy_calls = []\n        for d in self.kernel_ast.block_items:\n            # Only consider toplevel declarations from kernel ast\n            if type(d) is not c_ast.Decl: continue\n            if type(d.type) is c_ast.ArrayDecl:\n                dummy_calls.append(c_ast.FuncCall(\n                    c_ast.ID('dummy'),\n                    c_ast.ExprList([c_ast.ID(d.name)])))\n            else:\n                dummy_calls.append(c_ast.FuncCall(\n                    c_ast.ID('dummy'),\n                    c_ast.ExprList([c_ast.UnaryOp('&', c_ast.ID(d.name))])))\n        dummy_stmt = c_ast.If(\n            cond=c_ast.ID('var_false'),\n            iftrue=c_ast.Compound(dummy_calls),\n            iffalse=None)\n        return dummy_stmt", "response": "Generate false if branch with dummy calls"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds and return kernel function declaration", "response": "def _build_kernel_function_declaration(self, name='kernel'):\n        \"\"\"Build and return kernel function declaration\"\"\"\n        array_declarations, array_dimensions = self._build_array_declarations(with_init=False)\n        scalar_declarations = self._build_scalar_declarations(with_init=False)\n        const_declarations = self._build_const_declartions(with_init=False)\n        return c_ast.FuncDecl(args=c_ast.ParamList(params=array_declarations + scalar_declarations +\n                                                          const_declarations),\n                              type=c_ast.TypeDecl(declname=name,\n                                                  quals=[],\n                                                  type=c_ast.IdentifierType(names=['void'])))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _build_scalar_declarations(self, with_init=True):\n        # copy scalar declarations from from kernel ast\n        scalar_declarations = [deepcopy(d) for d in self.kernel_ast.block_items\n                               if type(d) is c_ast.Decl and type(d.type) is c_ast.TypeDecl]\n        # add init values to declarations\n        if with_init:\n            random.seed(2342)  # we want reproducible random numbers\n            for d in scalar_declarations:\n                if d.type.type.names[0] in ['double', 'float']:\n                    d.init = c_ast.Constant('float', str(random.uniform(1.0, 0.1)))\n                elif d.type.type.names[0] in ['int', 'long', 'long long',\n                                              'unsigned int', 'unsigned long', 'unsigned long long']:\n                    d.init = c_ast.Constant('int', 2)\n\n        return scalar_declarations", "response": "Build and return scalar variable declarations"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_kernel_code(self, openmp=False, as_filename=False, name='kernel'):\n        assert self.kernel_ast is not None, \"AST does not exist, this could be due to running \" \\\n                                            \"based on a kernel description rather than code.\"\n        file_name = 'kernel'\n        if openmp:\n            file_name += '-omp'\n        file_name += '.c'\n\n        fp, already_available = self._get_intermediate_file(\n            file_name, machine_and_compiler_dependent=False)\n\n        # Use already cached version\n        if already_available:\n            code = fp.read()\n        else:\n            array_declarations, array_dimensions = self._build_array_declarations()\n\n            # Prepare actual kernel loop nest\n            if openmp:\n                # with OpenMP code\n                kernel = deepcopy(self.get_kernel_loop_nest())\n                # find all array references in kernel\n                for aref in find_node_type(kernel, c_ast.ArrayRef):\n                    # transform to 1d references\n                    transform_multidim_to_1d_ref(aref, array_dimensions)\n                omp_pragmas = [p for p in find_node_type(kernel, c_ast.Pragma)\n                               if 'omp' in p.string]\n                # TODO if omp parallel was found, remove it (also replace \"parallel for\" -> \"for\")\n                # if no omp for pragmas are present, insert suitable ones\n                if not omp_pragmas:\n                    kernel.insert(0, c_ast.Pragma(\"omp for\"))\n                # otherwise do not change anything\n            else:\n                # with original code\n                kernel = deepcopy(self.get_kernel_loop_nest())\n                # find all array references in kernel\n                for aref in find_node_type(kernel, c_ast.ArrayRef):\n                    # transform to 1d references\n                    transform_multidim_to_1d_ref(aref, array_dimensions)\n\n            function_ast = c_ast.FuncDef(decl=c_ast.Decl(\n                name=name, type=self._build_kernel_function_declaration(name=name), quals=[],\n                storage=[], funcspec=[], init=None, bitsize=None),\n                body=c_ast.Compound(block_items=kernel),\n                param_decls=None)\n\n            # Generate code\n            code = CGenerator().visit(function_ast)\n\n            # Insert missing #includes from template to top of code\n            code = '#include \"kerncraft.h\"\\n\\n' + code\n\n            # Store to file\n            fp.write(code)\n        fp.close()\n\n        if as_filename:\n            return fp.name\n        else:\n            return code", "response": "Generate and return compilable source code with kernel function from AST."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _build_kernel_call(self, name='kernel'):\n        return c_ast.FuncCall(name=c_ast.ID(name=name), args=c_ast.ExprList(exprs=[\n            c_ast.ID(name=d.name) for d in (\n                    self._build_array_declarations()[0] +\n                    self._build_scalar_declarations() +\n                    self._build_const_declartions())]))", "response": "Generate and return kernel call ast."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_main_code(self, as_filename=False, kernel_function_name='kernel'):\n        # TODO produce nicer code, including help text and other \"comfort features\".\n        assert self.kernel_ast is not None, \"AST does not exist, this could be due to running \" \\\n                                            \"based on a kernel description rather than code.\"\n\n        fp, already_available = self._get_intermediate_file('main.c',\n                                                            machine_and_compiler_dependent=False)\n\n        # Use already cached version\n        if already_available:\n            code = fp.read()\n        else:\n            parser = CParser()\n            template_code = self.CODE_TEMPLATE\n            template_ast = parser.parse(clean_code(template_code,\n                                                   macros=True, comments=True, pragmas=False))\n            ast = deepcopy(template_ast)\n\n            # Define and replace DECLARE_CONSTS\n            replace_id(ast, \"DECLARE_CONSTS\", self._build_const_declartions(with_init=True))\n\n            # Define and replace DECLARE_ARRAYS\n            array_declarations, array_dimensions = self._build_array_declarations()\n            replace_id(ast, \"DECLARE_ARRAYS\", array_declarations)\n\n            # Define and replace DECLARE_INIT_SCALARS\n            replace_id(ast, \"DECLARE_INIT_SCALARS\", self._build_scalar_declarations())\n\n            # Define and replace DUMMY_CALLS\n            replace_id(ast, \"DUMMY_CALLS\", self._build_dummy_calls())\n\n            # Define and replace KERNEL_DECL\n            ast.ext.insert(0, self._build_kernel_function_declaration(\n                name=kernel_function_name))\n\n            # Define and replace KERNEL_CALL\n            replace_id(ast, \"KERNEL_CALL\", self._build_kernel_call())\n\n            # Define and replace INIT_ARRAYS based on previously generated kernel\n            replace_id(ast, \"INIT_ARRAYS\", self._build_array_initializations(array_dimensions))\n\n            # Generate code\n            code = CGenerator().visit(ast)\n\n            # Insert missing #includes from template to top of code\n            code = '\\n'.join([l for l in template_code.split('\\n') if l.startswith(\"#include\")]) + \\\n                   '\\n\\n' + code\n\n            # Store to file\n            fp.write(code)\n        fp.close()\n\n        if as_filename:\n            return fp.name\n        else:\n            return code", "response": "Generate and return compilable source code from the main. c file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef assemble_to_object(self, in_filename, verbose=False):\n        # Build file name\n        file_base_name = os.path.splitext(os.path.basename(in_filename))[0]\n        out_filename, already_exists = self._get_intermediate_file(file_base_name + '.o',\n                                                                   binary=True,\n                                                                   fp=False)\n        if already_exists:\n            # Do not use caching, because pointer_increment or asm_block selection may be different\n            pass\n        compiler, compiler_args = self._machine.get_compiler()\n\n        # Compile to object file\n        compiler_args.append('-c')\n\n        cmd = [compiler] + [\n            in_filename] + \\\n              compiler_args + ['-o', out_filename]\n\n        if verbose:\n            print('Executing (assemble_to_object): ', ' '.join(cmd))\n\n        try:\n            # Assemble all to a binary\n            subprocess.check_output(cmd)\n        except subprocess.CalledProcessError as e:\n            print(\"Assembly failed:\", e, file=sys.stderr)\n            sys.exit(1)\n\n        return out_filename", "response": "Assemble a file into a new object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncompiling the kernel code into a file and return the filename.", "response": "def compile_kernel(self, openmp=False, assembly=False, verbose=False):\n        \"\"\"\n        Compile source (from as_code(type_)) to assembly or object and return (fileptr, filename).\n\n        Output can be used with Kernel.assemble()\n        \"\"\"\n        compiler, compiler_args = self._machine.get_compiler()\n\n        in_filename = self.get_kernel_code(openmp=openmp, as_filename=True)\n\n        if assembly:\n            compiler_args += ['-S']\n            suffix = '.s'\n        else:\n            suffix = '.o'\n        out_filename, already_exists = self._get_intermediate_file(\n            os.path.splitext(os.path.basename(in_filename))[0]+suffix, binary=not assembly, fp=False)\n        if already_exists:\n            if verbose:\n                print('Executing (compile_kernel): ', 'using cached', out_filename)\n            return out_filename\n\n        compiler_args += ['-std=c99']\n\n        cmd = ([compiler] +\n               [in_filename,\n                '-c',\n                '-I'+reduce_path(os.path.abspath(os.path.dirname(\n                    os.path.realpath(__file__)))+'/headers/'),\n                '-o', out_filename] +\n               compiler_args)\n\n        if verbose:\n            print('Executing (compile_kernel): ', ' '.join(cmd))\n\n        try:\n            subprocess.check_output(cmd)\n\n        except subprocess.CalledProcessError as e:\n            print(\"Compilation failed:\", e, file=sys.stderr)\n            sys.exit(1)\n\n        # FIXME TODO FIXME TODO FIXME TODO\n        # Hacky workaround for icc issue (icc may issue vkmovb instructions with AVX512, which are\n        # invalid and should be kmovb):\n        if compiler == 'icc' and assembly:\n            with open(out_filename, 'r+') as f:\n                assembly = f.read()\n                f.seek(0)\n                f.write(assembly.replace('vkmovb', 'kmovb'))\n                f.truncate()\n        # FIXME TODO FIXME TODO FIXME TODO\n\n        # Let's return the out_file name\n        return out_filename"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns an IACA analysis and return its outcome.", "response": "def iaca_analysis(self, micro_architecture, asm_block='auto',\n                      pointer_increment='auto_with_manual_fallback', verbose=False):\n        \"\"\"\n        Run an IACA analysis and return its outcome.\n\n        *asm_block* controls how the to-be-marked block is chosen. \"auto\" (default) results in\n        the largest block, \"manual\" results in interactive and a number in the according block.\n\n        *pointer_increment* is the number of bytes the pointer is incremented after the loop or\n           - 'auto': automatic detection, RuntimeError is raised in case of failure\n           - 'auto_with_manual_fallback': automatic detection, fallback to manual input\n           - 'manual': prompt user\n        \"\"\"\n        asm_filename = self.compile_kernel(assembly=True, verbose=verbose)\n        asm_marked_filename = os.path.splitext(asm_filename)[0]+'-iaca.s'\n        with open(asm_filename, 'r') as in_file, open(asm_marked_filename, 'w') as out_file:\n            self.asm_block = iaca.iaca_instrumentation(\n                in_file, out_file,\n                block_selection=asm_block,\n                pointer_increment=pointer_increment)\n        obj_name = self.assemble_to_object(asm_marked_filename, verbose=verbose)\n        return iaca.iaca_analyse_instrumented_binary(obj_name, micro_architecture), self.asm_block"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncompiles source to executable with likwid capabilities and return the executable name.", "response": "def build_executable(self, lflags=None, verbose=False, openmp=False):\n        \"\"\"Compile source to executable with likwid capabilities and return the executable name.\"\"\"\n        compiler, compiler_args = self._machine.get_compiler()\n\n        kernel_obj_filename = self.compile_kernel(openmp=openmp, verbose=verbose)\n        out_filename, already_exists = self._get_intermediate_file(\n            os.path.splitext(os.path.basename(kernel_obj_filename))[0], binary=True, fp=False)\n\n        if not already_exists:\n            main_source_filename = self.get_main_code(as_filename=True)\n\n            if not (('LIKWID_INCLUDE' in os.environ or 'LIKWID_INC' in os.environ) and\n                    'LIKWID_LIB' in os.environ):\n                print('Could not find LIKWID_INCLUDE (e.g., \"-I/app/likwid/4.1.2/include\") and '\n                      'LIKWID_LIB (e.g., \"-L/apps/likwid/4.1.2/lib\") environment variables',\n                      file=sys.stderr)\n                sys.exit(1)\n\n            compiler_args += [\n                '-std=c99',\n                '-I'+reduce_path(os.path.abspath(os.path.dirname(\n                    os.path.realpath(__file__)))+'/headers/'),\n                os.environ.get('LIKWID_INCLUDE', ''),\n                os.environ.get('LIKWID_INC', ''),\n                '-llikwid']\n\n            # This is a special case for unittesting\n            if os.environ.get('LIKWID_LIB') == '':\n                compiler_args = compiler_args[:-1]\n\n            if lflags is None:\n                lflags = []\n            lflags += os.environ['LIKWID_LIB'].split(' ') + ['-pthread']\n            compiler_args += os.environ['LIKWID_LIB'].split(' ') + ['-pthread']\n\n            infiles = [reduce_path(os.path.abspath(os.path.dirname(\n                os.path.realpath(__file__)))+'/headers/dummy.c'),\n                       kernel_obj_filename, main_source_filename]\n\n            cmd = [compiler] + infiles + compiler_args + ['-o', out_filename]\n            # remove empty arguments\n            cmd = list(filter(bool, cmd))\n            if verbose:\n                print('Executing (build_executable): ', ' '.join(cmd))\n            try:\n                subprocess.check_output(cmd)\n            except subprocess.CalledProcessError as e:\n                print(\"Build failed:\", e, file=sys.stderr)\n                sys.exit(1)\n        else:\n            if verbose:\n                print('Executing (build_executable): ', 'using cached', out_filename)\n\n        return out_filename"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts any string to a sympy object or None.", "response": "def string_to_sympy(cls, s):\n        \"\"\"Convert any string to a sympy object or None.\"\"\"\n        if isinstance(s, int):\n            return sympy.Integer(s)\n        elif isinstance(s, list):\n            return tuple([cls.string_to_sympy(e) for e in s])\n        elif s is None:\n            return None\n        else:\n            # Step 1 build expression with the whole alphabet redefined:\n            local_dict = {c: symbol_pos_int(c) for c in s if c in string.ascii_letters}\n            # TODO find nicer solution for N and other pre-mapped letters\n            preliminary_expr = parse_expr(s, local_dict=local_dict)\n            # Replace all free symbols with positive integer versions:\n            local_dict.update(\n                {s.name: symbol_pos_int(s.name) for s in preliminary_expr.free_symbols})\n            return parse_expr(s, local_dict=local_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_identifier(self):\n        if self._path:\n            return os.path.basename(self._path)\n        else:\n            return hashlib.sha256(hashlib.sha256(repr(self._data).encode())).hexdigest()", "response": "Return identifier which is either the machine file name or sha256 checksum of data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning datetime object of modified time of machine file. Return now if not a file.", "response": "def get_last_modified_datetime(self):\n        \"\"\"Return datetime object of modified time of machine file. Return now if not a file.\"\"\"\n        if self._path:\n            statbuf = os.stat(self._path)\n            return datetime.utcfromtimestamp(statbuf.st_mtime)\n        else:\n            return datetime.now()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a cachesim. CacheSimulator object based on the machine description.", "response": "def get_cachesim(self, cores=1):\n        \"\"\"\n        Return a cachesim.CacheSimulator object based on the machine description.\n\n        :param cores: core count (default: 1)\n        \"\"\"\n        cache_dict = {}\n        for c in self['memory hierarchy']:\n            # Skip main memory\n            if 'cache per group' not in c:\n                continue\n            cache_dict[c['level']] = deepcopy(c['cache per group'])\n            # Scale size of shared caches according to cores\n            if c['cores per group'] > 1:\n                cache_dict[c['level']]['sets'] //= cores\n\n        cs, caches, mem = cachesim.CacheSimulator.from_dict(cache_dict)\n\n        return cs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_bandwidth(self, cache_level, read_streams, write_streams, threads_per_core, cores=None):\n        # try to find best fitting kernel (closest to read/write ratio):\n        # write allocate has to be handled in kernel information (all writes are also reads)\n        # TODO support for non-write-allocate architectures\n        try:\n            target_ratio = read_streams/write_streams\n        except ZeroDivisionError:\n            target_ratio = float('inf')\n        measurement_kernel = 'load'\n        measurement_kernel_info = self['benchmarks']['kernels'][measurement_kernel]\n        measurement_kernel_ratio = float('inf')\n        for kernel_name, kernel_info in sorted(self['benchmarks']['kernels'].items()):\n            try:\n                kernel_ratio = ((kernel_info['read streams']['streams'] +\n                                 kernel_info['write streams']['streams'] -\n                                 kernel_info['read+write streams']['streams']) /\n                                kernel_info['write streams']['streams'])\n            except ZeroDivisionError:\n                kernel_ratio = float('inf')\n\n            if abs(kernel_ratio - target_ratio) < abs(measurement_kernel_ratio - target_ratio):\n                measurement_kernel = kernel_name\n                measurement_kernel_info = kernel_info\n                measurement_kernel_ratio = kernel_ratio\n\n        # choose smt, and then use max/saturation bw\n        bw_level = self['memory hierarchy'][cache_level]['level']\n        bw_measurements = \\\n            self['benchmarks']['measurements'][bw_level][threads_per_core]\n        assert threads_per_core == bw_measurements['threads per core'], \\\n            'malformed measurement dictionary in machine file.'\n        if cores is not None:\n            # Used by Roofline model\n            run_index = bw_measurements['cores'].index(cores)\n            bw = bw_measurements['results'][measurement_kernel][run_index]\n        else:\n            # Used by ECM model\n            # Choose maximum number of cores to get bandwidth for\n            max_cores = min(self['memory hierarchy'][cache_level]['cores per group'],\n                            self['cores per NUMA domain'])\n            bw = max(bw_measurements['results'][measurement_kernel][:max_cores])\n\n        # Correct bandwidth due to miss-measurement of write allocation\n        # TODO support non-temporal stores and non-write-allocate architectures\n        if cache_level == 0:\n            # L1 does not have write-allocate, so everything is measured correctly\n            factor = 1.0\n        else:\n            factor = (float(measurement_kernel_info['read streams']['bytes']) +\n                      2.0*float(measurement_kernel_info['write streams']['bytes']) -\n                      float(measurement_kernel_info['read+write streams']['bytes'])) / \\\n                     (float(measurement_kernel_info['read streams']['bytes']) +\n                      float(measurement_kernel_info['write streams']['bytes']))\n        bw = bw * factor\n\n        return bw, measurement_kernel", "response": "Return best fitting bandwidth according to number of read and write streams."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns tuple of compiler and flags.", "response": "def get_compiler(self, compiler=None, flags=None):\n        \"\"\"\n        Return tuple of compiler and compiler flags.\n\n        Selects compiler and flags from machine description file, commandline arguments or call\n        arguements.\n        \"\"\"\n        if self._args:\n            compiler = compiler or self._args.compiler\n            flags = flags or self._args.compiler_flags\n        if compiler is None:\n            # Select first available compiler in machine description file's compiler dict\n            for c in self['compiler'].keys():\n                # Making sure compiler is available:\n                if find_executable(c) is not None:\n                    compiler = c\n                    break\n            else:\n                raise RuntimeError(\"No compiler ({}) was found. Add different one in machine file, \"\n                                   \"via --compiler argument or make sure it will be found in \"\n                                   \"$PATH.\".format(list(self['compiler'].keys())))\n        if flags is None:\n            # Select from machine description file\n            flags = self['compiler'].get(compiler, '')\n\n        return compiler, flags.split(' ')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_perfctr_event(perfctr):\n        split_perfctr = perfctr.split(':')\n        assert len(split_perfctr) >= 2, \"Atleast one colon (:) is required in the event name\"\n        event_tuple = split_perfctr[:2]\n        parameters = {}\n        for p in split_perfctr[2:]:\n            if '=' in p:\n                k, v = p.split('=')\n                if v.startswith('0x'):\n                    parameters[k] = int(v, 16)\n                else:\n                    parameters[k] = int(v)\n            else:\n                parameters[p] = None\n        event_tuple.append(parameters)\n        return tuple(event_tuple)", "response": "Parse a string in machine description to tuple representation used in Benchmark module."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_perfmetric(metric):\n        # Find all perfs counter references\n        perfcounters = re.findall(r'[A-Z0-9_]+:[A-Z0-9\\[\\]|\\-]+(?::[A-Za-z0-9\\-_=]+)*', metric)\n\n        # Build a temporary metric, with parser-friendly Symbol names\n        temp_metric = metric\n        temp_pc_names = {\"SYM{}\".format(re.sub(\"[\\[\\]\\-|=:]\", \"_\", pc)): pc\n                         for i, pc in enumerate(perfcounters)}\n        for var_name, pc in temp_pc_names.items():\n            temp_metric = temp_metric.replace(pc, var_name)\n        # Parse temporary expression\n        expr = parse_expr(temp_metric)\n\n        # Rename symbols to originals\n        for s in expr.free_symbols:\n            if s.name in temp_pc_names:\n                s.name = temp_pc_names[str(s)]\n\n        events = {s: MachineModel.parse_perfctr_event(s.name) for s in expr.free_symbols\n                  if s.name in perfcounters}\n\n        return expr, events", "response": "Return ( sympy expressions event names and symbols dict from performance metric str."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nenforce that no ranges overlap in internal storage.", "response": "def _enforce_no_overlap(self, start_at=0):\n        \"\"\"Enforce that no ranges overlap in internal storage.\"\"\"\n        i = start_at\n        while i+1 < len(self.data):\n            if self.data[i][1] >= self.data[i+1][0]:\n                # beginning of i+1-th range is contained in i-th range\n                if self.data[i][1] < self.data[i+1][1]:\n                    # i+1-th range is longer, thus enlarge i-th range\n                    self.data[i][1] = self.data[i+1][1]\n\n                # removed contained range\n                del self.data[i+1]\n            i += 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns local folder path of header files.", "response": "def get_header_path() -> str:\n    \"\"\"Return local folder path of header files.\"\"\"\n    import os\n    return os.path.abspath(os.path.dirname(os.path.realpath(__file__))) + '/headers/'"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _align_iteration_with_cl_boundary(self, iteration, subtract=True):\n        # FIXME handle multiple datatypes\n        element_size = self.kernel.datatypes_size[self.kernel.datatype]\n        cacheline_size = self.machine['cacheline size']\n        elements_per_cacheline = int(cacheline_size // element_size)\n\n        # Gathering some loop information:\n        inner_loop = list(self.kernel.get_loop_stack(subs_consts=True))[-1]\n        inner_increment = inner_loop['increment']\n\n        # do this by aligning either writes (preferred) or reads\n        # Assumption: writes (and reads) increase linearly\n        o = self.kernel.compile_global_offsets(iteration=iteration)[0]\n        if len(o[1]):\n            # we have a write to work with:\n            first_offset = min(o[1])\n        else:\n            # we use reads\n            first_offset = min(o[0])\n\n        diff = first_offset - \\\n               (int(first_offset) >> self.csim.first_level.cl_bits << self.csim.first_level.cl_bits)\n        if diff == 0:\n            return iteration\n        elif subtract:\n            return iteration - (diff // element_size) // inner_increment\n        else:\n            return iteration + (elements_per_cacheline - diff // element_size) // inner_increment", "response": "Align iteration with cacheline boundary."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_loads(self):\n        return [self.stats[cache_level]['LOAD_count'] / self.first_dim_factor\n                for cache_level in range(len(self.machine['memory hierarchy']))]", "response": "Return a list with number of loaded cache lines per memory hierarchy level."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list with number of hit cache lines per memory hierarchy level.", "response": "def get_hits(self):\n        \"\"\"Return a list with number of hit cache lines per memory hierarchy level.\"\"\"\n        return [self.stats[cache_level]['HIT_count']/self.first_dim_factor\n                for cache_level in range(len(self.machine['memory hierarchy']))]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list with number of missed cache lines per memory hierarchy level.", "response": "def get_misses(self):\n        \"\"\"Return a list with number of missed cache lines per memory hierarchy level.\"\"\"\n        return [self.stats[cache_level]['MISS_count']/self.first_dim_factor\n                for cache_level in range(len(self.machine['memory hierarchy']))]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_stores(self):\n        return [self.stats[cache_level]['STORE_count']/self.first_dim_factor\n                for cache_level in range(len(self.machine['memory hierarchy']))]", "response": "Return a list with number of stored cache lines per memory hierarchy level."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list with number of evicted cache lines per memory hierarchy level.", "response": "def get_evicts(self):\n        \"\"\"Return a list with number of evicted cache lines per memory hierarchy level.\"\"\"\n        return [self.stats[cache_level]['EVICT_count']/self.first_dim_factor\n                for cache_level in range(len(self.machine['memory hierarchy']))]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_infos(self):\n        first_dim_factor = self.first_dim_factor\n        infos = {'memory hierarchy': [], 'cache stats': self.stats,\n                 'cachelines in stats': first_dim_factor}\n        for cache_level, cache_info in list(enumerate(self.machine['memory hierarchy'])):\n            infos['memory hierarchy'].append({\n                'index': len(infos['memory hierarchy']),\n                'level': '{}'.format(cache_info['level']),\n                'total loads': self.stats[cache_level]['LOAD_byte']/first_dim_factor,\n                'total misses': self.stats[cache_level]['MISS_byte']/first_dim_factor,\n                'total hits': self.stats[cache_level]['HIT_byte']/first_dim_factor,\n                'total stores': self.stats[cache_level]['STORE_byte']/first_dim_factor,\n                'total evicts': self.stats[cache_level]['EVICT_byte']/first_dim_factor,\n                'total lines load': self.stats[cache_level]['LOAD_count']/first_dim_factor,\n                'total lines misses': self.stats[cache_level]['MISS_count']/first_dim_factor,\n                'total lines hits': self.stats[cache_level]['HIT_count']/first_dim_factor,\n                'total lines stores': self.stats[cache_level]['STORE_count']/first_dim_factor,\n                'total lines evicts': self.stats[cache_level]['EVICT_count']/first_dim_factor,\n                'cycles': None})\n        return infos", "response": "Return verbose information about the predictor."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef measure_bw(type_, total_size, threads_per_core, max_threads_per_core, cores_per_socket,\n               sockets):\n    \"\"\"*size* is given in kilo bytes\"\"\"\n    groups = []\n    for s in range(sockets):\n        groups += [\n            '-w',\n            'S' + str(s) + ':' + str(total_size) + 'kB:' +\n            str(threads_per_core * cores_per_socket) +\n            ':1:' + str(int(max_threads_per_core / threads_per_core))]\n    # for older likwid versions add ['-g', str(sockets), '-i', str(iterations)] to cmd\n    cmd = ['likwid-bench', '-t', type_] + groups\n    sys.stderr.write(' '.join(cmd))\n    output = subprocess.Popen(cmd, stdout=subprocess.PIPE).communicate()[0].decode('utf-8')\n    if not output:\n        print(' '.join(cmd) + ' returned no output, possibly wrong version installed '\n                              '(requires 4.0 or later)', file=sys.stderr)\n        sys.exit(1)\n    bw = float(get_match_or_break(r'^MByte/s:\\s+([0-9]+(?:\\.[0-9]+)?)\\s*$', output)[0])\n    print(' ', PrefixedUnit(bw, 'MB/s'), file=sys.stderr)\n    return PrefixedUnit(bw, 'MB/s')", "response": "measure the amount of memory needed for a given type of likwid"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfix environment variable to a value within context.", "response": "def fix_env_variable(name, value):\n    \"\"\"Fix environment variable to a value within context. Unset if value is None.\"\"\"\n    orig = os.environ.get(name, None)\n    if value is not None:\n        # Set if value is not None\n        os.environ[name] = value\n    elif name in os.environ:\n        # Unset if value is None\n        del os.environ[name]\n    try:\n        yield\n    finally:\n        if orig is not None:\n            # Restore original value\n            os.environ[name] = orig\n        elif name in os.environ:\n            # Unset\n            del os.environ[name]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns a command with likwid - perfctr and returns the result as a dict.", "response": "def perfctr(self, cmd, group='MEM', code_markers=True):\n        \"\"\"\n        Run *cmd* with likwid-perfctr and returns result as dict.\n\n        *group* may be a performance group known to likwid-perfctr or an event string.\n\n        if CLI argument cores > 1, running with multi-core, otherwise single-core\n        \"\"\"\n        # Making sure likwid-perfctr is available:\n        if find_executable('likwid-perfctr') is None:\n            print(\"likwid-perfctr was not found. Make sure likwid is installed and found in PATH.\",\n                  file=sys.stderr)\n            sys.exit(1)\n\n        # FIXME currently only single core measurements support!\n        perf_cmd = ['likwid-perfctr', '-f', '-O', '-g', group]\n\n        cpu = 'S0:0'\n        if self._args.cores > 1:\n            cpu += '-'+str(self._args.cores-1)\n\n        # Pinned and measured on cpu\n        perf_cmd += ['-C', cpu]\n\n        # code must be marked using likwid markers\n        perf_cmd.append('-m')\n\n        perf_cmd += cmd\n        if self.verbose > 1:\n            print(' '.join(perf_cmd))\n        try:\n            with fix_env_variable('OMP_NUM_THREADS', None):\n                output = subprocess.check_output(perf_cmd).decode('utf-8').split('\\n')\n        except subprocess.CalledProcessError as e:\n            print(\"Executing benchmark failed: {!s}\".format(e), file=sys.stderr)\n            sys.exit(1)\n\n        # TODO multicore output is different and needs to be considered here!\n        results = {}\n        for line in output:\n            line = line.split(',')\n            try:\n                # Metrics\n                results[line[0]] = float(line[1])\n            except ValueError:\n                # Would not convert to float\n                pass\n            except IndexError:\n                # Not a parable line (did not contain any commas)\n                continue\n            try:\n                # Event counters\n                if line[2] == '-' or line[2] == 'nan':\n                    counter_value = 0\n                else:\n                    counter_value = int(line[2])\n                if re.fullmatch(r'[A-Z0-9_]+', line[0]) and re.fullmatch(r'[A-Z0-9]+', line[1]):\n                    results.setdefault(line[0], {})\n                    results[line[0]][line[1]] = counter_value\n            except (IndexError, ValueError):\n                pass\n\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns analysis of the current state of the system.", "response": "def analyze(self):\n        \"\"\"Run analysis.\"\"\"\n        bench = self.kernel.build_executable(verbose=self.verbose > 1, openmp=self._args.cores > 1)\n        element_size = self.kernel.datatypes_size[self.kernel.datatype]\n\n        # Build arguments to pass to command:\n        args = [str(s) for s in list(self.kernel.constants.values())]\n\n        # Determine base runtime with 10 iterations\n        runtime = 0.0\n        time_per_repetition = 2.0 / 10.0\n        repetitions = self.iterations // 10\n        mem_results = {}\n\n        # TODO if cores > 1, results are for openmp run. Things might need to be changed here!\n\n        while runtime < 1.5:\n            # Interpolate to a 2.0s run\n            if time_per_repetition != 0.0:\n                repetitions = 2.0 // time_per_repetition\n            else:\n                repetitions = int(repetitions * 10)\n\n            mem_results = self.perfctr([bench] + [str(repetitions)] + args, group=\"MEM\")\n            runtime = mem_results['Runtime (RDTSC) [s]']\n            time_per_repetition = runtime / float(repetitions)\n        raw_results = [mem_results]\n\n        # Base metrics for further metric computations:\n        # An iteration is equal to one high-level code inner-most-loop iteration\n        iterations_per_repetition = reduce(\n            operator.mul,\n            [self.kernel.subs_consts(max_ - min_) / self.kernel.subs_consts(step)\n             for idx, min_, max_, step in self.kernel._loop_stack],\n            1)\n        iterations_per_cacheline = (float(self.machine['cacheline size']) /\n                                    self.kernel.bytes_per_iteration)\n        cys_per_repetition = time_per_repetition * float(self.machine['clock'])\n\n        # Gather remaining counters\n        if not self.no_phenoecm:\n            # Build events and sympy expressions for all model metrics\n            T_OL, event_counters = self.machine.parse_perfmetric(\n                self.machine['overlapping model']['performance counter metric'])\n            T_data, event_dict = self.machine.parse_perfmetric(\n                self.machine['non-overlapping model']['performance counter metric'])\n            event_counters.update(event_dict)\n            cache_metrics = defaultdict(dict)\n            for i in range(len(self.machine['memory hierarchy']) - 1):\n                cache_info = self.machine['memory hierarchy'][i]\n                name = cache_info['level']\n                for k, v in cache_info['performance counter metrics'].items():\n                    cache_metrics[name][k], event_dict = self.machine.parse_perfmetric(v)\n                    event_counters.update(event_dict)\n\n            # Compile minimal runs to gather all required events\n            minimal_runs = build_minimal_runs(list(event_counters.values()))\n            measured_ctrs = {}\n            for run in minimal_runs:\n                ctrs = ','.join([eventstr(e) for e in run])\n                r = self.perfctr([bench] + [str(repetitions)] + args, group=ctrs)\n                raw_results.append(r)\n                measured_ctrs.update(r)\n            # Match measured counters to symbols\n            event_counter_results = {}\n            for sym, ctr in event_counters.items():\n                event, regs, parameter = ctr[0], register_options(ctr[1]), ctr[2]\n                for r in regs:\n                    if r in measured_ctrs[event]:\n                        event_counter_results[sym] = measured_ctrs[event][r]\n\n            # Analytical metrics needed for futher calculation\n            cl_size = float(self.machine['cacheline size'])\n            total_iterations = iterations_per_repetition * repetitions\n            total_cachelines = total_iterations / iterations_per_cacheline\n\n            T_OL_result = T_OL.subs(event_counter_results) / total_cachelines\n            cache_metric_results = defaultdict(dict)\n            for cache, mtrcs in cache_metrics.items():\n                for m, e in mtrcs.items():\n                    cache_metric_results[cache][m] = e.subs(event_counter_results)\n\n            # Inter-cache transfers per CL\n            cache_transfers_per_cl = {cache: {k: PrefixedUnit(v / total_cachelines, 'CL/CL')\n                                              for k, v in d.items()}\n                                      for cache, d in cache_metric_results.items()}\n            cache_transfers_per_cl['L1']['accesses'].unit = 'LOAD/CL'\n\n            # Select appropriate bandwidth\n            mem_bw, mem_bw_kernel = self.machine.get_bandwidth(\n                -1,  # mem\n                cache_metric_results['L3']['misses'],  # load_streams\n                cache_metric_results['L3']['evicts'],  # store_streams\n                1)\n\n            data_transfers = {\n                # Assuming 0.5 cy / LOAD (SSE on SNB or IVB; AVX on HSW, BDW, SKL or SKX)\n                'T_nOL': (cache_metric_results['L1']['accesses'] / total_cachelines * 0.5),\n                'T_L1L2': ((cache_metric_results['L1']['misses'] +\n                            cache_metric_results['L1']['evicts']) /\n                           total_cachelines * cl_size /\n                           self.machine['memory hierarchy'][1]['non-overlap upstream throughput'][0]),\n                'T_L2L3': ((cache_metric_results['L2']['misses'] +\n                            cache_metric_results['L2']['evicts']) /\n                           total_cachelines * cl_size /\n                           self.machine['memory hierarchy'][2]['non-overlap upstream throughput'][0]),\n                'T_L3MEM': ((cache_metric_results['L3']['misses'] +\n                             cache_metric_results['L3']['evicts']) *\n                            float(self.machine['cacheline size']) /\n                            total_cachelines / mem_bw *\n                            float(self.machine['clock']))\n            }\n\n            # Build phenomenological ECM model:\n            ecm_model = {'T_OL': T_OL_result}\n            ecm_model.update(data_transfers)\n        else:\n            event_counters = {}\n            ecm_model = None\n            cache_transfers_per_cl = None\n\n        self.results = {'raw output': raw_results, 'ECM': ecm_model,\n                        'data transfers': cache_transfers_per_cl,\n                        'Runtime (per repetition) [s]': time_per_repetition,\n                        'event counters': event_counters,\n                        'Iterations per repetition': iterations_per_repetition,\n                        'Iterations per cacheline': iterations_per_cacheline}\n\n        # TODO make more generic to support other (and multiple) constant names\n        self.results['Runtime (per cacheline update) [cy/CL]'] = \\\n            (cys_per_repetition / iterations_per_repetition) * iterations_per_cacheline\n        self.results['MEM volume (per repetition) [B]'] = \\\n            mem_results['Memory data volume [GBytes]'] * 1e9 / repetitions\n        self.results['Performance [MFLOP/s]'] = \\\n            sum(self.kernel._flops.values()) / (\n            time_per_repetition / iterations_per_repetition) / 1e6\n        if 'Memory bandwidth [MBytes/s]' in mem_results:\n            self.results['MEM BW [MByte/s]'] = mem_results['Memory bandwidth [MBytes/s]']\n        else:\n            self.results['MEM BW [MByte/s]'] = mem_results['Memory BW [MBytes/s]']\n        self.results['Performance [MLUP/s]'] = (\n                                               iterations_per_repetition / time_per_repetition) / 1e6\n        self.results['Performance [MIt/s]'] = (\n                                              iterations_per_repetition / time_per_repetition) / 1e6"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreporting gathered analysis data in human readable form.", "response": "def report(self, output_file=sys.stdout):\n        \"\"\"Report gathered analysis data in human readable form.\"\"\"\n        if self.verbose > 1:\n            with pprint_nosort():\n                pprint.pprint(self.results)\n\n        if self.verbose > 0:\n            print('Runtime (per repetition): {:.2g} s'.format(\n                self.results['Runtime (per repetition) [s]']),\n                file=output_file)\n        if self.verbose > 0:\n            print('Iterations per repetition: {!s}'.format(\n                self.results['Iterations per repetition']),\n                file=output_file)\n        print('Runtime (per cacheline update): {:.2f} cy/CL'.format(\n            self.results['Runtime (per cacheline update) [cy/CL]']),\n            file=output_file)\n        print('MEM volume (per repetition): {:.0f} Byte'.format(\n            self.results['MEM volume (per repetition) [B]']),\n            file=output_file)\n        print('Performance: {:.2f} MFLOP/s'.format(self.results['Performance [MFLOP/s]']),\n              file=output_file)\n        print('Performance: {:.2f} MLUP/s'.format(self.results['Performance [MLUP/s]']),\n              file=output_file)\n        print('Performance: {:.2f} It/s'.format(self.results['Performance [MIt/s]']),\n              file=output_file)\n        if self.verbose > 0:\n            print('MEM bandwidth: {:.2f} MByte/s'.format(self.results['MEM BW [MByte/s]']),\n                  file=output_file)\n        print('', file=output_file)\n\n        if not self.no_phenoecm:\n            print(\"Data Transfers:\")\n            print(\"{:^8} |\".format(\"cache\"), end='')\n            for metrics in self.results['data transfers'].values():\n                for metric_name in sorted(metrics):\n                    print(\" {:^14}\".format(metric_name), end='')\n                print()\n                break\n            for cache, metrics in sorted(self.results['data transfers'].items()):\n                print(\"{!s:^8} |\".format(cache), end='')\n                for k, v in sorted(metrics.items()):\n                    print(\" {!s:^14}\".format(v), end='')\n                print()\n            print()\n\n            print('Phenomenological ECM model: {{ {T_OL:.1f} || {T_nOL:.1f} | {T_L1L2:.1f} | '\n                  '{T_L2L3:.1f} | {T_L3MEM:.1f} }} cy/CL'.format(\n                **{k: float(v) for k, v in self.results['ECM'].items()}),\n                file=output_file)\n            print('T_OL assumes that two loads per cycle may be retiered, which is true for '\n                  '128bit SSE/half-AVX loads on SNB and IVY, and 256bit full-AVX loads on HSW, '\n                  'BDW, SKL and SKX, but it also depends on AGU availability.',\n                  file=output_file)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_description():\n    from os.path import dirname, join, exists\n    readme_fpath = join(dirname(__file__), 'README.md')\n    # print('readme_fpath = %r' % (readme_fpath,))\n    # This breaks on pip install, so check that it exists.\n    if exists(readme_fpath):\n        # try:\n        #     # convert markdown to rst for pypi\n        #     import pypandoc\n        #     return pypandoc.convert(readme_fpath, 'rst')\n        # except Exception as ex:\n            # strip out markdown to make a clean readme for pypi\n            textlines = []\n            with open(readme_fpath, 'r') as f:\n                capture = False\n                for line in f.readlines():\n                    if '# Purpose' in line:\n                        capture = True\n                    elif line.startswith('##'):\n                        break\n                    elif capture:\n                        textlines += [line]\n            text = ''.join(textlines).strip()\n            text = text.replace('\\n\\n', '_NLHACK_')\n            text = text.replace('\\n', ' ')\n            text = text.replace('_NLHACK_', '\\n\\n')\n            return text\n    return ''", "response": "Parse the description in the README file and return the text."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef schedule_retry(self, config):\n    raise self.retry(countdown=config.get('SAILTHRU_RETRY_SECONDS'),\n                     max_retries=config.get('SAILTHRU_RETRY_ATTEMPTS'))", "response": "Schedule a retry of the current element in the sequence"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds and return Sailthru purchase item object", "response": "def _build_purchase_item(course_id, course_url, cost_in_cents, mode, course_data, sku):\n    \"\"\"Build and return Sailthru purchase item object\"\"\"\n\n    # build item description\n    item = {\n        'id': \"{}-{}\".format(course_id, mode),\n        'url': course_url,\n        'price': cost_in_cents,\n        'qty': 1,\n    }\n\n    # get title from course info if we don't already have it from Sailthru\n    if 'title' in course_data:\n        item['title'] = course_data['title']\n    else:\n        # can't find, just invent title\n        item['title'] = 'Course {} mode: {}'.format(course_id, mode)\n\n    if 'tags' in course_data:\n        item['tags'] = course_data['tags']\n\n    # add vars to item\n    item['vars'] = dict(course_data.get('vars', {}), mode=mode, course_run_id=course_id)\n\n    item['vars']['purchase_sku'] = sku\n\n    return item"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrecord a purchase in Sailthru", "response": "def _record_purchase(sailthru_client, email, item, purchase_incomplete, message_id, options):\n    \"\"\"Record a purchase in Sailthru\n\n    Arguments:\n        sailthru_client (object): SailthruClient\n        email (str): user's email address\n        item (dict): Sailthru required information about the course\n        purchase_incomplete (boolean): True if adding item to shopping cart\n        message_id (str): Cookie used to identify marketing campaign\n        options (dict): Sailthru purchase API options (e.g. template name)\n\n    Returns:\n        False if retryable error, else True\n    \"\"\"\n    try:\n        sailthru_response = sailthru_client.purchase(email, [item],\n                                                     incomplete=purchase_incomplete, message_id=message_id,\n                                                     options=options)\n\n        if not sailthru_response.is_ok():\n            error = sailthru_response.get_error()\n            logger.error(\"Error attempting to record purchase in Sailthru: %s\", error.get_message())\n            return not can_retry_sailthru_request(error)\n\n    except SailthruClientError as exc:\n        logger.exception(\"Exception attempting to record purchase for %s in Sailthru - %s\", email, text_type(exc))\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_course_content(course_id, course_url, sailthru_client, site_code, config):\n    # check cache first\n    cache_key = \"{}:{}\".format(site_code, course_url)\n    response = cache.get(cache_key)\n    if not response:\n        try:\n            sailthru_response = sailthru_client.api_get(\"content\", {\"id\": course_url})\n            if not sailthru_response.is_ok():\n                response = {}\n            else:\n                response = sailthru_response.json\n                cache.set(cache_key, response, config.get('SAILTHRU_CACHE_TTL_SECONDS'))\n\n        except SailthruClientError:\n            response = {}\n\n        if not response:\n            logger.error('Could not get course data from Sailthru on enroll/purchase event. '\n                         'Calling Ecommerce Course API to get course info for enrollment confirmation email')\n            response = _get_course_content_from_ecommerce(course_id, site_code=site_code)\n            if response:\n                cache.set(cache_key, response, config.get('SAILTHRU_CACHE_TTL_SECONDS'))\n\n    return response", "response": "Get course information using the Sailthru content api or from cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_course_content_from_ecommerce(course_id, site_code=None):\n    api = get_ecommerce_client(site_code=site_code)\n    try:\n        api_response = api.courses(course_id).get()\n    except Exception:  # pylint: disable=broad-except\n        logger.exception(\n            'An error occurred while retrieving data for course run [%s] from the Catalog API.',\n            course_id,\n            exc_info=True\n        )\n        return {}\n\n    return {\n        'title': api_response.get('name'),\n        'verification_deadline': api_response.get('verification_deadline')\n    }", "response": "Get course information from the Ecommerce API."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _update_unenrolled_list(sailthru_client, email, course_url, unenroll):\n    try:\n        # get the user 'vars' values from sailthru\n        sailthru_response = sailthru_client.api_get(\"user\", {\"id\": email, \"fields\": {\"vars\": 1}})\n        if not sailthru_response.is_ok():\n            error = sailthru_response.get_error()\n            logger.error(\"Error attempting to read user record from Sailthru: %s\", error.get_message())\n            return not can_retry_sailthru_request(error)\n\n        response_json = sailthru_response.json\n\n        unenroll_list = []\n        if response_json and \"vars\" in response_json and response_json[\"vars\"] \\\n           and \"unenrolled\" in response_json[\"vars\"]:\n            unenroll_list = response_json[\"vars\"][\"unenrolled\"]\n\n        changed = False\n        # if unenrolling, add course to unenroll list\n        if unenroll:\n            if course_url not in unenroll_list:\n                unenroll_list.append(course_url)\n                changed = True\n\n        # if enrolling, remove course from unenroll list\n        elif course_url in unenroll_list:\n            unenroll_list.remove(course_url)\n            changed = True\n\n        if changed:\n            # write user record back\n            sailthru_response = sailthru_client.api_post(\n                'user', {'id': email, 'key': 'email', 'vars': {'unenrolled': unenroll_list}})\n\n            if not sailthru_response.is_ok():\n                error = sailthru_response.get_error()\n                logger.error(\"Error attempting to update user record in Sailthru: %s\", error.get_message())\n                return not can_retry_sailthru_request(error)\n\n        return True\n\n    except SailthruClientError as exc:\n        logger.exception(\"Exception attempting to update user record for %s in Sailthru - %s\", email, text_type(exc))\n        return False", "response": "Maintain a list of courses the user has unenrolled from in the Sailthru user record\n 69"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd or updates a course enrollment event to Sailthru", "response": "def update_course_enrollment(self, email, course_url, purchase_incomplete, mode, unit_cost=None, course_id=None,\n                             currency=None, message_id=None, site_code=None, sku=None):\n    \"\"\"Adds/updates Sailthru when a user adds to cart/purchases/upgrades a course\n\n     Args:\n        email(str): The user's email address\n        course_url(str): Course home page url\n        purchase_incomplete(boolean): True if adding to cart\n        mode(string): enroll mode (audit, verification, ...)\n        unit_cost(decimal): cost if purchase event\n        course_id(CourseKey): course id\n        currency(str): currency if purchase event - currently ignored since Sailthru only supports USD\n        message_id(str): value from Sailthru marketing campaign cookie\n        site_code(str): site code\n\n    Returns:\n        None\n    \"\"\"\n    # Get configuration\n    config = get_sailthru_configuration(site_code)\n\n    try:\n        sailthru_client = get_sailthru_client(site_code)\n    except SailthruError:\n        # NOTE: We rely on the function to log the error for us\n        return\n\n    # Use event type to figure out processing required\n    new_enroll = False\n    send_template = None\n\n    if not purchase_incomplete:\n        if mode == 'verified':\n            # upgrade complete\n            send_template = config.get('SAILTHRU_UPGRADE_TEMPLATE')\n        elif mode == 'audit' or mode == 'honor':\n            # free enroll\n            new_enroll = True\n            send_template = config.get('SAILTHRU_ENROLL_TEMPLATE')\n        else:\n            # paid course purchase complete\n            new_enroll = True\n            send_template = config.get('SAILTHRU_PURCHASE_TEMPLATE')\n\n    # calc price in pennies for Sailthru\n    #  https://getstarted.sailthru.com/new-for-developers-overview/advanced-features/purchase/\n    cost_in_cents = int(unit_cost * 100)\n\n    # update the \"unenrolled\" course array in the user record on Sailthru if new enroll or unenroll\n    if new_enroll:\n        if not _update_unenrolled_list(sailthru_client, email, course_url, False):\n            schedule_retry(self, config)\n\n    # Get course data from Sailthru content library or cache\n    course_data = _get_course_content(course_id, course_url, sailthru_client, site_code, config)\n\n    # build item description\n    item = _build_purchase_item(course_id, course_url, cost_in_cents, mode, course_data, sku)\n\n    # build purchase api options list\n    options = {}\n    if purchase_incomplete and config.get('SAILTHRU_ABANDONED_CART_TEMPLATE'):\n        options['reminder_template'] = config.get('SAILTHRU_ABANDONED_CART_TEMPLATE')\n        # Sailthru reminder time format is '+n time unit'\n        options['reminder_time'] = \"+{} minutes\".format(config.get('SAILTHRU_ABANDONED_CART_DELAY'))\n\n    # add appropriate send template\n    if send_template:\n        options['send_template'] = send_template\n\n    if not _record_purchase(sailthru_client, email, item, purchase_incomplete, message_id, options):\n        schedule_retry(self, config)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef send_course_refund_email(self, email, refund_id, amount, course_name, order_number, order_url, site_code=None):\n    config = get_sailthru_configuration(site_code)\n\n    try:\n        sailthru_client = get_sailthru_client(site_code)\n    except SailthruError:\n        # NOTE: We rely on the function to log the error for us\n        return\n\n    email_vars = {\n        'amount': amount,\n        'course_name': course_name,\n        'order_number': order_number,\n        'order_url': order_url,\n    }\n\n    try:\n        response = sailthru_client.send(\n            template=config['templates']['course_refund'],\n            email=email,\n            _vars=email_vars\n        )\n    except SailthruClientError:\n        logger.exception(\n            'A client error occurred while attempting to send a course refund notification for refund [%d].',\n            refund_id\n        )\n        return\n\n    if response.is_ok():\n        logger.info('Course refund notification sent for refund %d.', refund_id)\n    else:\n        error = response.get_error()\n        logger.error(\n            'An error occurred while attempting to send a course refund notification for refund [%d]: %d - %s',\n            refund_id, error.get_error_code(), error.get_message()\n        )\n\n        if can_retry_sailthru_request(error):\n            logger.info(\n                'An attempt will be made again to send a course refund notification for refund [%d].',\n                refund_id\n            )\n            schedule_retry(self, config)\n        else:\n            logger.warning(\n                'No further attempts will be made to send a course refund notification for refund [%d].',\n                refund_id\n            )", "response": "Sends the email to the refund email."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending an email to the user who has the entry in the offer assignment model.", "response": "def send_offer_assignment_email(self, user_email, offer_assignment_id, subject, email_body, site_code=None):\n    \"\"\" Sends the offer assignment email.\n    Args:\n        self: Ignore.\n        user_email (str): Recipient's email address.\n        offer_assignment_id (str): Key of the entry in the offer_assignment model.\n        subject (str): Email subject.\n        email_body (str): The body of the email.\n        site_code (str): Identifier of the site sending the email.\n    \"\"\"\n    config = get_sailthru_configuration(site_code)\n    response = _send_offer_assignment_notification_email(config, user_email, subject, email_body, site_code, self)\n    if response and response.is_ok():\n        send_id = response.get_body().get('send_id')  # pylint: disable=no-member\n        if _update_assignment_email_status(offer_assignment_id, send_id, 'success'):\n            logger.info('[Offer Assignment] Offer assignment notification sent with message --- {message}'.format(\n                message=email_body))\n        else:\n            logger.exception(\n                '[Offer Assignment] An error occurred while updating email status data for '\n                'offer {token_offer} and email {token_email} via the ecommerce API.'.format(\n                    token_offer=offer_assignment_id,\n                    token_email=user_email,\n                )\n            )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend an email to the user s offer assignment notification emails.", "response": "def _send_offer_assignment_notification_email(config, user_email, subject, email_body, site_code, task):\n    \"\"\"Handles sending offer assignment notification emails and retrying failed emails when appropriate.\"\"\"\n    try:\n        sailthru_client = get_sailthru_client(site_code)\n    except SailthruError:\n        logger.exception(\n            '[Offer Assignment] A client error occurred while attempting to send a offer assignment notification.'\n            ' Message: {message}'.format(message=email_body)\n        )\n        return None\n    email_vars = {\n        'subject': subject,\n        'email_body': email_body,\n    }\n    try:\n        response = sailthru_client.send(\n            template=config['templates']['assignment_email'],\n            email=user_email,\n            _vars=email_vars\n        )\n    except SailthruClientError:\n        logger.exception(\n            '[Offer Assignment] A client error occurred while attempting to send a offer assignment notification.'\n            ' Message: {message}'.format(message=email_body)\n        )\n        return None\n\n    if not response.is_ok():\n        error = response.get_error()\n        logger.error(\n            '[Offer Assignment] A {token_error_code} - {token_error_message} error occurred'\n            ' while attempting to send a offer assignment notification.'\n            ' Message: {message}'.format(\n                message=email_body,\n                token_error_code=error.get_error_code(),\n                token_error_message=error.get_message()\n            )\n        )\n        if can_retry_sailthru_request(error):\n            logger.info(\n                '[Offer Assignment] An attempt will be made to resend the offer assignment notification.'\n                ' Message: {message}'.format(message=email_body)\n            )\n            schedule_retry(task, config)\n        else:\n            logger.warning(\n                '[Offer Assignment] No further attempts will be made to send the offer assignment notification.'\n                ' Failed Message: {message}'.format(message=email_body)\n            )\n\n    return response"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _update_assignment_email_status(offer_assignment_id, send_id, status, site_code=None):\n    api = get_ecommerce_client(url_postfix='assignment-email/', site_code=site_code)\n    post_data = {\n        'offer_assignment_id': offer_assignment_id,\n        'send_id': send_id,\n        'status': status,\n    }\n    try:\n        api_response = api.status().post(post_data)\n    except RequestException:\n        logger.exception(\n            '[Offer Assignment] An error occurred while updating offer assignment email status for '\n            'offer id {token_offer} and message id {token_send_id} via the Ecommerce API.'.format(\n                token_offer=offer_assignment_id,\n                token_send_id=send_id\n            )\n        )\n        return False\n    return True if api_response.get('status') == 'updated' else False", "response": "Update the status of an offer assignment email model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending the email after an offer is assigned to the current site.", "response": "def send_offer_update_email(self, user_email, subject, email_body, site_code=None):\n    \"\"\" Sends the offer emails after assignment, either for revoking or reminding.\n    Args:\n        self: Ignore.\n        user_email (str): Recipient's email address.\n        subject (str): Email subject.\n        email_body (str): The body of the email.\n        site_code (str): Identifier of the site sending the email.\n    \"\"\"\n    config = get_sailthru_configuration(site_code)\n    _send_offer_assignment_notification_email(config, user_email, subject, email_body, site_code, self)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_logger_config(log_dir='/var/tmp',\n                      logging_env='no_env',\n                      edx_filename='edx.log',\n                      dev_env=False,\n                      debug=False,\n                      local_loglevel='INFO',\n                      service_variant='ecomworker'):\n\n    \"\"\"\n    Returns a dictionary containing logging configuration.\n\n    If dev_env is True, logging will not be done via local rsyslogd.\n    Instead, application logs will be dropped into log_dir. 'edx_filename'\n    is ignored unless dev_env is True.\n    \"\"\"\n    # Revert to INFO if an invalid string is passed in\n    if local_loglevel not in ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL']:\n        local_loglevel = 'INFO'\n\n    hostname = platform.node().split('.')[0]\n    syslog_format = (\n        '[service_variant={service_variant}]'\n        '[%(name)s][env:{logging_env}] %(levelname)s '\n        '[{hostname}  %(process)d] [%(filename)s:%(lineno)d] '\n        '- %(message)s'\n    ).format(\n        service_variant=service_variant,\n        logging_env=logging_env, hostname=hostname\n    )\n\n    if debug:\n        handlers = ['console']\n    else:\n        handlers = ['local']\n\n    logger_config = {\n        'version': 1,\n        'disable_existing_loggers': False,\n        'formatters': {\n            'standard': {\n                'format': '%(asctime)s %(levelname)s %(process)d '\n                          '[%(name)s] %(filename)s:%(lineno)d - %(message)s',\n            },\n            'syslog_format': {'format': syslog_format},\n            'raw': {'format': '%(message)s'},\n        },\n        'handlers': {\n            'console': {\n                'level': 'DEBUG' if debug else 'INFO',\n                'class': 'logging.StreamHandler',\n                'formatter': 'standard',\n                'stream': sys.stdout,\n            },\n        },\n        'loggers': {\n            'requests': {\n                'handlers': handlers,\n                'level': 'WARNING',\n                'propagate': True\n            },\n            '': {\n                'handlers': handlers,\n                'level': 'DEBUG',\n                'propagate': False\n            },\n        }\n    }\n\n    if dev_env:\n        edx_file_loc = os.path.join(log_dir, edx_filename)\n        logger_config['handlers'].update({\n            'local': {\n                'class': 'logging.handlers.RotatingFileHandler',\n                'level': local_loglevel,\n                'formatter': 'standard',\n                'filename': edx_file_loc,\n                'maxBytes': 1024 * 1024 * 2,\n                'backupCount': 5,\n            },\n        })\n    else:\n        logger_config['handlers'].update({\n            'local': {\n                'level': local_loglevel,\n                'class': 'logging.handlers.SysLogHandler',\n                # Use a different address for Mac OS X\n                'address': '/var/run/syslog' if sys.platform == 'darwin' else '/dev/log',\n                'formatter': 'syslog_format',\n                'facility': SysLogHandler.LOG_LOCAL0,\n            },\n        })\n\n    return logger_config", "response": "Returns a dictionary containing logging configuration."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _retry_order(self, exception, max_fulfillment_retries, order_number):\n    retries = self.request.retries\n    if retries == max_fulfillment_retries:\n        logger.exception('Fulfillment of order [%s] failed. Giving up.', order_number)\n    else:\n        logger.warning('Fulfillment of order [%s] failed. Retrying.', order_number)\n\n    countdown = 2 ** retries\n    raise self.retry(exc=exception, countdown=countdown, max_retries=max_fulfillment_retries)", "response": "Retry the order with exponential backoff until the fulfillment\n    succeeds or the retry limit is reached."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfulfills an order. Arguments: order_number (str): Order number indicating which order to fulfill. Returns: None", "response": "def fulfill_order(self, order_number, site_code=None, email_opt_in=False):\n    \"\"\"Fulfills an order.\n\n    Arguments:\n        order_number (str): Order number indicating which order to fulfill.\n\n    Returns:\n        None\n    \"\"\"\n    max_fulfillment_retries = get_configuration('MAX_FULFILLMENT_RETRIES', site_code=site_code)\n    api = get_ecommerce_client(site_code=site_code)\n    try:\n        logger.info('Requesting fulfillment of order [%s].', order_number)\n        api.orders(order_number).fulfill.put(email_opt_in=email_opt_in)\n    except exceptions.HttpClientError as exc:\n        status_code = exc.response.status_code  # pylint: disable=no-member\n        if status_code == 406:\n            # The order is not fulfillable. Therefore, it must be complete.\n            logger.info('Order [%s] has already been fulfilled. Ignoring.', order_number)\n            raise Ignore()\n        else:\n            # Unknown client error. Let's retry to resolve it.\n            logger.warning(\n                'Fulfillment of order [%s] failed because of HttpClientError. Retrying',\n                order_number,\n                exc_info=True\n            )\n            _retry_order(self, exc, max_fulfillment_retries, order_number)\n\n    except (exceptions.HttpServerError, exceptions.Timeout, SSLError) as exc:\n        # Fulfillment failed, retry\n        _retry_order(self, exc, max_fulfillment_retries, order_number)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a Sailthru client for the specified site.", "response": "def get_sailthru_client(site_code):\n    \"\"\"\n    Returns a Sailthru client for the specified site.\n\n    Args:\n        site_code (str): Site for which the client should be configured.\n\n    Returns:\n        SailthruClient\n\n    Raises:\n        SailthruNotEnabled: If Sailthru is not enabled for the specified site.\n        ConfigurationError: If either the Sailthru API key or secret are not set for the site.\n    \"\"\"\n    # Get configuration\n    config = get_sailthru_configuration(site_code)\n\n    # Return if Sailthru integration disabled\n    if not config.get('SAILTHRU_ENABLE'):\n        msg = 'Sailthru is not enabled for site {}'.format(site_code)\n        log.debug(msg)\n        raise SailthruNotEnabled(msg)\n\n    # Make sure key and secret configured\n    key = config.get('SAILTHRU_KEY')\n    secret = config.get('SAILTHRU_SECRET')\n\n    if not (key and secret):\n        msg = 'Both key and secret are required for site {}'.format(site_code)\n        log.error(msg)\n        raise ConfigurationError(msg)\n\n    return SailthruClient(key, secret)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get(self, key):\n        lock.acquire()\n        try:\n            if key not in self:\n                return None\n\n            current_time = time.time()\n            if self[key].expire > current_time:\n                return self[key].value\n\n            # expired key, clean out all expired keys\n            deletes = []\n            for k, val in self.items():\n                if val.expire <= current_time:\n                    deletes.append(k)\n            for k in deletes:\n                del self[k]\n\n            return None\n        finally:\n            lock.release()", "response": "Get an object from the cache with the given key."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set(self, key, value, duration):\n        lock.acquire()\n        try:\n            self[key] = CacheObject(value, duration)\n        finally:\n            lock.release()", "response": "Save an object in the cache"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a value from the configuration module.", "response": "def get_configuration(variable, site_code=None):\n    \"\"\"\n    Get a value from configuration.\n\n    Retrieves the value corresponding to the given variable from the configuration module\n    currently in use by the app.  Specify a site_code value to check for a site-specific override.\n\n    Arguments:\n        variable (str): The name of a variable from the configuration module.\n\n    Keyword Arguments:\n        site_code (str): The SITE_OVERRIDES key to inspect for site-specific values\n\n    Returns:\n        The value corresponding to the variable, or None if the variable is not found.\n    \"\"\"\n    name = os.environ.get(CONFIGURATION_MODULE)\n\n    # __import__ performs a full import, but only returns the top-level\n    # package, not the targeted module. sys.modules is a dictionary\n    # mapping module names to loaded modules.\n    __import__(name)\n    module = sys.modules[name]\n\n    # Locate the setting in the specified module, then attempt to apply a site-specific override\n    setting_value = getattr(module, variable, None)\n    site_overrides = getattr(module, 'SITE_OVERRIDES', None)\n    if site_overrides and site_code is not None:\n        site_specific_overrides = site_overrides.get(site_code)\n        if site_specific_overrides:\n            override_value = site_specific_overrides.get(variable)\n            if override_value:\n                setting_value = override_value\n\n    if setting_value is None:\n        raise RuntimeError('Worker is improperly configured: {} is unset in {}.'.format(variable, module))\n    return setting_value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_ecommerce_client(url_postfix='', site_code=None):\n    ecommerce_api_root = get_configuration('ECOMMERCE_API_ROOT', site_code=site_code)\n    signing_key = get_configuration('JWT_SECRET_KEY', site_code=site_code)\n    issuer = get_configuration('JWT_ISSUER', site_code=site_code)\n    service_username = get_configuration('ECOMMERCE_SERVICE_USERNAME', site_code=site_code)\n    return EdxRestApiClient(\n        ecommerce_api_root + url_postfix, signing_key=signing_key, issuer=issuer, username=service_username)", "response": "Get client for fetching data from ecommerce API."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_overrides_filename(variable):\n    filename = os.environ.get(variable)\n\n    if filename is None:\n        msg = 'Please set the {} environment variable.'.format(variable)\n        raise EnvironmentError(msg)\n\n    return filename", "response": "Get the name of the file containing configuration overrides from the provided environment variable."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the output files layout for the specified output category", "response": "def get_output_files_layout(output_category):\n    \"\"\"\n    Parameters\n    ----------\n    output_category: str\n        inputs: epw, idf\n        table: summary table\n        other: other\n    \"\"\"\n    # check  category\n    if output_category not in (\"inputs\", \"table\", \"other\"):\n        raise RuntimeError(f\"unknown {output_category}\")\n\n    # get version dict\n    layouts = _layouts_matrix[OS_NAME][output_category]\n\n    # get version\n    return get_value_by_version(layouts)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds the value depending in current eplus version.", "response": "def get_value_by_version(d):\n    \"\"\"\n    Finds the value depending in current eplus version.\n\n\n    Parameters\n    ----------\n    d: dict\n        {(0, 0): value, (x, x): value, ...}\n        for current version (cv), current value is the value of version v such as v <= cv < v+1\n    \"\"\"\n    from oplus import CONF  # touchy import\n\n    cv = CONF.eplus_version[:2]\n    for v, value in sorted(d.items(), reverse=True):\n        if cv >= v:\n            return value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nswitch the DataFrame to datetime instants.", "response": "def switch_to_datetime_instants(df, start_year, eplus_frequency):\n    \"\"\"\n    works inplace\n    \"\"\"\n    # timestep -> monthly\n    if eplus_frequency in (TIMESTEP, DAILY, HOURLY, MONTHLY):\n        # prepare year switch\n        if eplus_frequency in (TIMESTEP, HOURLY, DAILY):\n            # print((df[[\"month\", \"day\"]] - df[[\"month\", \"day\"]].shift()) == pd.Series([-12, -31]))\n            year_counter = (\n                    (df[[\"month\", \"day\"]] - df[[\"month\", \"day\"]].shift()) ==\n                    pd.Series(dict(month=12, day=-31))\n            ).all(axis=1).cumsum()\n        else:\n            year_counter = ((df[\"month\"] - df[\"month\"].shift()) == -12).cumsum()\n\n        # add year columns\n        df[\"year\"] = year_counter + start_year\n\n        # create index\n        columns = {\n            TIMESTEP: (\"year\", \"month\", \"day\", \"hour\", \"minute\"),\n            HOURLY: (\"year\", \"month\", \"day\", \"hour\"),\n            DAILY: (\"year\", \"month\", \"day\"),\n            MONTHLY: (\"year\", \"month\")\n        }[eplus_frequency]\n        if eplus_frequency == MONTHLY:\n            df.index = df.apply(  # apply transforms ints to floats, we need to re-cast\n                lambda x: dt.datetime(*(tuple(int(x[k]) for k in columns) + (1,))),\n                axis=1\n            )\n        else:\n            df.index = df.apply(lambda x: dt.datetime(*(int(x[k]) for k in columns)), axis=1)\n\n        # drop old columns\n        df.drop(columns=list(columns), inplace=True)\n\n        # force frequency\n        if eplus_frequency == TIMESTEP:\n            # find freq\n            ts = df.index[1] - df.index[0]\n            # force\n            forced_df = df.asfreq(ts)\n        else:\n            forced_df = df.asfreq({\n                HOURLY: \"H\",\n                DAILY: \"D\",\n                MONTHLY: \"MS\"\n            }[eplus_frequency])\n\n        # if timestep, hourly or daily, check did not change (only those can suffer from leap year problems)\n        if eplus_frequency in (TIMESTEP, HOURLY, DAILY):\n            try:\n                assert_index_equal(df.index, forced_df.index)\n            except AssertionError:\n                raise ValueError(\n                    f\"Couldn't convert to datetime instants (frequency: {eplus_frequency}). Probable cause : \"\n                    f\"given start year ({start_year}) is incorrect and data can't match because of leap year issues.\"\n                ) from None\n\n        return forced_df\n\n    # annual\n    if eplus_frequency == ANNUAL:\n        # check first year\n        if df[\"year\"].iloc[0] != start_year:\n            raise ValueError(\n                f\"Given start year ({start_year}) differs from annual output data first year ({df['year'].iloc[0]}),\"\n                f\"can't switch to datetime instants.\")\n        df.index = df[\"year\"].map(lambda x: dt.datetime(x, 1, 1))\n        del df[\"year\"]\n\n        # force freq\n        df = df.asfreq(\"YS\")\n\n        return df\n\n    # run period\n    if eplus_frequency == RUN_PERIOD:\n        return df\n\n    raise AssertionError(\"should not be here\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the most recent eplus available version of the oplus package.", "response": "def eplus_version(self):\n        \"\"\"\n        if _eplus_version is defined => _eplus_version\n        else most recent eplus available version\n        \"\"\"\n        # check energy plus is installed\n        if len(self.eplus_available_versions) == 0:\n            raise RuntimeError(\"Energy plus is not install, can't use oplus package.\")\n\n        # see if version is defined\n        if self._eplus_version is not None:\n            return self._eplus_version\n\n        # return most recent version\n        return sorted(self.eplus_available_versions.keys(), reverse=True)[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks that the data is hourly datetime instants and if so sanitize the date and time variables.", "response": "def _check_and_sanitize_datetime_instants(df):\n    \"\"\"\n    Parameters\n    ----------\n    df\n\n    Returns\n    -------\n    sanitized df\n    \"\"\"\n    # leave if not relevant\n    if df is None or len(df) == 0:\n        return df\n\n    # check datetime index\n    if not isinstance(df.index, pd.DatetimeIndex):\n        raise ValueError(\"df index must be a datetime index.\")\n\n    # force frequency if needed\n    if df.index.freq != \"H\":\n        forced_df = df.asfreq(\"H\")\n        # check no change\n        try:\n            assert_index_equal(df.index, forced_df.index)\n        except AssertionError:\n            raise ValueError(\n                f\"Couldn't convert to hourly datetime instants. Probable cause : \"\n                f\"given start instant ({df.index[0]}) is incorrect and data can't match because of leap year issues.\"\n            ) from None\n        # replace old variable\n        df = forced_df\n\n    # check first minute is 0\n    if df.index[0].minute != 0:\n        raise ValueError(\"Minutes must be 0.\")\n\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn start and end of the calendar for the current weather set.", "response": "def get_bounds(self):\n        \"\"\"\n        Returns\n        -------\n        (start, end)\n\n        Datetime instants of beginning and end of data. If no data, will be: (None, None).\n        \"\"\"\n        start, end = None, None\n        if len(self._weather_series) == 0:\n            return start, end\n\n        for i in (0, -1):\n            # create or find instant\n            if self.has_tuple_instants:\n                row = self._weather_series.iloc[i, :]\n                instant = dt.datetime(row[\"year\"], row[\"month\"], row[\"day\"], row[\"hour\"], row[\"minute\"])\n            else:\n                instant = self._weather_series.index[i].to_pydatetime()\n\n            # store\n            if i == 0:\n                start = instant\n            else:\n                end = instant\n\n        return start, end"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a WeatherData instance from an epw file.", "response": "def from_epw(cls, buffer_or_path):\n        \"\"\"\n        Parameters\n        ----------\n        buffer_or_path: buffer or path containing epw format.\n\n        Returns\n        -------\n        WeatherData instance.\n        \"\"\"\n        from .epw_parse import parse_epw\n        _, buffer = to_buffer(buffer_or_path)\n        with buffer as f:\n            return parse_epw(f)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_epw(self, buffer_or_path=None):\n        # copy and change hours convention [0, 23] -> [1, 24]\n        df = self._weather_series.copy()\n        df[\"hour\"] += 1\n        epw_content = self._headers_to_epw() + df.to_csv(header=False, index=False, line_terminator=\"\\n\")\n        return multi_mode_write(\n            lambda buffer: buffer.write(epw_content),\n            lambda: epw_content,\n            buffer_or_path=buffer_or_path\n        )", "response": "Returns a string containing the epw info of the current state of the entry."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_idf(file_like):\n    tables_data = {}\n    head_comment = \"\"\n    record_data = None\n    make_new_record = True\n\n    copyright_list = get_multi_line_copyright_message().split(\"\\n\")\n\n    for i, raw_line in enumerate(file_like):\n        # manage if copyright\n        try:\n            copyright_line = copyright_list[i]\n            if raw_line.strip() == copyright_line:\n                # skip copyright line\n                continue\n        except IndexError:\n            pass\n\n        # GET LINE CONTENT AND COMMENT\n        split_line = raw_line.split(\"!\")\n\n        # no \"!\" in the raw_line\n        if len(split_line) == 1:\n            # this is an empty line\n            if len(split_line[0].strip()) == 0:\n                content, comment = None, None\n            # this is a record line with no comments\n            else:\n                content, comment = split_line[0].strip(), None\n        # there is at least one \"!\" in the raw_line\n        else:\n            # this is a comment line\n            if len(split_line[0].strip()) == 0:\n                content, comment = None, \"!\".join(split_line[1:])\n            # this is a record line with a comment\n            else:\n                content, comment = split_line[0].strip(), \"!\".join(split_line[1:])\n\n        # SKIP CURRENT LINE IF VOID\n        if (content, comment) == (None, None):\n            continue\n\n        # NO CONTENT\n        if not content:\n            if record_data is None:  # we only manage head idf comment\n                head_comment += comment.strip() + \"\\n\"\n            continue\n\n        # CONTENT\n        # check if record end and prepare\n        record_end = content[-1] == \";\"\n        content = content[:-1]  # we tear comma or semi-colon\n        content_l = [text.strip() for text in content.split(\",\")]\n\n        # record creation if needed\n        if make_new_record:\n            # get table ref\n            table_ref = table_name_to_ref(content_l[0].strip())\n\n            # skip if special table\n            if table_ref.lower() in (\n                    \"lead input\",\n                    \"end lead input\",\n                    \"simulation data\",\n                    \"end simulation data\"\n            ):\n                continue\n\n            # declare table if necessary\n            if table_ref not in tables_data:\n                tables_data[table_ref] = []\n\n            # create and store record\n            record_data = dict()\n            tables_data[table_ref].append(record_data)\n\n            # prepare in case fields on the same line\n            content_l = content_l[1:]\n            make_new_record = False\n\n        # fields\n        for value_s in content_l:\n            field_index = len(record_data)\n            record_data[field_index] = value_s\n\n        # signal that new record must be created\n        if record_end:\n            make_new_record = True\n\n    # add comment key\n    tables_data[\"_comment\"] = head_comment\n    return tables_data", "response": "Parse the idf file and return a dictionary of the idf and the record data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun EPlus on the given path.", "response": "def run_eplus(epm_or_idf_path, weather_data_or_epw_path, simulation_dir_path, stdout=None, stderr=None, beat_freq=None):\n    \"\"\"\n    Parameters\n    ----------\n    epm_or_idf_path:\n    weather_data_or_epw_path\n    simulation_dir_path\n    stdout: default sys.stdout\n    stderr: default sys.stderr\n    beat_freq: if not none, stdout will be used at least every beat_freq (in seconds)\n    \"\"\"\n    # work with absolute paths\n    simulation_dir_path = os.path.abspath(simulation_dir_path)\n\n    # check dir path\n    if not os.path.isdir(simulation_dir_path):\n        raise NotADirectoryError(\"Simulation directory does not exist: '%s'.\" % simulation_dir_path)\n\n    # epm\n    if not isinstance(epm_or_idf_path, Epm):\n        # we don't copy file directly because we want to manage it's external files\n        # could be optimized (use _copy_without_read_only)\n        epm = Epm.from_idf(epm_or_idf_path)\n    else:\n        epm = epm_or_idf_path\n\n    # create idf\n    simulation_idf_path = os.path.join(simulation_dir_path, CONF.default_model_name + \".idf\")\n    epm.to_idf(simulation_idf_path)\n\n    # weather data\n    simulation_epw_path = os.path.join(simulation_dir_path, CONF.default_model_name + \".epw\")\n    if isinstance(weather_data_or_epw_path, WeatherData):\n        weather_data_or_epw_path.to_epw(simulation_epw_path)\n    else:\n        # no need to load: we copy directly\n        _copy_without_read_only(weather_data_or_epw_path, simulation_epw_path)\n\n    # copy epw if needed (depends on os/eplus version)\n    temp_epw_path = get_simulated_epw_path()\n    if temp_epw_path is not None:\n        _copy_without_read_only(simulation_epw_path, temp_epw_path)\n\n    # prepare command\n    eplus_relative_cmd = get_simulation_base_command()\n    eplus_cmd = os.path.join(CONF.eplus_base_dir_path, eplus_relative_cmd)\n\n    # idf\n    idf_command_style = get_simulation_input_command_style(\"idf\")\n    if idf_command_style == SIMULATION_INPUT_COMMAND_STYLES.simu_dir:\n        idf_file_cmd = os.path.join(simulation_dir_path, CONF.default_model_name)\n    elif idf_command_style == SIMULATION_INPUT_COMMAND_STYLES.file_path:\n        idf_file_cmd = simulation_idf_path\n    else:\n        raise AssertionError(\"should not be here\")\n\n    # epw\n    epw_command_style = get_simulation_input_command_style(\"epw\")\n    if epw_command_style == SIMULATION_INPUT_COMMAND_STYLES.simu_dir:\n        epw_file_cmd = os.path.join(simulation_dir_path, CONF.default_model_name)\n    elif epw_command_style == SIMULATION_INPUT_COMMAND_STYLES.file_path:\n        epw_file_cmd = simulation_epw_path\n    else:\n        raise AssertionError(\"should not be here\")\n\n    # command list\n    simulation_command_style = get_simulation_command_style()\n    if simulation_command_style == SIMULATION_COMMAND_STYLES.args:\n        cmd_l = [eplus_cmd, idf_file_cmd, epw_file_cmd]\n    elif simulation_command_style == SIMULATION_COMMAND_STYLES.kwargs:\n        cmd_l = [eplus_cmd, \"-w\", epw_file_cmd, \"-r\", idf_file_cmd]\n    else:\n        raise RuntimeError(\"should not be here\")\n\n    # launch calculation\n    run_subprocess(\n        cmd_l,\n        cwd=simulation_dir_path,\n        stdout=stdout,\n        stderr=stderr,\n        beat_freq=beat_freq\n    )\n\n    # if needed, we delete temp weather data (only on Windows, see above)\n    if (temp_epw_path is not None) and os.path.isfile(temp_epw_path):\n        os.remove(os.path.join(temp_epw_path))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new simulation object for the given epm or path.", "response": "def simulate(\n            cls,\n            epm_or_path,\n            weather_data_or_path,\n            base_dir_path,\n            simulation_name=None,\n            stdout=None,\n            stderr=None,\n            beat_freq=None\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        epm_or_path\n        weather_data_or_path\n        base_dir_path: simulation dir path\n        simulation_name: str, default None\n            if provided, simulation will be done in {base_dir_path}/{simulation_name}\n            else, simulation will be done in {base_dir_path}\n        stdout: stream, default logger.info\n            stream where EnergyPlus standard output is redirected\n        stderr: stream, default logger.error\n            stream where EnergyPlus standard error is redirected\n        beat_freq: float, default None\n            if provided, subprocess in which EnergyPlus is run will write at given frequency in standard output. May\n            be used to monitor subprocess state.\n\n        Returns\n        -------\n        Simulation instance\n        \"\"\"\n        # manage simulation dir path\n        if not os.path.isdir(base_dir_path):\n            raise NotADirectoryError(\"Base dir path not found: '%s'\" % base_dir_path)\n        simulation_dir_path = base_dir_path if simulation_name is None else os.path.join(base_dir_path, simulation_name)\n\n        # make directory if does not exist\n        if not os.path.exists(simulation_dir_path):\n            os.mkdir(simulation_dir_path)\n\n        # run simulation\n        stdout = LoggerStreamWriter(logger_name=__name__, level=logging.INFO) if stdout is None else stdout\n        stderr = LoggerStreamWriter(logger_name=__name__, level=logging.ERROR) if stderr is None else stderr\n        run_eplus(\n            epm_or_path,\n            weather_data_or_path,\n            simulation_dir_path,\n            stdout=stdout,\n            stderr=stderr,\n            beat_freq=beat_freq\n        )\n\n        # return simulation object\n        return cls(\n            base_dir_path,\n            simulation_name=simulation_name\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _file_refs(self):\n        if self._prepared_file_refs is None:\n            self._prepared_file_refs = {\n                FILE_REFS.idf: FileInfo(\n                    constructor=lambda path: self._epm_cls.from_idf(path, idd_or_buffer_or_path=self._idd),\n                    get_path=lambda: get_input_file_path(self.dir_path, FILE_REFS.idf)\n                ),\n                FILE_REFS.epw: FileInfo(\n                    constructor=lambda path: self._weather_data_cls.from_epw(path),\n                    get_path=lambda: get_input_file_path(self.dir_path, FILE_REFS.epw)\n                ),\n                FILE_REFS.eio: FileInfo(\n                    constructor=lambda path: self._eio_cls(path),\n                    get_path=lambda: get_output_file_path(self.dir_path, FILE_REFS.eio)\n                ),\n                FILE_REFS.eso: FileInfo(\n                    constructor=lambda path: self._standard_output_cls(path),\n                    get_path=lambda: get_output_file_path(\n                        self.dir_path,\n                        FILE_REFS.eso\n                    )\n                ),\n                FILE_REFS.mtr: FileInfo(\n                    constructor=lambda path: self._standard_output_cls(path),\n                    get_path=lambda: get_output_file_path(self.dir_path, FILE_REFS.mtr)\n                ),\n                FILE_REFS.mtd: FileInfo(\n                    constructor=lambda path: self._mtd_cls(path),\n                    get_path=lambda: get_output_file_path(self.dir_path, FILE_REFS.mtd)\n                ),\n                FILE_REFS.mdd: FileInfo(\n                    constructor=lambda path: open(path).read(),\n                    get_path=lambda: get_output_file_path(self.dir_path, FILE_REFS.mdd)\n\n                ),\n                FILE_REFS.err: FileInfo(\n                    constructor=lambda path: self._err_cls(path),\n                    get_path=lambda: get_output_file_path(self.dir_path, FILE_REFS.err)\n                ),\n                FILE_REFS.summary_table: FileInfo(\n                    constructor=lambda path: self._summary_table_cls(path),\n                    get_path=lambda: get_output_file_path(self.dir_path, FILE_REFS.summary_table)\n                )\n            }\n        return self._prepared_file_refs", "response": "Defines the file references that are used by the file system."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef exists(self, file_ref):\n        if file_ref not in FILE_REFS:\n            raise ValueError(\"Unknown file_ref: '%s'. Available: '%s'.\" % (file_ref, list(sorted(FILE_REFS._fields))))\n        return os.path.isfile(self._path(file_ref))", "response": "Returns True if the file with the given file_ref exists in the local cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_file_path(self, file_ref):\n        if not self.exists(file_ref):\n            raise FileNotFoundError(\"File '%s' not found in simulation '%s'.\" % (file_ref, self._path(file_ref)))\n        return self._path(file_ref)", "response": "Returns the path of the file in the simulation."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef default_external_files_dir_name(model_name):\n    name, ext = os.path.splitext(model_name)\n    return name + CONF.external_files_suffix", "response": "Returns the default external files directory name for a given model name."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npopulates the internal state of the object with the data from the JSON data.", "response": "def _dev_populate_from_json_data(self, json_data):\n        \"\"\"\n        !! Must only be called once, when empty !!\n        \"\"\"\n\n        # workflow\n        # --------\n        # (methods belonging to create/update/delete framework:\n        #     epm._dev_populate_from_json_data, table.batch_add, record.update, queryset.delete, record.delete)\n        # 1. add inert\n        #     * data is checked\n        #     * old links are unregistered\n        #     * record is stored in table (=> pk uniqueness is checked)\n        # 2. activate: hooks, links, external files\n\n        # manage comment if any\n        comment = json_data.pop(\"_comment\", None)\n        if comment is not None:\n            self._comment = comment\n\n        # populate external files\n        external_files_data = json_data.pop(\"_external_files\", dict())\n        self._dev_external_files_manager.populate_from_json_data(external_files_data)\n\n        # manage records\n        added_records = []\n        for table_ref, json_data_records in json_data.items():\n            # find table\n            table = getattr(self, table_ref)\n\n            # create record (inert)\n            records = table._dev_add_inert(json_data_records)\n\n            # add records (inert)\n            added_records.extend(records)\n\n        # activate hooks\n        for r in added_records:\n            r._dev_activate_hooks()\n\n        # activate links and external files\n        for r in added_records:\n            r._dev_activate_links()\n            r._dev_activate_external_files()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_external_files(self):\n        external_files = []\n        for table in self._tables.values():\n            for r in table:\n                external_files.extend([ef for ef in r.get_external_files()])\n        return external_files", "response": "Returns a list of all external files in the database."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_defaults(self):\n        for table in self._tables.values():\n            for r in table:\n                r.set_defaults()", "response": "Set the default values of all tables and entries in this EPM."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_json_data(cls, json_data, check_required=True, idd_or_buffer_or_path=None):\n        epm = cls(\n            idd_or_buffer_or_path=idd_or_buffer_or_path,\n            check_required=check_required\n        )\n\n        epm._dev_populate_from_json_data(json_data)\n        return epm", "response": "Create an Epm instance from a dictionary of serialized data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating an Epm instance from an IDF file.", "response": "def from_idf(cls, buffer_or_path, check_required=True, idd_or_buffer_or_path=None):\n        \"\"\"\n        Parameters\n        ----------\n        buffer_or_path: idf buffer or path\n        check_required: boolean, default True\n            If True, will raise an exception if a required field is missing. If False, not not perform any checks.\n        idd_or_buffer_or_path: (expert) to load using a custom idd\n\n        Returns\n        -------\n        An Epm instance.\n        \"\"\"\n        # todo: add geometry only (or equivalent)\n        return cls._create_from_buffer_or_path(\n            parse_idf,\n            buffer_or_path,\n            idd_or_buffer_or_path=idd_or_buffer_or_path,\n            check_required=check_required\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_json(cls, buffer_or_path, check_required=True, idd_or_buffer_or_path=None):\n        return cls._create_from_buffer_or_path(\n            json.load,\n            buffer_or_path,\n            idd_or_buffer_or_path=idd_or_buffer_or_path,\n            check_required=check_required\n        )", "response": "Create an Epm instance from a json buffer or path."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_json_data(self):\n        # create data\n        d = collections.OrderedDict((t.get_ref(), t.to_json_data()) for t in self._tables.values())\n        d[\"_comment\"] = self._comment\n        d.move_to_end(\"_comment\", last=False)\n        d[\"_external_files\"] = self._dev_external_files_manager\n        return d", "response": "Returns a dictionary of serialized data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a json string of the current object.", "response": "def to_json(self, buffer_or_path=None, indent=2):\n        \"\"\"\n        Parameters\n        ----------\n        buffer_or_path: buffer or path, default None\n            output to write into. If None, will return a json string.\n        indent: int, default 2\n            Defines the indentation of the json\n\n        Returns\n        -------\n        None, or a json string (if buffer_or_path is None).\n        \"\"\"\n        # return json\n        return json_data_to_json(\n            self.to_json_data(),\n            buffer_or_path=buffer_or_path,\n            indent=indent\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a idf string of the current state of the object.", "response": "def to_idf(self, buffer_or_path=None, dump_external_files=True):\n        \"\"\"\n        Parameters\n        ----------\n        buffer_or_path: buffer or path, default None\n            output to write into. If None, will return a json string.\n        dump_external_files: boolean, default True\n            if True, external files will be dumped in external files directory\n\n        Returns\n        -------\n        None, or an idf string (if buffer_or_path is None).\n        \"\"\"\n        # prepare comment\n        comment = get_multi_line_copyright_message()\n        if self._comment != \"\":\n            comment += textwrap.indent(self._comment, \"! \", lambda line: True)\n        comment += \"\\n\\n\"\n\n        # prepare external files dir path if file path\n        if isinstance(buffer_or_path, str):\n            dir_path, file_name = os.path.split(buffer_or_path)\n            model_name, _ = os.path.splitext(file_name)\n        else:\n            model_name, dir_path = None, os.path.curdir\n\n        # dump files if asked\n        if dump_external_files:\n            self.dump_external_files(\n                target_dir_path=os.path.join(dir_path, get_external_files_dir_name(model_name=model_name))\n            )\n\n        # prepare body\n        formatted_records = []\n        for table_ref, table in self._tables.items():  # self._tables is already sorted\n            formatted_records.extend([r.to_idf(model_name=model_name) for r in sorted(table)])\n        body = \"\\n\\n\".join(formatted_records)\n\n        # return\n        content = comment + body\n        return multi_mode_write(\n            lambda f: f.write(content),\n            lambda: content,\n            buffer_or_path\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef select(self, filter_by=None):\n        iterator = self._records if filter_by is None else filter(filter_by, self._records)\n        return Queryset(self._table, iterator)", "response": "Returns a new Queryset containing only the records that match the filter_by."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef one(self, filter_by=None):\n        # filter if needed\n        qs = self if filter_by is None else self.select(filter_by=filter_by)\n\n        # check one and only one\n        if len(qs) == 0:\n            raise RecordDoesNotExistError(\"Queryset set contains no value.\")\n        if len(qs) > 1:\n            raise MultipleRecordsReturnedError(\"Queryset contains more than one value.\")\n\n        # return record\n        return qs[0]", "response": "Returns a single record from the queryset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_simulated_epw_path():\n    from oplus import CONF  # touchy imports\n\n    if OS_NAME == \"windows\":\n        return os.path.join(CONF.eplus_base_dir_path, \"WeatherData\", \"%s.epw\" % CONF.default_model_name)", "response": "Returns the path to the epw file that should be used for simulation."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prepare_extensible(self):\n        # see if extensible and store cycle len\n        for k in self._tags:\n            if \"extensible\" in k:\n                cycle_len = int(k.split(\":\")[1])\n                break\n        else:\n            # not extensible\n            return\n\n        # find cycle start and prepare patterns\n        cycle_start = None\n        cycle_patterns = []\n        for i, field_descriptor in enumerate(self._field_descriptors):\n            # quit if finished\n            if (cycle_start is not None) and (i >= (cycle_start + cycle_len)):\n                break\n\n            # set cycle start if not set yet\n            if (cycle_start is None) and (\"begin-extensible\" in field_descriptor.tags):\n                cycle_start = i\n\n            # leave if cycle start not reached yet\n            if cycle_start is None:\n                continue\n\n            # store pattern\n            cycle_patterns.append(field_descriptor.ref.replace(\"1\", r\"(\\d+)\"))\n        else:\n            raise RuntimeError(\"cycle start not found\")\n\n        # detach unnecessary field descriptors\n        self._field_descriptors = self._field_descriptors[:cycle_start + cycle_len]\n\n        # store cycle info\n        self.extensible_info = (cycle_start, cycle_len, tuple(cycle_patterns))\n\n        # set field descriptor cycle_start index (for error messages while serialization)\n        for i, fd in enumerate(self._field_descriptors[cycle_start:]):\n            fd.set_extensible_info(cycle_start, cycle_len, cycle_patterns[i])", "response": "This function is called when all the fields in the archive are marked as extensible."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the reduced index of the base field that has been applied to the extensible entry", "response": "def get_field_reduced_index(self, index):\n        \"\"\"\n        reduced index: modulo of extensible has been applied\n        \"\"\"\n        # return index if not extensible\n        if self.extensible_info is None:\n            return index\n        \n        # manage extensible\n        cycle_start, cycle_len, _ = self.extensible_info\n        \n        # base field\n        if index < cycle_start:\n            return index\n        \n        # extensible field\n        return cycle_start + ((index - cycle_start) % cycle_len)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_extended_name(self, index):\n        field_descriptor = self.get_field_descriptor(index)\n        if self.extensible_info is None:\n            return field_descriptor.name\n        cycle_start, cycle_len, _ = self.extensible_info\n        cycle_num = (index - cycle_start) // cycle_len\n        return None if field_descriptor.name is None else field_descriptor.name.replace(\"1\", str(cycle_num))", "response": "Gets the extended name of the entry at the given index."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef deserialize(self, value, index):\n        # -- serialize if not raw type\n        # transform to string if external file\n        if isinstance(value, ExternalFile):\n            value = value.pointer\n\n        # transform to string if record\n        if isinstance(value, Record):\n            try:\n                value = value[0]\n            except IndexError:\n                raise ValueError(\"can't set given record because it does not have a name field\")\n        \n        # -- prepare if string\n        if isinstance(value, str):\n            # change multiple spaces to mono spaces\n            value = re.sub(spaces_and_newlines_pattern, lambda x: \" \", value.strip())\n            \n            # see if still not empty\n            if value == \"\":\n                return None\n\n            # make ASCII compatible\n            value = unidecode.unidecode(value)\n\n            # make lower case if not retaincase\n            if \"retaincase\" not in self.tags:\n                value = value.lower()\n\n            # check not too big\n            if len(value) >= 100:\n                raise FieldValidationError(\n                    f\"Field has more than 100 characters which is the limit. \"\n                    f\"{self.get_error_location_message(value, index=index)}\"\n                )\n\n        # transform to external file if relevant\n        if self.is_file_name:\n            value = ExternalFile.deserialize(value)\n\n        # -- deserialize\n\n        # numeric types\n        if self.detailed_type in (\"integer\", \"real\"):\n            # manage none\n            if value is None:\n                return None\n\n            # special values: auto-calculate, auto-size, use-weather-file\n            if value in (\"autocalculate\", \"autosize\", \"useweatherfile\"):\n                return value\n            \n            if self.detailed_type == \"integer\":\n                try:\n                    return int(value)\n                except:\n                    raise FieldValidationError(\n                        f\"Couldn't parse to integer. {self.get_error_location_message(value, index=index)}\"\n                    )\n\n            try:\n                return float(value)\n            except:\n                raise FieldValidationError(\n                    f\"Couldn't parse to float. {self.get_error_location_message(value, index=index)}\"\n                )\n\n        # simple string types\n        if self.detailed_type in (\"alpha\", \"choice\", \"node\", \"external-list\"):\n            # manage none\n            if value is None:\n                return None\n\n            # ensure it was str\n            if not isinstance_str(value):\n                raise FieldValidationError(\n                    f\"Value must be a string. {self.get_error_location_message(value, index=index)}\"\n                )\n            return value\n\n        # manage hooks (eplus reference)\n        if self.detailed_type == \"reference\":\n            # manage None\n            if value is None:\n                return NONE_RECORD_HOOK\n\n            # reference class name appears in v9.0.1\n            references = self.tags.get(\"reference\", [])\n            # table_name, index, value, references, class_references\n            return RecordHook(references, index, value)\n\n        # manage links (eplus object-list)\n        if self.detailed_type == \"object-list\":\n            # manage None\n            if value is None:\n                return NONE_LINK\n\n            return Link(self.tags[\"object-list\"], value, index)\n\n        raise RuntimeError(\"should not be here\")", "response": "Deserialize a value into a value that can be used to store the error message in the database."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the detailed type of the resource.", "response": "def detailed_type(self):\n        \"\"\"\n        Uses EPlus double approach of type ('type' tag, and/or 'key', 'object-list', 'external-list', 'reference' tags)\n        to determine detailed type.\n        \n        Returns\n        -------\n        \"integer\", \"real\", \"alpha\", \"choice\", \"reference\", \"object-list\", \"external-list\", \"node\"\n        \"\"\"\n        if self._detailed_type is None:\n            if (\"reference\" in self.tags) or (\"reference-class-name\" in self.tags):\n                self._detailed_type = \"reference\"\n            elif \"type\" in self.tags:\n                self._detailed_type = self.tags[\"type\"][0].lower()  # idd is not very rigorous on case\n            elif \"key\" in self.tags:\n                self._detailed_type = \"choice\"\n            elif \"object-list\" in self.tags:\n                self._detailed_type = \"object-list\"\n            elif \"external-list\" in self.tags:\n                self._detailed_type = \"external-list\"\n            elif self.basic_type == \"A\":\n                self._detailed_type = \"alpha\"\n            elif self.basic_type == \"N\":\n                self._detailed_type = \"real\"\n            else:\n                raise ValueError(\"Can't find detailed type.\")\n        return self._detailed_type"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef short_refs(self):\n        naive_short_refs_d = dict()  # naive_short_ref: {refs, ...}\n        for ef in self._external_files:\n            if ef.naive_short_ref not in naive_short_refs_d:\n                naive_short_refs_d[ef.naive_short_ref] = set()\n            naive_short_refs_d[ef.naive_short_ref].add(ef.ref)\n\n        short_refs = dict()\n        for naive_short_ref, refs in naive_short_refs_d.items():\n            if len(refs) == 1:\n                short_refs[refs.pop()] = naive_short_ref\n                continue\n            base, ext = os.path.splitext(naive_short_ref)\n            for i, ref in enumerate(sorted(refs)):\n                short_refs[ref] = f\"{base}-{i}.{ext}\"\n\n        return short_refs", "response": "Returns a dictionary of all the short_refs of the external files that are not in the registry."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_value(self, column_name_or_i, filter_column_name_or_i, filter_criterion):\n        # find column indexes\n        column_i = self._get_column_index(column_name_or_i)\n        filter_column_i = self._get_column_index(filter_column_name_or_i)\n\n        filter_fct = {\n            float: lambda x: float(x) == filter_criterion,\n            int: lambda x: int(x) == filter_criterion,\n            str: lambda x: x.lower() == filter_criterion.lower()\n        }[type(filter_criterion)]\n\n        for row_i, row in enumerate(self._data):\n            if filter_fct(row[filter_column_i]):\n                break\n        else:\n            raise ValueError(\"Filter did not return any values.\")\n\n        return self._data[row_i][column_i]", "response": "Returns the value of the specified column matching the given filter criterion."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _update_value_inert(self, index, value):\n        # get field descriptor\n        field_descriptor = self._table._dev_descriptor.get_field_descriptor(index)\n\n        # prepare value\n        value = field_descriptor.deserialize(value, index)\n\n        # unregister previous link if relevant\n        if isinstance(value, Link):\n            # de-activate current link if any\n            current_link = self._data.get(index)\n            if current_link is not None:\n                current_link.unregister()\n\n        # unregister previous hook if relevant\n        if isinstance(value, RecordHook):\n            current_record_hook = self._data.get(index)\n            if current_record_hook is not None:\n                current_record_hook.unregister()\n\n        # unregister previous external file if relevant\n        if isinstance(value, ExternalFile):\n            current_external_file = self._data.get(index)\n            if current_external_file is not None:\n                current_external_file._dev_unregister()\n\n        # if None remove and leave\n        if value in (None, NONE_RECORD_HOOK, NONE_LINK, NONE_EXTERNAL_FILE):\n            # we don't check required, because this method is called by _update_inert which does the job\n            self._dev_set_none_without_unregistering(index, check_not_required=False)\n            return\n\n        # if relevant, store current pk to signal table\n        old_hook = None\n        if index == 0 and not self._table._dev_auto_pk:\n            old_hook = self._data.get(0)  # we use get, because record may not have a pk yet if it is being created\n\n        # set value\n        self._data[index] = value\n\n        # signal pk update if relevant\n        if old_hook is not None:\n            self._table._dev_record_pk_was_updated(old_hook.target_value)", "response": "Update the value of the relevant record entry in the internal list."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_serialized_value(self, ref_or_index, model_name=None):\n        index = (\n            self._table._dev_descriptor.get_field_index(ref_or_index) if isinstance(ref_or_index, str)\n            else ref_or_index\n        )\n\n        # get value\n        value = self._data.get(index)\n\n        # serialize\n        value = value.serialize() if isinstance(value, (Link, RecordHook)) else value\n\n        # manage file names\n        if isinstance(value, ExternalFile):\n            value = os.path.join(get_external_files_dir_name(model_name=model_name), value.naive_short_ref)\n\n        return value", "response": "Get the serialized value of the current entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_external_files(self):\n        return [v for v in self._data.values() if isinstance(v, ExternalFile)]", "response": "Returns ------- list of ExternalFiles instances contained by record."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update(self, data=None, **or_data):\n\n        # workflow\n        # --------\n        # (methods belonging to create/update/delete framework:\n        #     epm._dev_populate_from_json_data, table.batch_add, record.update, queryset.delete, record.delete)\n        # 1. add inert\n        #     * data is checked\n        #     * old links are unregistered\n        #     * record is stored in table (=> pk uniqueness is checked)\n        # 2. activate: hooks, links, external files\n\n        data = or_data if data is None else data\n\n        self._update_inert(data)\n\n        self._dev_activate_hooks()\n        self._dev_activate_links()\n        self._dev_activate_external_files()", "response": "Update the record with the given data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef copy(self, new_name=None):\n        # todo: check this really works, !! must not use same link, hook, external_file, ... for different records !!\n        # auto pk tables can just be copied\n        if self._table._dev_auto_pk:\n            return self._table.add(self._data)\n\n        # for ref pk tables, must manage name\n        name = str(uuid.uuid4()) if new_name is None else new_name\n        new_data = dict((k, name if k == 0 else v) for (k, v) in self._data.items())\n        return self._table.add(new_data)", "response": "Copy the current record to another record."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_defaults(self):\n        defaults = {}\n        for i in range(len(self)):\n            if i in self._data:\n                continue\n            default = self.get_field_descriptor(i).tags.get(\"default\", [None])[0]\n            if default is not None:\n                defaults[i] = default\n\n        self.update(defaults)", "response": "Sets all empty fields for which a default value is defined to default value\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_fields(self, *args):\n        if not self.is_extensible():\n            raise TypeError(\"Can't use add_fields on a non extensible record.\")\n\n        # prepare update data\n        self_len = len(self)\n        data = dict([(self_len + i, args[i]) for i in range(len(args))])\n\n        # update\n        self.update(data)", "response": "Adds fields to the record."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninsert a value into the index of the current record set.", "response": "def insert(self, index, value):\n        \"\"\"\n        This method only works for extensible fields. It allows to insert a value, and shifts all other following\n        values.\n\n        Parameters\n        ----------\n        index: position of insertion\n        value: value to insert\n        \"\"\"\n        # prepare index (will check for extensible)\n        index = self._prepare_pop_insert_index(index=index)\n\n        # remove extensible fields\n        fields = self.clear_extensible_fields()\n\n        # insert\n        fields.insert(index, value)\n\n        # add new list\n        self.add_fields(*fields)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef clear_extensible_fields(self):\n        if not self.is_extensible():\n            raise TypeError(\"Can't use add_fields on a non extensible record.\")\n        cycle_start, cycle_len, patterns = self.get_extensible_info()\n        return [self.get_serialized_value(i) for i in range(cycle_start, len(self))]", "response": "Returns a list of cleared fields from the record that are not extensible."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete(self):\n        # workflow\n        # --------\n        # (methods belonging to create/update/delete framework:\n        #     epm._dev_populate_from_json_data, table.batch_add, record.update, queryset.delete, record.delete)\n        # 1. unregister: links, hooks and external files\n        # 3. remove from table without unregistering\n\n        # unregister links\n        self._unregister_links()\n\n        # unregister hooks\n        self._unregister_hooks()\n\n        # unregister external files\n        self._unregister_external_files()\n\n        # tell table to remove without unregistering\n        self.get_table()._dev_remove_record_without_unregistering(self)\n\n        # make stale\n        self._table = None\n        self._data = None", "response": "Deletes record and removes it from database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the field descriptor of the current object.", "response": "def get_field_descriptor(self, ref_or_index):\n        \"\"\"\n        Parameters\n        ----------\n        ref_or_index: str or int\n            field lowercase name, or field position\n\n        Returns\n        -------\n        Field descriptor (info contained in Idd)\n        \"\"\"\n        if isinstance(ref_or_index, int):\n            index = ref_or_index\n        else:\n            index = self._table._dev_descriptor.get_field_index(ref_or_index)\n        return self._table._dev_descriptor.get_field_descriptor(index)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a dictionary of serialized data.", "response": "def to_json_data(self, model_name=None):\n        \"\"\"\n        Parameters\n        ----------\n        model_name: str, default None\n            if given, will be used as external file directory base name\n\n        Returns\n        -------\n        A dictionary of serialized data.\n        \"\"\"\n        return collections.OrderedDict([(k, self.get_serialized_value(k, model_name=model_name )) for k in self._data])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the string representation of the idd record as an idd record.", "response": "def to_idf(self, model_name=None):\n        \"\"\"\n        Parameters\n        ----------\n         model_name: str, default None\n            if given, will be used as external file directory base name\n\n        Returns\n        -------\n        idf string\n        \"\"\"\n\n        json_data = self.to_json_data(model_name=model_name)\n            \n        # record descriptor ref\n        s = f\"{self._table._dev_descriptor.table_name},\\n\"\n\n        # fields\n        # fields_nb: we don't use len(self) but max(self). We wan't to stop if no more values (even base fields)\n        #   because some idd records are defined without extensibles (although they should used them), for example\n        #   construction, and eplus does not know what to do...\n        fields_nb = max(self._data)+1\n        for i in range(fields_nb):\n            # value\n            tab = \" \" * TAB_LEN\n            raw_value = json_data.get(i, \"\")\n            content = f\"{tab}{raw_value}{';' if i == fields_nb-1 else ','}\"\n\n            # comment\n            spaces_nb = COMMENT_COLUMN_START - len(content)\n            if spaces_nb < 0:\n                spaces_nb = TAB_LEN\n               \n            # comment\n            name = self._table._dev_descriptor.get_extended_name(i)\n            comment = \"\" if name is None else \" \" * spaces_nb + f\"! {name}\"\n\n            # store\n            s += f\"{content}{comment}\\n\"\n\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if there are too many examples in the current order.", "response": "def check():\n    \"\"\"\n    Tested under EPlus 8.1.0 on Windows (Geoffroy).\n    \"\"\"\n    # !! CAN BE VERY LONG\n    epw_path = os.path.join(CONF.eplus_base_dir_path, \"WeatherData\",\n                            \"USA_VA_Sterling-Washington.Dulles.Intl.AP.724030_TMY3.epw\")\n    idf_dir_path = os.path.join(CONF.eplus_base_dir_path, \"ExampleFiles\")\n    test_num = 0\n    for file_num, file_name in enumerate(os.listdir(idf_dir_path)):\n        if file_num < START_FILE_NUM:\n            continue\n        base, ext = os.path.splitext(file_name)\n        if ext == \".idf\":\n            with tempfile.TemporaryDirectory() as simulation_dir_path:\n                s = simulate(os.path.join(idf_dir_path, file_name), epw_path,\n                             simulation_dir_path if DEBUG_SIMUL_DIR_PATH is None else\n                             DEBUG_SIMUL_DIR_PATH)\n                if s.exists(\"eio\"):\n                    eio = Eio(s.get_file_path(\"eio\"))  # raise error if problem\n                    test_num += 1\n        if test_num == MAX_TESTS_NB:\n            break"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the dataframes for the given environment.", "response": "def get_data(self, environment_title_or_num=-1, frequency=None):\n        \"\"\"\n        Parameters\n        ----------\n        environment_title_or_num\n        frequency: 'str', default None\n            'timestep', 'hourly', 'daily', 'monthly', 'annual', 'run_period'\n            If None, will look for the smallest frequency of environment.\n        \"\"\"\n\n        # manage environment num\n        if isinstance(environment_title_or_num, int):\n            environment_title = tuple(self._raw_environments.keys())[environment_title_or_num]\n        else:\n            environment_title = environment_title_or_num\n\n        if environment_title not in self._dfs:\n            raise ValueError(f\"No environment named {environment_title}. Available environments: {tuple(self._dfs)}.\")\n\n        # get environment dataframes\n        environment_dfs = self._dfs[environment_title]\n\n        # find first non null frequency if not given\n        if frequency is None:\n            for frequency in FREQUENCIES:\n                if environment_dfs[frequency] is not None:\n                    break\n\n        # check frequency\n        if frequency not in FREQUENCIES:\n            raise ValueError(f\"Unknown frequency: {frequency}. Available frequencies: {FREQUENCIES}\")\n\n        return self._dfs[environment_title][frequency]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _dev_add_inert(self, records_data):\n        added_records = []\n        for r_data in records_data:\n            # create record\n            record = Record(\n                self,\n                data=r_data\n            )\n\n            # store\n            # we don't check uniqueness here => will be done while checking hooks\n            self._records[record.get_pk()] = record\n            \n            # remember record\n            added_records.append(record)\n        \n        return added_records", "response": "Add inerts to the internal list of records."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef select(self, filter_by=None):\n        records = self._records.values() if filter_by is None else filter(filter_by, self._records.values())\n        return Queryset(self, records=records)", "response": "Returns a new Queryset containing only the records that match the filter_by."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a single record in the set.", "response": "def one(self, filter_by=None):\n        \"\"\"\n        Parameters\n        ----------\n        filter_by: callable, default None\n            Callable must take one argument (a record of table), and return True to keep record, or False to skip it.\n            Example : .one(lambda x: x.name == \"my_name\").\n            If None, records are not filtered.\n\n        Returns\n        -------\n        Record instance if one and only one record is found. Else raises.\n\n        Raises\n        ------\n        RecordDoesNotExistError if no record is found\n        MultipleRecordsReturnedError if multiple records are found\n        \"\"\"\n        return Queryset(self, records=self._records.values()).one(filter_by=filter_by)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nregistering a record hook.", "response": "def register_record_hook(self, hook):\n        \"\"\"\n        target record must have been set\n        \"\"\"\n        for key in hook.keys:\n            if key in self._record_hooks:\n                field_descriptor = hook.target_record.get_field_descriptor(hook.target_index)\n                raise FieldValidationError(\n                    f\"Reference key already exists, can't create: {key}. \"\n                    f\"{field_descriptor.get_error_location_message(hook.target_value, hook.target_index)}\"\n                )\n            self._record_hooks[key] = hook"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef register_link(self, link):\n        keys = tuple((ref, link.initial_hook_value) for ref in link.hook_references)\n\n        # look for a record hook\n        for k in keys:\n            if k in self._record_hooks:\n                # set link target\n                link.set_target(target_record=self._record_hooks[k].target_record)\n                break\n        else:\n            # look for a table hook\n            for k in keys:\n                if k in self._table_hooks:\n                    # set link target\n                    link.set_target(target_table=self._table_hooks[k])\n                    break\n            else:\n                field_descriptor = link.source_record.get_field_descriptor(link.source_index)\n                raise FieldValidationError(\n                    f\"No object found with any of given references : {keys}. \"\n                    f\"{field_descriptor.get_error_location_message(link.initial_hook_value)}\"\n                )\n\n        # store by source\n        if link.source_record not in self._links_by_source:\n            self._links_by_source[link.source_record] = set()\n        self._links_by_source[link.source_record].add(link)\n\n        # store by target\n        if link.target not in self._links_by_target:\n            self._links_by_target[link.target] = set()\n        self._links_by_target[link.target].add(link)", "response": "Register a link in the database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning a command in a new process and returns its return code.", "response": "def run_subprocess(command, cwd=None, stdout=None, stderr=None, shell=False, beat_freq=None):\n    \"\"\"\n    Parameters\n    ----------\n    command: command\n    cwd: current working directory\n    stdout: output info stream (must have 'write' method)\n    stderr: output error stream (must have 'write' method)\n    shell: see subprocess.Popen\n    beat_freq: if not none, stdout will be used at least every beat_freq (in seconds)\n    \"\"\"\n    sys.encoding = CONF.encoding\n    # prepare variables\n    stdout = sys.stdout if stdout is None else stdout\n    stderr = sys.stderr if stderr is None else stderr\n\n    # run subprocess\n    with subprocess.Popen(\n        command,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        cwd=cwd,\n        shell=shell,\n        universal_newlines=True\n    ) as sub_p:\n        # link output streams\n        with redirect_stream(sub_p.stdout, stdout), redirect_stream(sub_p.stderr, stderr):\n            while True:\n                try:\n                    sub_p.wait(timeout=beat_freq)\n                    break\n                except subprocess.TimeoutExpired:\n                    stdout.write(\"subprocess is still running\\n\")\n                    if hasattr(sys.stdout, \"flush\"):\n                        sys.stdout.flush()\n        return sub_p.returncode"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_string_buffer(path_or_content, expected_extension):\n    buffer, path = None, None\n\n    # path or content string\n\n    if isinstance(path_or_content, str):\n        if path_or_content[-len(expected_extension)-1:] == \".%s\" % expected_extension:\n            if not os.path.isfile(path_or_content):\n                raise FileNotFoundError(\"No file at given path: '%s'.\" % path_or_content)\n            buffer, path = open(path_or_content, encoding=CONF.encoding), path_or_content\n        else:\n            buffer = io.StringIO(path_or_content, )\n\n    # text io\n    elif isinstance(path_or_content, io.TextIOBase):\n        buffer = path_or_content\n\n    # bytes\n    elif isinstance(path_or_content, bytes):\n        buffer = io.StringIO(path_or_content.decode(encoding=CONF.encoding))\n    elif isinstance(path_or_content, io.BufferedIOBase):\n        buffer = io.StringIO(path_or_content.read().decode(encoding=CONF.encoding))\n    else:\n        raise ValueError(\"path_or_content type could not be identified\")\n\n    return buffer, path", "response": "Returns a string_buffer object for the given path_or_content."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_data(self, simulation_step=None, error_category=None):\n        if simulation_step is None and error_category is None:\n            return self._df.dropna(axis=\"rows\", how=\"all\")\n\n        if simulation_step is not None:\n            if simulation_step not in self._simulation_step_list:\n                raise RuntimeError(\"The simulation_step '%s' is not referred in the error file.\" % simulation_step)\n\n            if error_category is not None:\n                if error_category not in self.CATEGORIES:\n                    raise RuntimeError(\"The error_cat '%s' is wrong.\" % error_category)\n                iterables = [simulation_step, error_category]\n                columns = pd.MultiIndex.from_product(iterables)\n                series = self._df[simulation_step][error_category].dropna(axis=\"rows\", how=\"all\")\n\n                df = pd.DataFrame(index=series.index, columns=columns)\n                df[simulation_step] = series\n                return df\n\n            return self._df[simulation_step].dropna(axis=\"rows\", how=\"all\")\n\n        if error_category is not None:\n            if error_category not in self.CATEGORIES:\n                raise RuntimeError(\"The error_category '%s' is wrong.\" % error_category)\n            df = self._df.copy()\n            df.columns = df.columns.swaplevel(0, 1)\n            return df[error_category].dropna(axis=\"rows\", how=\"all\")", "response": "Returns a dataframe of the data for the specified simulation_step and error_category."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating regex and return.", "response": "def _create_regex(self, line, intent_name):\n        \"\"\" Create regex and return. If error occurs returns None. \"\"\"\n        try:\n            return re.compile(self._create_intent_pattern(line, intent_name),\n                              re.IGNORECASE)\n        except sre_constants.error as e:\n            LOG.warning('Failed to parse the line \"{}\" '\n                        'for {}'.format(line, intent_name))\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting status ( id ) to its string name.", "response": "def str(cls, value):\n        '''Convert status (id) to its string name.'''\n        for k, v in cls.__dict__.items():\n            if k[0] in string.ascii_uppercase and v == value:\n                return k.lower().replace('_', ' ')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remaining_duration(self, time):\n        '''Returns the remaining duration for a recording.\n        '''\n        return max(0, self.end - max(self.start, time))", "response": "Returns the remaining duration for a recording."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nserializing this object as dictionary usable for conversion to JSON.", "response": "def serialize(self):\n        '''Serialize this object as dictionary usable for conversion to JSON.\n\n        :return: Dictionary representing this object.\n        '''\n        return {\n            'type': 'event',\n            'id': self.uid,\n            'attributes': {\n                'start': self.start,\n                'end': self.end,\n                'uid': self.uid,\n                'title': self.title,\n                'data': self.get_data(),\n                'status': Status.str(self.status)\n                }\n            }"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef http_request(url, post_data=None):\n    '''Make an HTTP request to a given URL with optional parameters.\n    '''\n    logger.debug('Requesting URL: %s' % url)\n    buf = bio()\n    curl = pycurl.Curl()\n    curl.setopt(curl.URL, url.encode('ascii', 'ignore'))\n\n    # Disable HTTPS verification methods if insecure is set\n    if config()['server']['insecure']:\n        curl.setopt(curl.SSL_VERIFYPEER, 0)\n        curl.setopt(curl.SSL_VERIFYHOST, 0)\n\n    if config()['server']['certificate']:\n        # Make sure verification methods are turned on\n        curl.setopt(curl.SSL_VERIFYPEER, 1)\n        curl.setopt(curl.SSL_VERIFYHOST, 2)\n        # Import your certificates\n        curl.setopt(pycurl.CAINFO, config()['server']['certificate'])\n\n    if post_data:\n        curl.setopt(curl.HTTPPOST, post_data)\n    curl.setopt(curl.WRITEFUNCTION, buf.write)\n    curl.setopt(pycurl.HTTPAUTH, pycurl.HTTPAUTH_DIGEST)\n    curl.setopt(pycurl.USERPWD, \"%s:%s\" % (config()['server']['username'],\n                                           config()['server']['password']))\n    curl.setopt(curl.HTTPHEADER, ['X-Requested-Auth: Digest'])\n    curl.setopt(curl.FAILONERROR, True)\n    curl.setopt(curl.FOLLOWLOCATION, True)\n    curl.perform()\n    curl.close()\n    result = buf.getvalue()\n    buf.close()\n    return result", "response": "Make an HTTP request to a given URL with optional parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets available service endpoints for a given service type from the Opencast ServiceRegistry.", "response": "def get_service(service_type):\n    '''Get available service endpoints for a given service type from the\n    Opencast ServiceRegistry.\n    '''\n    endpoint = '/services/available.json?serviceType=' + str(service_type)\n    url = '%s%s' % (config()['server']['url'], endpoint)\n    response = http_request(url).decode('utf-8')\n    services = (json.loads(response).get('services') or {}).get('service', [])\n    services = ensurelist(services)\n    endpoints = [service['host'] + service['path'] for service in services\n                 if service['online'] and service['active']]\n    for endpoint in endpoints:\n        logger.info(u'Endpoint for %s: %s', service_type, endpoint)\n    return endpoints"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unix_ts(dtval):\n    '''Convert datetime into a unix timestamp.\n    This is the equivalent to Python 3's int(datetime.timestamp()).\n\n    :param dt: datetime to convert\n    '''\n    epoch = datetime(1970, 1, 1, 0, 0, tzinfo=tzutc())\n    delta = (dtval - epoch)\n    return delta.days * 24 * 3600 + delta.seconds", "response": "Convert a datetime into a unix timestamp."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef try_mkdir(directory):\n    '''Try to create a directory. Pass without error if it already exists.\n    '''\n    try:\n        os.mkdir(directory)\n    except OSError as err:\n        if err.errno != errno.EEXIST:\n            raise err", "response": "Try to create a directory. Pass without error."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef configure_service(service):\n    '''Get the location of a given service from Opencast and add it to the\n    current configuration.\n    '''\n    while not config().get('service-' + service) and not terminate():\n        try:\n            config()['service-' + service] = \\\n                get_service('org.opencastproject.' + service)\n        except pycurl.error as e:\n            logger.error('Could not get %s endpoint: %s. Retrying in 5s' %\n                         (service, e))\n            time.sleep(5.0)", "response": "Get the location of a given service from Opencast and add it to the current configuration."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef register_ca(status='idle'):\n    '''Register this capture agent at the Matterhorn admin server so that it\n    shows up in the admin interface.\n\n    :param address: Address of the capture agent web ui\n    :param status: Current status of the capture agent\n    '''\n    # If this is a backup CA we don't tell the Matterhorn core that we are\n    # here.  We will just run silently in the background:\n    if config()['agent']['backup_mode']:\n        return\n    params = [('address', config()['ui']['url']), ('state', status)]\n    name = urlquote(config()['agent']['name'].encode('utf-8'), safe='')\n    url = '%s/agents/%s' % (config()['service-capture.admin'][0], name)\n    try:\n        response = http_request(url, params).decode('utf-8')\n        if response:\n            logger.info(response)\n    except pycurl.error as e:\n        logger.warning('Could not set agent state to %s: %s' % (status, e))", "response": "Register this capture agent at the Matterhorn admin server so that it shows up in the admin interface."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef recording_state(recording_id, status):\n    '''Send the state of the current recording to the Matterhorn core.\n\n    :param recording_id: ID of the current recording\n    :param status: Status of the recording\n    '''\n    # If this is a backup CA we do not update the recording state since the\n    # actual CA does that and we want to interfere.  We will just run silently\n    # in the background:\n    if config()['agent']['backup_mode']:\n        return\n    params = [('state', status)]\n    url = config()['service-capture.admin'][0]\n    url += '/recordings/%s' % recording_id\n    try:\n        result = http_request(url, params)\n        logger.info(result)\n    except pycurl.error as e:\n        logger.warning('Could not set recording state to %s: %s' % (status, e))", "response": "Send the state of the current recording to the Matterhorn core."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates the status of a particular event in the database.", "response": "def update_event_status(event, status):\n    '''Update the status of a particular event in the database.\n    '''\n    dbs = db.get_session()\n    dbs.query(db.RecordedEvent).filter(db.RecordedEvent.start == event.start)\\\n                               .update({'status': status})\n    event.status = status\n    dbs.commit()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the status of a particular service in the database.", "response": "def set_service_status(service, status):\n    '''Update the status of a particular service in the database.\n    '''\n    srv = db.ServiceStates()\n    srv.type = service\n    srv.status = status\n\n    dbs = db.get_session()\n    dbs.merge(srv)\n    dbs.commit()\n    dbs.close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates the status of a particular service in the database.", "response": "def get_service_status(service):\n    '''Update the status of a particular service in the database.\n    '''\n    dbs = db.get_session()\n    srvs = dbs.query(db.ServiceStates).filter(db.ServiceStates.type == service)\n\n    if srvs.count():\n        return srvs[0].status\n\n    return db.ServiceStatus.STOPPED"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the current agent state in opencast.", "response": "def update_agent_state():\n    '''Update the current agent state in opencast.\n    '''\n    configure_service('capture.admin')\n    status = 'idle'\n\n    # Determine reported agent state with priority list\n    if get_service_status(db.Service.SCHEDULE) == db.ServiceStatus.STOPPED:\n        status = 'offline'\n    elif get_service_status(db.Service.CAPTURE) == db.ServiceStatus.BUSY:\n        status = 'capturing'\n    elif get_service_status(db.Service.INGEST) == db.ServiceStatus.BUSY:\n        status = 'uploading'\n\n    register_ca(status=status)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind the best match for the configuration file.", "response": "def configuration_file(cfgfile):\n    '''Find the best match for the configuration file.\n    '''\n    if cfgfile is not None:\n        return cfgfile\n    # If no file is explicitely specified, probe for the configuration file\n    # location.\n    cfg = './etc/pyca.conf'\n    if not os.path.isfile(cfg):\n        return '/etc/pyca.conf'\n    return cfg"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the configuration from a file.", "response": "def update_configuration(cfgfile=None):\n    '''Update configuration from file.\n\n    :param cfgfile: Configuration file to load.\n    '''\n    configobj.DEFAULT_INTERPOLATION = 'template'\n    cfgfile = configuration_file(cfgfile)\n    cfg = configobj.ConfigObj(cfgfile, configspec=cfgspec, encoding='utf-8')\n    validator = Validator()\n    val = cfg.validate(validator)\n    if val is not True:\n        raise ValueError('Invalid configuration: %s' % val)\n    if len(cfg['capture']['files']) != len(cfg['capture']['flavors']):\n        raise ValueError('List of files and flavors do not match')\n    globals()['__config'] = cfg\n    logger_init()\n    if cfg['server'].get('url', '').endswith('/'):\n        logger.warning('Base URL ends with /. This is most likely a '\n                       'configuration error. The URL should contain nothing '\n                       'of the service paths.')\n    logger.info('Configuration loaded from %s' % cfgfile)\n    check()\n    return cfg"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck configuration for sanity.", "response": "def check():\n    '''Check configuration for sanity.\n    '''\n    if config('server')['insecure']:\n        logger.warning('HTTPS CHECKS ARE TURNED OFF. A SECURE CONNECTION IS '\n                       'NOT GUARANTEED')\n    if config('server')['certificate']:\n        # Ensure certificate exists and is readable\n        open(config('server')['certificate'], 'rb').close()\n    if config('agent')['backup_mode']:\n        logger.info('Agent runs in backup mode. No data will be sent to '\n                    'Opencast')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef logger_init():\n    '''Initialize logger based on configuration\n    '''\n    handlers = []\n    logconf = config('logging')\n    if logconf['syslog']:\n        handlers.append(logging.handlers.SysLogHandler(address='/dev/log'))\n    if logconf['stderr']:\n        handlers.append(logging.StreamHandler(sys.stderr))\n    if logconf['file']:\n        handlers.append(logging.handlers.WatchedFileHandler(logconf['file']))\n    for handler in handlers:\n        handler.setFormatter(logging.Formatter(logconf['format']))\n        logging.root.addHandler(handler)\n\n    logging.root.setLevel(logconf['level'].upper())\n    logger.info('Log level set to %s' % logconf['level'])", "response": "Initialize logger based on configuration"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef home():\n    '''Serve the status page of the capture agent.\n    '''\n    # Get IDs of existing preview images\n    preview = config()['capture']['preview']\n    previewdir = config()['capture']['preview_dir']\n    preview = [p.replace('{{previewdir}}', previewdir) for p in preview]\n    preview = zip(preview, range(len(preview)))\n    preview = [p[1] for p in preview if os.path.isfile(p[0])]\n\n    # Get limits for recording table\n    try:\n        limit_upcoming = int(request.args.get('limit_upcoming', 5))\n        limit_processed = int(request.args.get('limit_processed', 15))\n    except ValueError:\n        limit_upcoming = 5\n        limit_processed = 15\n\n    db = get_session()\n    upcoming_events = db.query(UpcomingEvent)\\\n                        .order_by(UpcomingEvent.start)\\\n                        .limit(limit_upcoming)\n    recorded_events = db.query(RecordedEvent)\\\n                        .order_by(RecordedEvent.start.desc())\\\n                        .limit(limit_processed)\n    recording = get_service_status(Service.CAPTURE) == ServiceStatus.BUSY\n    uploading = get_service_status(Service.INGEST) == ServiceStatus.BUSY\n    processed = db.query(RecordedEvent).count()\n    upcoming = db.query(UpcomingEvent).count()\n\n    return render_template('home.html', preview=preview, config=config(),\n                           recorded_events=recorded_events,\n                           upcoming_events=upcoming_events,\n                           recording=recording, uploading=uploading,\n                           processed=processed, upcoming=upcoming,\n                           limit_upcoming=limit_upcoming,\n                           limit_processed=limit_processed,\n                           dtfmt=dtfmt)", "response": "Serve the status page of the capture agent."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nserve the preview image with the given id", "response": "def serve_image(image_id):\n    '''Serve the preview image with the given id\n    '''\n    try:\n        preview_dir = config()['capture']['preview_dir']\n        filepath = config()['capture']['preview'][image_id]\n        filepath = filepath.replace('{{previewdir}}', preview_dir)\n        filepath = os.path.abspath(filepath)\n        if os.path.isfile(filepath):\n            directory, filename = filepath.rsplit('/', 1)\n            return send_from_directory(directory, filename)\n    except (IndexError, KeyError):\n        pass\n    return '', 404"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nintercepting sigterm and terminate all processes.", "response": "def sigterm_handler(signum, frame):\n    '''Intercept sigterm and terminate all processes.\n    '''\n    sigint_handler(signum, frame)\n    for process in multiprocessing.active_children():\n        process.terminate()\n    sys.exit(0)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_ical(vcal):\n    '''Parse Opencast schedule iCalendar file and return events as dict\n    '''\n    vcal = vcal.replace('\\r\\n ', '').replace('\\r\\n\\r\\n', '\\r\\n')\n    vevents = vcal.split('\\r\\nBEGIN:VEVENT\\r\\n')\n    del(vevents[0])\n    events = []\n    for vevent in vevents:\n        event = {}\n        for line in vevent.split('\\r\\n'):\n            line = line.split(':', 1)\n            key = line[0].lower()\n            if len(line) <= 1 or key == 'end':\n                continue\n            if key.startswith('dt'):\n                event[key] = unix_ts(dateutil.parser.parse(line[1]))\n                continue\n            if not key.startswith('attach'):\n                event[key] = line[1]\n                continue\n            # finally handle attachments\n            event['attach'] = event.get('attach', [])\n            attachment = {}\n            for x in [x.split('=') for x in line[0].split(';')]:\n                if x[0].lower() in ['fmttype', 'x-apple-filename']:\n                    attachment[x[0].lower()] = x[1]\n            attachment['data'] = b64decode(line[1]).decode('utf-8')\n            event['attach'].append(attachment)\n        events.append(event)\n    return events", "response": "Parse Opencast schedule iCalendar file and return events as dict\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_schedule():\n    '''Try to load schedule from the Matterhorn core. Returns a valid schedule\n    or None on failure.\n    '''\n    params = {'agentid': config()['agent']['name'].encode('utf8')}\n    lookahead = config()['agent']['cal_lookahead'] * 24 * 60 * 60\n    if lookahead:\n        params['cutoff'] = str((timestamp() + lookahead) * 1000)\n    uri = '%s/calendars?%s' % (config()['service-scheduler'][0],\n                               urlencode(params))\n    try:\n        vcal = http_request(uri)\n    except pycurl.error as e:\n        logger.error('Could not get schedule: %s' % e)\n        return\n\n    try:\n        cal = parse_ical(vcal.decode('utf-8'))\n    except Exception:\n        logger.error('Could not parse ical')\n        logger.error(traceback.format_exc())\n        return\n    db = get_session()\n    db.query(UpcomingEvent).delete()\n    for event in cal:\n        # Ignore events that have already ended\n        if event['dtend'] <= timestamp():\n            continue\n        e = UpcomingEvent()\n        e.start = event['dtstart']\n        e.end = event['dtend']\n        e.uid = event.get('uid')\n        e.title = event.get('summary')\n        e.set_data(event)\n        db.add(e)\n    db.commit()", "response": "Try to load a schedule from the Matterhorn core. Returns a valid schedule\n    or None on failure."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef control_loop():\n    '''Main loop, retrieving the schedule.\n    '''\n    set_service_status(Service.SCHEDULE, ServiceStatus.BUSY)\n    notify.notify('READY=1')\n    while not terminate():\n        notify.notify('WATCHDOG=1')\n        # Try getting an updated schedule\n        get_schedule()\n        session = get_session()\n        next_event = session.query(UpcomingEvent)\\\n                            .filter(UpcomingEvent.end > timestamp())\\\n                            .order_by(UpcomingEvent.start)\\\n                            .first()\n        if next_event:\n            logger.info('Next scheduled recording: %s',\n                        datetime.fromtimestamp(next_event.start))\n            notify.notify('STATUS=Next scheduled recording: %s' %\n                          datetime.fromtimestamp(next_event.start))\n        else:\n            logger.info('No scheduled recording')\n            notify.notify('STATUS=No scheduled recording')\n        session.close()\n\n        next_update = timestamp() + config()['agent']['update_frequency']\n        while not terminate() and timestamp() < next_update:\n            time.sleep(0.1)\n\n    logger.info('Shutting down schedule service')\n    set_service_status(Service.SCHEDULE, ServiceStatus.STOPPED)", "response": "Main loop for the schedule."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef control_loop():\n    '''Main loop, updating the capture agent state.\n    '''\n    set_service_status(Service.AGENTSTATE, ServiceStatus.BUSY)\n    notify.notify('READY=1')\n    notify.notify('STATUS=Running')\n    while not terminate():\n        notify.notify('WATCHDOG=1')\n        update_agent_state()\n\n        next_update = timestamp() + config()['agent']['update_frequency']\n        while not terminate() and timestamp() < next_update:\n            time.sleep(0.1)\n\n    logger.info('Shutting down agentstate service')\n    set_service_status(Service.AGENTSTATE, ServiceStatus.STOPPED)", "response": "Main loop for the agentstate service."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a jsonapi error object", "response": "def make_error_response(error, status=500):\n    ''' Return a response with a jsonapi error object\n    '''\n    content = {\n        'errors': [{\n            'status': status,\n            'title': error\n        }]\n    }\n    return make_response(jsonify(content), status)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_data_response(data, status=200):\n    ''' Return a response with a list of jsonapi data objects\n    '''\n    content = {'data': ensurelist(data)}\n    return make_response(jsonify(content), status)", "response": "Return a response with a list of jsonapi data objects"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nserve a json representation of internal agentstate as meta data", "response": "def internal_state():\n    '''Serve a json representation of internal agentstate as meta data\n    '''\n    data = {'services': {\n        'capture': ServiceStatus.str(get_service_status(Service.CAPTURE)),\n        'ingest': ServiceStatus.str(get_service_status(Service.INGEST)),\n        'schedule': ServiceStatus.str(get_service_status(Service.SCHEDULE)),\n        'agentstate': ServiceStatus.str(get_service_status(Service.AGENTSTATE))\n        }\n    }\n    return make_response(jsonify({'meta': data}))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nserves a JSON representation of events", "response": "def events():\n    '''Serve a JSON representation of events\n    '''\n    db = get_session()\n    upcoming_events = db.query(UpcomingEvent)\\\n                        .order_by(UpcomingEvent.start)\n    recorded_events = db.query(RecordedEvent)\\\n                        .order_by(RecordedEvent.start.desc())\n\n    result = [event.serialize() for event in upcoming_events]\n    result += [event.serialize() for event in recorded_events]\n    return make_data_response(result)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a specific events JSON", "response": "def event(uid):\n    '''Return a specific events JSON\n    '''\n    db = get_session()\n    event = db.query(RecordedEvent).filter(RecordedEvent.uid == uid).first() \\\n        or db.query(UpcomingEvent).filter(UpcomingEvent.uid == uid).first()\n\n    if event:\n        return make_data_response(event.serialize())\n    return make_error_response('No event with specified uid', 404)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_event(uid):\n    '''Delete a specific event identified by its uid. Note that only recorded\n    events can be deleted. Events in the buffer for upcoming events are\n    regularly replaced anyway and a manual removal could have unpredictable\n    effects.\n\n    Use ?hard=true parameter to delete the recorded files on disk as well.\n\n    Returns 204 if the action was successful.\n    Returns 404 if event does not exist\n    '''\n    logger.info('deleting event %s via api', uid)\n    db = get_session()\n    events = db.query(RecordedEvent).filter(RecordedEvent.uid == uid)\n    if not events.count():\n        return make_error_response('No event with specified uid', 404)\n    hard_delete = request.args.get('hard', 'false')\n    if hard_delete == 'true':\n        logger.info('deleting recorded files at %s', events[0].directory())\n        shutil.rmtree(events[0].directory())\n    events.delete()\n    db.commit()\n    return make_response('', 204)", "response": "Delete an event identified by the uid."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmodify an event specified by its uid.", "response": "def modify_event(uid):\n    '''Modify an event specified by its uid. The modifications for the event\n    are expected as JSON with the content type correctly set in the request.\n\n    Note that this method works for recorded events only. Upcoming events part\n    of the scheduler cache cannot be modified.\n    '''\n    try:\n        data = request.get_json()['data'][0]\n        if data['type'] != 'event' or data['id'] != uid:\n            return make_error_response('Invalid data', 400)\n        # Check attributes\n        for key in data['attributes'].keys():\n            if key not in ('status', 'start', 'end'):\n                return make_error_response('Invalid data', 400)\n        # Check new status\n        new_status = data['attributes'].get('status')\n        if new_status:\n            new_status = new_status.upper().replace(' ', '_')\n            data['attributes']['status'] = int(getattr(Status, new_status))\n    except Exception:\n        return make_error_response('Invalid data', 400)\n\n    db = get_session()\n    event = db.query(RecordedEvent).filter(RecordedEvent.uid == uid).first()\n    if not event:\n        return make_error_response('No event with specified uid', 404)\n    event.start = data['attributes'].get('start', event.start)\n    event.end = data['attributes'].get('end', event.end)\n    event.status = data['attributes'].get('status', event.status)\n    logger.debug('Updating event %s via api', uid)\n    db.commit()\n    return make_data_response(event.serialize())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextract the set of configuration parameters from the properties attached to the schedule", "response": "def get_config_params(properties):\n    '''Extract the set of configuration parameters from the properties attached\n    to the schedule\n    '''\n    param = []\n    wdef = ''\n    for prop in properties.split('\\n'):\n        if prop.startswith('org.opencastproject.workflow.config'):\n            key, val = prop.split('=', 1)\n            key = key.split('.')[-1]\n            param.append((key, val))\n        elif prop.startswith('org.opencastproject.workflow.definition'):\n            wdef = prop.split('=', 1)[-1]\n    return wdef, param"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ingest(event):\n    '''Ingest a finished recording to the Opencast server.\n    '''\n    # Update status\n    set_service_status(Service.INGEST, ServiceStatus.BUSY)\n    notify.notify('STATUS=Uploading')\n    recording_state(event.uid, 'uploading')\n    update_event_status(event, Status.UPLOADING)\n\n    # Select ingest service\n    # The ingest service to use is selected at random from the available\n    # ingest services to ensure that not every capture agent uses the same\n    # service at the same time\n    service = config('service-ingest')\n    service = service[randrange(0, len(service))]\n    logger.info('Selecting ingest service to use: ' + service)\n\n    # create mediapackage\n    logger.info('Creating new mediapackage')\n    mediapackage = http_request(service + '/createMediaPackage')\n\n    # extract workflow_def, workflow_config and add DC catalogs\n    prop = 'org.opencastproject.capture.agent.properties'\n    dcns = 'http://www.opencastproject.org/xsd/1.0/dublincore/'\n    for attachment in event.get_data().get('attach'):\n        data = attachment.get('data')\n        if attachment.get('x-apple-filename') == prop:\n            workflow_def, workflow_config = get_config_params(data)\n\n        # Check for dublincore catalogs\n        elif attachment.get('fmttype') == 'application/xml' and dcns in data:\n            name = attachment.get('x-apple-filename', '').rsplit('.', 1)[0]\n            logger.info('Adding %s DC catalog' % name)\n            fields = [('mediaPackage', mediapackage),\n                      ('flavor', 'dublincore/%s' % name),\n                      ('dublinCore', data.encode('utf-8'))]\n            mediapackage = http_request(service + '/addDCCatalog', fields)\n\n    # add track\n    for (flavor, track) in event.get_tracks():\n        logger.info('Adding track ({0} -> {1})'.format(flavor, track))\n        track = track.encode('ascii', 'ignore')\n        fields = [('mediaPackage', mediapackage), ('flavor', flavor),\n                  ('BODY1', (pycurl.FORM_FILE, track))]\n        mediapackage = http_request(service + '/addTrack', fields)\n\n    # ingest\n    logger.info('Ingest recording')\n    fields = [('mediaPackage', mediapackage)]\n    if workflow_def:\n        fields.append(('workflowDefinitionId', workflow_def))\n    if event.uid:\n        fields.append(('workflowInstanceId',\n                       event.uid.encode('ascii', 'ignore')))\n    fields += workflow_config\n    mediapackage = http_request(service + '/ingest', fields)\n\n    # Update status\n    recording_state(event.uid, 'upload_finished')\n    update_event_status(event, Status.FINISHED_UPLOADING)\n    notify.notify('STATUS=Running')\n    set_service_status_immediate(Service.INGEST, ServiceStatus.IDLE)\n\n    logger.info('Finished ingest')", "response": "Ingest a finished recording to the Opencast server."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef safe_start_ingest(event):\n    '''Start a capture process but make sure to catch any errors during this\n    process, log them but otherwise ignore them.\n    '''\n    try:\n        ingest(event)\n    except Exception:\n        logger.error('Something went wrong during the upload')\n        logger.error(traceback.format_exc())\n        # Update state if something went wrong\n        recording_state(event.uid, 'upload_error')\n        update_event_status(event, Status.FAILED_UPLOADING)\n        set_service_status_immediate(Service.INGEST, ServiceStatus.IDLE)", "response": "Start a capture process but make sure to catch any errors during this\n    log them but otherwise ignore them."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nintercept sigterm and terminate all processes.", "response": "def sigterm_handler(signum, frame):\n    '''Intercept sigterm and terminate all processes.\n    '''\n    if captureproc and captureproc.poll() is None:\n        captureproc.terminate()\n    terminate(True)\n    sys.exit(0)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstarting the capture process creating all necessary files and directories as well as ingesting the captured files if no backup mode is configured.", "response": "def start_capture(upcoming_event):\n    '''Start the capture process, creating all necessary files and directories\n    as well as ingesting the captured files if no backup mode is configured.\n    '''\n    logger.info('Start recording')\n\n    # First move event to recording_event table\n    db = get_session()\n    event = db.query(RecordedEvent)\\\n              .filter(RecordedEvent.uid == upcoming_event.uid)\\\n              .filter(RecordedEvent.start == upcoming_event.start)\\\n              .first()\n    if not event:\n        event = RecordedEvent(upcoming_event)\n        db.add(event)\n        db.commit()\n\n    try_mkdir(config()['capture']['directory'])\n    os.mkdir(event.directory())\n\n    # Set state\n    update_event_status(event, Status.RECORDING)\n    recording_state(event.uid, 'capturing')\n    set_service_status_immediate(Service.CAPTURE, ServiceStatus.BUSY)\n\n    # Recording\n    tracks = recording_command(event)\n    event.set_tracks(tracks)\n    db.commit()\n\n    # Set status\n    update_event_status(event, Status.FINISHED_RECORDING)\n    recording_state(event.uid, 'capture_finished')\n    set_service_status_immediate(Service.CAPTURE, ServiceStatus.IDLE)\n\n    logger.info('Finished recording')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstarts a capture process but make sure to catch any errors during this log them but otherwise ignore them.", "response": "def safe_start_capture(event):\n    '''Start a capture process but make sure to catch any errors during this\n    process, log them but otherwise ignore them.\n    '''\n    try:\n        start_capture(event)\n    except Exception:\n        logger.error('Recording failed')\n        logger.error(traceback.format_exc())\n        # Update state\n        recording_state(event.uid, 'capture_error')\n        update_event_status(event, Status.FAILED_RECORDING)\n        set_service_status_immediate(Service.CAPTURE, ServiceStatus.IDLE)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef recording_command(event):\n    '''Run the actual command to record the a/v material.\n    '''\n    conf = config('capture')\n    # Prepare command line\n    cmd = conf['command']\n    cmd = cmd.replace('{{time}}', str(event.remaining_duration(timestamp())))\n    cmd = cmd.replace('{{dir}}', event.directory())\n    cmd = cmd.replace('{{name}}', event.name())\n    cmd = cmd.replace('{{previewdir}}', conf['preview_dir'])\n\n    # Signal configuration\n    sigterm_time = conf['sigterm_time']\n    sigkill_time = conf['sigkill_time']\n    sigcustom_time = conf['sigcustom_time']\n    sigcustom_time = 0 if sigcustom_time < 0 else event.end + sigcustom_time\n    sigterm_time = 0 if sigterm_time < 0 else event.end + sigterm_time\n    sigkill_time = 0 if sigkill_time < 0 else event.end + sigkill_time\n\n    # Launch capture command\n    logger.info(cmd)\n    args = shlex.split(cmd)\n    DEVNULL = getattr(subprocess, 'DEVNULL', os.open(os.devnull, os.O_RDWR))\n    captureproc = subprocess.Popen(args, stdin=DEVNULL)\n    hasattr(subprocess, 'DEVNULL') or os.close(DEVNULL)\n\n    # Set systemd status\n    notify.notify('STATUS=Capturing')\n\n    # Check process\n    while captureproc.poll() is None:\n        notify.notify('WATCHDOG=1')\n        if sigcustom_time and timestamp() > sigcustom_time:\n            logger.info(\"Sending custom signal to capture process\")\n            captureproc.send_signal(conf['sigcustom'])\n            sigcustom_time = 0  # send only once\n        if sigterm_time and timestamp() > sigterm_time:\n            logger.info(\"Terminating capture process\")\n            captureproc.terminate()\n            sigterm_time = 0  # send only once\n        elif sigkill_time and timestamp() > sigkill_time:\n            logger.warning(\"Killing capture process\")\n            captureproc.kill()\n            sigkill_time = 0  # send only once\n        time.sleep(0.1)\n\n    # Remove preview files:\n    for preview in conf['preview']:\n        try:\n            os.remove(preview.replace('{{previewdir}}', conf['preview_dir']))\n        except OSError:\n            logger.warning('Could not remove preview files')\n            logger.warning(traceback.format_exc())\n\n    # Check process for errors\n    exitcode = config()['capture']['exit_code']\n    if captureproc.poll() > 0 and captureproc.returncode != exitcode:\n        raise RuntimeError('Recording failed (%i)' % captureproc.returncode)\n\n    # Reset systemd status\n    notify.notify('STATUS=Waiting')\n\n    # Return [(flavor,path),\u2026]\n    files = (f.replace('{{dir}}', event.directory()) for f in conf['files'])\n    files = (f.replace('{{name}}', event.name()) for f in files)\n    return list(zip(conf['flavors'], files))", "response": "Run the actual command to record the a / v material."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef control_loop():\n    '''Main loop of the capture agent, retrieving and checking the schedule as\n    well as starting the capture process if necessry.\n    '''\n    set_service_status(Service.CAPTURE, ServiceStatus.IDLE)\n    notify.notify('READY=1')\n    notify.notify('STATUS=Waiting')\n    while not terminate():\n        notify.notify('WATCHDOG=1')\n        # Get next recording\n        event = get_session().query(UpcomingEvent)\\\n                             .filter(UpcomingEvent.start <= timestamp())\\\n                             .filter(UpcomingEvent.end > timestamp())\\\n                             .first()\n        if event:\n            safe_start_capture(event)\n        time.sleep(1.0)\n    logger.info('Shutting down capture service')\n    set_service_status(Service.CAPTURE, ServiceStatus.STOPPED)", "response": "Main loop of the capture agent."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef render_to_fragment(self, request, **kwargs):\n        fragment = Fragment(TEST_HTML)\n        fragment.add_javascript(TEST_JS)\n        fragment.add_css(TEST_CSS)\n        return fragment", "response": "Returns a simple fragment that can be used to render the page."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef resources(self):\n        seen = set()\n        # seen.add always returns None, so 'not seen.add(x)' is always True,\n        # but will only be called if the value is not already in seen (because\n        # 'and' short-circuits)\n        return [x for x in self._resources if x not in seen and not seen.add(x)]", "response": "Returns list of unique FragmentResource s by order of first appearance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_dict(self):\n        return {\n            'content': self.content,\n            'resources': [r._asdict() for r in self.resources],  # pylint: disable=W0212\n            'js_init_fn': self.js_init_fn,\n            'js_init_version': self.js_init_version,\n            'json_init_args': self.json_init_args\n        }", "response": "Returns the fragment in a dictionary representation."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_dict(cls, pods):\n        frag = cls()\n        frag.content = pods['content']\n        frag._resources = [FragmentResource(**d) for d in pods['resources']]  # pylint: disable=protected-access\n        frag.js_init_fn = pods['js_init_fn']\n        frag.js_init_version = pods['js_init_version']\n        frag.json_init_args = pods['json_init_args']\n        return frag", "response": "Returns a new Fragment from a dictionary representation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_content(self, content):\n        assert isinstance(content, six.text_type)\n        self.content += content", "response": "Add content to the content of the current tag."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a resource needed by this Fragment.", "response": "def add_resource(self, text, mimetype, placement=None):\n        \"\"\"\n        Add a resource needed by this Fragment.\n\n        Other helpers, such as :func:`add_css` or :func:`add_javascript` are\n        more convenient for those common types of resource.\n\n        `text`: the actual text of this resource, as a unicode string.\n\n        `mimetype`: the MIME type of the resource.\n\n        `placement`: where on the page the resource should be placed:\n\n            None: let the Fragment choose based on the MIME type.\n\n            \"head\": put this resource in the ``<head>`` of the page.\n\n            \"foot\": put this resource at the end of the ``<body>`` of the\n            page.\n\n        \"\"\"\n        if not placement:\n            placement = self._default_placement(mimetype)\n        res = FragmentResource('text', text, mimetype, placement)\n        self._resources.append(res)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a resource by URL needed by this Fragment.", "response": "def add_resource_url(self, url, mimetype, placement=None):\n        \"\"\"\n        Add a resource by URL needed by this Fragment.\n\n        Other helpers, such as :func:`add_css_url` or\n        :func:`add_javascript_url` are more convenent for those common types of\n        resource.\n\n        `url`: the URL to the resource.\n\n        Other parameters are as defined for :func:`add_resource`.\n        \"\"\"\n        if not placement:\n            placement = self._default_placement(mimetype)\n        self._resources.append(FragmentResource('url', url, mimetype, placement))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nregistering a Javascript function to initialize the Javascript resources.", "response": "def initialize_js(self, js_func, json_args=None):\n        \"\"\"\n        Register a Javascript function to initialize the Javascript resources.\n\n        `js_func` is the name of a Javascript function defined by one of the\n        Javascript resources.  As part of setting up the browser's runtime\n        environment, the function will be invoked, passing a runtime object\n        and a DOM element.\n        \"\"\"\n        self.js_init_fn = js_func\n        self.js_init_version = JS_API_VERSION\n        if json_args:\n            self.json_init_args = json_args"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting some resource HTML for this Fragment. placement is head or foot.", "response": "def resources_to_html(self, placement):\n        \"\"\"\n        Get some resource HTML for this Fragment.\n\n        `placement` is \"head\" or \"foot\".\n\n        Returns a unicode string, the HTML for the head or foot of the page.\n        \"\"\"\n        # - non url js could be wrapped in an anonymous function\n        # - non url css could be rewritten to match the wrapper tag\n\n        return '\\n'.join(\n            self.resource_to_html(resource)\n            for resource in self.resources\n            if resource.placement == placement\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef resource_to_html(resource):\n        if resource.mimetype == \"text/css\":\n            if resource.kind == \"text\":\n                return u\"<style type='text/css'>\\n%s\\n</style>\" % resource.data\n            elif resource.kind == \"url\":\n                return u\"<link rel='stylesheet' href='%s' type='text/css'>\" % resource.data\n            else:\n                raise Exception(\"Unrecognized resource kind %r\" % resource.kind)\n\n        elif resource.mimetype == \"application/javascript\":\n            if resource.kind == \"text\":\n                return u\"<script>\\n%s\\n</script>\" % resource.data\n            elif resource.kind == \"url\":\n                return u\"<script src='%s' type='application/javascript'></script>\" % resource.data\n            else:\n                raise Exception(\"Unrecognized resource kind %r\" % resource.kind)\n\n        elif resource.mimetype == \"text/html\":\n            assert resource.kind == \"text\"\n            return resource.data\n\n        else:\n            raise Exception(\"Unrecognized mimetype %r\" % resource.mimetype)", "response": "Converts a resource into an html tag."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(self, request, *args, **kwargs):\n        fragment = self.render_to_fragment(request, **kwargs)\n        response_format = request.GET.get('format') or request.POST.get('format') or 'html'\n        if response_format == 'json' or WEB_FRAGMENT_RESPONSE_TYPE in request.META.get('HTTP_ACCEPT', 'text/html'):\n            return JsonResponse(fragment.to_dict())\n        else:\n            return self.render_standalone_response(request, fragment, **kwargs)", "response": "Render a fragment to HTML or return JSON describing it based on the request."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef render_standalone_response(self, request, fragment, **kwargs):  # pylint: disable=unused-argument\n        if fragment is None:\n            return HttpResponse(status=204)\n\n        html = self.render_to_standalone_html(request, fragment, **kwargs)\n        return HttpResponse(html)", "response": "Renders a standalone page as a response for the specified fragment."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrender the specified fragment to HTML for a standalone page.", "response": "def render_to_standalone_html(self, request, fragment, **kwargs):  # pylint: disable=unused-argument\n        \"\"\"\n        Render the specified fragment to HTML for a standalone page.\n        \"\"\"\n        template = get_template(STANDALONE_TEMPLATE_NAME)\n        context = {\n            'head_html': fragment.head_html(),\n            'body_html': fragment.body_html(),\n            'foot_html': fragment.foot_html(),\n        }\n        return template.render(context)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef calc(pvalues, lamb):\n\n    m = len(pvalues)\n    pi0 = (pvalues > lamb).sum() / ((1 - lamb)*m)\n\n    pFDR = np.ones(m)\n    print(\"pFDR    y        Pr     fastPow\")\n    for i in range(m):\n        y = pvalues[i]\n        Pr = max(1, m - i) / float(m)\n        pFDR[i] = (pi0 * y) / (Pr * (1 - math.pow(1-y, m)))\n        print(i, pFDR[i], y, Pr, 1.0 - math.pow(1-y, m))\n\n\n    num_null = pi0*m\n    num_alt = m - num_null\n    num_negs = np.array(range(m))\n    num_pos = m - num_negs\n    pp = num_pos / float(m)\n\n    qvalues = np.ones(m)\n    qvalues[0] = pFDR[0]\n    for i in range(m-1):\n        qvalues[i+1] = min(qvalues[i], pFDR[i+1])\n\n    sens = ((1.0 - qvalues) * num_pos) / num_alt\n    sens[sens > 1.0] = 1.0\n\n    df = pd.DataFrame(dict(\n        pvalue=pvalues,\n        qvalue=qvalues,\n        FDR=pFDR,\n        percentile_positive=pp,\n        sens=sens\n    ))\n\n    df[\"svalue\"] = df.sens[::-1].cummax()[::-1]\n\n    return df, num_null, m", "response": "calculate the fastPow of a set of pvalues"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unwrap_self_for_multiprocessing(arg):\n    (inst, method_name, args) = arg\n    return getattr(inst, method_name)(*args)", "response": "Unwrap the object with the given method."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting list or flattens n - dim array to 1 - dim array if possible", "response": "def to_one_dim_array(values, as_type=None):\n    \"\"\" Converts list or flattens n-dim array to 1-dim array if possible \"\"\"\n\n    if isinstance(values, (list, tuple)):\n        values = np.array(values, dtype=np.float32)\n    elif isinstance(values, pd.Series):\n        values = values.values\n    values = values.flatten()\n    assert values.ndim == 1, \"values has wrong dimension\"\n    if as_type is not None:\n        return values.astype(as_type)\n    return values"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef lookup_values_from_error_table(scores, err_df):\n    ix = find_nearest_matches(np.float32(err_df.cutoff.values), np.float32(scores))\n    return err_df.pvalue.iloc[ix].values, err_df.svalue.iloc[ix].values, err_df.pep.iloc[ix].values, err_df.qvalue.iloc[ix].values", "response": "Find matching p - value s - value pep - value and q - value for each score in scores"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef posterior_chromatogram_hypotheses_fast(experiment, prior_chrom_null):\n\n    tg_ids = experiment.df.tg_num_id.values\n    pp_values = 1-experiment.df[\"pep\"].values\n\n    current_tg_id = tg_ids[0]\n    scores = []\n    final_result = []\n    final_result_h0 = []\n    for i in range(tg_ids.shape[0]):\n\n        id_ = tg_ids[i]\n        if id_ != current_tg_id:\n\n            # Actual computation for a single transition group (chromatogram)\n            prior_pg_true = (1.0 - prior_chrom_null) / len(scores)\n            rr = single_chromatogram_hypothesis_fast(\n                np.array(scores), prior_chrom_null, prior_pg_true)\n            final_result.extend(rr[1:])\n            final_result_h0.extend(rr[0] for i in range(len(scores)))\n\n            # Reset for next cycle\n            scores = []\n            current_tg_id = id_\n\n        scores.append(1.0 - pp_values[i])\n\n    # Last cycle\n    prior_pg_true = (1.0 - prior_chrom_null) / len(scores)\n    rr = single_chromatogram_hypothesis_fast(np.array(scores), prior_chrom_null, prior_pg_true)\n    final_result.extend(rr[1:])\n    final_result_h0.extend([rr[0]] * len(scores))\n\n    return final_result, final_result_h0", "response": "Compute the posterior probabilities for each chromatogram and each peak in the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes empirical values identically to bioconductor or qvalue empPvals", "response": "def pemp(stat, stat0):\n    \"\"\" Computes empirical values identically to bioconductor/qvalue empPvals \"\"\"\n\n    assert len(stat0) > 0\n    assert len(stat) > 0\n\n    stat = np.array(stat)\n    stat0 = np.array(stat0)\n\n    m = len(stat)\n    m0 = len(stat0)\n\n    statc = np.concatenate((stat, stat0))\n    v = np.array([True] * m + [False] * m0)\n    perm = np.argsort(-statc, kind=\"mergesort\")  # reversed sort, mergesort is stable\n    v = v[perm]\n\n    u = np.where(v)[0]\n    p = (u - np.arange(m)) / float(m0)\n\n    # ranks can be fractional, we round down to the next integer, ranking returns values starting\n    # with 1, not 0:\n    ranks = np.floor(scipy.stats.rankdata(-stat)).astype(int) - 1\n    p = p[ranks]\n    p[p <= 1.0 / m0] = 1.0 / m0\n\n    return p"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nestimate the pi0 according to bioconductor or qvalue reference implementation.", "response": "def pi0est(p_values, lambda_ = np.arange(0.05,1.0,0.05), pi0_method = \"smoother\", smooth_df = 3, smooth_log_pi0 = False):\n    \"\"\" Estimate pi0 according to bioconductor/qvalue \"\"\"\n\n    # Compare to bioconductor/qvalue reference implementation\n    # import rpy2\n    # import rpy2.robjects as robjects\n    # from rpy2.robjects import pandas2ri\n    # pandas2ri.activate()\n\n    # smoothspline=robjects.r('smooth.spline')\n    # predict=robjects.r('predict')\n\n    p = np.array(p_values)\n\n    rm_na = np.isfinite(p)\n    p = p[rm_na]\n    m = len(p)\n    ll = 1\n    if isinstance(lambda_, np.ndarray ):\n        ll = len(lambda_)\n        lambda_ = np.sort(lambda_)\n\n    if (min(p) < 0 or max(p) > 1):\n        raise click.ClickException(\"p-values not in valid range [0,1].\")\n    elif (ll > 1 and ll < 4):\n        raise click.ClickException(\"If lambda_ is not predefined (one value), at least four data points are required.\")\n    elif (np.min(lambda_) < 0 or np.max(lambda_) >= 1):\n        raise click.ClickException(\"Lambda must be within [0,1)\")\n\n    if (ll == 1):\n        pi0 = np.mean(p >= lambda_)/(1 - lambda_)\n        pi0_lambda = pi0\n        pi0 = np.minimum(pi0, 1)\n        pi0Smooth = False\n    else:\n        pi0 = []\n        for l in lambda_:\n            pi0.append(np.mean(p >= l)/(1 - l))\n        pi0_lambda = pi0\n\n        if (pi0_method == \"smoother\"):\n            if smooth_log_pi0:\n                pi0 = np.log(pi0)\n                spi0 = sp.interpolate.UnivariateSpline(lambda_, pi0, k=smooth_df)\n                pi0Smooth = np.exp(spi0(lambda_))\n                # spi0 = smoothspline(lambda_, pi0, df = smooth_df) # R reference function\n                # pi0Smooth = np.exp(predict(spi0, x = lambda_).rx2('y')) # R reference function\n            else:\n                spi0 = sp.interpolate.UnivariateSpline(lambda_, pi0, k=smooth_df)\n                pi0Smooth = spi0(lambda_)\n                # spi0 = smoothspline(lambda_, pi0, df = smooth_df) # R reference function\n                # pi0Smooth = predict(spi0, x = lambda_).rx2('y')  # R reference function\n            pi0 = np.minimum(pi0Smooth[ll-1],1)\n        elif (pi0_method == \"bootstrap\"):\n            minpi0 = np.percentile(pi0,0.1)\n            W = []\n            for l in lambda_:\n                W.append(np.sum(p >= l))\n            mse = (np.array(W) / (np.power(m,2) * np.power((1 - lambda_),2))) * (1 - np.array(W) / m) + np.power((pi0 - minpi0),2)\n            pi0 = np.minimum(pi0[np.argmin(mse)],1)\n            pi0Smooth = False\n        else:\n            raise click.ClickException(\"pi0_method must be one of 'smoother' or 'bootstrap'.\")\n    if (pi0<=0):\n        raise click.ClickException(\"The estimated pi0 <= 0. Check that you have valid p-values or use a different range of lambda.\")\n\n    return {'pi0': pi0, 'pi0_lambda': pi0_lambda, 'lambda_': lambda_, 'pi0_smooth': pi0Smooth}"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nestimates local FDR for a given set of p - values.", "response": "def lfdr(p_values, pi0, trunc = True, monotone = True, transf = \"probit\", adj = 1.5, eps = np.power(10.0,-8)):\n    \"\"\" Estimate local FDR / posterior error probability from p-values according to bioconductor/qvalue \"\"\"\n    p = np.array(p_values)\n\n    # Compare to bioconductor/qvalue reference implementation\n    # import rpy2\n    # import rpy2.robjects as robjects\n    # from rpy2.robjects import pandas2ri\n    # pandas2ri.activate()\n\n    # density=robjects.r('density')\n    # smoothspline=robjects.r('smooth.spline')\n    # predict=robjects.r('predict')\n\n    # Check inputs\n    lfdr_out = p\n    rm_na = np.isfinite(p)\n    p = p[rm_na]\n\n    if (min(p) < 0 or max(p) > 1):\n        raise click.ClickException(\"p-values not in valid range [0,1].\")\n    elif (pi0 < 0 or pi0 > 1):\n        raise click.ClickException(\"pi0 not in valid range [0,1].\")\n\n    # Local FDR method for both probit and logit transformations\n    if (transf == \"probit\"):\n        p = np.maximum(p, eps)\n        p = np.minimum(p, 1-eps)\n        x = scipy.stats.norm.ppf(p, loc=0, scale=1)\n\n        # R-like implementation\n        bw = bw_nrd0(x)\n        myd = KDEUnivariate(x)\n        myd.fit(bw=adj*bw, gridsize = 512)\n        splinefit = sp.interpolate.splrep(myd.support, myd.density)\n        y = sp.interpolate.splev(x, splinefit)\n        # myd = density(x, adjust = 1.5) # R reference function\n        # mys = smoothspline(x = myd.rx2('x'), y = myd.rx2('y')) # R reference function\n        # y = predict(mys, x).rx2('y') # R reference function\n\n        lfdr = pi0 * scipy.stats.norm.pdf(x) / y\n    elif (transf == \"logit\"):\n        x = np.log((p + eps) / (1 - p + eps))\n\n        # R-like implementation\n        bw = bw_nrd0(x)\n        myd = KDEUnivariate(x)\n        myd.fit(bw=adj*bw, gridsize = 512)\n\n        splinefit = sp.interpolate.splrep(myd.support, myd.density)\n        y = sp.interpolate.splev(x, splinefit)\n        # myd = density(x, adjust = 1.5) # R reference function\n        # mys = smoothspline(x = myd.rx2('x'), y = myd.rx2('y')) # R reference function\n        # y = predict(mys, x).rx2('y') # R reference function\n\n        dx = np.exp(x) / np.power((1 + np.exp(x)),2)\n        lfdr = (pi0 * dx) / y\n    else:\n        raise click.ClickException(\"Invalid local FDR method.\")\n\n    if (trunc):\n        lfdr[lfdr > 1] = 1\n    if (monotone):\n        lfdr = lfdr[p.ravel().argsort()]\n        for i in range(1,len(x)):\n            if (lfdr[i] < lfdr[i - 1]):\n                lfdr[i] = lfdr[i - 1]\n        lfdr = lfdr[scipy.stats.rankdata(p,\"min\")-1]\n\n    lfdr_out[rm_na] = lfdr\n    return lfdr_out"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates artificial cutoff sample points from given range of cutoff values in df", "response": "def final_err_table(df, num_cut_offs=51):\n    \"\"\" Create artificial cutoff sample points from given range of cutoff\n    values in df, number of sample points is 'num_cut_offs' \"\"\"\n\n    cutoffs = df.cutoff.values\n    min_ = min(cutoffs)\n    max_ = max(cutoffs)\n    # extend max_ and min_ by 5 % of full range\n    margin = (max_ - min_) * 0.05\n    sampled_cutoffs = np.linspace(min_ - margin, max_ + margin, num_cut_offs, dtype=np.float32)\n\n    # find best matching row index for each sampled cut off:\n    ix = find_nearest_matches(np.float32(df.cutoff.values), sampled_cutoffs)\n\n    # create sub dataframe:\n    sampled_df = df.iloc[ix].copy()\n    sampled_df.cutoff = sampled_cutoffs\n    # remove 'old' index from input df:\n    sampled_df.reset_index(inplace=True, drop=True)\n\n    return sampled_df"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef summary_err_table(df, qvalues=[0, 0.01, 0.02, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5]):\n\n    qvalues = to_one_dim_array(qvalues)\n    # find best matching fows in df for given qvalues:\n    ix = find_nearest_matches(np.float32(df.qvalue.values), qvalues)\n    # extract sub table\n    df_sub = df.iloc[ix].copy()\n    # remove duplicate hits, mark them with None / NAN:\n    for i_sub, (i0, i1) in enumerate(zip(ix, ix[1:])):\n        if i1 == i0:\n            df_sub.iloc[i_sub + 1, :] = None\n    # attach q values column\n    df_sub.qvalue = qvalues\n    # remove old index from original df:\n    df_sub.reset_index(inplace=True, drop=True)\n    return df_sub[['qvalue','pvalue','svalue','pep','fdr','fnr','fpr','tp','tn','fp','fn','cutoff']]", "response": "Summary error table for some typical q - values"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef error_statistics(target_scores, decoy_scores, parametric, pfdr, pi0_lambda, pi0_method = \"smoother\", pi0_smooth_df = 3, pi0_smooth_log_pi0 = False, compute_lfdr = False, lfdr_trunc = True, lfdr_monotone = True, lfdr_transf = \"probit\", lfdr_adj = 1.5, lfdr_eps = np.power(10.0,-8)):\n\n    target_scores = to_one_dim_array(target_scores)\n    target_scores = np.sort(target_scores[~np.isnan(target_scores)])\n\n    decoy_scores = to_one_dim_array(decoy_scores)\n    decoy_scores = np.sort(decoy_scores[~np.isnan(decoy_scores)])\n\n    # compute p-values using decoy scores\n    if parametric:\n        # parametric\n        target_pvalues = pnorm(target_scores, decoy_scores)\n    else:\n        # non-parametric\n        target_pvalues = pemp(target_scores, decoy_scores)\n\n    # estimate pi0\n    pi0 = pi0est(target_pvalues, pi0_lambda, pi0_method, pi0_smooth_df, pi0_smooth_log_pi0)\n\n    # compute q-value\n    target_qvalues = qvalue(target_pvalues, pi0['pi0'], pfdr)\n\n    # compute other metrics\n    metrics = stat_metrics(target_pvalues, pi0['pi0'], pfdr)\n\n    # generate main statistics table\n    error_stat = pd.DataFrame({'cutoff': target_scores, 'pvalue': target_pvalues, 'qvalue': target_qvalues, 'svalue': metrics['svalue'], 'tp': metrics['tp'], 'fp': metrics['fp'], 'tn': metrics['tn'], 'fn': metrics['fn'], 'fpr': metrics['fpr'], 'fdr': metrics['fdr'], 'fnr': metrics['fnr']})\n\n    # compute lfdr / PEP\n    if compute_lfdr:\n        error_stat['pep'] = lfdr(target_pvalues, pi0['pi0'], lfdr_trunc, lfdr_monotone, lfdr_transf, lfdr_adj, lfdr_eps)\n\n    return error_stat, pi0", "response": "Generates a dataframe of error statistics for a set of target scores and decoy scores."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_cutoff(tt_scores, td_scores, cutoff_fdr, parametric, pfdr, pi0_lambda, pi0_method, pi0_smooth_df, pi0_smooth_log_pi0):\n\n    error_stat, pi0 = error_statistics(tt_scores, td_scores, parametric, pfdr, pi0_lambda, pi0_method, pi0_smooth_df, pi0_smooth_log_pi0, False)\n    if not len(error_stat):\n        raise click.ClickException(\"Too little data for calculating error statistcs.\")\n    i0 = (error_stat.qvalue - cutoff_fdr).abs().idxmin()\n    cutoff = error_stat.iloc[i0][\"cutoff\"]\n    return cutoff", "response": "Find the cutoff target score for specified false discovery rate fdr."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef score(infile, outfile, classifier, xgb_autotune, apply_weights, xeval_fraction, xeval_num_iter, ss_initial_fdr, ss_iteration_fdr, ss_num_iter, ss_main_score, group_id, parametric, pfdr, pi0_lambda, pi0_method, pi0_smooth_df, pi0_smooth_log_pi0, lfdr_truncate, lfdr_monotone, lfdr_transformation, lfdr_adj, lfdr_eps, level, ipf_max_peakgroup_rank, ipf_max_peakgroup_pep, ipf_max_transition_isotope_overlap, ipf_min_transition_sn, tric_chromprob, threads, test):\n\n    if outfile is None:\n        outfile = infile\n    else:\n        outfile = outfile\n\n    # Prepare XGBoost-specific parameters\n    xgb_hyperparams = {'autotune': xgb_autotune, 'autotune_num_rounds': 10, 'num_boost_round': 100, 'early_stopping_rounds': 10, 'test_size': 0.33}\n\n    xgb_params = {'eta': 0.3, 'gamma': 0, 'max_depth': 6, 'min_child_weight': 1, 'subsample': 1, 'colsample_bytree': 1, 'colsample_bylevel': 1, 'colsample_bynode': 1, 'lambda': 1, 'alpha': 0, 'scale_pos_weight': 1, 'silent': 1, 'objective': 'binary:logitraw', 'nthread': 1, 'eval_metric': 'auc'}\n\n    xgb_params_space = {'eta': hp.uniform('eta', 0.0, 0.3), 'gamma': hp.uniform('gamma', 0.0, 0.5), 'max_depth': hp.quniform('max_depth', 2, 8, 1), 'min_child_weight': hp.quniform('min_child_weight', 1, 5, 1), 'subsample': 1, 'colsample_bytree': 1, 'colsample_bylevel': 1, 'colsample_bynode': 1, 'lambda': hp.uniform('lambda', 0.0, 1.0), 'alpha': hp.uniform('alpha', 0.0, 1.0), 'scale_pos_weight': 1.0, 'silent': 1, 'objective': 'binary:logitraw', 'nthread': 1, 'eval_metric': 'auc'}\n\n    if not apply_weights:\n        PyProphetLearner(infile, outfile, classifier, xgb_hyperparams, xgb_params, xgb_params_space, xeval_fraction, xeval_num_iter, ss_initial_fdr, ss_iteration_fdr, ss_num_iter, ss_main_score, group_id, parametric, pfdr, pi0_lambda, pi0_method, pi0_smooth_df, pi0_smooth_log_pi0, lfdr_truncate, lfdr_monotone, lfdr_transformation, lfdr_adj, lfdr_eps, level, ipf_max_peakgroup_rank, ipf_max_peakgroup_pep, ipf_max_transition_isotope_overlap, ipf_min_transition_sn, tric_chromprob, threads, test).run()\n    else:\n        PyProphetWeightApplier(infile, outfile, classifier, xgb_hyperparams, xgb_params, xgb_params_space, xeval_fraction, xeval_num_iter, ss_initial_fdr, ss_iteration_fdr, ss_num_iter, ss_main_score, group_id, parametric, pfdr, pi0_lambda, pi0_method, pi0_smooth_df, pi0_smooth_log_pi0, lfdr_truncate, lfdr_monotone, lfdr_transformation, lfdr_adj, lfdr_eps, level, ipf_max_peakgroup_rank, ipf_max_peakgroup_pep, ipf_max_transition_isotope_overlap, ipf_min_transition_sn, tric_chromprob, threads, test, apply_weights).run()", "response": "Score a file using semi - supervised learning and error - rate estimation."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninfers peptidoforms after scoring of MS1 MS2 and transition - level data.", "response": "def ipf(infile, outfile, ipf_ms1_scoring, ipf_ms2_scoring, ipf_h0, ipf_grouped_fdr, ipf_max_precursor_pep, ipf_max_peakgroup_pep, ipf_max_precursor_peakgroup_pep, ipf_max_transition_pep):\n    \"\"\"\n    Infer peptidoforms after scoring of MS1, MS2 and transition-level data.\n    \"\"\"\n\n    if outfile is None:\n        outfile = infile\n    else:\n        outfile = outfile\n\n    infer_peptidoforms(infile, outfile, ipf_ms1_scoring, ipf_ms2_scoring, ipf_h0, ipf_grouped_fdr, ipf_max_precursor_pep, ipf_max_peakgroup_pep, ipf_max_precursor_peakgroup_pep, ipf_max_transition_pep)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef peptide(infile, outfile, context, parametric, pfdr, pi0_lambda, pi0_method, pi0_smooth_df, pi0_smooth_log_pi0, lfdr_truncate, lfdr_monotone, lfdr_transformation, lfdr_adj, lfdr_eps):\n\n    if outfile is None:\n        outfile = infile\n    else:\n        outfile = outfile\n\n    infer_peptides(infile, outfile, context, parametric, pfdr, pi0_lambda, pi0_method, pi0_smooth_df, pi0_smooth_log_pi0, lfdr_truncate, lfdr_monotone, lfdr_transformation, lfdr_adj, lfdr_eps)", "response": "Infer peptides and conduct error - rate estimation in different contexts."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef protein(infile, outfile, context, parametric, pfdr, pi0_lambda, pi0_method, pi0_smooth_df, pi0_smooth_log_pi0, lfdr_truncate, lfdr_monotone, lfdr_transformation, lfdr_adj, lfdr_eps):\n\n    if outfile is None:\n        outfile = infile\n    else:\n        outfile = outfile\n\n    infer_proteins(infile, outfile, context, parametric, pfdr, pi0_lambda, pi0_method, pi0_smooth_df, pi0_smooth_log_pi0, lfdr_truncate, lfdr_monotone, lfdr_transformation, lfdr_adj, lfdr_eps)", "response": "Infer proteins and conduct error - rate estimation in different contexts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreducing the file to minimum for global scoring", "response": "def reduce(infile, outfile):\n    \"\"\"\n    Reduce scored PyProphet file to minimum for global scoring\n    \"\"\"\n\n    if outfile is None:\n        outfile = infile\n    else:\n        outfile = outfile\n\n    reduce_osw(infile, outfile)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmerges multiple OSW files and output the result.", "response": "def merge(infiles, outfile, same_run, templatefile):\n    \"\"\"\n    Merge multiple OSW files and (for large experiments, it is recommended to subsample first).\n    \"\"\"\n\n    if len(infiles) < 1:\n        raise click.ClickException(\"At least one PyProphet input file needs to be provided.\")\n\n    merge_osw(infiles, outfile, templatefile, same_run)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef backpropagate(infile, outfile, apply_scores):\n\n    if outfile is None:\n        outfile = infile\n    else:\n        outfile = outfile\n\n    backpropagate_oswr(infile, outfile, apply_scores)", "response": "Backpropagate multi - run peptide and protein scores to single files."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef export(infile, outfile, format, outcsv, transition_quantification, max_transition_pep, ipf, ipf_max_peptidoform_pep, max_rs_peakgroup_qvalue, peptide, max_global_peptide_qvalue, protein, max_global_protein_qvalue):\n    if format == \"score_plots\":\n        export_score_plots(infile)\n    else:\n        if outfile is None:\n            if outcsv:\n                outfile = infile.split(\".osw\")[0] + \".csv\"\n            else:\n                outfile = infile.split(\".osw\")[0] + \".tsv\"\n        else:\n            outfile = outfile\n\n        export_tsv(infile, outfile, format, outcsv, transition_quantification, max_transition_pep, ipf, ipf_max_peptidoform_pep, max_rs_peakgroup_qvalue, peptide, max_global_peptide_qvalue, protein, max_global_protein_qvalue)", "response": "Export the ISA - specific data to a file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexports Compound TSV or CSV tables", "response": "def export_compound(infile, outfile, format, outcsv, max_rs_peakgroup_qvalue):\n    \"\"\"\n    Export Compound TSV/CSV tables\n    \"\"\"\n    if format == \"score_plots\":\n        export_score_plots(infile)\n    else:\n        if outfile is None:\n            if outcsv:\n                outfile = infile.split(\".osw\")[0] + \".csv\"\n            else:\n                outfile = infile.split(\".osw\")[0] + \".tsv\"\n        else:\n            outfile = outfile\n\n        export_compound_tsv(infile, outfile, format, outcsv, max_rs_peakgroup_qvalue)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsearches for groups in the specified set of parameters.", "response": "def search_groups(self, **kwargs):\n        \"\"\"\n        Returns a list of restclients.GroupReference objects matching the\n        passed parameters. Valid parameters are:\n            name: parts_of_name\n                name may include the wild-card (*) character.\n            stem: group_stem\n            member: member netid\n            owner: admin netid\n            instructor: instructor netid\n                stem=\"course\" will be set when this parameter is passed.\n            student: student netid\n                stem=\"course\" will be set when this parameter is passed.\n            affiliate: affiliate_name\n            type: search_type\n                Values are 'direct' to search for direct membership and\n                'effective' to search for effective memberships. Default is\n                direct membership.\n            scope: search_scope\n                Values are 'one' to limit results to one level of stem name\n                and 'all' to return all groups.\n        \"\"\"\n        kwargs = dict((k.lower(), kwargs[k].lower()) for k in kwargs)\n        if 'type' in kwargs and (\n                kwargs['type'] != 'direct' and kwargs['type'] != 'effective'):\n            del(kwargs['type'])\n\n        if 'scope' in kwargs and (\n                kwargs['scope'] != 'one' and kwargs['scope'] != 'all'):\n            del(kwargs['scope'])\n\n        if \"instructor\" in kwargs or \"student\" in kwargs:\n            kwargs[\"stem\"] = \"course\"\n\n        url = \"{}/search?{}\".format(self.API, urlencode(kwargs))\n\n        data = self._get_resource(url)\n\n        groups = []\n        for datum in data.get('data', []):\n            group = GroupReference()\n            group.uwregid = datum.get('regid')\n            group.display_name = datum.get('displayName')\n            group.name = datum.get('id')\n            group.url = datum.get('url')\n            groups.append(group)\n\n        return groups"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_group_by_id(self, group_id):\n        self._valid_group_id(group_id)\n\n        url = \"{}/group/{}\".format(self.API, group_id)\n\n        data = self._get_resource(url)\n\n        return self._group_from_json(data.get(\"data\"))", "response": "Returns a restclients. Group object for the group identified by the passed group ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_group(self, group):\n        self._valid_group_id(group.id)\n\n        body = {\"data\": group.json_data()}\n        url = \"{}/group/{}\".format(self.API, group.name)\n\n        data = self._put_resource(url, headers={}, body=body)\n\n        return self._group_from_json(data.get(\"data\"))", "response": "Creates a group from the passed restclients. Group object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeleting the group identified by the passed group ID.", "response": "def delete_group(self, group_id):\n        \"\"\"\n        Deletes the group identified by the passed group ID.\n        \"\"\"\n        self._valid_group_id(group_id)\n\n        url = \"{}/group/{}\".format(self.API, group_id)\n\n        self._delete_resource(url)\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of restclients. GroupMember objects for the passed group ID.", "response": "def get_members(self, group_id):\n        \"\"\"\n        Returns a list of restclients.GroupMember objects for the group\n        identified by the passed group ID.\n        \"\"\"\n        self._valid_group_id(group_id)\n\n        url = \"{}/group/{}/member\".format(self.API, group_id)\n\n        data = self._get_resource(url)\n\n        members = []\n        for datum in data.get(\"data\"):\n            members.append(self._group_member_from_json(datum))\n        return members"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_members(self, group_id, members):\n        self._valid_group_id(group_id)\n\n        body = {\"data\": [m.json_data() for m in members]}\n        headers = {\"If-Match\": \"*\"}\n        url = \"{}/group/{}/member\".format(self.API, group_id)\n\n        data = self._put_resource(url, headers, body)\n\n        errors = data.get(\"errors\", [])\n        if len(errors):\n            return errors[0].get(\"notFound\", [])\n        return []", "response": "Updates the membership of the group represented by the passed group id. Returns a list of members not found."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_effective_member_count(self, group_id):\n        self._valid_group_id(group_id)\n\n        url = \"{}/group/{}/effective_member?view=count\".format(self.API,\n                                                               group_id)\n\n        data = self._get_resource(url)\n\n        count = data.get(\"data\").get(\"count\")\n        return int(count)", "response": "Returns a count of effective members for the group identified by the passed group ID."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_effective_member(self, group_id, netid):\n        self._valid_group_id(group_id)\n\n        # GWS doesn't accept EPPNs on effective member checks, for UW users\n        netid = re.sub('@washington.edu', '', netid)\n\n        url = \"{}/group/{}/effective_member/{}\".format(self.API,\n                                                       group_id,\n                                                       netid)\n\n        try:\n            data = self._get_resource(url)\n            return True  # 200\n        except DataFailureException as ex:\n            if ex.status == 404:\n                return False\n            else:\n                raise", "response": "Returns True if the netid is in the group False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmodify the conf. py file to include the necessary documentation.", "response": "def modify_conf():\n    \"\"\"\n    pip install redbaron\n    \"\"\"\n    import redbaron\n    import ubelt as ub\n    conf_path = 'docs/conf.py'\n\n    source = ub.readfrom(conf_path)\n    red = redbaron.RedBaron(source)\n\n    # Insert custom extensions\n    extra_extensions = [\n        '\"sphinxcontrib.napoleon\"'\n    ]\n\n    ext_node = red.find('name', value='extensions').parent\n    ext_node.value.value.extend(extra_extensions)\n\n    # Overwrite theme to read-the-docs\n    theme_node = red.find('name', value='html_theme').parent\n    theme_node.value.value = '\"sphinx_rtd_theme\"'\n\n    ub.writeto(conf_path, red.dumps())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_version():\n    from os.path import dirname, join\n    import ast\n    modname = setupkw['name']\n    init_fpath = join(dirname(__file__), modname, '__init__.py')\n    with open(init_fpath) as file_:\n        sourcecode = file_.read()\n    pt = ast.parse(sourcecode)\n    class VersionVisitor(ast.NodeVisitor):\n        def visit_Assign(self, node):\n            for target in node.targets:\n                if target.id == '__version__':\n                    self.version = node.value.s\n    visitor = VersionVisitor()\n    visitor.visit(pt)\n    return visitor.version", "response": "Statically parse the version number from the __init__. py file"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a 3D array in a group.", "response": "def create_dataset(self, name, shape=None, dtype=None, data=None,\n                       sparse_format=None, indptr_dtype=np.int64, indices_dtype=np.int32,\n                       **kwargs):\n        \"\"\"Create 3 datasets in a group to represent the sparse array.\n\n        Parameters\n        ----------\n        sparse_format:\n        \"\"\"\n        if isinstance(data, Dataset):\n            assert sparse_format is None\n            group = self.create_group(name)\n            group.attrs['h5sparse_format'] = data.attrs['h5sparse_format']\n            group.attrs['h5sparse_shape'] = data.attrs['h5sparse_shape']\n            group.create_dataset('data', data=data.h5py_group['data'],\n                                 dtype=dtype, **kwargs)\n            group.create_dataset('indices', data=data.h5py_group['indices'],\n                                 dtype=indices_dtype, **kwargs)\n            group.create_dataset('indptr', data=data.h5py_group['indptr'],\n                                 dtype=indptr_dtype, **kwargs)\n        elif ss.issparse(data):\n            if sparse_format is not None:\n                format_class = get_format_class(sparse_format)\n                data = format_class(data)\n            group = self.create_group(name)\n            group.attrs['h5sparse_format'] = get_format_str(data)\n            group.attrs['h5sparse_shape'] = data.shape\n            group.create_dataset('data', data=data.data, dtype=dtype, **kwargs)\n            group.create_dataset('indices', data=data.indices, dtype=indices_dtype, **kwargs)\n            group.create_dataset('indptr', data=data.indptr, dtype=indptr_dtype, **kwargs)\n        elif data is None and sparse_format is not None:\n            format_class = get_format_class(sparse_format)\n            if dtype is None:\n                dtype = np.float64\n            if shape is None:\n                shape = (0, 0)\n            data = format_class(shape, dtype=dtype)\n            group = self.create_group(name)\n            group.attrs['h5sparse_format'] = get_format_str(data)\n            group.attrs['h5sparse_shape'] = data.shape\n            group.create_dataset('data', data=data.data, dtype=dtype, **kwargs)\n            group.create_dataset('indices', data=data.indices, dtype=indices_dtype, **kwargs)\n            group.create_dataset('indptr', data=data.indptr, dtype=indptr_dtype, **kwargs)\n        else:\n            # forward the arguments to h5py\n            assert sparse_format is None\n            return super(Group, self).create_dataset(\n                name, data=data, shape=shape, dtype=dtype, **kwargs)\n        return Dataset(group)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cli_decrypt(context, key):\n    with context.io_manager.with_stdout() as stdout:\n        with context.io_manager.with_stdin() as stdin:\n            crypt_type = stdin.read(1)\n            if crypt_type == AES256CBC:\n                for chunk in aes_decrypt(key, stdin):\n                    stdout.write(chunk)\n                stdout.flush()\n            else:\n                raise ReturnCode(\n                    'contents encrypted with unsupported type %r' % crypt_type)", "response": "Decrypt a single element of a context. io_manager s stdin and sends that to\nAGEREA."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef spawn(self, ident, func, *args, **kwargs):\n        if self._pool:\n            self._pool.spawn_n(self._spawner, ident, func, *args, **kwargs)\n            sleep()\n        else:\n            self._spawner(ident, func, *args, **kwargs)", "response": "Spawns a new entry in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_results(self):\n        try:\n            while True:\n                ident, value = self._queue.get(block=False)\n                self._results[ident] = value\n        except queue.Empty:\n            pass\n        return self._results", "response": "Returns a dict of the results currently available."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef client_path_to_os_path(self, client_path):\n        if os.path.sep == '/':\n            return client_path\n        return client_path.replace(os.path.sep, '-').replace('/', os.path.sep)", "response": "Converts a client path into the operating system s path by replacing any instances of '/' with '-'."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert an operating system path into a client path by replacing any instances of os. path. sep with -.", "response": "def os_path_to_client_path(self, os_path):\n        \"\"\"\n        Converts an operating system path into a client path by\n        replacing instances of os.path.sep with '/'.\n\n        Note: If the client path contains any instances of '/'\n        already, they will be replaced with '-'.\n        \"\"\"\n        if os.path.sep == '/':\n            return os_path\n        return os_path.replace('/', '-').replace(os.path.sep, '/')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a file - like object based on the s stdin and the stdin - root.", "response": "def get_stdin(self, os_path=None, skip_sub_command=False):\n        \"\"\"\n        Returns a stdin-suitable file-like object based on the\n        optional os_path and optionally skipping any configured\n        sub-command.\n        \"\"\"\n        sub_command = None if skip_sub_command else self.stdin_sub_command\n        inn, path = self._get_in_and_path(\n            self.stdin, self.stdin_root, sub_command, os_path)\n        if hasattr(inn, 'stdout'):\n            return inn.stdout\n        return inn"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a file - like object based on the stdout - suitable file - like object based on the os_path.", "response": "def get_stdout(self, os_path=None, skip_sub_command=False):\n        \"\"\"\n        Returns a stdout-suitable file-like object based on the\n        optional os_path and optionally skipping any configured\n        sub-command.\n        \"\"\"\n        sub_command = None if skip_sub_command else self.stdout_sub_command\n        out, path = self._get_out_and_path(\n            self.stdout, self.stdout_root, sub_command, os_path)\n        if hasattr(out, 'stdin'):\n            return out.stdin\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_stderr(self, os_path=None, skip_sub_command=False):\n        sub_command = None if skip_sub_command else self.stderr_sub_command\n        out, path = self._get_out_and_path(\n            self.stderr, self.stderr_root, sub_command, os_path)\n        if hasattr(out, 'stdin'):\n            return out.stdin\n        return out", "response": "Returns a file - like object based on the stderr - root and stderr - sub - command."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a debug - output - suitable file - like object based on the optional os_path optional skip_sub_command.", "response": "def get_debug(self, os_path=None, skip_sub_command=False):\n        \"\"\"\n        Returns a debug-output-suitable file-like object based on the\n        optional os_path and optionally skipping any configured\n        sub-command.\n        \"\"\"\n        sub_command = None if skip_sub_command else self.debug_sub_command\n        out, path = self._get_out_and_path(\n            self.debug, self.debug_root, sub_command, os_path)\n        if hasattr(out, 'stdin'):\n            return out.stdin\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef with_stdin(self, os_path=None, skip_sub_command=False,\n                   disk_closed_callback=None):\n        \"\"\"\n        A context manager yielding a stdin-suitable file-like object\n        based on the optional os_path and optionally skipping any\n        configured sub-command.\n\n        :param os_path: Optional path to base the file-like object\n            on.\n        :param skip_sub_command: Set True to skip any configured\n            sub-command filter.\n        :param disk_closed_callback: If the backing of the file-like\n            object is an actual file that will be closed,\n            disk_closed_callback (if set) will be called with the\n            on-disk path just after closing it.\n        \"\"\"\n        sub_command = None if skip_sub_command else self.stdin_sub_command\n        inn, path = self._get_in_and_path(\n            self.stdin, self.stdin_root, sub_command, os_path)\n        try:\n            if hasattr(inn, 'stdout'):\n                yield inn.stdout\n            else:\n                yield inn\n        finally:\n            if hasattr(inn, 'stdout'):\n                self._close(inn.stdout)\n            self._wait(inn, path)\n            self._close(inn)\n            if disk_closed_callback and path:\n                disk_closed_callback(path)", "response": "A context manager that yields a file - like object that can be read from stdin."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef with_stderr(self, os_path=None, skip_sub_command=False,\n                    disk_closed_callback=None):\n        \"\"\"\n        A context manager yielding a stderr-suitable file-like object\n        based on the optional os_path and optionally skipping any\n        configured sub-command.\n\n        :param os_path: Optional path to base the file-like object\n            on.\n        :param skip_sub_command: Set True to skip any configured\n            sub-command filter.\n        :param disk_closed_callback: If the backing of the file-like\n            object is an actual file that will be closed,\n            disk_closed_callback (if set) will be called with the\n            on-disk path just after closing it.\n        \"\"\"\n        sub_command = None if skip_sub_command else self.stderr_sub_command\n        out, path = self._get_out_and_path(\n            self.stderr, self.stderr_root, sub_command, os_path)\n        try:\n            if hasattr(out, 'stdin'):\n                yield out.stdin\n            else:\n                yield out\n        finally:\n            if hasattr(out, 'stdin'):\n                self._close(out.stdin)\n            self._wait(out, path)\n            self._close(out)\n            if disk_closed_callback and path:\n                disk_closed_callback(path)", "response": "A context manager that yields a file - like object that will be written to stderr."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef with_debug(self, os_path=None, skip_sub_command=False,\n                   disk_closed_callback=None):\n        \"\"\"\n        A context manager yielding a debug-output-suitable file-like\n        object based on the optional os_path and optionally skipping\n        any configured sub-command.\n\n        :param os_path: Optional path to base the file-like object\n            on.\n        :param skip_sub_command: Set True to skip any configured\n            sub-command filter.\n        :param disk_closed_callback: If the backing of the file-like\n            object is an actual file that will be closed,\n            disk_closed_callback (if set) will be called with the\n            on-disk path just after closing it.\n        \"\"\"\n        sub_command = None if skip_sub_command else self.debug_sub_command\n        out, path = self._get_out_and_path(\n            self.debug, self.debug_root, sub_command, os_path)\n        try:\n            if hasattr(out, 'stdin'):\n                yield out.stdin\n            else:\n                yield out\n        finally:\n            if hasattr(out, 'stdin'):\n                self._close(out.stdin)\n            self._wait(out, path)\n            self._close(out)\n            if disk_closed_callback and path:\n                disk_closed_callback(path)", "response": "A context manager that yields a debug - output suitable file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cli_empty_account(context, yes_empty_account=False, until_empty=False):\n    if not yes_empty_account:\n        raise ReturnCode(\n            'called cli_empty_account without setting yes_empty_account=True')\n    marker = None\n    while True:\n        with context.client_manager.with_client() as client:\n            status, reason, headers, contents = client.get_account(\n                marker=marker, headers=context.headers, query=context.query,\n                cdn=context.cdn)\n        if status // 100 != 2:\n            if status == 404 and context.ignore_404:\n                return\n            raise ReturnCode('listing account: %s %s' % (status, reason))\n        if not contents:\n            if until_empty and marker:\n                marker = None\n                continue\n            break\n        for item in contents:\n            cli_delete(\n                context, item['name'], context.headers, recursive=True)\n        marker = item['name']", "response": "Delete all objects and containers in an account."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cli_empty_container(context, path, until_empty=False):\n    path = path.rstrip('/').decode('utf8')\n    conc = Concurrency(context.concurrency)\n\n    def check_conc():\n        for (exc_type, exc_value, exc_tb, result) in \\\n                six.itervalues(conc.get_results()):\n            if exc_value:\n                with context.io_manager.with_stderr() as fp:\n                    fp.write(str(exc_value))\n                    fp.write('\\n')\n                    fp.flush()\n\n    marker = None\n    while True:\n        with context.client_manager.with_client() as client:\n            status, reason, headers, contents = client.get_container(\n                path, marker=marker, headers=context.headers,\n                query=context.query, cdn=context.cdn)\n        if status // 100 != 2:\n            if status == 404 and context.ignore_404:\n                return\n            raise ReturnCode(\n                'listing container %r: %s %s' % (path, status, reason))\n        if not contents:\n            if until_empty and marker:\n                marker = None\n                continue\n            break\n        for item in contents:\n            newpath = '%s/%s' % (path, item['name'])\n            new_context = context.copy()\n            new_context.ignore_404 = True\n            check_conc()\n            conc.spawn(newpath, cli_delete, new_context, newpath)\n        marker = item['name']\n        conc.join()\n        check_conc()", "response": "Deletes all objects in the specified container."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndelete the item at the given path.", "response": "def cli_delete(context, path, body=None, recursive=False,\n               yes_empty_account=False, yes_delete_account=False,\n               until_empty=False):\n    \"\"\"\n    Deletes the item (account, container, or object) at the path.\n\n    See :py:mod:`swiftly.cli.delete` for context usage information.\n\n    See :py:class:`CLIDelete` for more information.\n\n    :param context: The :py:class:`swiftly.cli.context.CLIContext` to\n        use.\n    :param path: The path of the item (acount, container, or object)\n        to delete.\n    :param body: The body to send with the DELETE request. Bodies are\n        not normally sent with DELETE requests, but this can be\n        useful with bulk deletes for instance.\n    :param recursive: If True and the item is an account or\n        container, deletes will be issued for any containing items as\n        well. This does one pass at the deletion; so if objects revert\n        to previous versions or if new objects otherwise arise during\n        the process, the container(s) may not be empty once done. Set\n        `until_empty` to True if you want multiple passes to keep trying\n        to fully empty the containers.\n    :param until_empty: If True and recursive is True, this will cause\n        Swiftly to keep looping through the deletes until the containers\n        are completely empty. Useful if you have object versioning\n        turned on or otherwise have objects that seemingly reappear\n        after being deleted. It could also run forever if you have\n        something that's uploading objects at a faster rate than they\n        are deleted.\n    :param yes_empty_account: This must be set to True for\n        verification when the item is an account and recursive is\n        True.\n    :param yes_delete_account: This must be set to True for\n        verification when the item is an account and you really wish\n        a delete to be issued for the account itself.\n    \"\"\"\n    path = path.lstrip('/') if path else ''\n    if not path:\n        if yes_empty_account:\n            cli_empty_account(\n                context, yes_empty_account=yes_empty_account,\n                until_empty=until_empty)\n        if yes_delete_account:\n            with context.client_manager.with_client() as client:\n                status, reason, headers, contents = client.delete_account(\n                    headers=context.headers, query=context.query,\n                    cdn=context.cdn, body=body,\n                    yes_i_mean_delete_the_account=yes_delete_account)\n                if status // 100 != 2:\n                    if status == 404 and context.ignore_404:\n                        return\n                    raise ReturnCode(\n                        'deleting account: %s %s' % (status, reason))\n    elif '/' not in path.rstrip('/'):\n        path = path.rstrip('/')\n        if recursive:\n            cli_empty_container(context, path, until_empty=until_empty)\n        with context.client_manager.with_client() as client:\n            status, reason, headers, contents = client.delete_container(\n                path, headers=context.headers,\n                query=context.query, cdn=context.cdn, body=body)\n            if status // 100 != 2:\n                if status == 404 and context.ignore_404:\n                    return\n                raise ReturnCode(\n                    'deleting container %r: %s %s' % (path, status, reason))\n    else:\n        with context.client_manager.with_client() as client:\n            status, reason, headers, contents = client.delete_object(\n                *path.split('/', 1), headers=context.headers,\n                query=context.query, cdn=context.cdn, body=body)\n            if status // 100 != 2:\n                if status == 404 and context.ignore_404:\n                    return\n                raise ReturnCode(\n                    'deleting object %r: %s %s' % (path, status, reason))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninstance method decorator to convert an optional file keyword argument into an actual value, whether it be a passed value, a value obtained from an io_manager, or sys.stdout.", "response": "def _stdout_filed(func):\n    \"\"\"\n    Instance method decorator to convert an optional file keyword\n    argument into an actual value, whether it be a passed value, a\n    value obtained from an io_manager, or sys.stdout.\n    \"\"\"\n    def wrapper(self, file=None):\n        if file:\n            return func(self, file=file)\n        elif self.io_manager:\n            with self.io_manager.with_stdout() as stdout:\n                return func(self, file=stdout)\n        else:\n            return func(self, file=sys.stdout)\n    wrapper.__doc__ = func.__doc__\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninstancing method decorator to convert an optional file keyword argument into an actual value, whether it be a passed value, a value obtained from an io_manager, or sys.stderr.", "response": "def _stderr_filed(func):\n    \"\"\"\n    Instance method decorator to convert an optional file keyword\n    argument into an actual value, whether it be a passed value, a\n    value obtained from an io_manager, or sys.stderr.\n    \"\"\"\n    def wrapper(self, msg, file=None):\n        if file:\n            return func(self, msg, file=file)\n        elif self.io_manager:\n            with self.io_manager.with_stderr() as stderr:\n                return func(self, msg, file=stderr)\n        else:\n            return func(self, msg, file=sys.stderr)\n    wrapper.__doc__ = func.__doc__\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\noutputting the error message to the file if specified.", "response": "def error(self, msg, file=None):\n        \"\"\"\n        Outputs the error msg to the file if specified, or to the\n        io_manager's stderr if available, or to sys.stderr.\n        \"\"\"\n        self.error_encountered = True\n        file.write(self.error_prefix)\n        file.write(msg)\n        file.write('\\n')\n        file.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef exit(self, status=0, msg=None):\n        if msg:\n            self.error(msg)\n        sys.exit(status)", "response": "Immediately exits Python with the given status and optionally outputs the given msg."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprint help information to the file if specified or to sys. stdout.", "response": "def print_help(self, file=None):\n        \"\"\"\n        Outputs help information to the file if specified, or to the\n        io_manager's stdout if available, or to sys.stdout.\n        \"\"\"\n        optparse.OptionParser.print_help(self, file)\n        if self.raw_epilog:\n            file.write(self.raw_epilog)\n        file.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef print_usage(self, file=None):\n        optparse.OptionParser.print_usage(self, file)\n        file.flush()", "response": "Prints usage information to the file if specified or to sys. stdout."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef print_version(self, file=None):\n        optparse.OptionParser.print_version(self, file)\n        file.flush()", "response": "Prints version information to the file if specified or to sys. stdout."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the args and create a new context.", "response": "def parse_args_and_create_context(self, args):\n        \"\"\"\n        Helper method that will parse the args into options and\n        remaining args as well as create an initial\n        :py:class:`swiftly.cli.context.CLIContext`.\n\n        The new context will be a copy of\n        :py:attr:`swiftly.cli.cli.CLI.context` with the following\n        attributes added:\n\n        =======================  ===================================\n        muted_account_headers    The headers to omit when outputting\n                                 account headers.\n        muted_container_headers  The headers to omit when outputting\n                                 container headers.\n        muted_object_headers     The headers to omit when outputting\n                                 object headers.\n        =======================  ===================================\n\n        :returns: options, args, context\n        \"\"\"\n        original_args = args\n        try:\n            options, args = self.option_parser.parse_args(args)\n        except UnboundLocalError:\n            # Happens sometimes with an error handler that doesn't raise its\n            # own exception. We'll catch the error below with\n            # error_encountered.\n            pass\n        if self.option_parser.error_encountered:\n            if '-?' in original_args or '-h' in original_args or \\\n                    '--help' in original_args:\n                self.option_parser.print_help()\n            raise ReturnCode()\n        if options.help:\n            self.option_parser.print_help()\n            raise ReturnCode()\n        if self.min_args is not None and len(args) < self.min_args:\n            raise ReturnCode(\n                'requires at least %s args.' % self.min_args)\n        if self.max_args is not None and len(args) > self.max_args:\n            raise ReturnCode(\n                'requires no more than %s args.' % self.max_args)\n        context = self.cli.context.copy()\n        context.muted_account_headers = [\n            'accept-ranges', 'content-length', 'content-type', 'date']\n        context.muted_container_headers = [\n            'accept-ranges', 'content-length', 'content-type', 'date']\n        context.muted_object_headers = ['accept-ranges', 'date']\n        return options, args, context"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef request(self, method, path, contents, headers, decode_json=False,\n                stream=False, query=None, cdn=False):\n        \"\"\"\n        Performs a direct HTTP request to the Swift service.\n\n        :param method: The request method ('GET', 'HEAD', etc.)\n        :param path: The request path.\n        :param contents: The body of the request. May be a string or\n            a file-like object.\n        :param headers: A dict of request headers and values.\n        :param decode_json: If set True, the response body will be\n            treated as JSON and decoded result returned instead of\n            the raw contents.\n        :param stream: If set True, the response body will return as\n            a file-like object; otherwise, the response body will be\n            read in its entirety and returned as a string. Overrides\n            decode_json.\n        :param query: A dict of query parameters and values to append\n            to the path.\n        :param cdn: If set True, the request will be sent to the CDN\n            management endpoint instead of the default storage\n            endpoint.\n        :returns: A tuple of (status, reason, headers, contents).\n\n            :status: An int for the HTTP status code.\n            :reason: The str for the HTTP status (ex: \"Ok\").\n            :headers: A dict with all lowercase keys of the HTTP\n                headers; if a header has multiple values, it will be\n                a list.\n            :contents: Depending on the decode_json and stream\n                settings, this will either be the raw response\n                string, the JSON decoded object, or a file-like\n                object.\n        \"\"\"\n        raise Exception('request method not implemented')", "response": "This method performs a direct HTTP request to the Swift service."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nheading the account and returns the results.", "response": "def head_account(self, headers=None, query=None, cdn=False):\n        \"\"\"\n        HEADs the account and returns the results. Useful headers\n        returned are:\n\n        =========================== =================================\n        x-account-bytes-used        Object storage used for the\n                                    account, in bytes.\n        x-account-container-count   The number of containers in the\n                                    account.\n        x-account-object-count      The number of objects in the\n                                    account.\n        =========================== =================================\n\n        Also, any user headers beginning with x-account-meta- are\n        returned.\n\n        These values can be delayed depending the Swift cluster.\n\n        :param headers: Additional headers to send with the request.\n        :param query: Set to a dict of query values to send on the\n            query string of the request.\n        :param cdn: If set True, the CDN management interface will be\n            used.\n        :returns: A tuple of (status, reason, headers, contents).\n\n            :status: is an int for the HTTP status code.\n            :reason: is the str for the HTTP status (ex: \"Ok\").\n            :headers: is a dict with all lowercase keys of the HTTP\n                headers; if a header has multiple values, it will be a\n                list.\n            :contents: is the str for the HTTP body.\n        \"\"\"\n        return self.request(\n            'HEAD', '', '', headers, query=query, cdn=cdn)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_account(self, headers=None, prefix=None, delimiter=None,\n                    marker=None, end_marker=None, limit=None, query=None,\n                    cdn=False, decode_json=True):\n        \"\"\"\n        GETs the account and returns the results. This is done to list\n        the containers for the account. Some useful headers are also\n        returned:\n\n        =========================== =================================\n        x-account-bytes-used        Object storage used for the\n                                    account, in bytes.\n        x-account-container-count   The number of containers in the\n                                    account.\n        x-account-object-count      The number of objects in the\n                                    account.\n        =========================== =================================\n\n        Also, any user headers beginning with x-account-meta- are\n        returned.\n\n        These values can be delayed depending the Swift cluster.\n\n        :param headers: Additional headers to send with the request.\n        :param prefix: The prefix container names must match to be\n            listed.\n        :param delimiter: The delimiter for the listing. Delimiters\n            indicate how far to progress through container names\n            before \"rolling them up\". For instance, a delimiter='.'\n            query on an account with the containers::\n\n                one.one\n                one.two\n                two\n                three.one\n\n            would return the JSON value of::\n\n                [{'subdir': 'one.'},\n                 {'count': 0, 'bytes': 0, 'name': 'two'},\n                 {'subdir': 'three.'}]\n\n            Using this with prefix can allow you to traverse a psuedo\n            hierarchy.\n        :param marker: Only container names after this marker will be\n            returned. Swift returns a limited number of containers\n            per request (often 10,000). To get the next batch of\n            names, you issue another query with the marker set to the\n            last name you received. You can continue to issue\n            requests until you receive no more names.\n        :param end_marker: Only container names before this marker will be\n            returned.\n        :param limit: Limits the size of the list returned per\n            request. The default and maximum depends on the Swift\n            cluster (usually 10,000).\n        :param query: Set to a dict of query values to send on the\n            query string of the request.\n        :param cdn: If set True, the CDN management interface will be\n            used.\n        :param decode_json: If set False, the usual decoding of the\n            JSON response will be skipped and the raw contents will\n            be returned instead.\n        :returns: A tuple of (status, reason, headers, contents).\n\n            :status: is an int for the HTTP status code.\n            :reason: is the str for the HTTP status (ex: \"Ok\").\n            :headers: is a dict with all lowercase keys of the HTTP\n                headers; if a header has multiple values, it will be a\n                list.\n            :contents: is the decoded JSON response or the raw str\n                for the HTTP body.\n        \"\"\"\n        query = dict(query or {})\n        query['format'] = 'json'\n        if prefix:\n            query['prefix'] = prefix\n        if delimiter:\n            query['delimiter'] = delimiter\n        if marker:\n            query['marker'] = marker\n        if end_marker:\n            query['end_marker'] = end_marker\n        if limit:\n            query['limit'] = limit\n        return self.request(\n            'GET', '', '', headers, decode_json=decode_json, query=query,\n            cdn=cdn)", "response": "This method is used to get the account and returns the containers for the account."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef put_account(self, headers=None, query=None, cdn=False, body=None):\n        return self.request(\n            'PUT', '', body or '', headers, query=query, cdn=cdn)", "response": "This is the actual PUT method that is used to send a request to the server to set the HTTP status code and the HTTP reason and the HTTP body."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nposts the account and returns the results.", "response": "def post_account(self, headers=None, query=None, cdn=False, body=None):\n        \"\"\"\n        POSTs the account and returns the results. This is usually\n        done to set X-Account-Meta-xxx headers. Note that any existing\n        X-Account-Meta-xxx headers will remain untouched. To remove an\n        X-Account-Meta-xxx header, send the header with an empty\n        string as its value.\n\n        :param headers: Additional headers to send with the request.\n        :param query: Set to a dict of query values to send on the\n            query string of the request.\n        :param cdn: If set True, the CDN management interface will be\n            used.\n        :param body: No known Swift POSTs take a body; but the option\n            is there for the future.\n        :returns: A tuple of (status, reason, headers, contents).\n\n            :status: is an int for the HTTP status code.\n            :reason: is the str for the HTTP status (ex: \"Ok\").\n            :headers: is a dict with all lowercase keys of the HTTP\n                headers; if a header has multiple values, it will be a\n                list.\n            :contents: is the str for the HTTP body.\n        \"\"\"\n        return self.request(\n            'POST', '', body or '', headers, query=query, cdn=cdn)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete_account(self, headers=None,\n                       yes_i_mean_delete_the_account=False, query=None,\n                       cdn=False, body=None):\n        \"\"\"\n        Sends a DELETE request to the account and returns the results.\n\n        With ``query['bulk-delete'] = ''`` this might mean a bulk\n        delete request where the body of the request is new-line\n        separated, url-encoded list of names to delete. Be careful\n        with this! One wrong move and you might mark your account for\n        deletion of you have the access to do so!\n\n        For a plain DELETE to the account, on clusters that support\n        it and, assuming you have permissions to do so, the account\n        will be marked as deleted and immediately begin removing the\n        objects from the cluster in the backgound.\n\n        THERE IS NO GOING BACK!\n\n        :param headers: Additional headers to send with the request.\n        :param yes_i_mean_delete_the_account: Set to True to verify\n            you really mean to delete the entire account. This is\n            required unless ``body and 'bulk-delete' in query``.\n        :param query: Set to a dict of query values to send on the\n            query string of the request.\n        :param cdn: If set True, the CDN management interface will be\n            used.\n        :param body: Some account DELETE requests, like the bulk\n            delete request, take a body.\n        :returns: A tuple of (status, reason, headers, contents).\n\n            :status: is an int for the HTTP status code.\n            :reason: is the str for the HTTP status (ex: \"Ok\").\n            :headers: is a dict with all lowercase keys of the HTTP\n                headers; if a header has multiple values, it will be\n                a list.\n            :contents: is the str for the HTTP body.\n        \"\"\"\n        if not yes_i_mean_delete_the_account and (\n                not body or not query or 'bulk-delete' not in query):\n            return (0, 'yes_i_mean_delete_the_account was not set to True', {},\n                    '')\n        return self.request(\n            'DELETE', '', body or '', headers, query=query, cdn=cdn)", "response": "Sends a DELETE request to the account and returns the results."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nhead the container and returns the results.", "response": "def head_container(self, container, headers=None, query=None, cdn=False):\n        \"\"\"\n        HEADs the container and returns the results. Useful headers\n        returned are:\n\n        =========================== =================================\n        x-container-bytes-used      Object storage used for the\n                                    container, in bytes.\n        x-container-object-count    The number of objects in the\n                                    container.\n        =========================== =================================\n\n        Also, any user headers beginning with x-container-meta- are\n        returned.\n\n        These values can be delayed depending the Swift cluster.\n\n        :param container: The name of the container.\n        :param headers: Additional headers to send with the request.\n        :param query: Set to a dict of query values to send on the\n            query string of the request.\n        :param cdn: If set True, the CDN management interface will be\n            used.\n        :returns: A tuple of (status, reason, headers, contents).\n\n            :status: is an int for the HTTP status code.\n            :reason: is the str for the HTTP status (ex: \"Ok\").\n            :headers: is a dict with all lowercase keys of the HTTP\n                headers; if a header has multiple values, it will be a\n                list.\n            :contents: is the str for the HTTP body.\n        \"\"\"\n        path = self._container_path(container)\n        return self.request(\n            'HEAD', path, '', headers, query=query, cdn=cdn)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_container(self, container, headers=None, prefix=None,\n                      delimiter=None, marker=None, end_marker=None,\n                      limit=None, query=None, cdn=False, decode_json=True):\n        \"\"\"\n        GETs the container and returns the results. This is done to\n        list the objects for the container. Some useful headers are\n        also returned:\n\n        =========================== =================================\n        x-container-bytes-used      Object storage used for the\n                                    container, in bytes.\n        x-container-object-count    The number of objects in the\n                                    container.\n        =========================== =================================\n\n        Also, any user headers beginning with x-container-meta- are\n        returned.\n\n        These values can be delayed depending the Swift cluster.\n\n        :param container: The name of the container.\n        :param headers: Additional headers to send with the request.\n        :param prefix: The prefix object names must match to be\n            listed.\n        :param delimiter: The delimiter for the listing. Delimiters\n            indicate how far to progress through object names before\n            \"rolling them up\". For instance, a delimiter='/' query on\n            an container with the objects::\n\n                one/one\n                one/two\n                two\n                three/one\n\n            would return the JSON value of::\n\n                [{'subdir': 'one/'},\n                 {'count': 0, 'bytes': 0, 'name': 'two'},\n                 {'subdir': 'three/'}]\n\n            Using this with prefix can allow you to traverse a psuedo\n            hierarchy.\n        :param marker: Only object names after this marker will be\n            returned. Swift returns a limited number of objects per\n            request (often 10,000). To get the next batch of names,\n            you issue another query with the marker set to the last\n            name you received. You can continue to issue requests\n            until you receive no more names.\n        :param end_marker: Only object names before this marker will be\n            returned.\n        :param limit: Limits the size of the list returned per\n            request. The default and maximum depends on the Swift\n            cluster (usually 10,000).\n        :param query: Set to a dict of query values to send on the\n            query string of the request.\n        :param cdn: If set True, the CDN management interface will be\n            used.\n        :param decode_json: If set False, the usual decoding of the\n            JSON response will be skipped and the raw contents will\n            be returned instead.\n        :returns: A tuple of (status, reason, headers, contents).\n\n            :status: is an int for the HTTP status code.\n            :reason: is the str for the HTTP status (ex: \"Ok\").\n            :headers: is a dict with all lowercase keys of the HTTP\n                headers; if a header has multiple values, it will be a\n                list.\n            :contents: is the decoded JSON response or the raw str\n                for the HTTP body.\n        \"\"\"\n        query = dict(query or {})\n        query['format'] = 'json'\n        if prefix:\n            query['prefix'] = prefix\n        if delimiter:\n            query['delimiter'] = delimiter\n        if marker:\n            query['marker'] = marker\n        if end_marker:\n            query['end_marker'] = end_marker\n        if limit:\n            query['limit'] = limit\n        return self.request(\n            'GET', self._container_path(container), '', headers,\n            decode_json=decode_json, query=query, cdn=cdn)", "response": "This method is used to get the container and returns the results."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef put_container(self, container, headers=None, query=None, cdn=False,\n                      body=None):\n        \"\"\"\n        PUTs the container and returns the results. This is usually\n        done to create new containers and can also be used to set\n        X-Container-Meta-xxx headers. Note that if the container\n        already exists, any existing X-Container-Meta-xxx headers will\n        remain untouched. To remove an X-Container-Meta-xxx header,\n        send the header with an empty string as its value.\n\n        :param container: The name of the container.\n        :param headers: Additional headers to send with the request.\n        :param query: Set to a dict of query values to send on the\n            query string of the request.\n        :param cdn: If set True, the CDN management interface will be\n            used.\n        :param body: Some container PUT requests, like the\n            extract-archive bulk upload request, take a body.\n        :returns: A tuple of (status, reason, headers, contents).\n\n            :status: is an int for the HTTP status code.\n            :reason: is the str for the HTTP status (ex: \"Ok\").\n            :headers: is a dict with all lowercase keys of the HTTP\n                headers; if a header has multiple values, it will be a\n                list.\n            :contents: is the str for the HTTP body.\n        \"\"\"\n        path = self._container_path(container)\n        return self.request(\n            'PUT', path, body or '', headers, query=query, cdn=cdn)", "response": "This method PUTs a container and returns the results."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nhead the object and returns the results.", "response": "def head_object(self, container, obj, headers=None, query=None, cdn=False):\n        \"\"\"\n        HEADs the object and returns the results.\n\n        :param container: The name of the container.\n        :param obj: The name of the object.\n        :param headers: Additional headers to send with the request.\n        :param query: Set to a dict of query values to send on the\n            query string of the request.\n        :param cdn: If set True, the CDN management interface will be\n            used.\n        :returns: A tuple of (status, reason, headers, contents).\n\n            :status: is an int for the HTTP status code.\n            :reason: is the str for the HTTP status (ex: \"Ok\").\n            :headers: is a dict with all lowercase keys of the HTTP\n                headers; if a header has multiple values, it will be a\n                list.\n            :contents: is the str for the HTTP body.\n        \"\"\"\n        path = self._object_path(container, obj)\n        return self.request(\n            'HEAD', path, '', headers, query=query, cdn=cdn)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the object from the object store.", "response": "def get_object(self, container, obj, headers=None, stream=True, query=None,\n                   cdn=False):\n        \"\"\"\n        GETs the object and returns the results.\n\n        :param container: The name of the container.\n        :param obj: The name of the object.\n        :param headers: Additional headers to send with the request.\n        :param stream: Indicates whether to stream the contents or\n            preread them fully and return them as a str. Default:\n            True to stream the contents. When streaming, contents\n            will have the standard file-like-object read function,\n            which accepts an optional size parameter to limit how\n            much data is read per call. When streaming is on, be\n            certain to fully read the contents before issuing another\n            request.\n        :param query: Set to a dict of query values to send on the\n            query string of the request.\n        :param cdn: If set True, the CDN management interface will be\n            used.\n        :returns: A tuple of (status, reason, headers, contents).\n\n            :status: is an int for the HTTP status code.\n            :reason: is the str for the HTTP status (ex: \"Ok\").\n            :headers: is a dict with all lowercase keys of the HTTP\n                headers; if a header has multiple values, it will be a\n                list.\n            :contents: if *stream* was True, *contents* is a\n                file-like-object of the contents of the HTTP body. If\n                *stream* was False, *contents* is just a simple str of\n                the HTTP body.\n        \"\"\"\n        path = self._object_path(container, obj)\n        return self.request(\n            'GET', path, '', headers, query=query, stream=stream, cdn=cdn)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef put_object(self, container, obj, contents, headers=None, query=None,\n                   cdn=False):\n        \"\"\"\n        PUTs the object and returns the results. This is used to\n        create or overwrite objects. X-Object-Meta-xxx can optionally\n        be sent to be stored with the object. Content-Type,\n        Content-Encoding and other standard HTTP headers can often\n        also be set, depending on the Swift cluster.\n\n        Note that you can set the ETag header to the MD5 sum of the\n        contents for extra verification the object was stored\n        correctly.\n\n        :param container: The name of the container.\n        :param obj: The name of the object.\n        :param contents: The contents of the object to store. This can\n            be a simple str, or a file-like-object with at least a\n            read function. If the file-like-object also has tell and\n            seek functions, the PUT can be reattempted on any server\n            error.\n        :param headers: Additional headers to send with the request.\n        :param query: Set to a dict of query values to send on the\n            query string of the request.\n        :param cdn: If set True, the CDN management interface will be\n            used.\n        :returns: A tuple of (status, reason, headers, contents).\n\n            :status: is an int for the HTTP status code.\n            :reason: is the str for the HTTP status (ex: \"Ok\").\n            :headers: is a dict with all lowercase keys of the HTTP\n                headers; if a header has multiple values, it will be a\n                list.\n            :contents: is the str for the HTTP body.\n        \"\"\"\n        path = self._object_path(container, obj)\n        return self.request(\n            'PUT', path, contents, headers, query=query, cdn=cdn)", "response": "This method PUTs the object in the specified container and returns the results."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npost an object to the specified container and returns the results.", "response": "def post_object(self, container, obj, headers=None, query=None, cdn=False,\n                    body=None):\n        \"\"\"\n        POSTs the object and returns the results. This is used to\n        update the object's header values. Note that all headers must\n        be sent with the POST, unlike the account and container POSTs.\n        With account and container POSTs, existing headers are\n        untouched. But with object POSTs, any existing headers are\n        removed. The full list of supported headers depends on the\n        Swift cluster, but usually include Content-Type,\n        Content-Encoding, and any X-Object-Meta-xxx headers.\n\n        :param container: The name of the container.\n        :param obj: The name of the object.\n        :param headers: Additional headers to send with the request.\n        :param query: Set to a dict of query values to send on the\n            query string of the request.\n        :param cdn: If set True, the CDN management interface will be\n            used.\n        :param body: No known Swift POSTs take a body; but the option\n            is there for the future.\n        :returns: A tuple of (status, reason, headers, contents).\n\n            :status: is an int for the HTTP status code.\n            :reason: is the str for the HTTP status (ex: \"Ok\").\n            :headers: is a dict with all lowercase keys of the HTTP\n                headers; if a header has multiple values, it will be a\n                list.\n            :contents: is the str for the HTTP body.\n        \"\"\"\n        path = self._object_path(container, obj)\n        return self.request(\n            'POST', path, body or '', headers, query=query, cdn=cdn)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cli_ping(context, prefix):\n    if not prefix:\n        prefix = 'swiftly-ping'\n    ping_ring_object_puts = collections.defaultdict(lambda: [])\n    ping_ring_object_gets = collections.defaultdict(lambda: [])\n    ping_ring_object_deletes = collections.defaultdict(lambda: [])\n    context.ping_begin = context.ping_begin_last = time.time()\n    container = prefix + '-' + uuid.uuid4().hex\n    objects = [uuid.uuid4().hex for x in moves.range(context.ping_count)]\n    conc = Concurrency(context.concurrency)\n    with context.client_manager.with_client() as client:\n        client.auth()\n        _cli_ping_status(context, 'auth', '-', None, None, None, None)\n        _cli_ping_status(context, 'account head', '-', *client.head_account())\n        _cli_ping_status(\n            context, 'container put', '-', *client.put_container(container))\n    if _cli_ping_objects(\n            context, 'put', conc, container, objects, _cli_ping_object_put,\n            ping_ring_object_puts):\n        with context.io_manager.with_stderr() as fp:\n            fp.write(\n                'ERROR put objects did not complete successfully due to '\n                'previous error; but continuing\\n')\n            fp.flush()\n    if _cli_ping_objects(\n            context, 'get', conc, container, objects, _cli_ping_object_get,\n            ping_ring_object_gets):\n        with context.io_manager.with_stderr() as fp:\n            fp.write(\n                'ERROR get objects did not complete successfully due to '\n                'previous error; but continuing\\n')\n            fp.flush()\n    if _cli_ping_objects(\n            context, 'delete', conc, container, objects,\n            _cli_ping_object_delete, ping_ring_object_deletes):\n        with context.io_manager.with_stderr() as fp:\n            fp.write(\n                'ERROR delete objects did not complete successfully due to '\n                'previous error; but continuing\\n')\n            fp.flush()\n    for attempt in moves.range(5):\n        if attempt:\n            sleep(2**attempt)\n        with context.client_manager.with_client() as client:\n            try:\n                _cli_ping_status(\n                    context, 'container delete', '-',\n                    *client.delete_container(container))\n                break\n            except ReturnCode as err:\n                with context.io_manager.with_stderr() as fp:\n                    fp.write(str(err))\n                    fp.write('\\n')\n                    fp.flush()\n    else:\n        with context.io_manager.with_stderr() as fp:\n            fp.write(\n                'ERROR could not confirm deletion of container due to '\n                'previous error; but continuing\\n')\n            fp.flush()\n    end = time.time()\n    with context.io_manager.with_stdout() as fp:\n        if context.graphite:\n            fp.write(\n                '%s.ping_overall %.02f %d\\n' % (\n                    context.graphite, end - context.ping_begin, time.time()))\n        if context.ping_verbose:\n            fp.write('% 6.02fs total\\n' % (end - context.ping_begin))\n        elif not context.graphite:\n            fp.write('%.02fs\\n' % (end - context.ping_begin))\n        fp.flush()\n    ping_ring_overall = collections.defaultdict(lambda: [])\n    _cli_ping_ring_report(context, ping_ring_object_puts, 'PUT')\n    for ip, timings in six.iteritems(ping_ring_object_puts):\n        ping_ring_overall[ip].extend(timings)\n    _cli_ping_ring_report(context, ping_ring_object_gets, 'GET')\n    for ip, timings in six.iteritems(ping_ring_object_gets):\n        ping_ring_overall[ip].extend(timings)\n    _cli_ping_ring_report(context, ping_ring_object_deletes, 'DELETE')\n    for ip, timings in six.iteritems(ping_ring_object_deletes):\n        ping_ring_overall[ip].extend(timings)\n    _cli_ping_ring_report(context, ping_ring_overall, 'overall')", "response": "Performs a ping test on the specified container."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nperform a GET on the account as a listing request. See :py:mod:`swiftly.cli.get` for context usage information. See :py:class:`CLIGet` for more information.", "response": "def cli_get_account_listing(context):\n    \"\"\"\n    Performs a GET on the account as a listing request.\n\n    See :py:mod:`swiftly.cli.get` for context usage information.\n\n    See :py:class:`CLIGet` for more information.\n    \"\"\"\n    limit = context.query.get('limit')\n    delimiter = context.query.get('delimiter')\n    prefix = context.query.get('prefix')\n    marker = context.query.get('marker')\n    end_marker = context.query.get('end_marker')\n    if context.raw:\n        with context.client_manager.with_client() as client:\n            status, reason, headers, contents = client.get_account(\n                decode_json=False, headers=context.headers, limit=limit,\n                marker=marker, end_marker=end_marker, query=context.query,\n                cdn=context.cdn)\n            if hasattr(contents, 'read'):\n                contents = contents.read()\n        if status // 100 != 2:\n            if status == 404 and context.ignore_404:\n                return\n            raise ReturnCode('listing account: %s %s' % (status, reason))\n        with context.io_manager.with_stdout() as fp:\n            if context.output_headers:\n                context.write_headers(\n                    fp, headers, context.muted_account_headers)\n            fp.write(contents)\n            fp.flush()\n        return\n    with context.client_manager.with_client() as client:\n        status, reason, headers, contents = client.get_account(\n            headers=context.headers, limit=limit, marker=marker,\n            end_marker=end_marker, query=context.query, cdn=context.cdn)\n        if status // 100 != 2:\n            if status == 404 and context.ignore_404:\n                return\n            if hasattr(contents, 'read'):\n                contents.read()\n            raise ReturnCode('listing account: %s %s' % (status, reason))\n    if context.output_headers and not context.all_objects:\n        with context.io_manager.with_stdout() as fp:\n            context.write_headers(\n                fp, headers, context.muted_account_headers)\n    while contents:\n        if context.all_objects:\n            new_context = context.copy()\n            new_context.query = dict(new_context.query)\n            for remove in (\n                    'limit', 'delimiter', 'prefix', 'marker', 'end_marker'):\n                if remove in new_context.query:\n                    del new_context.query[remove]\n            for item in contents:\n                if 'name' in item:\n                    new_path = item['name'].encode('utf8')\n                    cli_get_container_listing(new_context, new_path)\n        else:\n            with context.io_manager.with_stdout() as fp:\n                for item in contents:\n                    if context.full:\n                        fp.write('%13s %13s ' % (\n                            item.get('bytes', '-'),\n                            item.get('count', '-')))\n                    fp.write(item.get(\n                        'name', item.get('subdir')))\n                    fp.write('\\n')\n                fp.flush()\n        if limit:\n            break\n        marker = contents[-1].get('name', contents[-1].get('subdir', ''))\n        with context.client_manager.with_client() as client:\n            status, reason, headers, contents = client.get_account(\n                headers=context.headers, limit=limit, delimiter=delimiter,\n                prefix=prefix, end_marker=end_marker, marker=marker,\n                query=context.query, cdn=context.cdn)\n            if status // 100 != 2:\n                if status == 404 and context.ignore_404:\n                    return\n                if hasattr(contents, 'read'):\n                    contents.read()\n                raise ReturnCode('listing account: %s %s' % (status, reason))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cli_get_container_listing(context, path=None):\n    path = path.strip('/') if path else None\n    if not path or '/' in path:\n        raise ReturnCode(\n            'tried to get a container listing for non-container path %r' %\n            path)\n    context.suppress_container_name = True\n    limit = context.query.get('limit')\n    delimiter = context.query.get('delimiter')\n    prefix = context.query.get('prefix')\n    marker = context.query.get('marker')\n    end_marker = context.query.get('end_marker')\n    if context.raw:\n        with context.client_manager.with_client() as client:\n            status, reason, headers, contents = client.get_container(\n                path, decode_json=False, headers=context.headers, limit=limit,\n                marker=marker, end_marker=end_marker, query=context.query,\n                cdn=context.cdn)\n            if hasattr(contents, 'read'):\n                contents = contents.read()\n        if status // 100 != 2:\n            if status == 404 and context.ignore_404:\n                return\n            raise ReturnCode(\n                'listing container %r: %s %s' % (path, status, reason))\n        with context.io_manager.with_stdout() as fp:\n            if context.output_headers:\n                context.write_headers(\n                    fp, headers, context.muted_container_headers)\n            fp.write(contents)\n            fp.flush()\n        return\n    with context.client_manager.with_client() as client:\n        status, reason, headers, contents = client.get_container(\n            path, headers=context.headers, limit=limit, delimiter=delimiter,\n            prefix=prefix, marker=marker, end_marker=end_marker,\n            query=context.query, cdn=context.cdn)\n        if status // 100 != 2:\n            if status == 404 and context.ignore_404:\n                return\n            if hasattr(contents, 'read'):\n                contents.read()\n            raise ReturnCode(\n                'listing container %r: %s %s' % (path, status, reason))\n    if context.output_headers and not context.all_objects:\n        with context.io_manager.with_stdout() as fp:\n            context.write_headers(\n                fp, headers, context.muted_container_headers)\n    conc = Concurrency(context.concurrency)\n    while contents:\n        if context.all_objects:\n            new_context = context.copy()\n            new_context.query = dict(new_context.query)\n            for remove in (\n                    'limit', 'delimiter', 'prefix', 'marker', 'end_marker'):\n                if remove in new_context.query:\n                    del new_context.query[remove]\n            for item in contents:\n                if 'name' in item:\n                    for (exc_type, exc_value, exc_tb, result) in \\\n                            six.itervalues(conc.get_results()):\n                        if exc_value:\n                            conc.join()\n                            raise exc_value\n                    new_path = path + '/' + item['name'].encode('utf8')\n                    conc.spawn(new_path, cli_get, new_context, new_path)\n        else:\n            with context.io_manager.with_stdout() as fp:\n                for item in contents:\n                    if context.full:\n                        fp.write('%13s %22s %32s %25s ' % (\n                            item.get('bytes', '-'),\n                            item.get('last_modified', '-')[:22].replace(\n                                'T', ' '),\n                            item.get('hash', '-'),\n                            item.get('content_type', '-')))\n                    fp.write(item.get(\n                        'name', item.get('subdir')))\n                    fp.write('\\n')\n                fp.flush()\n        if limit:\n            break\n        marker = contents[-1].get('name', contents[-1].get('subdir', ''))\n        with context.client_manager.with_client() as client:\n            status, reason, headers, contents = client.get_container(\n                path, headers=context.headers, limit=limit,\n                delimiter=delimiter, prefix=prefix, end_marker=end_marker,\n                marker=marker, query=context.query, cdn=context.cdn)\n            if status // 100 != 2:\n                if status == 404 and context.ignore_404:\n                    return\n                if hasattr(contents, 'read'):\n                    contents.read()\n                raise ReturnCode(\n                    'listing container %r: %s %s' % (path, status, reason))\n    conc.join()\n    for (exc_type, exc_value, exc_tb, result) in \\\n        six.itervalues(conc.get_results()):\n        if exc_value:\n            raise exc_value", "response": "This function performs a GET on the container as a listing request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nperform a GET on the item (account, container, or object). See :py:mod:`swiftly.cli.get` for context usage information. See :py:class:`CLIGet` for more information.", "response": "def cli_get(context, path=None):\n    \"\"\"\n    Performs a GET on the item (account, container, or object).\n\n    See :py:mod:`swiftly.cli.get` for context usage information.\n\n    See :py:class:`CLIGet` for more information.\n    \"\"\"\n    path = path.lstrip('/') if path else None\n    if not path:\n        return cli_get_account_listing(context)\n    elif '/' not in path.rstrip('/'):\n        return cli_get_container_listing(context, path)\n    status, reason, headers, contents = 0, 'Unknown', {}, ''\n    with context.client_manager.with_client() as client:\n        status, reason, headers, contents = client.get_object(\n            *path.split('/', 1), headers=context.headers, query=context.query,\n            cdn=context.cdn)\n        if status // 100 != 2:\n            if status == 404 and context.ignore_404:\n                return\n            if hasattr(contents, 'read'):\n                contents.read()\n            raise ReturnCode(\n                'getting object %r: %s %s' % (path, status, reason))\n        if context.decrypt:\n            crypt_type = contents.read(1)\n            if crypt_type == AES256CBC:\n                contents = FileLikeIter(aes_decrypt(\n                    context.decrypt, contents,\n                    chunk_size=getattr(client, 'chunk_size', 65536)))\n            else:\n                raise ReturnCode(\n                    'getting object %r: contents encrypted with unsupported '\n                    'type %r' % (path, crypt_type))\n\n        def disk_closed_callback(disk_path):\n            if context.remove_empty_files and not os.path.getsize(disk_path):\n                os.unlink(disk_path)\n                if context.io_manager.stdout_root:\n                    dirname = os.path.dirname(disk_path)\n                    while dirname and dirname.startswith(\n                            context.io_manager.stdout_root):\n                        try:\n                            os.rmdir(dirname)\n                        except OSError:\n                            pass\n                        dirname = os.path.dirname(dirname)\n                return\n            if (headers.get('content-type') in\n                    ['text/directory', 'application/directory'] and\n                    headers.get('content-length') == '0'):\n                os.unlink(disk_path)\n                os.makedirs(disk_path)\n            mtime = 0\n            if 'x-object-meta-mtime' in headers:\n                mtime = float(headers['x-object-meta-mtime'])\n            elif 'last-modified' in headers:\n                mtime = time.mktime(time.strptime(\n                    headers['last-modified'], '%a, %d %b %Y %H:%M:%S %Z'))\n            if mtime:\n                os.utime(disk_path, (mtime, mtime))\n\n        out_path = path\n        if context.suppress_container_name:\n            out_path = out_path.split('/', 1)[1]\n        out_path = context.io_manager.client_path_to_os_path(out_path)\n        with context.io_manager.with_stdout(\n                out_path, disk_closed_callback=disk_closed_callback) as fp:\n            if context.output_headers:\n                context.write_headers(\n                    fp, headers, context.muted_object_headers)\n                fp.write('\\n')\n            chunk = contents.read(65536)\n            while chunk:\n                fp.write(chunk)\n                chunk = contents.read(65536)\n            fp.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef request(self, method, path, contents, headers, decode_json=False,\n                stream=False, query=None, cdn=False):\n        \"\"\"\n        See :py:func:`swiftly.client.client.Client.request`\n        \"\"\"\n        if cdn:\n            raise Exception('CDN not yet supported with LocalClient')\n        if isinstance(contents, six.string_types):\n            contents = StringIO(contents)\n        if not headers:\n            headers = {}\n        if not query:\n            query = {}\n        rpath = path.lstrip('/')\n        if '/' in rpath:\n            container_name, object_name = rpath.split('/', 1)\n        else:\n            container_name = rpath\n            object_name = ''\n        if not container_name:\n            status, reason, hdrs, body = self._account(\n                method, contents, headers, stream, query, cdn)\n        elif not object_name:\n            status, reason, hdrs, body = self._container(\n                method, container_name, contents, headers, stream, query, cdn)\n        else:\n            status, reason, hdrs, body = self._object(\n                method, container_name, object_name, contents, headers, stream,\n                query, cdn)\n        if status and status // 100 != 5:\n            if not stream and decode_json and status // 100 == 2:\n                if body:\n                    body = loads(body)\n                else:\n                    body = None\n            return (status, reason, hdrs, body)\n        raise Exception('%s %s failed: %s %s' % (method, path, status, reason))", "response": "This method is used to make a request to the Unified InfluxDB API."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _resolve_option(self, options, option_name, section_name):\n        if getattr(options, option_name, None) is not None:\n            return\n        if option_name.startswith(section_name + '_'):\n            environ_name = option_name.upper()\n            conf_name = option_name[len(section_name) + 1:]\n        else:\n            environ_name = (section_name + '_' + option_name).upper()\n            conf_name = option_name\n        setattr(\n            options, option_name,\n            os.environ.get(\n                environ_name,\n                (self.context.conf.get(section_name, {})).get(conf_name)))", "response": "Resolves an option value into options."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cli_post(context, path, body=None):\n    path = path.lstrip('/') if path else ''\n    status, reason, headers, contents = 0, 'Unknown', {}, ''\n    with context.client_manager.with_client() as client:\n        if not path:\n            status, reason, headers, contents = client.post_account(\n                headers=context.headers, query=context.query, cdn=context.cdn,\n                body=body)\n            if status // 100 != 2:\n                raise ReturnCode('posting account: %s %s' % (status, reason))\n        elif '/' not in path.rstrip('/'):\n            path = path.rstrip('/')\n            status, reason, headers, contents = client.post_container(\n                path, headers=context.headers, query=context.query,\n                cdn=context.cdn, body=body)\n            if status // 100 != 2:\n                raise ReturnCode(\n                    'posting container %r: %s %s' % (path, status, reason))\n        else:\n            status, reason, headers, contents = client.post_object(\n                *path.split('/', 1), headers=context.headers,\n                query=context.query, cdn=context.cdn, body=body)\n            if status // 100 != 2:\n                raise ReturnCode(\n                    'posting object %r: %s %s' % (path, status, reason))", "response": "Performs a POST on the item."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a shallow copy of the CLIContext instance.", "response": "def copy(self):\n        \"\"\"\n        Returns a new CLIContext instance that is a shallow copy of\n        the original, much like dict's copy method.\n        \"\"\"\n        context = CLIContext()\n        for item in dir(self):\n            if item[0] != '_' and item not in ('copy', 'write_headers'):\n                setattr(context, item, getattr(self, item))\n        return context"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nauthenticate and outputs the resulting information.", "response": "def cli_auth(context):\n    \"\"\"\n    Authenticates and then outputs the resulting information.\n\n    See :py:mod:`swiftly.cli.auth` for context usage information.\n\n    See :py:class:`CLIAuth` for more information.\n    \"\"\"\n    with context.io_manager.with_stdout() as fp:\n        with context.client_manager.with_client() as client:\n            info = []\n            client.auth()\n            if getattr(client, 'auth_cache_path', None):\n                info.append(('Auth Cache', client.auth_cache_path))\n            if getattr(client, 'auth_url', None):\n                info.append(('Auth URL', client.auth_url))\n            if getattr(client, 'auth_user', None):\n                info.append(('Auth User', client.auth_user))\n            if getattr(client, 'auth_key', None):\n                info.append(('Auth Key', client.auth_key))\n            if getattr(client, 'auth_tenant', None):\n                info.append(('Auth Tenant', client.auth_tenant))\n            if getattr(client, 'auth_methods', None):\n                info.append(('Auth Methods', client.auth_methods))\n            if getattr(client, 'storage_path', None):\n                info.append(('Direct Storage Path', client.storage_path))\n            if getattr(client, 'cdn_path', None):\n                info.append(('Direct CDN Path', client.cdn_path))\n            if getattr(client, 'local_path', None):\n                info.append(('Local Path', client.local_path))\n            if getattr(client, 'regions', None):\n                info.append(('Regions', ' '.join(client.regions)))\n            if getattr(client, 'default_region', None):\n                info.append(('Default Region', client.default_region))\n            if getattr(client, 'region', None):\n                info.append(('Selected Region', client.region))\n            if getattr(client, 'snet', None):\n                info.append(('SNet', client.snet))\n            if getattr(client, 'storage_url', None):\n                info.append(('Storage URL', client.storage_url))\n            if getattr(client, 'cdn_url', None):\n                info.append(('CDN URL', client.cdn_url))\n            if getattr(client, 'auth_token', None):\n                info.append(('Auth Token', client.auth_token))\n            if not info:\n                info.append((\n                    'No auth information available',\n                    'Maybe no credentials were provided?'))\n            fmt = '%%-%ds %%s\\n' % (max(len(t) for t, v in info) + 1)\n            for t, v in info:\n                fp.write(fmt % (t + ':', v))\n            fp.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_temp_url(method, url, seconds, key):\n    method = method.upper()\n    base_url, object_path = url.split('/v1/')\n    object_path = '/v1/' + object_path\n    expires = int(time.time() + seconds)\n    hmac_body = '%s\\n%s\\n%s' % (method, expires, object_path)\n    sig = hmac.new(key, hmac_body, hashlib.sha1).hexdigest()\n    return '%s%s?temp_url_sig=%s&temp_url_expires=%s' % (\n        base_url, object_path, sig, expires)", "response": "Generate a TempURL good for the given request method url and number of seconds from now."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef headers_to_dict(headers):\n    hdrs = {}\n    for h, v in headers:\n        h = h.lower()\n        if h in hdrs:\n            if isinstance(hdrs[h], list):\n                hdrs[h].append(v)\n            else:\n                hdrs[h] = [hdrs[h], v]\n        else:\n            hdrs[h] = v\n    return hdrs", "response": "Converts a sequence of name value tuples into a dictionary where the key is the name of the element and the value is the value of the element."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nissuing commands for each item in an account or container listing.", "response": "def cli_fordo(context, path=None):\n    \"\"\"\n    Issues commands for each item in an account or container listing.\n\n    See :py:mod:`swiftly.cli.fordo` for context usage information.\n\n    See :py:class:`CLIForDo` for more information.\n    \"\"\"\n    path = path.lstrip('/') if path else None\n    if path and '/' in path:\n        raise ReturnCode(\n            'path must be an empty string or a container name; was %r' % path)\n    limit = context.query.get('limit')\n    delimiter = context.query.get('delimiter')\n    prefix = context.query.get('prefix')\n    marker = context.query.get('marker')\n    end_marker = context.query.get('end_marker')\n    conc = Concurrency(context.concurrency)\n    while True:\n        with context.client_manager.with_client() as client:\n            if not path:\n                status, reason, headers, contents = client.get_account(\n                    headers=context.headers, prefix=prefix,\n                    delimiter=delimiter, marker=marker, end_marker=end_marker,\n                    limit=limit, query=context.query, cdn=context.cdn)\n            else:\n                status, reason, headers, contents = client.get_container(\n                    path, headers=context.headers, prefix=prefix,\n                    delimiter=delimiter, marker=marker, end_marker=end_marker,\n                    limit=limit, query=context.query, cdn=context.cdn)\n            if status // 100 != 2:\n                if status == 404 and context.ignore_404:\n                    return\n                if hasattr(contents, 'read'):\n                    contents.read()\n                if not path:\n                    raise ReturnCode(\n                        'listing account: %s %s' % (status, reason))\n                else:\n                    raise ReturnCode(\n                        'listing container %r: %s %s' % (path, status, reason))\n        if not contents:\n            break\n        for item in contents:\n            name = (path + '/' if path else '') + item.get(\n                'name', item.get('subdir'))\n            args = list(context.remaining_args)\n            try:\n                index = args.index('<item>')\n            except ValueError:\n                raise ReturnCode(\n                    'No \"<item>\" designation found in the \"do\" clause.')\n            args[index] = name\n            for (exc_type, exc_value, exc_tb, result) in \\\n                    six.itervalues(conc.get_results()):\n                if exc_value:\n                    conc.join()\n                    raise exc_value\n            conc.spawn(name, _cli_call, context, name, args)\n        marker = contents[-1]['name']\n        if limit:\n            break\n    conc.join()\n    for (exc_type, exc_value, exc_tb, result) in \\\n            six.itervalues(conc.get_results()):\n        if exc_value:\n            conc.join()\n            raise exc_value"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_client(self):\n        client = None\n        try:\n            client = self.clients.get(block=False)\n        except queue.Empty:\n            pass\n        if not client:\n            self.client_id += 1\n            kwargs = dict(self.kwargs)\n            kwargs['verbose_id'] = kwargs.get(\n                'verbose_id', '') + str(self.client_id)\n            client = self.client_class(*self.args, **kwargs)\n        return client", "response": "Gets a client for use."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nperforms a HEAD on the item.", "response": "def cli_head(context, path=None):\n    \"\"\"\n    Performs a HEAD on the item (account, container, or object).\n\n    See :py:mod:`swiftly.cli.head` for context usage information.\n\n    See :py:class:`CLIHead` for more information.\n    \"\"\"\n    path = path.lstrip('/') if path else None\n    with context.client_manager.with_client() as client:\n        if not path:\n            status, reason, headers, contents = client.head_account(\n                headers=context.headers, query=context.query, cdn=context.cdn)\n            mute = context.muted_account_headers\n        elif '/' not in path.rstrip('/'):\n            path = path.rstrip('/')\n            status, reason, headers, contents = client.head_container(\n                path, headers=context.headers, query=context.query,\n                cdn=context.cdn)\n            mute = context.muted_container_headers\n        else:\n            status, reason, headers, contents = client.head_object(\n                *path.split('/', 1), headers=context.headers,\n                query=context.query, cdn=context.cdn)\n            mute = context.muted_object_headers\n        if hasattr(contents, 'read'):\n            contents = contents.read()\n    if status // 100 != 2:\n        if status == 404 and context.ignore_404:\n            return\n        if not path:\n            raise ReturnCode('heading account: %s %s' % (status, reason))\n        elif '/' not in path:\n            raise ReturnCode(\n                'heading container %r: %s %s' % (path, status, reason))\n        else:\n            raise ReturnCode(\n                'heading object %r: %s %s' % (path, status, reason))\n    else:\n        with context.io_manager.with_stdout() as fp:\n            context.write_headers(fp, headers, mute)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef aes_decrypt(key, stdin, chunk_size=65536):\n    if not AES256CBC_Support:\n        raise Exception(\n            'AES256CBC not supported; likely pycrypto is not installed')\n    # Always use 256-bit key\n    key = hashlib.sha256(key).digest()\n    # At least 16 and a multiple of 16\n    chunk_size = max(16, chunk_size >> 4 << 4)\n    iv = stdin.read(16)\n    while len(iv) < 16:\n        chunk = stdin.read(16 - len(iv))\n        if not chunk:\n            raise IOError('EOF reading IV')\n    decryptor = Crypto.Cipher.AES.new(key, Crypto.Cipher.AES.MODE_CBC, iv)\n    data = ''\n    while True:\n        chunk = stdin.read(chunk_size)\n        if not chunk:\n            if len(data) != 16:\n                raise IOError('EOF reading encrypted stream')\n            data = decryptor.decrypt(data)\n            trailing = ord(data[-1])\n            if trailing > 15:\n                raise IOError(\n                    'EOF reading encrypted stream or trailing value corrupted '\n                    '%s' % trailing)\n            yield data[:trailing]\n            break\n        data += chunk\n        if len(data) > 16:\n            # Always leave at least one byte pending\n            trailing = (len(data) % 16) or 16\n            yield decryptor.decrypt(data[:-trailing])\n            data = data[-trailing:]", "response": "A generator that decrypts a content stream using AES 256 in CBC mode."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cli_put_directory_structure(context, path):\n    if not context.input_:\n        raise ReturnCode(\n            'called cli_put_directory_structure without context.input_ set')\n    if not os.path.isdir(context.input_):\n        raise ReturnCode(\n            '%r is not a directory' % context.input_)\n    if not path:\n        raise ReturnCode(\n            'uploading a directory structure requires at least a container '\n            'name')\n    new_context = context.copy()\n    new_context.input_ = None\n    container = path.split('/', 1)[0]\n    cli_put_container(new_context, container)\n    ilen = len(context.input_)\n    if not context.input_.endswith(os.sep):\n        ilen += 1\n    conc = Concurrency(context.concurrency)\n    for (dirpath, dirnames, filenames) in os.walk(context.input_):\n        if not dirnames and not filenames:\n            new_context = context.copy()\n            new_context.headers = dict(context.headers)\n            new_context.headers['content-type'] = 'text/directory'\n            new_context.headers['x-object-meta-mtime'] = \\\n                '%f' % os.path.getmtime(context.input_)\n            new_context.input_ = None\n            new_context.empty = True\n            new_path = path\n            if path[-1] != '/':\n                new_path += '/'\n            new_path += dirpath[ilen:]\n            for (exc_type, exc_value, exc_tb, result) in \\\n                    six.itervalues(conc.get_results()):\n                if exc_value:\n                    conc.join()\n                    raise exc_value\n            conc.spawn(new_path, cli_put_object, new_context, new_path)\n        else:\n            for fname in filenames:\n                new_context = context.copy()\n                new_context.input_ = os.path.join(dirpath, fname)\n                new_path = path\n                if path[-1] != '/':\n                    new_path += '/'\n                if dirpath[ilen:]:\n                    new_path += dirpath[ilen:] + '/'\n                new_path += fname\n                for (exc_type, exc_value, exc_tb, result) in \\\n                        six.itervalues(conc.get_results()):\n                    if exc_value:\n                        conc.join()\n                        raise exc_value\n                conc.spawn(new_path, cli_put_object, new_context, new_path)\n    conc.join()\n    for (exc_type, exc_value, exc_tb, result) in \\\n            six.itervalues(conc.get_results()):\n        if exc_value:\n            raise exc_value", "response": "This function is used to upload a directory structure to a SWF file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nperforms a PUT on the account.", "response": "def cli_put_account(context):\n    \"\"\"\n    Performs a PUT on the account.\n\n    See :py:mod:`swiftly.cli.put` for context usage information.\n\n    See :py:class:`CLIPut` for more information.\n    \"\"\"\n    body = None\n    if context.input_:\n        if context.input_ == '-':\n            body = context.io_manager.get_stdin()\n        else:\n            body = open(context.input_, 'rb')\n    with context.client_manager.with_client() as client:\n        status, reason, headers, contents = client.put_account(\n            headers=context.headers, query=context.query, cdn=context.cdn,\n            body=body)\n        if hasattr(contents, 'read'):\n            contents.read()\n    if status // 100 != 2:\n        raise ReturnCode('putting account: %s %s' % (status, reason))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nperforms a PUT on the container. See :py:mod:`swiftly.cli.put` for context usage information. See :py:class:`CLIPut` for more information.", "response": "def cli_put_container(context, path):\n    \"\"\"\n    Performs a PUT on the container.\n\n    See :py:mod:`swiftly.cli.put` for context usage information.\n\n    See :py:class:`CLIPut` for more information.\n    \"\"\"\n    path = path.rstrip('/')\n    if '/' in path:\n        raise ReturnCode('called cli_put_container with object %r' % path)\n    body = None\n    if context.input_:\n        if context.input_ == '-':\n            body = context.io_manager.get_stdin()\n        else:\n            body = open(context.input_, 'rb')\n    with context.client_manager.with_client() as client:\n        status, reason, headers, contents = client.put_container(\n            path, headers=context.headers, query=context.query,\n            cdn=context.cdn, body=body)\n        if hasattr(contents, 'read'):\n            contents.read()\n    if status // 100 != 2:\n        raise ReturnCode(\n            'putting container %r: %s %s' % (path, status, reason))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nperform a PUT on the object.", "response": "def cli_put_object(context, path):\n    \"\"\"\n    Performs a PUT on the object.\n\n    See :py:mod:`swiftly.cli.put` for context usage information.\n\n    See :py:class:`CLIPut` for more information.\n    \"\"\"\n    if context.different and context.encrypt:\n        raise ReturnCode(\n            'context.different will not work properly with context.encrypt '\n            'since encryption may change the object size')\n    put_headers = dict(context.headers)\n    if context.empty:\n        body = ''\n        put_headers['content-length'] = '0'\n    elif not context.input_ or context.input_ == '-':\n        stdin = context.io_manager.get_stdin()\n\n        if context.stdin_segmentation:\n            def reader():\n                while True:\n                    chunk = stdin.read(65536)\n                    if chunk:\n                        yield chunk\n                    else:\n                        return\n\n            segment_body = FileLikeIter(reader(), context.segment_size)\n\n            prefix = _create_container(context, path, time.time(), 0)\n            new_context = context.copy()\n            new_context.stdin_segmentation = False\n            new_context.stdin = segment_body\n            new_context.headers = dict(context.headers)\n            segment_n = 0\n            path2info = {}\n            while not segment_body.is_empty():\n                segment_path = _get_segment_path(prefix, segment_n)\n                etag = cli_put_object(new_context, segment_path)\n                size = segment_body.limit - segment_body.left\n                path2info[segment_path] = (size, etag)\n\n                segment_body.reset_limit()\n                segment_n += 1\n            body = _get_manifest_body(context, prefix, path2info, put_headers)\n        else:\n            if hasattr(context, 'stdin'):\n                body = context.stdin\n            else:\n                body = stdin\n    elif context.seek is not None:\n        if context.encrypt:\n            raise ReturnCode(\n                'putting object %r: Cannot use encryption and context.seek' %\n                path)\n        body = open(context.input_, 'rb')\n        body.seek(context.seek)\n    else:\n        l_mtime = os.path.getmtime(context.input_)\n        l_size = os.path.getsize(context.input_)\n        put_headers['content-length'] = str(l_size)\n        if context.newer or context.different:\n            r_mtime = None\n            r_size = None\n            with context.client_manager.with_client() as client:\n                status, reason, headers, contents = client.head_object(\n                    *path.split('/', 1), headers=context.headers,\n                    query=context.query, cdn=context.cdn)\n                if hasattr(contents, 'read'):\n                    contents.read()\n            if status // 100 == 2:\n                r_mtime = headers.get('x-object-meta-mtime')\n                if r_mtime:\n                    try:\n                        r_mtime = float(r_mtime)\n                    except ValueError:\n                        r_mtime = None\n                r_size = headers.get('content-length')\n                if r_size:\n                    try:\n                        r_size = int(r_size)\n                    except ValueError:\n                        r_size = None\n            elif status != 404:\n                raise ReturnCode(\n                    'could not head %r for conditional check; skipping put: '\n                    '%s %s' % (path, status, reason))\n            if context.newer and r_mtime is not None or l_mtime <= r_mtime:\n                return\n            if context.different and r_mtime is not None and \\\n                    l_mtime == r_mtime and r_size is not None and \\\n                    l_size == r_size:\n                return\n        put_headers['x-object-meta-mtime'] = '%f' % l_mtime\n        size = os.path.getsize(context.input_)\n        if size > context.segment_size:\n            if context.encrypt:\n                raise ReturnCode(\n                    'putting object %r: Cannot use encryption for objects '\n                    'greater than the segment size' % path)\n            prefix = _create_container(context, path, l_mtime, size)\n            conc = Concurrency(context.concurrency)\n            start = 0\n            segment = 0\n            path2info = {}\n            while start < size:\n                new_context = context.copy()\n                new_context.headers = dict(context.headers)\n                new_context.headers['content-length'] = str(min(\n                    size - start, context.segment_size))\n                new_context.seek = start\n                new_path = _get_segment_path(prefix, segment)\n                for (ident, (exc_type, exc_value, exc_tb, result)) in \\\n                        six.iteritems(conc.get_results()):\n                    if exc_value:\n                        conc.join()\n                        raise exc_value\n                    path2info[ident] = result\n                conc.spawn(\n                    new_path, cli_put_object, new_context, new_path)\n                segment += 1\n                start += context.segment_size\n            conc.join()\n            for (ident, (exc_type, exc_value, exc_tb, result)) in \\\n                    six.iteritems(conc.get_results()):\n                if exc_value:\n                    raise exc_value\n                path2info[ident] = result\n            body = _get_manifest_body(context, prefix, path2info, put_headers)\n        else:\n            body = open(context.input_, 'rb')\n    with context.client_manager.with_client() as client:\n        if context.encrypt:\n            content_length = put_headers.get('content-length')\n            if content_length:\n                content_length = int(content_length)\n            if hasattr(body, 'read'):\n                body = FileLikeIter(aes_encrypt(\n                    context.encrypt, body, preamble=AES256CBC,\n                    chunk_size=getattr(client, 'chunk_size', 65536),\n                    content_length=content_length))\n            else:\n                body = FileLikeIter(aes_encrypt(\n                    context.encrypt, FileLikeIter([body]), preamble=AES256CBC,\n                    chunk_size=getattr(client, 'chunk_size', 65536),\n                    content_length=content_length))\n            if 'content-length' in put_headers:\n                del put_headers['content-length']\n        container, obj = path.split('/', 1)\n        status, reason, headers, contents = client.put_object(\n            container, obj, body, headers=put_headers, query=context.query,\n            cdn=context.cdn)\n        if hasattr(contents, 'read'):\n            contents = contents.read()\n    if status // 100 != 2:\n        raise ReturnCode(\n            'putting object %r: %s %s %r' % (path, status, reason, contents))\n    if context.seek is not None:\n        content_length = put_headers.get('content-length')\n        etag = headers.get('etag')\n        if content_length and etag:\n            content_length = int(content_length)\n        else:\n            with context.client_manager.with_client() as client:\n                container, obj = path.split('/', 1)\n                status, reason, headers, contents = client.head_object(\n                    container, obj, cdn=context.cdn)\n                if hasattr(contents, 'read'):\n                    contents = contents.read()\n            if status // 100 != 2:\n                raise ReturnCode(\n                    'heading object %r: %s %s %r' %\n                    (path, status, reason, contents))\n            content_length = headers.get('content-length')\n            etag = headers.get('etag')\n            if content_length:\n                content_length = int(content_length)\n        return content_length, etag\n    if context.stdin is not None:\n        return headers.get('etag')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nperforms a PUT on the item (account, container, or object). See :py:mod:`swiftly.cli.put` for context usage information. See :py:class:`CLIPut` for more information.", "response": "def cli_put(context, path):\n    \"\"\"\n    Performs a PUT on the item (account, container, or object).\n\n    See :py:mod:`swiftly.cli.put` for context usage information.\n\n    See :py:class:`CLIPut` for more information.\n    \"\"\"\n    path = path.lstrip('/') if path else ''\n    if context.input_ and os.path.isdir(context.input_):\n        return cli_put_directory_structure(context, path)\n    if not path:\n        return cli_put_account(context)\n    elif '/' not in path.rstrip('/'):\n        return cli_put_container(context, path)\n    else:\n        return cli_put_object(context, path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_manifest_body(context, prefix, path2info, put_headers):\n    if context.static_segments:\n        body = json.dumps([\n            {'path': '/' + p, 'size_bytes': s, 'etag': e}\n            for p, (s, e) in sorted(six.iteritems(path2info))\n        ])\n        put_headers['content-length'] = str(len(body))\n        context.query['multipart-manifest'] = 'put'\n    else:\n        body = ''\n        put_headers['content-length'] = '0'\n        put_headers['x-object-manifest'] = prefix\n\n    return body", "response": "Returns the body for the manifest file and modifies put_headers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a container for segments of file with path l_mtime and size", "response": "def _create_container(context, path, l_mtime, size):\n    \"\"\"\n    Creates container for segments of file with `path`\n    \"\"\"\n    new_context = context.copy()\n    new_context.input_ = None\n    new_context.headers = None\n    new_context.query = None\n    container = path.split('/', 1)[0] + '_segments'\n    cli_put_container(new_context, container)\n    prefix = container + '/' + path.split('/', 1)[1]\n    prefix = '%s/%s/%s/' % (prefix, l_mtime, size)\n\n    return prefix"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a new TempURL and sends it to the context. io_manager stdout.", "response": "def cli_tempurl(context, method, path, seconds=None, use_container=False):\n    \"\"\"\n    Generates a TempURL and sends that to the context.io_manager's\n    stdout.\n\n    See :py:mod:`swiftly.cli.tempurl` for context usage information.\n\n    See :py:class:`CLITempURL` for more information.\n\n    :param context: The :py:class:`swiftly.cli.context.CLIContext` to\n        use.\n    :param method: The method for the TempURL (GET, PUT, etc.)\n    :param path: The path the TempURL should direct to.\n    :param seconds: The number of seconds the TempURL should be good\n        for. Default: 3600\n    :param use_container: If True, will create a container level TempURL\n        useing X-Container-Meta-Temp-Url-Key instead of\n        X-Account-Meta-Temp-Url-Key.\n    \"\"\"\n    with contextlib.nested(\n            context.io_manager.with_stdout(),\n            context.client_manager.with_client()) as (fp, client):\n        method = method.upper()\n        path = path.lstrip('/')\n        seconds = seconds if seconds is not None else 3600\n        if '/' not in path:\n            raise ReturnCode(\n                'invalid tempurl path %r; should have a / within it' % path)\n        if use_container:\n            key_type = 'container'\n            container = path.split('/', 1)[0]\n            status, reason, headers, contents = \\\n                client.head_container(container)\n        else:\n            key_type = 'account'\n            status, reason, headers, contents = \\\n                client.head_account()\n        if status // 100 != 2:\n            raise ReturnCode(\n                'obtaining X-%s-Meta-Temp-Url-Key: %s %s' %\n                (key_type.title(), status, reason))\n        key = headers.get('x-%s-meta-temp-url-key' % key_type)\n        if not key:\n            raise ReturnCode(\n                'there is no X-%s-Meta-Temp-Url-Key set for this %s' %\n                (key_type.title(), key_type))\n        url = client.storage_url + '/' + path\n        fp.write(generate_temp_url(method, url, seconds, key))\n        fp.write('\\n')\n        fp.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nauthenticating the user with the specified auth url.", "response": "def auth(self):\n        \"\"\"\n        See :py:func:`swiftly.client.client.Client.auth`\n        \"\"\"\n        self.reset()\n        if not self.auth_url:\n            raise ValueError('No Auth URL has been provided.')\n        funcs = []\n        if self.auth_methods:\n            for method in self.auth_methods.split(','):\n                funcs.append(getattr(self, '_' + method))\n        if not funcs:\n            if '1.0' in self.auth_url:\n                funcs = [self._auth1, self._auth2key, self._auth2password]\n                if not self.auth_tenant:\n                    funcs.append(self._auth2password_force_tenant)\n            else:\n                funcs = [self._auth2key, self._auth2password]\n                if not self.auth_tenant:\n                    funcs.append(self._auth2password_force_tenant)\n                funcs.append(self._auth1)\n        info = []\n        for func in funcs:\n            status, reason = func()\n            info.append('%s %s' % (status, reason))\n            if status // 100 == 2:\n                break\n        else:\n            raise self.HTTPException('Auth failure %r.' % info)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef request(self, method, path, contents, headers, decode_json=False,\n                stream=False, query=None, cdn=False):\n        \"\"\"\n        See :py:func:`swiftly.client.client.Client.request`\n        \"\"\"\n        if query:\n            path += '?' + '&'.join(\n                ('%s=%s' % (quote(k), quote(v)) if v else quote(k))\n                for k, v in sorted(six.iteritems(query)))\n        reset_func = self._default_reset_func\n        if isinstance(contents, six.string_types):\n            contents = StringIO(contents)\n        tell = getattr(contents, 'tell', None)\n        seek = getattr(contents, 'seek', None)\n        if tell and seek:\n            try:\n                orig_pos = tell()\n                reset_func = lambda: seek(orig_pos)\n            except Exception:\n                tell = seek = None\n        elif not contents:\n            reset_func = lambda: None\n        status = 0\n        reason = 'Unknown'\n        attempt = 0\n        while attempt < self.attempts:\n            attempt += 1\n            if time() >= self.conn_discard:\n                self.storage_conn = None\n                self.cdn_conn = None\n            if cdn:\n                conn = self.cdn_conn\n                conn_path = self.cdn_path\n            else:\n                conn = self.storage_conn\n                conn_path = self.storage_path\n            if not conn:\n                parsed, conn = self._connect(cdn=cdn)\n                if conn:\n                    if cdn:\n                        self.cdn_conn = conn\n                        self.cdn_path = conn_path = parsed.path\n                    else:\n                        self.storage_conn = conn\n                        self.storage_path = conn_path = parsed.path\n                else:\n                    raise self.HTTPException(\n                        '%s %s failed: No connection' % (method, path))\n            self.conn_discard = time() + 4\n            titled_headers = dict((k.title(), v) for k, v in six.iteritems({\n                'User-Agent': self.user_agent,\n                'X-Auth-Token': self.auth_token}))\n            if headers:\n                titled_headers.update(\n                    (k.title(), v) for k, v in six.iteritems(headers))\n            try:\n                if not hasattr(contents, 'read'):\n                    if method not in self.no_content_methods and contents and \\\n                            'Content-Length' not in titled_headers and \\\n                            'Transfer-Encoding' not in titled_headers:\n                        titled_headers['Content-Length'] = str(\n                            len(contents or ''))\n                    verbose_headers = '  '.join(\n                        '%s: %s' % (k, v)\n                        for k, v in sorted(six.iteritems(titled_headers)))\n                    self.verbose(\n                        '> %s %s %s', method, conn_path + path,\n                        verbose_headers)\n                    conn.request(\n                        method, conn_path + path, contents, titled_headers)\n                else:\n                    conn.putrequest(method, conn_path + path)\n                    content_length = None\n                    for h, v in sorted(six.iteritems(titled_headers)):\n                        if h == 'Content-Length':\n                            content_length = int(v)\n                        conn.putheader(h, v)\n                    if method not in self.no_content_methods and \\\n                            content_length is None:\n                        titled_headers['Transfer-Encoding'] = 'chunked'\n                        conn.putheader('Transfer-Encoding', 'chunked')\n                    conn.endheaders()\n                    verbose_headers = '  '.join(\n                        '%s: %s' % (k, v)\n                        for k, v in sorted(six.iteritems(titled_headers)))\n                    self.verbose(\n                        '> %s %s %s', method, conn_path + path,\n                        verbose_headers)\n                    if method not in self.no_content_methods and \\\n                            content_length is None:\n                        chunk = contents.read(self.chunk_size)\n                        while chunk:\n                            conn.send('%x\\r\\n%s\\r\\n' % (len(chunk), chunk))\n                            chunk = contents.read(self.chunk_size)\n                        conn.send('0\\r\\n\\r\\n')\n                    else:\n                        left = content_length or 0\n                        while left > 0:\n                            size = self.chunk_size\n                            if size > left:\n                                size = left\n                            chunk = contents.read(size)\n                            if not chunk:\n                                raise IOError('Early EOF from input')\n                            conn.send(chunk)\n                            left -= len(chunk)\n                resp = conn.getresponse()\n                status = resp.status\n                reason = resp.reason\n                hdrs = headers_to_dict(resp.getheaders())\n                if stream:\n                    value = resp\n                else:\n                    value = resp.read()\n                    resp.close()\n            except Exception as err:\n                status = 0\n                reason = '%s %s' % (type(err), str(err))\n                hdrs = {}\n                value = None\n            self.verbose('< %s %s', status or '-', reason)\n            self.verbose('< %s', hdrs)\n            if status == 401:\n                if stream:\n                    value.close()\n                conn.close()\n                self.auth()\n                attempt -= 1\n            elif status and status // 100 != 5:\n                if not stream and decode_json and status // 100 == 2:\n                    if value:\n                        value = json.loads(value.decode('utf-8'))\n                    else:\n                        value = None\n                self.conn_discard = time() + 4\n                return (status, reason, hdrs, value)\n            else:\n                if stream and value:\n                    value.close()\n                conn.close()\n            if reset_func:\n                reset_func()\n            self.sleep(2 ** attempt)\n        raise self.HTTPException(\n            '%s %s failed: %s %s' % (method, path, status, reason))", "response": "This method is used to send a request to the specified path."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nresetting the internal state of the internal state of the internal state of the internal state.", "response": "def reset(self):\n        \"\"\"\n        See :py:func:`swiftly.client.client.Client.reset`\n        \"\"\"\n        for conn in (self.storage_conn, self.cdn_conn):\n            if conn:\n                try:\n                    conn.close()\n                except Exception:\n                    pass\n        self.storage_conn = None\n        self.cdn_conn = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_account_hash(self):\n        if not(self.storage_url or self.storage_path):\n            self.auth()\n        return (self.storage_url or self.storage_path).rsplit('/', 1)[1]", "response": "Get the hash of the account."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cli_trans(context, x_trans_id):\n    with context.io_manager.with_stdout() as fp:\n        trans_time = get_trans_id_time(x_trans_id)\n        trans_info = x_trans_id[34:]\n        msg = 'X-Trans-Id:      ' + x_trans_id + '\\n'\n        if not trans_time:\n            msg += 'Time Stamp:      None, old style id with no time ' \\\n                'embedded\\nUTC Time:        None, old style id with no time ' \\\n                'embedded\\n'\n        else:\n            msg += 'Time Stamp:      %s\\nUTC Time:        %s\\n' % (\n                trans_time,\n                time.strftime(\n                    '%a %Y-%m-%d %H:%M:%S UTC', time.gmtime(trans_time)))\n        msg += 'Additional Info: ' + trans_info + '\\n'\n        fp.write(msg)\n        fp.flush()", "response": "This function translates any information that can be determined from the\n    x_trans_id and sends it to the context. io_manager s stdout."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cli_help(context, command_name, general_parser, command_parsers):\n    if command_name == 'for':\n        command_name = 'fordo'\n    with context.io_manager.with_stdout() as stdout:\n        if not command_name:\n            general_parser.print_help(stdout)\n        elif command_name in command_parsers:\n            command_parsers[command_name].option_parser.print_help(stdout)\n        else:\n            raise ReturnCode('unknown command %r' % command_name)", "response": "Prints help information for a specific command."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread at most size bytes from the file and return them as a string.", "response": "def read(self, size=-1):\n        \"\"\"\n        read([size]) -> read at most size bytes, returned as a string.\n\n        If the size argument is negative or omitted, read until EOF is reached.\n        Notice that when in non-blocking mode, less data than what was\n        requested may be returned, even if no size parameter was given.\n        \"\"\"\n        if self.left is not None:\n            size = min(size, self.left)\n        if self.closed:\n            raise ValueError('I/O operation on closed file')\n        if size < 0:\n            return ''.join(self)\n        elif not size:\n            chunk = ''\n        elif self.buf:\n            chunk = self.buf\n            self.buf = None\n        else:\n            try:\n                chunk = next(self.iterator)\n            except StopIteration:\n                return ''\n        if len(chunk) > size:\n            self.buf = chunk[size:]\n            chunk = chunk[:size]\n        if self.left is not None:\n            self.left -= len(chunk)\n        return chunk"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads a line from the file.", "response": "def readline(self, size=-1):\n        \"\"\"\n        readline([size]) -> next line from the file, as a string.\n\n        Retain newline.  A non-negative size argument limits the maximum\n        number of bytes to return (an incomplete line may be returned then).\n        Return an empty string at EOF.\n        \"\"\"\n        if self.closed:\n            raise ValueError('I/O operation on closed file')\n        data = ''\n        while '\\n' not in data and (size < 0 or len(data) < size):\n            if size < 0:\n                chunk = self.read(1024)\n            else:\n                chunk = self.read(size - len(data))\n            if not chunk:\n                break\n            data += chunk\n        if '\\n' in data:\n            data, sep, rest = data.partition('\\n')\n            data += sep\n            if self.buf:\n                self.buf = rest + self.buf\n            else:\n                self.buf = rest\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef readlines(self, sizehint=-1):\n        if self.closed:\n            raise ValueError('I/O operation on closed file')\n        lines = []\n        while True:\n            line = self.readline(sizehint)\n            if not line:\n                break\n            lines.append(line)\n            if sizehint >= 0:\n                sizehint -= len(line)\n                if sizehint <= 0:\n                    break\n        return lines", "response": "Reads a list of lines from the file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking whether the file is empty reading the single byte.", "response": "def is_empty(self):\n        \"\"\"\n        Check whether the \"file\" is empty reading the single byte.\n        \"\"\"\n        something = self.read(1)\n        if something:\n            if self.buf:\n                self.buf = something + self.buf\n            else:\n                self.buf = something\n            return False\n        else:\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cli_encrypt(context, key):\n    with context.io_manager.with_stdout() as stdout:\n        with context.io_manager.with_stdin() as stdin:\n            for chunk in aes_encrypt(key, stdin, preamble=AES256CBC):\n                stdout.write(chunk)\n            stdout.flush()", "response": "Encrypts stdin and sends that to the stdout."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetermine the value for BUILD_COMMITS from the app and repository config.", "response": "def get_build_commits(app, repo_config):\n    \"\"\"\n    Determine the value for BUILD_COMMITS from the app and repository\n    config. Resolves the previous BUILD_ALL_COMMITS = True/False option\n    to BUILD_COMMITS = 'ALL'/'LAST' respectively.\n    \"\"\"\n    build_commits = repo_config.get(\"build_commits\")\n    build_all_commits = repo_config.get(\"build_all_commits\",\n                                        app.config.get(\"BUILD_ALL_COMMITS\"))\n\n    if not build_commits and build_all_commits is not None:\n        # Determine BUILD_COMMITS from legacy BUILD_ALL_COMMITS\n        if build_all_commits:\n            build_commits = BUILD_COMMITS_ALL\n        else:\n            build_commits = BUILD_COMMITS_LAST\n        warnings.warn(\"BUILD_ALL_COMMITS is deprecated. Use the BUILD_COMMITS \"\n                      \"setting instead.\", DeprecationWarning)\n    elif not build_commits:\n        # Determine BUILD_COMMITS from global app config.\n        build_commits = app.config[\"BUILD_COMMITS\"]\n    return build_commits"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the status of a commit.", "response": "def get_status(app, repo_config, repo_name, sha):\n    \"\"\"Gets the status of a commit.\n\n    .. note::\n        ``repo_name`` might not ever be anything other than\n        ``repo_config['github_repo']``.\n\n    :param app: Flask app for leeroy\n    :param repo_config: configuration for the repo\n    :param repo_name: The name of the owner/repo\n    :param sha: SHA for the status we are looking for\n    :return: returns json response of status\n    \"\"\"\n    url = get_api_url(app, repo_config, github_status_url).format(\n        repo_name=repo_name, sha=sha)\n    logging.debug(\"Getting status for %s %s\", repo_name, sha)\n    s = get_session_for_repo(app, repo_config)\n    response = s.get(url)\n    if not response.ok:\n        raise Exception(\"Unable to get status: {}\".format(response.status_code))\n    return response"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the pull request data for a given pull request.", "response": "def get_pull_request(app, repo_config, pull_request):\n    \"\"\"Data for a given pull request.\n\n    :param app: Flask app\n    :param repo_config: dict with ``github_repo`` key\n    :param pull_request: the pull request number\n    \"\"\"\n    response = get_api_response(\n        app, repo_config,\n        \"/repos/{{repo_name}}/pulls/{0}\".format(pull_request))\n    if not response.ok:\n        raise Exception(\"Unable to get pull request: status code {}\".format(response.status_code))\n    return response.json"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the last 30 pull requests from a repository.", "response": "def get_pull_requests(app, repo_config):\n    \"\"\"Last 30 pull requests from a repository.\n\n    :param app: Flask app\n    :param repo_config: dict with ``github_repo`` key\n\n    :returns: id for a pull request\n    \"\"\"\n    response = get_api_response(app, repo_config, \"/repos/{repo_name}/pulls\")\n    if not response.ok:\n        raise Exception(\"Unable to get pull requests: status code {}\".format(response.status_code))\n    return (item for item in response.json)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite obj in elasticsearch.", "response": "def write(self, obj, resource_id=None):\n        \"\"\"Write obj in elasticsearch.\n        :param obj: value to be written in elasticsearch.\n        :param resource_id: id for the resource.\n        :return: id of the transaction.\n        \"\"\"\n        self.logger.debug('elasticsearch::write::{}'.format(resource_id))\n        if resource_id is not None:\n            if self.driver._es.exists(\n                    index=self.driver._index,\n                    id=resource_id,\n                    doc_type='_doc'\n            ):\n                raise ValueError(\n                    \"Resource \\\"{}\\\" already exists, use update instead\".format(resource_id))\n        return self.driver._es.index(\n            index=self.driver._index,\n            id=resource_id,\n            body=obj,\n            doc_type='_doc',\n            refresh='wait_for'\n        )['_id']"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read(self, resource_id):\n        self.logger.debug('elasticsearch::read::{}'.format(resource_id))\n        return self.driver._es.get(\n            index=self.driver._index,\n            id=resource_id,\n            doc_type='_doc'\n        )['_source']", "response": "Read object in elasticsearch using the resource_id."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update(self, obj, resource_id):\n        self.logger.debug('elasticsearch::update::{}'.format(resource_id))\n        return self.driver._es.index(\n            index=self.driver._index,\n            id=resource_id,\n            body=obj,\n            doc_type='_doc',\n            refresh='wait_for'\n        )['_id']", "response": "Update the object in elasticsearch using the resource_id."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete(self, resource_id):\n        self.logger.debug('elasticsearch::delete::{}'.format(resource_id))\n        if self.driver._es.exists(\n                index=self.driver._index,\n                id=resource_id,\n                doc_type='_doc'\n        ) == False:\n            raise ValueError(\"Resource \\\"{}\\\" does not exists\".format(resource_id))\n        return self.driver._es.delete(\n            index=self.driver._index,\n            id=resource_id,\n            doc_type='_doc'\n        )", "response": "Delete an object from elasticsearch."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list(self, search_from=None, search_to=None, limit=None):\n        self.logger.debug('elasticsearch::list')\n        body = {\n            'sort': [\n                {\"_id\": \"asc\"},\n            ],\n            'query': {\n                'match_all': {}\n            }\n        }\n\n        if search_from:\n            body['from'] = search_from\n        if search_to:\n            body['size'] = search_to - search_from\n        if limit:\n            body['size'] = limit\n\n        page = self.driver._es.search(\n            index=self.driver._index,\n            doc_type='_doc',\n            body=body\n        )\n\n        object_list = []\n        for x in page['hits']['hits']:\n            object_list.append(x['_source'])\n        return object_list", "response": "List all the objects saved elasticsearch."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nquery elasticsearch for objects.", "response": "def query(self, search_model: QueryModel):\n        \"\"\"Query elasticsearch for objects.\n        :param search_model: object of QueryModel.\n        :return: list of objects that match the query.\n        \"\"\"\n        query_parsed = query_parser(search_model.query)\n        self.logger.debug(f'elasticsearch::query::{query_parsed[0]}')\n        if search_model.sort is not None:\n            self._mapping_to_sort(search_model.sort.keys())\n            sort = self._sort_object(search_model.sort)\n        else:\n            sort = [{\"_id\": \"asc\"}]\n        if search_model.query == {}:\n            query = {'match_all': {}}\n        else:\n            query = query_parsed[0]\n\n        body = {\n            'query': query,\n            'sort': sort,\n            'from': (search_model.page - 1) * search_model.offset,\n            'size': search_model.offset,\n        }\n\n        page = self.driver._es.search(\n            index=self.driver._index,\n            doc_type='_doc',\n            body=body,\n            q=query_parsed[1]\n        )\n\n        object_list = []\n        for x in page['hits']['hits']:\n            object_list.append(x['_source'])\n        return object_list"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nquery elasticsearch for objects.", "response": "def text_query(self, search_model: FullTextModel):\n        \"\"\"Query elasticsearch for objects.\n        :param search_model: object of FullTextModel\n        :return: list of objects that match the query.\n        \"\"\"\n        self.logger.debug('elasticsearch::text_query::{}'.format(search_model.text))\n        if search_model.sort is not None:\n            self._mapping_to_sort(search_model.sort.keys())\n            sort = self._sort_object(search_model.sort)\n        else:\n            sort = [{\"service.metadata.curation.rating\": \"asc\"}]\n        body = {\n            'sort': sort,\n            'from': (search_model.page - 1) * search_model.offset,\n            'size': search_model.offset,\n        }\n\n        page = self.driver._es.search(\n            index=self.driver._index,\n            doc_type='_doc',\n            body=body,\n            q=search_model.text\n        )\n\n        object_list = []\n        for x in page['hits']['hits']:\n            object_list.append(x['_source'])\n        return object_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef forwards(self, orm):\n        \"Write your forwards methods here.\"\n        # Note: Remember to use orm['appname.ModelName'] rather than \"from appname.models...\"\n        for title in orm['hero_slider.SliderItemTitle'].objects.all():\n            title.is_published = True\n            title.save()", "response": "Write your forwards methods here."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the published slider items.", "response": "def get_slider_items(context, amount=None):\n    \"\"\"Returns the published slider items.\"\"\"\n    req = context.get('request')\n    qs = SliderItem.objects.published(req).order_by('position')\n    if amount:\n        qs = qs[:amount]\n    return qs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrender the hero slider.", "response": "def render_hero_slider(context):\n    \"\"\"\n    Renders the hero slider.\n\n    \"\"\"\n    req = context.get('request')\n    qs = SliderItem.objects.published(req).order_by('position')\n    return {\n        'slider_items': qs,\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reader_acquire(self):\n\n        self._order_mutex.acquire()\n        self._readers_mutex.acquire()\n\n        if self._readers == 0:\n            self._access_mutex.acquire()\n        self._readers += 1\n\n        self._order_mutex.release()\n        self._readers_mutex.release()", "response": "Acquire the lock to read"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreleases the lock after reading", "response": "def reader_release(self):\n        \"\"\"Release the lock after reading\"\"\"\n\n        self._readers_mutex.acquire()\n\n        self._readers -= 1\n        if self._readers == 0:\n            self._access_mutex.release()\n\n        self._readers_mutex.release()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef writer_acquire(self):\n\n        self._order_mutex.acquire()\n        self._access_mutex.acquire()\n        self._order_mutex.release()", "response": "Acquire the lock to write"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a task to the registry.", "response": "def add(self, task_id, backend, category, backend_args,\n            archiving_cfg=None, scheduling_cfg=None):\n        \"\"\"Add a task to the registry.\n\n        This method adds task using `task_id` as identifier. If a task\n        with the same identifier already exists on the registry, a\n        `AlreadyExistsError` exception will be raised.\n\n        :param task_id: identifier of the task to add\n        :param backend: backend used to fetch data from the repository\n        :param category: category of the items to fetch\n        :param backend_args: dictionary of arguments required to run the backend\n        :param archiving_cfg: archiving config for the task, if needed\n        :param scheduling_cfg: scheduling config for the task, if needed\n\n        :returns: the new task added to the registry\n\n        :raises AlreadyExistsError: raised when the given task identifier\n            exists on the registry\n        \"\"\"\n        self._rwlock.writer_acquire()\n\n        if task_id in self._tasks:\n            self._rwlock.writer_release()\n            raise AlreadyExistsError(element=str(task_id))\n\n        task = Task(task_id, backend, category, backend_args,\n                    archiving_cfg=archiving_cfg,\n                    scheduling_cfg=scheduling_cfg)\n        self._tasks[task_id] = task\n\n        self._rwlock.writer_release()\n\n        logger.debug(\"Task %s added to the registry\", str(task_id))\n\n        return task"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove(self, task_id):\n        try:\n            self._rwlock.writer_acquire()\n            del self._tasks[task_id]\n        except KeyError:\n            raise NotFoundError(element=str(task_id))\n        finally:\n            self._rwlock.writer_release()\n\n        logger.debug(\"Task %s removed from the registry\", str(task_id))", "response": "Removes a task from the registry."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a task from the registry using its task identifier.", "response": "def get(self, task_id):\n        \"\"\"Get a task from the registry.\n\n        Retrieve a task from the registry using its task identifier. When\n        the task does not exist, a `NotFoundError` exception will be\n        raised.\n\n        :param task_id: task identifier\n\n        :returns: a task object\n\n        :raises NotFoundError: raised when the requested task is not\n            found on the registry\n        \"\"\"\n        try:\n            self._rwlock.reader_acquire()\n            task = self._tasks[task_id]\n        except KeyError:\n            raise NotFoundError(element=str(task_id))\n        finally:\n            self._rwlock.reader_release()\n\n        return task"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the list of tasks", "response": "def tasks(self):\n        \"\"\"Get the list of tasks\"\"\"\n\n        self._rwlock.reader_acquire()\n        tl = [v for v in self._tasks.values()]\n        tl.sort(key=lambda x: x.task_id)\n        self._rwlock.reader_release()\n\n        return tl"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a dict with the representation of this task configuration object.", "response": "def to_dict(self):\n        \"\"\"Returns a dict with the representation of this task configuration object.\"\"\"\n\n        properties = find_class_properties(self.__class__)\n        config = {\n            name: self.__getattribute__(name) for name, _ in properties\n        }\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating an instance of the class from a dictionary.", "response": "def from_dict(cls, config):\n        \"\"\"Create an configuration object from a dictionary.\n\n        Key,value pairs will be used to initialize a task configuration\n        object. If 'config' contains invalid configuration parameters\n        a `ValueError` exception will be raised.\n\n        :param config: dictionary used to create an instance of this object\n\n        :returns: a task config instance\n\n        :raises ValueError: when an invalid configuration parameter is found\n        \"\"\"\n        try:\n            obj = cls(**config)\n        except TypeError as e:\n            m = cls.KW_ARGS_ERROR_REGEX.match(str(e))\n            if m:\n                raise ValueError(\"unknown '%s' task config parameter\" % m.group(1))\n            else:\n                raise e\n        else:\n            return obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd metadata to an item. Decorator that adds metadata to Perceval items such as the identifier of the job that generated it or the version of the system. The contents from the original item will be stored under the 'data' keyword. Take into account that this function only can be called from a `PercevalJob` class due it needs access to some attributes and methods of this class.", "response": "def metadata(func):\n    \"\"\"Add metadata to an item.\n\n    Decorator that adds metadata to Perceval items such as the\n    identifier of the job that generated it or the version of\n    the system. The contents from the original item will\n    be stored under the 'data' keyword.\n\n    Take into account that this function only can be called from\n    a `PercevalJob` class due it needs access to some attributes\n    and methods of this class.\n    \"\"\"\n    @functools.wraps(func)\n    def decorator(self, *args, **kwargs):\n        for item in func(self, *args, **kwargs):\n            item['arthur_version'] = __version__\n            item['job_id'] = self.job_id\n            yield item\n    return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef execute_perceval_job(backend, backend_args, qitems, task_id, category,\n                         archive_args=None, max_retries=MAX_JOB_RETRIES):\n    \"\"\"Execute a Perceval job on RQ.\n\n    The items fetched during the process will be stored in a\n    Redis queue named `queue`.\n\n    Setting the parameter `archive_path`, raw data will be stored\n    with the archive manager. The contents from the archive can\n    be retrieved setting the pameter `fetch_from_archive` to `True`,\n    too. Take into account this behaviour will be only available\n    when the backend supports the use of the archive. If archiving\n    is not supported, an `AttributeError` exception will be raised.\n\n    :param backend: backend to execute\n    :param bakend_args: dict of arguments for running the backend\n    :param qitems: name of the RQ queue used to store the items\n    :param task_id: identifier of the task linked to this job\n    :param category: category of the items to retrieve\n    :param archive_args: archive arguments\n    :param max_retries: maximum number of attempts this job can execute\n        before failing\n\n    :returns: a `JobResult` instance\n\n    :raises NotFoundError: raised when the backend is not found\n    :raises AttributeError: raised when archiving is not supported but\n        any of the archive parameters were set\n    \"\"\"\n    rq_job = rq.get_current_job()\n\n    job = PercevalJob(rq_job.id, task_id, backend, category,\n                      rq_job.connection, qitems)\n\n    logger.debug(\"Running job #%s (task: %s) (%s) (cat:%s)\",\n                 job.job_id, task_id, backend, category)\n\n    if not job.has_archiving() and archive_args:\n        raise AttributeError(\"archive attributes set but archive is not supported\")\n\n    run_job = True\n    resume = False\n    failures = 0\n\n    while run_job:\n        try:\n            job.run(backend_args, archive_args=archive_args, resume=resume)\n        except AttributeError as e:\n            raise e\n        except Exception as e:\n            logger.debug(\"Error running job %s (%s) - %s\",\n                         job.job_id, backend, str(e))\n            failures += 1\n\n            if not job.has_resuming() or failures >= max_retries:\n                logger.error(\"Cancelling job #%s (task: %s) (%s)\",\n                             job.job_id, task_id, backend)\n                raise e\n\n            logger.warning(\"Resuming job #%s (task: %s) (%s) due to a failure (n %s, max %s)\",\n                           job.job_id, task_id, backend, failures, max_retries)\n            resume = True\n        else:\n            # No failure, do not retry\n            run_job = False\n\n    result = job.result\n\n    logger.debug(\"Job #%s (task: %s) completed (%s) - %s items (%s) fetched\",\n                 result.job_id, task_id, result.backend, str(result.nitems), result.category)\n\n    return result", "response": "Execute a Perceval job on the RQ queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef initialize_archive_manager(self, archive_path):\n        if archive_path == \"\":\n            raise ValueError(\"Archive manager path cannot be empty\")\n\n        if archive_path:\n            self.archive_manager = perceval.archive.ArchiveManager(archive_path)", "response": "Initialize the archive manager."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(self, backend_args, archive_args=None, resume=False):\n        args = backend_args.copy()\n\n        if archive_args:\n            self.initialize_archive_manager(archive_args['archive_path'])\n\n        if not resume:\n            max_date = backend_args.get('from_date', None)\n            offset = backend_args.get('offset', None)\n\n            if max_date:\n                max_date = datetime_to_utc(max_date).timestamp()\n\n            self._result = JobResult(self.job_id, self.task_id, self.backend, self.category,\n                                     None, max_date, 0, offset=offset,\n                                     nresumed=0)\n        else:\n            if self.result.max_date:\n                args['from_date'] = unixtime_to_datetime(self.result.max_date)\n            if self.result.offset:\n                args['offset'] = self.result.offset\n            self._result.nresumed += 1\n\n        for item in self._execute(args, archive_args):\n            self.conn.rpush(self.qitems, pickle.dumps(item))\n\n            self._result.nitems += 1\n            self._result.last_uuid = item['uuid']\n\n            if not self.result.max_date or self.result.max_date < item['updated_on']:\n                self._result.max_date = item['updated_on']\n            if 'offset' in item:\n                self._result.offset = item['offset']", "response": "This method runs the backend with the given parameters and stores the results in Redis."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _execute(self, backend_args, archive_args):\n        if not archive_args or not archive_args['fetch_from_archive']:\n            return perceval.backend.fetch(self._bklass, backend_args, self.category,\n                                          manager=self.archive_manager)\n        else:\n            return perceval.backend.fetch_from_archive(self._bklass, backend_args,\n                                                       self.archive_manager, self.category,\n                                                       archive_args['archived_after'])", "response": "Execute a backend of Perceval."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconfigure the index to work with", "response": "def create_index(idx_url, clean=False):\n        \"\"\"Configure the index to work with\"\"\"\n\n        try:\n            r = requests.get(idx_url)\n        except requests.exceptions.ConnectionError:\n            cause = \"Error connecting to Elastic Search (index: %s)\" % idx_url\n            raise ElasticSearchError(cause=cause)\n\n        if r.status_code != 200:\n            # The index does not exist\n            r = requests.put(idx_url)\n\n            if r.status_code != 200:\n                logger.info(\"Can't create index %s (%s)\", idx_url, r.status_code)\n                cause = \"Error creating Elastic Search index %s\" % idx_url\n                raise ElasticSearchError(cause=cause)\n\n            logger.info(\"Index %s created\", idx_url)\n            return True\n        elif r.status_code == 200 and clean:\n            requests.delete(idx_url)\n            requests.put(idx_url)\n            logger.info(\"Index deleted and created (index: %s)\", idx_url)\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a mapping in Elastic Search", "response": "def create_mapping(idx_url, mapping):\n        \"\"\"Create a mapping\"\"\"\n\n        mapping_url = idx_url + '/items/_mapping'\n        mapping = json.dumps(mapping)\n\n        try:\n            r = requests.put(mapping_url, data=mapping,\n                             headers={'Content-Type': 'application/json'})\n        except requests.exceptions.ConnectionError:\n            cause = \"Error connecting to Elastic Search (index: %s, url: %s)\" \\\n                % (idx_url, mapping_url)\n            raise ElasticSearchError(cause=cause)\n\n        if r.status_code != 200:\n            reason = r.json()['error']\n            logger.info(\"Can't create mapping in %s. %s\",\n                        mapping_url, reason)\n            cause = \"Error creating Elastic Search mapping %s. %s\" % \\\n                (mapping_url, reason)\n            raise ElasticSearchError(cause=cause)\n        else:\n            logger.info(\"Mapping created in %s\", mapping_url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef json_encoder(*args, **kwargs):\n\n    obj = cherrypy.serving.request._json_inner_handler(*args, **kwargs)\n\n    for chunk in JSONEncoder().iterencode(obj):\n        yield chunk.encode('utf-8')", "response": "Custom JSON encoder handler"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_items(cls, writer, items_generator):\n        while True:\n            items = items_generator()\n            writer.write(items)\n            time.sleep(1)", "response": "Write items to the queue\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add(self):\n\n        payload = cherrypy.request.json\n\n        logger.debug(\"Reading tasks...\")\n        for task_data in payload['tasks']:\n            try:\n                category = task_data['category']\n                backend_args = task_data['backend_args']\n                archive_args = task_data.get('archive', None)\n                sched_args = task_data.get('scheduler', None)\n            except KeyError as ex:\n                logger.error(\"Task badly formed\")\n                raise ex\n\n            from_date = backend_args.get('from_date', None)\n\n            if from_date:\n                backend_args['from_date'] = str_to_datetime(from_date)\n\n            super().add_task(task_data['task_id'],\n                             task_data['backend'],\n                             category,\n                             backend_args,\n                             archive_args=archive_args,\n                             sched_args=sched_args)\n        logger.debug(\"Done. Ready to work!\")\n\n        return \"Tasks added\"", "response": "Add tasks to the database"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove tasks from the cache", "response": "def remove(self):\n        \"\"\"Remove tasks\"\"\"\n\n        payload = cherrypy.request.json\n        logger.debug(\"Reading tasks to remove...\")\n\n        task_ids = {}\n\n        for task_data in payload['tasks']:\n            task_id = task_data['task_id']\n            removed = super().remove_task(task_id)\n            task_ids[task_id] = removed\n\n        result = {'tasks': task_ids}\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tasks(self):\n\n        logger.debug(\"API 'tasks' method called\")\n\n        result = [task.to_dict() for task in self._tasks.tasks]\n        result = {'tasks': result}\n\n        logger.debug(\"Tasks registry read\")\n\n        return result", "response": "Return a list of all tasks in the registry"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a task to the scheduler.", "response": "def add_task(self, task_id, backend, category, backend_args,\n                 archive_args=None, sched_args=None):\n        \"\"\"Add and schedule a task.\n\n        :param task_id: id of the task\n        :param backend: name of the backend\n        :param category: category of the items to fecth\n        :param backend_args: args needed to initialize the backend\n        :param archive_args: args needed to initialize the archive\n        :param sched_args: scheduling args for this task\n\n        :returns: the task created\n        \"\"\"\n        try:\n            archiving_cfg = self.__parse_archive_args(archive_args)\n            scheduling_cfg = self.__parse_schedule_args(sched_args)\n            self.__validate_args(task_id, backend, category, backend_args)\n        except ValueError as e:\n            raise e\n\n        try:\n            task = self._tasks.add(task_id, backend, category, backend_args,\n                                   archiving_cfg=archiving_cfg,\n                                   scheduling_cfg=scheduling_cfg)\n        except AlreadyExistsError as e:\n            raise e\n\n        self._scheduler.schedule_task(task.task_id)\n\n        return task"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove and cancel a task.", "response": "def remove_task(self, task_id):\n        \"\"\"Remove and cancel a task.\n\n        :param task_id: id of the task to be removed\n        \"\"\"\n        try:\n            self._scheduler.cancel_task(task_id)\n        except NotFoundError as e:\n            logger.info(\"Cannot cancel %s task because it does not exist.\",\n                        task_id)\n            return False\n        else:\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the items fetched by the jobs.", "response": "def items(self):\n        \"\"\"Get the items fetched by the jobs.\"\"\"\n\n        # Get and remove queued items in an atomic transaction\n        pipe = self.conn.pipeline()\n        pipe.lrange(Q_STORAGE_ITEMS, 0, -1)\n        pipe.ltrim(Q_STORAGE_ITEMS, 1, 0)\n        items = pipe.execute()[0]\n\n        for item in items:\n            item = pickle.loads(item)\n            yield item"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __validate_args(task_id, backend, category, backend_args):\n\n        if not task_id or task_id.strip() == \"\":\n            msg = \"Missing task_id for task\"\n            raise ValueError(msg)\n\n        if not backend or backend.strip() == \"\":\n            msg = \"Missing backend for task '%s'\" % task_id\n            raise ValueError(msg)\n\n        if backend_args and not isinstance(backend_args, dict):\n            msg = \"Backend_args is not a dict, task '%s'\" % task_id\n            raise ValueError(msg)\n\n        if not category or category.strip() == \"\":\n            msg = \"Missing category for task '%s'\" % task_id\n            raise ValueError(msg)", "response": "Check that the task arguments received are valid"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __parse_archive_args(self, archive_args):\n\n        if not archive_args:\n            return None\n\n        archiving_args = copy.deepcopy(archive_args)\n\n        if self.archive_path:\n            archiving_args['archive_path'] = self.archive_path\n        else:\n            archiving_args['archive_path'] = os.path.expanduser(ARCHIVES_DEFAULT_PATH)\n\n        return ArchivingTaskConfig.from_dict(archiving_args)", "response": "Parse the archive arguments of a task"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef perform_job(self, job, queue):\n\n        result = super().perform_job(job, queue)\n\n        job_status = job.get_status()\n        job_result = job.return_value if job_status == 'finished' else None\n\n        data = {\n            'job_id': job.id,\n            'status': job_status,\n            'result': job_result\n        }\n\n        msg = pickle.dumps(data)\n        self.connection.publish(self.pubsub_channel, msg)\n\n        return result", "response": "Custom method to execute a job and notify the result of its job"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nscheduling a job in the given queue.", "response": "def schedule_job_task(self, queue_id, task_id, job_args, delay=0):\n        \"\"\"Schedule a job in the given queue.\"\"\"\n\n        self._rwlock.writer_acquire()\n\n        job_id = self._generate_job_id(task_id)\n\n        event = self._scheduler.enter(delay, 1, self._enqueue_job,\n                                      argument=(queue_id, job_id, job_args,))\n        self._jobs[job_id] = event\n        self._tasks[task_id] = job_id\n\n        self._rwlock.writer_release()\n\n        logging.debug(\"Job #%s (task: %s) scheduled on %s (wait: %s)\",\n                      job_id, task_id, queue_id, delay)\n\n        return job_id"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncanceling the job related to the given task.", "response": "def cancel_job_task(self, task_id):\n        \"\"\"Cancel the job related to the given task.\"\"\"\n\n        try:\n            self._rwlock.writer_acquire()\n\n            job_id = self._tasks.get(task_id, None)\n\n            if job_id:\n                self._cancel_job(job_id)\n            else:\n                logger.warning(\"Task %s set to be removed was not found\",\n                               task_id)\n        finally:\n            self._rwlock.writer_release()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run(self):\n\n        try:\n            self.listen()\n        except Exception as e:\n            logger.critical(\"JobListener instence crashed. Error: %s\", str(e))\n            logger.critical(traceback.format_exc())", "response": "Run the thread to listen for jobs and reschedule successful ones."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef listen(self):\n\n        pubsub = self.conn.pubsub()\n        pubsub.subscribe(self.pubsub_channel)\n\n        logger.debug(\"Listening on channel %s\", self.pubsub_channel)\n\n        for msg in pubsub.listen():\n            logger.debug(\"New message received of type %s\", str(msg['type']))\n\n            if msg['type'] != 'message':\n                logger.debug(\"Ignoring job message\")\n                continue\n\n            data = pickle.loads(msg['data'])\n            job_id = data['job_id']\n\n            job = rq.job.Job.fetch(job_id, connection=self.conn)\n\n            if data['status'] == 'finished':\n                logging.debug(\"Job #%s completed\", job_id)\n                handler = self.result_handler\n            elif data['status'] == 'failed':\n                logging.debug(\"Job #%s failed\", job_id)\n                handler = self.result_handler_err\n            else:\n                continue\n\n            if handler:\n                logging.debug(\"Calling handler for job #%s\", job_id)\n                handler(job)", "response": "Listen for completed jobs and reschedule successful ones."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef schedule_task(self, task_id):\n        task = self.registry.get(task_id)\n\n        job_args = self._build_job_arguments(task)\n\n        archiving_cfg = task.archiving_cfg\n\n        fetch_from_archive = False if not archiving_cfg else archiving_cfg.fetch_from_archive\n        # Schedule the job as soon as possible\n        queue = Q_ARCHIVE_JOBS if fetch_from_archive else Q_CREATION_JOBS\n        job_id = self._scheduler.schedule_job_task(queue,\n                                                   task.task_id, job_args,\n                                                   delay=0)\n\n        logger.info(\"Job #%s (task: %s) scheduled\", job_id, task.task_id)\n\n        return job_id", "response": "Schedules a task in the registry."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cancel_task(self, task_id):\n        self.registry.remove(task_id)\n        self._scheduler.cancel_job_task(task_id)\n\n        logger.info(\"Task %s canceled\", task_id)", "response": "Cancels a task from the scheduler."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _handle_successful_job(self, job):\n\n        result = job.result\n        task_id = job.kwargs['task_id']\n\n        try:\n            task = self.registry.get(task_id)\n        except NotFoundError:\n            logger.warning(\"Task %s not found; related job #%s will not be rescheduled\",\n                           task_id, job.id)\n            return\n\n        if task.archiving_cfg and task.archiving_cfg.fetch_from_archive:\n            logger.info(\"Job #%s (task: %s) successfully finished\", job.id, task_id)\n            return\n\n        if result.nitems > 0:\n            task.backend_args['next_from_date'] = unixtime_to_datetime(result.max_date)\n\n            if result.offset:\n                task.backend_args['next_offset'] = result.offset\n\n        job_args = self._build_job_arguments(task)\n\n        delay = task.scheduling_cfg.delay if task.scheduling_cfg else WAIT_FOR_QUEUING\n\n        job_id = self._scheduler.schedule_job_task(Q_UPDATING_JOBS,\n                                                   task_id, job_args,\n                                                   delay=delay)\n\n        logger.info(\"Job #%s (task: %s, old job: %s) re-scheduled\",\n                    job_id, task_id, job.id)", "response": "Handle a successful job."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding the set of arguments required for running a job", "response": "def _build_job_arguments(task):\n        \"\"\"Build the set of arguments required for running a job\"\"\"\n\n        job_args = {}\n        job_args['qitems'] = Q_STORAGE_ITEMS\n        job_args['task_id'] = task.task_id\n\n        # Backend parameters\n        job_args['backend'] = task.backend\n        backend_args = copy.deepcopy(task.backend_args)\n\n        if 'next_from_date' in backend_args:\n            backend_args['from_date'] = backend_args.pop('next_from_date')\n\n        if 'next_offset' in backend_args:\n            backend_args['offset'] = backend_args.pop('next_offset')\n\n        job_args['backend_args'] = backend_args\n\n        # Category\n        job_args['category'] = task.category\n\n        # Archiving parameters\n        archiving_cfg = task.archiving_cfg\n        job_args['archive_args'] = archiving_cfg.to_dict() if archiving_cfg else None\n\n        # Scheduler parameters\n        sched_cfg = task.scheduling_cfg\n        job_args['max_retries'] = sched_cfg.max_retries if sched_cfg else MAX_JOB_RETRIES\n\n        return job_args"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_secret(secret_name, default=None):\n    secrets_dir = get_secrets_dir()\n    secret_path = os.path.join(secrets_dir, secret_name)\n    try:\n        with open(secret_path, \"r\") as secret_file:\n            return secret_file.read()\n    except OSError:\n        return default", "response": "Gets the contents of a secret file in the BANANAS_SECRETS_DIR"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nregisters the API view class in the bananas router.", "response": "def register(view):  # Type[BananasAPI]\n    \"\"\"\n    Register the API view class in the bananas router.\n\n    :param BananasAPI view:\n    \"\"\"\n    meta = view.get_admin_meta()\n    prefix = meta.basename.replace(\".\", \"/\")\n    router.register(prefix, view, meta.basename)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nregister a generic class based view wrapped with ModelAdmin and fake modelAdminView", "response": "def register(view=None, *, admin_site=None, admin_class=ModelAdminView):\n    \"\"\"\n    Register a generic class based view wrapped with ModelAdmin and fake model\n\n    :param view: The AdminView to register.\n    :param admin_site: The AdminSite to register the view on.\n        Defaults to bananas.admin.ExtendedAdminSite.\n    :param admin_class: The ModelAdmin class to use for eg. permissions.\n        Defaults to bananas.admin.ModelAdminView.\n\n    Example:\n\n    @register  # Or with args @register(admin_class=MyModelAdminSubclass)\n    class MyAdminView(bananas.admin.AdminView):\n        def get(self, request):\n            return self.render('template.html', {})\n\n    # Also possible:\n    register(MyAdminView, admin_class=MyModelAdminSublass)\n\n    \"\"\"\n    if not admin_site:\n        admin_site = site\n\n    def wrapped(inner_view):\n        module = inner_view.__module__\n        app_label = re.search(r\"\\.?(\\w+)\\.admin\", module).group(1)\n        app_config = apps.get_app_config(app_label)\n\n        label = getattr(inner_view, \"label\", None)\n        if not label:\n            label = re.sub(\"(Admin)|(View)\", \"\", inner_view.__name__).lower()\n        inner_view.label = label\n\n        model_name = label.capitalize()\n        verbose_name = getattr(inner_view, \"verbose_name\", model_name)\n        inner_view.verbose_name = verbose_name\n\n        access_perm_codename = \"can_access_\" + model_name.lower()\n        access_perm_name = _(\"Can access {verbose_name}\").format(\n            verbose_name=verbose_name\n        )\n        # The first permission here is expected to be\n        # the general access permission.\n        permissions = tuple(\n            [(access_perm_codename, access_perm_name)]\n            + list(getattr(inner_view, \"permissions\", []))\n        )\n\n        model = type(\n            model_name,\n            (Model,),\n            {\n                \"__module__\": module + \".__models__\",  # Fake\n                \"View\": inner_view,\n                \"app_config\": app_config,\n                \"Meta\": type(\n                    \"Meta\",\n                    (object,),\n                    dict(\n                        managed=False,\n                        abstract=True,\n                        app_label=app_config.label,\n                        verbose_name=verbose_name,\n                        verbose_name_plural=verbose_name,\n                        permissions=permissions,\n                    ),\n                ),\n            },\n        )\n\n        admin_site._registry[model] = admin_class(model, admin_site)\n        return inner_view\n\n    if view is None:  # Used as a decorator\n        return wrapped\n\n    return wrapped(view)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\noverride to use the default namespace if not available", "response": "def reverse_action(self, url_name, *args, **kwargs):\n        \"\"\"\n        Extended DRF with fallback to requested namespace if request.version is missing\n        \"\"\"\n        if self.request and not self.request.version:\n            return reverse(self.get_url_name(url_name), *args, **kwargs)\n\n        return super().reverse_action(url_name, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget full namespaced url name to use for reverse", "response": "def get_url_name(self, action_url_name=\"list\"):\n        \"\"\"\n        Get full namespaced url name to use for reverse()\n        \"\"\"\n        url_name = \"{}-{}\".format(self.basename, action_url_name)\n\n        namespace = self.request.resolver_match.namespace\n        if namespace:\n            url_name = \"{}:{}\".format(namespace, url_name)\n\n        return url_name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget or generate human readable view name.", "response": "def get_view_name(self, respect_name=True):\n        \"\"\"\n        Get or generate human readable view name.\n        Extended version from DRF to support usage from both class and instance.\n        \"\"\"\n        if isinstance(self, type):\n            view = self\n        else:\n            view = self.__class__\n\n        # Name may be set by some Views, such as a ViewSet.\n        if respect_name:\n            name = getattr(view, \"name\", None)\n            if name is not None:\n                return name\n\n        name = view.__name__\n        for suffix in (\"ViewSet\", \"View\", \"API\", \"Admin\"):\n            name = formatting.remove_trailing_string(name, suffix)\n        name = formatting.camelcase_to_spaces(name)\n\n        # Suffix may be set by some Views, such as a ViewSet.\n        suffix = getattr(view, \"suffix\", None)\n        if suffix:\n            name += \" \" + suffix\n\n        return name"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_version(version=None):\n    if version is None:\n        version = VERSION\n    assert len(version) == 5\n    assert version[3] in (\"alpha\", \"beta\", \"rc\", \"final\")\n\n    # Now build the two parts of the version number:\n    # main = X.Y[.Z]\n    # sub = .devN - for pre-alpha releases\n    #     | {a|b|c}N - for alpha, beta and rc releases\n\n    parts = 2 if version[2] == 0 else 3\n    main = \".\".join(str(x) for x in version[:parts])\n\n    sub = \"\"\n    if version[3] != \"final\":\n        mapping = {\"alpha\": \"a\", \"beta\": \"b\", \"rc\": \"c\"}\n        sub = mapping[version[3]] + str(version[4])\n\n    return main + sub", "response": "Derives a PEP386 - compliant version number from VERSION."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn summary and description of the current resource.", "response": "def get_summary_and_description(self):\n        \"\"\"\n        Compat: drf-yasg 1.12+\n        \"\"\"\n        summary = self.get_summary()\n        _, description = super().get_summary_and_description()\n        return summary, description"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_summary(self):\n        title = None\n\n        method_name = getattr(self.view, \"action\", self.method.lower())\n        action = getattr(self.view, method_name, None)\n        action_kwargs = getattr(action, \"kwargs\", None)\n\n        if action_kwargs:\n            title = action_kwargs.get(\"name\")\n\n        if not title and is_custom_action(self.view.action):\n            title = _(self.view.action.replace(\"_\", \" \")).capitalize()\n\n        if not title:\n            meta = self.view.get_admin_meta()\n            if self.view.action in [\"retrieve\", \"update\", \"partial_update\"]:\n                title = str(meta.get(\"verbose_name\") or meta.name)\n            elif self.view.action == \"create\":\n                title = meta.get(\"verbose_name\")\n                if title:\n                    title = str(_(\"Add\")) + \" \" + str(title).lower()\n                else:\n                    title = meta.name\n            elif self.view.action == \"list\":\n                title = str(meta.get(\"verbose_name_plural\") or meta.name)\n            else:\n                title = str(meta.name)\n\n        return title", "response": "Returns the summary of the current resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_versioned_viewname(self, viewname, request):\n        namespace = request.resolver_match.namespace\n        if namespace:\n            viewname = \"{}:{}\".format(namespace, viewname)\n\n        return viewname", "response": "Return the versioned viewname for the given request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nresolving alias - instances to a sibling target.", "response": "def resolve(cursor, key):\n    \"\"\"\n    Get engine or raise exception, resolves Alias-instances to a sibling target.\n\n    :param cursor: The object so search in\n    :param key: The key to get\n    :return: The object found\n    \"\"\"\n    try:\n        result = cursor[key]\n\n        # Resolve alias\n        if isinstance(result, Alias):\n            result = cursor[result.target]\n\n        return result\n    except KeyError:\n        raise KeyError(\"No matches for engine %s\" % key)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_engine(scheme):\n    path = scheme.split(\"+\")\n    first, rest = path[0], path[1:]\n\n    second = rest[0] if rest else None\n\n    engine = resolve(ENGINE_MAPPING, first)\n\n    # If the selected engine does not have a second level.\n    if not isinstance(engine, list):\n        # If second level engine was expected\n        if second:\n            raise KeyError(\"%s has no sub-engines\" % first)\n\n        return engine\n\n    try:\n        engine, extra = engine\n    except ValueError:\n        # engine was not a list of length 2\n        raise ValueError(\n            \"django-bananas.url' engine \"\n            \"configuration is invalid: %r\" % ENGINE_MAPPING\n        )\n\n    # Get second-level engine\n    if second is not None:\n        engine = resolve(extra, second)\n\n    # Sanity-check the value before returning\n    assert not isinstance(\n        engine, (list, dict)\n    ), \"Only two levels of engines \" \"are allowed\"\n    assert engine, \"The returned engine is not truthy\"\n\n    return engine", "response": "Perform a lookup in _ENGINE_MAPPING using engine_string."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_path(path):\n    if path is None:\n        raise ValueError(\"path must be a string\")\n\n    parts = path.strip(\"/\").split(\"/\")\n\n    database = unquote_plus(parts[0]) if len(parts) else None\n    schema = parts[1] if len(parts) > 1 else None\n\n    return database, schema", "response": "Parse a path into database and schema."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a dictionary of django - style database configuration based on url.", "response": "def database_conf_from_url(url):\n    \"\"\"\n    Return a django-style database configuration based on ``url``.\n\n    :param url: Database URL\n    :return: Django-style database configuration dict\n\n    Example:\n    >>> conf = database_conf_from_url(\n    ...     'pgsql://joar:hunter2@5monkeys.se:4242/tweets/tweetschema'\n    ...     '?hello=world')\n    >>> sorted(conf.items())  # doctest: +NORMALIZE_WHITESPACE\n    [('ENGINE', 'django.db.backends.postgresql_psycopg2'),\n     ('HOST', '5monkeys.se'),\n     ('NAME', 'tweets'),\n     ('PARAMS', {'hello': 'world'}),\n     ('PASSWORD', 'hunter2'),\n     ('PORT', 4242),\n     ('SCHEMA', 'tweetschema'),\n     ('USER', 'joar')]\n    \"\"\"\n    return {key.upper(): val for key, val in parse_database_url(url)._asdict().items()}"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a database URL and return a DatabaseInfo named tuple.", "response": "def parse_database_url(url):\n    \"\"\"\n    Parse a database URL and return a DatabaseInfo named tuple.\n\n    :param url: Database URL\n    :return: DatabaseInfo instance\n\n    Example:\n    >>> conf = parse_database_url(\n    ...     'pgsql://joar:hunter2@5monkeys.se:4242/tweets/tweetschema'\n    ...     '?hello=world')\n    >>> conf  # doctest: +NORMALIZE_WHITESPACE\n    DatabaseInfo(engine='django.db.backends.postgresql_psycopg2',\n                 name='tweets',\n                 schema='tweetschema',\n                 user='joar',\n                 password='hunter2',\n                 host='5monkeys.se',\n                 port=4242,\n                 params={'hello': 'world'})\n    \"\"\"\n    if url == \"sqlite://:memory:\":\n        raise Exception(\n            'Your url is \"sqlite://:memory:\", if you want '\n            'an sqlite memory database, just use \"sqlite://\"'\n        )\n    url_parts = urlsplit(url)\n\n    engine = get_engine(url_parts.scheme)\n    database, schema = parse_path(url_parts.path)\n    port = url_parts.port\n    host = url_parts.hostname\n    user = url_parts.username\n    password = url_parts.password\n\n    # Take the last element of every parameter list\n    params = {key: val.pop() for key, val in parse_qs(url_parts.query).items()}\n\n    return DatabaseInfo(\n        engine=engine,\n        name=database,\n        schema=schema,\n        user=user,\n        password=password,\n        host=host,\n        port=port,\n        params=params,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list(self, request):\n        serializer = self.get_serializer(request.user)\n        return Response(serializer.data, status=status.HTTP_200_OK)", "response": "Retrieve logged in user info"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate new user record", "response": "def create(self, request):\n        \"\"\"\n        Change password for logged in django staff user\n        \"\"\"\n        # TODO: Decorate api with sensitive post parameters as Django admin do?\n\n        password_form = PasswordChangeForm(request.user, data=request.data)\n\n        if not password_form.is_valid():\n            raise serializers.ValidationError(password_form.errors)\n\n        password_form.save()\n        update_session_auth_hash(request, password_form.user)\n\n        return Response(status=status.HTTP_204_NO_CONTENT)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_url_field(self, field_name, model_class):\n        field, kwargs = super().build_url_field(field_name, model_class)\n\n        view = self.root.context[\"view\"]\n        kwargs[\"view_name\"] = view.get_url_name(\"detail\")\n\n        return field, kwargs", "response": "Build url field for the current queryset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse string to bool.", "response": "def parse_bool(value):\n    \"\"\"\n    Parse string to bool.\n\n    :param str value: String value to parse as bool\n    :return bool:\n    \"\"\"\n    boolean = parse_str(value).capitalize()\n\n    if boolean in (\"True\", \"Yes\", \"On\", \"1\"):\n        return True\n    elif boolean in (\"False\", \"No\", \"Off\", \"0\"):\n        return False\n    else:\n        raise ValueError('Unable to parse boolean value \"{}\"'.format(value))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_int(value):\n    value = parse_str(value=value)\n    if value.startswith(\"0\"):\n        return int(value.lstrip(\"0o\"), 8)\n    else:\n        return int(value)", "response": "Parses numeric string to int. Supports oct formatted string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_parser(typ):\n    try:\n        return {\n            str: parse_str,\n            bool: parse_bool,\n            int: parse_int,\n            tuple: parse_tuple,\n            list: parse_list,\n            set: parse_set,\n        }[typ]\n    except KeyError:\n        raise NotImplementedError(\"Unsupported setting type: %r\", typ)", "response": "Return appropriate parser for given type."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_settings():\n    settings = {}\n    prefix = environ.get(\"DJANGO_SETTINGS_PREFIX\", \"DJANGO_\")\n\n    for key, value in environ.items():\n        _, _, key = key.partition(prefix)\n        if key:\n            if key in UNSUPPORTED_ENV_SETTINGS:\n                raise ValueError(\n                    'Django setting \"{}\" can not be '\n                    \"configured through environment.\".format(key)\n                )\n\n            default_value = getattr(global_settings, key, UNDEFINED)\n\n            if default_value is not UNDEFINED:\n                if default_value is None and key in SETTINGS_TYPES.keys():\n                    # Handle typed django settings defaulting to None\n                    parse = get_parser(SETTINGS_TYPES[key])\n                else:\n                    # Determine parser by django setting type\n                    parse = get_parser(type(default_value))\n\n                value = parse(value)\n\n            settings[key] = value\n\n    return settings", "response": "Get and parse django settings from environment variables."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new instance of the class ModelDict from a django model instance.", "response": "def from_model(cls, model, *fields, **named_fields):\n        \"\"\"\n        Work-in-progress constructor,\n        consuming fields and values from django model instance.\n        \"\"\"\n        d = ModelDict()\n\n        if not (fields or named_fields):\n            # Default to all fields\n            fields = [f.attname for f in model._meta.concrete_fields]\n\n        not_found = object()\n\n        for name, field in chain(zip(fields, fields), named_fields.items()):\n            _fields = field.split(\"__\")\n            value = model\n            for i, _field in enumerate(_fields, start=1):\n                # NOTE: we don't want to rely on hasattr here\n                previous_value = value\n                value = getattr(previous_value, _field, not_found)\n\n                if value is not_found:\n                    if _field in dir(previous_value):\n                        raise ValueError(\n                            \"{!r}.{} had an AttributeError exception\".format(\n                                previous_value, _field\n                            )\n                        )\n                    else:\n                        raise AttributeError(\n                            \"{!r} does not have {!r} attribute\".format(\n                                previous_value, _field\n                            )\n                        )\n\n                elif value is None:\n                    if name not in named_fields:\n                        name = \"__\".join(_fields[:i])\n                    break\n\n            d[name] = value\n\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a field by field info dict.", "response": "def create_field(field_info):\n    \"\"\"\n    Create a field by field info dict.\n    \"\"\"\n    field_type = field_info.get('type')\n    if field_type not in FIELDS_NAME_MAP:\n        raise ValueError(_('not support this field: {}').format(field_type))\n    field_class = FIELDS_NAME_MAP.get(field_type)\n    params = dict(field_info)\n    params.pop('type')\n    return field_class.from_dict(params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_validator(data_struct_dict, name=None):\n\n    if name is None:\n        name = 'FromDictValidator'\n    attrs = {}\n    for field_name, field_info in six.iteritems(data_struct_dict):\n        field_type = field_info['type']\n        if field_type == DictField.FIELD_TYPE_NAME and isinstance(field_info.get('validator'), dict):\n            field_info['validator'] = create_validator(field_info['validator'])\n        attrs[field_name] = create_field(field_info)\n    name = force_str(name)\n    return type(name, (Validator, ), attrs)", "response": "create a Validator instance from a dict"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cartesian_product(parameter_dict, combined_parameters=()):\n    if not combined_parameters:\n        combined_parameters = list(parameter_dict)\n    else:\n        combined_parameters = list(combined_parameters)\n\n    for idx, item in enumerate(combined_parameters):\n        if isinstance(item, str):\n            combined_parameters[idx] = (item,)\n\n    iterator_list = []\n    for item_tuple in combined_parameters:\n        inner_iterator_list = [parameter_dict[key] for key in item_tuple]\n        zipped_iterator = zip(*inner_iterator_list)\n        iterator_list.append(zipped_iterator)\n\n    result_dict = {}\n    for key in parameter_dict:\n        result_dict[key] = []\n\n    cartesian_iterator = itools.product(*iterator_list)\n\n    for cartesian_tuple in cartesian_iterator:\n        for idx, item_tuple in enumerate(combined_parameters):\n            for inneridx, key in enumerate(item_tuple):\n                result_dict[key].append(cartesian_tuple[idx][inneridx])\n\n    return result_dict", "response": "Generates a Cartesian product of the input parameter dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntaking a list of explored parameters and finds unique parameter combinations.", "response": "def find_unique_points(explored_parameters):\n    \"\"\"Takes a list of explored parameters and finds unique parameter combinations.\n\n    If parameter ranges are hashable operates in O(N), otherwise O(N**2).\n\n    :param explored_parameters:\n\n        List of **explored** parameters\n\n    :return:\n\n        List of tuples, first entry being the parameter values, second entry a list\n        containing the run position of the unique combination.\n\n    \"\"\"\n    ranges = [param.f_get_range(copy=False) for param in explored_parameters]\n    zipped_tuples = list(zip(*ranges))\n    try:\n        unique_elements = OrderedDict()\n        for idx, val_tuple in enumerate(zipped_tuples):\n            if val_tuple not in unique_elements:\n                unique_elements[val_tuple] = []\n            unique_elements[val_tuple].append(idx)\n        return list(unique_elements.items())\n    except TypeError:\n        logger = logging.getLogger('pypet.find_unique')\n        logger.error('Your parameter entries could not be hashed, '\n                     'now I am sorting slowly in O(N**2).')\n        unique_elements = []\n        for idx, val_tuple in enumerate(zipped_tuples):\n            matches = False\n            for added_tuple, pos_list in unique_elements:\n                matches = True\n                for idx2, val in enumerate(added_tuple):\n                    if not explored_parameters[idx2]._equal_values(val_tuple[idx2], val):\n                        matches = False\n                        break\n                if matches:\n                    pos_list.append(idx)\n                    break\n            if not matches:\n                unique_elements.append((val_tuple, [idx]))\n        return unique_elements"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _change_logging_kwargs(kwargs):\n    log_levels = kwargs.pop('log_level', None)\n    log_folder = kwargs.pop('log_folder', 'logs')\n    logger_names = kwargs.pop('logger_names', '')\n    if log_levels is None:\n        log_levels = kwargs.pop('log_levels', logging.INFO)\n    log_multiproc = kwargs.pop('log_multiproc', True)\n\n    if not isinstance(logger_names, (tuple, list)):\n        logger_names = [logger_names]\n    if not isinstance(log_levels, (tuple, list)):\n        log_levels = [log_levels]\n    if len(log_levels) == 1:\n        log_levels = [log_levels[0] for _ in logger_names]\n\n    # We don't want to manipulate the original dictionary\n    dictionary = copy.deepcopy(LOGGING_DICT)\n    prefixes = ['']\n    if not log_multiproc:\n        for key in list(dictionary.keys()):\n            if key.startswith('multiproc_'):\n                del dictionary[key]\n    else:\n        prefixes.append('multiproc_')\n\n    # Add all handlers to all loggers\n    for prefix in prefixes:\n        for handler_dict in dictionary[prefix + 'handlers'].values():\n            if 'filename' in handler_dict:\n                filename = os.path.join(log_folder, handler_dict['filename'])\n                filename = os.path.normpath(filename)\n                handler_dict['filename'] = filename\n        dictionary[prefix + 'loggers'] = {}\n        logger_dict = dictionary[prefix + 'loggers']\n        for idx, logger_name in enumerate(logger_names):\n            logger_dict[logger_name] = {\n                'level': log_levels[idx],\n                'handlers': list(dictionary[prefix + 'handlers'].keys())\n            }\n\n    kwargs['log_config'] = dictionary", "response": "Helper function to turn the simple logging kwargs into a log_config."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef simple_logging_config(func):\n\n    @functools.wraps(func)\n    def new_func(self, *args, **kwargs):\n        if use_simple_logging(kwargs):\n            if 'log_config' in kwargs:\n                raise ValueError('Please do not specify `log_config` '\n                                 'if you want to use the simple '\n                                 'way of providing logging configuration '\n                                 '(i.e using `log_folder`, `logger_names` and/or `log_levels`).')\n            _change_logging_kwargs(kwargs)\n\n        return func(self, *args, **kwargs)\n\n    return new_func", "response": "Decorator to allow a simple logging configuration."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef try_make_dirs(filename):\n    try:\n        dirname = os.path.dirname(os.path.normpath(filename))\n        racedirs(dirname)\n    except Exception as exc:\n        sys.stderr.write('ERROR during log config file handling, could not create dirs for '\n                         'filename `%s` because of: %s' % (filename, repr(exc)))", "response": "Tries to make directories for a given log file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn all valid python strings inside a given string.", "response": "def get_strings(args):\n    \"\"\"Returns all valid python strings inside a given argument string.\"\"\"\n    string_list = []\n    for elem in ast.walk(ast.parse(args)):\n        if isinstance(elem, ast.Str):\n            string_list.append(elem.s)\n    return string_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rename_log_file(filename, trajectory=None,\n                    env_name=None,\n                    traj_name=None,\n                    set_name=None,\n                    run_name=None,\n                    process_name=None,\n                    host_name=None):\n    \"\"\" Renames a given `filename` with valid wildcard placements.\n\n    :const:`~pypet.pypetconstants.LOG_ENV` ($env) is replaces by the name of the\n    trajectory`s environment.\n\n    :const:`~pypet.pypetconstants.LOG_TRAJ` ($traj) is replaced by the name of the\n    trajectory.\n\n    :const:`~pypet.pypetconstants.LOG_RUN` ($run) is replaced by the name of the current\n    run. If the trajectory is not set to a run 'run_ALL' is used.\n\n    :const:`~pypet.pypetconstants.LOG_SET` ($set) is replaced by the name of the current\n    run set. If the trajectory is not set to a run 'run_set_ALL' is used.\n\n    :const:`~pypet.pypetconstants.LOG_PROC` ($proc) is replaced by the name fo the\n    current process.\n\n    :const:`~pypet.pypetconstant.LOG_HOST` ($host) is replaced by the name of the current host.\n\n    :param filename:  A filename string\n    :param traj:  A trajectory container, leave `None` if you provide all the parameters below\n    :param env_name: Name of environemnt, leave `None` to get it from `traj`\n    :param traj_name: Name of trajectory, leave `None` to get it from `traj`\n    :param set_name: Name of run set, leave `None` to get it from `traj`\n    :param run_name: Name of run, leave `None` to get it from `traj`\n    :param process_name:\n\n        The name of the desired process. If `None` the name of the current process is\n        taken determined by the multiprocessing module.\n\n    :param host_name:\n\n        Name of host, leave `None` to determine it automatically with the platform module.\n\n    :return: The new filename\n\n    \"\"\"\n    if pypetconstants.LOG_ENV in filename:\n        if env_name is None:\n            env_name = trajectory.v_environment_name\n        filename = filename.replace(pypetconstants.LOG_ENV, env_name)\n    if pypetconstants.LOG_TRAJ in filename:\n        if traj_name is None:\n            traj_name = trajectory.v_name\n        filename = filename.replace(pypetconstants.LOG_TRAJ, traj_name)\n    if pypetconstants.LOG_RUN in filename:\n        if run_name is None:\n            run_name = trajectory.f_wildcard('$')\n        filename = filename.replace(pypetconstants.LOG_RUN, run_name)\n    if pypetconstants.LOG_SET in filename:\n        if set_name is None:\n            set_name = trajectory.f_wildcard('$set')\n        filename = filename.replace(pypetconstants.LOG_SET, set_name)\n    if pypetconstants.LOG_PROC in filename:\n        if process_name is None:\n            process_name = multip.current_process().name + '-' + str(os.getpid())\n        filename = filename.replace(pypetconstants.LOG_PROC, process_name)\n    if pypetconstants.LOG_HOST in filename:\n        if host_name is None:\n            host_name = socket.getfqdn().replace('.', '-')\n        filename = filename.replace(pypetconstants.LOG_HOST, host_name)\n    return filename", "response": "Renames a given log file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a logger with a given name.", "response": "def _set_logger(self, name=None):\n        \"\"\"Adds a logger with a given `name`.\n\n        If no name is given, name is constructed as\n        `type(self).__name__`.\n\n        \"\"\"\n        if name is None:\n            cls = self.__class__\n            name = '%s.%s' % (cls.__module__, cls.__name__)\n        self._logger = logging.getLogger(name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef extract_replacements(self, trajectory):\n        self.env_name = trajectory.v_environment_name\n        self.traj_name = trajectory.v_name\n        self.set_name =  trajectory.f_wildcard('$set')\n        self.run_name = trajectory.f_wildcard('$')", "response": "Extracts the wildcards and file replacements from the trajectory"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _check_and_replace_parser_args(parser, section, option, rename_func, make_dirs=True):\n        args = parser.get(section, option, raw=True)\n        strings = get_strings(args)\n        replace = False\n        for string in strings:\n            isfilename = any(x in string for x in FILENAME_INDICATORS)\n            if isfilename:\n                newstring = rename_func(string)\n                if make_dirs:\n                    try_make_dirs(newstring)\n                # To work with windows path specifications we need this replacement:\n                raw_string = string.replace('\\\\', '\\\\\\\\')\n                raw_newstring = newstring.replace('\\\\', '\\\\\\\\')\n                args = args.replace(raw_string, raw_newstring)\n                replace = True\n        if replace:\n            parser.set(section, option, args)", "response": "Checks if the given parser settings are found and replaces them with the corresponding values."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nturns a ConfigParser into a StringIO stream.", "response": "def _parser_to_string_io(parser):\n        \"\"\"Turns a ConfigParser into a StringIO stream.\"\"\"\n        memory_file = StringIO()\n        parser.write(memory_file)\n        memory_file.flush()\n        memory_file.seek(0)\n        return memory_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsearches for multiprocessing options within a ConfigParser.", "response": "def _find_multiproc_options(parser):\n        \"\"\" Searches for multiprocessing options within a ConfigParser.\n\n        If such options are found, they are copied (without the `'multiproc_'` prefix)\n        into a new parser.\n\n        \"\"\"\n        sections = parser.sections()\n        if not any(section.startswith('multiproc_') for section in sections):\n            return None\n        mp_parser = NoInterpolationParser()\n        for section in sections:\n            if section.startswith('multiproc_'):\n                new_section = section.replace('multiproc_', '')\n                mp_parser.add_section(new_section)\n                options = parser.options(section)\n                for option in options:\n                    val = parser.get(section, option, raw=True)\n                    mp_parser.set(new_section, option, val)\n        return mp_parser"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _find_multiproc_dict(dictionary):\n        if not any(key.startswith('multiproc_') for key in dictionary.keys()):\n            return None\n        mp_dictionary = {}\n        for key in dictionary.keys():\n            if key.startswith('multiproc_'):\n                new_key = key.replace('multiproc_', '')\n                mp_dictionary[new_key] = dictionary[key]\n        mp_dictionary['version'] = dictionary['version']\n        if 'disable_existing_loggers' in dictionary:\n            mp_dictionary['disable_existing_loggers'] = dictionary['disable_existing_loggers']\n        return mp_dictionary", "response": "Searches for multiprocessing options in a given dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_log_config(self):\n        if self.report_progress:\n            if self.report_progress is True:\n                self.report_progress = (5, 'pypet', logging.INFO)\n            elif isinstance(self.report_progress, (int, float)):\n                self.report_progress = (self.report_progress, 'pypet', logging.INFO)\n            elif isinstance(self.report_progress, str):\n                self.report_progress = (5, self.report_progress, logging.INFO)\n            elif len(self.report_progress) == 2:\n                self.report_progress = (self.report_progress[0], self.report_progress[1],\n                                        logging.INFO)\n\n        if self.log_config:\n            if self.log_config == pypetconstants.DEFAULT_LOGGING:\n                pypet_path = os.path.abspath(os.path.dirname(__file__))\n                init_path = os.path.join(pypet_path, 'logging')\n                self.log_config = os.path.join(init_path, 'default.ini')\n\n            if isinstance(self.log_config, str):\n                if not os.path.isfile(self.log_config):\n                    raise ValueError('Could not find the logger init file '\n                                     '`%s`.' % self.log_config)\n                parser = NoInterpolationParser()\n                parser.read(self.log_config)\n            elif isinstance(self.log_config, cp.RawConfigParser):\n                parser = self.log_config\n            else:\n                parser = None\n\n            if parser is not None:\n                self._sp_config = self._parser_to_string_io(parser)\n                self._mp_config = self._find_multiproc_options(parser)\n                if self._mp_config is not None:\n                    self._mp_config = self._parser_to_string_io(self._mp_config)\n\n            elif isinstance(self.log_config, dict):\n                self._sp_config = self.log_config\n                self._mp_config = self._find_multiproc_dict(self._sp_config)\n\n        if self.log_stdout:\n            if self.log_stdout is True:\n                self.log_stdout = ('STDOUT', logging.INFO)\n            if isinstance(self.log_stdout, str):\n                self.log_stdout = (self.log_stdout, logging.INFO)\n            if isinstance(self.log_stdout, int):\n                self.log_stdout = ('STDOUT', self.log_stdout)", "response": "Checks and converts all settings if necessary passed to the Manager."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks for filenames within a config file and translates them.", "response": "def _handle_config_parsing(self, log_config):\n        \"\"\" Checks for filenames within a config file and translates them.\n\n        Moreover, directories for the files are created as well.\n\n        :param log_config: Config file as a stream (like StringIO)\n\n        \"\"\"\n        parser = NoInterpolationParser()\n        parser.readfp(log_config)\n\n        rename_func = lambda string: rename_log_file(string,\n                                                     env_name=self.env_name,\n                                                     traj_name=self.traj_name,\n                                                     set_name=self.set_name,\n                                                     run_name=self.run_name)\n\n        sections = parser.sections()\n        for section in sections:\n            options = parser.options(section)\n            for option in options:\n                if option == 'args':\n                    self._check_and_replace_parser_args(parser, section, option,\n                                                        rename_func=rename_func)\n        return parser"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _handle_dict_config(self, log_config):\n        new_dict = dict()\n        for key in log_config.keys():\n            if key == 'filename':\n                filename = log_config[key]\n                filename = rename_log_file(filename,\n                                           env_name=self.env_name,\n                                           traj_name=self.traj_name,\n                                           set_name=self.set_name,\n                                           run_name=self.run_name)\n                new_dict[key] = filename\n                try_make_dirs(filename)\n            elif isinstance(log_config[key], dict):\n                inner_dict = self._handle_dict_config(log_config[key])\n                new_dict[key] = inner_dict\n            else:\n                new_dict[key] = log_config[key]\n        return new_dict", "response": "Recursively walks and copies the log_config dict and creates directories if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate logging handlers and redirects stdout.", "response": "def make_logging_handlers_and_tools(self, multiproc=False):\n        \"\"\"Creates logging handlers and redirects stdout.\"\"\"\n\n        log_stdout = self.log_stdout\n        if sys.stdout is self._stdout_to_logger:\n            # If we already redirected stdout we don't neet to redo it again\n            log_stdout = False\n\n        if self.log_config:\n            if multiproc:\n                proc_log_config = self._mp_config\n            else:\n                proc_log_config = self._sp_config\n\n            if proc_log_config:\n                if isinstance(proc_log_config, dict):\n                    new_dict = self._handle_dict_config(proc_log_config)\n                    dictConfig(new_dict)\n                else:\n                    parser = self._handle_config_parsing(proc_log_config)\n                    memory_file = self._parser_to_string_io(parser)\n                    fileConfig(memory_file, disable_existing_loggers=False)\n\n        if log_stdout:\n            #  Create a logging mock for stdout\n            std_name, std_level = self.log_stdout\n\n            stdout = StdoutToLogger(std_name, log_level=std_level)\n            stdout.start()\n            self._tools.append(stdout)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinalizes the manager closes and removes all handlers if desired.", "response": "def finalize(self, remove_all_handlers=True):\n        \"\"\"Finalizes the manager, closes and removes all handlers if desired.\"\"\"\n        for tool in self._tools:\n            tool.finalize()\n        self._tools = []\n        self._stdout_to_logger = None\n        for config in (self._sp_config, self._mp_config):\n            if hasattr(config, 'close'):\n                config.close()\n        self._sp_config = None\n        self._mp_config = None\n        if remove_all_handlers:\n            self.tabula_rasa()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstarts redirection of stdout.", "response": "def start(self):\n        \"\"\"Starts redirection of `stdout`\"\"\"\n        if sys.stdout is not self:\n            self._original_steam = sys.stdout\n            sys.stdout = self\n            self._redirection = True\n        if self._redirection:\n            print('Established redirection of `stdout`.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write(self, buf):\n        if not self._recursion:\n            self._recursion = True\n            try:\n                for line in buf.rstrip().splitlines():\n                    self._logger.log(self._log_level, line.rstrip())\n            finally:\n                self._recursion = False\n        else:\n            # If stderr is redirected to stdout we can avoid further recursion by\n            sys.__stderr__.write('ERROR: Recursion in Stream redirection!')", "response": "Writes data from buffer to logger"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef finalize(self):\n        if self._original_steam is not None and self._redirection:\n            sys.stdout = self._original_steam\n            print('Disabled redirection of `stdout`.')\n            self._redirection = False\n            self._original_steam = None", "response": "Disables redirection of stdout."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef results_equal(a, b):\n    if a.v_is_parameter and b.v_is_parameter:\n        raise ValueError('Both inputs are not results.')\n\n    if a.v_is_parameter or b.v_is_parameter:\n        return False\n\n    if a.v_full_name != b.v_full_name:\n        return False\n\n    if hasattr(a, '_data') and not hasattr(b, '_data'):\n        return False\n\n    if hasattr(a, '_data'):\n        akeyset = set(a._data.keys())\n        bkeyset = set(b._data.keys())\n\n        if akeyset != bkeyset:\n            return False\n\n        for key in a._data:\n            val = a._data[key]\n            bval = b._data[key]\n\n            if not nested_equal(val, bval):\n                return False\n\n    return True", "response": "Compares two result instances."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncompares two parameter instances.", "response": "def parameters_equal(a, b):\n    \"\"\"Compares two parameter instances\n\n    Checks full name, data, and ranges. Does not consider the comment.\n\n    :return: True or False\n\n    :raises: ValueError if both inputs are no parameter instances\n\n    \"\"\"\n    if (not b.v_is_parameter and\n            not a.v_is_parameter):\n        raise ValueError('Both inputs are not parameters')\n\n    if (not b.v_is_parameter or\n            not a.v_is_parameter):\n        return False\n\n    if a.v_full_name != b.v_full_name:\n        return False\n\n    if a.f_is_empty() and b.f_is_empty():\n        return True\n\n    if a.f_is_empty() != b.f_is_empty():\n        return False\n\n    if not a._values_of_same_type(a.f_get(), b.f_get()):\n        return False\n\n    if not a._equal_values(a.f_get(), b.f_get()):\n        return False\n\n    if a.f_has_range() != b.f_has_range():\n        return False\n\n    if a.f_has_range():\n        if a.f_get_range_length() != b.f_get_range_length():\n            return False\n\n        for myitem, bitem in zip(a.f_get_range(copy=False), b.f_get_range(copy=False)):\n            if not a._values_of_same_type(myitem, bitem):\n                return False\n            if not a._equal_values(myitem, bitem):\n                return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_all_attributes(instance):\n    try:\n        result_dict = instance.__dict__.copy()\n    except AttributeError:\n        result_dict = {}\n\n    if hasattr(instance, '__all_slots__'):\n        all_slots = instance.__all_slots__\n    else:\n        all_slots = slots.get_all_slots(instance.__class__)\n\n    for slot in all_slots:\n        result_dict[slot] = getattr(instance, slot)\n\n    result_dict.pop('__dict__', None)\n    result_dict.pop('__weakref__', None)\n\n    return result_dict", "response": "Returns an attribute value dictionary much like __dict__ but incorporates __slots__"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncompares two objects recursively by their elements.", "response": "def nested_equal(a, b):\n    \"\"\"Compares two objects recursively by their elements.\n\n    Also handles numpy arrays, pandas data and sparse matrices.\n\n    First checks if the data falls into the above categories.\n    If not, it is checked if a or b are some type of sequence or mapping and\n    the contained elements are compared.\n    If this is not the case, it is checked if a or b do provide a custom `__eq__` that\n    evaluates to a single boolean value.\n    If this is not the case, the attributes of a and b are compared.\n    If this does not help either, normal `==` is used.\n\n    Assumes hashable items are not mutable in a way that affects equality.\n    Based on the suggestion from HERE_, thanks again Lauritz V. Thaulow :-)\n\n    .. _HERE: http://stackoverflow.com/questions/18376935/best-practice-for-equality-in-python\n\n    \"\"\"\n    if a is b:\n        return True\n\n    if a is None or b is None:\n        return False\n\n    a_sparse = spsp.isspmatrix(a)\n    b_sparse = spsp.isspmatrix(b)\n    if a_sparse != b_sparse:\n        return False\n    if a_sparse:\n        if a.nnz == 0:\n            return b.nnz == 0\n        else:\n            return not np.any((a != b).data)\n\n    a_series = isinstance(a, pd.Series)\n    b_series = isinstance(b, pd.Series)\n    if a_series != b_series:\n        return False\n    if a_series:\n        try:\n            eq = (a == b).all()\n            return eq\n        except (TypeError, ValueError):\n            # If Sequence itself contains numpy arrays we get here\n            if not len(a) == len(b):\n                return False\n            for idx, itema in enumerate(a):\n                itemb = b[idx]\n                if not nested_equal(itema, itemb):\n                    return False\n            return True\n\n    a_frame = isinstance(a, pd.DataFrame)\n    b_frame = isinstance(b, pd.DataFrame)\n    if a_frame != b_frame:\n        return False\n    if a_frame:\n        try:\n            if a.empty and b.empty:\n                return True\n            new_frame = a == b\n            new_frame = new_frame | (pd.isnull(a) & pd.isnull(b))\n            if isinstance(new_frame, pd.DataFrame):\n                return np.all(new_frame.as_matrix())\n        except (ValueError, TypeError):\n            # The Value Error can happen if the data frame is of dtype=object and contains\n            # numpy arrays. Numpy array comparisons do not evaluate to a single truth value\n            for name in a:\n                cola = a[name]\n                if not name in b:\n                    return False\n                colb = b[name]\n                if not len(cola) == len(colb):\n                    return False\n                for idx, itema in enumerate(cola):\n                    itemb = colb[idx]\n                    if not nested_equal(itema, itemb):\n                        return False\n            return True\n\n    a_array = isinstance(a, np.ndarray)\n    b_array = isinstance(b, np.ndarray)\n    if a_array != b_array:\n        return False\n    if a_array:\n        if a.shape != b.shape:\n            return False\n        return np.all(a == b)\n\n    a_list = isinstance(a, (Sequence, list, tuple))\n    b_list = isinstance(b, (Sequence, list, tuple))\n    if a_list != b_list:\n        return False\n    if a_list:\n        return all(nested_equal(x, y) for x, y in zip(a, b))\n\n    a_mapping = isinstance(a, (Mapping, dict))\n    b_mapping = isinstance(b, (Mapping, dict))\n    if a_mapping != b_mapping:\n        return False\n    if a_mapping:\n        keys_a = a.keys()\n        if set(keys_a) != set(b.keys()):\n            return False\n        return all(nested_equal(a[k], b[k]) for k in keys_a)\n\n    # Equality for general objects\n    # for types that support __eq__ or __cmp__\n    equality = NotImplemented\n    try:\n        equality = a.__eq__(b)\n    except (AttributeError, NotImplementedError, TypeError, ValueError):\n        pass\n    if equality is NotImplemented:\n        try:\n            equality = b.__eq__(a)\n        except (AttributeError, NotImplementedError, TypeError, ValueError):\n            pass\n    if equality is NotImplemented:\n        try:\n            cmp = a.__cmp__(b)\n            if cmp is not NotImplemented:\n                equality = cmp == 0\n        except (AttributeError, NotImplementedError, TypeError, ValueError):\n            pass\n    if equality is NotImplemented:\n        try:\n            cmp = b.__cmp__(a)\n            if cmp is not NotImplemented:\n                equality = cmp == 0\n        except (AttributeError, NotImplementedError, TypeError, ValueError):\n            pass\n    if equality is not NotImplemented:\n        try:\n            return bool(equality)\n        except (AttributeError, NotImplementedError, TypeError, ValueError):\n            pass\n\n    # Compare objects based on their attributes\n    attributes_a = get_all_attributes(a)\n    attributes_b = get_all_attributes(b)\n    if len(attributes_a) != len(attributes_b):\n        return False\n    if len(attributes_a) > 0:\n        keys_a = list(attributes_a.keys())\n        if set(keys_a) != set(attributes_b.keys()):\n            return False\n\n        return all(nested_equal(attributes_a[k], attributes_b[k]) for k in keys_a)\n\n    # Ok they are really not equal\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef manual_run(turn_into_run=True, store_meta_data=True, clean_up=True):\n    def wrapper(func):\n        @functools.wraps(func)\n        def new_func(traj, *args, **kwargs):\n            do_wrap = not traj._run_by_environment\n            if do_wrap:\n                traj.f_start_run(turn_into_run=turn_into_run)\n            result = func(traj, *args, **kwargs)\n            if do_wrap:\n                traj.f_finalize_run(store_meta_data=store_meta_data,\n                                    clean_up=clean_up)\n            return result\n\n        return new_func\n\n    return wrapper", "response": "Decorator to make a function a manual run."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef deprecated(msg=''):\n\n    def wrapper(func):\n        @functools.wraps(func)\n        def new_func(*args, **kwargs):\n            warning_string = \"Call to deprecated function or property `%s`.\" % func.__name__\n            warning_string = warning_string + ' ' + msg\n            warnings.warn(\n                warning_string,\n                category=DeprecationWarning,\n            )\n            return func(*args, **kwargs)\n\n        return new_func\n\n    return wrapper", "response": "A decorator that marks functions\nFormula as deprecated."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef copydoc(fromfunc, sep=\"\\n\"):\n\n    def _decorator(func):\n        sourcedoc = fromfunc.__doc__\n\n        # Remove the ABSTRACT line:\n        split_doc = sourcedoc.split('\\n')\n        split_doc_no_abstract = [line for line in split_doc if not 'ABSTRACT' in line]\n\n        # If the length is different we have found an ABSTRACT line\n        # Finally we want to remove the final blank line, otherwise\n        # we would have three blank lines at the end\n        if len(split_doc) != len(split_doc_no_abstract):\n            sourcedoc = '\\n'.join(split_doc_no_abstract[:-1])\n\n        if func.__doc__ is None:\n            func.__doc__ = sourcedoc\n        else:\n            func.__doc__ = sep.join([sourcedoc, func.__doc__])\n        return func\n\n    return _decorator", "response": "Decorator to copy the docstring of fromfunc into a new function."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef kwargs_mutual_exclusive(param1_name, param2_name, map2to1=None):\n    def wrapper(func):\n        @functools.wraps(func)\n        def new_func(*args, **kwargs):\n            if param2_name in kwargs:\n                if param1_name in kwargs:\n                    raise ValueError('You cannot specify `%s` and `%s` at the same time, '\n                                     'they are mutually exclusive.' % (param1_name, param2_name))\n                param2 = kwargs.pop(param2_name)\n                if map2to1 is not None:\n                    param1 = map2to1(param2)\n                else:\n                    param1 = param2\n                kwargs[param1_name] = param1\n\n            return func(*args, **kwargs)\n\n        return new_func\n\n    return wrapper", "response": "Decorator to specify mutually exclusive parameters."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef prefix_naming(cls):\n    if hasattr(cls, '__getattr__'):\n        raise TypeError('__getattr__ already defined')\n    cls.__getattr__ = _prfx_getattr_\n    cls.__setattr__ = _prfx_setattr_\n    return cls", "response": "Decorate that adds the prefix naming scheme to the class."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_params(traj):\n\n    # We set the BrianParameter to be the standard parameter\n    traj.v_standard_parameter=Brian2Parameter\n    traj.v_fast_access=True\n\n    # Add parameters we need for our network\n    traj.f_add_parameter('Net.C',281*pF)\n    traj.f_add_parameter('Net.gL',30*nS)\n    traj.f_add_parameter('Net.EL',-70.6*mV)\n    traj.f_add_parameter('Net.VT',-50.4*mV)\n    traj.f_add_parameter('Net.DeltaT',2*mV)\n    traj.f_add_parameter('Net.tauw',40*ms)\n    traj.f_add_parameter('Net.a',4*nS)\n    traj.f_add_parameter('Net.b',0.08*nA)\n    traj.f_add_parameter('Net.I',.8*nA)\n    traj.f_add_parameter('Net.Vcut','vm > 0*mV') # practical threshold condition\n    traj.f_add_parameter('Net.N',50)\n\n    eqs='''\n    dvm/dt=(gL*(EL-vm)+gL*DeltaT*exp((vm-VT)/DeltaT)+I-w)/C : volt\n    dw/dt=(a*(vm-EL)-w)/tauw : amp\n    Vr:volt\n    '''\n    traj.f_add_parameter('Net.eqs', eqs)\n    traj.f_add_parameter('reset', 'vm=Vr;w+=b')", "response": "Adds all necessary parameters to the trajectory."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_net(traj):\n\n    eqs=traj.eqs\n\n    # Create a namespace dictionairy\n    namespace = traj.Net.f_to_dict(short_names=True, fast_access=True)\n    # Create the Neuron Group\n    neuron=NeuronGroup(traj.N, model=eqs, threshold=traj.Vcut, reset=traj.reset,\n                       namespace=namespace)\n    neuron.vm=traj.EL\n    neuron.w=traj.a*(neuron.vm-traj.EL)\n    neuron.Vr=linspace(-48.3*mV,-47.7*mV,traj.N) # bifurcation parameter\n\n    # Run the network initially for 100 milliseconds\n    print('Initial Run')\n    net = Network(neuron)\n    net.run(100*ms, report='text') # we discard the first spikes\n\n    # Create a Spike Monitor\n    MSpike=SpikeMonitor(neuron)\n    net.add(MSpike)\n    # Create a State Monitor for the membrane voltage, record from neurons 1-3\n    MStateV = StateMonitor(neuron, variables=['vm'],record=[1,2,3])\n    net.add(MStateV)\n\n    # Now record for 500 milliseconds\n    print('Measurement run')\n    net.run(500*ms,report='text')\n\n    # Add the BRAIN monitors\n    traj.v_standard_result = Brian2MonitorResult\n    traj.f_add_result('SpikeMonitor',MSpike)\n    traj.f_add_result('StateMonitorV', MStateV)", "response": "Creates and runs BRIAN network based on the parameters in trajectory."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_parameters(traj):\n\n    traj.f_add_parameter('steps', 10000, comment='Number of time steps to simulate')\n    traj.f_add_parameter('dt', 0.01, comment='Step size')\n\n    # Here we want to add the initial conditions as an array parameter. We will simulate\n    # a 3-D differential equation, the Lorenz attractor.\n    traj.f_add_parameter(ArrayParameter,'initial_conditions', np.array([0.0,0.0,0.0]),\n                         comment = 'Our initial conditions, as default we will start from'\n                                   ' origin!')\n\n    # We will group all parameters of the Lorenz differential equation into the group 'func_params'\n    traj.f_add_parameter('func_params.sigma', 10.0)\n    traj.f_add_parameter('func_params.beta', 8.0/3.0)\n    traj.f_add_parameter('func_params.rho', 28.0)\n\n    #For the fun of it we will annotate the  group\n    traj.func_params.v_annotations.info='This group contains as default the original values chosen ' \\\n                                   'by Edward Lorenz in 1963. Check it out on wikipedia ' \\\n                                   '(https://en.wikipedia.org/wiki/Lorenz_attractor)!'", "response": "Adds all necessary parameters to the trajectory container."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef diff_lorenz(value_array, sigma, beta, rho):\n    diff_array = np.zeros(3)\n    diff_array[0] = sigma * (value_array[1]-value_array[0])\n    diff_array[1] = value_array[0] * (rho - value_array[2]) - value_array[1]\n    diff_array[2] = value_array[0] * value_array[1] - beta * value_array[2]\n\n    return diff_array", "response": "The Lorenz attractor differential equation"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a storage service from a constructor and checks which kwargs are not used", "response": "def _create_storage(storage_service, trajectory=None, **kwargs):\n    \"\"\"Creates a service from a constructor and checks which kwargs are not used\"\"\"\n    kwargs_copy = kwargs.copy()\n    kwargs_copy['trajectory'] = trajectory\n    matching_kwargs = get_matching_kwargs(storage_service, kwargs_copy)\n    storage_service = storage_service(**matching_kwargs)\n    unused_kwargs = set(kwargs.keys()) - set(matching_kwargs.keys())\n    return storage_service, unused_kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef storage_factory(storage_service, trajectory=None, **kwargs):\n\n    if 'filename' in kwargs and storage_service is None:\n        filename = kwargs['filename']\n        _, ext = os.path.splitext(filename)\n        if ext in ('.hdf', '.h4', '.hdf4', '.he2', '.h5', '.hdf5', '.he5'):\n            storage_service = HDF5StorageService\n        else:\n            raise ValueError('Extension `%s` of filename `%s` not understood.' %\n                             (ext, filename))\n    elif isinstance(storage_service, str):\n        class_name = storage_service.split('.')[-1]\n        storage_service = create_class(class_name, [storage_service, HDF5StorageService])\n\n    if inspect.isclass(storage_service):\n        return _create_storage(storage_service, trajectory, **kwargs)\n    else:\n        return storage_service, set(kwargs.keys())", "response": "Creates a storage service instance to be extended if new storage services are added"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds all necessary parameters to the trajectory container.", "response": "def add_parameters(traj):\n    \"\"\"Adds all necessary parameters to the `traj` container.\n\n    You can choose between two parameter sets. One for the Lorenz attractor and\n    one for the Roessler attractor.\n    The former is chosen for `traj.diff_name=='diff_lorenz'`, the latter for\n    `traj.diff_name=='diff_roessler'`.\n    You can use parameter presetting to switch between the two cases.\n\n    :raises: A ValueError if `traj.diff_name` is none of the above\n\n    \"\"\"\n    traj.f_add_parameter('steps', 10000, comment='Number of time steps to simulate')\n    traj.f_add_parameter('dt', 0.01, comment='Step size')\n\n    # Here we want to add the initial conditions as an array parameter, since we will simulate\n    # a 3-D differential equation, that is the Roessler attractor\n    # (https://en.wikipedia.org/wiki/R%C3%B6ssler_attractor)\n    traj.f_add_parameter(ArrayParameter,'initial_conditions', np.array([0.0,0.0,0.0]),\n                         comment = 'Our initial conditions, as default we will start from'\n                                   ' origin!')\n\n    # Per default we choose the name `'diff_lorenz'` as in the last example\n    traj.f_add_parameter('diff_name','diff_lorenz', comment= 'Name of our differential equation')\n\n    # We want some control flow depending on which name we really choose\n    if traj.diff_name == 'diff_lorenz':\n        # These parameters are for the Lorenz differential equation\n        traj.f_add_parameter('func_params.sigma', 10.0)\n        traj.f_add_parameter('func_params.beta', 8.0/3.0)\n        traj.f_add_parameter('func_params.rho', 28.0)\n    elif traj.diff_name == 'diff_roessler':\n        # If we use the Roessler system we need different parameters\n        traj.f_add_parameter('func_params.a', 0.1)\n        traj.f_add_parameter('func_params.c', 14.0)\n    else:\n        raise ValueError('I don\\'t know what %s is.' % traj.diff_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if one of the parameters in group_node is explored.", "response": "def _explored_parameters_in_group(traj, group_node):\n    \"\"\"Checks if one the parameters in `group_node` is explored.\n\n    :param traj: Trajectory container\n    :param group_node: Group node\n    :return: `True` or `False`\n\n    \"\"\"\n    explored = False\n    for param in traj.f_get_explored_parameters():\n            if param in group_node:\n                explored = True\n                break\n\n    return explored"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding all parameters to the trajectory.", "response": "def add_parameters(traj):\n        \"\"\"Adds all neuron group parameters to `traj`.\"\"\"\n        assert(isinstance(traj,Trajectory))\n\n        scale = traj.simulation.scale\n\n\n        traj.v_standard_parameter = Brian2Parameter\n\n        model_eqs = '''dV/dt= 1.0/tau_POST * (mu - V) + I_syn : 1\n                       mu : 1\n                       I_syn =  - I_syn_i + I_syn_e : Hz\n                    '''\n\n        conn_eqs = '''I_syn_PRE = x_PRE/(tau2_PRE-tau1_PRE) : Hz\n                      dx_PRE/dt = -(normalization_PRE*y_PRE+x_PRE)*invtau1_PRE : 1\n                      dy_PRE/dt = -y_PRE*invtau2_PRE : 1\n                   '''\n\n        traj.f_add_parameter('model.eqs', model_eqs,\n                           comment='The differential equation for the neuron model')\n\n        traj.f_add_parameter('model.synaptic.eqs', conn_eqs,\n                           comment='The differential equation for the synapses. '\n                                   'PRE will be replaced by `i` or `e` depending '\n                                   'on the source population')\n\n        traj.f_add_parameter('model.synaptic.tau1', 1*ms, comment = 'The decay time')\n        traj.f_add_parameter('model.synaptic.tau2_e', 3*ms, comment = 'The rise time, excitatory')\n        traj.f_add_parameter('model.synaptic.tau2_i', 2*ms, comment = 'The rise time, inhibitory')\n\n        traj.f_add_parameter('model.V_th', 'V >= 1.0', comment = \"Threshold value\")\n        traj.f_add_parameter('model.reset_func', 'V=0.0',\n                             comment = \"String representation of reset function\")\n        traj.f_add_parameter('model.refractory', 5*ms, comment = \"Absolute refractory period\")\n\n        traj.f_add_parameter('model.N_e', int(2000*scale), comment = \"Amount of excitatory neurons\")\n        traj.f_add_parameter('model.N_i', int(500*scale), comment = \"Amount of inhibitory neurons\")\n\n        traj.f_add_parameter('model.tau_e', 15*ms, comment = \"Membrane time constant, excitatory\")\n        traj.f_add_parameter('model.tau_i', 10*ms, comment = \"Membrane time constant, inhibitory\")\n\n        traj.f_add_parameter('model.mu_e_min', 1.1, comment = \"Lower bound for bias, excitatory\")\n        traj.f_add_parameter('model.mu_e_max', 1.2, comment = \"Upper bound for bias, excitatory\")\n\n        traj.f_add_parameter('model.mu_i_min', 1.0, comment = \"Lower bound for bias, inhibitory\")\n        traj.f_add_parameter('model.mu_i_max', 1.05, comment = \"Upper bound for bias, inhibitory\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _build_model_eqs(traj):\n        model_eqs = traj.model.eqs\n        post_eqs={}\n        for name_post in ['i','e']:\n            variables_dict ={}\n            new_model_eqs=model_eqs.replace('POST', name_post)\n            for name_pre in ['i', 'e']:\n                conn_eqs = traj.model.synaptic.eqs\n                new_conn_eqs = conn_eqs.replace('PRE', name_pre)\n                new_model_eqs += new_conn_eqs\n\n                tau1 = traj.model.synaptic['tau1']\n                tau2 = traj.model.synaptic['tau2_'+name_pre]\n\n                normalization = (tau1-tau2) / tau2\n                invtau1=1.0/tau1\n                invtau2 = 1.0/tau2\n\n                variables_dict['invtau1_'+name_pre] = invtau1\n                variables_dict['invtau2_'+name_pre] = invtau2\n                variables_dict['normalization_'+name_pre] = normalization\n                variables_dict['tau1_'+name_pre] = tau1\n                variables_dict['tau2_'+name_pre] = tau2\n\n            variables_dict['tau_'+name_post] = traj.model['tau_'+name_post]\n\n            post_eqs[name_post] = Equations(new_model_eqs, **variables_dict)\n\n        return post_eqs", "response": "Builds the model equations for the inhibitory and inhibitory neurons and excitatory neurons."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pre_build(self, traj, brian_list, network_dict):\n        self._pre_build = not _explored_parameters_in_group(traj, traj.parameters.model)\n\n        if self._pre_build:\n            self._build_model(traj, brian_list, network_dict)", "response": "Pre - builds the neuron groups."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build(self, traj, brian_list, network_dict):\n        if not hasattr(self, '_pre_build') or not self._pre_build:\n            self._build_model(traj, brian_list, network_dict)", "response": "Builds the neuron groups."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _build_model(self, traj, brian_list, network_dict):\n\n        model = traj.parameters.model\n\n        # Create the equations for both models\n        eqs_dict = self._build_model_eqs(traj)\n\n        # Create inhibitory neurons\n        eqs_i = eqs_dict['i']\n        neurons_i = NeuronGroup(N=model.N_i,\n                              model = eqs_i,\n                              threshold=model.V_th,\n                              reset=model.reset_func,\n                              refractory=model.refractory,\n                              method='Euler')\n\n        # Create excitatory neurons\n        eqs_e = eqs_dict['e']\n        neurons_e = NeuronGroup(N=model.N_e,\n                              model = eqs_e,\n                              threshold=model.V_th,\n                              reset=model.reset_func,\n                              refractory=model.refractory,\n                              method='Euler')\n\n\n        # Set the bias terms\n        neurons_e.mu =rand(model.N_e) * (model.mu_e_max - model.mu_e_min) + model.mu_e_min\n        neurons_i.mu =rand(model.N_i) * (model.mu_i_max - model.mu_i_min) + model.mu_i_min\n\n        # Set initial membrane potentials\n        neurons_e.V = rand(model.N_e)\n        neurons_i.V = rand(model.N_i)\n\n        # Add both groups to the `brian_list` and the `network_dict`\n        brian_list.append(neurons_i)\n        brian_list.append(neurons_e)\n        network_dict['neurons_e']=neurons_e\n        network_dict['neurons_i']=neurons_i", "response": "Builds the neuron groups from the trajectory."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_parameters(traj):\n        assert(isinstance(traj,Trajectory))\n\n        traj.v_standard_parameter = Brian2Parameter\n        scale = traj.simulation.scale\n\n        traj.f_add_parameter('connections.R_ee', 1.0, comment='Scaling factor for clustering')\n\n        traj.f_add_parameter('connections.clustersize_e', 100, comment='Size of a cluster')\n        traj.f_add_parameter('connections.strength_factor', 2.5,\n                             comment='Factor for scaling cluster weights')\n\n        traj.f_add_parameter('connections.p_ii', 0.25,\n                            comment='Connection probability from inhibitory to inhibitory' )\n        traj.f_add_parameter('connections.p_ei', 0.25,\n                            comment='Connection probability from inhibitory to excitatory' )\n        traj.f_add_parameter('connections.p_ie', 0.25,\n                            comment='Connection probability from excitatory to inhibitory' )\n        traj.f_add_parameter('connections.p_ee', 0.1,\n                            comment='Connection probability from excitatory to excitatory' )\n\n        traj.f_add_parameter('connections.J_ii', 0.027/np.sqrt(scale),\n                             comment='Connection strength from inhibitory to inhibitory')\n        traj.f_add_parameter('connections.J_ei', 0.032/np.sqrt(scale),\n                             comment='Connection strength from inhibitory to excitatroy')\n        traj.f_add_parameter('connections.J_ie', 0.009/np.sqrt(scale),\n                             comment='Connection strength from excitatory to inhibitory')\n        traj.f_add_parameter('connections.J_ee', 0.012/np.sqrt(scale),\n                             comment='Connection strength from excitatory to excitatory')", "response": "Adds all parameters to the trajectory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pre_build(self, traj, brian_list, network_dict):\n        self._pre_build = not _explored_parameters_in_group(traj, traj.parameters.connections)\n\n        self._pre_build = (self._pre_build and 'neurons_i' in network_dict and\n                           'neurons_e' in network_dict)\n\n        if self._pre_build:\n            self._build_connections(traj, brian_list, network_dict)", "response": "Pre - builds the connections."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build(self, traj, brian_list, network_dict):\n        if not hasattr(self, '_pre_build') or not self._pre_build:\n            self._build_connections(traj, brian_list, network_dict)", "response": "Builds the connections.\n\n        Build is only performed if connections have not\n        been pre-build.\n\n        :param traj: Trajectory container\n\n        :param brian_list:\n\n            List of objects passed to BRIAN network constructor.\n\n            Adds:\n\n            Connections, amount depends on clustering\n\n        :param network_dict:\n\n            Dictionary of elements shared among the components\n\n            Expects:\n\n            'neurons_i': Inhibitory neuron group\n\n            'neurons_e': Excitatory neuron group\n\n            Adds:\n\n            Connections, amount depends on clustering"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconnects neurons i and e and adds all connections to brian_list and adds a list of connections to the network_dict.", "response": "def _build_connections(self, traj, brian_list, network_dict):\n        \"\"\"Connects neuron groups `neurons_i` and `neurons_e`.\n\n        Adds all connections to `brian_list` and adds a list of connections\n        with the key 'connections' to the `network_dict`.\n\n        \"\"\"\n\n        connections = traj.connections\n\n        neurons_i = network_dict['neurons_i']\n        neurons_e = network_dict['neurons_e']\n\n        print('Connecting ii')\n        self.conn_ii = Synapses(neurons_i,neurons_i, on_pre='y_i += %f' % connections.J_ii)\n        self.conn_ii.connect('i != j', p=connections.p_ii)\n\n        print('Connecting ei')\n        self.conn_ei = Synapses(neurons_i,neurons_e, on_pre='y_i += %f' % connections.J_ei)\n        self.conn_ei.connect('i != j', p=connections.p_ei)\n\n        print('Connecting ie')\n        self.conn_ie = Synapses(neurons_e,neurons_i, on_pre='y_e += %f' % connections.J_ie)\n        self.conn_ie.connect('i != j', p=connections.p_ie)\n\n        conns_list = [self.conn_ii, self.conn_ei, self.conn_ie]\n\n\n        if connections.R_ee > 1.0:\n            # If we come here we want to create clusters\n\n            cluster_list=[]\n            cluster_conns_list=[]\n            model=traj.model\n\n            # Compute the number of clusters\n            clusters = int(model.N_e/connections.clustersize_e)\n            traj.f_add_derived_parameter('connections.clusters', clusters, comment='Number of clusters')\n\n            # Compute outgoing connection probability\n            p_out = (connections.p_ee*model.N_e) / \\\n                    (connections.R_ee*connections.clustersize_e+model.N_e- connections.clustersize_e)\n\n            # Compute within cluster connection probability\n            p_in = p_out * connections.R_ee\n\n            # We keep these derived parameters\n            traj.f_add_derived_parameter('connections.p_ee_in', p_in ,\n                                         comment='Connection prob within cluster')\n            traj.f_add_derived_parameter('connections.p_ee_out', p_out ,\n                                         comment='Connection prob to outside of cluster')\n\n\n            low_index = 0\n            high_index = connections.clustersize_e\n            # Iterate through cluster and connect within clusters and to the rest of the neurons\n            for irun in range(clusters):\n\n                cluster = neurons_e[low_index:high_index]\n\n                # Connections within cluster\n                print('Connecting ee cluster #%d of %d' % (irun, clusters))\n                conn = Synapses(cluster,cluster,\n                                on_pre='y_e += %f' % (connections.J_ee*connections.strength_factor))\n                conn.connect('i != j', p=p_in)\n                cluster_conns_list.append(conn)\n\n                # Connections reaching out from cluster\n                # A cluster consists of `clustersize_e` neurons with consecutive indices.\n                # So usually the outside world consists of two groups, neurons with lower\n                # indices than the cluster indices, and neurons with higher indices.\n                # Only the clusters at the index boundaries project to neurons with only either\n                # lower or higher indices\n                if low_index > 0:\n                    rest_low = neurons_e[0:low_index]\n                    print('Connecting cluster with other neurons of lower index')\n                    low_conn = Synapses(cluster,rest_low,\n                                on_pre='y_e += %f' % connections.J_ee)\n                    low_conn.connect('i != j', p=p_out)\n\n                    cluster_conns_list.append(low_conn)\n\n                if high_index < model.N_e:\n                    rest_high = neurons_e[high_index:model.N_e]\n                    print('Connecting cluster with other neurons of higher index')\n\n                    high_conn = Synapses(cluster,rest_high,\n                                on_pre='y_e += %f' % connections.J_ee)\n                    high_conn.connect('i != j', p=p_out)\n\n                    cluster_conns_list.append(high_conn)\n\n                low_index=high_index\n                high_index+=connections.clustersize_e\n\n            self.cluster_conns=cluster_conns_list\n            conns_list+=cluster_conns_list\n        else:\n            # Here we don't cluster and connection probabilities are homogeneous\n            print('Connectiong ee')\n\n            self.conn_ee = Synapses(neurons_e,neurons_e,\n                                on_pre='y_e += %f' % connections.J_ee)\n            self.conn_ee.connect('i != j', p=connections.p_ee)\n\n            conns_list.append(self.conn_ee)\n\n\n        # Add the connections to the `brian_list` and the network dict\n        brian_list.extend(conns_list)\n        network_dict['connections'] = conns_list"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd all necessary parameters to the trajectory container.", "response": "def add_parameters(self, traj):\n        \"\"\"Adds all necessary parameters to `traj` container.\"\"\"\n        par= traj.f_add_parameter(Brian2Parameter,'simulation.durations.initial_run', 500*ms,\n                             comment='Initialisation run for more realistic '\n                                            'measurement conditions.')\n\n        par.v_annotations.order=0\n        par=traj.f_add_parameter(Brian2Parameter,'simulation.durations.measurement_run', 1500*ms,\n                             comment='Measurement run that is considered for '\n                                                'statistical evaluation')\n        par.v_annotations.order=1"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the Fano Factor for a single neuron.", "response": "def _compute_fano_factor(spike_res, neuron_id, time_window, start_time, end_time):\n        \"\"\"Computes Fano Factor for one neuron.\n\n        :param spike_res:\n\n            Result containing the spiketimes of all neurons\n\n        :param neuron_id:\n\n            Index of neuron for which FF is computed\n\n        :param time_window:\n\n            Length of the consecutive time windows to compute the FF\n\n        :param start_time:\n\n            Start time of measurement to consider\n\n        :param end_time:\n\n            End time of measurement to consider\n\n        :return:\n\n            Fano Factor (float) or\n            returns 0 if mean firing activity is 0.\n\n        \"\"\"\n        assert(end_time >= start_time+time_window)\n\n        # Number of time bins\n        bins = (end_time-start_time)/time_window\n        bins = int(np.floor(bins))\n\n        # Arrays for binning of spike counts\n        binned_spikes = np.zeros(bins)\n\n        # DataFrame only containing spikes of the particular neuron\n        spike_array_neuron = spike_res.t[spike_res.i==neuron_id]\n\n        for bin in range(bins):\n            # We iterate over the bins to calculate the spike counts\n            lower_time = start_time+time_window*bin\n            upper_time = start_time+time_window*(bin+1)\n\n            # Filter the spikes\n            spike_array_interval = spike_array_neuron[spike_array_neuron >= lower_time]\n            spike_array_interval = spike_array_interval[spike_array_interval < upper_time]\n\n            # Add count to bins\n            spikes = len(spike_array_interval)\n            binned_spikes[bin]=spikes\n\n\n        var = np.var(binned_spikes)\n        avg = np.mean(binned_spikes)\n\n        if avg > 0:\n            return var/float(avg)\n        else:\n            return 0"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _compute_mean_fano_factor( neuron_ids, spike_res, time_window, start_time, end_time):\n        ffs = np.zeros(len(neuron_ids))\n\n        for idx, neuron_id in enumerate(neuron_ids):\n            ff=CNFanoFactorComputer._compute_fano_factor(\n                            spike_res, neuron_id, time_window, start_time, end_time)\n            ffs[idx]=ff\n\n        mean_ff = np.mean(ffs)\n        return mean_ff", "response": "Compute the mean fano factor over many neurons."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nanalyzing the average Fano Factor of a trajectory.", "response": "def analyse(self, traj, network, current_subrun, subrun_list, network_dict):\n        \"\"\"Calculates average Fano Factor of a network.\n\n        :param traj:\n\n            Trajectory container\n\n            Expects:\n\n            `results.monitors.spikes_e`: Data from SpikeMonitor for excitatory neurons\n\n            Adds:\n\n            `results.statistics.mean_fano_factor`: Average Fano Factor\n\n        :param network:\n\n            The BRIAN network\n\n        :param current_subrun:\n\n            BrianParameter\n\n        :param subrun_list:\n\n            Upcoming subruns, analysis is only performed if subruns is empty,\n            aka the final subrun has finished.\n\n        :param network_dict:\n\n            Dictionary of items shared among componetns\n\n        \"\"\"\n        #Check if we finished all subruns\n        if len(subrun_list)==0:\n            spikes_e = traj.results.monitors.spikes_e\n\n            time_window = traj.parameters.analysis.statistics.time_window\n            start_time = traj.parameters.simulation.durations.initial_run\n            end_time = start_time+traj.parameters.simulation.durations.measurement_run\n            neuron_ids = traj.parameters.analysis.statistics.neuron_ids\n\n            mean_ff = self._compute_mean_fano_factor(\n                neuron_ids, spikes_e, time_window, start_time, end_time)\n\n            traj.f_add_result('statistics.mean_fano_factor', mean_ff, comment='Average Fano '\n                                                                      'Factor over all '\n                                                                      'exc neurons')\n\n            print('R_ee: %f, Mean FF: %f' % (traj.R_ee, mean_ff))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding monitors to the network if the measurement run is carried out.", "response": "def add_to_network(self, traj, network, current_subrun, subrun_list, network_dict):\n        \"\"\"Adds monitors to the network if the measurement run is carried out.\n\n        :param traj: Trajectory container\n\n        :param network: The BRIAN network\n\n        :param current_subrun: BrianParameter\n\n        :param subrun_list: List of coming subrun_list\n\n        :param network_dict:\n\n            Dictionary of items shared among the components\n\n            Expects:\n\n            'neurons_e': Excitatory neuron group\n\n            Adds:\n\n            'monitors': List of monitors\n\n                0. SpikeMonitor of excitatory neurons\n\n                1. StateMonitor of membrane potential of some excitatory neurons\n                (specified in `neuron_records`)\n\n                2. StateMonitor of excitatory synaptic currents of some excitatory neurons\n\n                3. State monitor of inhibitory currents of some excitatory neurons\n\n        \"\"\"\n        if current_subrun.v_annotations.order == 1:\n            self._add_monitors(traj, network, network_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _add_monitors(self, traj,  network, network_dict):\n\n        neurons_e = network_dict['neurons_e']\n\n        monitor_list = []\n\n        # Spiketimes\n        self.spike_monitor = SpikeMonitor(neurons_e)\n        monitor_list.append(self.spike_monitor)\n\n        # Membrane Potential\n        self.V_monitor = StateMonitor(neurons_e,'V',\n                                              record=list(traj.neuron_records))\n\n        monitor_list.append(self.V_monitor)\n\n        # Exc. syn .Current\n        self.I_syn_e_monitor = StateMonitor(neurons_e, 'I_syn_e',\n                                            record=list(traj.neuron_records))\n        monitor_list.append(self.I_syn_e_monitor)\n\n        # Inh. syn. Current\n        self.I_syn_i_monitor = StateMonitor(neurons_e, 'I_syn_i',\n                                            record=list(traj.neuron_records))\n        monitor_list.append(self.I_syn_i_monitor)\n\n        # Add monitors to network and dictionary\n        network.add(*monitor_list)\n        network_dict['monitors'] = monitor_list", "response": "Adds monitors to the network and dictionary of monitors"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes a subfolder for plots.", "response": "def _make_folder(self, traj):\n        \"\"\"Makes a subfolder for plots.\n\n        :return: Path name to print folder\n\n        \"\"\"\n        print_folder = os.path.join(traj.analysis.plot_folder,\n                                    traj.v_name, traj.v_crun)\n        print_folder = os.path.abspath(print_folder)\n        if not os.path.isdir(print_folder):\n            os.makedirs(print_folder)\n\n        return print_folder"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nplotting a state variable graph for several neurons into one figure", "response": "def _plot_result(self, traj, result_name):\n        \"\"\"Plots a state variable graph for several neurons into one figure\"\"\"\n        result = traj.f_get(result_name)\n        varname = result.record_variables[0]\n        values = result[varname]\n        times = result.t\n\n        record = result.record\n\n        for idx, celia_neuron in enumerate(record):\n            plt.subplot(len(record), 1, idx+1)\n            plt.plot(times, values[idx,:])\n            if idx==0:\n                plt.title('%s' % varname)\n            if idx==1:\n                plt.ylabel('%s' % ( varname))\n            if idx == len(record)-1:\n                plt.xlabel('t')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _print_graphs(self, traj):\n        print_folder = self._make_folder(traj)\n\n        # If we use BRIAN's own raster_plot functionality we\n        # need to sue the SpikeMonitor directly\n        plt.figure()\n        plt.scatter(self.spike_monitor.t, self.spike_monitor.i, s=1)\n        plt.xlabel('t')\n        plt.ylabel('Exc. Neurons')\n        plt.title('Spike Raster Plot')\n\n        filename=os.path.join(print_folder,'spike.png')\n\n        print('Current plot: %s ' % filename)\n        plt.savefig(filename)\n        plt.close()\n\n        fig=plt.figure()\n        self._plot_result(traj, 'monitors.V')\n        filename=os.path.join(print_folder,'V.png')\n        print('Current plot: %s ' % filename)\n        fig.savefig(filename)\n        plt.close()\n\n        plt.figure()\n        self._plot_result(traj, 'monitors.I_syn_e')\n        filename=os.path.join(print_folder,'I_syn_e.png')\n        print('Current plot: %s ' % filename)\n        plt.savefig(filename)\n        plt.close()\n\n        plt.figure()\n        self._plot_result(traj, 'monitors.I_syn_i')\n        filename=os.path.join(print_folder,'I_syn_i.png')\n        print('Current plot: %s ' % filename)\n        plt.savefig(filename)\n        plt.close()\n\n        if not traj.analysis.show_plots:\n            plt.close('all')\n        else:\n            plt.show()", "response": "Makes some plots and stores them into subfolders"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef analyse(self, traj, network, current_subrun, subrun_list, network_dict):\n        if len(subrun_list)==0:\n\n            traj.f_add_result(Brian2MonitorResult, 'monitors.spikes_e', self.spike_monitor,\n                              comment = 'The spiketimes of the excitatory population')\n\n            traj.f_add_result(Brian2MonitorResult, 'monitors.V', self.V_monitor,\n                              comment = 'Membrane voltage of four neurons from 2 clusters')\n\n            traj.f_add_result(Brian2MonitorResult, 'monitors.I_syn_e', self.I_syn_e_monitor,\n                              comment = 'I_syn_e of four neurons from 2 clusters')\n\n            traj.f_add_result(Brian2MonitorResult, 'monitors.I_syn_i', self.I_syn_i_monitor,\n                              comment = 'I_syn_i of four neurons from 2 clusters')\n\n            print('Plotting')\n\n            if traj.parameters.analysis.make_plots:\n                self._print_graphs(traj)", "response": "This function extracts monitor data and plots from the monitors and stores them into the trajectory."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfunction that parses the batch id from the command line arguments", "response": "def get_batch():\n    \"\"\"Function that parses the batch id from the command line arguments\"\"\"\n    optlist, args = getopt.getopt(sys.argv[1:], '', longopts='batch=')\n    batch = 0\n    for o, a in optlist:\n        if o == '--batch':\n            batch = int(a)\n            print('Found batch %d' % batch)\n\n    return batch"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchooses exploration according to batch", "response": "def explore_batch(traj, batch):\n    \"\"\"Chooses exploration according to `batch`\"\"\"\n    explore_dict = {}\n    explore_dict['sigma'] = np.arange(10.0 * batch, 10.0*(batch+1), 1.0).tolist()\n    # for batch = 0 explores sigma in [0.0, 1.0, 2.0, ..., 9.0],\n    # for batch = 1 explores sigma in [10.0, 11.0, 12.0, ..., 19.0]\n    # and so on\n    traj.f_explore(explore_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef func(self):\n        if self._func is None:\n            self._func = NNTreeNodeFunc(self)\n        return self._func", "response": "Alternative naming you can use node. f_func. name instead of node. f_func. name"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrenames the tree node.", "response": "def _rename(self, full_name):\n        \"\"\"Renames the tree node\"\"\"\n        self._full_name = full_name\n        if full_name:\n            self._name = full_name.rsplit('.', 1)[-1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _set_details(self, depth, branch, run_branch):\n        self._depth = depth\n        self._branch = branch\n        self._run_branch = run_branch", "response": "Sets some details for internal handling."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmapping a type representation string to a corresponding dictionary in the root.", "response": "def _map_type_to_dict(self, type_name):\n        \"\"\" Maps a an instance type representation string (e.g. 'RESULT')\n        to the corresponding dictionary in root.\n\n        \"\"\"\n        root = self._root_instance\n\n        if type_name == RESULT:\n            return root._results\n        elif type_name == PARAMETER:\n            return root._parameters\n        elif type_name == DERIVED_PARAMETER:\n            return root._derived_parameters\n        elif type_name == CONFIG:\n            return root._config\n        elif type_name == LEAF:\n            return root._other_leaves\n        else:\n            raise RuntimeError('You shall not pass!')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _fetch_from_node(self, store_load, node, args, kwargs):\n        msg = self._node_to_msg(store_load, node)\n\n        return msg, node, args, kwargs", "response": "Method used by f_store_load_or_remove to find a corresponding item in the tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _node_to_msg(store_load, node):\n        if node.v_is_leaf:\n            if store_load == STORE:\n                return pypetconstants.LEAF\n            elif store_load == LOAD:\n                return pypetconstants.LEAF\n            elif store_load == REMOVE:\n                return pypetconstants.DELETE\n        else:\n            if store_load == STORE:\n                return pypetconstants.GROUP\n            elif store_load == LOAD:\n                return pypetconstants.GROUP\n            elif store_load == REMOVE:\n                return pypetconstants.DELETE", "response": "Maps a given node and a store_load constant to the message that is understood by the storage service."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _remove_subtree(self, start_node, name, predicate=None):\n        def _delete_from_children(node, child_name):\n            del node._children[child_name]\n            if child_name in node._groups:\n                del node._groups[child_name]\n            elif child_name in node._leaves:\n                del node._leaves[child_name]\n            else:\n                raise RuntimeError('You shall not pass!')\n\n        def _remove_subtree_inner(node, predicate):\n\n            if not predicate(node):\n                return False\n            elif node.v_is_group:\n                for name_ in itools.chain(list(node._leaves.keys()),\n                                          list(node._groups.keys())):\n                    child_ = node._children[name_]\n                    child_deleted = _remove_subtree_inner(child_, predicate)\n                    if child_deleted:\n                        _delete_from_children(node, name_)\n                        del child_\n\n                for link_ in list(node._links.keys()):\n                    node.f_remove_link(link_)\n\n                if len(node._children) == 0:\n                    self._delete_node(node)\n                    return True\n                else:\n                    return False\n            else:\n                self._delete_node(node)\n                return True\n\n        if name in start_node._links:\n            start_node.f_remove_link(name)\n        else:\n            child = start_node._children[name]\n            if predicate is None:\n                predicate = lambda x: True\n\n            if _remove_subtree_inner(child, predicate):\n                _delete_from_children(start_node, name)\n                del child\n                return True\n            else:\n                return False", "response": "Removes a subtree from the trajectory tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes a single node from the tree.", "response": "def _delete_node(self, node):\n        \"\"\"Deletes a single node from the tree.\n\n        Removes all references to the node.\n\n        Note that the 'parameters', 'results', 'derived_parameters', and 'config' groups\n        hanging directly below root cannot be deleted. Also the root node itself cannot be\n        deleted. (This would cause a tremendous wave of uncontrollable self destruction, which\n        would finally lead to the Apocalypse!)\n\n        \"\"\"\n        full_name = node.v_full_name\n        root = self._root_instance\n\n        if full_name == '':\n            # You cannot delete root\n            return\n\n        if node.v_is_leaf:\n            if full_name in root._parameters:\n                del root._parameters[full_name]\n\n            elif full_name in root._config:\n                del root._config[full_name]\n\n            elif full_name in root._derived_parameters:\n                del root._derived_parameters[full_name]\n\n            elif full_name in root._results:\n                del root._results[full_name]\n\n            elif full_name in root._other_leaves:\n                del root._other_leaves[full_name]\n\n            if full_name in root._explored_parameters:\n\n                if root._stored:\n                    # We always keep the explored parameters in case the trajectory was stored\n                    root._explored_parameters[full_name] = None\n                else:\n                    del root._explored_parameters[full_name]\n                if len(root._explored_parameters) == 0:\n                    root.f_shrink()\n\n            del self._flat_leaf_storage_dict[full_name]\n        else:\n            del root._all_groups[full_name]\n\n            if full_name in root._run_parent_groups:\n                del root._run_parent_groups[full_name]\n\n        # Delete all links to the node\n        if full_name in root._linked_by:\n            linking = root._linked_by[full_name]\n            for linking_name in list(linking.keys()):\n                linking_group, link_set = linking[linking_name]\n                for link in list(link_set):\n                    linking_group.f_remove_link(link)\n        \n        if (node.v_location, node.v_name) in self._root_instance._new_nodes:\n            del self._root_instance._new_nodes[(node.v_location, node.v_name)]\n\n        # Finally remove all references in the dictionaries for fast search\n        self._remove_from_nodes_and_leaves(node)\n\n        # Remove circular references\n        node._vars = None\n        node._func = None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove a single node from the tree.", "response": "def _remove_node_or_leaf(self, instance, recursive=False):\n        \"\"\"Removes a single node from the tree.\n\n        Only from RAM not from hdf5 file!\n\n        :param instance: The node to be deleted\n\n        :param recursive: If group nodes with children should be deleted\n\n        \"\"\"\n        full_name = instance.v_full_name\n        split_name = deque(full_name.split('.'))\n        self._remove_along_branch(self._root_instance, split_name, recursive)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove a given node from the tree.", "response": "def _remove_along_branch(self, actual_node, split_name, recursive=False):\n        \"\"\"Removes a given node from the tree.\n\n        Starts from a given node and walks recursively down the tree to the location of the node\n        we want to remove.\n\n        We need to walk from a start node in case we want to check on the way back whether we got\n        empty group nodes due to deletion.\n\n        :param actual_node: Current node\n\n        :param split_name: DEQUE of names to get the next nodes.\n\n        :param recursive:\n\n            To also delete all children of a group node\n\n        :return: True if node was deleted, otherwise False\n\n        \"\"\"\n\n        # If the names list is empty, we have reached the node we want to delete.\n\n        if len(split_name) == 0:\n            if actual_node.v_is_group and actual_node.f_has_children():\n                if recursive:\n                    for child in list(actual_node._children.keys()):\n                        actual_node.f_remove_child(child, recursive=True)\n                else:\n                    raise TypeError('Cannot remove group `%s` it contains children. Please '\n                                    'remove with `recursive=True`.' % actual_node.v_full_name)\n            self._delete_node(actual_node)\n            return True\n\n        # Otherwise get the next node by using the first name in the list\n        name = split_name.popleft()\n\n        if name in actual_node._links:\n            if len(split_name)>0:\n                raise RuntimeError('You cannot remove nodes while hopping over links!')\n            actual_node.f_remove_link(name)\n        else:\n            child = actual_node._children[name]\n\n            if self._remove_along_branch(child, split_name, recursive=recursive):\n\n                del actual_node._children[name]\n                if name in actual_node._groups:\n                    del actual_node._groups[name]\n                elif name in actual_node._leaves:\n                    del actual_node._leaves[name]\n                else:\n                    raise RuntimeError('You shall not pass!')\n                del child\n                return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntranslates a given shortcut to a corresponding name.", "response": "def _translate_shortcut(self, name):\n        \"\"\"Maps a given shortcut to corresponding name\n\n        * 'run_X' or 'r_X' to 'run_XXXXXXXXX'\n\n        * 'crun' to the current run name in case of a\n          single run instance if trajectory is used via `v_crun`\n\n        * 'par' 'parameters'\n\n        * 'dpar' to 'derived_parameters'\n\n        * 'res' to 'results'\n\n        * 'conf' to 'config'\n\n        :return: True or False and the mapped name.\n\n        \"\"\"\n\n        if isinstance(name, int):\n            return True, self._root_instance.f_wildcard('$', name)\n\n        if name.startswith('run_') or name.startswith('r_'):\n            split_name = name.split('_')\n            if len(split_name) == 2:\n                index = split_name[1]\n                if index.isdigit():\n                    return True, self._root_instance.f_wildcard('$', int(index))\n                elif index == 'A':\n                    return True, self._root_instance.f_wildcard('$', -1)\n\n        if name.startswith('runtoset_') or name.startswith('rts_'):\n            split_name = name.split('_')\n            if len(split_name) == 2:\n                index = split_name[1]\n                if index.isdigit():\n                    return True, self._root_instance.f_wildcard('$set', int(index))\n                elif index == 'A':\n                    return True, self._root_instance.f_wildcard('$set', -1)\n\n        if name in SHORTCUT_SET:\n            if name == 'par':\n                return True, 'parameters'\n            elif name == 'dpar':\n                return True, 'derived_parameters'\n            elif name == 'res':\n                return True, 'results'\n            elif name == 'conf':\n                return True, 'config'\n            else:\n                raise RuntimeError('You shall not pass!')\n\n        return False, name"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _add_prefix(self, split_names, start_node, group_type_name):\n        root = self._root_instance\n\n        # If the start node of our insertion is root or one below root\n        # we might need to add prefixes.\n        # In case of derived parameters and results we also need to add prefixes containing the\n        # subbranch and the current run in case of a single run.\n        # For instance, a prefix could be 'results.runs.run_00000007'.\n        prepend = []\n        if start_node.v_depth < 3 and not group_type_name == GROUP:\n            if start_node.v_depth == 0:\n\n                if group_type_name == DERIVED_PARAMETER_GROUP:\n                    if split_names[0] == 'derived_parameters':\n                        return split_names\n                    else:\n                        prepend += ['derived_parameters']\n\n                elif group_type_name == RESULT_GROUP:\n                    if split_names[0] == 'results':\n                        return split_names\n                    else:\n                        prepend += ['results']\n\n                elif group_type_name == CONFIG_GROUP:\n                    if split_names[0] == 'config':\n                        return split_names\n                    else:\n                        prepend += ['config']\n\n                elif group_type_name == PARAMETER_GROUP:\n                    if split_names[0] == 'parameters':\n                        return split_names[0]\n                    else:\n                        prepend += ['parameters']\n                else:\n                    raise RuntimeError('Why are you here?')\n\n            # Check if we have to add a prefix containing the current run\n            if root._is_run and root._auto_run_prepend:\n                dummy = root.f_wildcard('$', -1)\n                crun = root.f_wildcard('$')\n                if any(name in root._run_information for name in split_names):\n                    pass\n                elif any(name == dummy for name in split_names):\n                    pass\n                elif (group_type_name == RESULT_GROUP or\n                        group_type_name == DERIVED_PARAMETER_GROUP):\n\n                    if start_node.v_depth == 0:\n                        prepend += ['runs', crun]\n\n                    elif start_node.v_depth == 1:\n\n                        if len(split_names) == 1 and split_names[0] == 'runs':\n                            return split_names\n                        else:\n                            prepend += ['runs', crun]\n\n                    elif start_node.v_depth == 2 and start_node.v_name == 'runs':\n                        prepend += [crun]\n\n        if prepend:\n            split_names = prepend + split_names\n\n        return split_names", "response": "Adds the correct sub branch prefix to a given name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndetermining types for generic additions", "response": "def _determine_types(start_node, first_name, add_leaf, add_link):\n        \"\"\"Determines types for generic additions\"\"\"\n        if start_node.v_is_root:\n            where = first_name\n        else:\n            where = start_node._branch\n\n        if where in SUBTREE_MAPPING:\n            type_tuple = SUBTREE_MAPPING[where]\n        else:\n            type_tuple = (GROUP, LEAF)\n\n        if add_link:\n            return type_tuple[0], LINK\n        if add_leaf:\n            return type_tuple\n        else:\n            return type_tuple[0], type_tuple[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _add_generic(self, start_node, type_name, group_type_name, args, kwargs,\n                     add_prefix=True, check_naming=True):\n        \"\"\"Adds a given item to the tree irrespective of the subtree.\n\n        Infers the subtree from the arguments.\n\n        :param start_node: The parental node the adding was initiated from\n\n        :param type_name:\n\n            The type of the new instance. Whether it is a parameter, parameter group, config,\n            config group, etc. See the name of the corresponding constants at the top of this\n            python module.\n\n\n        :param group_type_name:\n\n            Type of the subbranch. i.e. whether the item is added to the 'parameters',\n            'results' etc. These subbranch types are named as the group names\n            (e.g. 'PARAMETER_GROUP') in order to have less constants.\n            For all constants used see beginning of this python module.\n\n        :param args:\n\n            Arguments specifying how the item is added.\n\n            If len(args)==1 and the argument is the a given instance of a result or parameter,\n            this one is added to the tree.\n\n            Otherwise it is checked if the first argument is a class specifying how to\n            construct a new item and the second argument is the name of the new class.\n\n            If the first argument is not a class but a string, the string is assumed to be\n            the name of the new instance.\n\n            Additional args are later on used for the construction of the instance.\n\n        :param kwargs:\n\n            Additional keyword arguments that might be handed over to the instance constructor.\n\n        :param add_prefix:\n\n            If a prefix group, i.e. `results`, `config`, etc. should be added\n\n        :param check_naming:\n\n            If it should be checked for correct namings, can be set to ``False`` if data is loaded\n            and we know that all names are correct.\n\n        :return: The new added instance\n\n        \"\"\"\n        args = list(args)\n        create_new = True\n        name = ''\n        instance = None\n        constructor = None\n\n        add_link = type_name == LINK\n\n        # First check if the item is already a given instance or we want to add a link\n        if add_link:\n            name = args[0]\n            instance = args[1]\n            create_new = False\n        elif len(args) == 1 and len(kwargs) == 0:\n            item = args[0]\n            try:\n                name = item.v_full_name\n                instance = item\n\n                create_new = False\n            except AttributeError:\n                pass\n\n        # If the item is not an instance yet, check if args[0] is a class and args[1] is\n        # a string describing the new name of the instance.\n        # If args[0] is not a class it is assumed to be the name of the new instance.\n        if create_new:\n            if len(args) > 0 and inspect.isclass(args[0]):\n                constructor = args.pop(0)\n            if len(args) > 0 and isinstance(args[0], str):\n                name = args.pop(0)\n            elif 'name' in kwargs:\n                name = kwargs.pop('name')\n            elif 'full_name' in kwargs:\n                name = kwargs.pop('full_name')\n            else:\n                raise ValueError('Could not determine a name of the new item you want to add. '\n                                 'Either pass the name as positional argument or as a keyword '\n                                 'argument `name`.')\n\n        split_names = name.split('.')\n        if check_naming:\n\n            for idx, name in enumerate(split_names):\n                translated_shortcut, name = self._translate_shortcut(name)\n                replaced, name = self._replace_wildcards(name)\n                if translated_shortcut or replaced:\n                    split_names[idx] = name\n\n            # First check if the naming of the new item is appropriate\n            faulty_names = self._check_names(split_names, start_node)\n\n            if faulty_names:\n                full_name = '.'.join(split_names)\n                raise ValueError(\n                    'Your Parameter/Result/Node `%s` contains the following not admissible names: '\n                    '%s please choose other names.' % (full_name, faulty_names))\n\n            if add_link:\n                if instance is None:\n                    raise ValueError('You must provide an instance to link to!')\n                if instance.v_is_root:\n                    raise ValueError('You cannot create a link to the root node')\n                if start_node.v_is_root and name in SUBTREE_MAPPING:\n                    raise ValueError('`%s` is a reserved name for a group under root.' % name)\n                if not self._root_instance.f_contains(instance, with_links=False, shortcuts=False):\n                    raise ValueError('You can only link to items within the trajectory tree!')\n\n        # Check if the name fulfils the prefix conditions, if not change the name accordingly.\n        if add_prefix:\n            split_names = self._add_prefix(split_names, start_node, group_type_name)\n\n        if group_type_name == GROUP:\n            add_leaf = type_name != group_type_name and not add_link\n            # If this is equal we add a group node\n            group_type_name, type_name = self._determine_types(start_node, split_names[0],\n                                                               add_leaf, add_link)\n\n        # Check if we are allowed to add the data\n        if self._root_instance._is_run and type_name in SENSITIVE_TYPES:\n            raise TypeError('You are not allowed to add config or parameter data or groups '\n                            'during a single run.')\n\n        return self._add_to_tree(start_node, split_names, type_name, group_type_name, instance,\n                                 constructor, args, kwargs)", "response": "Internal function to add a generic item to the tree."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreplace the $ wildcards and returns True or False in case it was replaced", "response": "def _replace_wildcards(self, name, run_idx=None):\n        \"\"\"Replaces the $ wildcards and returns True/False in case it was replaced\"\"\"\n        if self._root_instance.f_is_wildcard(name):\n            return True, self._root_instance.f_wildcard(name, run_idx)\n        else:\n            return False, name"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _add_to_tree(self, start_node, split_names, type_name, group_type_name,\n                     instance, constructor, args, kwargs):\n        \"\"\"Adds a new item to the tree.\n\n        The item can be an already given instance or it is created new.\n\n        :param start_node:\n\n            Parental node the adding of the item was initiated from.\n\n        :param split_names:\n\n            List of names of the new item\n\n        :param type_name:\n\n            Type of item 'RESULT', 'RESULT_GROUP', 'PARAMETER', etc. See name of constants\n            at beginning of the python module.\n\n        :param group_type_name:\n\n            Name of the subbranch the item is added to 'RESULT_GROUP', 'PARAMETER_GROUP' etc.\n            See name of constants at beginning of this python module.\n\n        :param instance:\n\n            Here an already given instance can be passed. If instance should be created new\n            pass None.\n\n        :param constructor:\n\n            If instance should be created new pass a constructor class. If None is passed\n            the standard constructor for the instance is chosen.\n\n        :param args:\n\n            Additional arguments passed to instance construction\n\n        :param kwargs:\n\n            Additional keyword arguments passed to instance construction\n\n        :return: The new added instance\n\n        :raises: ValueError if naming of the new item is invalid\n\n        \"\"\"\n\n        # Then walk iteratively from the start node as specified by the new name and create\n        # new empty groups on the fly\n        try:\n            act_node = start_node\n            last_idx = len(split_names) - 1\n            add_link = type_name == LINK\n            link_added = False\n            # last_name = start_node.v_crun\n            for idx, name in enumerate(split_names):\n                if name not in act_node._children:\n                    if idx == last_idx:\n                        if add_link:\n                            new_node = self._create_link(act_node, name, instance)\n                            link_added = True\n                        elif group_type_name != type_name:\n                            # We are at the end of the chain and we add a leaf node\n\n                            new_node = self._create_any_param_or_result(act_node,\n                                                                        name,\n                                                                        type_name,\n                                                                        instance,\n                                                                        constructor,\n                                                                        args, kwargs)\n\n                            self._flat_leaf_storage_dict[new_node.v_full_name] = new_node\n                        else:\n                            # We add a group as desired\n                            new_node = self._create_any_group(act_node, name,\n                                                              group_type_name,\n                                                              instance,\n                                                              constructor,\n                                                              args, kwargs)\n                    else:\n                        # We add a group on the fly\n                        new_node = self._create_any_group(act_node, name,\n                                                          group_type_name)\n\n\n                    if name in self._root_instance._run_information:\n                        self._root_instance._run_parent_groups[act_node.v_full_name] = act_node\n                    if self._root_instance._is_run:\n                        if link_added:\n                            self._root_instance._new_links[(act_node.v_full_name, name)] = \\\n                                (act_node, new_node)\n                        else:\n                            self._root_instance._new_nodes[(act_node.v_full_name, name)] = \\\n                                (act_node, new_node)\n                else:\n                    if name in act_node._links:\n                        raise AttributeError('You cannot hop over links when adding '\n                                             'data to the tree. '\n                                             'There is a link called `%s` under `%s`.' %\n                                             (name, act_node.v_full_name))\n                    if idx == last_idx:\n                        if self._root_instance._no_clobber:\n                            self._logger.warning('You already have a group/instance/link `%s` '\n                                                 'under `%s`. '\n                                                 'However, you set `v_no_clobber=True`, '\n                                                 'so I will ignore your addition of '\n                                                 'data.' % (name, act_node.v_full_name))\n                        else:\n                            raise AttributeError('You already have a group/instance/link `%s` '\n                                                 'under `%s`' % (name, act_node.v_full_name))\n                act_node = act_node._children[name]\n            return act_node\n        except:\n            self._logger.error('Failed adding `%s` under `%s`.' %\n                               (name, start_node.v_full_name))\n            raise", "response": "Add a new item to the tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _create_link(self, act_node, name, instance):\n\n        act_node._links[name] = instance\n        act_node._children[name] = instance\n\n        full_name = instance.v_full_name\n        if full_name not in self._root_instance._linked_by:\n            self._root_instance._linked_by[full_name] = {}\n        linking = self._root_instance._linked_by[full_name]\n        if act_node.v_full_name not in linking:\n            linking[act_node.v_full_name] = (act_node, set())\n        linking[act_node.v_full_name][1].add(name)\n\n        if name not in self._links_count:\n            self._links_count[name] = 0\n        self._links_count[name] = self._links_count[name] + 1\n\n        self._logger.debug('Added link `%s` under `%s` pointing '\n                           'to `%s`.' % (name, act_node.v_full_name,\n                                         instance.v_full_name))\n        return instance", "response": "Creates a link and checks if names are appropriate"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if a list contains invalid names.", "response": "def _check_names(self, split_names, parent_node=None):\n        \"\"\"Checks if a list contains strings with invalid names.\n\n        Returns a description of the name violations. If names are correct the empty\n        string is returned.\n\n        :param split_names: List of strings\n\n        :param parent_node:\n\n            The parental node from where to start (only applicable for node names)\n\n        \"\"\"\n\n        faulty_names = ''\n\n        if parent_node is not None and parent_node.v_is_root and split_names[0] == 'overview':\n            faulty_names = '%s `overview` cannot be added directly under the root node ' \\\n                           'this is a reserved keyword,' % (faulty_names)\n\n        for split_name in split_names:\n\n            if len(split_name) == 0:\n                faulty_names = '%s `%s` contains no characters, please use at least 1,' % (\n                    faulty_names, split_name)\n\n            elif split_name.startswith('_'):\n                faulty_names = '%s `%s` starts with a leading underscore,' % (\n                    faulty_names, split_name)\n\n            elif re.match(CHECK_REGEXP, split_name) is None:\n                faulty_names = '%s `%s` contains non-admissible characters ' \\\n                               '(use only [A-Za-z0-9_-]),' % \\\n                               (faulty_names, split_name)\n\n            elif '$' in split_name:\n                if split_name not in self._root_instance._wildcard_keys:\n                    faulty_names = '%s `%s` contains `$` but has no associated ' \\\n                                   'wildcard function,' % (faulty_names, split_name)\n\n            elif split_name in self._not_admissible_names:\n                warnings.warn('`%s` is a method/attribute of the '\n                              'trajectory/treenode/naminginterface, you may not be '\n                              'able to access it via natural naming but only by using '\n                              '`[]` square bracket notation. ' % split_name,\n                              category=SyntaxWarning)\n\n            elif split_name in self._python_keywords:\n                warnings.warn('`%s` is a python keyword, you may not be '\n                              'able to access it via natural naming but only by using '\n                              '`[]` square bracket notation. ' % split_name,\n                              category=SyntaxWarning)\n\n        name = split_names[-1]\n        if len(name) >= pypetconstants.HDF5_STRCOL_MAX_NAME_LENGTH:\n            faulty_names = '%s `%s` is too long the name can only have %d characters but it has ' \\\n                           '%d,' % \\\n                           (faulty_names, name, len(name),\n                            pypetconstants.HDF5_STRCOL_MAX_NAME_LENGTH)\n\n        return faulty_names"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _set_details_tree_node(self, parent_node, name, instance):\n        depth = parent_node._depth + 1\n        if parent_node.v_is_root:\n            branch = name  # We add below root\n        else:\n            branch = parent_node._branch\n        if name in self._root_instance._run_information:\n            run_branch = name\n        else:\n            run_branch = parent_node._run_branch\n\n        instance._set_details(depth, branch, run_branch)", "response": "Sets the details of a given instance based on parent_node and name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn an iterator over nodes hanging below a given start node.", "response": "def _iter_nodes(self, node, recursive=False, max_depth=float('inf'),\n                    with_links=True, in_search=False, predicate=None):\n        \"\"\"Returns an iterator over nodes hanging below a given start node.\n\n        :param node:\n\n            Start node\n\n        :param recursive:\n\n            Whether recursively also iterate over the children of the start node's children\n\n        :param max_depth:\n\n            Maximum depth to search for\n\n        :param in_search:\n\n            if it is used during get search and if detailed info should be returned\n\n        :param with_links:\n\n            If links should be considered\n\n        :param predicate:\n\n            A predicate to filter nodes\n\n        :return: Iterator\n\n        \"\"\"\n        def _run_predicate(x, run_name_set):\n            branch = x.v_run_branch\n            return branch == 'trajectory' or branch in run_name_set\n\n        if max_depth is None:\n            max_depth = float('inf')\n\n        if predicate is None:\n            predicate = lambda x: True\n        elif isinstance(predicate, (tuple, list)):\n            # Create a predicate from a list of run names or run indices\n            run_list = predicate\n            run_name_set = set()\n            for item in run_list:\n                if item == -1:\n                    run_name_set.add(self._root_instance.f_wildcard('$', -1))\n                elif isinstance(item, int):\n                    run_name_set.add(self._root_instance.f_idx_to_run(item))\n                else:\n                    run_name_set.add(item)\n            predicate = lambda x: _run_predicate(x, run_name_set)\n\n        if recursive:\n            return NaturalNamingInterface._recursive_traversal_bfs(node,\n                                            self._root_instance._linked_by,\n                                            max_depth, with_links,\n                                            in_search, predicate)\n        else:\n            iterator = (x for x in self._make_child_iterator(node, with_links) if\n                        predicate(x[2]))\n            if in_search:\n                return iterator # Here we return tuples: (depth, name, object)\n            else:\n                return (x[2] for x in iterator)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dictionary with pairings of full names as keys and instances as values.", "response": "def _to_dict(self, node, fast_access=True, short_names=False, nested=False,\n                 copy=True, with_links=True):\n        \"\"\" Returns a dictionary with pairings of (full) names as keys and instances as values.\n\n        :param fast_access:\n\n            If true parameter or result values are returned instead of the\n            instances.\n\n        :param short_names:\n\n            If true keys are not full names but only the names.\n            Raises a ValueError if the names are not unique.\n\n        :param nested:\n\n            If true returns a nested dictionary.\n\n        :param with_links:\n\n            If links should be considered\n\n        :return: dictionary\n\n        :raises: ValueError\n\n        \"\"\"\n\n        if (fast_access or short_names or nested) and not copy:\n            raise ValueError('You can not request the original data with >>fast_access=True<< or'\n                             ' >>short_names=True<< of >>nested=True<<.')\n\n        if nested and short_names:\n            raise ValueError('You cannot use short_names and nested at the '\n                             'same time.')\n\n        # First, let's check if we can return the `flat_leaf_storage_dict` or a copy of that, this\n        # is faster than creating a novel dictionary by tree traversal.\n        if node.v_is_root:\n            temp_dict = self._flat_leaf_storage_dict\n\n            if not fast_access and not short_names:\n                if copy:\n                    return temp_dict.copy()\n                else:\n                    return temp_dict\n\n            else:\n                iterator = temp_dict.values()\n        else:\n            iterator = node.f_iter_leaves(with_links=with_links)\n\n        # If not we need to build the dict by iterating recursively over all leaves:\n        result_dict = {}\n        for val in iterator:\n            if short_names:\n                new_key = val.v_name\n            else:\n                new_key = val.v_full_name\n\n            if new_key in result_dict:\n                raise ValueError('Your short names are not unique. '\n                                 'Duplicate key `%s`!' % new_key)\n\n            new_val = self._apply_fast_access(val, fast_access)\n            result_dict[new_key] = new_val\n\n        if nested:\n            if node.v_is_root:\n                nest_dict = result_dict\n            else:\n                # remove the name of the current node\n                # such that the nested dictionary starts with the children\n                strip = len(node.v_full_name) + 1\n                nest_dict = {key[strip:]: val for key, val in result_dict.items()}\n            result_dict = nest_dictionary(nest_dict, '.')\n\n        return result_dict"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _make_child_iterator(node, with_links, current_depth=0):\n        cdp1 = current_depth + 1\n        if with_links:\n            iterator = ((cdp1, x[0], x[1]) for x in node._children.items())\n        else:\n            leaves = ((cdp1, x[0], x[1]) for x in node._leaves.items())\n            groups = ((cdp1, y[0], y[1]) for y in node._groups.items())\n            iterator = itools.chain(groups, leaves)\n        return iterator", "response": "Returns an iterator over a node s children."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _recursive_traversal_bfs(node, linked_by=None,\n                                 max_depth=float('inf'),\n                                 with_links=True, in_search=False, predicate=None):\n        \"\"\"Iterator function traversing the tree below `node` in breadth first search manner.\n\n        If `run_name` is given only sub branches of this run are considered and the rest is\n        blinded out.\n\n        \"\"\"\n\n        if predicate is None:\n            predicate = lambda x: True\n\n        iterator_queue = IteratorChain([(0, node.v_name, node)])\n        #iterator_queue = iter([(0, node.v_name, node)])\n        start = True\n        visited_linked_nodes = set([])\n\n        while True:\n            try:\n                depth, name, item = next(iterator_queue)\n                full_name = item._full_name\n                if start or predicate(item):\n                    if full_name in visited_linked_nodes:\n                        if in_search:\n                            # We need to return the node again to check if a link to the node\n                            # has to be found\n                            yield depth, name, item\n                    elif depth <= max_depth:\n                        if start:\n                            start = False\n                        else:\n                            if in_search:\n                                yield depth, name, item\n                            else:\n                                yield item\n\n                        if full_name in linked_by:\n                            visited_linked_nodes.add(full_name)\n\n                        if not item._is_leaf and depth < max_depth:\n                            child_iterator = NaturalNamingInterface._make_child_iterator(item,\n                                                                        with_links,\n                                                                        current_depth=depth)\n                            iterator_queue.add(child_iterator)\n                            #iterator_queue = itools.chain(iterator_queue, child_iterator)\n            except StopIteration:\n                break", "response": "Iterator function traversing the tree below node in breadth first search manner."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfasting search for a node in the tree.", "response": "def _very_fast_search(self, node, key, max_depth, with_links, crun):\n        \"\"\"Fast search for a node in the tree.\n\n        The tree is not traversed but the reference dictionaries are searched.\n\n        :param node:\n\n            Parent node to start from\n\n        :param key:\n\n            Name of node to find\n\n        :param max_depth:\n\n            Maximum depth.\n\n        :param with_links:\n\n            If we work with links than we can only be sure to found the node in case we\n            have a single match. Otherwise the other match might have been linked as well.\n\n        :param crun:\n\n            If given only nodes belonging to this particular run are searched and the rest\n            is blinded out.\n\n        :return: The found node and its depth\n\n        :raises:\n\n            TooManyGroupsError:\n\n                If search cannot performed fast enough, an alternative search method is needed.\n\n            NotUniqueNodeError:\n\n                If several nodes match the key criterion\n\n        \"\"\"\n\n        if key in self._links_count:\n            return\n\n        parent_full_name = node.v_full_name\n        starting_depth = node.v_depth\n        candidate_dict = self._get_candidate_dict(key, crun)\n\n        # If there are to many potential candidates sequential search might be too slow\n        if with_links:\n            upper_bound = 1\n        else:\n            upper_bound = FAST_UPPER_BOUND\n        if len(candidate_dict) > upper_bound:\n            raise pex.TooManyGroupsError('Too many nodes')\n\n        # Next check if the found candidates could be reached from the parent node\n        result_node = None\n        for goal_name in candidate_dict:\n\n            # Check if we have found a matching node\n            if goal_name.startswith(parent_full_name):\n                candidate = candidate_dict[goal_name]\n                if candidate.v_depth - starting_depth <= max_depth:\n                    # In case of several solutions raise an error:\n                    if result_node is not None:\n                        raise pex.NotUniqueNodeError('Node `%s` has been found more than once, '\n                                                     'full name of first occurrence is `%s` and of'\n                                                     'second `%s`'\n                                                     % (key, goal_name, result_node.v_full_name))\n                    result_node = candidate\n\n        if result_node is not None:\n            return result_node, result_node.v_depth"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _search(self, node, key, max_depth=float('inf'), with_links=True, crun=None):\n\n        # If we find it directly there is no need for an exhaustive search\n        if key in node._children and (with_links or key not in node._links):\n            return node._children[key], 1\n\n        # First the very fast search is tried that does not need tree traversal.\n        try:\n            result = self._very_fast_search(node, key, max_depth, with_links, crun)\n            if result:\n                return result\n        except pex.TooManyGroupsError:\n            pass\n        except pex.NotUniqueNodeError:\n            pass\n\n        # Slowly traverse the entire tree\n        nodes_iterator = self._iter_nodes(node, recursive=True,\n                                          max_depth=max_depth, in_search=True,\n                                          with_links=with_links)\n        result_node = None\n        result_depth = float('inf')\n        for depth, name, child in nodes_iterator:\n\n            if depth > result_depth:\n                # We can break here because we enter a deeper stage of the tree and we\n                # cannot find matching node of the same depth as the one we found\n                break\n\n            if key == name:\n                # If result_node is not None means that we care about uniqueness and the search\n                # has found more than a single solution.\n                if result_node is not None:\n                    raise pex.NotUniqueNodeError('Node `%s` has been found more than once within '\n                                                 'the same depth %d. '\n                                                 'Full name of first occurrence is `%s` and of '\n                                                 'second `%s`'\n                                                 % (key, child.v_depth, result_node.v_full_name,\n                                                    child.v_full_name))\n\n                result_node = child\n                result_depth = depth\n\n        return result_node, result_depth", "response": "Search the tree for an item in the tree below the given node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming a backwards search from the terminal node back to the start node :param start_node: The node from where search starts, or here better way where backwards search should end. :param split_name: List of names :param max_depth: Maximum search depth where to look for :param shortcuts: If shortcuts are allowed", "response": "def _backwards_search(self, start_node, split_name, max_depth=float('inf'), shortcuts=True):\n        \"\"\" Performs a backwards search from the terminal node back to the start node\n\n        :param start_node:\n\n            The node from where search starts, or here better way where backwards search should\n            end.\n\n        :param split_name:\n\n            List of names\n\n        :param max_depth:\n\n            Maximum search depth where to look for\n\n        :param shortcuts:\n\n            If shortcuts are allowed\n\n        \"\"\"\n\n        result_list = [] # Result list of all found items\n        full_name_set = set() # Set containing full names of all found items to avoid finding items\n        # twice due to links\n\n        colon_name = '.'.join(split_name)\n        key = split_name[-1]\n        candidate_dict = self._get_candidate_dict(key, None, use_upper_bound=False)\n        parent_full_name = start_node.v_full_name\n\n        split_length = len(split_name)\n\n        for candidate_name in candidate_dict:\n            # Check if candidate startswith the parent's name\n            candidate = candidate_dict[candidate_name]\n            if key != candidate.v_name or candidate.v_full_name in full_name_set:\n                # If this is not the case we do have link, that we need to skip\n                continue\n\n            if candidate_name.startswith(parent_full_name):\n                if parent_full_name != '':\n                    reduced_candidate_name = candidate_name[len(parent_full_name) + 1:]\n                else:\n                    reduced_candidate_name = candidate_name\n\n                candidate_split_name = reduced_candidate_name.split('.')\n\n                if len(candidate_split_name) > max_depth:\n                    break\n\n                if len(split_name) == 1 or reduced_candidate_name.endswith(colon_name):\n                    result_list.append(candidate)\n                    full_name_set.add(candidate.v_full_name)\n\n                elif shortcuts:\n\n                    candidate_set = set(candidate_split_name)\n                    climbing = True\n                    for name in split_name:\n                        if name not in candidate_set:\n                            climbing = False\n                            break\n\n                    if climbing:\n                        count = 0\n                        candidate_length = len(candidate_split_name)\n                        for idx in range(candidate_length):\n\n                            if idx + split_length - count > candidate_length:\n                                break\n\n                            if split_name[count] == candidate_split_name[idx]:\n                                count += 1\n                                if count == len(split_name):\n                                    result_list.append(candidate)\n                                    full_name_set.add(candidate.v_full_name)\n                                    break\n\n        return result_list"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_all(self, node, name, max_depth, shortcuts):\n\n        if max_depth is None:\n            max_depth = float('inf')\n\n        if isinstance(name, list):\n            split_name = name\n        elif isinstance(name, tuple):\n            split_name = list(name)\n        elif isinstance(name, int):\n            split_name = [name]\n        else:\n            split_name = name.split('.')\n\n        for idx, key in enumerate(split_name):\n            _, key = self._translate_shortcut(key)\n            _, key = self._replace_wildcards(key)\n            split_name[idx] = key\n\n        return self._backwards_search(node, split_name, max_depth, shortcuts)", "response": "Searches for all occurrences of name under node."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsearching for an item with the given name.", "response": "def _get(self, node, name, fast_access,\n             shortcuts, max_depth, auto_load, with_links):\n        \"\"\"Searches for an item (parameter/result/group node) with the given `name`.\n\n        :param node: The node below which the search is performed\n\n        :param name: Name of the item (full name or parts of the full name)\n\n        :param fast_access: If the result is a parameter, whether fast access should be applied.\n\n        :param max_depth:\n\n            Maximum search depth relative to start node.\n\n        :param auto_load:\n\n            If data should be automatically loaded\n\n        :param with_links\n\n            If links should be considered\n\n        :return:\n\n            The found instance (result/parameter/group node) or if fast access is True and you\n            found a parameter or result that supports fast access, the contained value is returned.\n\n        :raises:\n\n            AttributeError if no node with the given name can be found.\n            Raises errors that are raised by the storage service if `auto_load=True` and\n            item cannot be found.\n\n        \"\"\"\n        if auto_load and not with_links:\n            raise ValueError('If you allow auto-loading you mus allow links.')\n\n        if isinstance(name, list):\n            split_name = name\n        elif isinstance(name, tuple):\n            split_name = list(name)\n        elif isinstance(name, int):\n            split_name = [name]\n        else:\n            split_name = name.split('.')\n\n        if node.v_is_root:\n            # We want to add `parameters`, `config`, `derived_parameters` and `results`\n            # on the fly if they don't exist\n            if len(split_name) == 1 and split_name[0] == '':\n                return node\n            key = split_name[0]\n            _, key = self._translate_shortcut(key)\n            if key in SUBTREE_MAPPING and key not in node._children:\n                node.f_add_group(key)\n\n        if max_depth is None:\n            max_depth = float('inf')\n\n        if len(split_name) > max_depth and shortcuts:\n            raise ValueError(\n                'Name of node to search for (%s) is longer thant the maximum depth %d' %\n                (str(name), max_depth))\n\n        try_auto_load_directly1 = False\n        try_auto_load_directly2 = False\n        wildcard_positions = []\n\n        root = self._root_instance\n        # # Rename shortcuts and check keys:\n        for idx, key in enumerate(split_name):\n            translated_shortcut, key = self._translate_shortcut(key)\n            if translated_shortcut:\n                split_name[idx] = key\n\n            if key[0] == '_':\n                raise AttributeError('Leading underscores are not allowed '\n                                     'for group or parameter '\n                                     'names. Cannot return %s.' % key)\n\n            is_wildcard = self._root_instance.f_is_wildcard(key)\n            if (not is_wildcard and key not in self._nodes_and_leaves and\n                    key not in self._links_count):\n                try_auto_load_directly1 = True\n                try_auto_load_directly2 = True\n\n            if is_wildcard:\n                wildcard_positions.append((idx, key))\n                if root.f_wildcard(key) not in self._nodes_and_leaves:\n                    try_auto_load_directly1 = True\n                if root.f_wildcard(key, -1) not in self._nodes_and_leaves:\n                    try_auto_load_directly2 = True\n\n        run_idx = root.v_idx\n        wildcard_exception = None # Helper variable to store the exception thrown in case\n        # of using a wildcard, to be re-thrown later on.\n\n        if try_auto_load_directly1 and try_auto_load_directly2 and not auto_load:\n            for wildcard_pos, wildcard in wildcard_positions:\n                split_name[wildcard_pos] = root.f_wildcard(wildcard, run_idx)\n            raise AttributeError('%s is not part of your trajectory or it\\'s tree. ' %\n                                 str('.'.join(split_name)))\n\n        if run_idx > -1:\n            # If we count the wildcard we have to perform the search twice,\n            # one with a run name and one with the dummy:\n            with self._disable_logging:\n                try:\n                    for wildcard_pos, wildcard in wildcard_positions:\n                        split_name[wildcard_pos] = root.f_wildcard(wildcard, run_idx)\n\n                    result = self._perform_get(node, split_name, fast_access,\n                                               shortcuts, max_depth, auto_load, with_links,\n                                               try_auto_load_directly1)\n                    return result\n                except (pex.DataNotInStorageError, AttributeError) as exc:\n                    wildcard_exception = exc\n\n        if wildcard_positions:\n            for wildcard_pos, wildcard in wildcard_positions:\n                split_name[wildcard_pos] = root.f_wildcard(wildcard, -1)\n        try:\n            return self._perform_get(node, split_name, fast_access,\n                                     shortcuts, max_depth, auto_load, with_links,\n                                     try_auto_load_directly2)\n        except (pex.DataNotInStorageError, AttributeError):\n            # Re-raise the old error in case it was thronw already\n            if wildcard_exception is not None:\n                raise wildcard_exception\n            else:\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nperforming a search for an item in the given node.", "response": "def _perform_get(self, node, split_name, fast_access,\n                     shortcuts, max_depth, auto_load, with_links,\n                     try_auto_load_directly):\n        \"\"\"Searches for an item (parameter/result/group node) with the given `name`.\n\n        :param node: The node below which the search is performed\n\n        :param split_name: Name split into list according to '.'\n\n        :param fast_access: If the result is a parameter, whether fast access should be applied.\n\n        :param max_depth:\n\n            Maximum search depth relative to start node.\n\n        :param auto_load:\n\n            If data should be automatically loaded\n\n        :param with_links:\n\n            If links should be considered.\n\n        :param try_auto_load_directly:\n\n            If one should skip search and directly try auto_loading\n\n        :return:\n\n            The found instance (result/parameter/group node) or if fast access is True and you\n            found a parameter or result that supports fast access, the contained value is returned.\n\n        :raises:\n\n            AttributeError if no node with the given name can be found\n            Raises errors that are raiesd by the storage service if `auto_load=True`\n\n        \"\"\"\n\n        result = None\n        name = '.'.join(split_name)\n\n        if len(split_name) > max_depth:\n            raise AttributeError('The node or param/result `%s`, cannot be found under `%s`'\n                                 'The name you are looking for is larger than the maximum '\n                                 'search depth.' %\n                                 (name, node.v_full_name))\n\n        if shortcuts and not try_auto_load_directly:\n            first = split_name[0]\n\n            if len(split_name) == 1 and first in node._children and (with_links or\n                                                                    first not in node._links):\n                result = node._children[first]\n            else:\n\n                result = self._check_flat_dicts(node, split_name)\n\n                if result is None:\n                    # Check in O(N) with `N` number of groups and nodes\n                    # [Worst Case O(N), average case is better\n                    # since looking into a single dict costs O(1)].\n                    result = node\n                    crun = None\n                    for key in split_name:\n                        if key in self._root_instance._run_information:\n                            crun = key\n                        result, depth = self._search(result, key, max_depth, with_links, crun)\n                        max_depth -= depth\n                        if result is None:\n                            break\n\n        elif not try_auto_load_directly:\n            result = node\n            for name in split_name:\n                if name in result._children and (with_links or not name in result._links):\n                    result = result._children[name]\n                else:\n                    raise AttributeError(\n                        'You did not allow for shortcuts and `%s` was not directly '\n                        'found  under node `%s`.' % (name, result.v_full_name))\n\n        if result is None and auto_load:\n            try:\n                result = node.f_load_child('.'.join(split_name),\n                                           load_data=pypetconstants.LOAD_DATA)\n                if (self._root_instance.v_idx != -1 and\n                            result.v_is_leaf and\n                            result.v_is_parameter and\n                            result.v_explored):\n                    result._set_parameter_access(self._root_instance.v_idx)\n            except:\n                self._logger.error('Error while auto-loading `%s` under `%s`.' %\n                                   (name, node.v_full_name))\n                raise\n\n        if result is None:\n            raise AttributeError('The node or param/result `%s`, cannot be found under `%s`' %\n                                 (name, node.v_full_name))\n        if result.v_is_leaf:\n            if auto_load and result.f_is_empty():\n\n                try:\n                    self._root_instance.f_load_item(result)\n                    if (self._root_instance.v_idx != -1 and\n                            result.v_is_parameter and\n                            result.v_explored):\n                        result._set_parameter_access(self._root_instance.v_idx)\n                except:\n                    self._logger.error('Error while auto-loading `%s` under `%s`. I found the '\n                                       'item but I could not load the data.' %\n                                       (name, node.v_full_name))\n                    raise\n\n            return self._apply_fast_access(result, fast_access)\n        else:\n            return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _add_group_from_storage(self, args, kwargs):\n        return self._nn_interface._add_generic(self,\n                                                type_name=GROUP,\n                                                group_type_name=GROUP,\n                                                args=args,\n                                                kwargs=kwargs,\n                                                add_prefix=False,\n                                                check_naming=False)", "response": "Add a new group from storage service"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _add_leaf_from_storage(self, args, kwargs):\n        return self._nn_interface._add_generic(self,\n                                                type_name=LEAF,\n                                                group_type_name=GROUP,\n                                                args=args, kwargs=kwargs,\n                                                add_prefix=False,\n                                                check_naming=False)", "response": "Add a leaf from storage service to the network interface"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of all children names", "response": "def f_dir_data(self):\n        \"\"\"Returns a list of all children names\"\"\"\n        if (self._nn_interface is not None and\n                    self._nn_interface._root_instance is not None\n                        and self.v_root.v_auto_load):\n            try:\n                if self.v_is_root:\n                    self.f_load(recursive=True, max_depth=1,\n                                load_data=pypetconstants.LOAD_SKELETON,\n                                with_meta_data=False,\n                                with_run_information=False)\n                else:\n                    self.f_load(recursive=True, max_depth=1, load_data=pypetconstants.LOAD_SKELETON)\n            except Exception as exc:\n                pass\n        return list(self._children.keys())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a dummy object containing the whole tree to make unfolding easier.", "response": "def _debug(self):\n        \"\"\"Creates a dummy object containing the whole tree to make unfolding easier.\n\n        This method is only useful for debugging purposes.\n        If you use an IDE and want to unfold the trajectory tree, you always need to\n        open the private attribute `_children`. Use to this function to create a new\n        object that contains the tree structure in its attributes.\n\n        Manipulating the returned object does not change the original tree!\n\n        \"\"\"\n\n        class Bunch(object):\n            \"\"\"Dummy container class\"\"\"\n            pass\n\n        debug_tree = Bunch()\n\n        if not self.v_annotations.f_is_empty():\n            debug_tree.v_annotations = self.v_annotations\n        if not self.v_comment == '':\n            debug_tree.v_comment = self.v_comment\n\n        for leaf_name in self._leaves:\n            leaf = self._leaves[leaf_name]\n            setattr(debug_tree, leaf_name, leaf)\n\n        for link_name in self._links:\n            linked_node = self._links[link_name]\n            setattr(debug_tree, link_name, 'Link to `%s`' % linked_node.v_full_name)\n\n        for group_name in self._groups:\n            group = self._groups[group_name]\n            setattr(debug_tree, group_name, group._debug())\n\n        return debug_tree"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the parent of the current node.", "response": "def f_get_parent(self):\n        \"\"\"Returns the parent of the node.\n\n        Raises a TypeError if current node is root.\n\n        \"\"\"\n        if self.v_is_root:\n            raise TypeError('Root does not have a parent')\n        elif self.v_location == '':\n            return self.v_root\n        else:\n            return self.v_root.f_get(self.v_location, fast_access=False, shortcuts=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef f_add_group(self, *args, **kwargs):\n\n        return self._nn_interface._add_generic(self, type_name=GROUP,\n                                               group_type_name=GROUP,\n                                               args=args, kwargs=kwargs, add_prefix=False)", "response": "Adds an empty generic group under the current node."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a link to an existing node.", "response": "def f_add_link(self, name_or_item, full_name_or_item=None):\n        \"\"\"Adds a link to an existing node.\n\n        Can be called as ``node.f_add_link(other_node)`` this will add a link the `other_node`\n        with the link name as the name of the node.\n\n        Or can be called as ``node.f_add_link(name, other_node)`` to add a link to the\n        `other_node` and the given `name` of the link.\n\n        In contrast to addition of groups and leaves,  colon separated names\n        are **not** allowed, i.e. ``node.f_add_link('mygroup.mylink', other_node)``\n        does not work.\n\n        \"\"\"\n        if isinstance(name_or_item, str):\n            name = name_or_item\n            if isinstance(full_name_or_item, str):\n                instance = self.v_root.f_get(full_name_or_item)\n            else:\n                instance =  full_name_or_item\n        else:\n            instance = name_or_item\n            name = instance.v_name\n\n        return self._nn_interface._add_generic(self, type_name=LINK,\n                                               group_type_name=GROUP, args=(name, instance),\n                                               kwargs={},\n                                               add_prefix=False)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef f_remove_link(self, name):\n        if name not in self._links:\n            raise ValueError('No link with name `%s` found under `%s`.' % (name, self._full_name))\n\n        self._nn_interface._remove_link(self, name)", "response": "Removes a link from the current group node with a given name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef f_add_leaf(self, *args, **kwargs):\n\n        return self._nn_interface._add_generic(self, type_name=LEAF,\n                                               group_type_name=GROUP,\n                                               args=args, kwargs=kwargs,\n                                               add_prefix=False)", "response": "Adds an empty generic leaf under the current node."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef f_remove_child(self, name, recursive=False, predicate=None):\n        if name not in self._children:\n            raise ValueError('Your group `%s` does not contain the child `%s`.' %\n                             (self.v_full_name, name))\n        else:\n            child = self._children[name]\n            if (name not in self._links and\n                    not child.v_is_leaf and\n                    child.f_has_children() and\n                    not recursive):\n                raise TypeError('Cannot remove child. It is a group with children. Use'\n                                ' f_remove with ``recursive = True``')\n            else:\n                self._nn_interface._remove_subtree(self, name, predicate)", "response": "Removes a child of the current group."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef f_contains(self, item, with_links=True, shortcuts=False, max_depth=None):\n\n        # Check if an instance or a name was supplied by the user\n        try:\n            search_string = item.v_full_name\n            parent_full_name = self.v_full_name\n\n            if not search_string.startswith(parent_full_name):\n                return False\n\n            if parent_full_name != '':\n                search_string = search_string[len(parent_full_name) + 1:]\n            else:\n                search_string = search_string\n\n            shortcuts = False # if we search for a particular item we do not allow shortcuts\n\n        except AttributeError:\n            search_string = item\n            item = None\n\n        if search_string == '':\n            return False # To allow to search for nodes wit name = '', which are never part\n            # of the trajectory\n\n        try:\n            result = self.f_get(search_string,\n                                shortcuts=shortcuts, max_depth=max_depth, with_links=with_links)\n        except AttributeError:\n            return False\n\n        if item is not None:\n            return id(item) == id(result)\n        else:\n            return True", "response": "Checks if the node contains a specific parameter or result."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\niterates over nodes in the search tree.", "response": "def f_iter_nodes(self, recursive=True, with_links=True, max_depth=None, predicate=None):\n        \"\"\"Iterates recursively (default) over nodes hanging below this group.\n\n        :param recursive: Whether to iterate the whole sub tree or only immediate children.\n\n        :param with_links: If links should be considered\n\n        :param max_depth: Maximum depth in search tree relative to start node (inclusive)\n\n        :param predicate:\n\n            A predicate function that is applied for each node and only returns the node\n            if it evaluates to ``True``. If ``False``\n            and you iterate recursively also the children are spared.\n\n            Leave to `None` if you don't want to filter and simply iterate over all nodes.\n\n\n            For example, to iterate only over groups you could use:\n\n            >>> traj.f_iter_nodes(recursive=True, predicate=lambda x: x.v_is_group)\n\n            To blind out all runs except for a particular set, you can simply pass a tuple\n            of run indices with -1 referring to the ``run_ALL`` node.\n\n            For instance\n\n            >>> traj.f_iter_nodes(recursive=True, predicate=(0,3,-1))\n\n            Will blind out all nodes hanging below a group named ``run_XXXXXXXXX``\n            (including the group) except ``run_00000000``, ``run_00000003``, and ``run_ALL``.\n\n\n        :return: Iterator over nodes\n\n        \"\"\"\n        return self._nn_interface._iter_nodes(self, recursive=recursive, with_links=with_links,\n                                              max_depth=max_depth,\n                                              predicate=predicate)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef f_iter_leaves(self, with_links=True):\n        for node in self.f_iter_nodes(with_links=with_links):\n            if node.v_is_leaf:\n                yield node", "response": "Iterates recursively over all leaves in the current group."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsearches for all occurrences of name under node.", "response": "def f_get_all(self, name, max_depth=None, shortcuts=True):\n        \"\"\" Searches for all occurrences of `name` under `node`.\n\n        Links are NOT considered since nodes are searched bottom up in the tree.\n\n        :param node:\n\n            Start node\n\n        :param name:\n\n            Name of what to look for, can be separated by colons, i.e.\n            ``'mygroupA.mygroupB.myparam'``.\n\n        :param max_depth:\n\n            Maximum search depth relative to start node.\n            `None` for no limit.\n\n        :param shortcuts:\n\n            If shortcuts are allowed, otherwise the stated name defines a\n            consecutive name.For instance. ``'mygroupA.mygroupB.myparam'`` would\n            also find ``mygroupA.mygroupX.mygroupB.mygroupY.myparam`` if shortcuts\n            are allowed, otherwise not.\n\n        :return:\n\n            List of nodes that match the name, empty list if nothing was found.\n\n        \"\"\"\n        return self._nn_interface._get_all(self, name, max_depth=max_depth, shortcuts=shortcuts)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the default value for the given name if the name is not found in the trajectory.", "response": "def f_get_default(self, name, default=None, fast_access=True, with_links=True,\n              shortcuts=True, max_depth=None, auto_load=False):\n        \"\"\" Similar to `f_get`, but returns the default value if `name` is not found in the\n        trajectory.\n\n        This function uses the `f_get` method and will return the default value\n        in case `f_get` raises an AttributeError or a DataNotInStorageError.\n        Other errors are not handled.\n\n        In contrast to `f_get`, fast access is True by default.\n\n        \"\"\"\n        try:\n            return self.f_get(name, fast_access=fast_access,\n                           shortcuts=shortcuts,\n                           max_depth=max_depth,\n                           auto_load=auto_load,\n                           with_links=with_links)\n\n        except (AttributeError, pex.DataNotInStorageError):\n            return default"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsearch and returns an item with the given name.", "response": "def f_get(self, name, fast_access=False, with_links=True,\n              shortcuts=True, max_depth=None, auto_load=False):\n        \"\"\"Searches and returns an item (parameter/result/group node) with the given `name`.\n\n        :param name: Name of the item (full name or parts of the full name)\n\n        :param fast_access: Whether fast access should be applied.\n\n        :param with_links:\n\n            If links are considered. Cannot be set to ``False`` if ``auto_load`` is ``True``.\n\n        :param shortcuts:\n\n            If shortcuts are allowed and the trajectory can *hop* over nodes in the\n            path.\n\n        :param max_depth:\n\n            Maximum depth relative to starting node (inclusive).\n            `None` means no depth limit.\n\n        :param auto_load:\n\n            If data should be loaded from the storage service if it cannot be found in the\n            current trajectory tree. Auto-loading will load group and leaf nodes currently\n            not in memory and it will load data into empty leaves. Be aware that auto-loading\n            does not work with shortcuts.\n\n        :return:\n\n            The found instance (result/parameter/group node) or if fast access is True and you\n            found a parameter or result that supports fast access, the contained value is returned.\n\n        :raises:\n\n            AttributeError: If no node with the given name can be found\n\n            NotUniqueNodeError\n\n                In case of forward search if more than one candidate node is found within a\n                particular depth of the tree. In case of backwards search if more than\n                one candidate is found regardless of the depth.\n\n            DataNotInStorageError:\n\n                In case auto-loading fails\n\n            Any exception raised by the StorageService in case auto-loading is enabled\n\n        \"\"\"\n        return self._nn_interface._get(self, name, fast_access=fast_access,\n                                       shortcuts=shortcuts,\n                                       max_depth=max_depth,\n                                       auto_load=auto_load,\n                                       with_links=with_links)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef f_get_children(self, copy=True):\n        if copy:\n            return self._children.copy()\n        else:\n            return self._children", "response": "Returns a dictionary of children."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef f_get_groups(self, copy=True):\n        if copy:\n            return self._groups.copy()\n        else:\n            return self._groups", "response": "Returns a dictionary of groups hanging immediately below this group."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef f_get_leaves(self, copy=True):\n        if copy:\n            return self._leaves.copy()\n        else:\n            return self._leaves", "response": "Returns a dictionary of all leaves hanging immediately below this group."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a dictionary of nodes with their link keys.", "response": "def f_get_links(self, copy=True):\n        \"\"\"Returns a link dictionary.\n\n        :param copy:\n\n            Whether the group's original dictionary or a shallow copy is returned.\n            If you want the real dictionary please do not modify it at all!\n\n        :returns: Dictionary of nodes\n\n        \"\"\"\n        if copy:\n            return self._links.copy()\n        else:\n            return self._links"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef f_store_child(self, name, recursive=False, store_data=pypetconstants.STORE_DATA,\n                      max_depth=None):\n        \"\"\"Stores a child or recursively a subtree to disk.\n\n        :param name:\n\n            Name of child to store. If grouped ('groupA.groupB.childC') the path along the way\n            to last node in the chain is stored. Shortcuts are NOT allowed!\n\n        :param recursive:\n\n            Whether recursively all children's children should be stored too.\n\n        :param store_data:\n\n            For how to choose 'store_data' see :ref:`more-on-storing`.\n\n        :param max_depth:\n\n            In case `recursive` is `True`, you can specify the maximum depth to store\n            data relative from current node. Leave `None` if you don't want to limit\n            the depth.\n\n        :raises: ValueError if the child does not exist.\n\n        \"\"\"\n        if not self.f_contains(name, shortcuts=False):\n            raise ValueError('Your group `%s` does not (directly) contain the child `%s`. '\n                             'Please not that shortcuts are not allowed for `f_store_child`.' %\n                             (self.v_full_name, name))\n\n        traj = self._nn_interface._root_instance\n        storage_service = traj.v_storage_service\n\n        storage_service.store(pypetconstants.TREE, self, name,\n                              trajectory_name=traj.v_name,\n                              recursive=recursive,\n                              store_data=store_data,\n                              max_depth=max_depth)", "response": "Stores a child or recursively a subtree to disk."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstoring a group node to disk.", "response": "def f_store(self, recursive=True, store_data=pypetconstants.STORE_DATA,\n                max_depth=None):\n        \"\"\"Stores a group node to disk\n\n        :param recursive:\n\n            Whether recursively all children should be stored too. Default is ``True``.\n\n        :param store_data:\n\n            For how to choose 'store_data' see :ref:`more-on-storing`.\n\n        :param max_depth:\n\n            In case `recursive` is `True`, you can specify the maximum depth to store\n            data relative from current node. Leave `None` if you don't want to limit\n            the depth.\n\n        \"\"\"\n        traj = self._nn_interface._root_instance\n        storage_service = traj.v_storage_service\n\n        storage_service.store(pypetconstants.GROUP, self,\n                              trajectory_name=traj.v_name,\n                              recursive=recursive,\n                              store_data=store_data,\n                              max_depth=max_depth)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads a child or recursively a subtree from disk.", "response": "def f_load_child(self, name, recursive=False, load_data=pypetconstants.LOAD_DATA,\n                     max_depth=None):\n        \"\"\"Loads a child or recursively a subtree from disk.\n\n        :param name:\n\n            Name of child to load. If grouped ('groupA.groupB.childC') the path along the way\n            to last node in the chain is loaded. Shortcuts are NOT allowed!\n\n        :param recursive:\n\n            Whether recursively all nodes below the last child should be loaded, too.\n            Note that links are never evaluated recursively. Only the linked node\n            will be loaded if it does not exist in the tree, yet. Any nodes or links\n            of this linked node are not loaded.\n\n        :param load_data:\n\n            Flag how to load the data.\n            For how to choose 'load_data' see :ref:`more-on-loading`.\n\n        :param max_depth:\n\n            In case `recursive` is `True`, you can specify the maximum depth to load\n            load data relative from current node. Leave `None` if you don't want to limit\n            the depth.\n\n        :returns:\n\n            The loaded child, in case of grouping ('groupA.groupB.childC') the last\n            node (here 'childC') is returned.\n\n        \"\"\"\n\n        traj = self._nn_interface._root_instance\n        storage_service = traj.v_storage_service\n\n        storage_service.load(pypetconstants.TREE, self, name,\n                             trajectory_name=traj.v_name,\n                             load_data=load_data,\n                             recursive=recursive,\n                             max_depth=max_depth)\n\n        return self.f_get(name, shortcuts=False)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload a group from disk.", "response": "def f_load(self, recursive=True, load_data=pypetconstants.LOAD_DATA,\n               max_depth=None):\n        \"\"\"Loads a group from disk.\n\n        :param recursive:\n\n            Default is ``True``.\n            Whether recursively all nodes below the current node should be loaded, too.\n            Note that links are never evaluated recursively. Only the linked node\n            will be loaded if it does not exist in the tree, yet. Any nodes or links\n            of this linked node are not loaded.\n\n        :param load_data:\n\n            Flag how to load the data.\n            For how to choose 'load_data' see :ref:`more-on-loading`.\n\n        :param max_depth:\n\n            In case `recursive` is `True`, you can specify the maximum depth to load\n            load data relative from current node.\n\n        :returns:\n\n            The node itself.\n\n        \"\"\"\n\n        traj = self._nn_interface._root_instance\n        storage_service = traj.v_storage_service\n\n        storage_service.load(pypetconstants.GROUP, self,\n                             trajectory_name=traj.v_name,\n                             load_data=load_data,\n                             recursive=recursive,\n                             max_depth=max_depth)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds an empty parameter group under the current node.", "response": "def f_add_parameter_group(self, *args, **kwargs):\n        \"\"\"Adds an empty parameter group under the current node.\n\n        Can be called with ``f_add_parameter_group('MyName', 'this is an informative comment')``\n        or ``f_add_parameter_group(name='MyName', comment='This is an informative comment')``\n        or with a given new group instance:\n        ``f_add_parameter_group(ParameterGroup('MyName', comment='This is a comment'))``.\n\n        Adds the full name of the current node as prefix to the name of the group.\n        If current node is the trajectory (root), the prefix `'parameters'`\n        is added to the full name.\n\n        The `name` can also contain subgroups separated via colons, for example:\n        `name=subgroup1.subgroup2.subgroup3`. These other parent groups will be automatically\n        created.\n\n        \"\"\"\n        return self._nn_interface._add_generic(self, type_name=PARAMETER_GROUP,\n                                               group_type_name=PARAMETER_GROUP,\n                                               args=args, kwargs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef f_add_parameter(self, *args, **kwargs):\n        return self._nn_interface._add_generic(self, type_name=PARAMETER,\n                                               group_type_name=PARAMETER_GROUP,\n                                               args=args, kwargs=kwargs)", "response": "Adds a parameter to the trajectory."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd an empty result group under the current node.", "response": "def f_add_result_group(self, *args, **kwargs):\n        \"\"\"Adds an empty result group under the current node.\n\n        Adds the full name of the current node as prefix to the name of the group.\n        If current node is a single run (root) adds the prefix `'results.runs.run_08%d%'` to the\n        full name where `'08%d'` is replaced by the index of the current run.\n\n        The `name` can also contain subgroups separated via colons, for example:\n        `name=subgroup1.subgroup2.subgroup3`. These other parent groups will be automatically\n        be created.\n\n        \"\"\"\n\n        return self._nn_interface._add_generic(self, type_name=RESULT_GROUP,\n                                               group_type_name=RESULT_GROUP,\n                                               args=args, kwargs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a result to the current node.", "response": "def f_add_result(self, *args, **kwargs):\n        \"\"\"Adds a result under the current node.\n\n        There are two ways to add a new result either by adding a result instance:\n\n        >>> new_result = Result('group1.group2.myresult', 1666, x=3, y=4, comment='Example!')\n        >>> traj.f_add_result(new_result)\n\n        Or by passing the values directly to the function, with the name being the first\n        (non-keyword!) argument:\n\n        >>> traj.f_add_result('group1.group2.myresult', 1666, x=3, y=3,comment='Example!')\n\n\n        If you want to create a different result than the standard result, you can\n        give the constructor as the first (non-keyword!) argument followed by the name\n        (non-keyword!):\n\n        >>> traj.f_add_result(PickleResult,'group1.group2.myresult', 1666, x=3, y=3, comment='Example!')\n\n        Additional arguments (here `1666`) or keyword arguments (here `x=3, y=3`) are passed\n        onto the constructor of the result.\n\n\n        Adds the full name of the current node as prefix to the name of the result.\n        If current node is a single run (root) adds the prefix `'results.runs.run_08%d%'` to the\n        full name where `'08%d'` is replaced by the index of the current run.\n\n        \"\"\"\n        return self._nn_interface._add_generic(self, type_name=RESULT,\n                                               group_type_name=RESULT_GROUP,\n                                               args=args, kwargs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef f_add_derived_parameter_group(self, *args, **kwargs):\n\n        return self._nn_interface._add_generic(self, type_name=DERIVED_PARAMETER_GROUP,\n                                               group_type_name=DERIVED_PARAMETER_GROUP,\n                                               args=args, kwargs=kwargs)", "response": "Adds an empty derived parameter group under the current node."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef f_add_derived_parameter(self, *args, **kwargs):\n        return self._nn_interface._add_generic(self, type_name=DERIVED_PARAMETER,\n                                               group_type_name=DERIVED_PARAMETER_GROUP,\n                                               args=args, kwargs=kwargs)", "response": "Adds a derived parameter under the current group."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef f_add_config_group(self, *args, **kwargs):\n        return self._nn_interface._add_generic(self, type_name=CONFIG_GROUP,\n                                               group_type_name=CONFIG_GROUP,\n                                               args=args, kwargs=kwargs)", "response": "Adds an empty config group under the current node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef f_add_config(self, *args, **kwargs):\n        return self._nn_interface._add_generic(self, type_name=CONFIG,\n                                               group_type_name=CONFIG_GROUP,\n                                               args=args, kwargs=kwargs)", "response": "Adds a config parameter under the current group."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntakes a unit string like 1. * volt and returns the BRIAN2 unit.", "response": "def unit_from_expression(expr):\n    \"\"\"Takes a unit string like ``'1. * volt'`` and returns the BRIAN2 unit.\"\"\"\n    if expr == '1':\n        return get_unit_fast(1)\n    elif isinstance(expr, str):\n        mod = ast.parse(expr, mode='eval')\n        expr = mod.body\n        return unit_from_expression(expr)\n    elif expr.__class__ is ast.Name:\n        return ALLUNITS[expr.id]\n    elif expr.__class__ is ast.Num:\n        return expr.n\n    elif expr.__class__ is ast.UnaryOp:\n        op = expr.op.__class__.__name__\n        operand = unit_from_expression(expr.operand)\n        if op=='USub':\n            return -operand\n        else:\n            raise SyntaxError(\"Unsupported operation \"+op)\n    elif expr.__class__ is ast.BinOp:\n        op = expr.op.__class__.__name__\n        left = unit_from_expression(expr.left)\n        right = unit_from_expression(expr.right)\n        if op=='Add':\n            u = left+right\n        elif op=='Sub':\n            u = left-right\n        elif op=='Mult':\n            u = left*right\n        elif op=='Div':\n            u = left/right\n        elif op=='Pow':\n            n = unit_from_expression(expr.right)\n            u = left**n\n        elif op=='Mod':\n            u = left % right\n        else:\n            raise SyntaxError(\"Unsupported operation \"+op)\n        return u\n    else:\n        raise RuntimeError('You shall not pass')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if data is supported by this parameter.", "response": "def f_supports(self, data):\n        \"\"\" Simply checks if data is supported \"\"\"\n        if isinstance(data, Quantity):\n            return True\n        elif super(Brian2Parameter, self).f_supports(data):\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _supports(self, data):\n        if isinstance(data, Quantity):\n            return True\n        elif super(Brian2Result, self)._supports(data):\n            return True\n        return False", "response": "Check if data is supported by this class."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset single item in Brian2MonitorResult.", "response": "def f_set_single(self, name, item):\n        \"\"\" To add a monitor use `f_set_single('monitor', brian_monitor)`.\n\n        Otherwise `f_set_single` works similar to :func:`~pypet.parameter.Result.f_set_single`.\n        \"\"\"\n        if type(item) in [SpikeMonitor, StateMonitor, PopulationRateMonitor]:\n            if self.v_stored:\n                self._logger.warning('You are changing an already stored result. If '\n                                     'you not explicitly overwrite the data on disk, '\n                                     'this change might be lost and not propagated to disk.')\n\n            self._extract_monitor_data(item)\n        else:\n            super(Brian2MonitorResult, self).f_set_single(name, item)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding the commit information to the trajectory.", "response": "def add_commit_variables(traj, commit):\n    \"\"\"Adds commit information to the trajectory.\"\"\"\n\n    git_time_value = time.strftime('%Y_%m_%d_%Hh%Mm%Ss', time.localtime(commit.committed_date))\n\n    git_short_name = str(commit.hexsha[0:7])\n    git_commit_name = 'commit_%s_' % git_short_name\n    git_commit_name = 'git.' + git_commit_name + git_time_value\n\n    if not traj.f_contains('config.'+git_commit_name, shortcuts=False):\n\n        git_commit_name += '.'\n        # Add the hexsha\n        traj.f_add_config(git_commit_name+'hexsha', commit.hexsha,\n                          comment='SHA-1 hash of commit')\n\n        # Add the description string\n        traj.f_add_config(git_commit_name+'name_rev', commit.name_rev,\n                          comment='String describing the commits hex sha based on '\n                                  'the closest Reference')\n\n        # Add unix epoch\n        traj.f_add_config(git_commit_name+'committed_date',\n                          commit.committed_date, comment='Date of commit as unix epoch seconds')\n\n        # Add commit message\n        traj.f_add_config(git_commit_name+'message', str(commit.message),\n                          comment='The commit message')"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmakes a commit and returns if a new commit was triggered and the SHA_1 code of the commit.", "response": "def make_git_commit(environment, git_repository, user_message, git_fail):\n    \"\"\" Makes a commit and returns if a new commit was triggered and the SHA_1 code of the commit.\n\n    If `git_fail` is `True` program fails instead of triggering a new commit given\n    not committed changes. Then a `GitDiffError` is raised.\n\n    \"\"\"\n\n    # Import GitPython, we do it here to allow also users not having GitPython installed\n    # to use the normal environment\n\n    # Open the repository\n    repo = git.Repo(git_repository)\n    index = repo.index\n\n    traj = environment.v_trajectory\n\n    # Create the commit message and append the trajectory name and comment\n    if traj.v_comment:\n        commentstr = ', Comment: `%s`' % traj.v_comment\n    else:\n        commentstr = ''\n\n    if user_message:\n        user_message += ' -- '\n\n    message = '%sTrajectory: `%s`, Time: `%s`, %s' % \\\n              (user_message, traj.v_name, traj.v_time, commentstr)\n\n    # Detect changes:\n    diff = index.diff(None)\n\n    if diff:\n        if git_fail:\n            # User requested fail instead of a new commit\n            raise pex.GitDiffError('Found not committed changes!')\n        # Make the commit\n        repo.git.add('-u')\n        commit = index.commit(message)\n        new_commit = True\n\n    else:\n        # Take old commit\n        commit = repo.commit(None)\n        new_commit = False\n\n    # Add the commit info to the trajectory\n    add_commit_variables(traj, commit)\n\n    return new_commit, commit.hexsha"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef flatten_dictionary(nested_dict, separator):\n    flat_dict = {}\n    for key, val in nested_dict.items():\n        if isinstance(val, dict):\n            new_flat_dict = flatten_dictionary(val, separator)\n            for flat_key, inval in new_flat_dict.items():\n                new_key = key + separator + flat_key\n                flat_dict[new_key] = inval\n        else:\n            flat_dict[key] = val\n\n    return flat_dict", "response": "Flattens a nested dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef nest_dictionary(flat_dict, separator):\n    nested_dict = {}\n    for key, val in flat_dict.items():\n        split_key = key.split(separator)\n        act_dict = nested_dict\n        final_key = split_key.pop()\n        for new_key in split_key:\n            if not new_key in act_dict:\n                act_dict[new_key] = {}\n\n            act_dict = act_dict[new_key]\n\n        act_dict[final_key] = val\n    return nested_dict", "response": "Nests a given flat dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef progressbar(index, total, percentage_step=10, logger='print', log_level=logging.INFO,\n                 reprint=True, time=True, length=20, fmt_string=None, reset=False):\n    \"\"\"Plots a progress bar to the given `logger` for large for loops.\n\n    To be used inside a for-loop at the end of the loop:\n\n    .. code-block:: python\n\n        for irun in range(42):\n            my_costly_job() # Your expensive function\n            progressbar(index=irun, total=42, reprint=True) # shows a growing progressbar\n\n\n    There is no initialisation of the progressbar necessary before the for-loop.\n    The progressbar will be reset automatically if used in another for-loop.\n\n    :param index: Current index of for-loop\n    :param total: Total size of for-loop\n    :param percentage_step: Steps with which the bar should be plotted\n    :param logger:\n\n        Logger to write to - with level INFO. If string 'print' is given, the print statement is\n        used. Use ``None`` if you don't want to print or log the progressbar statement.\n\n    :param log_level: Log level with which to log.\n    :param reprint:\n\n        If no new line should be plotted but carriage return (works only for printing)\n\n    :param time: If the remaining time should be estimated and displayed\n    :param length: Length of the bar in `=` signs.\n    :param fmt_string:\n\n        A string which contains exactly one `%s` in order to incorporate the progressbar.\n        If such a string is given, ``fmt_string % progressbar`` is printed/logged.\n\n    :param reset:\n\n        If the progressbar should be restarted. If progressbar is called with a lower\n        index than the one before, the progressbar is automatically restarted.\n\n    :return:\n\n        The progressbar string or `None` if the string has not been updated.\n\n    \"\"\"\n    return _progressbar(index=index, total=total, percentage_step=percentage_step,\n                        logger=logger, log_level=log_level, reprint=reprint,\n                        time=time, length=length, fmt_string=fmt_string, reset=reset)", "response": "Plots a progress bar to the given logger."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntake a function and keyword arguments and returns the ones that can be passed.", "response": "def get_matching_kwargs(func, kwargs):\n    \"\"\"Takes a function and keyword arguments and returns the ones that can be passed.\"\"\"\n    args, uses_startstar = _get_argspec(func)\n    if uses_startstar:\n        return kwargs.copy()\n    else:\n        matching_kwargs = dict((k, kwargs[k]) for k in args if k in kwargs)\n        return matching_kwargs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef result_sort(result_list, start_index=0):\n    if len(result_list) < 2:\n        return result_list\n    to_sort = result_list[start_index:]\n    minmax = [x[0] for x in to_sort]\n    minimum = min(minmax)\n    maximum = max(minmax)\n    #print minimum, maximum\n    sorted_list = [None for _ in range(minimum, maximum + 1)]\n    for elem in to_sort:\n        key = elem[0] - minimum\n        sorted_list[key] = elem\n    idx_count = start_index\n    for elem in sorted_list:\n        if elem is not None:\n            result_list[idx_count] = elem\n            idx_count += 1\n    return result_list", "response": "Sorts a list of results in O ( n ) in place"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nformats timestamp to human readable format", "response": "def format_time(timestamp):\n    \"\"\"Formats timestamp to human readable format\"\"\"\n    format_string = '%Y_%m_%d_%Hh%Mm%Ss'\n    formatted_time = datetime.datetime.fromtimestamp(timestamp).strftime(format_string)\n    return formatted_time"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef port_to_tcp(port=None):\n    #address = 'tcp://' + socket.gethostbyname(socket.getfqdn())\n    domain_name = socket.getfqdn()\n    try:\n        addr_list = socket.getaddrinfo(domain_name, None)\n    except Exception:\n        addr_list = socket.getaddrinfo('127.0.0.1', None)\n    family, socktype, proto, canonname, sockaddr = addr_list[0]\n    host = convert_ipv6(sockaddr[0])\n    address =  'tcp://' + host\n    if port is None:\n        port = ()\n    if not isinstance(port, int):\n        # determine port automatically\n        context = zmq.Context()\n        try:\n            socket_ = context.socket(zmq.REP)\n            socket_.ipv6 = is_ipv6(address)\n            port = socket_.bind_to_random_port(address, *port)\n        except Exception:\n            print('Could not connect to {} using {}'.format(address, addr_list))\n            pypet_root_logger = logging.getLogger('pypet')\n            pypet_root_logger.exception('Could not connect to {}'.format(address))\n            raise\n        socket_.close()\n        context.term()\n    return address + ':' + str(port)", "response": "Returns local tcp address for a given port if None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlike os. makedirs but takes care about race conditions", "response": "def racedirs(path):\n    \"\"\"Like os.makedirs but takes care about race conditions\"\"\"\n    if os.path.isfile(path):\n        raise IOError('Path `%s` is already a file not a directory')\n    while True:\n        try:\n            if os.path.isdir(path):\n                # only break if full path has been created or exists\n                break\n            os.makedirs(path)\n        except EnvironmentError as exc:\n            # Part of the directory path already exist\n            if exc.errno != 17:\n                # This error won't be any good\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreset the progressbar to the start of a new one", "response": "def _reset(self, index, total, percentage_step, length):\n        \"\"\"Resets to the progressbar to start a new one\"\"\"\n        self._start_time = datetime.datetime.now()\n        self._start_index = index\n        self._current_index = index\n        self._percentage_step = percentage_step\n        self._total = float(total)\n        self._total_minus_one = total - 1\n        self._length = length\n        self._norm_factor = total * percentage_step / 100.0\n        self._current_interval = int((index + 1.0) / self._norm_factor)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_remaining(self, index):\n        try:\n            current_time = datetime.datetime.now()\n            time_delta = current_time - self._start_time\n            try:\n                total_seconds = time_delta.total_seconds()\n            except AttributeError:\n                # for backwards-compatibility\n                # Python 2.6 does not support `total_seconds`\n                total_seconds = ((time_delta.microseconds +\n                                    (time_delta.seconds +\n                                     time_delta.days * 24 * 3600) * 10 ** 6) / 10.0 ** 6)\n            remaining_seconds = int((self._total - self._start_index - 1.0) *\n                                    total_seconds / float(index - self._start_index) -\n                                    total_seconds)\n            remaining_delta = datetime.timedelta(seconds=remaining_seconds)\n            remaining_str = ', remaining: ' + str(remaining_delta)\n        except ZeroDivisionError:\n            remaining_str = ''\n        return remaining_str", "response": "Calculates the remaining time as a string"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef f_to_dict(self, copy=True):\n        if copy:\n            return self._dict.copy()\n        else:\n            return self._dict", "response": "Returns annotations as dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef f_get(self, *args):\n\n        if len(args) == 0:\n            if len(self._dict) == 1:\n                return self._dict[list(self._dict.keys())[0]]\n            elif len(self._dict) > 1:\n                raise ValueError('Your annotation contains more than one entry: '\n                                 '`%s` Please use >>f_get<< with one of these.' %\n                                 (str(list(self._dict.keys()))))\n            else:\n                raise AttributeError('Your annotation is empty, cannot access data.')\n\n        result_list = []\n        for name in args:\n            name = self._translate_key(name)\n            try:\n                result_list.append(self._dict[name])\n            except KeyError:\n                raise AttributeError('Your annotation does not contain %s.' % name)\n\n        if len(args) == 1:\n            return result_list[0]\n        else:\n            return tuple(result_list)", "response": "Returns the annotations with name annotation_X."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset annotations and annotations_X for all the items in args and kwargs.", "response": "def f_set(self, *args, **kwargs):\n        \"\"\"Sets annotations\n\n        Items in args are added as `annotation` and `annotation_X` where\n        'X' is the position in args for following arguments.\n\n        \"\"\"\n        for idx, arg in enumerate(args):\n            valstr = self._translate_key(idx)\n            self.f_set_single(valstr, arg)\n\n        for key, arg in kwargs.items():\n            self.f_set_single(key, arg)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef f_remove(self, key):\n        key = self._translate_key(key)\n        try:\n            del self._dict[key]\n        except KeyError:\n            raise AttributeError('Your annotations do not contain %s' % key)", "response": "Removes the key from the annotations"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef f_ann_to_str(self):\n        resstr = ''\n        for key in sorted(self._dict.keys()):\n            resstr += '%s=%s; ' % (key, str(self._dict[key]))\n        return resstr[:-2]", "response": "Returns all annotations lexicographically sorted as a concatenated string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_ordinary_result(result, key, trajectory=None, reload=True):\n    shared_data = result.f_get(key)\n    if trajectory is not None:\n        shared_data.traj = trajectory\n    shared_data._request_data('make_ordinary')\n    result.f_remove(key)\n    if reload:\n        trajectory.f_load_item(result, load_data=pypetconstants.OVERWRITE_DATA)\n    return result", "response": "Turns a given shared data item into an ordinary one."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nturning an ordinary data item into a shared one.", "response": "def make_shared_result(result, key, trajectory, new_class=None):\n    \"\"\"Turns an ordinary data item into a shared one.\n\n    Removes the old result from the trajectory and replaces it.\n    Empties the given result.\n\n    :param result: The result containing ordinary data\n    :param key: Name of ordinary data item\n    :param trajectory: Trajectory container\n    :param new_class:\n\n        Class of new shared data item.\n        Leave `None` for automatic detection.\n\n    :return: The `result`\n\n    \"\"\"\n    data = result.f_get(key)\n    if new_class is None:\n        if isinstance(data, ObjectTable):\n            new_class = SharedTable\n        elif isinstance(data, pd.DataFrame):\n            new_class = SharedPandasFrame\n        elif isinstance(data, (tuple, list)):\n            new_class = SharedArray\n        elif isinstance(data, (np.ndarray, np.matrix)):\n            new_class = SharedCArray\n        else:\n            raise RuntimeError('Your data `%s` is not understood.' % key)\n    shared_data = new_class(result.f_translate_key(key), result, trajectory=trajectory)\n    result[key] = shared_data\n    shared_data._request_data('make_shared')\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_shared_data(self, **kwargs):\n        if 'flag' not in kwargs:\n            kwargs['flag'] = self.FLAG\n        if 'data' in kwargs:\n            kwargs['obj'] = kwargs.pop('data')\n        if 'trajectory' in kwargs:\n            self.traj = kwargs.pop('trajectory')\n        if 'traj' in kwargs:\n            self.traj = kwargs.pop('traj')\n        if 'name' in kwargs:\n            self.name = kwargs.pop['name']\n        if 'parent' in kwargs:\n            self.parent = kwargs.pop('parent')\n            if self.name is not None:\n                self.parent[self.name] = self\n        return self._request_data('create_shared_data', kwargs=kwargs)", "response": "Creates shared data on disk with a StorageService on disk."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _request_data(self, request, args=None, kwargs=None):\n        return self._storage_service.store(pypetconstants.ACCESS_DATA,\n                                           self.parent.v_full_name,\n                                           self.name,\n                                           request, args, kwargs,\n                                           trajectory_name=self.traj.v_name)", "response": "This method is used to store the data for the current node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the actula node of the underlying data.", "response": "def get_data_node(self):\n        \"\"\"Returns the actula node of the underlying data.\n\n        In case one uses HDF5 this will be the HDF5 leaf node.\n\n        \"\"\"\n        if not self._storage_service.is_open:\n            warnings.warn('You requesting the data item but your store is not open, '\n                          'the item itself will be closed, too!',\n                          category=RuntimeWarning)\n        return self._request_data('__thenode__')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _supports(self, item):\n        result = super(SharedResult, self)._supports(item)\n        result = result or type(item) in SharedResult.SUPPORTED_DATA\n        return result", "response": "Checks if outer data structure is supported."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncall the corresponding function of the shared data item", "response": "def create_shared_data(self, name=None, **kwargs):\n        \"\"\"Calls the corresponding function of the shared data item\"\"\"\n        if name is None:\n            item = self.f_get()\n        else:\n            item = self.f_get(name)\n        return item.create_shared_data(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntarget function that manipulates the trajectory and stores the manipulated data in the trajectory and the multiprocessing safe storage service.", "response": "def manipulate_multiproc_safe(traj):\n    \"\"\" Target function that manipulates the trajectory.\n\n    Stores the current name of the process into the trajectory and\n    **overwrites** previous settings.\n\n    :param traj:\n\n        Trajectory container with multiprocessing safe storage service\n\n    \"\"\"\n\n    # Manipulate the data in the trajectory\n    traj.last_process_name = mp.current_process().name\n    # Store the manipulated data\n    traj.results.f_store(store_data=3)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _unlock(self, name, client_id, request_id):\n        if name in self._locks:\n            other_client_id, other_request_id = self._locks[name]\n            if other_client_id != client_id:\n                response = (self.RELEASE_ERROR + self.DELIMITER +\n                            'Lock `%s` was acquired by `%s` (old request id `%s`) and not by '\n                            '`%s` (request id `%s`)' % (name,\n                                                        other_client_id,\n                                                        other_request_id,\n                                                        client_id,\n                                                        request_id))\n                self._logger.error(response)\n                return response\n            else:\n                del self._locks[name]\n                return self.RELEASED\n        else:\n            response = (self.RELEASE_ERROR + self.DELIMITER +\n                        'Lock `%s` cannot be found in database (client id `%s`, '\n                        'request id `%s`)' % (name, client_id, request_id))\n            self._logger.error(response)\n            return response", "response": "Handles unlocking a non - existent lock."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns the lock server.", "response": "def run(self):\n        \"\"\"Runs server\"\"\"\n        try:\n            self._start()\n\n            running = True\n            while running:\n                msg = ''\n                name = ''\n                client_id = ''\n                request_id = ''\n\n                request = self._socket.recv_string()\n                self._logger.log(1, 'Recevied REQ `%s`', request)\n\n                split_msg = request.split(self.DELIMITER)\n\n                if len(split_msg) == 4:\n                    msg, name, client_id, request_id = split_msg\n\n                if msg == self.LOCK:\n                    response = self._lock(name, client_id, request_id)\n\n                elif msg == self.UNLOCK:\n                    response = self._unlock(name, client_id, request_id)\n\n                elif msg == self.PING:\n                    response = self.PONG\n\n                elif msg == self.DONE:\n                    response = self.CLOSED\n                    running = False\n\n                else:\n                    response = (self.MSG_ERROR + self.DELIMITER +\n                                'Request `%s` not understood '\n                                '(or wrong number of delimiters)' % request)\n                    self._logger.error(response)\n\n                respond = self._pre_respond_hook(response)\n                if respond:\n                    self._logger.log(1, 'Sending REP `%s` to `%s` (request id `%s`)',\n                                     response, client_id, request_id)\n                    self._socket.send_string(response)\n\n            # Close everything in the end\n            self._close()\n        except Exception:\n            self._logger.exception('Crashed Lock Server!')\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhandling locking a resource.", "response": "def _lock(self, name, client_id, request_id):\n        \"\"\"Handles locking\n\n        Locking time is stored to determine time out.\n        If a lock is timed out it can be acquired by a different client.\n\n        \"\"\"\n        if name in self._locks:\n            other_client_id, other_request_id, lock_time = self._locks[name]\n            if other_client_id == client_id:\n                response = (self.LOCK_ERROR + self.DELIMITER +\n                            'Re-request of lock `%s` (old request id `%s`) by `%s` '\n                            '(request id `%s`)' % (name, client_id, other_request_id, request_id))\n                self._logger.warning(response)\n                return response\n            else:\n                current_time = time.time()\n                if current_time - lock_time < self._timeout:\n                    return self.WAIT\n                else:\n                    response = (self.GO + self.DELIMITER + 'Lock `%s` by `%s` (old request id `%s) '\n                                                          'timed out' % (name,\n                                                                         other_client_id,\n                                                                         other_request_id))\n                    self._logger.info(response)\n                    self._locks[name] = (client_id, request_id, time.time())\n                    self._timeout_locks[(name, other_client_id)] = (request_id, lock_time)\n                    return response\n        else:\n            self._locks[name] = (client_id, request_id, time.time())\n            return self.GO"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhandles unlocking of a lock.", "response": "def _unlock(self, name, client_id, request_id):\n        \"\"\"Handles unlocking\"\"\"\n        if name in self._locks:\n            other_client_id, other_request_id, lock_time = self._locks[name]\n            if other_client_id != client_id:\n                response = (self.RELEASE_ERROR + self.DELIMITER +\n                            'Lock `%s` was acquired by `%s` (old request id `%s`) and not by '\n                            '`%s` (request id `%s`)' % (name,\n                                                        other_client_id,\n                                                        other_request_id,\n                                                        client_id,\n                                                        request_id))\n                self._logger.error(response)\n                return response\n            else:\n                del self._locks[name]\n                return self.RELEASED\n        elif (name, client_id) in self._timeout_locks:\n            other_request_id, lock_time = self._timeout_locks[(name, client_id)]\n            timeout = time.time() - lock_time - self._timeout\n            response = (self.RELEASE_ERROR + self.DELIMITER +\n                        'Lock `%s` timed out %f seconds ago (client id `%s`, '\n                        'old request id `%s`)' % (name, timeout, client_id, other_request_id))\n            return response\n        else:\n            response = (self.RELEASE_ERROR + self.DELIMITER +\n                        'Lock `%s` cannot be found in database (client id `%s`, '\n                        'request id `%s`)' % (name, client_id, request_id))\n            self._logger.warning(response)\n            return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends a shutdown signal to the server", "response": "def send_done(self):\n        \"\"\"Notifies the Server to shutdown\"\"\"\n        self.start(test_connection=False)\n        self._logger.debug('Sending shutdown signal')\n        self._req_rep(ZMQServer.DONE)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef finalize(self):\n        if self._context is not None:\n            if self._socket is not None:\n                self._close_socket(confused=False)\n            self._context.term()\n            self._context = None\n            self._poll = None", "response": "Closes socket and terminates context\n            NO - OP"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstarting a new connection to the server if not existent.", "response": "def start(self, test_connection=True):\n        \"\"\"Starts connection to server if not existent.\n\n        NO-OP if connection is already established.\n        Makes ping-pong test as well if desired.\n\n        \"\"\"\n        if self._context is None:\n            self._logger.debug('Starting Client')\n            self._context = zmq.Context()\n            self._poll = zmq.Poller()\n            self._start_socket()\n            if test_connection:\n                self.test_ping()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretries a REQ request and return the response and the number of retries left.", "response": "def _req_rep_retry(self, request):\n        \"\"\"Returns response and number of retries\"\"\"\n        retries_left = self.RETRIES\n        while retries_left:\n            self._logger.log(1, 'Sending REQ `%s`', request)\n            self._send_request(request)\n            socks = dict(self._poll.poll(self.TIMEOUT))\n            if socks.get(self._socket) == zmq.POLLIN:\n                response = self._receive_response()\n                self._logger.log(1, 'Received REP `%s`', response)\n                return response, self.RETRIES - retries_left\n            else:\n                self._logger.debug('No response from server (%d retries left)' %\n                                   retries_left)\n                self._close_socket(confused=True)\n                retries_left -= 1\n                if retries_left == 0:\n                    raise RuntimeError('Server seems to be offline!')\n                time.sleep(self.SLEEP)\n                self._start_socket()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nacquires lock and returns True if it is available.", "response": "def acquire(self):\n        \"\"\"Acquires lock and returns `True`\n\n        Blocks until lock is available.\n\n        \"\"\"\n        self.start(test_connection=False)\n        while True:\n            str_response, retries = self._req_rep_retry(LockerServer.LOCK)\n            response = str_response.split(LockerServer.DELIMITER)\n            if response[0] == LockerServer.GO:\n                return True\n            elif response[0] == LockerServer.LOCK_ERROR and retries > 0:\n                # Message was sent but Server response was lost and we tried again\n                self._logger.error(str_response + '; Probably due to retry')\n                return True\n            elif response[0] == LockerServer.WAIT:\n                time.sleep(self.SLEEP)\n            else:\n                raise RuntimeError('Response `%s` not understood' % response)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef release(self):\n        # self.start(test_connection=False)\n        str_response, retries = self._req_rep_retry(LockerServer.UNLOCK)\n        response = str_response.split(LockerServer.DELIMITER)\n        if response[0] == LockerServer.RELEASED:\n            pass  # Everything is fine\n        elif response[0] == LockerServer.RELEASE_ERROR and retries > 0:\n            # Message was sent but Server response was lost and we tried again\n            self._logger.error(str_response + '; Probably due to retry')\n        else:\n            raise RuntimeError('Response `%s` not understood' % response)", "response": "Releases a lock from the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef listen(self):\n        count = 0\n        self._start()\n        while True:\n            result = self._socket.recv_pyobj()\n\n            if isinstance(result, tuple):\n                request, data = result\n            else:\n                request = result\n                data = None\n\n            if request == self.SPACE:\n                if self.queue.qsize() + count < self.queue_maxsize:\n                    self._socket.send_string(self.SPACE_AVAILABLE)\n                    count += 1\n                else:\n                    self._socket.send_string(self.SPACE_NOT_AVAILABLE)\n\n            elif request == self.PING:\n                self._socket.send_string(self.PONG)\n\n            elif request == self.DATA:\n                self._socket.send_string(self.STORING)\n                self.queue.put(data)\n                count -= 1\n\n            elif request == self.DONE:\n                self._socket.send_string(ZMQServer.CLOSED)\n                self.queue.put(('DONE', [], {}))\n                self._close()\n                break\n\n            else:\n                raise RuntimeError('I did not understand your request %s' % request)", "response": "Handles listening for new items from the client."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend data to the server", "response": "def put(self, data, block=True):\n        \"\"\" If there is space it sends data to server\n\n        If no space in the queue\n\n        It returns the request in every 10 millisecond\n\n        until there will be space in the queue.\n\n        \"\"\"\n\n        self.start(test_connection=False)\n        while True:\n            response = self._req_rep(QueuingServerMessageListener.SPACE)\n            if response == QueuingServerMessageListener.SPACE_AVAILABLE:\n                self._req_rep((QueuingServerMessageListener.DATA, data))\n                break\n            else:\n                time.sleep(0.01)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _detect_fork(self):\n        if self._pid is None:\n            self._pid = os.getpid()\n        if self._context is not None:\n            current_pid = os.getpid()\n            if current_pid != self._pid:\n                self._logger.debug('Fork detected: My pid `%s` != os pid `%s`. '\n                                   'Restarting connection.' % (str(self._pid), str(current_pid)))\n                self._context = None\n                self._pid = current_pid", "response": "Detects if the lock client was forked."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef start(self, test_connection=True):\n        self._detect_fork()\n        super(ForkAwareLockerClient, self).start(test_connection)", "response": "Starts the lock manager."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nputs data on the queue.", "response": "def _put_on_queue(self, to_put):\n        \"\"\"Puts data on queue\"\"\"\n        old = self.pickle_queue\n        self.pickle_queue = False\n        try:\n            self.queue.put(to_put, block=True)\n        finally:\n            self.pickle_queue = old"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nputs data on the pipe", "response": "def _put_on_pipe(self, to_put):\n        \"\"\"Puts data on queue\"\"\"\n        self.acquire_lock()\n        self._send_chunks(to_put)\n        self.release_lock()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _handle_data(self, msg, args, kwargs):\n        stop = False\n        try:\n            if msg == 'DONE':\n                stop = True\n            elif msg == 'STORE':\n                if 'msg' in kwargs:\n                    store_msg = kwargs.pop('msg')\n                else:\n                    store_msg = args[0]\n                    args = args[1:]\n                if 'stuff_to_store' in kwargs:\n                    stuff_to_store = kwargs.pop('stuff_to_store')\n                else:\n                    stuff_to_store = args[0]\n                    args = args[1:]\n                trajectory_name = kwargs['trajectory_name']\n                if self._trajectory_name != trajectory_name:\n                    if self._storage_service.is_open:\n                        self._close_file()\n                    self._trajectory_name = trajectory_name\n                    self._open_file()\n                self._storage_service.store(store_msg, stuff_to_store, *args, **kwargs)\n                self._storage_service.store(pypetconstants.FLUSH, None)\n                self._check_and_collect_garbage()\n            else:\n                raise RuntimeError('You queued something that was not '\n                                   'intended to be queued. I did not understand message '\n                                   '`%s`.' % msg)\n        except Exception:\n            self._logger.exception('ERROR occurred during storing!')\n            time.sleep(0.01)\n            pass  # We don't want to kill the queue process in case of an error\n\n        return stop", "response": "Handles data and returns True or False."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstarts listening to the queue.", "response": "def run(self):\n        \"\"\"Starts listening to the queue.\"\"\"\n        try:\n            while True:\n                msg, args, kwargs = self._receive_data()\n                stop = self._handle_data(msg, args, kwargs)\n                if stop:\n                    break\n        finally:\n            if self._storage_service.is_open:\n                self._close_file()\n            self._trajectory_name = ''"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _receive_data(self):\n        result = self.queue.get(block=True)\n        if hasattr(self.queue, 'task_done'):\n            self.queue.task_done()\n        return result", "response": "Gets data from queue and returns it"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _receive_data(self):\n        while True:\n            while len(self._buffer) < self.max_size and self.conn.poll():\n                data = self._read_chunks()\n                if data is not None:\n                    self._buffer.append(data)\n            if len(self._buffer) > 0:\n                return self._buffer.popleft()", "response": "Reads data from the pipe and returns it"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstoring the given data into the database.", "response": "def store(self, *args, **kwargs):\n        \"\"\"Acquires a lock before storage and releases it afterwards.\"\"\"\n        try:\n            self.acquire_lock()\n            return self._storage_service.store(*args, **kwargs)\n        finally:\n            if self.lock is not None:\n                try:\n                    self.release_lock()\n                except RuntimeError:\n                    self._logger.error('Could not release lock `%s`!' % str(self.lock))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstoring the data in the store.", "response": "def store(self, msg, stuff_to_store, *args, **kwargs):\n        \"\"\"Simply keeps a reference to the stored data \"\"\"\n        trajectory_name = kwargs['trajectory_name']\n        if trajectory_name not in self.references:\n            self.references[trajectory_name] = []\n        self.references[trajectory_name].append((msg, cp.copy(stuff_to_store), args, kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstoring the given references to disk and may collect garbage.", "response": "def store_references(self, references):\n        \"\"\"Stores references to disk and may collect garbage.\"\"\"\n        for trajectory_name in references:\n            self._storage_service.store(pypetconstants.LIST, references[trajectory_name], trajectory_name=trajectory_name)\n        self._check_and_collect_garbage()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncollects all settings within a section", "response": "def _collect_section(self, section):\n        \"\"\"Collects all settings within a section\"\"\"\n        kwargs = {}\n        try:\n            if self.parser.has_section(section):\n                options = self.parser.options(section)\n                for option in options:\n                    str_val = self.parser.get(section, option)\n                    val = ast.literal_eval(str_val)\n                    kwargs[option] = val\n            return kwargs\n        except:\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _collect_config(self):\n        kwargs = {}\n        sections = ('storage_service', 'trajectory', 'environment')\n        for section in sections:\n            kwargs.update(self._collect_section(section))\n        return kwargs", "response": "Collects all info from three sections"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef interpret(self):\n        if self.config_file:\n            new_kwargs = self._collect_config()\n            for key in new_kwargs:\n                # Already specified kwargs take precedence over the ini file\n                if key not in self.kwargs:\n                    self.kwargs[key] = new_kwargs[key]\n            if not use_simple_logging(self.kwargs) and 'log_config' not in self.kwargs:\n                self.kwargs['log_config'] = self.config_file\n        return self.kwargs", "response": "Copies parsed arguments into the kwargs passed to the environment"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_parameters(self, traj):\n        if self.config_file:\n            parameters = self._collect_section('parameters')\n            for name in parameters:\n                value = parameters[name]\n                if not isinstance(value, tuple):\n                    value = (value,)\n                traj.f_add_parameter(name, *value)\n            config = self._collect_section('config')\n            for name in config:\n                value = config[name]\n                if not isinstance(value, tuple):\n                    value = (value,)\n                traj.f_add_config(name, *value)", "response": "Adds parameters and config from the. ini file to the trajectory"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert_rule(rule_number):\n    binary_rule = [(rule_number // pow(2,i)) % 2 for i in range(8)]\n    return np.array(binary_rule)", "response": "Converts a rule number into a binary list representation."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_initial_state(name, ncells, seed=42):\n    if name == 'single':\n        just_one_cell = np.zeros(ncells)\n        just_one_cell[int(ncells/2)] = 1.0\n        return just_one_cell\n    elif name == 'random':\n        np.random.seed(seed)\n        random_init = np.random.randint(2, size=ncells)\n        return random_init\n    else:\n        raise ValueError('I cannot handel your initial state `%s`.' % name)", "response": "Creates an initial state for the automaton."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_pattern(pattern, rule_number, filename):\n    plt.figure()\n    plt.imshow(pattern)\n    plt.xlabel('Cell No.')\n    plt.ylabel('Time Step')\n    plt.title('CA with Rule %s' % str(rule_number))\n    plt.savefig(filename)\n    #plt.show()\n    plt.close()", "response": "Plots an automaton pattern and stores the image under a given filename."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cellular_automaton_1D(initial_state, rule_number, steps):\n\n    ncells = len(initial_state)\n    # Create an array for the full pattern\n    pattern = np.zeros((steps, ncells))\n\n    # Pass initial state:\n    pattern[0,:] = initial_state\n\n    # Get the binary rule list\n    binary_rule = convert_rule(rule_number)\n\n    # Conversion list to get the position in the binary rule list\n    neighbourhood_factors = np.array([1, 2, 4])\n\n    # Iterate over all steps to compute the CA\n    all_cells = range(ncells)\n    for step in range(steps-1):\n        current_row = pattern[step, :]\n        next_row = pattern[step+1, :]\n        for irun in all_cells:\n            # Get the neighbourhood\n            neighbour_indices = range(irun - 1, irun + 2)\n            neighbourhood = np.take(current_row, neighbour_indices, mode='wrap')\n            # Convert neighborhood to decimal\n            decimal_neighborhood = int(np.sum(neighbourhood * neighbourhood_factors))\n            # Get next state from rule book\n            next_state = binary_rule[decimal_neighborhood]\n            # Update next state of cell\n            next_row[irun] = next_state\n\n    return pattern", "response": "Simulates a 1 dimensional cellular automaton."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_all_slots(cls):\n    slots_iterator = (getattr(c, '__slots__', ()) for c in cls.__mro__)\n    # `__slots__` might only be a single string,\n    # so we need to put the strings into a tuple.\n    slots_converted = ((slots,) if isinstance(slots, str) else slots\n                                for slots in slots_iterator)\n    all_slots = set()\n    all_slots.update(*slots_converted)\n    return all_slots", "response": "Iterates through a class s mro to get all slots as a set."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsignal the process timer.", "response": "def signal_update(self):\n        \"\"\"Signals the process timer.\n\n        If more time than the display time has passed a message is emitted.\n\n        \"\"\"\n        if not self.active:\n            return\n\n        self._updates += 1\n        current_time = time.time()\n        dt = current_time - self._last_time\n        if dt > self._display_time:\n            dfullt = current_time - self._start_time\n            seconds = int(dfullt) % 60\n            minutes = int(dfullt) / 60\n            if minutes == 0:\n                formatted_time = '%ds' % seconds\n            else:\n                formatted_time = '%dm%02ds' % (minutes, seconds)\n            nodespersecond = self._updates / dfullt\n            message = 'Processed %d nodes in %s (%.2f nodes/s).' % \\\n                      (self._updates, formatted_time, nodespersecond)\n            self._logger.info(message)\n            self._last_time = current_time"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndirect link to the overview group", "response": "def _overview_group(self):\n        \"\"\"Direct link to the overview group\"\"\"\n        if self._overview_group_ is None:\n            self._overview_group_ = self._all_create_or_get_groups('overview')[0]\n        return self._overview_group_"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _all_get_filters(self, kwargs=None):\n        if kwargs is None:\n            kwargs = {}\n        complib = kwargs.pop('complib', None)\n        complevel = kwargs.pop('complevel', None)\n        shuffle = kwargs.pop('shuffle', None)\n        fletcher32 = kwargs.pop('fletcher32', None)\n        if complib is not None:\n            self._filters = None\n        else:\n            complib = self._complib\n        if complevel is not None:\n            self._filters = None\n        else:\n            complevel = self._complevel\n        if shuffle is not None:\n            self._filters = None\n        else:\n            shuffle = self._shuffle\n        if fletcher32 is not None:\n            self._filters = None\n        else:\n            fletcher32 = self._fletcher32\n\n        if self._filters is None:\n            # Recreate the filters if something was changed\n            self._filters = pt.Filters(complib=complib, complevel=complevel,\n                                       shuffle=shuffle, fletcher32=fletcher32)\n            self._hdf5file.filters = self._filters\n            self._hdf5store._filters = self._filters\n            self._hdf5store._complevel = complevel\n            self._hdf5store._complib = complib\n            self._hdf5store._fletcher32 = fletcher32\n\n        return self._filters", "response": "Makes a list of filters from kwargs and returns them"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _srvc_set_config(self, trajectory):\n        def _set_config(name, value, comment):\n            if not trajectory.f_contains('config.'+name, shortcuts=False):\n                trajectory.f_add_config(Parameter, name, value, comment=comment)\n\n        for attr_name in HDF5StorageService.NAME_TABLE_MAPPING:\n            table_name = HDF5StorageService.NAME_TABLE_MAPPING[attr_name]\n            value = getattr(self, attr_name)\n            _set_config('hdf5.overview.' + table_name,\n                         value,\n                         comment='Whether or not to have an overview '\n                                 'table with that name')\n\n        _set_config('hdf5.purge_duplicate_comments',\n                    self._purge_duplicate_comments,\n                    comment='Whether comments of results and'\n                            ' derived parameters should only'\n                            ' be stored for the very first instance.'\n                            ' Works only if the summary tables are'\n                            ' active.')\n\n        _set_config('hdf5.results_per_run', self._results_per_run,\n                    comment='Expected number of results per run,'\n                            ' a good guess can increase storage performance')\n\n        _set_config('hdf5.derived_parameters_per_run',\n                    self._derived_parameters_per_run,\n                    comment='Expected number of derived parameters per run,'\n                            ' a good guess can increase storage performance')\n\n        _set_config('hdf5.complevel', self._complevel,\n                    comment='Compression Level (0 no compression '\n                            'to 9 highest compression)')\n\n        _set_config('hdf5.complib', self._complib,\n                    comment='Compression Algorithm')\n\n        _set_config('hdf5.encoding', self._encoding,\n                    comment='Encoding for unicode characters')\n\n        _set_config('hdf5.fletcher32', self._fletcher32,\n                    comment='Whether to use fletcher 32 checksum')\n\n        _set_config('hdf5.shuffle', self._shuffle,\n                    comment='Whether to use shuffle filtering.')\n\n        _set_config('hdf5.pandas_format', self._pandas_format,\n                    comment='''How to store pandas data frames, either'''\n                            ''' 'fixed' ('f') or 'table' ('t').''')\n\n        if trajectory.f_contains('config.hdf5', shortcuts=False):\n            if trajectory.config.hdf5.v_comment == '':\n                # If this has not happened yet, add a description of the hdf5 config group\n                trajectory.config.hdf5.v_comment = 'Settings for the standard HDF5 storage service'\n\n        trajectory.v_storage_service = self", "response": "Sets a config value to the trajectory or changes it if the trajectory was loaded."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load(self, msg, stuff_to_load, *args, **kwargs):\n        opened = True\n        try:\n\n            opened = self._srvc_opening_routine('r', kwargs=kwargs)\n\n            if msg == pypetconstants.TRAJECTORY:\n                self._trj_load_trajectory(stuff_to_load, *args, **kwargs)\n\n            elif msg == pypetconstants.LEAF:\n                self._prm_load_parameter_or_result(stuff_to_load, *args, **kwargs)\n\n            elif msg == pypetconstants.GROUP:\n                self._grp_load_group(stuff_to_load, *args, **kwargs)\n\n            elif msg == pypetconstants.TREE:\n                self._tree_load_sub_branch(stuff_to_load, *args, **kwargs)\n\n            elif msg == pypetconstants.LIST:\n                self._srvc_load_several_items(stuff_to_load, *args, **kwargs)\n\n            else:\n                raise pex.NoSuchServiceError('I do not know how to handle `%s`' % msg)\n\n        except pt.NoSuchNodeError as exc:\n            self._logger.error('Failed loading  `%s`' % str(stuff_to_load))\n            raise pex.DataNotInStorageError(repr(exc))\n        except:\n            self._logger.error('Failed loading  `%s`' % str(stuff_to_load))\n            raise\n        finally:\n            self._srvc_closing_routine(opened)", "response": "Loads a particular item from disk."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstores a particular item in the storage service.", "response": "def store(self, msg, stuff_to_store, *args, **kwargs):\n        \"\"\" Stores a particular item to disk.\n\n        The storage service always accepts these parameters:\n\n        :param trajectory_name: Name or current trajectory and name of top node in hdf5 file\n\n        :param filename: Name of the hdf5 file\n\n        :param file_title: If file needs to be created, assigns a title to the file.\n\n\n        The following messages (first argument msg) are understood and the following arguments\n        can be provided in combination with the message:\n\n            * :const:`pypet.pypetconstants.PREPARE_MERGE` ('PREPARE_MERGE'):\n\n                Called to prepare a trajectory for merging, see also 'MERGE' below.\n\n                Will also be called if merging cannot happen within the same hdf5 file.\n                Stores already enlarged parameters and updates meta information.\n\n                :param stuff_to_store: Trajectory that is about to be extended by another one\n\n                :param changed_parameters:\n\n                    List containing all parameters that were enlarged due to merging\n\n                :param old_length:\n\n                    Old length of trajectory before merge\n\n            * :const:`pypet.pypetconstants.MERGE` ('MERGE')\n\n                Note that before merging within HDF5 file, the storage service will be called\n                with msg='PREPARE_MERGE' before, see above.\n\n                Raises a ValueError if the two trajectories are not stored within the very\n                same hdf5 file. Then the current trajectory needs to perform the merge slowly\n                item by item.\n\n                Merges two trajectories, parameters are:\n\n                :param stuff_to_store: The trajectory data is merged into\n\n                :param other_trajectory_name: Name of the other trajectory\n\n                :param rename_dict:\n\n                    Dictionary containing the old result and derived parameter names in the\n                    other trajectory and their new names in the current trajectory.\n\n                :param move_nodes:\n\n                    Whether to move the nodes from the other to the current trajectory\n\n                :param delete_trajectory:\n\n                    Whether to delete the other trajectory after merging.\n\n            * :const:`pypet.pypetconstants.BACKUP` ('BACKUP')\n\n                :param stuff_to_store: Trajectory to be backed up\n\n                :param backup_filename:\n\n                    Name of file where to store the backup. If None the backup file will be in\n                    the same folder as your hdf5 file and named 'backup_XXXXX.hdf5'\n                    where 'XXXXX' is the name of your current trajectory.\n\n            * :const:`pypet.pypetconstants.TRAJECTORY` ('TRAJECTORY')\n\n                Stores the whole trajectory\n\n                :param stuff_to_store: The trajectory to be stored\n\n                :param only_init:\n\n                    If you just want to initialise the store. If yes, only meta information about\n                    the trajectory is stored and none of the nodes/leaves within the trajectory.\n\n                :param store_data:\n\n                    How to store data, the following settings are understood:\n\n                     :const:`pypet.pypetconstants.STORE_NOTHING`: (0)\n\n                        Nothing is stored\n\n                    :const:`pypet.pypetconstants.STORE_DATA_SKIPPING`: (1)\n\n                        Data of not already stored nodes is stored\n\n                    :const:`pypet.pypetconstants.STORE_DATA`: (2)\n\n                        Data of all nodes is stored. However, existing data on disk is left\n                        untouched.\n\n                    :const:`pypet.pypetconstants.OVERWRITE_DATA`: (3)\n\n                        Data of all nodes is stored and data on disk is overwritten.\n                        May lead to fragmentation of the HDF5 file. The user is adviced\n                        to recompress the file manually later on.\n\n            * :const:`pypet.pypetconstants.SINGLE_RUN` ('SINGLE_RUN')\n\n                :param stuff_to_store: The trajectory\n\n                :param store_data: How to store data see above\n\n                :param store_final: If final meta info should be stored\n\n            * :const:`pypet.pypetconstants.LEAF`\n\n                Stores a parameter or result\n\n                Note that everything that is supported by the storage service and that is\n                stored to disk will be perfectly recovered.\n                For instance, you store a tuple of numpy 32 bit integers, you will get a tuple\n                of numpy 32 bit integers after loading independent of the platform!\n\n                :param stuff_to_sore: Result or parameter to store\n\n                    In order to determine what to store, the function '_store' of the parameter or\n                    result is called. This function returns a dictionary with name keys and data to\n                    store as values. In order to determine how to store the data, the storage flags\n                    are considered, see below.\n\n                    The function '_store' has to return a dictionary containing values only from\n                    the following objects:\n\n                        * python natives (int, long, str, bool, float, complex),\n\n                        *\n                            numpy natives, arrays and matrices of type np.int8-64, np.uint8-64,\n                            np.float32-64, np.complex, np.str\n\n                        *\n\n                            python lists and tuples of the previous types\n                            (python natives + numpy natives and arrays)\n                            Lists and tuples are not allowed to be nested and must be\n                            homogeneous, i.e. only contain data of one particular type.\n                            Only integers, or only floats, etc.\n\n                        *\n\n                            python dictionaries of the previous types (not nested!), data can be\n                            heterogeneous, keys must be strings. For example, one key-value-pair\n                            of string and int and one key-value pair of string and float, and so\n                            on.\n\n                        * pandas DataFrames_\n\n                        * :class:`~pypet.parameter.ObjectTable`\n\n                    .. _DataFrames: http://pandas.pydata.org/pandas-docs/dev/dsintro.html#dataframe\n\n                    The keys from the '_store' dictionaries determine how the data will be named\n                    in the hdf5 file.\n\n                :param store_data:\n\n                    How to store the data, see above for a descitpion.\n\n                :param store_flags: Flags describing how to store data.\n\n                        :const:`~pypet.HDF5StorageService.ARRAY` ('ARRAY')\n\n                            Store stuff as array\n\n                        :const:`~pypet.HDF5StorageService.CARRAY` ('CARRAY')\n\n                            Store stuff as carray\n\n                        :const:`~pypet.HDF5StorageService.TABLE` ('TABLE')\n\n                            Store stuff as pytable\n\n                        :const:`~pypet.HDF5StorageService.DICT` ('DICT')\n\n                            Store stuff as pytable but reconstructs it later as dictionary\n                            on loading\n\n                        :const:`~pypet.HDF%StorageService.FRAME` ('FRAME')\n\n                            Store stuff as pandas data frame\n\n                    Storage flags can also be provided by the parameters and results themselves\n                    if they implement a function '_store_flags' that returns a dictionary\n                    with the names of the data to store as keys and the flags as values.\n\n                    If no storage flags are provided, they are automatically inferred from the\n                    data. See :const:`pypet.HDF5StorageService.TYPE_FLAG_MAPPING` for the mapping\n                    from type to flag.\n\n                :param overwrite:\n\n                    Can be used if parts of a leaf should be replaced. Either a list of\n                    HDF5 names or `True` if this should account for all.\n\n            * :const:`pypet.pypetconstants.DELETE` ('DELETE')\n\n                Removes an item from disk. Empty group nodes, results and non-explored\n                parameters can be removed.\n\n                :param stuff_to_store: The item to be removed.\n\n                :param delete_only:\n\n                    Potential list of parts of a leaf node that should be deleted.\n\n                :param remove_from_item:\n\n                    If `delete_only` is used, whether deleted nodes should also be erased\n                    from the leaf nodes themseleves.\n\n                :param recursive:\n\n                    If you want to delete a group node you can recursively delete all its\n                    children.\n\n            * :const:`pypet.pypetconstants.GROUP` ('GROUP')\n\n                :param stuff_to_store: The group to store\n\n                :param store_data: How to store data\n\n                :param recursive: To recursively load everything below.\n\n                :param max_depth:\n\n                    Maximum depth in case of recursion. `None` for no limit.\n\n            * :const:`pypet.pypetconstants.TREE`\n\n                Stores a single node or a full subtree\n\n                :param stuff_to_store: Node to store\n\n                :param store_data: How to store data\n\n                :param recursive: Whether to store recursively the whole sub-tree\n\n                :param max_depth:\n\n                    Maximum depth in case of recursion. `None` for no limit.\n\n            * :const:`pypet.pypetconstants.DELETE_LINK`\n\n                Deletes a link from hard drive\n\n                :param name: The full colon separated name of the link\n\n            * :const:`pypet.pypetconstants.LIST`\n\n                .. _store-lists:\n\n                Stores several items at once\n\n                :param stuff_to_store:\n\n                    Iterable whose items are to be stored. Iterable must contain tuples,\n                    for example `[(msg1,item1,arg1,kwargs1),(msg2,item2,arg2,kwargs2),...]`\n\n            * :const:`pypet.pypetconstants.ACCESS_DATA`\n\n                Requests and manipulates data within the storage.\n                Storage must be open.\n\n                :param stuff_to_store:\n\n                    A colon separated name to the data path\n\n                :param item_name:\n\n                    The name of the data item to interact with\n\n                :param request:\n\n                    A functional request in form of a string\n\n                :param args:\n\n                    Positional arguments passed to the reques\n\n                :param kwargs:\n\n                    Keyword arguments passed to the request\n\n            * :const:`pypet.pypetconstants.OPEN_FILE`\n\n                Opens the HDF5 file and keeps it open\n\n                :param stuff_to_store: ``None``\n\n            * :const:`pypet.pypetconstants.CLOSE_FILE`\n\n                Closes an HDF5 file that was kept open, must be open before.\n\n                :param stuff_to_store: ``None``\n\n            * :const:`pypet.pypetconstants.FLUSH`\n\n                Flushes an open file, must be open before.\n\n                :param stuff_to_store: ``None``\n\n        :raises: NoSuchServiceError if message or data is not understood\n\n        \"\"\"\n        opened = True\n        try:\n\n            opened = self._srvc_opening_routine('a', msg, kwargs)\n\n            if msg == pypetconstants.MERGE:\n                self._trj_merge_trajectories(*args, **kwargs)\n\n            elif msg == pypetconstants.BACKUP:\n                self._trj_backup_trajectory(stuff_to_store, *args, **kwargs)\n\n            elif msg == pypetconstants.PREPARE_MERGE:\n                self._trj_prepare_merge(stuff_to_store, *args, **kwargs)\n\n            elif msg == pypetconstants.TRAJECTORY:\n                self._trj_store_trajectory(stuff_to_store, *args, **kwargs)\n\n            elif msg == pypetconstants.SINGLE_RUN:\n                self._srn_store_single_run(stuff_to_store, *args, **kwargs)\n\n            elif msg in pypetconstants.LEAF:\n                self._prm_store_parameter_or_result(stuff_to_store, *args, **kwargs)\n\n            elif msg == pypetconstants.DELETE:\n                self._all_delete_parameter_or_result_or_group(stuff_to_store, *args, **kwargs)\n\n            elif msg == pypetconstants.GROUP:\n                self._grp_store_group(stuff_to_store, *args, **kwargs)\n\n            elif msg == pypetconstants.TREE:\n                self._tree_store_sub_branch(stuff_to_store, *args, **kwargs)\n\n            elif msg == pypetconstants.DELETE_LINK:\n                self._lnk_delete_link(stuff_to_store, *args, **kwargs)\n\n            elif msg == pypetconstants.LIST:\n                self._srvc_store_several_items(stuff_to_store, *args, **kwargs)\n\n            elif msg == pypetconstants.ACCESS_DATA:\n                return self._hdf5_interact_with_data(stuff_to_store, *args, **kwargs)\n\n            elif msg == pypetconstants.OPEN_FILE:\n                opened = False  # Wee need to keep the file open to allow later interaction\n                self._keep_open = True\n                self._node_processing_timer.active = False # This might be open quite long\n                # so we don't want to display horribly long opening times\n\n            elif msg == pypetconstants.CLOSE_FILE:\n                opened = True  # Simply conduct the closing routine afterwards\n                self._keep_open = False\n\n            elif msg == pypetconstants.FLUSH:\n                self._hdf5file.flush()\n\n            else:\n                raise pex.NoSuchServiceError('I do not know how to handle `%s`' % msg)\n\n        except:\n            self._logger.error('Failed storing `%s`' % str(stuff_to_store))\n            raise\n        finally:\n            self._srvc_closing_routine(opened)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _srvc_load_several_items(self, iterable, *args, **kwargs):\n        for input_tuple in iterable:\n            msg = input_tuple[0]\n            item = input_tuple[1]\n            if len(input_tuple) > 2:\n                args = input_tuple[2]\n            if len(input_tuple) > 3:\n                kwargs = input_tuple[3]\n            if len(input_tuple) > 4:\n                raise RuntimeError('You shall not pass!')\n\n            self.load(msg, item, *args, **kwargs)", "response": "Loads several items from an iterable"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread out the properties for storing new data into the hdf5file", "response": "def _srvc_check_hdf_properties(self, traj):\n        \"\"\"Reads out the properties for storing new data into the hdf5file\n\n        :param traj:\n\n            The trajectory\n\n        \"\"\"\n\n        for attr_name in HDF5StorageService.ATTR_LIST:\n            try:\n                config = traj.f_get('config.hdf5.' + attr_name).f_get()\n                setattr(self, attr_name, config)\n            except AttributeError:\n                self._logger.debug('Could not find `%s` in traj config, '\n                                     'using (default) value `%s`.' %\n                                     (attr_name, str(getattr(self, attr_name))))\n\n        for attr_name, table_name in HDF5StorageService.NAME_TABLE_MAPPING.items():\n            try:\n                if table_name in ('parameters', 'config'):\n                    table_name += '_overview'\n                config = traj.f_get('config.hdf5.overview.' + table_name).f_get()\n                setattr(self, attr_name, config)\n            except AttributeError:\n                self._logger.debug('Could not find `%s` in traj config, '\n                                     'using (default) value `%s`.' %\n                                     (table_name, str(getattr(self, attr_name))))\n\n        for attr_name, name in HDF5StorageService.PR_ATTR_NAME_MAPPING.items():\n            try:\n                config = traj.f_get('config.hdf5.' + name).f_get()\n                setattr(self, attr_name, config)\n            except AttributeError:\n                self._logger.debug('Could not find `%s` in traj config, '\n                                     'using (default) value `%s`.' %\n                                     (name, str(getattr(self, attr_name))))\n\n        if ((not self._overview_results_summary or\n                    not self._overview_derived_parameters_summary) and\n                    self._purge_duplicate_comments):\n            raise RuntimeError('You chose to purge duplicate comments but disabled a summary '\n                                   'table. You can only use the purging if you enable '\n                                   'the summary tables.')\n\n        self._filters = None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _srvc_store_several_items(self, iterable, *args, **kwargs):\n        for input_tuple in iterable:\n            msg = input_tuple[0]\n            item = input_tuple[1]\n            if len(input_tuple) > 2:\n                args = input_tuple[2]\n            if len(input_tuple) > 3:\n                kwargs = input_tuple[3]\n            if len(input_tuple) > 4:\n                raise RuntimeError('You shall not pass!')\n\n            self.store(msg, item, *args, **kwargs)", "response": "Stores several items from an iterable into the SRVC store"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nopens an hdf5 file for reading or writing.", "response": "def _srvc_opening_routine(self, mode, msg=None, kwargs=()):\n        \"\"\"Opens an hdf5 file for reading or writing\n\n        The file is only opened if it has not been opened before (i.e. `self._hdf5file is None`).\n\n        :param mode:\n\n            'a' for appending\n\n            'r' for reading\n\n                Unfortunately, pandas currently does not work with read-only mode.\n                Thus, if mode is chosen to be 'r', the file will still be opened in\n                append mode.\n\n        :param msg:\n\n            Message provided to `load` or `store`. Only considered to check if a trajectory\n            was stored before.\n\n        :param kwargs:\n\n            Arguments to extract file information from\n\n        :return:\n\n            `True` if file is opened\n\n            `False` if the file was already open before calling this function\n\n        \"\"\"\n        self._mode = mode\n        self._srvc_extract_file_information(kwargs)\n\n        if not self.is_open:\n\n            if 'a' in mode:\n                (path, filename) = os.path.split(self._filename)\n                racedirs(os.path.abspath(path))\n\n                self._hdf5store = HDFStore(self._filename, mode=self._mode, complib=self._complib,\n                                           complevel=self._complevel, fletcher32=self._fletcher32)\n                self._hdf5file = self._hdf5store._handle\n                self._hdf5file.title = self._file_title\n\n                if self._trajectory_name is not None:\n                    if not '/' + self._trajectory_name in self._hdf5file:\n                        # If we want to store individual items we we have to check if the\n                        # trajectory has been stored before\n                        if not msg == pypetconstants.TRAJECTORY:\n                            raise ValueError('Your trajectory cannot be found in the hdf5file, '\n                                             'please use >>traj.f_store()<< '\n                                             'before storing anything else.')\n\n                    else:\n                        # Keep a reference to the top trajectory node\n                        self._trajectory_group = self._hdf5file.get_node('/' +\n                                                                         self._trajectory_name)\n                else:\n                    raise ValueError('I don`t know which trajectory to load')\n                self._logger.debug('Opening HDF5 file `%s` in mode `a` with trajectory `%s`' %\n                                   (self._filename, self._trajectory_name))\n\n            elif mode == 'r':\n\n                if self._trajectory_name is not None and self._trajectory_index is not None:\n                    raise ValueError('Please specify either a name of a trajectory or an index, '\n                                     'but not both at the same time.')\n\n                if not os.path.isfile(self._filename):\n                    raise ValueError('File `' + self._filename + '` does not exist.')\n\n                self._hdf5store = HDFStore(self._filename, mode=self._mode, complib=self._complib,\n                                           complevel=self._complevel, fletcher32=self._fletcher32)\n                self._hdf5file = self._hdf5store._handle\n\n                if self._trajectory_index is not None:\n                    # If an index is provided pick the trajectory at the corresponding\n                    # position in the trajectory node list\n                    nodelist = self._hdf5file.list_nodes(where='/')\n\n                    if (self._trajectory_index >= len(nodelist) or\n                            self._trajectory_index < -len(nodelist)):\n                        raise ValueError('Trajectory No. %d does not exists, there are only '\n                                         '%d trajectories in %s.'\n                                         % (self._trajectory_index, len(nodelist), self._filename))\n\n                    self._trajectory_group = nodelist[self._trajectory_index]\n                    self._trajectory_name = self._trajectory_group._v_name\n\n                elif self._trajectory_name is not None:\n                    # Otherwise pick the trajectory group by name\n                    if not '/' + self._trajectory_name in self._hdf5file:\n                        raise ValueError('File %s does not contain trajectory %s.'\n                                         % (self._filename, self._trajectory_name))\n\n                    self._trajectory_group = self._hdf5file.get_node('/' + self._trajectory_name)\n                else:\n                    raise ValueError('Please specify a name of a trajectory to load or its '\n                                     'index, otherwise I cannot open one.')\n\n                self._logger.debug('Opening HDF5 file `%s` in mode `r` with trajectory `%s`' %\n                                   (self._filename, self._trajectory_name))\n\n            else:\n                raise RuntimeError('You shall not pass!')\n\n            self._node_processing_timer = NodeProcessingTimer(display_time=self._display_time,\n                                                              logger_name=self._logger.name)\n            self._overview_group_ = None\n\n            return True\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _srvc_extract_file_information(self, kwargs):\n        if 'filename' in kwargs:\n            self._filename = kwargs.pop('filename')\n\n        if 'file_title' in kwargs:\n            self._file_title = kwargs.pop('file_title')\n\n        if 'trajectory_name' in kwargs:\n            self._trajectory_name = kwargs.pop('trajectory_name')\n\n        if 'trajectory_index' in kwargs:\n            self._trajectory_index = kwargs.pop('trajectory_index')", "response": "Extracts file information from kwargs."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbacks up a trajectory.", "response": "def _trj_backup_trajectory(self, traj, backup_filename=None):\n        \"\"\"Backs up a trajectory.\n\n        :param traj: Trajectory that should be backed up\n\n        :param backup_filename:\n\n            Path and filename of backup file. If None is specified the storage service\n            defaults to `path_to_trajectory_hdf5_file/backup_trajectory_name.hdf`.\n\n        \"\"\"\n        self._logger.info('Storing backup of %s.' % traj.v_name)\n\n        mypath, _ = os.path.split(self._filename)\n\n        if backup_filename is None:\n            backup_filename = os.path.join('%s' % mypath, 'backup_%s.hdf5' % traj.v_name)\n\n        backup_hdf5file = pt.open_file(filename=backup_filename,\n                                             mode='a', title=backup_filename)\n\n        if '/' + self._trajectory_name in backup_hdf5file:\n            raise ValueError('I cannot backup  `%s` into file `%s`, there is already a '\n                             'trajectory with that name.' % (traj.v_name, backup_filename))\n\n        backup_root = backup_hdf5file.root\n\n        self._trajectory_group._f_copy(newparent=backup_root, recursive=True)\n\n        backup_hdf5file.flush()\n        backup_hdf5file.close()\n\n        self._logger.info('Finished backup of %s.' % traj.v_name)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _trj_read_out_row(colnames, row):\n        result_dict = {}\n        for colname in colnames:\n            result_dict[colname] = row[colname]\n\n        return result_dict", "response": "Reads out a row and returns a dictionary containing the row content."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmerging two trajectories into the current trajectory.", "response": "def _trj_merge_trajectories(self, other_trajectory_name, rename_dict, move_nodes=False,\n                                delete_trajectory=False, other_filename=None):\n        \"\"\"Merges another trajectory into the current trajectory (as in self._trajectory_name).\n\n        :param other_trajectory_name: Name of other trajectory\n        :param rename_dict: Dictionary with old names (keys) and new names (values).\n        :param move_nodes: Whether to move hdf5 nodes or copy them\n        :param delete_trajectory: Whether to delete the other trajectory\n\n        \"\"\"\n        if other_filename is None or other_filename == self.filename:\n            other_filename = self.filename\n            other_file = self._hdf5file\n            other_is_different = False\n        else:\n            other_file = pt.open_file(filename=other_filename, mode='r+')\n            other_is_different = True\n\n        try:\n            if not '/' + other_trajectory_name in other_file:\n                raise ValueError('Cannot merge `%s` and `%s`, because the second trajectory cannot '\n                                 'be found in file: %s.' % (self._trajectory_name,\n                                                            other_trajectory_name,\n                                                            other_filename))\n            for old_name in rename_dict:\n                new_name = rename_dict[old_name]\n\n                # Iterate over all items that need to be merged\n                split_name = old_name.split('.')\n                old_location = '/' + other_trajectory_name + '/' + '/'.join(split_name)\n\n                split_name = new_name.split('.')\n                new_parent_location = '/' + self._trajectory_name + '/' + '/'.join(split_name[:-1])\n\n                new_short_name = split_name[-1]\n\n                # Get the data from the other trajectory\n                old_node = other_file.get_node(old_location)\n\n                # Now move or copy the data\n                if move_nodes:\n                    self._hdf5file.move_node( where=old_node, newparent=new_parent_location,\n                                       newname=new_short_name, createparents=True)\n                else:\n                    if other_is_different:\n                        new_parent_dot_location = '.'.join(split_name[:-1])\n                        new_parent_or_loc, _ = self._all_create_or_get_groups(\n                            new_parent_dot_location)\n                        create_parents = False\n                    else:\n                        new_parent_or_loc = new_parent_location\n                        create_parents = True\n                    self._hdf5file.copy_node(where=old_node, newparent=new_parent_or_loc,\n                                       newname=new_short_name, createparents=create_parents,\n                                       recursive=True)\n\n\n            if delete_trajectory:\n                other_file.remove_node(where='/', name=other_trajectory_name, recursive=True)\n        finally:\n            if other_is_different:\n                other_file.flush()\n                other_file.close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprepare a trajectory for merging.", "response": "def _trj_prepare_merge(self, traj, changed_parameters, old_length):\n        \"\"\"Prepares a trajectory for merging.\n\n        This function will already store extended parameters.\n\n        :param traj: Target of merge\n        :param changed_parameters: List of extended parameters (i.e. their names).\n\n        \"\"\"\n\n        if not traj._stored:\n            traj.f_store()\n\n        # Update meta information\n        infotable = getattr(self._overview_group, 'info')\n        insert_dict = self._all_extract_insert_dict(traj, infotable.colnames)\n        self._all_add_or_modify_row(traj.v_name, insert_dict, infotable, index=0,\n                                    flags=(HDF5StorageService.MODIFY_ROW,))\n\n        # Store extended parameters\n        for param_name in changed_parameters:\n            param = traj.f_get(param_name)\n\n            try:\n                self._all_delete_parameter_or_result_or_group(param)\n            except pt.NoSuchNodeError:\n                pass  # We are fine and the node did not exist in the first place\n\n        # Increase the run table by the number of new runs\n        run_table = getattr(self._overview_group, 'runs')\n        actual_rows = run_table.nrows\n        self._trj_fill_run_table(traj, actual_rows, len(traj))\n\n        # Extract parameter summary and if necessary create new explored parameter tables\n        # in the result groups\n        for idx in range(old_length, len(traj)):\n            run_name = traj.f_idx_to_run(idx)\n            run_info = traj.f_get_run_information(run_name)\n            run_info['name'] = run_name\n\n            traj._set_explored_parameters_to_idx(idx)\n\n            run_summary = self._srn_summarize_explored_parameters(list(\n                                                            traj._explored_parameters.values()))\n\n            run_info['parameter_summary'] = run_summary\n\n            self._all_add_or_modify_row(run_name, run_info, run_table, index=idx,\n                                        flags=(HDF5StorageService.MODIFY_ROW,))\n\n        traj.f_restore_default()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading a single trajectory from a given file.", "response": "def _trj_load_trajectory(self, traj, as_new, load_parameters, load_derived_parameters,\n                             load_results, load_other_data, recursive, max_depth,\n                             with_run_information, with_meta_data, force):\n        \"\"\"Loads a single trajectory from a given file.\n\n\n        :param traj: The trajectory\n\n        :param as_new: Whether to load trajectory as new\n\n        :param load_parameters: How to load parameters and config\n\n        :param load_derived_parameters: How to load derived parameters\n\n        :param load_results: How to load results\n\n        :param load_other_data: How to load anything not within the four subbranches\n\n        :param recursive: If data should be loaded recursively\n\n        :param max_depth: Maximum depth of loading\n\n        :param with_run_information:\n\n            If run information should be loaded\n\n        :param with_meta_data:\n\n            If meta data infor should be loaded\n\n        :param force: Force load in case there is a pypet version mismatch\n\n        You can specify how to load the parameters, derived parameters and results\n        as follows:\n\n        :const:`pypet.pypetconstants.LOAD_NOTHING`: (0)\n\n            Nothing is loaded\n\n        :const:`pypet.pypetconstants.LOAD_SKELETON`: (1)\n\n            The skeleton including annotations are loaded, i.e. the items are empty.\n            Non-empty items in RAM are left untouched.\n\n        :const:`pypet.pypetconstants.LOAD_DATA`: (2)\n\n            The whole data is loaded.\n            Only empty or in RAM non-existing instance are filled with the\n            data found on disk.\n\n        :const:`pypet.pypetconstants.OVERWRITE_DATA`: (3)\n\n            The whole data is loaded.\n            If items that are to be loaded are already in RAM and not empty,\n            they are emptied and new data is loaded from disk.\n\n\n        If `as_new=True` the old trajectory is loaded into the new one, only parameters can be\n        loaded. If `as_new=False` the current trajectory is completely replaced by the one\n        on disk, i.e. the name from disk, the timestamp, etc. are assigned to `traj`.\n\n        \"\"\"\n        # Some validity checks, if `as_new` is used correctly\n        if (as_new and (load_derived_parameters != pypetconstants.LOAD_NOTHING or\n                                load_results != pypetconstants.LOAD_NOTHING or\n                                load_other_data != pypetconstants.LOAD_NOTHING)):\n            raise ValueError('You cannot load a trajectory as new and load the derived '\n                             'parameters and results. Only parameters are allowed.')\n\n        if as_new and load_parameters != pypetconstants.LOAD_DATA:\n            raise ValueError('You cannot load the trajectory as new and not load the data of '\n                             'the parameters.')\n\n        loadconstants = (pypetconstants.LOAD_NOTHING, pypetconstants.LOAD_SKELETON,\n                         pypetconstants.LOAD_DATA, pypetconstants.OVERWRITE_DATA)\n\n        if not (load_parameters in loadconstants and load_derived_parameters in loadconstants and\n                        load_results in loadconstants and load_other_data in loadconstants):\n            raise ValueError('Please give a valid option on how to load data. Options for '\n                             '`load_parameter`, `load_derived_parameters`, `load_results`, '\n                             'and `load_other_data` are %s. See function documentation for '\n                             'the semantics of the values.' % str(loadconstants))\n\n        traj._stored = not as_new\n\n        # Loads meta data like the name, timestamps etc.\n        # load_data is only used here to determine how to load the annotations\n        load_data = max(load_parameters, load_derived_parameters, load_results, load_other_data)\n        if with_meta_data:\n            self._trj_load_meta_data(traj, load_data, as_new, with_run_information, force)\n\n        if (load_parameters != pypetconstants.LOAD_NOTHING or\n                load_derived_parameters != pypetconstants.LOAD_NOTHING or\n                load_results != pypetconstants.LOAD_NOTHING or\n                load_other_data != pypetconstants.LOAD_NOTHING):\n            self._logger.info('Loading trajectory `%s`.' % traj.v_name)\n        else:\n            self._logger.info('Checked meta data of trajectory `%s`.' % traj.v_name)\n            return\n\n        maximum_display_other = 10\n        counter = 0\n\n        for children in [self._trajectory_group._v_groups, self._trajectory_group._v_links]:\n            for hdf5_group_name in children:\n                hdf5_group = children[hdf5_group_name]\n                child_name = hdf5_group._v_name\n\n                load_subbranch = True\n                if child_name == 'config':\n                    if as_new:\n                        loading = pypetconstants.LOAD_NOTHING\n                    else:\n                        # If the trajectory is loaded as new, we don't care about old config stuff\n                        # and only load the parameters\n                        loading = load_parameters\n                elif child_name == 'parameters':\n                    loading = load_parameters\n                elif child_name == 'results':\n                    loading = load_results\n                elif child_name == 'derived_parameters':\n                    loading = load_derived_parameters\n                elif child_name == 'overview':\n                    continue\n                else:\n                    loading = load_other_data\n                    load_subbranch = False\n\n                if loading == pypetconstants.LOAD_NOTHING:\n                    continue\n\n                if load_subbranch:\n                    # Load the subbranches recursively\n                    self._logger.info('Loading branch `%s` in mode `%s`.' %\n                                          (child_name, str(loading)))\n                else:\n                    if counter < maximum_display_other:\n                        self._logger.info(\n                            'Loading branch/node `%s` in mode `%s`.' % (child_name, str(loading)))\n                    elif counter == maximum_display_other:\n                        self._logger.info('To many branchs or nodes at root for display. '\n                                          'I will not inform you about loading anymore. '\n                                          'Branches are loaded silently '\n                                          'in the background. Do not worry, '\n                                          'I will not freeze! Pinky promise!!!')\n                    counter += 1\n\n                self._tree_load_sub_branch(traj, child_name, load_data=loading, with_links=True,\n                                     recursive=recursive,\n                                     max_depth=max_depth,\n                                     _trajectory=traj, _as_new=as_new,\n                                     _hdf5_group=self._trajectory_group)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads the meta information about the trajectory and updates the run information as well.", "response": "def _trj_load_meta_data(self, traj, load_data, as_new, with_run_information, force):\n        \"\"\"Loads meta information about the trajectory\n\n        Checks if the version number does not differ from current pypet version\n        Loads, comment, timestamp, name, version from disk in case trajectory is not loaded\n        as new. Updates the run information as well.\n\n        \"\"\"\n\n        metatable = self._overview_group.info\n        metarow = metatable[0]\n\n        try:\n            version = metarow['version'].decode('utf-8')\n        except (IndexError, ValueError) as ke:\n            self._logger.error('Could not check version due to: %s' % str(ke))\n            version = '`COULD NOT BE LOADED`'\n\n        try:\n            python = metarow['python'].decode('utf-8')\n        except (IndexError, ValueError) as ke:\n            self._logger.error('Could not check version due to: %s' % str(ke))\n            python = '`COULD NOT BE LOADED`'\n\n        self._trj_check_version(version, python, force)\n\n        # Load the skeleton information\n        self._grp_load_group(traj, load_data=load_data,\n                             with_links=False, recursive=False, _traj=traj,\n                             _as_new=as_new, _hdf5_group=self._trajectory_group)\n\n        if as_new:\n            length = int(metarow['length'])\n            for irun in range(length):\n                traj._add_run_info(irun)\n        else:\n            traj._comment =  metarow['comment'].decode('utf-8')\n            traj._timestamp = float(metarow['timestamp'])\n            traj._trajectory_timestamp = traj._timestamp\n            traj._time = metarow['time'].decode('utf-8')\n            traj._trajectory_time = traj._time\n            traj._name = metarow['name'].decode('utf-8')\n            traj._trajectory_name = traj._name\n            traj._version = version\n            traj._python = python\n\n            single_run_table = self._overview_group.runs\n\n            if with_run_information:\n                for row in single_run_table.iterrows():\n                    name = row['name'].decode('utf-8')\n                    idx = int(row['idx'])\n                    timestamp = float(row['timestamp'])\n                    time_ = row['time'].decode('utf-8')\n                    completed = int(row['completed'])\n                    summary = row['parameter_summary'].decode('utf-8')\n                    hexsha = row['short_environment_hexsha'].decode('utf-8')\n\n\n                    # To allow backwards compatibility we need this try catch block\n                    try:\n                        runtime = row['runtime'].decode('utf-8')\n                        finish_timestamp = float(row['finish_timestamp'])\n                    except (IndexError, ValueError) as ke:\n                        runtime = ''\n                        finish_timestamp = 0.0\n                        self._logger.debug('Could not load runtime, ' + repr(ke))\n\n                    info_dict = {'idx': idx,\n                                 'timestamp': timestamp,\n                                 'finish_timestamp': finish_timestamp,\n                                 'runtime': runtime,\n                                 'time': time_,\n                                 'completed': completed,\n                                 'name': name,\n                                 'parameter_summary': summary,\n                                 'short_environment_hexsha': hexsha}\n\n                    traj._add_run_info(**info_dict)\n            else:\n                traj._length = single_run_table.nrows\n\n        # Load explorations\n        self._trj_load_exploration(traj)\n        # Load the hdf5 config data:\n        self._srvc_load_hdf5_settings()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload data starting from a node along a branch and starts recursively loading all data at end of branch.", "response": "def _tree_load_sub_branch(self, traj_node, branch_name,\n                              load_data=pypetconstants.LOAD_DATA,\n                              with_links=True, recursive=False,\n                              max_depth=None, _trajectory=None,\n                              _as_new=False, _hdf5_group=None):\n        \"\"\"Loads data starting from a node along a branch and starts recursively loading\n        all data at end of branch.\n\n        :param traj_node: The node from where loading starts\n\n        :param branch_name:\n\n            A branch along which loading progresses. Colon Notation is used:\n            'group1.group2.group3' loads 'group1', then 'group2', then 'group3' and then finally\n            recursively all children and children's children below 'group3'\n\n        :param load_data:\n\n            How to load the data\n\n\n        :param with_links:\n\n            If links should be loaded\n\n        :param recursive:\n\n            If loading recursively\n\n        :param max_depth:\n\n            The maximum depth to load the tree\n\n        :param _trajectory:\n\n            The trajectory\n\n        :param _as_new:\n\n            If trajectory is loaded as new\n\n        :param _hdf5_group:\n\n            HDF5 node in the file corresponding to `traj_node`.\n\n        \"\"\"\n        if load_data == pypetconstants.LOAD_NOTHING:\n            return\n\n        if max_depth is None:\n            max_depth = float('inf')\n\n        if _trajectory is None:\n            _trajectory = traj_node.v_root\n\n        if _hdf5_group is None:\n            hdf5_group_name = traj_node.v_full_name.replace('.', '/')\n\n            # Get child node to load\n            if hdf5_group_name == '':\n                _hdf5_group = self._trajectory_group\n            else:\n                try:\n                    _hdf5_group = self._hdf5file.get_node(where=self._trajectory_group,\n                                                  name=hdf5_group_name)\n                except pt.NoSuchNodeError:\n                    self._logger.error('Cannot find `%s` the hdf5 node `%s` does not exist!'\n                                       % (traj_node.v_full_name, hdf5_group_name))\n                    raise\n\n        split_names = branch_name.split('.')\n\n        final_group_name = split_names.pop()\n\n        current_depth = 1\n\n        for name in split_names:\n            if current_depth > max_depth:\n                return\n            # First load along the branch\n            _hdf5_group = getattr(_hdf5_group, name)\n\n            self._tree_load_nodes_dfs(traj_node, load_data=load_data, with_links=with_links,\n                                  recursive=False, max_depth=max_depth, current_depth=current_depth,\n                                  trajectory=_trajectory, as_new=_as_new,\n                                  hdf5_group=_hdf5_group)\n\n            current_depth += 1\n\n            traj_node = traj_node._children[name]\n\n        if current_depth <= max_depth:\n            # Then load recursively all data in the last group and below\n            _hdf5_group = getattr(_hdf5_group, final_group_name)\n            self._tree_load_nodes_dfs(traj_node, load_data=load_data, with_links=with_links,\n                                  recursive=recursive, max_depth=max_depth,\n                                  current_depth=current_depth, trajectory=_trajectory,\n                                  as_new=_as_new, hdf5_group=_hdf5_group)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck for version mismatch and raises a VersionMismatchError if it is not raised.", "response": "def _trj_check_version(self, version, python, force):\n        \"\"\"Checks for version mismatch\n\n        Raises a VersionMismatchError if version of loaded trajectory and current pypet version\n        do not match. In case of `force=True` error is not raised only a warning is emitted.\n\n        \"\"\"\n        curr_python = pypetconstants.python_version_string\n\n        if (version != VERSION or curr_python != python) and not force:\n            raise pex.VersionMismatchError('Current pypet version is %s used under python %s '\n                                           '  but your trajectory'\n                                           ' was created with version %s and python %s.'\n                                           ' Use >>force=True<< to perform your load regardless'\n                                           ' of version mismatch.' %\n                                           (VERSION, curr_python, version, python))\n        elif version != VERSION or curr_python != python:\n            self._logger.warning('Current pypet version is %s with python %s but your trajectory'\n                                 ' was created with version %s under python %s.'\n                                 ' Yet, you enforced the load, so I will'\n                                 ' handle the trajectory despite the'\n                                 ' version mismatch.' %\n                                 (VERSION, curr_python, version, python))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _trj_fill_run_table(self, traj, start, stop):\n\n        def _make_row(info_dict):\n            row = (info_dict['idx'],\n                   info_dict['name'],\n                   info_dict['time'],\n                   info_dict['timestamp'],\n                   info_dict['finish_timestamp'],\n                   info_dict['runtime'],\n                   info_dict['parameter_summary'],\n                   info_dict['short_environment_hexsha'],\n                   info_dict['completed'])\n            return row\n\n        runtable = getattr(self._overview_group, 'runs')\n\n        rows = []\n        updated_run_information = traj._updated_run_information\n        for idx in range(start, stop):\n            info_dict = traj._run_information[traj._single_run_ids[idx]]\n            rows.append(_make_row(info_dict))\n            updated_run_information.discard(idx)\n\n        if rows:\n            runtable.append(rows)\n            runtable.flush()\n\n        # Store all runs that are updated and that have not been stored yet\n        rows = []\n        indices = []\n        for idx in updated_run_information:\n            info_dict = traj.f_get_run_information(idx, copy=False)\n            rows.append(_make_row(info_dict))\n            indices.append(idx)\n\n        if rows:\n            runtable.modify_coordinates(indices, rows)\n\n        traj._updated_run_information = set()", "response": "Fills the run overview table with information."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _trj_store_meta_data(self, traj):\n\n        # Description of the `info` table\n        descriptiondict = {'name': pt.StringCol(pypetconstants.HDF5_STRCOL_MAX_LOCATION_LENGTH,\n                                                pos=0),\n                           'time': pt.StringCol(len(traj.v_time), pos=1),\n                           'timestamp': pt.FloatCol(pos=3),\n                           'comment': pt.StringCol(pypetconstants.HDF5_STRCOL_MAX_COMMENT_LENGTH,\n                                                   pos=4),\n                           'length': pt.IntCol(pos=2),\n                           'version': pt.StringCol(pypetconstants.HDF5_STRCOL_MAX_NAME_LENGTH,\n                                                   pos=5),\n                           'python': pt.StringCol(pypetconstants.HDF5_STRCOL_MAX_NAME_LENGTH,\n                                                  pos=5)}\n        # 'loaded_from' : pt.StringCol(pypetconstants.HDF5_STRCOL_MAX_LOCATION_LENGTH)}\n\n        infotable = self._all_get_or_create_table(where=self._overview_group, tablename='info',\n                                                  description=descriptiondict,\n                                                  expectedrows=len(traj))\n\n        insert_dict = self._all_extract_insert_dict(traj, infotable.colnames)\n        self._all_add_or_modify_row(traj.v_name, insert_dict, infotable, index=0,\n                                    flags=(HDF5StorageService.ADD_ROW,\n                                           HDF5StorageService.MODIFY_ROW))\n\n        # Description of the `run` table\n        rundescription_dict = {'name': pt.StringCol(pypetconstants.HDF5_STRCOL_MAX_NAME_LENGTH,\n                                                    pos=1),\n                               'time': pt.StringCol(len(traj.v_time), pos=2),\n                               'timestamp': pt.FloatCol(pos=3),\n                               'idx': pt.IntCol(pos=0),\n                               'completed': pt.IntCol(pos=8),\n                               'parameter_summary': pt.StringCol(\n                                   pypetconstants.HDF5_STRCOL_MAX_COMMENT_LENGTH,\n                                   pos=6),\n                               'short_environment_hexsha': pt.StringCol(7, pos=7),\n                               'finish_timestamp': pt.FloatCol(pos=4),\n                               'runtime': pt.StringCol(\n                                   pypetconstants.HDF5_STRCOL_MAX_RUNTIME_LENGTH,\n                                   pos=5)}\n\n        runtable = self._all_get_or_create_table(where=self._overview_group,\n                                                 tablename='runs',\n                                                 description=rundescription_dict)\n\n        hdf5_description_dict = {'complib': pt.StringCol(7, pos=0),\n                                 'complevel': pt.IntCol(pos=1),\n                                 'shuffle': pt.BoolCol(pos=2),\n                                 'fletcher32': pt.BoolCol(pos=3),\n                                 'pandas_format': pt.StringCol(7, pos=4),\n                                 'encoding': pt.StringCol(11, pos=5)}\n\n        pos = 7\n        for name, table_name in HDF5StorageService.NAME_TABLE_MAPPING.items():\n            hdf5_description_dict[table_name] = pt.BoolCol(pos=pos)\n            pos += 1\n\n        # Store the hdf5 properties in an overview table\n        hdf5_description_dict.update({'purge_duplicate_comments': pt.BoolCol(pos=pos + 2),\n                                      'results_per_run': pt.IntCol(pos=pos + 3),\n                                      'derived_parameters_per_run': pt.IntCol(pos=pos + 4)})\n\n        hdf5table = self._all_get_or_create_table(where=self._overview_group,\n                                                  tablename='hdf5_settings',\n                                                  description=hdf5_description_dict)\n\n        insert_dict = {}\n        for attr_name in self.ATTR_LIST:\n            insert_dict[attr_name] = getattr(self, attr_name)\n\n        for attr_name, table_name in self.NAME_TABLE_MAPPING.items():\n            insert_dict[table_name] = getattr(self, attr_name)\n\n        for attr_name, name in self.PR_ATTR_NAME_MAPPING.items():\n            insert_dict[name] = getattr(self, attr_name)\n\n        self._all_add_or_modify_row(traj.v_name, insert_dict, hdf5table, index=0,\n                                    flags=(HDF5StorageService.ADD_ROW,\n                                           HDF5StorageService.MODIFY_ROW))\n\n\n        # Fill table with dummy entries starting from the current table size\n        actual_rows = runtable.nrows\n        self._trj_fill_run_table(traj, actual_rows, len(traj._run_information))\n        # stop != len(traj) to allow immediate post-proc with QUEUE wrapping\n\n        # Store the annotations and comment of the trajectory node\n        self._grp_store_group(traj, store_data=pypetconstants.STORE_DATA,\n                                  with_links=False,\n                                  recursive=False,\n                                  _hdf5_group=self._trajectory_group)\n\n        # Store the list of explored paramters\n        self._trj_store_explorations(traj)\n\n        # Prepare the exploration tables\n        # Prepare the overview tables\n        tostore_tables = []\n\n        for name, table_name in HDF5StorageService.NAME_TABLE_MAPPING.items():\n\n            # Check if we want the corresponding overview table\n            # If the trajectory does not contain information about the table\n            # we assume it should be created.\n\n            if getattr(self, name):\n                tostore_tables.append(table_name)\n\n        self._srvc_make_overview_tables(tostore_tables, traj)", "response": "Stores the meta data for the trajectory in the info table and the run table."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _trj_load_exploration(self, traj):\n        if hasattr(self._overview_group, 'explorations'):\n            explorations_table = self._overview_group._f_get_child( 'explorations')\n            for row in explorations_table.iterrows():\n                param_name = row['explorations'].decode('utf-8')\n                if param_name not in traj._explored_parameters:\n                    traj._explored_parameters[param_name] = None\n        else:\n            # This is for backwards compatibility\n            for what in ('parameters', 'derived_parameters'):\n                if hasattr(self._trajectory_group, what):\n                    parameters = self._trajectory_group._f_get_child(what)\n                    for group in parameters._f_walk_groups():\n                        if self._all_get_from_attrs(group, HDF5StorageService.LENGTH):\n                            group_location = group._v_pathname\n                            full_name = '.'.join(group_location.split('/')[2:])\n                            traj._explored_parameters[full_name] = None", "response": "Load all explored parameters from the overview group."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstoring a all explored parameter names for internal recall", "response": "def _trj_store_explorations(self, traj):\n        \"\"\"Stores a all explored parameter names for internal recall\"\"\"\n        nexplored = len(traj._explored_parameters)\n        if nexplored > 0:\n            if hasattr(self._overview_group, 'explorations'):\n                explorations_table = self._overview_group._f_get_child('explorations')\n                if len(explorations_table) != nexplored:\n                    self._hdf5file.remove_node(where=self._overview_group,\n                                         name='explorations')\n        if not hasattr(self._overview_group, 'explorations'):\n            explored_list = list(traj._explored_parameters.keys())\n            if explored_list:\n                string_col = self._all_get_table_col('explorations',\n                                                      explored_list,\n                                                      'overview.explorations')\n            else:\n                string_col = pt.StringCol(1)\n            description = {'explorations': string_col}\n            explorations_table = self._hdf5file.create_table(where=self._overview_group,\n                                                       name='explorations',\n                                                       description=description)\n            rows = [(x.encode('utf-8'),) for x in explored_list]\n            if rows:\n                explorations_table.append(rows)\n                explorations_table.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _srvc_make_overview_tables(self, tables_to_make, traj=None):\n        for table_name in tables_to_make:\n            # Prepare the tables desciptions, depending on which overview table we create\n            # we need different columns\n            paramdescriptiondict = {}\n            expectedrows = 0\n\n            # Every overview table has a name and location column\n            paramdescriptiondict['location'] = pt.StringCol(\n                pypetconstants.HDF5_STRCOL_MAX_LOCATION_LENGTH,\n                pos=0)\n            paramdescriptiondict['name'] = pt.StringCol(pypetconstants.HDF5_STRCOL_MAX_NAME_LENGTH,\n                                                        pos=1)\n\n            paramdescriptiondict['comment'] = pt.StringCol(\n                    pypetconstants.HDF5_STRCOL_MAX_COMMENT_LENGTH)\n\n            paramdescriptiondict['value'] = pt.StringCol(\n                    pypetconstants.HDF5_STRCOL_MAX_VALUE_LENGTH, pos=2)\n\n            if table_name == 'config_overview':\n                if traj is not None:\n                    expectedrows = len(traj._config)\n\n            if table_name == 'parameters_overview':\n                if traj is not None:\n                    expectedrows = len(traj._parameters)\n\n            if table_name == 'explored_parameters_overview':\n                paramdescriptiondict['range'] = pt.StringCol(\n                    pypetconstants.HDF5_STRCOL_MAX_RANGE_LENGTH)\n                paramdescriptiondict['length'] = pt.IntCol()\n                if traj is not None:\n                    expectedrows = len(traj._explored_parameters)\n\n            if table_name.endswith('summary'):\n                paramdescriptiondict['hexdigest'] = pt.StringCol(64, pos=10)\n\n            # Check if the user provided an estimate of the amount of results per run\n            # This can help to speed up storing\n            if table_name == 'derived_parameters_overview':\n\n                expectedrows = self._derived_parameters_per_run\n\n                if traj is not None:\n                    expectedrows *= len(traj)\n                    expectedrows += len(traj._derived_parameters)\n\n            if table_name == 'results_overview':\n                expectedrows = self._results_per_run\n\n                if traj is not None:\n                    expectedrows *= len(traj)\n                    expectedrows += len(traj._results)\n\n            if expectedrows > 0:\n                paramtable = self._all_get_or_create_table(where=self._overview_group,\n                                                           tablename=table_name,\n                                                           description=paramdescriptiondict,\n                                                           expectedrows=expectedrows)\n            else:\n                paramtable = self._all_get_or_create_table(where=self._overview_group,\n                                                           tablename=table_name,\n                                                           description=paramdescriptiondict)\n\n            paramtable.flush()", "response": "Creates the overview tables in overview group."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _trj_store_trajectory(self, traj, only_init=False, store_data=pypetconstants.STORE_DATA,\n                              max_depth=None):\n        \"\"\" Stores a trajectory to an hdf5 file\n\n        Stores all groups, parameters and results\n\n        \"\"\"\n        if not only_init:\n            self._logger.info('Start storing Trajectory `%s`.' % self._trajectory_name)\n        else:\n            self._logger.info('Initialising storage or updating meta data of Trajectory `%s`.' %\n                              self._trajectory_name)\n            store_data = pypetconstants.STORE_NOTHING\n\n        # In case we accidentally chose a trajectory name that already exist\n        # We do not want to mess up the stored trajectory but raise an Error\n        if not traj._stored and self._trajectory_group is not None:\n            raise RuntimeError('You want to store a completely new trajectory with name'\n                               ' `%s` but this trajectory is already found in file `%s`.'\n                               'Did you try to accidentally overwrite existing data? If '\n                               'you DO want to override existing data, use `overwrite_file=True`.'\n                               'Note that this deletes the whole HDF5 file not just the particular '\n                               'trajectroy therein! ' %\n                               (traj.v_name, self._filename))\n\n        # Extract HDF5 settings from the trajectory\n        self._srvc_check_hdf_properties(traj)\n\n        # Store the trajectory for the first time if necessary:\n        if self._trajectory_group is None:\n            self._trajectory_group = self._hdf5file.create_group(where='/',\n                                                           name=self._trajectory_name,\n                                                           title=self._trajectory_name,\n                                                           filters=self._all_get_filters())\n        # Store meta information\n        self._trj_store_meta_data(traj)\n\n        # # Store recursively the config subtree\n        # self._tree_store_recursively(pypetconstants.LEAF,traj.config,self._trajectory_group)\n\n        if store_data in (pypetconstants.STORE_DATA_SKIPPING,\n                          pypetconstants.STORE_DATA,\n                          pypetconstants.OVERWRITE_DATA):\n\n            counter = 0\n            maximum_display_other = 10\n            name_set = set(['parameters', 'config', 'derived_parameters', 'results'])\n\n            for child_name in traj._children:\n\n                if child_name in name_set:\n                    self._logger.info('Storing branch `%s`.' % child_name)\n                else:\n                    if counter < maximum_display_other:\n                        self._logger.info('Storing branch/node `%s`.' % child_name)\n                    elif counter == maximum_display_other:\n                        self._logger.info('To many branches or nodes at root for display. '\n                                          'I will not inform you about storing anymore. '\n                                          'Branches are stored silently in the background. '\n                                          'Do not worry, I will not freeze! Pinky promise!!!')\n                    counter += 1\n\n                # Store recursively the elements\n                self._tree_store_sub_branch(traj, child_name, store_data=store_data,\n                                            with_links=True,\n                                            recursive=True, max_depth=max_depth,\n                                            hdf5_group=self._trajectory_group)\n\n            self._logger.info('Finished storing Trajectory `%s`.' % self._trajectory_name)\n        else:\n            self._logger.info('Finished init or meta data update for `%s`.' %\n                              self._trajectory_name)\n        traj._stored = True", "response": "Stores a trajectory to an hdf5 file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _tree_store_sub_branch(self, traj_node, branch_name,\n                               store_data=pypetconstants.STORE_DATA,\n                               with_links=True,\n                               recursive=False,\n                               max_depth=None,\n                               hdf5_group=None):\n        \"\"\"Stores data starting from a node along a branch and starts recursively loading\n        all data at end of branch.\n\n        :param traj_node: The node where storing starts\n\n        :param branch_name:\n\n            A branch along which storing progresses. Colon Notation is used:\n            'group1.group2.group3' loads 'group1', then 'group2', then 'group3', and then finally\n            recursively all children and children's children below 'group3'.\n\n        :param store_data: How data should be stored\n\n        :param with_links: If links should be stored\n\n        :param recursive:\n\n            If the rest of the tree should be recursively stored\n\n        :param max_depth:\n\n            Maximum depth to store\n\n        :param hdf5_group:\n\n            HDF5 node in the file corresponding to `traj_node`\n\n        \"\"\"\n        if store_data == pypetconstants.STORE_NOTHING:\n            return\n\n        if max_depth is None:\n            max_depth = float('inf')\n\n        if hdf5_group is None:\n            # Get parent hdf5 node\n            location = traj_node.v_full_name\n            hdf5_location = location.replace('.', '/')\n            try:\n                if location == '':\n                    hdf5_group = self._trajectory_group\n                else:\n                    hdf5_group = self._hdf5file.get_node( where=self._trajectory_group,\n                                                         name=hdf5_location)\n            except pt.NoSuchNodeError:\n                self._logger.debug('Cannot store `%s` the parental hdf5 node with path `%s` does '\n                                     'not exist on disk.' %\n                                     (traj_node.v_name, hdf5_location))\n\n                if traj_node.v_is_leaf:\n                    self._logger.error('Cannot store `%s` the parental hdf5 '\n                                       'node with path `%s` does '\n                                       'not exist on disk! The child '\n                                       'you want to store is a leaf node,'\n                                       'that cannot be stored without '\n                                       'the parental node existing on '\n                                       'disk.' % (traj_node.v_name, hdf5_location))\n                    raise\n                else:\n                    self._logger.debug('I will try to store the path from trajectory root to '\n                                         'the child now.')\n\n                    self._tree_store_sub_branch(traj_node._nn_interface._root_instance,\n                                                traj_node.v_full_name + '.' + branch_name,\n                                                store_data=store_data, with_links=with_links,\n                                                recursive=recursive,\n                                                max_depth=max_depth + traj_node.v_depth,\n                                                hdf5_group=self._trajectory_group)\n                    return\n\n        current_depth = 1\n\n        split_names = branch_name.split('.')\n\n        leaf_name = split_names.pop()\n\n        for name in split_names:\n            if current_depth > max_depth:\n                return\n            # Store along a branch\n            self._tree_store_nodes_dfs(traj_node, name, store_data=store_data, with_links=with_links,\n                                   recursive=False, max_depth=max_depth,\n                                   current_depth=current_depth, parent_hdf5_group=hdf5_group)\n            current_depth += 1\n\n            traj_node = traj_node._children[name]\n\n            hdf5_group = getattr(hdf5_group, name)\n\n        # Store final group and recursively everything below it\n        if current_depth <= max_depth:\n            self._tree_store_nodes_dfs(traj_node, leaf_name, store_data=store_data,\n                               with_links=with_links, recursive=recursive,\n                               max_depth=max_depth, current_depth=current_depth,\n                               parent_hdf5_group=hdf5_group)", "response": "Stores data starting from a node along a branch and starts recursively loading all data at end of branch."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _tree_create_leaf(self, name, trajectory, hdf5_group):\n        class_name = self._all_get_from_attrs(hdf5_group, HDF5StorageService.CLASS_NAME)\n\n        # Create the instance with the appropriate constructor\n        class_constructor = trajectory._create_class(class_name)\n\n        instance = trajectory._construct_instance(class_constructor, name)\n\n        return instance", "response": "Create a new pypet leaf instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading a node from hdf5 file and if desired recursively everything below the hdf5_group_name.", "response": "def _tree_load_nodes_dfs(self, parent_traj_node, load_data, with_links, recursive,\n                         max_depth, current_depth, trajectory, as_new, hdf5_group):\n        \"\"\"Loads a node from hdf5 file and if desired recursively everything below\n\n        :param parent_traj_node: The parent node whose child should be loaded\n        :param load_data: How to load the data\n        :param with_links: If links should be loaded\n        :param recursive: Whether loading recursively below hdf5_group\n        :param max_depth: Maximum depth\n        :param current_depth: Current depth\n        :param trajectory: The trajectory object\n        :param as_new: If trajectory is loaded as new\n        :param hdf5_group: The hdf5 group containing the child to be loaded\n\n        \"\"\"\n        if max_depth is None:\n            max_depth = float('inf')\n\n        loading_list = [(parent_traj_node, current_depth, hdf5_group)]\n\n        while loading_list:\n            parent_traj_node, current_depth, hdf5_group = loading_list.pop()\n\n            if isinstance(hdf5_group, pt.link.SoftLink):\n                if with_links:\n                    # We end up here when auto-loading a soft link\n                    self._tree_load_link(parent_traj_node, load_data=load_data, traj=trajectory,\n                                         as_new=as_new, hdf5_soft_link=hdf5_group)\n                continue\n\n\n            name = hdf5_group._v_name\n            is_leaf = self._all_get_from_attrs(hdf5_group, HDF5StorageService.LEAF)\n            in_trajectory = name in parent_traj_node._children\n\n            if is_leaf:\n                # In case we have a leaf node, we need to check if we have to create a new\n                # parameter or result\n\n                if in_trajectory:\n                    instance = parent_traj_node._children[name]\n                # Otherwise we need to create a new instance\n                else:\n                    instance = self._tree_create_leaf(name, trajectory, hdf5_group)\n\n                    # Add the instance to the trajectory tree\n                    parent_traj_node._add_leaf_from_storage(args=(instance,), kwargs={})\n\n                self._prm_load_parameter_or_result(instance, load_data=load_data,\n                                                   _hdf5_group=hdf5_group)\n                if as_new:\n                    instance._stored = False\n\n            else:\n                if in_trajectory:\n                    traj_group = parent_traj_node._children[name]\n\n                    if load_data == pypetconstants.OVERWRITE_DATA:\n                        traj_group.v_annotations.f_empty()\n                        traj_group.v_comment = ''\n                else:\n                    if HDF5StorageService.CLASS_NAME in hdf5_group._v_attrs:\n                        class_name = self._all_get_from_attrs(hdf5_group,\n                                                              HDF5StorageService.CLASS_NAME)\n                        class_constructor = trajectory._create_class(class_name)\n                        instance = trajectory._construct_instance(class_constructor, name)\n                        args = (instance,)\n                    else:\n                        args = (name,)\n                    # If the group does not exist create it'\n                    traj_group = parent_traj_node._add_group_from_storage(args=args, kwargs={})\n\n                # Load annotations and comment\n                self._grp_load_group(traj_group, load_data=load_data, with_links=with_links,\n                                     recursive=False, max_depth=max_depth,\n                                     _traj=trajectory, _as_new=as_new,\n                                     _hdf5_group=hdf5_group)\n\n                if recursive and current_depth < max_depth:\n                    new_depth = current_depth + 1\n                    for children in (hdf5_group._v_groups, hdf5_group._v_links):\n                        for new_hdf5_group_name in children:\n                            new_hdf5_group = children[new_hdf5_group_name]\n                            loading_list.append((traj_group, new_depth, new_hdf5_group))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads a link in a tree.", "response": "def _tree_load_link(self, new_traj_node, load_data, traj, as_new, hdf5_soft_link):\n        \"\"\" Loads a link\n        \n        :param new_traj_node: Node in traj containing link \n        :param load_data: How to load data in the linked node\n        :param traj: The trajectory\n        :param as_new: If data in linked node should be loaded as new\n        :param hdf5_soft_link: The hdf5 soft link\n\n        \"\"\"\n        try:\n            linked_group = hdf5_soft_link()\n            link_name = hdf5_soft_link._v_name\n\n            if (not link_name in new_traj_node._links or\n                        load_data==pypetconstants.OVERWRITE_DATA):\n\n                link_location = linked_group._v_pathname\n                full_name = '.'.join(link_location.split('/')[2:])\n                if not full_name in traj:\n                    self._tree_load_sub_branch(traj, full_name,\n                                               load_data=pypetconstants.LOAD_SKELETON,\n                                               with_links=False, recursive=False, _trajectory=traj,\n                                               _as_new=as_new, _hdf5_group=self._trajectory_group)\n\n                if (load_data == pypetconstants.OVERWRITE_DATA and\n                            link_name in new_traj_node._links):\n                    new_traj_node.f_remove_link(link_name)\n                if not link_name in new_traj_node._links:\n                    new_traj_node._nn_interface._add_generic(new_traj_node,\n                                                                type_name=nn.LINK,\n                                                                group_type_name=nn.GROUP,\n                                                                args=(link_name,\n                                                                      traj.f_get(full_name)),\n                                                                kwargs={},\n                                                                add_prefix=False,\n                                                                check_naming=False)\n                else:\n                    raise RuntimeError('You shall not pass!')\n        except pt.NoSuchNodeError:\n            self._logger.error('Link `%s` under `%s` is broken, cannot load it, '\n                               'I will ignore it, you have to '\n                               'manually delete it!' %\n                               (hdf5_soft_link._v_name, new_traj_node.v_full_name))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _tree_store_nodes_dfs(self, parent_traj_node, name, store_data, with_links, recursive,\n                          max_depth, current_depth,\n                          parent_hdf5_group):\n        \"\"\"Stores a node to hdf5 and if desired stores recursively everything below it.\n\n        :param parent_traj_node: The parental node\n        :param name: Name of node to be stored\n        :param store_data: How to store data\n        :param with_links: If links should be stored\n        :param recursive: Whether to store recursively the subtree\n        :param max_depth: Maximum recursion depth in tree\n        :param current_depth: Current depth\n        :param parent_hdf5_group: Parent hdf5 group\n\n        \"\"\"\n        if max_depth is None:\n            max_depth = float('inf')\n\n        store_list = [(parent_traj_node, name, current_depth, parent_hdf5_group)]\n\n        while store_list:\n            parent_traj_node, name, current_depth, parent_hdf5_group = store_list.pop()\n\n            # Check if we create a link\n            if name in parent_traj_node._links:\n                if with_links:\n                    self._tree_store_link(parent_traj_node, name, parent_hdf5_group)\n                continue\n\n            traj_node = parent_traj_node._children[name]\n\n            # If the node does not exist in the hdf5 file create it\n            if not hasattr(parent_hdf5_group, name):\n                newly_created = True\n                new_hdf5_group = self._hdf5file.create_group(where=parent_hdf5_group,\n                                                       name=name, filters=self._all_get_filters())\n            else:\n                newly_created = False\n                new_hdf5_group = getattr(parent_hdf5_group, name)\n\n            if traj_node.v_is_leaf:\n                self._prm_store_parameter_or_result(traj_node, store_data=store_data,\n                                                     _hdf5_group=new_hdf5_group,\n                                                    _newly_created=newly_created)\n\n            else:\n                self._grp_store_group(traj_node, store_data=store_data, with_links=with_links,\n                                      recursive=False, max_depth=max_depth,\n                                      _hdf5_group=new_hdf5_group,\n                                      _newly_created=newly_created)\n\n                if recursive and current_depth < max_depth:\n                    for child in traj_node._children.keys():\n                        store_list.append((traj_node, child, current_depth + 1, new_hdf5_group))", "response": "Stores a node to hdf5 and if desired stores recursively everything below it."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _tree_store_link(self, node_in_traj, link, hdf5_group):\n\n        if hasattr(hdf5_group, link):\n            return\n\n        linked_traj_node = node_in_traj._links[link]\n        linking_name = linked_traj_node.v_full_name.replace('.','/')\n        linking_name = '/' + self._trajectory_name + '/' + linking_name\n        try:\n            to_link_hdf5_group = self._hdf5file.get_node(where=linking_name)\n        except pt.NoSuchNodeError:\n            self._logger.debug('Could not store link `%s` under `%s` immediately, '\n                               'need to store `%s` first. '\n                               'Will store the link right after.' % (link,\n                                                                     node_in_traj.v_full_name,\n                                                                     linked_traj_node.v_full_name))\n            root = node_in_traj._nn_interface._root_instance\n            self._tree_store_sub_branch(root, linked_traj_node.v_full_name,\n                                        store_data=pypetconstants.STORE_DATA_SKIPPING,\n                                        with_links=False, recursive=False,\n                                        hdf5_group=self._trajectory_group)\n            to_link_hdf5_group = self._hdf5file.get_node(where=linking_name)\n        self._hdf5file.create_soft_link(where=hdf5_group,\n                                  name=link,\n                                  target=to_link_hdf5_group)", "response": "Stores a soft link under the given node under the given parental node."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _srn_store_single_run(self, traj,\n                              recursive=True,\n                              store_data=pypetconstants.STORE_DATA,\n                              max_depth=None):\n        \"\"\" Stores a single run instance to disk (only meta data)\"\"\"\n\n        if store_data != pypetconstants.STORE_NOTHING:\n            self._logger.debug('Storing Data of single run `%s`.' % traj.v_crun)\n            if max_depth is None:\n                max_depth = float('inf')\n            for name_pair in traj._new_nodes:\n                _, name = name_pair\n                parent_group, child_node = traj._new_nodes[name_pair]\n                if not child_node._stored:\n                    self._tree_store_sub_branch(parent_group, name,\n                                          store_data=store_data,\n                                          with_links=True,\n                                          recursive=recursive,\n                                          max_depth=max_depth - child_node.v_depth,\n                                          hdf5_group=None)\n            for name_pair in traj._new_links:\n                _, link = name_pair\n                parent_group, _ = traj._new_links[name_pair]\n                self._tree_store_sub_branch(parent_group, link,\n                                            store_data=store_data,\n                                            with_links=True,\n                                            recursive=recursive,\n                                            max_depth=max_depth - parent_group.v_depth - 1,\n                                            hdf5_group=None)", "response": "Stores a single run instance to disk."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _srn_summarize_explored_parameters(self, paramlist):\n\n        runsummary = ''\n        paramlist = sorted(paramlist, key=lambda name: name.v_name + name.v_location)\n        for idx, expparam in enumerate(paramlist):\n\n            # Create the run summary for the `run` overview\n            if idx > 0:\n                runsummary += ',   '\n\n            valstr = expparam.f_val_to_str()\n\n            if len(valstr) >= pypetconstants.HDF5_STRCOL_MAX_COMMENT_LENGTH:\n                valstr = valstr[0:pypetconstants.HDF5_STRCOL_MAX_COMMENT_LENGTH - 3]\n                valstr += '...'\n\n            if expparam.v_name in runsummary:\n                param_name = expparam.v_full_name\n            else:\n                param_name = expparam.v_name\n\n            runsummary = runsummary + param_name + ': ' + valstr\n\n        return runsummary", "response": "Summarizes the parameter settings."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstore a single row into an overview table.", "response": "def _all_store_param_or_result_table_entry(self, instance, table, flags,\n                                               additional_info=None):\n        \"\"\"Stores a single row into an overview table\n\n        :param instance: A parameter or result instance\n\n        :param table: Table where row will be inserted\n\n        :param flags:\n\n            Flags how to insert into the table. Potential Flags are\n            `ADD_ROW`, `REMOVE_ROW`, `MODIFY_ROW`\n\n        :param additional_info:\n\n            Dictionary containing information that cannot be extracted from\n            `instance`, but needs to be inserted, too.\n\n\n        \"\"\"\n        # assert isinstance(table, pt.Table)\n\n        location = instance.v_location\n        name = instance.v_name\n        fullname = instance.v_full_name\n\n        if (flags == (HDF5StorageService.ADD_ROW,) and table.nrows < 2\n                and 'location' in table.colnames):\n            # We add the modify row option here because you cannot delete the very first\n            # row of the table, so there is the rare condition, that the row might already\n            # exist.\n            # We also need to check if 'location' is in the columns in order to avoid\n            # confusion with the smaller explored parameter overviews\n            flags = (HDF5StorageService.ADD_ROW, HDF5StorageService.MODIFY_ROW)\n\n        if flags == (HDF5StorageService.ADD_ROW,):\n            # If we are sure we only want to add a row we do not need to search!\n            condvars = None\n            condition = None\n        else:\n            # Condition to search for an entry\n            condvars = {'namecol': table.cols.name, 'locationcol': table.cols.location,\n                        'name': name, 'location': location}\n\n            condition = \"\"\"(namecol == name) & (locationcol == location)\"\"\"\n\n        if HDF5StorageService.REMOVE_ROW in flags:\n            # If we want to remove a row, we don't need to extract information\n            insert_dict = {}\n        else:\n            # Extract information to insert from the instance and the additional info dict\n            colnames = set(table.colnames)\n            insert_dict = self._all_extract_insert_dict(instance, colnames, additional_info)\n\n        # Write the table entry\n        self._all_add_or_modify_row(fullname, insert_dict, table, condition=condition,\n                                    condvars=condvars, flags=flags)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new table or returns it.", "response": "def _all_get_or_create_table(self, where, tablename, description, expectedrows=None):\n        \"\"\"Creates a new table, or if the table already exists, returns it.\"\"\"\n        where_node = self._hdf5file.get_node(where)\n\n        if not tablename in where_node:\n            if not expectedrows is None:\n                table = self._hdf5file.create_table(where=where_node, name=tablename,\n                                              description=description, title=tablename,\n                                              expectedrows=expectedrows,\n                                              filters=self._all_get_filters())\n            else:\n                table = self._hdf5file.create_table(where=where_node, name=tablename,\n                                              description=description, title=tablename,\n                                              filters=self._all_get_filters())\n        else:\n            table = where_node._f_get_child(tablename)\n\n        return table"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _all_get_node_by_name(self, name):\n        path_name = name.replace('.', '/')\n        where = '/%s/%s' % (self._trajectory_name, path_name)\n        return self._hdf5file.get_node(where=where)", "response": "Returns an HDF5 node by the path specified in name"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _all_set_attributes_to_recall_natives(data, ptitem, prefix):\n\n        # If `data` is a container, remember the container type\n        if type(data) is tuple:\n            HDF5StorageService._all_set_attr(ptitem, prefix + HDF5StorageService.COLL_TYPE,\n                                   HDF5StorageService.COLL_TUPLE)\n\n        elif type(data) is list:\n            HDF5StorageService._all_set_attr(ptitem, prefix + HDF5StorageService.COLL_TYPE,\n                                   HDF5StorageService.COLL_LIST)\n\n        elif type(data) is np.ndarray:\n            HDF5StorageService._all_set_attr(ptitem, prefix + HDF5StorageService.COLL_TYPE,\n                                   HDF5StorageService.COLL_NDARRAY)\n\n        elif type(data) is np.matrix:\n            HDF5StorageService._all_set_attr(ptitem, prefix + HDF5StorageService.COLL_TYPE,\n                                   HDF5StorageService.COLL_MATRIX)\n\n        elif type(data) in pypetconstants.PARAMETER_SUPPORTED_DATA:\n            HDF5StorageService._all_set_attr(ptitem, prefix + HDF5StorageService.COLL_TYPE,\n                                   HDF5StorageService.COLL_SCALAR)\n\n            strtype = type(data).__name__\n\n            if not strtype in pypetconstants.PARAMETERTYPEDICT:\n                raise TypeError('I do not know how to handle `%s` its type is `%s`.' %\n                                (str(data), repr(type(data))))\n\n            HDF5StorageService._all_set_attr(ptitem, prefix + HDF5StorageService.SCALAR_TYPE, strtype)\n\n        elif type(data) is dict:\n            if len(data) > 0:\n                HDF5StorageService._all_set_attr(ptitem, prefix + HDF5StorageService.COLL_TYPE,\n                                   HDF5StorageService.COLL_DICT)\n            else:\n                HDF5StorageService._all_set_attr(ptitem, prefix + HDF5StorageService.COLL_TYPE,\n                                   HDF5StorageService.COLL_EMPTY_DICT)\n        else:\n            raise TypeError('I do not know how to handle `%s` its type is `%s`.' %\n                            (str(data), repr(type(data))))\n\n        if type(data) in (list, tuple):\n            # If data is a list or tuple we need to remember the data type of the elements\n            # in the list or tuple.\n            # We do NOT need to remember the elements of `dict` explicitly, though.\n            # `dict` is stored\n            # as an `ObjectTable` and thus types are already conserved.\n            if len(data) > 0:\n                strtype = type(data[0]).__name__\n\n                if not strtype in pypetconstants.PARAMETERTYPEDICT:\n                    raise TypeError('I do not know how to handle `%s` its type is '\n                                    '`%s`.' % (str(data), strtype))\n\n                HDF5StorageService._all_set_attr(ptitem, prefix +\n                                       HDF5StorageService.SCALAR_TYPE, strtype)\n        elif (type(data) in (np.ndarray, np.matrix) and\n                  np.issubdtype(data.dtype, str)):\n            HDF5StorageService._all_set_attr(ptitem, prefix + HDF5StorageService.SCALAR_TYPE,\n                                   str.__name__)", "response": "Stores original data type to HDF5 node attributes for preserving the data type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if loaded data has the type it was stored in. If not converts it.", "response": "def _all_recall_native_type(self, data, ptitem, prefix):\n        \"\"\"Checks if loaded data has the type it was stored in. If not converts it.\n\n        :param data: Data item to be checked and converted\n        :param ptitem: HDf5 Node or Leaf from where data was loaded\n        :param prefix: Prefix for recalling the data type from the hdf5 node attributes\n\n        :return:\n\n            Tuple, first item is the (converted) `data` item, second boolean whether\n            item was converted or not.\n\n        \"\"\"\n        typestr = self._all_get_from_attrs(ptitem, prefix + HDF5StorageService.SCALAR_TYPE)\n        colltype = self._all_get_from_attrs(ptitem, prefix + HDF5StorageService.COLL_TYPE)\n        type_changed = False\n\n        # Check what the original data type was from the hdf5 node attributes\n        if colltype == HDF5StorageService.COLL_SCALAR:\n            # Here data item was a scalar\n\n            if isinstance(data, np.ndarray):\n                # If we recall a numpy scalar, pytables loads a 1d array :-/\n                # So we have to change it to a real scalar value\n                data = np.array([data])[0]\n                type_changed = True\n\n            if not typestr is None:\n                # Check if current type and stored type match\n                # if not convert the data\n                if typestr != type(data).__name__:\n\n                    if typestr == str.__name__:\n                        data = data.decode(self._encoding)\n                    else:\n                        try:\n                            data = pypetconstants.PARAMETERTYPEDICT[typestr](data)\n                        except KeyError:\n                            # For compatibility with files from older pypet versions\n                            data = pypetconstants.COMPATPARAMETERTYPEDICT[typestr](data)\n\n                    type_changed = True\n\n        elif (colltype == HDF5StorageService.COLL_TUPLE or\n                        colltype == HDF5StorageService.COLL_LIST):\n            # Here data item was originally a tuple or a list\n\n            if type(data) is not list and type is not tuple:\n                # If the original type cannot be recalled, first convert it to a list\n                type_changed = True\n                data = list(data)\n\n            if len(data) > 0:\n                first_item = data[0]\n                # Check if the type of the first item was conserved\n                if not typestr == type(first_item).__name__:\n\n                    if not isinstance(data, list):\n                        data = list(data)\n\n                    # If type was not conserved we need to convert all items\n                    # in the list or tuple\n                    for idx, item in enumerate(data):\n                        if typestr == str.__name__:\n                            data[idx] = data[idx].decode(self._encoding)\n                        else:\n                            try:\n                                data[idx] = pypetconstants.PARAMETERTYPEDICT[typestr](item)\n                            except KeyError:\n                                # For compatibility with files from older pypet versions:\n                                data[idx] = pypetconstants.COMPATPARAMETERTYPEDICT[typestr](item)\n                        type_changed = True\n\n            if colltype == HDF5StorageService.COLL_TUPLE:\n                # If it was originally a tuple we need to convert it back to tuple\n                if type(data) is not tuple:\n                    data = tuple(data)\n                    type_changed = True\n        elif colltype == HDF5StorageService.COLL_EMPTY_DICT:\n            data = {}\n            type_changed = True\n        elif isinstance(data, np.ndarray):\n\n            if typestr == str.__name__:\n                data = np.core.defchararray.decode(data, self._encoding)\n                type_changed = True\n\n            if colltype == HDF5StorageService.COLL_MATRIX:\n                # Here data item was originally a matrix\n                data = np.matrix(data)\n                type_changed = True\n\n        return data, type_changed"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds or changes a row in a pytable. :param item_name: Name of item, the row is about, only important for throwing errors. :param insert_dict: Dictionary of data that is about to be inserted into the pytables row. :param table: The table to insert or modify a row in :param index: Index of row to be modified. Instead of an index a search condition can be used as well, see below. :param condition: Condition to search for in the table :param condvars: Variables for the search condition :param flags: Flags whether to add, modify, or remove a row in the table", "response": "def _all_add_or_modify_row(self, item_name, insert_dict, table, index=None, condition=None,\n                               condvars=None,\n                               flags=(ADD_ROW, MODIFY_ROW,)):\n        \"\"\"Adds or changes a row in a pytable.\n\n        :param item_name: Name of item, the row is about, only important for throwing errors.\n\n        :param insert_dict:\n\n            Dictionary of data that is about to be inserted into the pytables row.\n\n        :param table:\n\n            The table to insert or modify a row in\n\n        :param index:\n\n            Index of row to be modified. Instead of an index a search condition can be\n            used as well, see below.\n\n        :param condition:\n\n            Condition to search for in the table\n\n        :param condvars:\n\n            Variables for the search condition\n\n        :param flags:\n\n            Flags whether to add, modify, or remove a row in the table\n\n\n        \"\"\"\n        if len(flags) == 0:\n            # No flags means no-op\n            return\n\n        # You can only specify either an index or a condition not both\n        if index is not None and condition is not None:\n            raise ValueError('Please give either a condition or an index or none!')\n        elif condition is not None:\n            row_iterator = table.where(condition, condvars=condvars)\n        elif index is not None:\n            row_iterator = table.iterrows(index, index + 1)\n        else:\n            row_iterator = None\n\n        try:\n            row = next(row_iterator)\n        except TypeError:\n            row = None\n        except StopIteration:\n            row = None\n\n        # multiple_entries = []\n\n        if ((HDF5StorageService.MODIFY_ROW in flags or HDF5StorageService.ADD_ROW in flags) and\n                    HDF5StorageService.REMOVE_ROW in flags):\n            # You cannot remove and modify or add at the same time\n            raise ValueError('You cannot add or modify and remove a row at the same time.')\n\n        if row is None and HDF5StorageService.ADD_ROW in flags:\n            # Here we add a new row\n\n            row = table.row\n\n            self._all_insert_into_row(row, insert_dict)\n\n            row.append()\n\n        elif row is not None and HDF5StorageService.MODIFY_ROW in flags:\n            # Here we modify an existing row\n\n            self._all_insert_into_row(row, insert_dict)\n\n            row.update()\n\n        elif HDF5StorageService.REMOVE_ROW in flags:\n            # Here we delete an existing row\n\n            if row is not None:\n                # Only delete if the row does exist otherwise we do not have to do anything\n                row_number = row.nrow\n                try:\n                    table.remove_rows(start=row_number, stop=row_number+1)\n                except NotImplementedError:\n                    pass\n                    # We get here if we try to remove the last row of a table\n                    # there is nothing we can do but keep it :-(\n        else:\n            raise ValueError('Something is wrong, you might not have found '\n                             'a row, or your flags are not set appropriately')\n\n        self._all_kill_iterator(row_iterator)\n        table.flush()\n\n        if HDF5StorageService.REMOVE_ROW not in flags and row is None:\n            raise RuntimeError('Could not add or modify entries of `%s` in '\n                               'table %s' % (item_name, table._v_name))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncopying data from insert_dict into a pytables row.", "response": "def _all_insert_into_row(self, row, insert_dict):\n        \"\"\"Copies data from `insert_dict` into a pytables `row`.\"\"\"\n        for key, val in insert_dict.items():\n            try:\n                row[key] = val\n            except KeyError as ke:\n                self._logger.warning('Could not write `%s` into a table, ' % key + repr(ke))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _all_extract_insert_dict(self, item, colnames, additional_info=None):\n        insert_dict = {}\n\n        if 'length' in colnames:\n            insert_dict['length'] = len(item)\n\n        if 'comment' in colnames:\n            comment = self._all_cut_string(item.v_comment.encode('utf-8'),\n                                           pypetconstants.HDF5_STRCOL_MAX_COMMENT_LENGTH,\n                                           self._logger)\n\n            insert_dict['comment'] = comment\n\n        if 'location' in colnames:\n            insert_dict['location'] = item.v_location.encode('utf-8')\n\n        if 'name' in colnames:\n            name = item._name if (not item.v_is_root or not item.v_is_run) else item._crun\n            insert_dict['name'] = name.encode('utf-8')\n\n        if 'class_name' in colnames:\n            insert_dict['class_name'] = item.f_get_class_name().encode('utf-8')\n\n        if 'value' in colnames:\n            insert_dict['value'] = self._all_cut_string(\n                item.f_val_to_str().encode('utf-8'),\n                pypetconstants.HDF5_STRCOL_MAX_VALUE_LENGTH,\n                self._logger)\n\n        if 'hexdigest' in colnames:\n            insert_dict['hexdigest'] = additional_info['hexdigest']\n\n        if 'idx' in colnames:\n            insert_dict['idx'] = item.v_idx\n\n        if 'time' in colnames:\n            time_ = item._time\n            insert_dict['time'] = time_.encode('utf-8')\n\n        if 'timestamp' in colnames:\n            timestamp = item._timestamp\n            insert_dict['timestamp'] = timestamp\n\n        if 'range' in colnames:\n            third_length = pypetconstants.HDF5_STRCOL_MAX_RANGE_LENGTH // 3 + 10\n            item_range = itools.islice(item.f_get_range(copy=False), 0, third_length)\n            range_string = ', '.join([repr(x) for x in item_range])\n            insert_dict['range'] = self._all_cut_string(\n                range_string.encode('utf-8'),\n                pypetconstants.HDF5_STRCOL_MAX_RANGE_LENGTH,\n                self._logger)\n\n        # To allow backwards compatibility\n        if 'array' in colnames:\n            third_length = pypetconstants.HDF5_STRCOL_MAX_RANGE_LENGTH // 3 + 10\n            item_range = itools.islice(item.f_get_range(copy=False), 0, third_length)\n            range_string = ', '.join([repr(x) for x in item_range])\n            insert_dict['array'] = self._all_cut_string(\n                range_string.encode('utf-8'),\n                pypetconstants.HDF5_STRCOL_MAX_RANGE_LENGTH,\n                self._logger)\n\n        if 'version' in colnames:\n            insert_dict['version'] = item.v_version.encode('utf-8')\n\n        if 'python' in colnames:\n            insert_dict['python'] = item.v_python.encode('utf-8')\n\n        if 'finish_timestamp' in colnames:\n            insert_dict['finish_timestamp'] = item._finish_timestamp_run\n\n        return insert_dict", "response": "Extracts the information from a given item into a dictionary that can be inserted into a pytable row."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _all_cut_string(string, max_length, logger):\n        if len(string) > max_length:\n            logger.debug('The string `%s` was too long I truncated it to'\n                         ' %d characters' %\n                         (string, max_length))\n            string = string[0:max_length - 3] + '...'.encode('utf-8')\n\n        return string", "response": "Cuts string data to the maximum length allowed in a pytables column\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _all_create_or_get_group(self, name, parent_hdf5_group=None):\n        if not name in parent_hdf5_group:\n            new_hdf5_group = self._hdf5file.create_group(where=parent_hdf5_group,\n                                                      name=name,\n                                                      title=name,\n                                                      filters=self._all_get_filters())\n            return new_hdf5_group, True\n        else:\n            new_hdf5_group = parent_hdf5_group._f_get_child(name)\n            return new_hdf5_group, False", "response": "Creates or returns a group"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates new or follows existing group nodes along a given colon separated key.", "response": "def _all_create_or_get_groups(self, key, start_hdf5_group=None):\n        \"\"\"Creates new or follows existing group nodes along a given colon separated `key`.\n\n        :param key:\n\n            Colon separated path along hdf5 file, e.g. `parameters.mobiles.cars`.\n\n        :param start_hdf5_group:\n\n            HDF5 group from where to start, leave `None` for the trajectory group.\n\n        :return:\n\n            Final group node, e.g. group node with name `cars`.\n\n        \"\"\"\n        if start_hdf5_group is None:\n            newhdf5_group = self._trajectory_group\n        else:\n            newhdf5_group = start_hdf5_group\n        created = False\n        if key == '':\n            return newhdf5_group, created\n        split_key = key.split('.')\n        for name in split_key:\n            newhdf5_group, created = self._all_create_or_get_group(name, newhdf5_group)\n\n        return newhdf5_group, created"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _ann_store_annotations(self, item_with_annotations, node, overwrite=False):\n\n        # If we overwrite delete all annotations first\n        if overwrite is True or overwrite == 'v_annotations':\n            annotated = self._all_get_from_attrs(node, HDF5StorageService.ANNOTATED)\n            if annotated:\n                current_attrs = node._v_attrs\n                for attr_name in current_attrs._v_attrnames:\n                    if attr_name.startswith(HDF5StorageService.ANNOTATION_PREFIX):\n                        delattr(current_attrs, attr_name)\n                delattr(current_attrs, HDF5StorageService.ANNOTATED)\n                self._hdf5file.flush()\n\n        # Only store annotations if the item has some\n        if not item_with_annotations.v_annotations.f_is_empty():\n\n            anno_dict = item_with_annotations.v_annotations._dict\n\n            current_attrs = node._v_attrs\n\n            changed = False\n\n            for field_name in anno_dict:\n                val = anno_dict[field_name]\n\n                field_name_with_prefix = HDF5StorageService.ANNOTATION_PREFIX + field_name\n                if field_name_with_prefix not in current_attrs:\n                    # Only store *new* annotations, if they already exist on disk, skip storage\n                    setattr(current_attrs, field_name_with_prefix, val)\n                    changed = True\n\n            if changed:\n                setattr(current_attrs, HDF5StorageService.ANNOTATED, True)\n                self._hdf5file.flush()", "response": "Stores annotations into an hdf5 file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads annotations from disk.", "response": "def _ann_load_annotations(self, item_with_annotations, node):\n        \"\"\"Loads annotations from disk.\"\"\"\n\n        annotated = self._all_get_from_attrs(node, HDF5StorageService.ANNOTATED)\n\n        if annotated:\n\n            annotations = item_with_annotations.v_annotations\n\n            # You can only load into non-empty annotations, to prevent overwriting data in RAM\n            if not annotations.f_is_empty():\n                raise TypeError('Loading into non-empty annotations!')\n\n            current_attrs = node._v_attrs\n\n            for attr_name in current_attrs._v_attrnames:\n\n                if attr_name.startswith(HDF5StorageService.ANNOTATION_PREFIX):\n                    key = attr_name\n                    key = key.replace(HDF5StorageService.ANNOTATION_PREFIX, '')\n\n                    data = getattr(current_attrs, attr_name)\n                    setattr(annotations, key, data)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _grp_store_group(self, traj_group, store_data=pypetconstants.STORE_DATA,\n                         with_links=True, recursive=False, max_depth=None,\n                         _hdf5_group=None, _newly_created=False):\n        \"\"\"Stores a group node.\n\n        For group nodes only annotations and comments need to be stored.\n\n        \"\"\"\n        if store_data == pypetconstants.STORE_NOTHING:\n            return\n        elif store_data == pypetconstants.STORE_DATA_SKIPPING and traj_group._stored:\n            self._logger.debug('Already found `%s` on disk I will not store it!' %\n                                   traj_group.v_full_name)\n        elif not recursive:\n            if _hdf5_group is None:\n                _hdf5_group, _newly_created = self._all_create_or_get_groups(traj_group.v_full_name)\n\n            overwrite = store_data == pypetconstants.OVERWRITE_DATA\n\n            if (traj_group.v_comment != '' and\n                    (HDF5StorageService.COMMENT not in _hdf5_group._v_attrs or overwrite)):\n                setattr(_hdf5_group._v_attrs, HDF5StorageService.COMMENT, traj_group.v_comment)\n\n            if ((_newly_created or overwrite) and\n                type(traj_group) not in (nn.NNGroupNode, nn.ConfigGroup, nn.ParameterGroup,\n                                             nn.DerivedParameterGroup, nn.ResultGroup)):\n                # We only store the name of the class if it is not one of the standard groups,\n                # that are always used.\n                setattr(_hdf5_group._v_attrs, HDF5StorageService.CLASS_NAME,\n                        traj_group.f_get_class_name())\n\n            self._ann_store_annotations(traj_group, _hdf5_group, overwrite=overwrite)\n            self._hdf5file.flush()\n            traj_group._stored = True\n\n            # Signal completed node loading\n            self._node_processing_timer.signal_update()\n\n        if recursive:\n            parent_traj_group = traj_group.f_get_parent()\n            parent_hdf5_group = self._all_create_or_get_groups(parent_traj_group.v_full_name)[0]\n\n            self._tree_store_nodes_dfs(parent_traj_group, traj_group.v_name, store_data=store_data,\n                                       with_links=with_links, recursive=recursive,\n                                       max_depth=max_depth, current_depth=0,\n                                       parent_hdf5_group=parent_hdf5_group)", "response": "Stores a group node."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads a group node and potentially everything below", "response": "def _grp_load_group(self, traj_group, load_data=pypetconstants.LOAD_DATA, with_links=True,\n                        recursive=False, max_depth=None,\n                        _traj=None, _as_new=False, _hdf5_group=None):\n        \"\"\"Loads a group node and potentially everything recursively below\"\"\"\n        if _hdf5_group is None:\n            _hdf5_group = self._all_get_node_by_name(traj_group.v_full_name)\n            _traj = traj_group.v_root\n\n        if recursive:\n            parent_traj_node = traj_group.f_get_parent()\n            self._tree_load_nodes_dfs(parent_traj_node, load_data=load_data, with_links=with_links,\n                                  recursive=recursive, max_depth=max_depth,\n                                  current_depth=0,\n                                  trajectory=_traj, as_new=_as_new,\n                                  hdf5_group=_hdf5_group)\n        else:\n            if load_data == pypetconstants.LOAD_NOTHING:\n                return\n\n            elif load_data == pypetconstants.OVERWRITE_DATA:\n                traj_group.v_annotations.f_empty()\n                traj_group.v_comment = ''\n\n            self._all_load_skeleton(traj_group, _hdf5_group)\n            traj_group._stored = not _as_new\n\n            # Signal completed node loading\n            self._node_processing_timer.signal_update()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreload skeleton data of a tree node", "response": "def _all_load_skeleton(self, traj_node, hdf5_group):\n        \"\"\"Reloads skeleton data of a tree node\"\"\"\n        if traj_node.v_annotations.f_is_empty():\n            self._ann_load_annotations(traj_node, hdf5_group)\n        if traj_node.v_comment == '':\n            comment = self._all_get_from_attrs(hdf5_group, HDF5StorageService.COMMENT)\n            if comment is None:\n                comment = ''\n            traj_node.v_comment = comment"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _prm_extract_missing_flags(data_dict, flags_dict):\n        for key, data in data_dict.items():\n            if not key in flags_dict:\n                dtype = type(data)\n                if (dtype is np.ndarray or dtype is dict) and len(data) == 0:\n                        # Empty containers are stored as an Array\n                        # No need to ask for tuple or list, because they are always\n                        # stored as arrays.\n                        flags_dict[key] = HDF5StorageService.ARRAY\n                        continue\n                else:\n                    try:\n                        flags_dict[key] = HDF5StorageService.TYPE_FLAG_MAPPING[dtype]\n                    except KeyError:\n                        raise pex.NoSuchServiceError('I cannot store `%s`, I do not understand the'\n                                                     'type `%s`.' % (key, str(dtype)))", "response": "Extracts missing flags for data in data_dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds data to the summary tables and returns if the comment has to be stored.", "response": "def _prm_meta_add_summary(self, instance):\n        \"\"\"Adds data to the summary tables and returns if `instance`s comment has to be stored.\n\n        Also moves comments upwards in the hierarchy if purge_duplicate_comments is true\n        and a lower index run has completed. Only necessary for *multiprocessing*.\n\n        :return: Tuple\n\n            * String specifying the subtree\n\n            * Boolean whether to store the comment to `instance`s hdf5 node\n\n        \"\"\"\n        if instance.v_comment == '':\n            return False\n\n        where = instance.v_branch\n        definitely_store_comment = True\n\n        # Get the hexdigest of the comment to see if such a comment has been stored before\n        bytes_comment = instance.v_comment.encode('utf-8')\n        hexdigest = hashlib.sha1(bytes_comment).hexdigest()\n        hexdigest = hexdigest.encode('utf-8')\n\n        # Get the overview table\n        table_name = where + '_summary'\n\n        # Check if the overview table exists, otherwise skip the rest of\n        # the meta adding\n        if table_name in self._overview_group:\n            table = getattr(self._overview_group, table_name)\n        else:\n            return definitely_store_comment\n\n\n        try:\n            condvars = {'hexdigestcol': table.cols.hexdigest,\n                'hexdigest': hexdigest}\n\n            condition = \"\"\"(hexdigestcol == hexdigest)\"\"\"\n\n            row_iterator = table.where(condition, condvars=condvars)\n\n            row = None\n            try:\n                row = next(row_iterator)\n            except StopIteration:\n                pass\n\n            if row is None:\n                self._all_store_param_or_result_table_entry(instance, table,\n                                                            flags=(\n                                                                HDF5StorageService.ADD_ROW,),\n                                                            additional_info={\n                                                                'hexdigest': hexdigest})\n\n                definitely_store_comment = True\n            else:\n                definitely_store_comment = False\n                self._all_kill_iterator(row_iterator)\n\n        except pt.NoSuchNodeError:\n            definitely_store_comment = True\n\n        return definitely_store_comment"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _prm_add_meta_info(self, instance, group, overwrite=False):\n\n        if overwrite:\n            flags = ()\n        else:\n            flags = (HDF5StorageService.ADD_ROW,)\n\n        definitely_store_comment = True\n        try:\n            # Check if we need to store the comment. Maybe update the overview tables\n            # accordingly if the current run index is lower than the one in the table.\n            definitely_store_comment = self._prm_meta_add_summary(instance)\n\n            try:\n                # Update the summary overview table\n                table_name = instance.v_branch + '_overview'\n\n                table = getattr(self._overview_group, table_name)\n                if len(table) < pypetconstants.HDF5_MAX_OVERVIEW_TABLE_LENGTH:\n\n                    self._all_store_param_or_result_table_entry(instance, table,\n                                                                flags=flags)\n            except pt.NoSuchNodeError:\n                pass\n        except Exception as exc:\n            self._logger.error('Could not store information table due to `%s`.' % repr(exc))\n\n        if ((not self._purge_duplicate_comments or definitely_store_comment) and\n                    instance.v_comment != ''):\n            # Only add the comment if necessary\n            setattr(group._v_attrs, HDF5StorageService.COMMENT, instance.v_comment)\n\n        # Add class name and whether node is a leaf to the HDF5 attributes\n        setattr(group._v_attrs, HDF5StorageService.CLASS_NAME, instance.f_get_class_name())\n        setattr(group._v_attrs, HDF5StorageService.LEAF, True)\n\n        if instance.v_is_parameter and instance.v_explored:\n            # If the stored parameter was an explored one we need to mark this in the\n            # explored overview table\n            try:\n                tablename = 'explored_parameters_overview'\n                table = getattr(self._overview_group, tablename)\n\n                if len(table) < pypetconstants.HDF5_MAX_OVERVIEW_TABLE_LENGTH:\n                    self._all_store_param_or_result_table_entry(instance, table,\n                                                                flags=flags)\n            except pt.NoSuchNodeError:\n                pass\n            except Exception as exc:\n                self._logger.error('Could not store information '\n                                   'table due to `%s`.' % repr(exc))", "response": "Adds information to overview tables and meta information to HDF5 group."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _prm_store_from_dict(self, fullname, store_dict, hdf5_group, store_flags, kwargs):\n        for key, data_to_store in store_dict.items():\n            # self._logger.log(1, 'SUB-Storing %s [%s]', key, str(store_dict[key]))\n            original_hdf5_group = None\n\n            flag = store_flags[key]\n\n            if '.' in key:\n                original_hdf5_group = hdf5_group\n                split_key = key.split('.')\n                key = split_key.pop()\n                for inner_key in split_key:\n                    hdf5_group, newly_created =  self._all_create_or_get_group(inner_key,\n                                                                                hdf5_group)\n                    if newly_created:\n                        setattr(hdf5_group._v_attrs, HDF5StorageService.STORAGE_TYPE,\n                                HDF5StorageService.NESTED_GROUP)\n                    else:\n                        store_type = self._all_get_from_attrs(hdf5_group, HDF5StorageService.STORAGE_TYPE)\n                        if store_type != HDF5StorageService.NESTED_GROUP:\n                            raise ValueError('You want to nested results but `%s` is already '\n                                             'of type `%s`!' % (hdf5_group._v_name, store_type))\n\n            # Iterate through the data and store according to the storage flags\n            if key in hdf5_group:\n                # We won't change any data that is found on disk\n                self._logger.debug(\n                    'Found %s already in hdf5 node of %s, so I will ignore it.' %\n                    (key, fullname))\n                continue\n\n            if flag == HDF5StorageService.TABLE:\n                # self._logger.log(1, 'SUB-Storing %s TABLE', key)\n                self._prm_write_into_pytable(key, data_to_store, hdf5_group, fullname,\n                                             **kwargs)\n            elif flag == HDF5StorageService.DICT:\n                # self._logger.log(1, 'SUB-Storing %s DICT', key)\n                self._prm_write_dict_as_table(key, data_to_store, hdf5_group, fullname,\n                                              **kwargs)\n            elif flag == HDF5StorageService.ARRAY:\n                # self._logger.log(1, 'SUB-Storing %s ARRAY', key)\n                self._prm_write_into_array(key, data_to_store, hdf5_group, fullname,\n                                           **kwargs)\n            elif flag in (HDF5StorageService.CARRAY,\n                          HDF5StorageService.EARRAY,\n                          HDF5StorageService.VLARRAY):\n                self._prm_write_into_other_array(key, data_to_store,\n                                                 hdf5_group, fullname,\n                                                 flag=flag, **kwargs)\n            elif flag in (HDF5StorageService.SERIES,\n                          HDF5StorageService.FRAME,\n                          #  HDF5StorageService.PANEL\n                          ):\n                # self._logger.log(1, 'SUB-Storing %s PANDAS', key)\n                self._prm_write_pandas_data(key, data_to_store, hdf5_group, fullname,\n                                            flag, **kwargs)\n            elif flag == HDF5StorageService.SHARED_DATA:\n                pass  # Shared data needs to be explicitly created and is not stored on\n                # the fly\n            else:\n                raise RuntimeError('You shall not pass!')\n\n            if original_hdf5_group is not None:\n                hdf5_group = original_hdf5_group", "response": "Stores a dictionary of data into the HDF5 group."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _prm_store_parameter_or_result(self,\n                                       instance,\n                                       store_data=pypetconstants.STORE_DATA,\n                                       store_flags=None,\n                                       overwrite=None,\n                                       with_links=False,\n                                       recursive=False,\n                                       _hdf5_group=None,\n                                       _newly_created=False,\n                                       **kwargs):\n        \"\"\"Stores a parameter or result to hdf5.\n\n        :param instance:\n\n            The instance to be stored\n\n        :param store_data:\n\n            How to store data\n\n        :param store_flags:\n\n            Dictionary containing how to store individual data, usually empty.\n\n        :param overwrite:\n\n            Instructions how to overwrite data\n\n        :param with_links:\n\n            Placeholder because leaves have no links\n\n        :param recursive:\n\n            Placeholder, because leaves have no children\n\n        :param _hdf5_group:\n\n            The hdf5 group for storing the parameter or result\n\n        :param _newly_created:\n\n            If should be created in a new form\n\n        \"\"\"\n        if store_data == pypetconstants.STORE_NOTHING:\n            return\n        elif store_data == pypetconstants.STORE_DATA_SKIPPING and instance._stored:\n            self._logger.debug('Already found `%s` on disk I will not store it!' %\n                                   instance.v_full_name)\n            return\n        elif store_data == pypetconstants.OVERWRITE_DATA:\n            if not overwrite:\n                overwrite = True\n\n        fullname = instance.v_full_name\n        self._logger.debug('Storing `%s`.' % fullname)\n\n        if _hdf5_group is None:\n            # If no group is provided we might need to create one\n            _hdf5_group, _newly_created = self._all_create_or_get_groups(fullname)\n\n        # kwargs_flags = {} # Dictionary to change settings\n        # old_kwargs = {}\n        store_dict = {}\n        # If the user did not supply storage flags, we need to set it to the empty dictionary\n        if store_flags is None:\n            store_flags = {}\n\n        try:\n            # Get the data to store from the instance\n            if not instance.f_is_empty():\n                store_dict = instance._store()\n            try:\n                # Ask the instance for storage flags\n                instance_flags = instance._store_flags().copy() # copy to avoid modifying the\n                # original data\n            except AttributeError:\n                # If it does not provide any, set it to the empty dictionary\n                instance_flags = {}\n\n            # User specified flags have priority over the flags from the instance\n            instance_flags.update(store_flags)\n            store_flags = instance_flags\n\n            # If we still have data in `store_dict` about which we do not know how to store\n            # it, pick default storage flags\n            self._prm_extract_missing_flags(store_dict, store_flags)\n\n            if overwrite:\n                if isinstance(overwrite, str):\n                    overwrite = [overwrite]\n\n                if overwrite is True:\n                    to_delete = [key for key in store_dict.keys() if key in _hdf5_group]\n                    self._all_delete_parameter_or_result_or_group(instance,\n                                                                  delete_only=to_delete,\n                                                                  _hdf5_group=_hdf5_group)\n                elif isinstance(overwrite, (list, tuple)):\n                    overwrite_set = set(overwrite)\n                    key_set = set(store_dict.keys())\n\n                    stuff_not_to_be_overwritten = overwrite_set - key_set\n\n                    if overwrite!='v_annotations' and len(stuff_not_to_be_overwritten) > 0:\n                        self._logger.warning('Cannot overwrite `%s`, these items are not supposed to '\n                                             'be stored by the leaf node.' %\n                                             str(stuff_not_to_be_overwritten))\n\n                    stuff_to_overwrite = overwrite_set & key_set\n                    if len(stuff_to_overwrite) > 0:\n                        self._all_delete_parameter_or_result_or_group(instance,\n                                                                      delete_only=list(\n                                                                          stuff_to_overwrite))\n                else:\n                    raise ValueError('Your value of overwrite `%s` is not understood. '\n                                     'Please pass `True` of a list of strings to fine grain '\n                                     'overwriting.' % str(overwrite))\n\n            self._prm_store_from_dict(fullname, store_dict, _hdf5_group, store_flags, kwargs)\n\n            # Store annotations\n            self._ann_store_annotations(instance, _hdf5_group, overwrite=overwrite)\n\n            if _newly_created or overwrite is True:\n                # If we created a new group or the parameter was extended we need to\n                # update the meta information and summary tables\n                self._prm_add_meta_info(instance, _hdf5_group,\n                                        overwrite=not _newly_created)\n\n            instance._stored = True\n\n            #self._logger.debug('Finished Storing `%s`.' % fullname)\n            # Signal completed node loading\n            self._node_processing_timer.signal_update()\n\n        except:\n            # I anything fails, we want to remove the data of the parameter again\n            self._logger.error(\n                'Failed storing leaf `%s`. I will remove the hdf5 data I added  again.' % fullname)\n            # Delete data\n            for key in store_dict.keys():\n                if key in  _hdf5_group:\n                    hdf5_child = _hdf5_group._f_get_child(key)\n                    hdf5_child._f_remove(recursive=True)\n            # If no data left delete the whole parameter\n            if _hdf5_group._v_nchildren == 0:\n                _hdf5_group._f_remove(recursive=True)\n            raise", "response": "Stores a parameter or result in the hdf5."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads a DataFrame from the pandas store and selects the shared data.", "response": "def _prm_select_shared_pandas_data(self, pd_node, full_name, **kwargs):\n        \"\"\"Reads a DataFrame from dis.\n\n        :param pd_node:\n\n            hdf5 node storing the pandas DataFrame\n\n        :param full_name:\n\n            Full name of the parameter or result whose data is to be loaded\n\n        :param kwargs:\n\n            Arguments passed to pandas' select method\n\n        \"\"\"\n        try:\n            pathname = pd_node._v_pathname\n            pandas_store = self._hdf5store\n            return pandas_store.select(pathname, **kwargs)\n        except:\n            self._logger.error('Failed loading `%s` of `%s`.' % (pd_node._v_name, full_name))\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _prm_write_shared_array(self, key, data, hdf5_group, full_name, flag, **kwargs):\n\n        if flag == HDF5StorageService.ARRAY:\n            self._prm_write_into_array(key, data, hdf5_group, full_name, **kwargs)\n        elif flag in (HDF5StorageService.CARRAY,\n                    HDF5StorageService.EARRAY,\n                    HDF5StorageService.VLARRAY):\n            self._prm_write_into_other_array(key, data, hdf5_group, full_name,\n                                             flag=flag, **kwargs)\n        else:\n            raise RuntimeError('Flag `%s` of hdf5 data `%s` of `%s` not understood' %\n                               (flag, key, full_name))\n\n        self._hdf5file.flush()", "response": "Creates and array that can be used with an HDF5 array object"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _prm_write_shared_table(self, key, hdf5_group, fullname, **kwargs):\n        first_row = None\n        description = None\n        if 'first_row' in kwargs:\n            first_row = kwargs.pop('first_row')\n            if not 'description' in kwargs:\n                description = {}\n                for colname in first_row:\n                    data = first_row[colname]\n                    column = self._all_get_table_col(key, [data], fullname)\n                    description[colname] = column\n\n        if 'description' in kwargs:\n            description = kwargs.pop('description')\n\n        if 'filters' in kwargs:\n            filters = kwargs.pop('filters')\n        else:\n            filters = self._all_get_filters(kwargs)\n\n        table = self._hdf5file.create_table(where=hdf5_group, name=key,\n                                          description=description,\n                                          filters=filters,\n                                          **kwargs)\n        table.flush()\n\n        if first_row is not None:\n            row = table.row\n            for key in description:\n                row[key] = first_row[key]\n\n            row.append()\n            table.flush()", "response": "Writes a shared table to the file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _prm_write_dict_as_table(self, key, data_to_store, group, fullname, **kwargs):\n        if key in group:\n            raise ValueError(\n                'Dictionary `%s` already exists in `%s`. Appending is not supported (yet).')\n\n        if key in group:\n            raise ValueError('Dict `%s` already exists in `%s`. Appending is not supported (yet).')\n\n        temp_dict = {}\n        for innerkey in data_to_store:\n            val = data_to_store[innerkey]\n            temp_dict[innerkey] = [val]\n\n        # Convert dictionary to object table\n        objtable = ObjectTable(data=temp_dict)\n\n        # Then store the object table\n        self._prm_write_into_pytable(key, objtable, group, fullname, **kwargs)\n\n        new_table = group._f_get_child(key)\n\n        # Remember that the Object Table represents a dictionary\n        self._all_set_attributes_to_recall_natives(temp_dict, new_table,\n                                                   HDF5StorageService.DATA_PREFIX)\n\n        setattr(new_table._v_attrs, HDF5StorageService.STORAGE_TYPE,\n                HDF5StorageService.DICT)\n\n        self._hdf5file.flush()", "response": "Stores a python dictionary into an object table and stores it into the HDF5 file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _prm_write_pandas_data(self, key, data, group, fullname, flag, **kwargs):\n        try:\n            if 'filters' not in kwargs:\n                filters = self._all_get_filters(kwargs)\n                kwargs['filters'] = filters\n            if 'format' not in kwargs:\n                kwargs['format'] = self.pandas_format\n            if 'encoding' not in kwargs:\n                kwargs['encoding'] = self.encoding\n\n            overwrite = kwargs.pop('overwrite', False)\n\n            if key in group and not (overwrite or kwargs.get('append', False)):\n                raise ValueError(\n                    'DataFrame `%s` already exists in `%s`. '\n                    'To append pass ``append=`True```.' % (key, fullname))\n            else:\n                self._logger.debug('Appending to pandas data `%s` in `%s`' % (key, fullname))\n\n            if data is not None and (kwargs['format'] == 'f' or kwargs['format'] == 'fixed'):\n                kwargs['expectedrows'] = data.shape[0]\n\n            name = group._v_pathname + '/' + key\n            self._hdf5store.put(name, data, **kwargs)\n            self._hdf5store.flush()\n            self._hdf5file.flush()\n\n            frame_group = group._f_get_child(key)\n            setattr(frame_group._v_attrs, HDF5StorageService.STORAGE_TYPE, flag)\n            self._hdf5file.flush()\n\n        except:\n            self._logger.error('Failed storing pandas data `%s` of `%s`.' % (key, fullname))\n            raise", "response": "Stores a pandas DataFrame into hdf5 file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstoring data into the other array.", "response": "def _prm_write_into_other_array(self, key, data, group, fullname,\n                                    flag, **kwargs):\n        \"\"\"Stores data as carray, earray or vlarray depending on `flag`.\n\n        :param key:\n\n            Name of data item to store\n\n        :param data:\n\n            Data to store\n\n        :param group:\n\n            Group node where to store data in hdf5 file\n\n        :param fullname:\n\n            Full name of the `data_to_store`s original container, only needed for throwing errors.\n\n        :param recall:\n\n            If container type and data type for perfect recall should be stored\n\n        :param flag:\n\n            How to store:\n                CARRAY, EARRAY, VLARRAY\n\n        \"\"\"\n        try:\n\n            if flag == HDF5StorageService.CARRAY:\n                factory = self._hdf5file.create_carray\n            elif flag == HDF5StorageService.EARRAY:\n                factory = self._hdf5file.create_earray\n            elif flag == HDF5StorageService.VLARRAY:\n                factory = self._hdf5file.create_vlarray\n            else:\n                raise RuntimeError('You shall not pass!')\n\n            if key in group:\n                raise ValueError(\n                    'CArray `%s` already exists in `%s`. Appending is not supported (yet).')\n\n            if 'filters' in kwargs:\n                filters = kwargs.pop('filters')\n            else:\n                filters = self._all_get_filters(kwargs)\n\n            try:\n                other_array = factory(where=group, name=key, obj=data,\n                                                filters=filters, **kwargs)\n            except (ValueError, TypeError) as exc:\n                try:\n                    conv_data = data[:]\n                    conv_data = np.core.defchararray.encode(conv_data, self.encoding)\n                    other_array = factory(where=group, name=key,\n                                                obj=conv_data,\n                                                filters=filters, **kwargs)\n                except Exception:\n                    # Re-raise original Error\n                    raise exc\n\n            if data is not None:\n                # Remember the types of the original data to recall them on loading\n                self._all_set_attributes_to_recall_natives(data, other_array,\n                                                       HDF5StorageService.DATA_PREFIX)\n            setattr(other_array._v_attrs, HDF5StorageService.STORAGE_TYPE, flag)\n            self._hdf5file.flush()\n        except:\n            self._logger.error('Failed storing %s `%s` of `%s`.' % (flag, key, fullname))\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _prm_write_into_array(self, key, data, group, fullname, **kwargs):\n\n        try:\n            if key in group:\n                raise ValueError(\n                    'Array `%s` already exists in `%s`. Appending is not supported (yet).')\n\n            try:\n\n                array = self._hdf5file.create_array(where=group,\n                                              name=key, obj=data, **kwargs)\n            except (TypeError, ValueError) as exc:\n                try:\n                    if type(data) is dict and len(data) == 0:\n                        # We cannot store an empty dictionary,\n                        # but we can use an empty tuple as a dummy.\n                        conv_data = ()\n                    elif isinstance(data, str):\n                        conv_data = data.encode(self._encoding)\n                    elif isinstance(data, int):\n                        conv_data = np.int64(data)\n                    else:\n                        conv_data = []\n                        for string in data:\n                            conv_data.append(string.encode(self._encoding))\n                    array = self._hdf5file.create_array(where=group,\n                                                  name=key, obj=conv_data, **kwargs)\n                except Exception:\n                    # Re-raise original error\n                    raise exc\n\n            if data is not None:\n                # Remember the types of the original data to recall them on loading\n                self._all_set_attributes_to_recall_natives(data, array,\n                                                           HDF5StorageService.DATA_PREFIX)\n            setattr(array._v_attrs, HDF5StorageService.STORAGE_TYPE,\n                    HDF5StorageService.ARRAY)\n            self._hdf5file.flush()\n        except:\n            self._logger.error('Failed storing array `%s` of `%s`.' % (key, fullname))\n            raise", "response": "Stores data into an array."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _lnk_delete_link(self, link_name):\n        translated_name = '/' + self._trajectory_name + '/' + link_name.replace('.','/')\n        link = self._hdf5file.get_node(where=translated_name)\n        link._f_remove()", "response": "Removes a link from disk"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _all_delete_parameter_or_result_or_group(self, instance,\n                                                 delete_only=None,\n                                                 remove_from_item=False,\n                                                 recursive=False,\n                                                 _hdf5_group=None):\n        \"\"\"Removes a parameter or result or group from the hdf5 file.\n\n        :param instance: Instance to be removed\n\n        :param delete_only:\n\n            List of elements if you only want to delete parts of a leaf node. Note that this\n            needs to list the names of the hdf5 subnodes. BE CAREFUL if you erase parts of a leaf.\n            Erasing partly happens at your own risk, it might be the case that you can\n            no longer reconstruct the leaf from the leftovers!\n\n        :param remove_from_item:\n\n            If using `delete_only` and `remove_from_item=True` after deletion the data item is\n            also removed from the `instance`.\n\n        :param recursive:\n\n            If a group node has children, you will can delete it if recursive is True.\n\n\n        \"\"\"\n        split_name = instance.v_location.split('.')\n        if _hdf5_group is None:\n            where = '/' + self._trajectory_name + '/' + '/'.join(split_name)\n            node_name = instance.v_name\n            _hdf5_group = self._hdf5file.get_node(where=where, name=node_name)\n\n        if delete_only is None:\n            if instance.v_is_group and not recursive and len(_hdf5_group._v_children) != 0:\n                    raise TypeError('You cannot remove the group `%s`, it has children, please '\n                                    'use `recursive=True` to enforce removal.' %\n                                    instance.v_full_name)\n            _hdf5_group._f_remove(recursive=True)\n        else:\n            if not instance.v_is_leaf:\n                raise ValueError('You can only choose `delete_only` mode for leafs.')\n\n            if isinstance(delete_only, str):\n                delete_only = [delete_only]\n\n            for delete_item in delete_only:\n                if (remove_from_item and\n                        hasattr(instance, '__contains__') and\n                        hasattr(instance, '__delattr__') and\n                            delete_item in instance):\n                    delattr(instance, delete_item)\n                try:\n                    _hdf5_sub_group = self._hdf5file.get_node(where=_hdf5_group,\n                                                        name=delete_item)\n                    _hdf5_sub_group._f_remove(recursive=True)\n                except pt.NoSuchNodeError:\n                    self._logger.warning('Could not delete `%s` from `%s`. HDF5 node not found!' %\n                                         (delete_item, instance.v_full_name))", "response": "Removes a parameter or result or group from the hdf5 file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _prm_write_into_pytable(self, tablename, data, hdf5_group, fullname, **kwargs):\n\n        datasize = data.shape[0]\n\n        try:\n\n            # Get a new pytables description from the data and create a new table\n            description_dict, data_type_dict = self._prm_make_description(data, fullname)\n            description_dicts = [{}]\n\n            if len(description_dict) > ptpa.MAX_COLUMNS:\n                # For optimization we want to store the original data types into another table\n                # and split the tables into several ones\n                new_table_group = self._hdf5file.create_group(where=hdf5_group,\n                                                    name=tablename,\n                                                    filters=self._all_get_filters(kwargs.copy()))\n\n                count = 0\n                for innerkey in description_dict:\n                    val = description_dict[innerkey]\n                    if count == ptpa.MAX_COLUMNS:\n                        description_dicts.append({})\n                        count = 0\n                    description_dicts[-1][innerkey] = val\n                    count += 1\n\n                setattr(new_table_group._v_attrs, HDF5StorageService.STORAGE_TYPE,\n                        HDF5StorageService.TABLE)\n                setattr(new_table_group._v_attrs, HDF5StorageService.SPLIT_TABLE, 1)\n\n                hdf5_group = new_table_group\n            else:\n                description_dicts = [description_dict]\n\n            for idx, descr_dict in enumerate(description_dicts):\n\n                if idx == 0:\n                    tblname = tablename\n                else:\n                    tblname = tablename + '_%d' % idx\n\n                table = self._hdf5file.create_table(where=hdf5_group, name=tblname,\n                                              description=descr_dict,\n                                              title=tblname,\n                                              expectedrows=datasize,\n                                              filters=self._all_get_filters(kwargs.copy()))\n\n                row = table.row\n                for n in range(datasize):\n                    # Fill the columns with data, note if the parameter was extended nstart!=0\n\n                    for key in descr_dict:\n                        row[key] = data[key][n]\n\n                    row.append()\n\n                # Remember the original types of the data for perfect recall\n                if idx == 0 and len(description_dict) <= ptpa.MAX_COLUMNS:\n                    # We only have a single table and\n                    # we can store the original data types as attributes\n                    for field_name in data_type_dict:\n                        type_description = data_type_dict[field_name]\n                        self._all_set_attr(table, field_name, type_description)\n\n                    setattr(table._v_attrs, HDF5StorageService.STORAGE_TYPE,\n                            HDF5StorageService.TABLE)\n\n                table.flush()\n                self._hdf5file.flush()\n\n            if len(description_dict) > ptpa.MAX_COLUMNS:\n                # We have potentially many split tables and the data types are\n                # stored into an additional table for performance reasons\n                tblname = tablename + '__' + HDF5StorageService.STORAGE_TYPE\n                field_names, data_types = list(zip(*data_type_dict.items()))\n                data_type_table_dict = {'field_name': field_names, 'data_type': data_types}\n                descr_dict, _ = self._prm_make_description(data_type_table_dict, fullname)\n\n                table = self._hdf5file.create_table(where=hdf5_group, name=tblname,\n                                              description=descr_dict,\n                                              title=tblname,\n                                              expectedrows=len(field_names),\n                                              filters=self._all_get_filters(kwargs))\n\n                row = table.row\n\n                for n in range(len(field_names)):\n                    # Fill the columns with data\n\n                    for key in data_type_table_dict:\n                        row[key] = data_type_table_dict[key][n]\n\n                    row.append()\n\n                setattr(table._v_attrs, HDF5StorageService.DATATYPE_TABLE, 1)\n\n                table.flush()\n                self._hdf5file.flush()\n\n        except:\n            self._logger.error('Failed storing table `%s` of `%s`.' % (tablename, fullname))\n            raise", "response": "Stores data into a pytable."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a description dictionary for a pytables table creation", "response": "def _prm_make_description(self, data, fullname):\n        \"\"\" Returns a description dictionary for pytables table creation\"\"\"\n\n        def _convert_lists_and_tuples(series_of_data):\n            \"\"\"Converts lists and tuples to numpy arrays\"\"\"\n            if isinstance(series_of_data[0],\n                          (list, tuple)):  # and not isinstance(series_of_data[0], np.ndarray):\n                # If the first data item is a list, the rest must be as well, since\n                # data has to be homogeneous\n                for idx, item in enumerate(series_of_data):\n                    series_of_data[idx] = np.array(item)\n\n        descriptiondict = {}  # dictionary containing the description to build a pytables table\n        original_data_type_dict = {}  # dictionary containing the original data types\n\n        for key in data:\n            val = data[key]\n\n\n            # remember the original data types\n            self._all_set_attributes_to_recall_natives(val[0], PTItemMock(original_data_type_dict),\n                                                       HDF5StorageService.FORMATTED_COLUMN_PREFIX %\n                                                       key)\n\n            _convert_lists_and_tuples(val)\n\n            # get a pytables column from the data\n            col = self._all_get_table_col(key, val, fullname)\n\n            descriptiondict[key] = col\n\n        return descriptiondict, original_data_type_dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a pytables column instance.", "response": "def _all_get_table_col(self, key, column, fullname):\n        \"\"\" Creates a pytables column instance.\n\n        The type of column depends on the type of `column[0]`.\n        Note that data in `column` must be homogeneous!\n\n        \"\"\"\n        val = column[0]\n\n        try:\n\n            # # We do not want to loose int_\n            if type(val) is int:\n                return pt.IntCol()\n\n            if isinstance(val, (str, bytes)):\n                itemsize = int(self._prm_get_longest_stringsize(column))\n                return pt.StringCol(itemsize)\n\n            if isinstance(val, np.ndarray):\n                if (np.issubdtype(val.dtype, str) or\n                        np.issubdtype(val.dtype, bytes)):\n                    itemsize = int(self._prm_get_longest_stringsize(column))\n                    return pt.StringCol(itemsize, shape=val.shape)\n                else:\n                    return pt.Col.from_dtype(np.dtype((val.dtype, val.shape)))\n            else:\n                return pt.Col.from_dtype(np.dtype(type(val)))\n        except Exception:\n            self._logger.error('Failure in storing `%s` of Parameter/Result `%s`.'\n                               ' Its type was `%s`.' % (key, fullname, repr(type(val))))\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _prm_get_longest_stringsize(string_list):\n        maxlength = 1\n\n        for stringar in string_list:\n            if isinstance(stringar, np.ndarray):\n                if stringar.ndim > 0:\n                    for string in stringar.ravel():\n                        maxlength = max(len(string), maxlength)\n                else:\n                    maxlength = max(len(stringar.tolist()), maxlength)\n            else:\n                maxlength = max(len(stringar), maxlength)\n\n        # Make the string Col longer than needed in order to allow later on slightly larger strings\n        return int(maxlength * 1.5)", "response": "Returns the longest string size for a string entry across data."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _prm_load_into_dict(self, full_name, load_dict, hdf5_group, instance,\n                            load_only, load_except, load_flags, _prefix = ''):\n        \"\"\"Loads into dictionary\"\"\"\n        for node in hdf5_group:\n\n            load_type = self._all_get_from_attrs(node, HDF5StorageService.STORAGE_TYPE)\n\n            if _prefix:\n                load_name = '%s.%s' % (_prefix, node._v_name)\n            else:\n                load_name = node._v_name\n\n            if load_type == HDF5StorageService.NESTED_GROUP:\n                self._prm_load_into_dict(full_name=full_name,\n                                         load_dict=load_dict,\n                                         hdf5_group=node,\n                                         instance=instance,\n                                         load_only=load_only,\n                                         load_except=load_except,\n                                         load_flags=load_flags,\n                                         _prefix=load_name)\n                continue\n\n            if load_only is not None:\n\n                if load_name not in load_only:\n                    continue\n                else:\n                    load_only.remove(load_name)\n\n            elif load_except is not None:\n                if load_name in load_except:\n                    load_except.remove(load_name)\n                    continue\n\n            # Recall from the hdf5 node attributes how the data was stored and reload accordingly\n            if load_name in load_flags:\n                load_type = load_flags[load_name]\n\n            if load_type == HDF5StorageService.DICT:\n                to_load = self._prm_read_dictionary(node, full_name)\n            elif load_type == HDF5StorageService.TABLE:\n                to_load = self._prm_read_table(node, full_name)\n            elif load_type in (HDF5StorageService.ARRAY, HDF5StorageService.CARRAY,\n                                HDF5StorageService.EARRAY, HDF5StorageService.VLARRAY):\n                to_load = self._prm_read_array(node, full_name)\n            elif load_type in (HDF5StorageService.FRAME,\n                               HDF5StorageService.SERIES,\n                               #  HDF5StorageService.PANEL\n                               ):\n                to_load = self._prm_read_pandas(node, full_name)\n            elif load_type.startswith(HDF5StorageService.SHARED_DATA):\n                to_load = self._prm_read_shared_data(node, instance)\n            else:\n                raise pex.NoSuchServiceError('Cannot load %s, do not understand the hdf5 file '\n                                             'structure of %s [%s].' %\n                                             (full_name, str(node), str(load_type)))\n\n            if to_load is None:\n                raise RuntimeError('You shall not pass!')\n\n            load_dict[load_name] = to_load", "response": "Loads into a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _prm_load_parameter_or_result(self, instance,\n                                      load_data=pypetconstants.LOAD_DATA,\n                                      load_only=None,\n                                      load_except=None,\n                                      load_flags=None,\n                                      with_links=False,\n                                      recursive=False,\n                                      max_depth=None,\n                                      _hdf5_group=None,):\n        \"\"\"Loads a parameter or result from disk.\n\n        :param instance:\n\n            Empty parameter or result instance\n\n        :param load_data:\n\n            How to load stuff\n\n        :param load_only:\n\n            List of data keys if only parts of a result should be loaded\n\n        :param load_except:\n\n            List of data key that should NOT be loaded.\n\n        :param load_flags:\n\n            Dictionary to determine how something is loaded\n\n        :param with_links:\n\n            Placeholder, because leaves have no links\n\n        :param recursive:\n\n            Dummy variable, no-op because leaves have no children\n\n        :param max_depth:\n\n            Dummy variable, no-op because leaves have no children\n\n        :param _hdf5_group:\n\n            The corresponding hdf5 group of the instance\n\n        \"\"\"\n        if load_data == pypetconstants.LOAD_NOTHING:\n            return\n\n        if _hdf5_group is None:\n            _hdf5_group = self._all_get_node_by_name(instance.v_full_name)\n\n        if load_data == pypetconstants.OVERWRITE_DATA:\n            if instance.v_is_parameter and instance.v_locked:\n                self._logger.debug('Parameter `%s` is locked, I will skip loading.' %\n                                     instance.v_full_name)\n                return\n            instance.f_empty()\n            instance.v_annotations.f_empty()\n            instance.v_comment = ''\n\n        self._all_load_skeleton(instance, _hdf5_group)\n        instance._stored = True\n\n        # If load only is just a name and not a list of names, turn it into a 1 element list\n        if isinstance(load_only, str):\n            load_only = [load_only]\n        if isinstance(load_except, str):\n            load_except = [load_except]\n\n        if load_data == pypetconstants.LOAD_SKELETON:\n            # We only load skeleton if asked for it and thus only\n            # signal completed node loading\n            self._node_processing_timer.signal_update()\n            return\n        elif load_only is not None:\n            if load_except is not None:\n                raise ValueError('Please use either `load_only` or `load_except` and not '\n                             'both at the same time.')\n            elif instance.v_is_parameter and instance.v_locked:\n                raise pex.ParameterLockedException('Parameter `%s` is locked, '\n                                                   'I will skip loading.' %\n                                                    instance.v_full_name)\n            self._logger.debug('I am in load only mode, I will only load %s.' %\n                               str(load_only))\n            load_only = set(load_only)\n        elif load_except is not None:\n            if instance.v_is_parameter and instance.v_locked:\n                raise pex.ParameterLockedException('Parameter `%s` is locked, '\n                                                   'I will skip loading.' %\n                                                    instance.v_full_name)\n            self._logger.debug('I am in load except mode, I will load everything except %s.' %\n                               str(load_except))\n            # We do not want to modify the original list\n            load_except = set(load_except)\n        elif not instance.f_is_empty():\n            # We only load data if the instance is empty or we specified load_only or\n            # load_except and thus only\n            # signal completed node loading\n            self._node_processing_timer.signal_update()\n            return\n\n        full_name = instance.v_full_name\n        self._logger.debug('Loading data of %s' % full_name)\n\n        load_dict = {}  # Dict that will be used to keep all data for loading the parameter or\n        # result\n\n        if load_flags is None:\n            load_flags = {}\n        try:\n            # Ask the instance for load flags\n            instance_flags = instance._load_flags().copy() # copy to avoid modifying the\n            # original data\n        except AttributeError:\n            # If it does not provide any, set it to the empty dictionary\n            instance_flags = {}\n        # User specified flags have priority over the flags from the instance\n        instance_flags.update(load_flags)\n        load_flags = instance_flags\n\n        self._prm_load_into_dict(full_name=full_name,\n                                 load_dict=load_dict,\n                                 hdf5_group=_hdf5_group,\n                                 instance=instance,\n                                 load_only=load_only,\n                                 load_except=load_except,\n                                 load_flags=load_flags)\n\n        if load_only is not None:\n            # Check if all data in `load_only` was actually found in the hdf5 file\n            if len(load_only) > 0:\n                self._logger.warning('You marked %s for load only, '\n                                     'but I cannot find these for `%s`' %\n                                     (str(load_only), full_name))\n        elif load_except is not None:\n            if len(load_except) > 0:\n                self._logger.warning(('You marked `%s` for not loading, but these were not part '\n                                      'of `%s` anyway.' % (str(load_except), full_name)))\n\n        # Finally tell the parameter or result to load the data, if there was any ;-)\n        if load_dict:\n            try:\n                instance._load(load_dict)\n                if instance.v_is_parameter:\n                    # Lock parameter as soon as data is loaded\n                    instance.f_lock()\n            except:\n                self._logger.error(\n                    'Error while reconstructing data of leaf `%s`.' % full_name)\n                raise\n\n        # Signal completed node loading\n        self._node_processing_timer.signal_update()", "response": "Loads a parameter or result from disk."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads data that was originally a dictionary when stored is not yet loaded.", "response": "def _prm_read_dictionary(self, leaf, full_name):\n        \"\"\"Loads data that was originally a dictionary when stored\n\n        :param leaf:\n\n            PyTables table containing the dictionary data\n\n        :param full_name:\n\n            Full name of the parameter or result whose data is to be loaded\n\n        :return:\n\n            Data to be loaded\n\n        \"\"\"\n        try:\n            # Load as Pbject Table\n            temp_table = self._prm_read_table(leaf, full_name)\n            # Turn the ObjectTable into a dictionary of lists (with length 1).\n            temp_dict = temp_table.to_dict('list')\n\n            innder_dict = {}\n\n            # Turn the dictionary of lists into a normal dictionary\n            for innerkey, vallist in temp_dict.items():\n                innder_dict[innerkey] = vallist[0]\n\n            return innder_dict\n        except:\n            self._logger.error('Failed loading `%s` of `%s`.' % (leaf._v_name, full_name))\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _prm_read_shared_data(self, shared_node, instance):\n        try:\n            data_type = self._all_get_from_attrs(shared_node,\n                                                 HDF5StorageService.SHARED_DATA_TYPE)\n            constructor = shared.FLAG_CLASS_MAPPING[data_type]\n            name = shared_node._v_name\n            result = constructor(name=name, parent=instance)\n            return result\n        except:\n            self._logger.error('Failed loading `%s` of `%s`.' % (shared_node._v_name,\n                                                                 instance.v_full_name))\n            raise", "response": "Reads the shared data and creates the appropraite class."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _prm_read_pandas(self, pd_node, full_name):\n        try:\n            name = pd_node._v_name\n            pathname = pd_node._v_pathname\n            pandas_store = self._hdf5store\n            pandas_data = pandas_store.get(pathname)\n            return pandas_data\n        except:\n            self._logger.error('Failed loading `%s` of `%s`.' % (pd_node._v_name, full_name))\n            raise", "response": "Reads a DataFrame from the pandas file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _prm_read_table(self, table_or_group, full_name):\n        try:\n            result_table = None\n\n            if self._all_get_from_attrs(table_or_group, HDF5StorageService.SPLIT_TABLE):\n                table_name = table_or_group._v_name\n\n                data_type_table_name = table_name + '__' + HDF5StorageService.STORAGE_TYPE\n\n                data_type_table = table_or_group._v_children[data_type_table_name]\n                data_type_dict = {}\n\n\n                for row in data_type_table:\n                    fieldname = row['field_name'].decode('utf-8')\n                    data_type_dict[fieldname] = row['data_type'].decode('utf-8')\n\n                for sub_table in table_or_group:\n                    sub_table_name = sub_table._v_name\n\n                    if sub_table_name == data_type_table_name:\n                        continue\n\n                    for colname in sub_table.colnames:\n                        # Read Data column by column\n                        col = sub_table.col(colname)\n                        data_list = list(col)\n\n                        prefix = HDF5StorageService.FORMATTED_COLUMN_PREFIX % colname\n                        for idx, data in enumerate(data_list):\n                            # Recall original type of data\n                            data, type_changed = self._all_recall_native_type(data,\n                                                                              PTItemMock(\n                                                                                  data_type_dict),\n                                                                              prefix)\n                            if type_changed:\n                                data_list[idx] = data\n                            else:\n                                break\n\n                        # Construct or insert into an ObjectTable\n                        if result_table is None:\n                            result_table = ObjectTable(data={colname: data_list})\n                        else:\n                            result_table[colname] = data_list\n\n            else:\n                for colname in table_or_group.colnames:\n                    # Read Data column by column\n                    col = table_or_group.col(colname)\n                    data_list = list(col)\n\n                    prefix = HDF5StorageService.FORMATTED_COLUMN_PREFIX % colname\n                    for idx, data in enumerate(data_list):\n                        # Recall original type of data\n                        data, type_changed = self._all_recall_native_type(data, table_or_group,\n                                                                          prefix)\n                        if type_changed:\n                            data_list[idx] = data\n                        else:\n                            break\n\n                    # Construct or insert into an ObjectTable\n                    if result_table is None:\n                        result_table = ObjectTable(data={colname: data_list})\n                    else:\n                        result_table[colname] = data_list\n\n            return result_table\n        except:\n            self._logger.error(\n                'Failed loading `%s` of `%s`.' % (table_or_group._v_name, full_name))\n            raise", "response": "Reads a non - nested PyTables table column by column and creates a new ObjectTable for the loaded data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread data from an array or carray and returns the result.", "response": "def _prm_read_array(self, array, full_name):\n        \"\"\"Reads data from an array or carray\n\n        :param array:\n\n            PyTables array or carray to read from\n\n        :param full_name:\n\n            Full name of the parameter or result whose data is to be loaded\n\n        :return:\n\n            Data to load\n\n        \"\"\"\n        try:\n            result = self._svrc_read_array(array)\n            # Recall original data types\n            result, dummy = self._all_recall_native_type(result, array,\n                                                         HDF5StorageService.DATA_PREFIX)\n\n            return result\n        except:\n            self._logger.error('Failed loading `%s` of `%s`.' % (array._v_name, full_name))\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_set_name(idx):\n    GROUPSIZE = 1000\n    set_idx = idx // GROUPSIZE\n    if set_idx >= 0:\n        return pypetconstants.FORMATTED_SET_NAME % set_idx\n    else:\n        return pypetconstants.SET_NAME_DUMMY", "response": "Creates a run set name based on the index."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds wildcard functions to the internal dictionary.", "response": "def f_add_wildcard_functions(self, func_dict):\n        \"\"\"#TODO\"\"\"\n        for wildcards, function in func_dict.items():\n            if not isinstance(wildcards, tuple):\n                wildcards = (wildcards,)\n            for wildcard in wildcards:\n                if wildcard in self._wildcard_keys:\n                    raise ValueError('Your wildcard `%s` is used twice1' % wildcard)\n                self._wildcard_keys[wildcard] = wildcards\n            self._wildcard_functions[wildcards] = function\n            self._logger.debug('Added wildcard function `%s`.' % str(wildcards))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef f_wildcard(self, wildcard='$', run_idx=None):\n        if run_idx is None:\n            run_idx = self.v_idx\n        wildcards = self._wildcard_keys[wildcard]\n        try:\n            return self._wildcard_cache[(wildcards, run_idx)]\n        except KeyError:\n            translation = self._wildcard_functions[wildcards](run_idx)\n            self._wildcard_cache[(wildcards, run_idx)] = translation\n            return translation", "response": "Return the value of a wildcard."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef v_full_copy(self, val):\n        self._full_copy = bool(val)\n        for param in self._explored_parameters.values():\n            if param is not None:\n                param.v_full_copy = bool(val)", "response": "Sets full copy mode of trajectory and ALL explored parameters."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef f_set_properties(self, **kwargs):\n        for name in kwargs:\n            val = kwargs[name]\n            if not name.startswith('v_'):\n                name = 'v_' + name\n            if not name in self._nn_interface._not_admissible_names:\n                raise AttributeError('Cannot set property `%s` does not exist.' % name)\n            else:\n                setattr(self, name, val)", "response": "Sets properties like v_fast_access v_auto_load."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef f_add_to_dynamic_imports(self, dynamic_imports):\n\n        if not isinstance(dynamic_imports, (list, tuple)):\n            dynamic_imports = [dynamic_imports]\n\n        for item in dynamic_imports:\n            if not (isinstance(item, str) or inspect.isclass(item)):\n                raise TypeError('Your dynamic import `%s` is neither a class nor a string.' %\n                                str(item))\n\n        self._dynamic_imports.extend(dynamic_imports)", "response": "Adds classes or paths to classes to the trajectory to create custom parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef f_set_crun(self, name_or_idx):\n        if (name_or_idx is None or name_or_idx == self.f_wildcard('$', -1) or\n                    name_or_idx == -1):\n            self.f_restore_default()\n        else:\n            if isinstance(name_or_idx, str):\n                self._idx = self.f_idx_to_run(name_or_idx)\n                self._crun = name_or_idx\n            else:\n                self._crun = self.f_idx_to_run(name_or_idx)\n                self._idx = name_or_idx\n\n            self._set_explored_parameters_to_idx(self.v_idx)", "response": "Set the current run name for the given                   ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef f_iter_runs(self, start=0, stop=None, step=1, yields='name'):\n        if stop is None:\n            stop =len(self)\n        elif stop > len(self):\n            raise ValueError('Stop cannot be larger than the trajectory lenght.')\n        yields = yields.lower()\n        if yields == 'name':\n            yield_func = lambda x: self.f_idx_to_run(x)\n        elif yields == 'idx':\n            yield_func = lambda x: x\n        elif yields == 'self':\n            yield_func = lambda x: self\n        elif yields == 'copy':\n            yield_func = lambda x: self.__copy__()\n        else:\n            raise ValueError('Please choose yields among: `name`, `idx`, or `self`.')\n        for idx in range(start, stop, step):\n            self.f_set_crun(idx)\n            yield yield_func(idx)\n\n        self.f_set_crun(None)", "response": "Makes the trajectory iterate over all runs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nshrinking the trajectory and removes all exploration ranges from the run information.", "response": "def f_shrink(self, force=False):\n        \"\"\" Shrinks the trajectory and removes all exploration ranges from the parameters.\n        Only possible if the trajectory has not been stored to disk before or was loaded as new.\n\n        :param force:\n\n            Usually you cannot shrink the trajectory if it has been stored to disk,\n            because there's no guarantee that it is actually shrunk if there\n            still exist explored parameters on disk. In case you are certain that\n            you did not store explored parameters to disk set or you deleted all\n            of them from disk set `force=True`.\n\n        :raises: TypeError if the trajectory was stored before.\n\n        \"\"\"\n        if self._stored and not force:\n            raise TypeError('Your trajectory is already stored to disk or database, shrinking is '\n                            'not allowed.')\n\n        for param in self._explored_parameters.values():\n            param.f_unlock()\n            try:\n                param._shrink()\n            except Exception as exc:\n                self._logger.error('Could not shrink `%s` because of:`%s`' %\n                                   (param.v_full_name, repr(exc)))\n\n        # If we shrink, we do not have any explored parameters left and we can erase all\n        # run information, and the length of the trajectory is 1 again.\n        self._explored_parameters = {}\n        self._run_information = {}\n        self._single_run_ids = {}\n        self._add_run_info(0)\n        self._test_run_addition(1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef f_preset_config(self, config_name, *args, **kwargs):\n\n        if not config_name.startswith('config.'):\n            config_name = 'config.' + config_name\n\n        self._preset(config_name, args, kwargs)", "response": "Similar to func : ~pypet. trajectory. Trajectory. f_preset_parameter"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef f_preset_parameter(self, param_name, *args, **kwargs):\n\n        if not param_name.startswith('parameters.'):\n            param_name = 'parameters.' + param_name\n\n        self._preset(param_name, args, kwargs)", "response": "Presets the value of a parameter in the trajectory."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncall by the environment to make some initial configurations before performing the individual runs.", "response": "def _prepare_experiment(self):\n        \"\"\"Called by the environment to make some initial configurations before performing the\n        individual runs.\n\n        Checks if all parameters marked for presetting were preset. If not raises a\n        DefaultReplacementError.\n\n        Locks all parameters.\n\n        Removal of potential results of previous runs in case the trajectory was expanded to avoid\n        mixing up undesired shortcuts in natural naming.\n\n        \"\"\"\n        if len(self._changed_default_parameters):\n            raise pex.PresettingError(\n                'The following parameters were supposed to replace a '\n                'default value, but it was never tried to '\n                'add default values with these names: %s' %\n                str(self._changed_default_parameters))\n\n        self.f_lock_parameters()\n        self.f_lock_derived_parameters()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsearches for all occurrences of name in each run and returns a dictionary with the keys and values as keys and values as values.", "response": "def f_get_from_runs(self, name, include_default_run=True, use_indices=False,\n                        fast_access=False, with_links = True,\n                        shortcuts=True, max_depth=None, auto_load=False):\n        \"\"\"Searches for all occurrences of `name` in each run.\n\n        Generates an ordered dictionary with the run names or indices as keys and\n        found items as values.\n\n        Example:\n\n        >>> traj.f_get_from_runs(self, 'deep.universal_answer', use_indices=True, fast_access=True)\n        OrderedDict([(0, 42), (1, 42), (2, 'fortytwo), (3, 43)])\n\n\n        :param name:\n\n            String description of the item(s) to find.\n            Cannot be full names but the part of the names that are below\n            a `run_XXXXXXXXX` group.\n\n        :param include_default_run:\n\n            If results found under ``run_ALL`` should be accounted for every run or simply be\n            ignored.\n\n        :param use_indices:\n\n            If `True` the keys of the resulting dictionary are the run indices\n            (e.g. 0,1,2,3), otherwise the keys are run names (e.g. `run_00000000`,\n            `run_000000001`)\n\n        :param fast_access:\n\n            Whether to return parameter or result instances or the values handled by these.\n\n        :param with_links:\n\n            If links should be considered\n\n        :param shortcuts:\n\n            If shortcuts are allowed and the trajectory can *hop* over nodes in the\n            path.\n\n        :param max_depth:\n\n            Maximum depth (relative to start node) how search should progress in tree.\n            `None` means no depth limit.  Only relevant if `shortcuts` are allowed.\n\n        :param auto_load:\n\n            If data should be loaded from the storage service if it cannot be found in the\n            current trajectory tree. Auto-loading will load group and leaf nodes currently\n            not in memory and it will load data into empty leaves. Be aware that auto-loading\n            does not work with shortcuts.\n\n        :return:\n\n            Ordered dictionary with run names or indices as keys and found items as values.\n            Will only include runs where an item was actually found.\n\n        \"\"\"\n        result_dict = OrderedDict()\n        old_crun = self.v_crun\n\n        try:\n\n            if len(self._run_parent_groups) > 0:\n                for run_name in self.f_iter_runs():\n                    # Iterate over all runs\n                    value = None\n                    already_found = False\n                    for run_parent_group in self._run_parent_groups.values():\n                        if run_name not in run_parent_group._children:\n                            continue\n\n                        try:\n                            value = run_parent_group.f_get(run_name + '.' + name,\n                                                           fast_access=False,\n                                                           with_links=with_links,\n                                                           shortcuts=shortcuts,\n                                                           max_depth=max_depth,\n                                                           auto_load=auto_load)\n                            if already_found:\n                                raise pex.NotUniqueNodeError('`%s` has been found several times '\n                                                             'in one run.' % name)\n                            else:\n                                already_found = True\n\n                        except (AttributeError, pex.DataNotInStorageError):\n                            pass\n\n                    if value is None and include_default_run:\n                        for run_parent_group in self._run_parent_groups.values():\n                            try:\n                                value = run_parent_group.f_get(self.f_wildcard('$', -1) +\n                                                               '.' + name,\n                                                               fast_access=False,\n                                                               with_links=with_links,\n                                                               shortcuts=shortcuts,\n                                                               max_depth=max_depth,\n                                                               auto_load=auto_load)\n                                if already_found:\n                                    raise pex.NotUniqueNodeError('`%s` has been found several '\n                                                                 'times in one run.' % name)\n                                else:\n                                    already_found = True\n                            except (AttributeError, pex.DataNotInStorageError):\n                                pass\n\n                    if value is not None:\n\n                        if value.v_is_leaf:\n                            value = self._nn_interface._apply_fast_access(value, fast_access)\n\n                        if use_indices:\n                            key = self.f_idx_to_run(run_name)\n                        else:\n                            key = run_name\n\n                        result_dict[key] = value\n\n            return result_dict\n        finally:\n            self.v_crun = old_crun"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexplores the trajectory and return the new trajectory with the expansion of the parameters.", "response": "def f_expand(self, build_dict, fail_safe=True):\n        \"\"\"Similar to :func:`~pypet.trajectory.Trajectory.f_explore`, but can be used to enlarge\n        already completed trajectories.\n\n        Please ensure before usage, that all explored parameters are loaded!\n\n        :param build_dict:\n\n            Dictionary containing the expansion\n\n        :param fail_safe:\n\n            If old ranges should be **deep-copied** in order to allow to restore\n            the original exploration if something fails during expansion.\n            Set to `False` if deep-copying your parameter ranges causes errors.\n\n        :raises:\n\n            TypeError: If not all explored parameters are enlarged\n\n            AttributeError: If keys of dictionary cannot be found in the trajectory\n\n            NotUniqueNodeError:\n\n                If dictionary keys do not unambiguously map to single parameters\n\n            ValueError: If not all explored parameter ranges are of the same length\n\n        \"\"\"\n        if len(self._explored_parameters) == 0:\n            self._logger.info('Your trajectory has not been explored, yet. '\n                              'I will call `f_explore` instead.')\n            return self.f_explore(build_dict)\n\n        enlarge_set = set([self.f_get(key).v_full_name\n                           for key in build_dict.keys()])\n\n        # Check if all explored parameters will be enlarged, otherwise\n        # We cannot enlarge the trajectory\n        if not set(self._explored_parameters.keys()) == enlarge_set:\n            raise TypeError('You have to enlarge dimensions you have explored before! Currently'\n                            ' explored parameters are not the ones you specified in your building'\n                            ' dictionary, i.e. %s != %s' %\n                            (str(set(self._explored_parameters.keys())),\n                             str(set(build_dict.keys()))))\n        if any(x is None for x in self._explored_parameters.values()):\n            raise TypeError('At least one of your explored parameters is not fully loaded, '\n                            'please load it.')\n\n        old_ranges = None\n        if fail_safe:\n            old_ranges = {}\n            for param_name in self._explored_parameters:\n                old_ranges[param_name] = self._explored_parameters[param_name].f_get_range()\n            try:\n                old_ranges = cp.deepcopy(old_ranges)\n            except Exception:\n                self._logger.error('Cannot deepcopy old parameter ranges, if '\n                                     'something fails during `f_expand` I cannot revert the '\n                                     'trajectory to old settings.')\n                old_ranges = None\n\n        try:\n            count = 0\n            length = None\n            for key, builditerable in build_dict.items():\n                act_param = self.f_get(key)\n\n                act_param.f_unlock()\n                act_param._expand(builditerable)\n\n                name = act_param.v_full_name\n\n                self._explored_parameters[name] = act_param\n\n                # Compare the length of two consecutive parameters in the `build_dict`\n                if count == 0:\n                    length = act_param.f_get_range_length()\n                elif not length == act_param.f_get_range_length():\n                    raise ValueError('The parameters to explore have not the same size!')\n                count += 1\n\n            original_length = len(self)\n            for irun in range(original_length, length):\n                self._add_run_info(irun)\n            self._test_run_addition(length)\n\n            # We need to update the explored parameters in case they were stored:\n            self._remove_exploration()\n\n        except Exception:\n            if old_ranges is not None:\n                # Try to restore the original parameter exploration\n                for param_name in old_ranges:\n                    param_range = old_ranges[param_name]\n                    param = self._explored_parameters[param_name]\n                    param.f_unlock()\n                    try:\n                        param._shrink()\n                    except Exception as exc:\n                        self._logger.error('Could not shrink parameter `%s` '\n                                           'because of:`%s`' % (param_name, repr(exc)))\n                    param._explore(param_range)\n                    param._explored = True\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _remove_exploration(self):\n        for param in self._explored_parameters.values():\n            if param._stored:\n                try:\n                    self.f_delete_item(param)\n                except Exception:\n                    self._logger.exception('Could not delete expanded parameter `%s` '\n                                           'from disk.' % param.v_full_name)", "response": "Called if trajectory is expanded deletes all explored parameters from disk"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a shallow copy of the current trajectory.", "response": "def f_copy(self, copy_leaves=True,\n                     with_links=True):\n        \"\"\"Returns a *shallow* copy of a trajectory.\n\n        :param copy_leaves:\n\n            If leaves should be **shallow** copied or simply referred to by both trees.\n            **Shallow** copying is established using the copy module.\n\n            Accepts the setting ``'explored'`` to only copy explored parameters.\n            Note that ``v_full_copy`` determines how these will be copied.\n\n        :param with_links: If links should be ignored or followed and copied as well\n\n        :return: A shallow copy\n\n        \"\"\"\n        new_traj = Trajectory(_copy_traj=True)\n\n        new_traj._length = self._length\n        new_traj._name = self._name\n\n        new_traj._timestamp = self._timestamp\n        new_traj._time = self._time\n\n        new_traj._single_run_ids = self._single_run_ids\n        new_traj._run_information = self._run_information\n        new_traj._updated_run_information = self._updated_run_information\n\n        new_traj._fast_access = self._fast_access\n        new_traj._shortcuts = self._shortcuts\n        new_traj._iter_recursive = self._iter_recursive\n        new_traj._max_depth = self._max_depth\n        new_traj._auto_load = self._auto_load\n        new_traj._with_links = self._with_links\n\n        new_traj._environment_hexsha = self._environment_hexsha\n        new_traj._environment_name = self._environment_name\n\n        # Index of a trajectory is -1, if the trajectory should behave like a single run\n        # and blind out other single run results, this can be changed via 'v_crun'.\n        new_traj._idx = self._idx\n        new_traj._crun = self._crun\n\n        new_traj._standard_parameter = self._standard_parameter\n        new_traj._standard_result = self._standard_result\n        new_traj._standard_leaf = self._standard_leaf\n\n        new_traj._auto_run_prepend = self._auto_run_prepend\n        new_traj._no_clobber = self._no_clobber\n\n        new_traj._run_started = self._run_started # For manually using a trajectory\n        new_traj._run_by_environment = self._run_by_environment  # To disable manual running of experiment\n\n        new_traj._full_copy = self._full_copy\n\n        new_traj._dynamic_imports = self._dynamic_imports\n\n        new_traj._wildcard_functions = self._wildcard_functions\n        new_traj._wildcard_keys = self._wildcard_keys\n        new_traj._reversed_wildcards = self._reversed_wildcards\n        new_traj._wildcard_cache = self._wildcard_cache\n\n        new_traj._comment = self._comment\n\n        new_traj._stored = self._stored\n\n        new_traj._storage_service = self._storage_service\n\n        new_traj._is_run = self._is_run\n\n        new_traj._copy_from(self, copy_leaves=copy_leaves,\n                                   with_links=with_links)\n\n        # Copy references to new nodes and leaves\n        for my_dict, new_dict in ((self._new_nodes, new_traj._new_nodes),\n                                 (self._new_links, new_traj._new_links)):\n            for key in my_dict:\n                value = my_dict[key]\n                parent, child = value\n                if parent is self:\n                    new_parent = new_traj\n                else:\n                    new_parent = new_traj.f_get(parent.v_full_name,\n                                            shortcuts=False,\n                                            with_links=False,\n                                            auto_load=False)\n                new_child = new_parent._children[key[1]]\n                new_dict[key] = (new_parent, new_child)\n\n        return new_traj"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _copy_from(self, node,\n                          copy_leaves=True,\n                          overwrite=False,\n                          with_links=True):\n        \"\"\"Pass a ``node`` to insert the full tree to the trajectory.\n\n        Considers all links in the given node!\n        Ignored nodes already found in the current trajectory.\n\n        :param node: The node to insert\n\n        :param copy_leaves:\n\n            If leaves should be **shallow** copied or simply referred to by both trees.\n            **Shallow** copying is established using the copy module.\n\n            Accepts the setting ``'explored'`` to only copy explored parameters.\n            Note that ``v_full_copy`` determines how these will be copied.\n\n        :param overwrite:\n\n            If existing elemenst should be overwritten. Requries ``__getstate__`` and\n            ``__setstate__`` being implemented in the leaves.\n\n        :param with_links: If links should be ignored or followed and copied as well\n\n        :return: The corresponding (new) node in the tree.\n\n        \"\"\"\n\n        def _copy_skeleton(node_in, node_out):\n            \"\"\"Copies the skeleton of from `node_out` to `node_in`\"\"\"\n            new_annotations = node_out.v_annotations\n            node_in._annotations = new_annotations\n            node_in.v_comment = node_out.v_comment\n\n        def _add_leaf(leaf):\n            \"\"\"Adds a leaf to the trajectory\"\"\"\n            leaf_full_name = leaf.v_full_name\n            try:\n                found_leaf = self.f_get(leaf_full_name,\n                                        with_links=False,\n                                        shortcuts=False,\n                                        auto_load=False)\n                if overwrite:\n                    found_leaf.__setstate__(leaf.__getstate__())\n                return found_leaf\n            except AttributeError:\n                pass\n            if copy_leaves is True or (copy_leaves == 'explored' and\n                                           leaf.v_is_parameter and leaf.v_explored):\n                new_leaf = self.f_add_leaf(cp.copy(leaf))\n            else:\n                new_leaf = self.f_add_leaf(leaf)\n            if new_leaf.v_is_parameter and new_leaf.v_explored:\n                self._explored_parameters[new_leaf.v_full_name] = new_leaf\n            return new_leaf\n\n        def _add_group(group):\n            \"\"\"Adds a new group to the trajectory\"\"\"\n            group_full_name = group.v_full_name\n            try:\n                found_group = self.f_get(group_full_name,\n                                             with_links=False,\n                                             shortcuts=False,\n                                             auto_load=False)\n                if overwrite:\n                    _copy_skeleton(found_group, group)\n                return found_group\n            except AttributeError:\n                pass\n            new_group = self.f_add_group(group_full_name)\n            _copy_skeleton(new_group, group)\n            return new_group\n\n        is_run = self._is_run\n        self._is_run = False  # So that we can copy Config Groups and Config Data\n        try:\n            if node.v_is_leaf:\n                return _add_leaf(node)\n            elif node.v_is_group:\n                other_root = node.v_root\n                if other_root is self:\n                    raise RuntimeError('You cannot copy a given tree to itself!')\n                result = _add_group(node)\n                nodes_iterator = node.f_iter_nodes(recursive=True, with_links=with_links)\n                has_links = []\n                if node._links:\n                    has_links.append(node)\n                for child in nodes_iterator:\n                    if child.v_is_leaf:\n                        _add_leaf(child)\n                    else:\n                        _add_group(child)\n                        if child._links:\n                            has_links.append(child)\n\n                if with_links:\n                    for current in has_links:\n                        mine = self.f_get(current.v_full_name, with_links=False,\n                                          shortcuts=False, auto_load=False)\n                        my_link_set = set(mine._links.keys())\n                        other_link_set = set(current._links.keys())\n                        new_links = other_link_set - my_link_set\n                        for link in new_links:\n                            where_full_name = current._links[link].v_full_name\n                            mine.f_add_link(link, where_full_name)\n\n                return result\n            else:\n                raise RuntimeError('You shall not pass!')\n        except Exception:\n            self._is_run = is_run", "response": "Copy a node from the current trajectory to the new trajectory."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef f_explore(self, build_dict):\n        for run_idx in range(len(self)):\n            if self.f_is_completed(run_idx):\n                raise TypeError('You cannot explore a trajectory which has been explored before, '\n                                'please use `f_expand` instead.')\n\n        added_explored_parameters = []\n        try:\n            length = len(self)\n            for key, builditerable in build_dict.items():\n                act_param = self.f_get(key)\n                if not act_param.v_is_leaf or not act_param.v_is_parameter:\n                    raise ValueError('%s is not an appropriate search string for a parameter.' % key)\n\n                act_param.f_unlock()\n\n                act_param._explore(builditerable)\n                added_explored_parameters.append(act_param)\n\n                full_name = act_param.v_full_name\n                self._explored_parameters[full_name] = act_param\n\n                act_param._explored = True\n\n                # Compare the length of two consecutive parameters in the `build_dict`\n                if len(self._explored_parameters) == 1:\n                    length = act_param.f_get_range_length()\n                elif not length == act_param.f_get_range_length():\n                    raise ValueError('The parameters to explore have not the same size!')\n\n            for irun in range(length):\n                self._add_run_info(irun)\n            self._test_run_addition(length)\n\n        except Exception:\n            # Remove the added parameters again\n            for param in added_explored_parameters:\n                param.f_unlock()\n                param._shrink()\n                param._explored = False\n                full_name = param.v_full_name\n                del self._explored_parameters[full_name]\n            if len(self._explored_parameters) == 0:\n                self.f_shrink(force=True)\n            raise", "response": "Prepares the trajectory to explore the parameter space."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _update_run_information(self, run_information_dict):\n        idx = run_information_dict['idx']\n        name = run_information_dict['name']\n        self._run_information[name] = run_information_dict\n        self._updated_run_information.add(idx)", "response": "Updates the run information of a particular run"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _add_run_info(self, idx, name='', timestamp=42.0, finish_timestamp=1.337,\n                      runtime='forever and ever', time='>>Maybe time`s gone on strike',\n                      completed=0, parameter_summary='Not yet my friend!',\n                      short_environment_hexsha='N/A'):\n        \"\"\"Adds a new run to the `_run_information` dict.\"\"\"\n\n        if idx in self._single_run_ids:\n            # Delete old entries, they might be replaced by a new name\n            old_name = self._single_run_ids[idx]\n            del self._single_run_ids[old_name]\n            del self._single_run_ids[idx]\n            del self._run_information[old_name]\n\n        if name == '':\n            name = self.f_wildcard('$', idx)\n        # The `_single_run_ids` dict is bidirectional and maps indices to run names and vice versa\n        self._single_run_ids[name] = idx\n        self._single_run_ids[idx] = name\n\n        info_dict = {'idx': idx,\n                     'timestamp': timestamp,\n                     'finish_timestamp': finish_timestamp,\n                     'runtime': runtime,\n                     'time': time,\n                     'completed': completed,\n                     'name': name,\n                     'parameter_summary': parameter_summary,\n                     'short_environment_hexsha': short_environment_hexsha}\n\n        self._run_information[name] = info_dict\n        self._length = len(self._run_information)", "response": "Adds a new run to the _run_information dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef f_lock_parameters(self):\n        for par in self._parameters.values():\n            if not par.f_is_empty():\n                par.f_lock()", "response": "Locks all non - empty parameters"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlocks all non - empty derived parameters", "response": "def f_lock_derived_parameters(self):\n        \"\"\"Locks all non-empty derived parameters\"\"\"\n        for par in self._derived_parameters.values():\n            if not par.f_is_empty():\n                par.f_lock()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _finalize(self, store_meta_data=True):\n        self._is_run = False\n        self.f_set_crun(None)\n        if store_meta_data:\n            self.f_store(only_init=True)", "response": "Final rollback initiated by the environment\n\n        Restores the trajectory as root of the tree, and stores meta data to disk.\n        This updates the trajectory's information about single runs, i.e. if they've been\n        completed, when they were started, etc."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef f_load_skeleton(self):\n        self.f_load(self.v_name, as_new=False, load_parameters=pypetconstants.LOAD_SKELETON,\n                    load_derived_parameters=pypetconstants.LOAD_SKELETON,\n                    load_results=pypetconstants.LOAD_SKELETON,\n                    load_other_data=pypetconstants.LOAD_SKELETON,\n                    with_run_information=False)", "response": "Loads the full skeleton from the storage service."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading a trajectory from the hdf5 file.", "response": "def f_load(self, name=None, index=None, as_new=False, load_parameters=pypetconstants.LOAD_DATA,\n               load_derived_parameters=pypetconstants.LOAD_SKELETON,\n               load_results=pypetconstants.LOAD_SKELETON,\n               load_other_data=pypetconstants.LOAD_SKELETON,\n               recursive=True,\n               load_data=None,\n               max_depth=None,\n               force=False,\n               dynamic_imports=None,\n               with_run_information=True,\n               with_meta_data=True,\n               storage_service=None, **kwargs):\n        \"\"\"Loads a trajectory via the storage service.\n\n\n        If you want to load individual results or parameters manually, you can take\n        a look at :func:`~pypet.trajectory.Trajectory.f_load_items`.\n        To only load subtrees check out :func:`~pypet.naturalnaming.NNGroupNode.f_load_child`.\n\n\n        For `f_load` you can pass the following arguments:\n\n        :param name:\n\n            Name of the trajectory to be loaded. If no name or index is specified\n            the current name of the trajectory is used.\n\n        :param index:\n\n            If you don't specify a name you can specify an integer index instead.\n            The corresponding trajectory in the hdf5 file at the index\n            position is loaded (counting starts with 0). Negative indices are also allowed\n            counting in reverse order. For instance, `-1` refers to the last trajectory in\n            the file, `-2` to the second last, and so on.\n\n        :param as_new:\n\n            Whether you want to rerun the experiments. So the trajectory is loaded only\n            with parameters. The current trajectory name is kept in this case, which should be\n            different from the trajectory name specified in the input parameter `name`.\n            If you load `as_new=True` all parameters are unlocked.\n            If you load `as_new=False` the current trajectory is replaced by the one on disk,\n            i.e. name, timestamp, formatted time etc. are all taken from disk.\n\n        :param load_parameters: How parameters and config items are loaded\n\n        :param load_derived_parameters: How derived parameters are loaded\n\n        :param load_results: How results are loaded\n\n            You can specify how to load the parameters, derived parameters and results\n            as follows:\n\n                * :const:`pypet.pypetconstants.LOAD_NOTHING`: (0)\n\n                    Nothing is loaded.\n\n                * :const:`pypet.pypetconstants.LOAD_SKELETON`: (1)\n\n                    The skeleton is loaded including annotations (See :ref:`more-on-annotations`).\n                    This means that only empty\n                    *parameter* and *result* objects will\n                    be created  and you can manually load the data into them afterwards.\n                    Note that :class:`pypet.annotations.Annotations` do not count as data and they\n                    will be loaded because they are assumed to be small.\n\n                * :const:`pypet.pypetconstants.LOAD_DATA`: (2)\n\n                    The whole data is loaded. Note in case you have non-empty leaves\n                    already in RAM, these are left untouched.\n\n                * :const:`pypet.pypetconstants.OVERWRITE_DATA`: (3)\n\n                    As before, but non-empty nodes are emptied and reloaded.\n\n                Note that in all cases except :const:`pypet.pypetconstants.LOAD_NOTHING`,\n                annotations will be reloaded if the corresponding instance\n                is created or the annotations of an existing instance were emptied before.\n\n        :param recursive:\n\n            If data should be loaded recursively. If set to `None`, this is equivalent to\n            set all data loading to `:const:`pypet.pypetconstants.LOAD_NOTHING`.\n\n        :param load_data:\n\n            As the above, per default set to `None`. If not `None` the setting of `load_data`\n            will overwrite the settings of `load_parameters`, `load_derived_parameters`,\n            `load_results`, and `load_other_data`. This is more or less or shortcut if all\n            types should be loaded the same.\n\n        :param max_depth:\n\n            Maximum depth to load nodes (inclusive).\n\n        :param force:\n\n            *pypet* will refuse to load trajectories that have been created using *pypet* with a\n            different version number or a different python version.\n            To force the load of a trajectory from a previous version\n            simply set ``force = True``. Note that it is not checked if other versions of packages\n            differ from previous experiments, i.e. numpy, scipy, etc. But you can check\n            for this manually. The versions of other packages can be found under\n            ``'config.environment.name_of_environment.versions.package_name'``.\n\n        :param dynamic_imports:\n\n            If you've written a custom parameter that needs to be loaded dynamically\n            during runtime, this needs to be specified here as a list of classes or\n            strings naming classes and there module paths. For example:\n            `dynamic_imports = ['pypet.parameter.PickleParameter',MyCustomParameter]`\n\n            If you only have a single class to import, you do not need the list brackets:\n            `dynamic_imports = 'pypet.parameter.PickleParameter'`\n\n            The classes passed here are added for good and will be kept by the trajectory.\n            Please add your dynamically imported classes only once.\n\n        :param with_run_information:\n\n            If information about the individual runs should be loaded. If you have many\n            runs, like 1,000,000 or more you can spare time by setting\n            `with_run_information=False`.\n            Note that `f_get_run_information` and `f_idx_to_run` do not work in such a case.\n            Moreover, setting `v_idx` does not work either. If you load the trajectory\n            without this information, be careful, this is not recommended.\n\n        :param wiht_meta_data:\n\n            If meta data should be loaded.\n\n        :param storage_service:\n\n            Pass a storage service used by the trajectory. Alternatively pass a constructor\n            and other ``**kwargs`` are passed onto the constructor. Leave `None` in combination\n            with using no other kwargs, if you don't want to change the service\n            the trajectory is currently using.\n\n        :param kwargs:\n\n            Other arguments passed to the storage service constructor. Don't pass any\n            other kwargs and ``storage_service=None``,\n            if you don't want to change the current service.\n\n        \"\"\"\n        # Do some argument validity checks first\n        if name is None and index is None:\n            name = self.v_name\n\n        if as_new:\n            load_parameters = pypetconstants.LOAD_DATA\n            load_derived_parameters = pypetconstants.LOAD_NOTHING\n            load_results = pypetconstants.LOAD_NOTHING\n            load_other_data = pypetconstants.LOAD_NOTHING\n\n        unused_kwargs = set(kwargs.keys())\n        if self.v_storage_service is None or storage_service is not None or len(kwargs) > 0:\n            self._storage_service, unused_kwargs = storage_factory(storage_service=storage_service,\n                                                                   trajectory=self, **kwargs)\n        if len(unused_kwargs) > 0:\n            raise ValueError('The following keyword arguments were not used: `%s`' %\n                             str(unused_kwargs))\n\n        if dynamic_imports is not None:\n            self.f_add_to_dynamic_imports(dynamic_imports)\n\n        if load_data is not None:\n            load_parameters = load_data\n            load_derived_parameters = load_data\n            load_results = load_data\n            load_other_data = load_data\n\n        self._storage_service.load(pypetconstants.TRAJECTORY, self, trajectory_name=name,\n                                   trajectory_index=index,\n                                   as_new=as_new, load_parameters=load_parameters,\n                                   load_derived_parameters=load_derived_parameters,\n                                   load_results=load_results,\n                                   load_other_data=load_other_data,\n                                   recursive=recursive,\n                                   max_depth=max_depth,\n                                   with_run_information=with_run_information,\n                                   with_meta_data=with_meta_data,\n                                   force=force)\n\n        # If a trajectory is newly loaded, all parameters are unlocked.\n        if as_new:\n            for param in self._parameters.values():\n                param.f_unlock()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if two trajectories live in the same space and can be merged.", "response": "def _check_if_both_have_same_parameters(self, other_trajectory,\n                                            ignore_data, consecutive_merge):\n        \"\"\" Checks if two trajectories live in the same space and can be merged. \"\"\"\n\n        if not isinstance(other_trajectory, Trajectory):\n            raise TypeError('Can only merge trajectories, the other trajectory'\n                            ' is of type `%s`.' % str(type(other_trajectory)))\n\n        if self._stored and not consecutive_merge:\n            self.f_load_skeleton()\n        if other_trajectory._stored:\n            other_trajectory.f_load_skeleton()\n\n        # Check wildcard set\n        other_wildcard_set = set(x[1] for x in other_trajectory._wildcard_functions.keys())\n        wildcard_set = set(x[1] for x in self._wildcard_functions.keys())\n        diff = wildcard_set.symmetric_difference(other_wildcard_set)\n        if diff:\n            raise TypeError('The wildcard sets are not matching. `%s` != `%s`' %\n                            (str(wildcard_set), str(other_wildcard_set)))\n\n        # Load all parameters of the current and the other trajectory\n        if self._stored:\n            # To suppress warnings if nothing needs to be loaded\n            with self._nn_interface._disable_logging:\n                self.f_load_items(self._parameters.keys(), only_empties=True)\n        if other_trajectory._stored:\n            with self._nn_interface._disable_logging:\n                other_trajectory.f_load_items(other_trajectory._parameters.keys(),\n                                              only_empties=True)\n\n        self.f_restore_default()\n        other_trajectory.f_restore_default()\n\n        allmyparams = self._parameters.copy()\n        allotherparams = other_trajectory._parameters.copy()\n\n        # If not ignored, add also the trajectory derived parameters to check for merging\n        if 'derived_parameters' in self:\n            my_traj_dpars = self._derived_parameters\n            if self._stored:\n                with self._nn_interface._disable_logging:\n                    self.f_load_items(my_traj_dpars.keys(), only_empties=True)\n            allmyparams.update(my_traj_dpars)\n            other_traj_dpars = other_trajectory._derived_parameters\n            if other_trajectory._stored:\n                with self._nn_interface._disable_logging:\n                    other_trajectory.f_load_items(other_traj_dpars.keys(), only_empties=True)\n            allotherparams.update(other_traj_dpars)\n\n        # Check if the trajectories have the same parameters:\n        my_keyset = set(allmyparams.keys())\n        other_keyset = set(allotherparams.keys())\n        diff = my_keyset.symmetric_difference(other_keyset) - ignore_data\n        run_dummys = (self.f_wildcard('$', -1), other_trajectory.f_wildcard('$', -1))\n        if diff:\n            run_difference_can_be_resolved = True\n            for full_name in diff:\n                split_name = full_name.split('.')\n                if not any(x in self._run_information or\n                                x in other_trajectory._run_information or\n                                    x in run_dummys\n                                        for x in split_name):\n                    run_difference_can_be_resolved = False\n                    break\n                elif full_name in allotherparams:\n                    del allotherparams[full_name]\n            if not run_difference_can_be_resolved:\n                raise TypeError('Cannot merge trajectories, '\n                                'they do not live in the same space,the '\n                                'set of parameters `%s` is only '\n                                'found in one trajectory.' % str(diff))\n\n        # Check if corresponding parameters in both trajectories are of the same type\n        for key, other_param in allotherparams.items():\n            if key in ignore_data:\n                continue\n            my_param = self.f_get(key)\n            split_key = key.split('.')\n            if any(x in self._run_information or\n                        x in other_trajectory._run_information\n                            for x in split_key):\n                pass\n            else:\n                if not my_param._values_of_same_type(my_param.f_get(), other_param.f_get()):\n                    raise TypeError('Cannot merge trajectories, values of parameters `%s` are not '\n                                    'of the same type. Types are %s (current) and %s (other).' %\n                                    (key, str(type(my_param.f_get())),\n                                     str(type(other_param.f_get()))))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef f_backup(self, **kwargs):\n        self._storage_service.store(pypetconstants.BACKUP, self, trajectory_name=self.v_name,\n                                    **kwargs)", "response": "Backs up the current trajectory with the given storage service."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _make_reversed_wildcards(self, old_length=-1):\n        if len(self._reversed_wildcards) > 0:\n            # We already created reversed wildcards, so we don't need to do all of them\n            # again\n            start = old_length\n        else:\n            start = -1\n        for wildcards, func in self._wildcard_functions.items():\n            for irun in range(start, len(self)):\n                translated_name = func(irun)\n                if not translated_name in self._reversed_wildcards:\n                    self._reversed_wildcards[translated_name] = ([], wildcards)\n                self._reversed_wildcards[translated_name][0].append(irun)", "response": "Creates a full mapping from all wildcard translations to the corresponding wildcards"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef f_merge_many(self, other_trajectories,\n                ignore_data=(),\n                move_data=False,\n                delete_other_trajectory=False,\n                keep_info=True,\n                keep_other_trajectory_info=True,\n                merge_config=True,\n                backup=True):\n        \"\"\"Can be used to merge several `other_trajectories` into your current one.\n\n        IMPORTANT `backup=True` only backs up the current trajectory not any of\n        the `other_trajectories`. If you need a backup of these, do it manually.\n\n        Parameters as for :func:`~pypet.trajectory.Trajectory.f_merge`.\n\n        \"\"\"\n        other_length = len(other_trajectories)\n        self._logger.info('Merging %d trajectories into the current one.' % other_length)\n        self.f_load_skeleton()\n\n        if backup:\n            self.f_backup()\n\n        for idx, other in enumerate(other_trajectories):\n            self.f_merge(other, ignore_data=ignore_data,\n                         move_data=move_data,\n                         delete_other_trajectory=delete_other_trajectory,\n                         keep_info=keep_info,\n                         keep_other_trajectory_info=keep_other_trajectory_info,\n                         merge_config=merge_config,\n                         backup=False,\n                         consecutive_merge=True)\n            self._logger.log(21,'Merged %d out of %d' % (idx + 1, other_length))\n        self._logger.info('Storing data to disk')\n        self._reversed_wildcards = {}\n        self.f_store()\n        self._logger.info('Finished final storage')", "response": "This function merges two trajectories into the current trajectory."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef f_merge(self, other_trajectory, trial_parameter=None, remove_duplicates=False,\n                ignore_data=(),\n                backup=True,\n                move_data=False,\n                delete_other_trajectory=False,\n                keep_info=True,\n                keep_other_trajectory_info=True,\n                merge_config=True,\n                consecutive_merge=False,\n                slow_merge=False):\n        \"\"\"Merges another trajectory into the current trajectory.\n\n        Both trajectories must live in the same space. This means both need to have the same\n        parameters with similar types of values.\n\n        Note that links are also merged. There are exceptions: Links found under\n        a generic run group called `run_ALL` or links linking to a node under such a\n        group are NOT merged and simply skipped, because there is no straightforward\n        way to resolve the link.\n\n        :param other_trajectory: Other trajectory instance to merge into the current one.\n\n        :param trial_parameter:\n\n            If you have a particular parameter that specifies only the trial\n            number, i.e. an integer parameter running form 0 to T1 and\n            0 to T2, the parameter is modified such that after merging it will\n            cover the range 0 to T1+T2+1. T1 is the number of individual trials in the current\n            trajectory and T2 number of trials in the other trajectory.\n\n        :param remove_duplicates:\n\n            Whether you want to remove duplicate parameter points.\n            Requires N1 * N2 (quadratic complexity in single runs).\n            A ValueError is raised if no runs would be merged.\n\n        :param ignore_data:\n\n            List of full names of data that should be ignored and not merged.\n\n        :param backup:\n\n            If ``True``, backs up both trajectories into files chosen automatically\n            by the storage services. If you want to customize your backup use\n            the `f_backup` function instead.\n\n        :param move_data:\n\n           Tells the storage service to move data from one trajectory to the other\n           instead of copying it.\n\n           If you use the HDF5 storage service and both trajectories are\n           stored in the same file, merging is performed fast directly within\n           the file. You can choose if you want to copy nodes ('move_nodes=False`)\n           from the other trajectory to the current one, or if you want to move them.\n           Accordingly, the stored data is no longer accessible in the other trajectory.\n\n        :param delete_other_trajectory:\n\n            If you want to delete the other trajectory after merging.\n\n        :param keep_info:\n\n            If `True`, information about the merge is added to the trajectory `config` tree under\n            `config.merge`.\n\n        :param merge_config:\n\n            Whether or not to merge all config parameters under `.config.git`,\n            `.config.environment`, and `.config.merge` of the other trajectory\n            into the current one.\n\n        :param keep_other_trajectory_info:\n\n            Whether to keep information like length, name, etc. of the other trajectory\n            in case you want to keep all the information. Setting of `keep_other_trajectory_info`\n            is irrelevant in case `keep_info=False`.\n\n        :param consecutive_merge:\n            Can be set to `True` if you are about to merge several trajectories into the current\n            one within a loop to avoid quadratic complexity.\n            But remember to store your trajectory manually after\n            all merges. Also make sure that all parameters and derived parameters are available\n            in your current trajectory and load them before the consecutive merging.\n            Also avoid specifying a `trial_parameter` and set `backup=False`\n            to avoid quadratic complexity in case of consecutive merges.\n\n        :param slow_merge:\n            Enforces a slow merging. This means all data is loaded one after the other to\n            memory and stored to disk. Otherwise it is tried to directly copy the data\n            from one file into another without explicitly loading the data.\n\n        If you cannot directly merge trajectories within one HDF5 file, a slow merging process\n        is used. Results are loaded, stored, and emptied again one after the other. Might take\n        some time!\n\n        Annotations of parameters and derived parameters under `.derived_parameters.trajectory`\n        are NOT merged. If you wish to extract the annotations of these parameters you have to\n        do that manually before merging. Note that annotations of results and derived parameters\n        of single runs are copied, so you don't have to worry about these.\n\n        \"\"\"\n        if consecutive_merge and trial_parameter is not None:\n            self._logger.warning('If you do a consecutive merge and specify a trial parameter, '\n                                 'your merging will still suffer from quadratic time complexity!')\n        if consecutive_merge and backup:\n            self._logger.warning('If you do a consecutive merge and backup, '\n                                 'your merging will still suffer from quadratic time complexity!')\n\n        # Keep the timestamp of the merge\n        timestamp = time.time()\n        original_ignore_data = set(ignore_data)\n        ignore_data = original_ignore_data.copy()\n\n        old_len = len(self)\n\n        # Check if trajectories can be merged\n        self._check_if_both_have_same_parameters(other_trajectory, ignore_data, consecutive_merge)\n\n        # Create a full mapping set for renaming\n        self._make_reversed_wildcards(old_length=old_len)\n        other_trajectory._make_reversed_wildcards()\n\n        # BACKUP if merge is possible\n        if backup:\n            other_trajectory.f_backup()\n            self.f_backup()\n\n        # Merge parameters and keep track which runs where used and which parameters need\n        # to be updated\n        self._logger.info('Merging the parameters')\n        used_runs, changed_parameters = self._merge_parameters(\n            other_trajectory,\n            remove_duplicates,\n            trial_parameter,\n            ignore_data)\n\n        ignore_data.update(set(changed_parameters))\n\n        if len(used_runs) == 0:\n            raise ValueError('Your merge discards all runs of the other trajectory, maybe you '\n                             'try to merge a trajectory with itself?')\n\n        # Dictionary containing the mappings between run names in the other trajectory\n        # and their new names in the current trajectory\n        rename_dict = {}\n\n        self._logger.info('Merging run information')\n        allowed_translations = set([translation for translation, pair in\n                                    other_trajectory._reversed_wildcards.items() if\n                                        any(x in used_runs for x in pair[0])])\n        self._merge_single_runs(other_trajectory, used_runs)\n\n        self._logger.info('Merging derived parameters')\n        self._merge_derived_parameters(other_trajectory=other_trajectory,\n                                         used_runs=used_runs,\n                                         rename_dict=rename_dict,\n                                         allowed_translations=allowed_translations,\n                                         ignore_data=ignore_data)\n\n        # Keep track of all trajectory results that should be merged and put\n        # information into `rename_dict`\n        self._merge_results(other_trajectory=other_trajectory,\n                             used_runs=used_runs,\n                             rename_dict=rename_dict,\n                             allowed_translations=allowed_translations,\n                             ignore_data=ignore_data)\n\n\n\n        # The storage service needs to prepare the file for merging.\n        # This includes updating meta information and already storing the merged parameters\n        self._logger.info('Start copying results and single run derived parameters')\n        self._logger.info('Updating Trajectory information and changed parameters in storage')\n        self._storage_service.store(pypetconstants.PREPARE_MERGE, self,\n                                    trajectory_name=self.v_name,\n                                    changed_parameters=changed_parameters,\n                                    old_length=old_len)\n\n        if not slow_merge:\n            try:\n                # Merge the single run derived parameters and all results\n                # within the same hdf5 file based on `renamed_dict`\n                try:\n                    # try to get the other file\n                    other_filename = other_trajectory.v_storage_service.filename\n                except AttributeError:\n                    self._logger.warning('Could not determine the filename of the other '\n                                         'trajectory, I will assume it`s in the same file.')\n                    other_filename = None\n                self._storage_service.store(pypetconstants.MERGE, 'FAST MERGE', trajectory_name=self.v_name,\n                                            other_trajectory_name=other_trajectory.v_name,\n                                            rename_dict=rename_dict, move_nodes=move_data,\n                                            delete_trajectory=delete_other_trajectory,\n                                            other_filename=other_filename)\n\n            except pex.NoSuchServiceError as exc:\n                # If the storage service does not support merge we end up here\n                self._logger.exception('My storage service does not support merging of trajectories, '\n                                     'I will use the f_load mechanism of the other trajectory and '\n                                     'store the results slowly item by item. '\n                                     'Note that thereby the other '\n                                     'trajectory will be altered (in RAM).')\n                slow_merge = True\n\n            except ValueError as exc:\n                # If both trajectories are stored in separate files we end up here\n                self._logger.exception('Could not perfom fast merging. '\n                                     'I will use the `f_load` method of the other trajectory and '\n                                     'store the results slowly item by item. '\n                                     'Note that thereby the other '\n                                     'trajectory will be altered (in RAM).')\n                slow_merge = True\n\n        if slow_merge:\n            self._merge_slowly(other_trajectory, rename_dict)\n\n        # We will merge the git commits and other config data\n        if merge_config:\n            self._merge_config(other_trajectory)\n\n        # Finally merging links\n        self._merge_links(other_trajectory=other_trajectory,\n                         used_runs=used_runs,\n                         allowed_translations=allowed_translations,\n                         ignore_data=original_ignore_data)\n\n        # Add meta information about the merge to the current trajectory\n        self._logger.info('Adding merge information')\n        formatted_time = datetime.datetime.fromtimestamp(timestamp).strftime('%Y_%m_%d_%Hh%Mm%Ss')\n        hexsha = hashlib.sha1((self.v_name +\n                                             str(self.v_timestamp) +\n                                             other_trajectory.v_name +\n                                             str(other_trajectory.v_timestamp) +\n                                             VERSION).encode('utf-8')).hexdigest()\n\n        short_hexsha = hexsha[0:7]\n\n        if keep_info:\n            merge_name = 'merge_%s_%s' % (short_hexsha, formatted_time)\n\n            config_name = 'merge.%s.merged_runs' % merge_name\n            self.f_add_config(config_name, len(used_runs),\n                              comment='Number of merged runs')\n\n            config_name = 'merge.%s.timestamp' % merge_name\n            self.f_add_config(config_name, timestamp,\n                              comment='Timestamp of merge')\n\n            config_name = 'merge.%s.hexsha' % merge_name\n            self.f_add_config(config_name, hexsha,\n                              comment='SHA-1 identifier of the merge')\n\n            config_name = 'merge.%s.remove_duplicates' % merge_name\n            self.f_add_config(config_name, remove_duplicates,\n                              comment='Option to remove duplicate entries')\n\n            if original_ignore_data:\n                config_name = 'merge.%s.ignore_data' % merge_name\n                self.f_add_config(config_name, tuple(original_ignore_data),\n                                  comment='Data to ignore during merge')\n\n            config_name = 'merge.%s.length_before_merge' % merge_name\n            self.f_add_config(config_name, len(self),\n                              comment='Length of trajectory before merge')\n\n            self.config.merge.v_comment = 'Settings and information of the different merges'\n\n            if self.v_version != VERSION:\n                config_name = 'merge.%s.version' % merge_name\n                self.f_add_config(config_name, self.v_version,\n                                  comment='Pypet version if it differs from the version'\n                                          ' of the trajectory')\n\n            if trial_parameter is not None:\n                config_name = 'merge.%s.trial_parameter' % merge_name\n                self.f_add_config(config_name, len(other_trajectory),\n                                  comment='Name of trial parameter')\n\n            if keep_other_trajectory_info:\n\n                if other_trajectory.v_version != self.v_version:\n                    config_name = 'merge.%s.other_trajectory.version' % merge_name\n                    self.f_add_config(config_name, other_trajectory.v_version,\n                                      comment='The version of pypet you used to manage the other'\n                                              ' trajectory. Only added if other trajectory\\'s'\n                                              ' version differs from current trajectory version.')\n\n                config_name = 'merge.%s.other_trajectory.name' % merge_name\n                self.f_add_config(config_name, other_trajectory.v_name,\n                                  comment='Name of other trajectory merged into the current one')\n\n                config_name = 'merge.%s.other_trajectory.timestamp' % merge_name\n                self.f_add_config(config_name, other_trajectory.v_timestamp,\n                                  comment='Timestamp of creation of other trajectory '\n                                          'merged into the current one')\n\n                config_name = 'merge.%s.other_trajectory.length' % merge_name\n                self.f_add_config(config_name, len(other_trajectory),\n                                  comment='Length of other trajectory')\n\n                if other_trajectory.v_comment:\n                    config_name = 'merge.%s.other_trajectory.comment' % merge_name\n                    self.f_add_config(config_name, other_trajectory.v_comment,\n                                      comment='Comment of other trajectory')\n\n        # Write out the merged data to disk\n        if not consecutive_merge:\n            self._logger.info('Writing merged data to disk')\n            self.f_store(store_data=pypetconstants.STORE_DATA)\n            self._reversed_wildcards = {}\n        other_trajectory._reversed_wildcards = {}\n\n        self._logger.info('Finished Merging!')", "response": "Merges another trajectory into the current trajectory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _merge_single_runs(self, other_trajectory, used_runs):\n        count = len(self)  # Variable to count the increasing new run indices and create\n        # new run names\n\n        run_indices = range(len(other_trajectory))\n\n        run_name_dict = OrderedDict()\n\n        to_store_groups_with_annotations = []\n\n        for idx in run_indices:\n            # Iterate through all used runs and store annotated groups and mark results and\n            # derived parameters for merging\n            if idx in used_runs:\n                # Update the run information dict of the current trajectory\n                other_info_dict = other_trajectory.f_get_run_information(idx)\n                time_ = other_info_dict['time']\n                timestamp = other_info_dict['timestamp']\n                completed = other_info_dict['completed']\n                short_environment_hexsha = other_info_dict['short_environment_hexsha']\n                finish_timestamp = other_info_dict['finish_timestamp']\n                runtime = other_info_dict['runtime']\n\n                new_idx = used_runs[idx]\n                new_runname = self.f_wildcard('$', new_idx)\n\n                run_name_dict[idx] = new_runname\n\n                info_dict = dict(\n                    idx=new_idx,\n                    time=time_,\n                    timestamp=timestamp,\n                    completed=completed,\n                    short_environment_hexsha=short_environment_hexsha,\n                    finish_timestamp=finish_timestamp,\n                    runtime=runtime)\n\n                self._add_run_info(**info_dict)", "response": "Updates the run information dict of the current trajectory with the run information of the other trajectory."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _rename_full_name(self, full_name, other_trajectory, used_runs=None, new_run_idx=None):\n        split_name = full_name.split('.')\n        for idx, name in enumerate(split_name):\n            if name in other_trajectory._reversed_wildcards:\n                run_indices, wildcards = other_trajectory._reversed_wildcards[name]\n                if new_run_idx is None:\n                    # We can safely take the first index of the index list that matches\n                    run_idx = None\n                    for run_jdx in run_indices:\n                        if run_jdx in used_runs:\n                            run_idx = used_runs[run_jdx]\n                            break\n                        elif run_jdx == -1:\n                            run_idx = -1\n                            break\n                    if run_idx is None:\n                        raise RuntimeError('You shall not pass!')\n                else:\n                    run_idx = new_run_idx\n                new_name = self.f_wildcard(wildcards[0], run_idx)\n                split_name[idx] = new_name\n        full_name = '.'.join(split_name)\n        return full_name", "response": "Renames a full name based on the wildcards and a particular run"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _merge_derived_parameters(self,\n                                    other_trajectory,\n                                    used_runs,\n                                    rename_dict,\n                                    allowed_translations,\n                                    ignore_data):\n        \"\"\" Merges derived parameters that have the `run_ALL` in a name.\n\n        Creates a new parameter with the name of the first new run and links to this\n        parameter to avoid copying in all other runs.\n\n        \"\"\"\n        other_derived_parameters = other_trajectory._derived_parameters.copy()\n        # get first run_idx\n        new_first_run_idx = min(used_runs.values())\n\n        run_name_dummy = other_trajectory.f_wildcard('$', -1)\n        for param_name in other_derived_parameters:\n            if param_name in ignore_data:\n                continue\n            split_name = param_name.split('.')\n            if not any(x in run_name_dummy for x in split_name):\n                continue\n            ignore_data.add(param_name)\n\n            param = other_derived_parameters[param_name]\n            new_param_name = self._rename_full_name(param_name, other_trajectory,\n                                                    used_runs=used_runs)\n            if new_param_name in self:\n                my_param = self.f_get(new_param_name, fast_access=False)\n                if (my_param._equal_values(my_param.f_get(), param.f_get()) and\n                        not (my_param.f_has_range() or param.f_has_range())):\n                    continue\n\n            first_new_param_name = self._rename_full_name(param_name,\n                                                          other_trajectory,\n                                                          new_run_idx=new_first_run_idx)\n\n            rename_dict[param_name] = first_new_param_name\n            comment = param.v_comment\n            param_type = param.f_get_class_name()\n            param_type = self._create_class(param_type)\n            first_param = self.f_add_leaf(param_type,\n                                          first_new_param_name,\n                                          comment=comment)\n            for run_idx in used_runs.values():\n                if run_idx == new_first_run_idx:\n                    continue\n                next_name = self._rename_full_name(param_name, other_trajectory,\n                                                    new_run_idx=run_idx)\n                split_name = next_name.split('.')\n                link_name = split_name.pop()\n                location_name = '.'.join(split_name)\n                if not self.f_contains(location_name, shortcuts=False):\n                    the_group = self.f_add_group(location_name)\n                else:\n                    the_group = self.f_get(location_name)\n\n                the_group.f_add_link(link_name, first_param)\n\n        for param_name in other_derived_parameters:\n            if param_name in ignore_data:\n                continue\n            split_name = param_name.split('.')\n            ignore_data.add(param_name)\n            if any(x in other_trajectory._reversed_wildcards and x not in allowed_translations\n                        for x in split_name):\n                continue\n            new_name = self._rename_full_name(param_name, other_trajectory,\n                                                    used_runs=used_runs)\n            if self.f_contains(new_name):\n                my_param = self.f_get(new_name, fast_access=False)\n                param = other_derived_parameters[param_name]\n                if (my_param._equal_values(my_param.f_get(), param.f_get()) and\n                        not (my_param.f_has_range() or param.f_has_range())):\n                    continue\n                else:\n                    self._logger.error('Could not merge parameter `%s`. '\n                                       'I will ignore it!' % new_name)\n            rename_dict[param_name] = new_name", "response": "Merges derived parameters that have the run_ALL flag set."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmerge all links in the other_trajectory with the current one.", "response": "def _merge_links(self, other_trajectory, used_runs, allowed_translations, ignore_data):\n        \"\"\" Merges all links\"\"\"\n        linked_items = other_trajectory._linked_by\n        run_name_dummys = set([f(-1) for f in other_trajectory._wildcard_functions.values()])\n        if len(linked_items) > 0:\n            self._logger.info('Merging potential links!')\n            for old_linked_name in other_trajectory._linked_by:\n                if old_linked_name in ignore_data:\n                    continue\n                split_name = old_linked_name.split('.')\n                if any(x in run_name_dummys for x in split_name):\n                    self._logger.warning('Ignoring all links linking to `%s` because '\n                                     'I don`t know how to resolve links under `%s` nodes.' %\n                                     (old_linked_name, str(run_name_dummys)))\n                    continue\n                old_link_dict = other_trajectory._linked_by[old_linked_name]\n                split_name = old_linked_name.split('.')\n                if all(x in allowed_translations for x in split_name):\n                    new_linked_full_name = self._rename_full_name(old_linked_name,\n                                                                  other_trajectory,\n                                                                  used_runs=used_runs)\n                else:\n                    new_linked_full_name = old_linked_name\n\n                for linking_node, link_set in old_link_dict.values():\n\n                    linking_full_name = linking_node.v_full_name\n                    split_name = linking_full_name .split('.')\n                    if any(x in run_name_dummys for x in split_name):\n                        self._logger.warning('Ignoring links under `%s` because '\n                                             'I don`t know how to resolve links '\n                                             'under a `%s` node.' %\n                                             (linking_full_name, str(run_name_dummys)))\n\n                    split_name = linking_full_name .split('.')\n                    if any(x in allowed_translations for x in split_name):\n                        new_linking_full_name = self._rename_full_name(linking_full_name,\n                                                                      other_trajectory,\n                                                                      used_runs=used_runs)\n                    else:\n                        new_linking_full_name = linking_full_name\n\n                    for link in link_set:\n                        if (linking_full_name + '.' + link) in ignore_data:\n                            continue\n                        if link in run_name_dummys:\n                            self._logger.warning('Ignoring link `%s` under `%s` because '\n                                                 'I don`t know how to resolve '\n                                                 'links named as `%s`.' %\n                                                 (link,\n                                                  linking_full_name,\n                                                  str(run_name_dummys)))\n                            continue\n\n                        try:\n                            new_linked_item = self.f_get(new_linked_full_name,\n                                                         shortcuts=False)\n                            if self.f_contains(new_linking_full_name):\n                                new_linking_item = self.f_get(new_linking_full_name,\n                                                              shortcuts=False)\n                            else:\n                                new_linking_item =  self.f_add_group(new_linking_full_name)\n                            if link in allowed_translations:\n                                run_indices, wildcards = other_trajectory._reversed_wildcards[link]\n                                link = self.f_wildcard(wildcards[0], used_runs[run_indices[0]])\n                            if not link in new_linking_item._links:\n                                new_linking_item.f_add_link(link, new_linked_item)\n                            else:\n                                self._logger.debug('Link `%s` exists already under `%s`.' %\n                                                    (link, new_linked_item.v_full_name))\n                        except (AttributeError, ValueError) as exc:\n                            self._logger.error('Could not copy link `%s` under `%s` linking '\n                                               'to `%s` due to `%s`' %\n                                               (link, linking_full_name, old_linked_name,\n                                                repr(exc)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _merge_config(self, other_trajectory):\n        self._logger.info('Merging config!')\n\n        # Merge git commit meta data\n        if 'config.git' in other_trajectory:\n\n            self._logger.info('Merging git commits!')\n            git_node = other_trajectory.f_get('config.git')\n            param_list = []\n            for param in git_node.f_iter_leaves(with_links=False):\n                if not self.f_contains(param.v_full_name, shortcuts=False):\n                    param_list.append(self.f_add_config(param))\n\n            if param_list:\n                self.f_store_items(param_list)\n\n            self._logger.info('Merging git commits successful!')\n\n        # Merge environment meta data\n        if 'config.environment' in other_trajectory:\n\n            self._logger.info('Merging environment config!')\n            env_node = other_trajectory.f_get('config.environment')\n            param_list = []\n            for param in env_node.f_iter_leaves(with_links=False):\n                if not self.f_contains(param.v_full_name, shortcuts=False):\n                    param_list.append(self.f_add_config(param))\n\n            if param_list:\n                self.f_store_items(param_list)\n\n            self._logger.info('Merging config successful!')\n\n        # Merge meta data of previous merges\n        if 'config.merge' in other_trajectory:\n\n            self._logger.info('Merging merge config!')\n            merge_node = other_trajectory.f_get('config.merge')\n            param_list = []\n            for param in merge_node.f_iter_leaves(with_links=False):\n                if not self.f_contains(param.v_full_name, shortcuts=False):\n                    param_list.append(self.f_add_config(param))\n\n            if param_list:\n                self.f_store_items(param_list)\n\n            self._logger.info('Merging config successful!')", "response": "Merges the config of the current trajectory into the other trajectory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _merge_slowly(self, other_trajectory, rename_dict):\n        for other_key in rename_dict:\n            new_key = rename_dict[other_key]\n\n            other_instance = other_trajectory.f_get(other_key)\n\n            if other_instance.f_is_empty():\n                # To suppress warnings if nothing needs to be loaded\n                with self._nn_interface._disable_logging:\n                    other_trajectory.f_load_item(other_instance)\n\n            if not self.f_contains(new_key):\n                class_name = other_instance.f_get_class_name()\n                class_ = self._create_class(class_name)\n                my_instance = self.f_add_leaf(class_, new_key)\n            else:\n                my_instance = self.f_get(new_key, shortcuts=False)\n\n            if not my_instance.f_is_empty():\n                raise RuntimeError('Something is wrong! Your item `%s` should be empty.' % new_key)\n\n            load_dict = other_instance._store()\n            my_instance._load(load_dict)\n            my_instance.f_set_annotations(**other_instance.v_annotations.f_to_dict(copy=False))\n            my_instance.v_comment = other_instance.v_comment\n\n            self.f_store_item(my_instance)\n\n            # We do not want to blow up the RAM Memory\n            if other_instance.v_is_parameter:\n                other_instance.f_unlock()\n                my_instance.f_unlock()\n            other_instance.f_empty()\n            my_instance.f_empty()", "response": "Merges trajectories by loading iteratively items of the other trajectory and store them into the current trajectory."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _merge_results(self, other_trajectory, rename_dict, used_runs, allowed_translations,\n                       ignore_data):\n        \"\"\"Merges all results.\n\n        :param rename_dict:\n\n            Dictionary that is filled with the names of results in the `other_trajectory`\n            as keys and the corresponding new names in the current trajectory as values.\n            Note for results kept under trajectory run branch there is actually no need to\n            change the names. So we will simply keep the original name.\n\n        \"\"\"\n\n        other_results = other_trajectory._results.copy()\n\n        for result_name in other_results:\n            if result_name in ignore_data:\n                continue\n            split_name = result_name.split('.')\n            ignore_data.add(result_name)\n            if any(x in other_trajectory._reversed_wildcards and x not in allowed_translations\n                        for x in split_name):\n                continue\n            new_name = self._rename_full_name(result_name, other_trajectory,\n                                                    used_runs=used_runs)\n            if self.f_contains(new_name):\n                self._logger.warning('I found result `%s` already, I will ignore it.' % new_name)\n                continue\n            rename_dict[result_name] = new_name", "response": "Merges all results in the current trajectory with the other trajectory."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _merge_parameters(self, other_trajectory, remove_duplicates=False,\n                          trial_parameter_name=None,\n                          ignore_data=()):\n        \"\"\"Merges parameters from the other trajectory into the current one.\n\n        The explored parameters in the current trajectory are directly enlarged (in RAM),\n        no storage service is needed here. Later on in `f_merge` the storage service\n        will be requested to store the enlarge parameters to disk.\n\n        Note explored parameters are always enlarged. Unexplored parameters might become\n        new explored parameters if they differ in their default values\n        in the current and the other trajectory, respectively.\n\n        :return: A tuple with two elements:\n\n            1.\n\n                Dictionary of run index mappings from old trajectroy to the new one.\n\n            2.\n\n                List of names of parameters that were altered.\n\n        \"\"\"\n\n        if trial_parameter_name:\n            if remove_duplicates:\n                self._logger.warning('You have given a trial parameter and you want to '\n                                     'remove_items duplicates. There cannot be any duplicates '\n                                     'when adding trials, I will not look for duplicates.')\n                remove_duplicates = False\n\n        # Dictionary containing full parameter names as keys\n        # and pairs of parameters from both trajectories as values.\n        # Parameters kept in this dictionary are marked for merging and will be enlarged\n        # with ranges and values of corresponding parameters in the other trajectory\n        params_to_change = {}\n\n        if trial_parameter_name:\n            # We want to merge a trial parameter\n            # First make some sanity checks\n            my_trial_parameter = self.f_get(trial_parameter_name)\n            other_trial_parameter = other_trajectory.f_get(trial_parameter_name)\n            if not isinstance(my_trial_parameter, BaseParameter):\n                raise TypeError('Your trial_parameter `%s` does not evaluate to a real parameter'\n                                ' in the trajectory' % trial_parameter_name)\n\n            # Extract the ranges of both trial parameters\n            if my_trial_parameter.f_has_range():\n                my_trial_list = my_trial_parameter.f_get_range(copy=False)\n            else:\n                # If we only have a single trial, we need to make a range of length 1\n                # This is probably a very exceptional case\n                my_trial_list = [my_trial_parameter.f_get()]\n\n            if other_trial_parameter.f_has_range():\n                other_trial_list = other_trial_parameter.f_get_range(copy=False)\n            else:\n                other_trial_list = [other_trial_parameter.f_get()]\n\n            # Make sanity checks if both ranges contain all numbers from 0 to T1\n            # for the current trajectory and 0 to T2 for the other trajectory\n            mytrialset = set(my_trial_list)\n            mymaxtrial_T1 = max(mytrialset)  # maximum trial index in current trajectory aka T1\n\n            if mytrialset != set(range(mymaxtrial_T1 + 1)):\n                raise TypeError('In order to specify a trial parameter, this parameter must '\n                                'contain integers from 0 to %d, but it in fact it '\n                                'contains `%s`.' % (mymaxtrial_T1, str(mytrialset)))\n\n            othertrialset = set(other_trial_list)\n            othermaxtrial_T2 = max(othertrialset)  # maximum trial index in other trajectory aka T2\n            if othertrialset != set(range(othermaxtrial_T2 + 1)):\n                raise TypeError('In order to specify a trial parameter, this parameter must '\n                                'contain integers from 0 to %d, but it infact it contains `%s` '\n                                'in the other trajectory.' %\n                                (othermaxtrial_T2, str(othertrialset)))\n\n            # If the trial parameter's name was just given in parts we update it here\n            # to the full name\n            trial_parameter_name = my_trial_parameter.v_full_name\n\n            # If we had the very exceptional case, that our trial parameter was not explored,\n            # aka we only had 1 trial, we have to add it to the explored parameters\n            if not trial_parameter_name in self._explored_parameters:\n                self._explored_parameters[trial_parameter_name] = my_trial_parameter\n\n            # We need to mark the trial parameter for merging\n            params_to_change[trial_parameter_name] = (my_trial_parameter, other_trial_parameter)\n\n        # Dictionary containing all parameters of the other trajectory, we will iterate through it\n        # to spot parameters that need to be enlarge or become new explored parameters\n        params_to_merge = other_trajectory._parameters.copy()\n\n        params_to_merge.update(other_trajectory._derived_parameters)\n        for ignore in ignore_data:\n            if ignore in params_to_merge:\n                del params_to_merge[ignore]\n\n        run_name_dummys = set([f(-1) for f in other_trajectory._wildcard_functions.values()])\n        # Iterate through all parameters of the other trajectory\n        # and check which differ from the parameters of the current trajectory\n        for key in params_to_merge:\n            other_param = params_to_merge[key]\n\n            # We don't need to merge anything based on wildcards\n            split_key = key.split('.')\n            if any(x in other_trajectory._reversed_wildcards for x in split_key):\n                continue\n\n            my_param = self.f_get(key)\n            if not my_param._values_of_same_type(my_param.f_get(), other_param.f_get()):\n                raise TypeError('The parameters with name `%s` are not of the same type, cannot '\n                                'merge trajectory.' % key)\n\n            # We have taken care about the trial parameter before, it is already\n            # marked for merging\n            if my_param.v_full_name == trial_parameter_name:\n                continue\n\n            # If a parameter was explored in one of the trajectories or two unexplored\n            # parameters differ, we need to mark them for merge\n            if (my_param.f_has_range() or\n                    other_param.f_has_range() or\n                    not my_param._equal_values(my_param.f_get(), other_param.f_get())):\n\n                # If two unexplored parameters differ, that means they differ in every run,\n                # accordingly we do not need to check for duplicate runs anymore\n                params_to_change[key] = (my_param, other_param)\n                if not my_param.f_has_range() and not other_param.f_has_range():\n                    remove_duplicates = False\n\n        # Check if we use all runs or remove duplicates:\n        used_runs = {}\n        for idx in range(len(other_trajectory)):\n            used_runs[idx] = idx\n        if remove_duplicates:\n\n            # We need to compare all parameter combinations in the current trajectory\n            # to all parameter combinations in the other trajectory to spot duplicate points.\n            # Quadratic Complexity!\n            for irun in range(len(other_trajectory)):\n                for jrun in range(len(self)):\n                    change = True\n\n                    # Check all marked parameters\n                    for my_param, other_param in params_to_change.values():\n                        if other_param.f_has_range():\n                            other_param._set_parameter_access(irun)\n\n                        if my_param.f_has_range():\n                            my_param._set_parameter_access(jrun)\n\n                        val1 = my_param.f_get()\n                        val2 = other_param.f_get()\n\n                        # If only one parameter differs, the parameter space point differs\n                        # and we can skip the rest of the parameters\n                        if not my_param._equal_values(val1, val2):\n                            change = False\n                            break\n\n                    # If we found one parameter space point in the current trajectory\n                    # that matches the ith point in the other, we do not need the ith\n                    # point. We can also skip comparing to the rest of the points in the\n                    # current trajectory\n                    if change:\n                        del used_runs[irun]\n                        break\n\n            # Restore changed default values\n            for my_param, other_param in params_to_change.values():\n                other_param._restore_default()\n                my_param._restore_default()\n\n\n        # Merge parameters into the current trajectory\n        adding_length = len(used_runs)\n        starting_length = len(self)\n        if adding_length == 0:\n            return used_runs, []\n        count = 0\n        for key in sorted(used_runs.keys()):\n            used_runs[key] = starting_length + count\n            count += 1\n\n        for my_param, other_param in params_to_change.values():\n            fullname = my_param.v_full_name\n\n            # We need new ranges to enlarge all parameters marked for merging\n            if fullname == trial_parameter_name:\n                # The trial parameter now has to cover the range 0 to T1+T2+1\n                other_range = [x + mymaxtrial_T1 + 1 for x in other_trial_list]\n            else:\n                # In case we do not use all runs we need to filter the ranges of the\n                # parameters of the other trajectory\n                if other_param.f_has_range():\n                    other_range = (x for jdx, x in enumerate(other_param.f_get_range(copy=False))\n                                                             if jdx in used_runs)\n                else:\n                    other_range = (other_param.f_get() for _ in range(adding_length))\n\n            # If a parameter in the current trajectory was marked for merging but was not\n            # explored before, we need to explore it first, simply by creating the range of\n            # the current trajectory's length containing only it's default value\n            if not my_param.f_has_range():\n                my_param.f_unlock()\n                my_param._explore((my_param.f_get() for _ in  range(len(self))))\n\n            # After determining the new range extension `other_range`,\n            # expand the parameters\n            my_param.f_unlock()\n            my_param._expand(other_range)\n\n            if not fullname in self._explored_parameters:\n                self._explored_parameters[fullname] = my_param\n\n        return used_runs, list(params_to_change.keys())", "response": "Merges parameters from the current trajectory into the other trajectory."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstores the trajectory to disk and recursively all data in the tree.", "response": "def f_store(self, only_init=False, store_data=pypetconstants.STORE_DATA,\n                max_depth=None):\n        \"\"\" Stores the trajectory to disk and recursively all data in the tree.\n\n        :param only_init:\n\n            If you just want to initialise the store. If yes, only meta information about\n            the trajectory is stored and none of the groups/leaves within the trajectory.\n            Alternatively, you can pass `recursive=False`.\n\n        :param store_data:\n\n            Only considered if ``only_init=False``. Choose of the following:\n\n                * :const:`pypet.pypetconstants.STORE_NOTHING`: (0)\n\n                    Nothing is store.\n\n                * :const:`pypet.pypetconstants.STORE_DATA_SKIPPING`: (1)\n\n                    Speedy version of normal ``STORE_DATA`` will entirely skip groups\n                    (but not their children) and leaves if they have been stored before.\n                    No new data is added in this case.\n\n                * :const:`pypet.pypetconstants.STORE_DATA`: (2)\n\n                    Stores every group and leave node. If they contain data that is not yet stored\n                    to disk it is added.\n\n                * :const:`pypet.pypetconstants.OVERWRITE_DATA`: (3)\n\n                    Stores all groups and leave nodes and will delete all data on disk\n                    and overwrite it with the current data in RAM.\n\n                    **NOT RECOMMENDED**! Overwriting data on disk fragments the HDF5 file and\n                    yields badly compressed large files. Better stick to the concept\n                    write once and read many!\n\n\n        If you use the HDF5 Storage Service usually (STORE_DATA (2)) only novel data\n        is stored to disk.\n        If you have results that have been stored to disk before only new data items are added and\n        already present data is NOT overwritten.\n\n        Overwriting (OVERWRITE_DATA (3)) existing data with the HDF5 storage service\n        is not recommended due to fragmentation of the HDF5 file. Better stick to the concept\n        write once, but read often.\n\n        If you want to store individual parameters or results, you might want to\n        take a look at :func:`~pypet.Trajectory.f_store_items`.\n        To store whole subtrees of your trajectory check out\n        :func:`~pypet.naturalnaming.NNGroupNode.f_store_child`.\n        Note both functions require that your trajectory was stored to disk with `f_store`\n        at least once before.\n\n\n        **ATTENTION**: Calling `f_store` during a single run the behavior is different.\n\n        To avoid re-storing the full trajectory in every single run, which is redundant,\n        only sub-trees of the trajectory are really stored.\n\n        The storage serivce looks for new data that is added below groups called `run_XXXXXXXXXX`\n        and stores it where `XXXXXXXXX` is the index of this run. The `only_init` parameter is\n        ignored in this case. You can avoid this behavior by using the argument from below.\n\n        :param max_depth:\n\n            Maximum depth to store tree (inclusive). During single runs `max_depth` is also counted\n            from root.\n\n\n        \"\"\"\n        if self._is_run:\n            if self._new_nodes or self._new_links:\n                self._storage_service.store(pypetconstants.SINGLE_RUN, self,\n                                        trajectory_name=self.v_name,\n                                        recursive=not only_init,\n                                        store_data=store_data,\n                                        max_depth=max_depth)\n        else:\n            self._storage_service.store(pypetconstants.TRAJECTORY, self,\n                                        trajectory_name=self.v_name,\n                                        only_init=only_init,\n                                        store_data=store_data,\n                                        max_depth=max_depth)\n\n            self._stored = True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef f_is_empty(self):\n        return (len(self._parameters) == 0 and\n                len(self._derived_parameters) == 0 and\n                len(self._results) == 0 and\n                len(self._other_leaves) == 0)", "response": "Whether the trajectory contains no results nor derived parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef f_restore_default(self):\n        self._idx = -1\n        self._crun = None\n        for param in self._explored_parameters.values():\n            if param is not None:\n                param._restore_default()", "response": "Restores the default value in all explored parameters and sets the\n        v_idx property to - 1 and v_crun to None."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the index of the explored parameters to the index idx.", "response": "def _set_explored_parameters_to_idx(self, idx):\n        \"\"\" Notifies the explored parameters what current point in the parameter space\n        they should represent.\n\n        \"\"\"\n        for param in self._explored_parameters.values():\n            if param is not None:\n                param._set_parameter_access(idx)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _make_single_run(self):\n        self._is_run = False # to be able to use f_set_crun\n        self._new_nodes = OrderedDict()\n        self._new_links = OrderedDict()\n        self._is_run = True\n        return self", "response": "Modifies the trajectory for single runs executed by the environment"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of run names.", "response": "def f_get_run_names(self, sort=True):\n        \"\"\" Returns a list of run names.\n\n        ONLY useful for a single run during multiprocessing if ``v_full_copy` was set to ``True``.\n        Otherwise only the current run is available.\n\n        :param sort:\n\n            Whether to get them sorted, will only require O(N) [and not O(N*log N)] since we\n            use (sort of) bucket sort.\n\n        \"\"\"\n        if sort:\n            return [self.f_idx_to_run(idx) for idx in range(len(self))]\n        else:\n            return list(self._run_information.keys())"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dictionary containing information about a single run.", "response": "def f_get_run_information(self, name_or_idx=None, copy=True):\n        \"\"\" Returns a dictionary containing information about a single run.\n\n        ONLY useful during a single run if ``v_full_copy` was set to ``True``.\n        Otherwise only the current run is available.\n\n        The information dictionaries have the following key, value pairings:\n\n            * completed: Boolean, whether a run was completed\n\n            * idx: Index of a run\n\n            * timestamp: Timestamp of the run as a float\n\n            * time: Formatted time string\n\n            * finish_timestamp: Timestamp of the finishing of the run\n\n            * runtime: Total runtime of the run in human readable format\n\n            * name: Name of the run\n\n            * parameter_summary:\n\n                A string summary of the explored parameter settings for the particular run\n\n            * short_environment_hexsha: The short version of the environment SHA-1 code\n\n\n        If no name or idx is given then a nested dictionary with keys as run names and\n        info dictionaries as values is returned.\n\n        :param name_or_idx: str or int\n\n        :param copy:\n\n            Whether you want the dictionary used by the trajectory or a copy. Note if\n            you want the real thing, please do not modify it, i.e. popping or adding stuff. This\n            could mess up your whole trajectory.\n\n        :return:\n\n            A run information dictionary or a nested dictionary of information dictionaries\n            with the run names as keys.\n\n        \"\"\"\n        if name_or_idx is None:\n            if copy:\n                return cp.deepcopy(self._run_information)\n            else:\n                return self._run_information\n        try:\n            if copy:\n                # Since the information dictionaries only contain immutable items\n                # (float, int, str)\n                # the normal copy operation is sufficient\n                return self._run_information[name_or_idx].copy()\n            else:\n                return self._run_information[name_or_idx]\n        except KeyError:\n            # Maybe the user provided an idx, this would yield a key error and we\n            # have to convert it to a run name\n            name_or_idx = self.f_idx_to_run(name_or_idx)\n            if copy:\n                return self._run_information[name_or_idx].copy()\n            else:\n                return self._run_information[name_or_idx]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef f_find_idx(self, name_list, predicate):\n        if self._is_run and not self.v_full_copy:\n            raise TypeError('You cannot use this function during a multiprocessing signle run and '\n                            'not having ``v_full_copy=True``.')\n        if isinstance(name_list, str):\n            name_list = [name_list]\n\n        # First create a list of iterators, each over the range of the matched parameters\n        iter_list = []\n        for name in name_list:\n            param = self.f_get(name)\n            if not param.v_is_parameter:\n                raise TypeError('`%s` is not a parameter it is a %s, find idx is not applicable' %\n                                (name, str(type(param))))\n\n            if param.f_has_range():\n                iter_list.append(iter(param.f_get_range(copy=False)))\n            else:\n                iter_list.append(itools.repeat(param.f_get(), len(self)))\n\n        # Create a logical iterator returning `True` or `False`\n        # whether the user's predicate matches the parameter data\n        logic_iter = map(predicate, *iter_list)\n\n        # Now the run indices are the the indices where `logic_iter` evaluates to `True`\n        for idx, item in enumerate(logic_iter):\n            if item:\n                yield idx", "response": "Find a single run index given a particular condition on parameters."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef f_start_run(self, run_name_or_idx=None, turn_into_run=True):\n        if self._run_started:\n            return self\n\n        if run_name_or_idx is None:\n            if self.v_idx == -1:\n                raise ValueError('Cannot start run if trajectory is not set to a particular run')\n        else:\n            self.f_set_crun(run_name_or_idx)\n\n        self._run_started = True\n\n        if turn_into_run:\n            self._make_single_run()\n\n        self._set_start()\n\n        return self", "response": "Start a run of the current run."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the start timestamp and formatted time to the current time.", "response": "def _set_start(self):\n        \"\"\" Sets the start timestamp and formatted time to the current time. \"\"\"\n        init_time = time.time()\n        formatted_time = datetime.datetime.fromtimestamp(init_time).strftime('%Y_%m_%d_%Hh%Mm%Ss')\n        run_info_dict = self._run_information[self.v_crun]\n        run_info_dict['timestamp'] = init_time\n        run_info_dict['time'] = formatted_time\n        if self._environment_hexsha is not None:\n            run_info_dict['short_environment_hexsha'] = self._environment_hexsha[0:7]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _summarize_explored_parameters(self):\n\n        runsummary = ''\n        for idx, expparam in enumerate(self._explored_parameters.values()):\n\n            # Create the run summary for the `run` overview\n            if idx > 0:\n                runsummary += ',   '\n\n            valstr = expparam.f_val_to_str()\n\n            if len(valstr) >= pypetconstants.HDF5_STRCOL_MAX_COMMENT_LENGTH:\n                valstr = valstr[0:pypetconstants.HDF5_STRCOL_MAX_COMMENT_LENGTH - 3]\n                valstr += '...'\n\n            if expparam.v_name in runsummary:\n                param_name = expparam.v_full_name\n            else:\n                param_name = expparam.v_name\n\n            runsummary = runsummary + param_name + ': ' + valstr\n\n        return runsummary", "response": "Summarizes the parameter settings."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _set_finish(self):\n\n        run_info_dict = self._run_information[self.v_crun]\n        timestamp_run = run_info_dict['timestamp']\n\n        run_summary = self._summarize_explored_parameters()\n\n        finish_timestamp_run = time.time()\n\n        findatetime = datetime.datetime.fromtimestamp(finish_timestamp_run)\n        startdatetime = datetime.datetime.fromtimestamp(timestamp_run)\n\n        runtime_run = str(findatetime - startdatetime)\n\n        run_info_dict['parameter_summary'] = run_summary\n        run_info_dict['completed'] = 1\n        run_info_dict['finish_timestamp'] = finish_timestamp_run\n        run_info_dict['runtime'] = runtime_run", "response": "Sets the finish time and computes the runtime in human readable format"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new node.", "response": "def _construct_instance(self, constructor, full_name, *args, **kwargs):\n        \"\"\" Creates a new node. Checks if the new node needs to know the trajectory.\n\n        :param constructor:  The constructor to use\n        :param full_name: Full name of node\n        :param args: Arguments passed to constructor\n        :param kwargs: Keyword arguments passed to the constructor\n        :return:\n        \"\"\"\n        if getattr(constructor, 'KNOWS_TRAJECTORY', False):\n            return constructor(full_name, self, *args, **kwargs)\n        else:\n            return constructor(full_name, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _return_item_dictionary(param_dict, fast_access, copy):\n\n        if not copy and fast_access:\n            raise ValueError('You cannot access the original dictionary and use fast access at the'\n                             ' same time!')\n        if not fast_access:\n            if copy:\n                return param_dict.copy()\n            else:\n                return param_dict\n        else:\n            resdict = {}\n            for key in param_dict:\n                param = param_dict[key]\n\n                val = param.f_get()\n                resdict[key] = val\n\n            return resdict", "response": "Returns a dictionary containing all parameters all explored parameters all config all derived parameters all results."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncall by the environment after storing to perform some rollback operations. All results and derived parameters created in the current run are removed. Important for single processing to not blow up the parent trajectory with the results of all runs.", "response": "def _finalize_run(self):\n        \"\"\"Called by the environment after storing to perform some rollback operations.\n\n        All results and derived parameters created in the current run are removed.\n\n        Important for single processing to not blow up the parent trajectory with the results\n        of all runs.\n\n        \"\"\"\n        self._run_information[self.v_crun]['completed'] = 1\n        while len(self._new_links):\n            name_pair, child_parent_pair = self._new_links.popitem(last=False)\n            parent_node, _ = child_parent_pair\n            _, link = name_pair\n            parent_node.f_remove_child(link)\n\n        while len(self._new_nodes):\n            _, child_parent_pair = self._new_nodes.popitem(last=False)\n            parent, child = child_parent_pair\n            child_name = child.v_name\n            parent.f_remove_child(child_name, recursive=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef f_to_dict(self, fast_access=False, short_names=False, nested=False,\n                  copy=True, with_links=True):\n        \"\"\"Returns a dictionary with pairings of (full) names as keys and instances/values.\n\n\n        :param fast_access:\n\n            If True, parameter values are returned instead of the instances.\n            Works also for results if they contain a single item with the name of the result.\n\n        :param short_names:\n\n            If true, keys are not full names but only the names. Raises a ValueError\n            if the names are not unique.\n\n        :param nested:\n\n            If true, a nested dictionary is returned.\n\n        :param copy:\n\n            If `fast_access=False` and `short_names=False` you can access the original\n            data dictionary if you set `copy=False`. If you do that, please do not\n            modify anything! Raises ValueError if `copy=False` and `fast_access=True`\n            or `short_names=True`.\n\n        :param with_links:\n\n            If links should be ignored\n\n        :return: dictionary\n\n        :raises: ValueError\n\n        \"\"\"\n        return self._nn_interface._to_dict(self, fast_access=fast_access,\n                                           short_names=short_names,\n                                           nested=nested,\n                                           copy=copy, with_links=with_links)", "response": "Returns a dictionary with pairings of full names as keys and instances and values as values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef f_get_config(self, fast_access=False, copy=True):\n        return self._return_item_dictionary(self._config, fast_access, copy)", "response": "Returns a dictionary containing the full config names as keys and the config parameters as values."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef f_get_parameters(self, fast_access=False, copy=True):\n        return self._return_item_dictionary(self._parameters, fast_access, copy)", "response": "Returns a dictionary containing the full parameter names as keys and the parameter data items as values."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a dictionary containing the full parameter names as keys and the parameter data items as values.", "response": "def f_get_explored_parameters(self, fast_access=False, copy=True):\n        \"\"\" Returns a dictionary containing the full parameter names as keys and the parameters\n         or the parameter data items as values.\n\n         IMPORTANT: This dictionary always contains all explored parameters as keys.\n         Even when they are not loaded, in this case the value is simply `None`.\n         `fast_access` only works if all explored parameters are loaded.\n\n\n        :param fast_access:\n\n            Determines whether the parameter objects or their values are returned\n            in the dictionary.\n\n        :param copy:\n\n            Whether the original dictionary or a shallow copy is returned.\n            If you want the real dictionary please do not modify it at all!\n            Not Copying and fast access do not work at the same time! Raises ValueError\n            if fast access is true and copy false.\n\n        :return: Dictionary containing the parameters.\n\n        :raises: ValueError\n\n        \"\"\"\n        return self._return_item_dictionary(self._explored_parameters, fast_access, copy)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef f_get_derived_parameters(self, fast_access=False, copy=True):\n        return self._return_item_dictionary(self._derived_parameters, fast_access, copy)", "response": "Returns a dictionary containing the full parameter names as keys and the parameter data items as values."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a dictionary containing the full result names as keys and the corresponding result objects or result data items as values.", "response": "def f_get_results(self, fast_access=False, copy=True):\n        \"\"\" Returns a dictionary containing the full result names as keys and the corresponding\n        result objects or result data items as values.\n\n\n        :param fast_access:\n\n            Determines whether the result objects or their values are returned\n            in the dictionary. Works only for results if they contain a single item with\n            the name of the result.\n\n        :param copy:\n\n            Whether the original dictionary or a shallow copy is returned.\n            If you want the real dictionary please do not modify it at all!\n            Not Copying and fast access do not work at the same time! Raises ValueError\n            if fast access is true and copy false.\n\n        :return: Dictionary containing the results.\n\n        :raises: ValueError\n\n        \"\"\"\n        return self._return_item_dictionary(self._results, fast_access, copy)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstoring a single item in the trajectory store.", "response": "def f_store_item(self, item, *args, **kwargs):\n        \"\"\"Stores a single item, see also :func:`~pypet.trajectory.Trajectory.f_store_items`.\"\"\"\n        self.f_store_items([item], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstore individual items in the hdf5 storage service.", "response": "def f_store_items(self, iterator, *args, **kwargs):\n        \"\"\"Stores individual items to disk.\n\n        This function is useful if you calculated very large results (or large derived parameters)\n        during runtime and you want to write these to disk immediately and empty them afterwards\n        to free some memory.\n\n        Instead of storing individual parameters or results you can also store whole subtrees with\n        :func:`~pypet.naturalnaming.NNGroupNode.f_store_child`.\n\n\n        You can pass the following arguments to `f_store_items`:\n\n        :param iterator:\n\n            An iterable containing the parameters or results to store, either their\n            names or the instances. You can also pass group instances or names here\n            to store the annotations of the groups.\n\n        :param non_empties:\n\n            Optional keyword argument (boolean),\n            if `True` will only store the subset of provided items that are not empty.\n            Empty parameters or results found in `iterator` are simply ignored.\n\n        :param args: Additional arguments passed to the storage service\n\n        :param kwargs:\n\n            If you use the standard hdf5 storage service, you can pass the following additional\n            keyword argument:\n\n            :param overwrite:\n\n                List names of parts of your item that should\n                be erased and overwritten by the new data in your leaf.\n                You can also set `overwrite=True`\n                to overwrite all parts.\n\n                For instance:\n\n                    >>> traj.f_add_result('mygroup.myresult', partA=42, partB=44, partC=46)\n                    >>> traj.f_store()\n                    >>> traj.mygroup.myresult.partA = 333\n                    >>> traj.mygroup.myresult.partB = 'I am going to change to a string'\n                    >>> traj.f_store_item('mygroup.myresult', overwrite=['partA', 'partB'])\n\n                Will store `'mygroup.myresult'` to disk again and overwrite the parts\n                `'partA'` and `'partB'` with the new values `333` and\n                `'I am going to change to a string'`.\n                The data stored as `partC` is not changed.\n\n                Be aware that you need to specify the names of parts as they were stored\n                to HDF5. Depending on how your leaf construction works, this may differ\n                from the names the data might have in your leaf in the trajectory container.\n\n                Note that massive overwriting will fragment and blow up your HDF5 file.\n                Try to avoid changing data on disk whenever you can.\n\n        :raises:\n\n            TypeError:\n\n                If the (parent) trajectory has never been stored to disk. In this case\n                use :func:`pypet.trajectory.f_store` first.\n\n            ValueError: If no item could be found to be stored.\n\n        Note if you use the standard hdf5 storage service, there are no additional arguments\n        or keyword arguments to pass!\n\n        \"\"\"\n\n        if not self._stored:\n            raise TypeError('Cannot store stuff for a trajectory that has never been '\n                            'stored to disk. Please call traj.f_store(only_init=True) first.')\n\n        fetched_items = self._nn_interface._fetch_items(STORE, iterator, args, kwargs)\n\n        if fetched_items:\n            self._storage_service.store(pypetconstants.LIST, fetched_items,\n                                        trajectory_name=self.v_name)\n        else:\n            raise ValueError('Your storage was not successful, could not find a single item '\n                             'to store.')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading a single item from the store", "response": "def f_load_item(self, item, *args, **kwargs):\n        \"\"\"Loads a single item, see also :func:`~pypet.trajectory.Trajectory.f_load_items`\"\"\"\n        self.f_load_items([item], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading parameters and results specified in iterator.", "response": "def f_load_items(self, iterator, *args, **kwargs):\n        \"\"\"Loads parameters and results specified in `iterator`.\n\n        You can directly list the Parameter objects or just their names.\n\n        If names are given the `~pypet.naturalnaming.NNGroupNode.f_get` method is applied to find the\n        parameters or results in the trajectory. Accordingly, the parameters and results\n        you want to load must already exist in your trajectory (in RAM), probably they\n        are just empty skeletons waiting desperately to handle data.\n        If they do not exist in RAM yet, but have been stored to disk before,\n        you can call :func:`~pypet.trajectory.Trajectory.f_load_skeleton` in order to\n        bring your trajectory tree skeleton up to date. In case of a single run you can\n        use the :func:`~pypet.naturalnaming.NNGroupNode.f_load_child` method to recursively\n        load a subtree without any data.\n        Then you can load the data of individual results or parameters one by one.\n\n        If want to load the whole trajectory at once or ALL results and parameters that are\n        still empty take a look at :func:`~pypet.trajectory.Trajectory.f_load`.\n        As mentioned before, to load subtrees of your trajectory you might want to check out\n        :func:`~pypet.naturalnaming.NNGroupNode.f_load_child`.\n\n        To load a list of parameters or results with `f_load_items` you can pass\n        the following arguments:\n\n        :param iterator: A list with parameters or results to be loaded.\n\n        :param only_empties:\n\n            Optional keyword argument (boolean),\n            if `True` only empty parameters or results are passed\n            to the storage service to get loaded. Non-empty parameters or results found in\n            `iterator` are simply ignored.\n\n        :param args: Additional arguments directly passed to the storage service\n\n        :param kwargs:\n\n            Additional keyword arguments directly passed to the storage service\n            (except the kwarg `only_empties`)\n\n            If you use the standard hdf5 storage service, you can pass the following additional\n            keyword arguments:\n\n            :param load_only:\n\n                If you load a result, you can partially load it and ignore the rest of data items.\n                Just specify the name of the data you want to load. You can also provide a list,\n                for example `load_only='spikes'`, `load_only=['spikes','membrane_potential']`.\n\n                Be aware that you need to specify the names of parts as they were stored\n                to HDF5. Depending on how your leaf construction works, this may differ\n                from the names the data might have in your leaf in the trajectory container.\n\n                A warning is issued if data specified in `load_only` cannot be found in the\n                instances specified in `iterator`.\n\n            :param load_except:\n\n                Analogous to the above, but everything is loaded except names or parts\n                specified in `load_except`.\n                You cannot use `load_only` and `load_except` at the same time. If you do\n                a ValueError is thrown.\n\n                A warning is issued if names listed in `load_except` are not part of the\n                items to load.\n\n        \"\"\"\n\n        if not self._stored:\n            raise TypeError(\n                'Cannot load stuff from disk for a trajectory that has never been stored.')\n\n        fetched_items = self._nn_interface._fetch_items(LOAD, iterator, args, kwargs)\n        if fetched_items:\n            self._storage_service.load(pypetconstants.LIST, fetched_items,\n                                       trajectory_name=self.v_name)\n        else:\n            self._logger.warning('Your loading was not successful, could not find a single item '\n                                 'to load.')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef f_remove_item(self, item, recursive=False):\n        self.f_remove_items([item], recursive=recursive)", "response": "Removes a single item from the trajectory"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef f_remove_items(self, iterator, recursive=False):\n\n        # Will format the request in a form that is understood by the storage service\n        # aka (msg, item, args, kwargs)\n        fetched_items = self._nn_interface._fetch_items(REMOVE, iterator, (), {})\n\n        if fetched_items:\n            for _, item, dummy1, dummy2 in fetched_items:\n                self._nn_interface._remove_node_or_leaf(item, recursive=recursive)\n\n        else:\n            self._logger.warning('Your removal was not successful, could not find a single '\n                                 'item to remove.')", "response": "Removes items from the trajectory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting several links from the hard disk.", "response": "def f_delete_links(self, iterator_of_links, remove_from_trajectory=False):\n        \"\"\"Deletes several links from the hard disk.\n\n        Links can be passed as a string ``'groupA.groupB.linkA'``\n        or as a tuple  containing the node from which the link should be removed and the\n        name of the link ``(groupWithLink, 'linkA')``.\n\n        \"\"\"\n\n        to_delete_links = []\n\n        group_link_pairs = []\n\n        for elem in iterator_of_links:\n            if isinstance(elem, str):\n                split_names = elem.split('.')\n                parent_name = '.'.join(split_names[:-1])\n                link = split_names[-1]\n                parent_node = self.f_get(parent_name) if parent_name != '' else self\n                link_name = parent_node.v_full_name + '.' + link if parent_name != '' else link\n                to_delete_links.append((pypetconstants.DELETE_LINK, link_name))\n                group_link_pairs.append((parent_node, link))\n            else:\n                link_name = elem[0].v_full_name + '.' + elem[1]\n                to_delete_links.append((pypetconstants.DELETE_LINK, link_name))\n                group_link_pairs.append(elem)\n        try:\n            self._storage_service.store(pypetconstants.LIST, to_delete_links,\n                                        trajectory_name=self.v_name)\n        except:\n            self._logger.error('Could not remove `%s` from the trajectory. Maybe the'\n                               ' item(s) was/were never stored to disk.' % str(to_delete_links))\n            raise\n\n        if remove_from_trajectory:\n            for group, link in group_link_pairs:\n                group.f_remove_link(link)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting a single item from the store", "response": "def f_delete_item(self, item, *args, **kwargs):\n        \"\"\"Deletes a single item, see :func:`~pypet.trajectory.Trajectory.f_delete_items`\"\"\"\n        self.f_delete_items([item], *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete items from storage on disk. Per default the item is NOT removed from the trajectory. Links are NOT deleted on the hard disk, please delete links manually before deleting data! :param iterator: A sequence of items you want to remove. Either the instances themselves or strings with the names of the items. :param remove_from_trajectory: If items should also be removed from trajectory. Default is `False`. :param args: Additional arguments passed to the storage service :param kwargs: Additional keyword arguments passed to the storage service If you use the standard hdf5 storage service, you can pass the following additional keyword argument: :param delete_only: You can partially delete leaf nodes. Specify a list of parts of the result node that should be deleted like `delete_only=['mystuff','otherstuff']`. This wil only delete the hdf5 sub parts `mystuff` and `otherstuff` from disk. BE CAREFUL, erasing data partly happens at your own risk. Depending on how complex the loading process of your result node is, you might not be able to reconstruct any data due to partially deleting some of it. Be aware that you need to specify the names of parts as they were stored to HDF5. Depending on how your leaf construction works, this may differ from the names the data might have in your leaf in the trajectory container. If the hdf5 nodes you specified in `delete_only` cannot be found a warning is issued. Note that massive deletion will fragment your HDF5 file. Try to avoid changing data on disk whenever you can. If you want to erase a full node, simply ignore this argument or set to `None`. :param remove_from_item: If data that you want to delete from storage should also be removed from the items in `iterator` if they contain these. Default is `False`. :param recursive: If you want to delete a group node and it has children you need to set `recursive` to `True. Default is `False`.", "response": "def f_delete_items(self, iterator, *args, **kwargs):\n        \"\"\"Deletes items from storage on disk.\n\n        Per default the item is NOT removed from the trajectory.\n\n        Links are NOT deleted on the hard disk, please delete links manually before deleting\n        data!\n\n        :param iterator:\n\n            A sequence of items you want to remove. Either the instances themselves\n            or strings with the names of the items.\n\n        :param remove_from_trajectory:\n\n            If items should also be removed from trajectory. Default is `False`.\n\n\n        :param args: Additional arguments passed to the storage service\n\n        :param kwargs: Additional keyword arguments passed to the storage service\n\n            If you use the standard hdf5 storage service, you can pass the following additional\n            keyword argument:\n\n            :param delete_only:\n\n                You can partially delete leaf nodes. Specify a list of parts of the result node\n                that should be deleted like `delete_only=['mystuff','otherstuff']`.\n                This wil only delete the hdf5 sub parts `mystuff` and `otherstuff` from disk.\n                BE CAREFUL,\n                erasing data partly happens at your own risk. Depending on how complex the\n                loading process of your result node is, you might not be able to reconstruct\n                any data due to partially deleting some of it.\n\n                Be aware that you need to specify the names of parts as they were stored\n                to HDF5. Depending on how your leaf construction works, this may differ\n                from the names the data might have in your leaf in the trajectory container.\n\n                If the hdf5 nodes you specified in `delete_only` cannot be found a warning\n                is issued.\n\n                Note that massive deletion will fragment your HDF5 file.\n                Try to avoid changing data on disk whenever you can.\n\n                If you want to erase a full node, simply ignore this argument or set to `None`.\n\n            :param remove_from_item:\n\n                If data that you want to delete from storage should also be removed from\n                the items in `iterator` if they contain these. Default is `False`.\n\n            :param recursive:\n\n                If you want to delete a group node and it has children you need to\n                set `recursive` to `True. Default is `False`.\n\n        \"\"\"\n\n        remove_from_trajectory = kwargs.pop('remove_from_trajectory', False)\n        recursive = kwargs.get('recursive', False)\n\n        # Will format the request in a form that is understood by the storage service\n        # aka (msg, item, args, kwargs)\n        fetched_items = self._nn_interface._fetch_items(REMOVE, iterator, args, kwargs)\n\n        if fetched_items:\n\n            try:\n                self._storage_service.store(pypetconstants.LIST, fetched_items,\n                                            trajectory_name=self.v_name)\n            except:\n                self._logger.error('Could not remove `%s` from the trajectory. Maybe the'\n                                   ' item(s) was/were never stored to disk.' % str(fetched_items))\n                raise\n\n\n            for _, item, dummy1, dummy2 in fetched_items:\n                if remove_from_trajectory:\n                    self._nn_interface._remove_node_or_leaf(item, recursive=recursive)\n                else:\n                    item._stored = False\n\n        else:\n            self._logger.warning('Your removal was not successful, could not find a single '\n                                 'item to remove.')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nstart a pool single run and passes the storage service", "response": "def _pool_single_run(kwargs):\n    \"\"\"Starts a pool single run and passes the storage service\"\"\"\n    wrap_mode = kwargs['wrap_mode']\n    traj = kwargs['traj']\n    traj.v_storage_service = _pool_single_run.storage_service\n    if wrap_mode == pypetconstants.WRAP_MODE_LOCAL:\n        # Free references from previous runs\n        traj.v_storage_service.free_references()\n    return _sigint_handling_single_run(kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsingle run wrapper for the frozen pool makes a single run and passes kwargs", "response": "def _frozen_pool_single_run(kwargs):\n    \"\"\"Single run wrapper for the frozen pool, makes a single run and passes kwargs\"\"\"\n    idx = kwargs.pop('idx')\n    frozen_kwargs = _frozen_pool_single_run.kwargs\n    frozen_kwargs.update(kwargs)  # in case of `run_map`\n    # we need to update job's args and kwargs\n    traj = frozen_kwargs['traj']\n    traj.f_set_crun(idx)\n    return _sigint_handling_single_run(frozen_kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _configure_pool(kwargs):\n    _pool_single_run.storage_service = kwargs['storage_service']\n    _configure_niceness(kwargs)\n    _configure_logging(kwargs, extract=False)", "response": "Configures the pool and keeps the storage service"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconfigure the frozen pool and keeps all kwargs", "response": "def _configure_frozen_pool(kwargs):\n    \"\"\"Configures the frozen pool and keeps all kwargs\"\"\"\n    _frozen_pool_single_run.kwargs = kwargs\n    _configure_niceness(kwargs)\n    _configure_logging(kwargs, extract=False)\n    # Reset full copy to it's old value\n    traj = kwargs['traj']\n    traj.v_full_copy = kwargs['full_copy']"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _process_single_run(kwargs):\n    _configure_niceness(kwargs)\n    _configure_logging(kwargs)\n    result_queue = kwargs['result_queue']\n    result = _sigint_handling_single_run(kwargs)\n    result_queue.put(result)\n    result_queue.close()", "response": "Wrapper function that first configures logging and starts a single run afterwards."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrap function that configures a frozen SCOOP set up.", "response": "def _configure_frozen_scoop(kwargs):\n    \"\"\"Wrapper function that configures a frozen SCOOP set up.\n\n    Deletes of data if necessary.\n\n    \"\"\"\n    def _delete_old_scoop_rev_data(old_scoop_rev):\n        if old_scoop_rev is not None:\n            try:\n                elements = shared.elements\n                for key in elements:\n                    var_dict = elements[key]\n                    if old_scoop_rev in var_dict:\n                        del var_dict[old_scoop_rev]\n                logging.getLogger('pypet.scoop').debug('Deleted old SCOOP data from '\n                                                       'revolution `%s`.' % old_scoop_rev)\n            except AttributeError:\n                logging.getLogger('pypet.scoop').error('Could not delete old SCOOP data from '\n                                                       'revolution `%s`.' % old_scoop_rev)\n    scoop_rev = kwargs.pop('scoop_rev')\n    # Check if we need to reconfigure SCOOP\n    try:\n        old_scoop_rev = _frozen_scoop_single_run.kwargs['scoop_rev']\n        configured = old_scoop_rev == scoop_rev\n    except (AttributeError, KeyError):\n        old_scoop_rev = None\n        configured = False\n    if not configured:\n        _frozen_scoop_single_run.kwargs = shared.getConst(scoop_rev, timeout=424.2)\n        frozen_kwargs = _frozen_scoop_single_run.kwargs\n        frozen_kwargs['scoop_rev'] = scoop_rev\n        frozen_kwargs['traj'].v_full_copy = frozen_kwargs['full_copy']\n        if not scoop.IS_ORIGIN:\n            _configure_niceness(frozen_kwargs)\n            _configure_logging(frozen_kwargs, extract=False)\n        _delete_old_scoop_rev_data(old_scoop_rev)\n        logging.getLogger('pypet.scoop').info('Configured Worker %s' % str(scoop.worker))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwrapping function for scoop that does not configure logging", "response": "def _scoop_single_run(kwargs):\n    \"\"\"Wrapper function for scoop, that does not configure logging\"\"\"\n    try:\n        try:\n            is_origin = scoop.IS_ORIGIN\n        except AttributeError:\n            # scoop is not properly started, i.e. with `python -m scoop...`\n            # in this case scoop uses default `map` function, i.e.\n            # the main process\n            is_origin = True\n        if not is_origin:\n            # configure logging and niceness if not the main process:\n            _configure_niceness(kwargs)\n            _configure_logging(kwargs)\n        return _single_run(kwargs)\n    except Exception:\n        scoop.logger.exception('ERROR occurred during a single run!')\n        raise"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconfigure the logging system.", "response": "def _configure_logging(kwargs, extract=True):\n    \"\"\"Requests the logging manager to configure logging.\n\n    :param extract:\n\n        If naming data should be extracted from the trajectory\n\n    \"\"\"\n    try:\n        logging_manager = kwargs['logging_manager']\n        if extract:\n            logging_manager.extract_replacements(kwargs['traj'])\n        logging_manager.make_logging_handlers_and_tools(multiproc=True)\n    except Exception as exc:\n        sys.stderr.write('Could not configure logging system because of: %s' % repr(exc))\n        traceback.print_exc()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the niceness of a process", "response": "def _configure_niceness(kwargs):\n    \"\"\"Sets niceness of a process\"\"\"\n    niceness = kwargs['niceness']\n    if niceness is not None:\n        try:\n            try:\n                current = os.nice(0)\n                if niceness - current > 0:\n                    # Under Linux you cannot decrement niceness if set elsewhere\n                    os.nice(niceness-current)\n            except AttributeError:\n                # Fall back on psutil under Windows\n                psutil.Process().nice(niceness)\n        except Exception as exc:\n            sys.stderr.write('Could not configure niceness because of: %s' % repr(exc))\n            traceback.print_exc()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _sigint_handling_single_run(kwargs):\n    try:\n        graceful_exit = kwargs['graceful_exit']\n\n        if graceful_exit:\n            sigint_handling.start()\n            if sigint_handling.hit:\n                result = (sigint_handling.SIGINT, None)\n            else:\n                result = _single_run(kwargs)\n                if sigint_handling.hit:\n                    result = (sigint_handling.SIGINT, result)\n            return result\n        return _single_run(kwargs)\n\n    except:\n        # Log traceback of exception\n        pypet_root_logger = logging.getLogger('pypet')\n        pypet_root_logger.exception('ERROR occurred during a single run! ')\n        raise", "response": "Wrapper that allows graceful exits of single runs of a single element."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _single_run(kwargs):\n    pypet_root_logger = logging.getLogger('pypet')\n    traj = kwargs['traj']\n    runfunc = kwargs['runfunc']\n    runargs = kwargs['runargs']\n    kwrunparams = kwargs['runkwargs']\n    clean_up_after_run = kwargs['clean_up_runs']\n    automatic_storing = kwargs['automatic_storing']\n    wrap_mode = kwargs['wrap_mode']\n\n    idx = traj.v_idx\n    total_runs = len(traj)\n\n    pypet_root_logger.info('\\n=========================================\\n '\n              'Starting single run #%d of %d '\n              '\\n=========================================\\n' % (idx, total_runs))\n\n    # Measure start time\n    traj.f_start_run(turn_into_run=True)\n\n    # Run the job function of the user\n    result = runfunc(traj, *runargs, **kwrunparams)\n\n    # Store data if desired\n    if automatic_storing:\n        traj.f_store()\n\n    # Add the index to the result and the run information\n    if wrap_mode == pypetconstants.WRAP_MODE_LOCAL:\n        result = ((traj.v_idx, result),\n                   traj.f_get_run_information(traj.v_idx, copy=False),\n                   traj.v_storage_service.references)\n        traj.v_storage_service.free_references()\n    else:\n        result = ((traj.v_idx, result),\n                   traj.f_get_run_information(traj.v_idx, copy=False))\n\n    # Measure time of finishing\n    traj.f_finalize_run(store_meta_data=False,\n                        clean_up=clean_up_after_run)\n\n    pypet_root_logger.info('\\n=========================================\\n '\n              'Finished single run #%d of %d '\n              '\\n=========================================\\n' % (idx, total_runs))\n\n    return result", "response": "This function performs a single run of the experiment."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstarting running a queue handler and creates a log file for the queue.", "response": "def _wrap_handling(kwargs):\n    \"\"\" Starts running a queue handler and creates a log file for the queue.\"\"\"\n    _configure_logging(kwargs, extract=False)\n    # Main job, make the listener to the queue start receiving message for writing to disk.\n    handler=kwargs['handler']\n    graceful_exit = kwargs['graceful_exit']\n    # import cProfile as profile\n    # profiler = profile.Profile()\n    # profiler.enable()\n    if graceful_exit:\n        sigint_handling.start()\n    handler.run()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_class(full_class_string):\n\n    class_data = full_class_string.split(\".\")\n    module_path = \".\".join(class_data[:-1])\n    class_str = class_data[-1]\n    module = importlib.import_module(module_path)\n\n    # We retrieve the Class from the module\n    return getattr(module, class_str)", "response": "Loads a class from a string naming the module and class name."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the length of the parameter range.", "response": "def f_get_range_length(self):\n        \"\"\"Returns the length of the parameter range.\n\n        Raises TypeError if the parameter has no range.\n\n        Does not need to be implemented if the parameter supports\n        ``__len__`` appropriately.\n\n        \"\"\"\n        if not self.f_has_range():\n            raise TypeError('Not applicable, parameter does not have a range')\n        elif hasattr(self, '__len__'):\n            return len(self)\n        else:\n            raise NotImplementedError(\"Should have implemented this.\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nstring summary of the value handled by the parameter.", "response": "def f_val_to_str(self):\n        \"\"\"String summary of the value handled by the parameter.\n\n        Note that representing the parameter as a string accesses its value,\n        but for simpler debugging, this does not lock the parameter or counts as usage!\n\n        Calls `__repr__` of the contained value.\n\n        \"\"\"\n        old_locked = self._locked\n        try:\n            return repr(self.f_get())\n        except Exception:\n            return 'No Evaluation possible (yet)!'\n        finally:\n            self._locked = old_locked"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if the parameter considers two values as equal.", "response": "def _equal_values(self, val1, val2):\n        \"\"\"Checks if the parameter considers two values as equal.\n\n        This is important for the trajectory in case of merging. In case you want to delete\n        duplicate parameter points, the trajectory needs to know when two parameters\n        are equal. Since equality is not always implemented by values handled by\n        parameters in the same way, the parameters need to judge whether their values are equal.\n\n        The straightforward example here is a numpy array.\n        Checking for equality of two numpy arrays yields\n        a third numpy array containing truth values of a piecewise comparison.\n        Accordingly, the parameter could judge two numpy arrays equal if ALL of the numpy\n        array elements are equal.\n\n        In this BaseParameter class values are considered to be equal if they obey\n        the function :func:`~pypet.utils.comparisons.nested_equal`.\n        You might consider implementing a different equality comparison in your subclass.\n\n        :raises: TypeError: If both values are not supported by the parameter.\n\n        \"\"\"\n        if self.f_supports(val1) != self.f_supports(val2):\n            return False\n\n        if not self.f_supports(val1) and not self.f_supports(val2):\n            raise TypeError('I do not support the types of both inputs (`%s` and `%s`), '\n                            'therefore I cannot judge whether '\n                            'the two are equal.' % (str(type(val1)), str(type(val2))))\n\n        if not self._values_of_same_type(val1, val2):\n            return False\n\n        return comparisons.nested_equal(val1, val2)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _values_of_same_type(self, val1, val2):\n        if self.f_supports(val1) != self.f_supports(val2):\n            return False\n\n        if not self.f_supports(val1) and not self.f_supports(val2):\n            raise TypeError('I do not support the types of both inputs (`%s` and `%s`),'\n                            ' therefore I cannot judge whether the two are of same type.' %\n                            str(type(val1)), str(type(val2)))\n\n        return type(val1) is type(val2)", "response": "Checks if two values agree in type."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef f_supports(self, data):\n        dtype = type(data)\n        if dtype is tuple or dtype is list:\n\n            # Parameters cannot handle empty tuples\n            if len(data) == 0:\n                return False\n\n            old_type = None\n\n            # Check if the data in the tuple is homogeneous\n            for item in data:\n                if not type(item) in pypetconstants.PARAMETER_SUPPORTED_DATA:\n                    return False\n                if not old_type is None and old_type != type(item):\n                    return False\n                old_type = type(item)\n            return True\n\n        elif dtype is np.ndarray or dtype is np.matrix:\n\n            if data.size == 0:\n                return False # Parameters cannot handle empty arrays and matrices\n\n            # Numpy has many string types that depend on the length of the string,\n            # We allow all of them\n            dtype = data.dtype\n            if np.issubdtype(dtype, np.str):\n                dtype = np.str\n\n        return dtype in pypetconstants.PARAMETER_SUPPORTED_DATA", "response": "Checks if input data is supported by the parameter."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _values_of_same_type(self, val1, val2):\n        if self.f_supports(val1) != self.f_supports(val2):\n            return False\n\n        if not self.f_supports(val1) and not self.f_supports(val2):\n            raise TypeError('I do not support the types of both inputs (`%s` and `%s`),'\n                            ' therefore I cannot judge whether the two are of same type.' %\n                            str(type(val1)), str(type(val2)))\n\n        if not type(val1) is type(val2):\n            return False\n\n        # Numpy arrays must agree in data type and shape\n        if type(val1) is np.array:\n            if not val1.dtype is val2.dtype:\n                return False\n\n            if not np.shape(val1) == np.shape(val2):\n                return False\n\n        # For tuples we now from earlier checks that the data is homogeneous.\n        # Thus, only the type of the first item and the length must agree.\n        if type(val1) is tuple:\n            return (type(val1[0]) is type(val2[0])) and (len(val1) == len(val2))\n\n        return True", "response": "Checks if two values agree in type."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a python iterable containing the exploration range of the current state of the user.", "response": "def f_get_range(self, copy=True):\n        \"\"\"Returns a python iterable containing the exploration range.\n\n        :param copy:\n\n            If the range should be copied before handed over to avoid tempering with data\n\n        Example usage:\n\n        >>> param = Parameter('groupA.groupB.myparam',data=22, comment='I am a neat example')\n        >>> param._explore([42,43,43])\n        >>> param.f_get_range()\n        (42,43,44)\n\n        :raises: TypeError: If parameter is not explored.\n\n        \"\"\"\n        if not self.f_has_range():\n            raise TypeError('Your parameter `%s` is not array, so cannot return array.' %\n                            self.v_full_name)\n        elif copy:\n            return self._explored_range[:]\n        else:\n            return self._explored_range"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _explore(self, explore_iterable):\n        if self.v_locked:\n            raise pex.ParameterLockedException('Parameter `%s` is locked!' % self.v_full_name)\n\n        if self.f_has_range():\n            raise TypeError('Your parameter `%s` is already explored, '\n                            'cannot _explore it further!' % self._name)\n\n        if self._data is None:\n            raise TypeError('Your parameter `%s` has no default value, please specify one '\n                            'via `f_set` before exploration. ' % self.v_full_name)\n\n        data_list = self._data_sanity_checks(explore_iterable)\n\n        self._explored_range = data_list\n        self._explored = True\n        self.f_lock()", "response": "Explores the parameter according to the given iterable."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _expand(self, explore_iterable):\n        if self.v_locked:\n            raise pex.ParameterLockedException('Parameter `%s` is locked!' % self.v_full_name)\n\n        if not self.f_has_range():\n            raise TypeError('Your Parameter `%s` is not an array and can therefore '\n                            'not be expanded.' % self._name)\n\n        data_list = self._data_sanity_checks(explore_iterable)\n\n        self._explored_range.extend(data_list)\n        self.f_lock()", "response": "Explores the parameter according to the iterable and appends to the exploration range."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _data_sanity_checks(self, explore_iterable):\n        data_list = []\n\n        for val in explore_iterable:\n\n            if not self.f_supports(val):\n                raise TypeError('%s is of not supported type %s.' % (repr(val), str(type(val))))\n\n            if not self._values_of_same_type(val, self._default):\n                raise TypeError(\n                    'Data of `%s` is not of the same type as the original entry value, '\n                    'new type is %s vs old type %s.' %\n                    (self.v_full_name, str(type(val)), str(type(self._default))))\n\n            data_list.append(val)\n\n        if len(data_list) == 0:\n            raise ValueError('Cannot explore an empty list!')\n\n        return data_list", "response": "Checks if the data values are valid."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a dictionary containing the formatted data understood by the storage service.", "response": "def _store(self):\n        \"\"\"Returns a dictionary of formatted data understood by the storage service.\n\n        The data is put into an :class:`~pypet.parameter.ObjectTable` named 'data'.\n        If the parameter is explored, the exploration range is also put into another table\n        named 'explored_data'.\n\n        :return: Dictionary containing the data and optionally the exploration range.\n\n        \"\"\"\n        if self._data is not None:\n            store_dict = {'data': ObjectTable(data={'data': [self._data]})}\n\n        if self.f_has_range():\n            store_dict['explored_data'] = ObjectTable(data={'data': self._explored_range})\n\n        self._locked = True\n\n        return store_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _load(self, load_dict):\n        if self.v_locked:\n            raise pex.ParameterLockedException('Parameter `%s` is locked!' % self.v_full_name)\n\n        if 'data' in load_dict:\n            self._data = load_dict['data']['data'][0]\n            self._default = self._data\n        else:\n            self._logger.warning('Your parameter `%s` is empty, '\n                                 'I did not find any data on disk.' % self.v_full_name)\n\n        if 'explored_data' in load_dict:\n            self._explored_range = [x for x in load_dict['explored_data']['data'].tolist()]\n            self._explored = True\n\n        self._locked = True", "response": "Loads the data and exploration range from the load_dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _store(self):\n        if type(self._data) not in (np.ndarray, tuple, np.matrix, list):\n            return super(ArrayParameter, self)._store()\n        else:\n            store_dict = {'data' + ArrayParameter.IDENTIFIER: self._data}\n\n            if self.f_has_range():\n                # Supports smart storage by hashable arrays\n                # Keys are the hashable arrays or tuples and values are the indices\n                smart_dict = {}\n\n                store_dict['explored_data' + ArrayParameter.IDENTIFIER] = \\\n                    ObjectTable(columns=['idx'], index=list(range(len(self))))\n\n                count = 0\n                for idx, elem in enumerate(self._explored_range):\n\n                    # First we need to distinguish between tuples and array and extract a\n                    # hashable part of the array\n                    if isinstance(elem, np.ndarray):\n                        # You cannot hash numpy arrays themselves, but if they are read only\n                        # you can hash array.data\n                        hash_elem = HashArray(elem)\n                    elif isinstance(elem, list):\n                        hash_elem = tuple(elem)\n                    else:\n                        hash_elem = elem\n\n                    # Check if we have used the array before,\n                    # i.e. element can be found in the dictionary\n                    if hash_elem in smart_dict:\n                        name_idx = smart_dict[hash_elem]\n                        add = False\n                    else:\n                        name_idx = count\n                        add = True\n\n                    name = self._build_name(name_idx)\n                    # Store the reference to the array\n                    store_dict['explored_data' + ArrayParameter.IDENTIFIER]['idx'][idx] = \\\n                        name_idx\n\n                    # Only if the array was not encountered before,\n                    # store the array and remember the index\n                    if add:\n                        store_dict[name] = elem\n                        smart_dict[hash_elem] = name_idx\n                        count += 1\n\n            self._locked = True\n\n            return store_dict", "response": "Stores the data in the storage service."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads the data and exploration array from the given dictionary.", "response": "def _load(self, load_dict):\n        \"\"\"Reconstructs the data and exploration array.\n\n        Checks if it can find the array identifier in the `load_dict`, i.e. '__rr__'.\n        If not calls :class:`~pypet.parameter.Parameter._load` of the parent class.\n\n        If the parameter is explored, the exploration range of arrays is reconstructed\n        as it was stored in :func:`~pypet.parameter.ArrayParameter._store`.\n\n        \"\"\"\n        if self.v_locked:\n            raise pex.ParameterLockedException('Parameter `%s` is locked!' % self.v_full_name)\n\n        try:\n            self._data = load_dict['data' + ArrayParameter.IDENTIFIER]\n\n            if 'explored_data' + ArrayParameter.IDENTIFIER in load_dict:\n                explore_table = load_dict['explored_data' + ArrayParameter.IDENTIFIER]\n\n                idx = explore_table['idx']\n\n                explore_list = []\n\n                # Recall the arrays in the order stored in the ObjectTable 'explored_data__rr__'\n                for name_idx in idx:\n                    arrayname = self._build_name(name_idx)\n                    explore_list.append(load_dict[arrayname])\n\n                self._explored_range = [x for x in explore_list]\n                self._explored = True\n\n        except KeyError:\n            super(ArrayParameter, self)._load(load_dict)\n\n        self._default = self._data\n        self._locked = True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if two values agree in type.", "response": "def _values_of_same_type(self, val1, val2):\n        \"\"\"Checks if two values agree in type.\n\n        The array parameter is less restrictive than the parameter. If both values\n        are arrays, matrices or tuples, they are considered to be of same type\n        regardless of their size and values they contain.\n\n        \"\"\"\n        if (type(val1) in (np.ndarray, tuple, np.matrix)) and (type(val2) is type(val1)):\n            return True\n        else:\n            return super(ArrayParameter, self)._values_of_same_type(val1, val2)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef f_supports(self, data):\n        dtype = type(data)\n        if dtype is tuple or dtype is list and len(data) == 0:\n            return True  #  ArrayParameter does support empty tuples\n        elif dtype is np.ndarray and data.size == 0 and data.ndim == 1:\n                return True  #  ArrayParameter supports empty numpy arrays\n        else:\n            return super(ArrayParameter, self).f_supports(data)", "response": "Checks if input data is supported by the parameter."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if two values agree in type.", "response": "def _values_of_same_type(self, val1, val2):\n        \"\"\"Checks if two values agree in type.\n\n        The sparse parameter is less restrictive than the parameter. If both values\n        are sparse matrices they are considered to be of same type\n        regardless of their size and values they contain.\n\n        \"\"\"\n        if self._is_supported_matrix(val1) and self._is_supported_matrix(val2):\n            return True\n        else:\n            return super(SparseParameter, self)._values_of_same_type(val1, val2)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _equal_values(self, val1, val2):\n        if self._is_supported_matrix(val1):\n            if self._is_supported_matrix(val2):\n\n                _, _, hash_tuple_1 = self._serialize_matrix(val1)\n                _, _, hash_tuple_2 = self._serialize_matrix(val2)\n\n                return hash(hash_tuple_1) == hash(hash_tuple_2)\n            else:\n                return False\n        else:\n            return super(SparseParameter, self)._equal_values(val1, val2)", "response": "Matrices are equal if they hash to the same value."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _is_supported_matrix(data):\n        return (spsp.isspmatrix_csc(data) or\n                spsp.isspmatrix_csr(data) or\n                spsp.isspmatrix_bsr(data) or\n                spsp.isspmatrix_dia(data))", "response": "Checks if a data is a Scipy sparse matrix"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef f_supports(self, data):\n        if self._is_supported_matrix(data):\n            return True\n        else:\n            return super(SparseParameter, self).f_supports(data)", "response": "Returns True if the array parameter supports the data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract data from a sparse matrix to make it serializable in a human readable format.", "response": "def _serialize_matrix(matrix):\n        \"\"\"Extracts data from a sparse matrix to make it serializable in a human readable format.\n\n        :return: Tuple with following elements:\n\n            1.\n\n                A list containing data that is necessary to reconstruct the matrix.\n                For csr, csc, and bsr matrices the following attributes are extracted:\n                `format`, `data`, `indices`, `indptr`, `shape`.\n                Where format is simply one of the strings 'csr', 'csc', or 'bsr'.\n\n                For dia matrices the following attributes are extracted:\n                `format`, `data`, `offsets`, `shape`.\n                Where `format` is simply the string 'dia'.\n\n            2.\n\n                A list containing the names of the extracted attributes.\n                For csr, csc, and bsr:\n\n                    [`format`, `data`, `indices`, `indptr`, `shape`]\n\n                For dia:\n\n                    [`format`, `data`, `offsets`, `shape`]\n\n            3.\n\n                A tuple containing the hashable parts of (1) in order to use the tuple as\n                a key for a dictionary. Accordingly, the numpy arrays of (1) are\n                changed to read-only.\n\n        \"\"\"\n        if (spsp.isspmatrix_csc(matrix) or\n                spsp.isspmatrix_csr(matrix) or\n                spsp.isspmatrix_bsr(matrix)):\n            if matrix.size > 0:\n                return_list = [matrix.data, matrix.indices, matrix.indptr, matrix.shape]\n            else:\n                # For empty matrices we only need the shape\n                return_list = ['__empty__', (), (), matrix.shape]\n\n            return_names = SparseParameter.OTHER_NAME_LIST\n\n            if spsp.isspmatrix_csc(matrix):\n                return_list = ['csc'] + return_list\n            elif spsp.isspmatrix_csr(matrix):\n                return_list = ['csr'] + return_list\n            elif spsp.isspmatrix_bsr(matrix):\n                return_list = ['bsr'] + return_list\n            else:\n                raise RuntimeError('You shall not pass!')\n\n        elif spsp.isspmatrix_dia(matrix):\n            if matrix.size > 0:\n                return_list = ['dia', matrix.data, matrix.offsets, matrix.shape]\n            else:\n                # For empty matrices we only need the shape\n                return_list = ['dia', '__empty__', (), matrix.shape]\n\n            return_names = SparseParameter.DIA_NAME_LIST\n        else:\n            raise RuntimeError('You shall not pass!')\n\n        hash_list = []\n\n        # Extract the `data` property of a read-only numpy array in order to have something\n        # hashable.\n        for item in return_list:\n            if type(item) is np.ndarray:\n                # item.flags.writeable = False\n                hash_list.append(HashArray(item))\n            else:\n                hash_list.append(item)\n\n        return return_list, return_names, tuple(hash_list)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _store(self):\n        if not self._is_supported_matrix(self._data):\n            return super(SparseParameter, self)._store()\n        else:\n            store_dict = {}\n            data_list, name_list, hash_tuple = self._serialize_matrix(self._data)\n            rename_list = ['data%s%s' % (SparseParameter.IDENTIFIER, name)\n                           for name in name_list]\n\n            is_dia = int(len(rename_list) == 4)\n            store_dict['data%sis_dia' % SparseParameter.IDENTIFIER] = is_dia\n\n            for idx, name in enumerate(rename_list):\n                store_dict[name] = data_list[idx]\n\n            if self.f_has_range():\n                # # Supports smart storage by hashing\n                smart_dict = {}\n\n                store_dict['explored_data' + SparseParameter.IDENTIFIER] = \\\n                    ObjectTable(columns=['idx', 'is_dia'],\n                                index=list(range(len(self))))\n\n                count = 0\n                for idx, elem in enumerate(self._explored_range):\n\n                    data_list, name_list, hash_tuple = self._serialize_matrix(elem)\n\n                    # Use the hash_tuple as a key for the smart_dict\n                    if hash_tuple in smart_dict:\n                        name_idx = smart_dict[hash_tuple]\n                        add = False\n                    else:\n                        name_idx = count\n                        add = True\n\n                    is_dia = int(len(name_list) == 4)\n                    rename_list = self._build_names(name_idx, is_dia)\n\n                    store_dict['explored_data' + SparseParameter.IDENTIFIER]['idx'][idx] = name_idx\n\n                    store_dict['explored_data' + SparseParameter.IDENTIFIER]['is_dia'][\n                        idx] = is_dia\n\n                    if add:\n\n                        for irun, name in enumerate(rename_list):\n                            store_dict[name] = data_list[irun]\n\n                        smart_dict[hash_tuple] = name_idx\n                        count += 1\n\n            self._locked = True\n\n            return store_dict", "response": "Stores the data in the sparse parameter table."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nformat a name for storage and returns a tuple of names with the following format.", "response": "def _build_names(self, name_idx, is_dia):\n        \"\"\"Formats a name for storage\n\n        :return: A tuple of names with the following format:\n\n            `xspm__spsp__XXXX__spsp__XXXXXXXX` where the first 'XXXX' refer to the property and\n            the latter 'XXXXXXX' to the sparse matrix index.\n\n        \"\"\"\n        name_list = self._get_name_list(is_dia)\n        return tuple(['explored%s.set_%05d.xspm_%s_%08d' % (SparseParameter.IDENTIFIER,\n                                                         name_idx // 200, name, name_idx)\n                                                                        for name in name_list])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreconstructing a sparse matrix extracted properties and returns it.", "response": "def _reconstruct_matrix(data_list):\n        \"\"\"Reconstructs a matrix from a list containing sparse matrix extracted properties\n\n        `data_list` needs to be formatted as the first result of\n        :func:`~pypet.parameter.SparseParameter._serialize_matrix`\n\n        \"\"\"\n        matrix_format = data_list[0]\n        data = data_list[1]\n        is_empty = isinstance(data, str) and data == '__empty__'\n\n        if matrix_format == 'csc':\n            if is_empty:\n                return spsp.csc_matrix(data_list[4])\n            else:\n                return spsp.csc_matrix(tuple(data_list[1:4]), shape=data_list[4])\n        elif matrix_format == 'csr':\n            if is_empty:\n                return spsp.csr_matrix(data_list[4])\n            else:\n                return spsp.csr_matrix(tuple(data_list[1:4]), shape=data_list[4])\n        elif matrix_format == 'bsr':\n            if is_empty:\n                # We have an empty matrix, that cannot be build as in elee case\n                return spsp.bsr_matrix(data_list[4])\n            else:\n                return spsp.bsr_matrix(tuple(data_list[1:4]), shape=data_list[4])\n        elif matrix_format == 'dia':\n            if is_empty:\n                return spsp.dia_matrix(data_list[3])\n            else:\n                return spsp.dia_matrix(tuple(data_list[1:3]), shape=data_list[3])\n        else:\n            raise RuntimeError('You shall not pass!')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload the data and exploration matrix of the array.", "response": "def _load(self, load_dict):\n        \"\"\"Reconstructs the data and exploration array\n\n        Checks if it can find the array identifier in the `load_dict`, i.e. '__spsp__'.\n        If not, calls :class:`~pypet.parameter.ArrayParameter._load` of the parent class.\n\n        If the parameter is explored, the exploration range of matrices is reconstructed\n        as it was stored in :func:`~pypet.parameter.SparseParameter._store`.\n\n        \"\"\"\n        if self.v_locked:\n            raise pex.ParameterLockedException('Parameter `%s` is locked!' % self.v_full_name)\n\n        try:\n            is_dia = load_dict['data%sis_dia' % SparseParameter.IDENTIFIER]\n\n            name_list = self._get_name_list(is_dia)\n            rename_list = ['data%s%s' % (SparseParameter.IDENTIFIER, name)\n                           for name in name_list]\n\n            data_list = [load_dict[name] for name in rename_list]\n            self._data = self._reconstruct_matrix(data_list)\n\n            if 'explored_data' + SparseParameter.IDENTIFIER in load_dict:\n                explore_table = load_dict['explored_data' + SparseParameter.IDENTIFIER]\n\n                idx_col = explore_table['idx']\n                dia_col = explore_table['is_dia']\n\n                explore_list = []\n                for irun, name_id in enumerate(idx_col):\n                    is_dia = dia_col[irun]\n\n                    # To make everything work with the old format we have the try catch block\n                    try:\n                        name_list = self._build_names(name_id, is_dia)\n                        data_list = [load_dict[name] for name in name_list]\n                    except KeyError:\n                        name_list = self._build_names_old(name_id, is_dia)\n                        data_list = [load_dict[name] for name in name_list]\n\n                    matrix = self._reconstruct_matrix(data_list)\n                    explore_list.append(matrix)\n\n                self._explored_range = explore_list\n                self._explored = True\n\n        except KeyError:\n            super(SparseParameter, self)._load(load_dict)\n\n        self._default = self._data\n        self._locked = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _store(self):\n        store_dict = {}\n\n        if self._data is not None:\n            dump = pickle.dumps(self._data, protocol=self.v_protocol)\n            store_dict['data'] = dump\n            store_dict[PickleParameter.PROTOCOL] = self.v_protocol\n\n        if self.f_has_range():\n\n            store_dict['explored_data'] = \\\n                ObjectTable(columns=['idx'], index=list(range(len(self))))\n\n            smart_dict = {}\n            count = 0\n\n            for idx, val in enumerate(self._explored_range):\n\n                obj_id = id(val)\n\n                if obj_id in smart_dict:\n                    name_id = smart_dict[obj_id]\n                    add = False\n                else:\n                    name_id = count\n                    add = True\n\n                name = self._build_name(name_id)\n                store_dict['explored_data']['idx'][idx] = name_id\n\n                if add:\n                    store_dict[name] = pickle.dumps(val, protocol=self.v_protocol)\n                    smart_dict[obj_id] = name_id\n                    count += 1\n\n        self._locked = True\n\n        return store_dict", "response": "Returns a dictionary for storage."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreconstructing objects from the pickle dumps in load_dict.", "response": "def _load(self, load_dict):\n        \"\"\"Reconstructs objects from the pickle dumps in `load_dict`.\n\n        The 'explored_data' entry in `load_dict` is used to reconstruct\n        the exploration range in the correct order.\n\n        Sets the `v_protocol` property to the protocol used to store 'data'.\n\n        \"\"\"\n        if self.v_locked:\n            raise pex.ParameterLockedException('Parameter `%s` is locked!' % self.v_full_name)\n\n        if 'data' in load_dict:\n            dump = load_dict['data']\n            self._data = pickle.loads(dump)\n        else:\n            self._logger.warning('Your parameter `%s` is empty, '\n                                 'I did not find any data on disk.' % self.v_full_name)\n\n        try:\n            self.v_protocol = load_dict[PickleParameter.PROTOCOL]\n        except KeyError:\n            # For backwards compatibility\n            self.v_protocol = PickleParameter._get_protocol(dump)\n\n        if 'explored_data' in load_dict:\n            explore_table = load_dict['explored_data']\n\n            name_col = explore_table['idx']\n\n            explore_list = []\n            for name_id in name_col:\n                arrayname = self._build_name(name_id)\n                loaded = pickle.loads(load_dict[arrayname])\n                explore_list.append(loaded)\n\n            self._explored_range = explore_list\n            self._explored = True\n\n        self._default = self._data\n        self._locked = True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntranslate integer indices into the appropriate names", "response": "def f_translate_key(self, key):\n        \"\"\"Translates integer indices into the appropriate names\"\"\"\n        if isinstance(key, int):\n            if key == 0:\n                key = self.v_name\n            else:\n                key = self.v_name + '_%d' % key\n        return key"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef f_val_to_str(self):\n\n        resstrlist = []\n        strlen = 0\n\n        for key in self._data:\n            val = self._data[key]\n            resstr = '%s=%s, ' % (key, repr(val))\n            resstrlist.append(resstr)\n\n            strlen += len(resstr)\n            if strlen > pypetconstants.HDF5_STRCOL_MAX_VALUE_LENGTH:\n                break\n\n        return_string = \"\".join(resstrlist)\n        if len(return_string) > pypetconstants.HDF5_STRCOL_MAX_VALUE_LENGTH:\n            return_string =\\\n                return_string[0:pypetconstants.HDF5_STRCOL_MAX_VALUE_LENGTH - 3] + '...'\n        else:\n            return_string = return_string[0:-2] # Delete the last `, `\n\n        return return_string", "response": "Summarizes data handled by the result as a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef f_to_dict(self, copy=True):\n        if copy:\n            return self._data.copy()\n        else:\n            return self._data", "response": "Returns all handled data as a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef f_set(self, *args, **kwargs):\n        if args and self.v_name is None:\n            raise AttributeError('Cannot set positional value because I do not have a name!')\n        for idx, arg in enumerate(args):\n            valstr = self.f_translate_key(idx)\n            self.f_set_single(valstr, arg)\n\n        for key, arg in kwargs.items():\n            self.f_set_single(key, arg)", "response": "Method to put data into the result."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning items handled by the result.", "response": "def f_get(self, *args):\n        \"\"\"Returns items handled by the result.\n\n         If only a single name is given, a single data item is returned. If several names are\n         given, a list is returned. For integer inputs the result returns `resultname_X`.\n\n         If the result contains only a single entry you can call `f_get()` without arguments.\n         If you call `f_get()` and the result contains more than one element a ValueError is\n         thrown.\n\n         If the requested item(s) cannot be found an AttributeError is thrown.\n\n        :param args: strings-names or integers\n\n        :return: Single data item or tuple of data\n\n        Example:\n\n        >>> res = Result('supergroup.subgroup.myresult', comment='I am a neat example!' \\\n        [1000,2000], {'a':'b','c':333}, hitchhiker='Arthur Dent')\n        >>> res.f_get('hitchhiker')\n        'Arthur Dent'\n        >>> res.f_get(0)\n        [1000,2000]\n        >>> res.f_get('hitchhiker', 'myresult')\n        ('Arthur Dent', [1000,2000])\n\n        \"\"\"\n\n        if len(args) == 0:\n            if len(self._data) == 1:\n                return list(self._data.values())[0]\n            elif len(self._data) > 1:\n                raise ValueError('Your result `%s` contains more than one entry: '\n                                 '`%s` Please use >>f_get<< with one of these.' %\n                                 (self.v_full_name, str(list(self._data.keys()))))\n            else:\n                raise AttributeError('Your result `%s` is empty, cannot access data.' %\n                                     self.v_full_name)\n\n        result_list = []\n        for name in args:\n            name = self.f_translate_key(name)\n            if not name in self._data:\n                if name == 'data' and len(self._data) == 1:\n                    return self._data[list(self._data.keys())[0]]\n                else:\n                    raise AttributeError('`%s` is not part of your result `%s`.' %\n                                         (name, self.v_full_name))\n\n            result_list.append(self._data[name])\n\n        if len(args) == 1:\n            return result_list[0]\n        else:\n            return result_list"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef f_set_single(self, name, item):\n\n        if self.v_stored:\n            self._logger.debug('You are changing an already stored result. If '\n                                 'you not explicitly overwrite the data on disk, this change '\n                                 'might be lost and not propagated to disk.')\n\n        if self._supports(item):\n\n            # self._check_if_empty(item, name) # No longer needed\n\n            if name in self._data:\n                self._logger.debug('Replacing `%s` in result `%s`.' % (name, self.v_full_name))\n\n            self._data[name] = item\n        else:\n            raise TypeError('Your result `%s` of type `%s` is not supported.' %\n                            (name, str(type(item))))", "response": "Sets a single data item of the result."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef f_remove(self, *args):\n        for arg in args:\n            arg = self.f_translate_key(arg)\n            if arg in self._data:\n                del self._data[arg]\n            else:\n                raise AttributeError('Your result `%s` does not contain %s.' % (self.name_, arg))", "response": "Removes the specified items from the result."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _supports(self, item):\n        if SparseParameter._is_supported_matrix(item):\n            return True\n        else:\n            return super(SparseResult, self)._supports(item)", "response": "Supports everything of parent class csr csc bsr and dia sparse matrices."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a dictionary understood by the storage service.", "response": "def _store(self):\n        \"\"\"Returns a storage dictionary understood by the storage service.\n\n        Sparse matrices are extracted similar to the :class:`~pypet.parameter.SparseParameter` and\n        marked with the identifier `__spsp__`.\n\n        \"\"\"\n        store_dict = {}\n        for key in self._data:\n            val = self._data[key]\n            if SparseParameter._is_supported_matrix(val):\n\n                data_list, name_list, hash_tuple = SparseParameter._serialize_matrix(val)\n                rename_list = ['%s%s%s' % (key, SparseParameter.IDENTIFIER, name)\n                               for name in name_list]\n\n                is_dia = int(len(rename_list) == 4)\n                store_dict[key + SparseResult.IDENTIFIER + 'is_dia'] = is_dia\n\n                for idx, name in enumerate(rename_list):\n                    store_dict[name] = data_list[idx]\n\n            else:\n                store_dict[key] = val\n\n        return store_dict"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _load(self, load_dict):\n        for key in list(load_dict.keys()):\n            # We delete keys over time:\n            if key in load_dict:\n                if SparseResult.IDENTIFIER in key:\n                    new_key = key.split(SparseResult.IDENTIFIER)[0]\n\n                    is_dia = load_dict.pop(new_key + SparseResult.IDENTIFIER + 'is_dia')\n\n                    name_list = SparseParameter._get_name_list(is_dia)\n                    rename_list = ['%s%s%s' % (new_key, SparseResult.IDENTIFIER, name)\n                                   for name in name_list]\n\n                    data_list = [load_dict.pop(name) for name in rename_list]\n                    matrix = SparseParameter._reconstruct_matrix(data_list)\n                    self._data[new_key] = matrix\n                else:\n                    self._data[key] = load_dict[key]", "response": "Loads data from load_dict Reconstruction of sparse matrices similar to the PyPSA s SparseParameter. _load method."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a single data item to the pickled result.", "response": "def f_set_single(self, name, item):\n        \"\"\"Adds a single data item to the pickle result.\n\n         Note that it is NOT checked if the item can be pickled!\n\n        \"\"\"\n\n        if self.v_stored:\n            self._logger.debug('You are changing an already stored result. If '\n                                 'you not explicitly overwrite the data on disk, this change '\n                                 'might be lost and not propagated to disk.')\n\n        if name == PickleResult.PROTOCOL:\n            raise AttributeError('You cannot name an entry `%s`' % PickleResult.PROTOCOL)\n\n        self._data[name] = item"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a dictionary containing pickle dumps", "response": "def _store(self):\n        \"\"\"Returns a dictionary containing pickle dumps\"\"\"\n        store_dict = {}\n        for key, val in self._data.items():\n            store_dict[key] = pickle.dumps(val, protocol=self.v_protocol)\n        store_dict[PickleResult.PROTOCOL] = self.v_protocol\n        return store_dict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreconstruct all items from the pickle dumps in load_dict.", "response": "def _load(self, load_dict):\n        \"\"\"Reconstructs all items from the pickle dumps in `load_dict`.\n\n        Sets the `v_protocol` property to the protocol of the first reconstructed item.\n\n        \"\"\"\n        try:\n            self.v_protocol = load_dict.pop(PickleParameter.PROTOCOL)\n        except KeyError:\n            # For backwards compatibility\n            dump = next(load_dict.values())\n            self.v_protocol = PickleParameter._get_protocol(dump)\n        for key in load_dict:\n            val = load_dict[key]\n            self._data[key] = pickle.loads(val)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef main():\n    folder = os.getcwd()\n    print('Merging all files')\n    merge_all_in_folder(folder,\n                        delete_other_files=True,  # We will only keep one trajectory\n                        dynamic_imports=FunctionParameter,\n                        backup=False)\n    print('Done')", "response": "Simply merge all trajectories in the working directory"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupload a file to the current working directory", "response": "def upload_file(filename, session):\n    \"\"\" Uploads a file \"\"\"\n    print('Uploading file %s' % filename)\n    outfilesource = os.path.join(os.getcwd(), filename)\n    outfiletarget = 'sftp://' + ADDRESS + WORKING_DIR\n    out = saga.filesystem.File(outfilesource, session=session, flags=OVERWRITE)\n    out.copy(outfiletarget)\n    print('Transfer of `%s` to `%s` successful' % (filename, outfiletarget))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndownloads a file from the server to the current working directory", "response": "def download_file(filename, session):\n    \"\"\" Downloads a file \"\"\"\n    print('Downloading file %s' % filename)\n    infilesource = os.path.join('sftp://' + ADDRESS + WORKING_DIR,\n                                 filename)\n    infiletarget = os.path.join(os.getcwd(), filename)\n    incoming = saga.filesystem.File(infilesource, session=session, flags=OVERWRITE)\n    incoming.copy(infiletarget)\n    print('Transfer of `%s` to `%s` successful' % (filename, infiletarget))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate and returns a new SAGA session", "response": "def create_session():\n    \"\"\" Creates and returns a new SAGA session \"\"\"\n    ctx = saga.Context(\"UserPass\")\n    ctx.user_id = USER\n    ctx.user_pass = PASSWORD\n\n    session = saga.Session()\n    session.add_context(ctx)\n\n    return session"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmerge all trajectories found in the working directory", "response": "def merge_trajectories(session):\n    \"\"\" Merges all trajectories found in the working directory \"\"\"\n    jd = saga.job.Description()\n\n    jd.executable      = 'python'\n    jd.arguments       = ['merge_trajs.py']\n    jd.output          = \"mysagajob_merge.stdout\"\n    jd.error           = \"mysagajob_merge.stderr\"\n    jd.working_directory = WORKING_DIR\n\n    js = saga.job.Service('ssh://' + ADDRESS, session=session)\n    myjob = js.create_job(jd)\n    print(\"\\n...starting job...\\n\")\n\n    # Now we can start our job.\n    myjob.run()\n    print(\"Job ID    : %s\" % (myjob.id))\n    print(\"Job State : %s\" % (myjob.state))\n\n    print(\"\\n...waiting for job...\\n\")\n    # wait for the job to either finish or fail\n    myjob.wait()\n\n    print(\"Job State : %s\" % (myjob.state))\n    print(\"Exitcode  : %s\" % (myjob.exit_code))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstarts all jobs and runs the_task. py in batches and returns a list of the job IDs.", "response": "def start_jobs(session):\n    \"\"\" Starts all jobs and runs `the_task.py` in batches. \"\"\"\n\n    js = saga.job.Service('ssh://' + ADDRESS, session=session)\n\n    batches = range(3)\n    jobs = []\n\n    for batch in batches:\n        print('Starting batch %d' % batch)\n\n        jd = saga.job.Description()\n\n        jd.executable      = 'python'\n        jd.arguments       = ['the_task.py --batch=' + str(batch)]\n        jd.output          = \"mysagajob.stdout\" + str(batch)\n        jd.error           = \"mysagajob.stderr\" + str(batch)\n        jd.working_directory = WORKING_DIR\n\n        myjob = js.create_job(jd)\n\n        print(\"Job ID    : %s\" % (myjob.id))\n        print(\"Job State : %s\" % (myjob.state))\n\n        print(\"\\n...starting job...\\n\")\n\n        myjob.run()\n        jobs.append(myjob)\n\n    for myjob in jobs:\n        print(\"Job ID    : %s\" % (myjob.id))\n        print(\"Job State : %s\" % (myjob.state))\n\n        print(\"\\n...waiting for job...\\n\")\n        # wait for the job to either finish or fail\n        myjob.wait()\n\n        print(\"Job State : %s\" % (myjob.state))\n        print(\"Exitcode  : %s\" % (myjob.exit_code))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsophisticating simulation of multiplication", "response": "def multiply(traj):\n    \"\"\"Sophisticated simulation of multiplication\"\"\"\n    z=traj.x*traj.y\n    traj.f_add_result('z',z=z, comment='I am the product of two reals!')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef main():\n\n    # Create an environment that handles running.\n    # Let's enable multiprocessing with scoop:\n    filename = os.path.join('hdf5', 'example_21.hdf5')\n    env = Environment(trajectory='Example_21_SCOOP',\n                      filename=filename,\n                      file_title='Example_21_SCOOP',\n                      log_stdout=True,\n                      comment='Multiprocessing example using SCOOP!',\n                      multiproc=True,\n                      freeze_input=True, # We want to save overhead and freeze input\n                      use_scoop=True, # Yes we want SCOOP!\n                      wrap_mode=pypetconstants.WRAP_MODE_LOCAL,  # SCOOP only works with 'LOCAL'\n                      # or 'NETLOCK' wrapping\n                      overwrite_file=True)\n\n    # Get the trajectory from the environment\n    traj = env.trajectory\n\n    # Add both parameters\n    traj.f_add_parameter('x', 1.0, comment='I am the first dimension!')\n    traj.f_add_parameter('y', 1.0, comment='I am the second dimension!')\n\n    # Explore the parameters with a cartesian product, but we want to explore a bit more\n    traj.f_explore(cartesian_product({'x':[float(x) for x in range(20)],\n                                      'y':[float(y) for y in range(20)]}))\n    # Run the simulation\n    env.run(multiply)\n\n    # Let's check that all runs are completed!\n    assert traj.f_is_completed()\n\n    # Finally disable logging and close all log-files\n    env.disable_logging()", "response": "This is the main function that creates a new environment and runs the simulation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run_neuron(traj):\n\n    # Extract all parameters from `traj`\n    V_init = traj.par.neuron.V_init\n    I = traj.par.neuron.I\n    tau_V = traj.par.neuron.tau_V\n    tau_ref = traj.par.neuron.tau_ref\n    dt = traj.par.simulation.dt\n    duration = traj.par.simulation.duration\n\n    steps = int(duration / float(dt))\n    # Create some containers for the Euler integration\n    V_array = np.zeros(steps)\n    V_array[0] = V_init\n    spiketimes = []  # List to collect all times of action potentials\n\n    # Do the Euler integration:\n    print('Starting Euler Integration')\n    for step in range(1, steps):\n        if V_array[step-1] >= 1:\n            # The membrane potential crossed the threshold and we mark this as\n            # an action potential\n            V_array[step] = 0\n            spiketimes.append((step-1)*dt)\n        elif spiketimes and step * dt - spiketimes[-1] <= tau_ref:\n            # We are in the refractory period, so we simply clamp the voltage\n            # to 0\n            V_array[step] = 0\n        else:\n            # Euler Integration step:\n            dV = -1/tau_V * V_array[step-1] + I\n            V_array[step] = V_array[step-1] + dV*dt\n\n    print('Finished Euler Integration')\n\n    # Add the voltage trace and spike times\n    traj.f_add_result('neuron.$', V=V_array, nspikes=len(spiketimes),\n                      comment='Contains the development of the membrane potential over time '\n                              'as well as the number of spikes.')\n    # This result will be renamed to `traj.results.neuron.run_XXXXXXXX`.\n\n    # And finally we return the estimate of the firing rate\n    return len(spiketimes) / float(traj.par.simulation.duration) *1000", "response": "Runs a simulation of a model neuron."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_parameters(traj):\n    print('Adding Parameters')\n\n    traj.f_add_parameter('neuron.V_init', 0.0,\n                         comment='The initial condition for the '\n                                    'membrane potential')\n    traj.f_add_parameter('neuron.I', 0.0,\n                         comment='The externally applied current.')\n    traj.f_add_parameter('neuron.tau_V', 10.0,\n                         comment='The membrane time constant in milliseconds')\n    traj.f_add_parameter('neuron.tau_ref', 5.0,\n                        comment='The refractory period in milliseconds '\n                                'where the membrane potnetial '\n                                'is clamped.')\n\n    traj.f_add_parameter('simulation.duration', 1000.0,\n                         comment='The duration of the experiment in '\n                                'milliseconds.')\n    traj.f_add_parameter('simulation.dt', 0.1,\n                         comment='The step size of an Euler integration step.')", "response": "Adds all parameters to the given trajectory."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_exploration(traj):\n\n    print('Adding exploration of I and tau_ref')\n\n    explore_dict = {'neuron.I': np.arange(0, 1.01, 0.01).tolist(),\n                    'neuron.tau_ref': [5.0, 7.5, 10.0]}\n\n    explore_dict = cartesian_product(explore_dict, ('neuron.tau_ref', 'neuron.I'))\n    # The second argument, the tuple, specifies the order of the cartesian product,\n    # The variable on the right most side changes fastest and defines the\n    # 'inner for-loop' of the cartesian product\n\n    traj.f_explore(explore_dict)", "response": "Add exploration of I and tau_ref to the trajectory."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexecutes a network before the actual experiment.", "response": "def execute_network_pre_run(self, traj, network, network_dict, component_list, analyser_list):\n        \"\"\"Runs a network before the actual experiment.\n\n        Called by a :class:`~pypet.brian2.network.NetworkManager`.\n        Similar to :func:`~pypet.brian2.network.NetworkRunner.run_network`.\n\n        Subruns and their durations are extracted from the trajectory. All\n        :class:`~pypet.brian2.parameter.Brian2Parameter` instances found under\n        `traj.parameters.simulation.pre_durations` (default, you can change the\n        name of the group where to search for durations at runner initialisation).\n        The order is determined from\n        the `v_annotations.order` attributes. There must be at least one subrun in the trajectory,\n        otherwise an AttributeError is thrown. If two subruns equal in their order\n        property a RuntimeError is thrown.\n\n        :param traj: Trajectory container\n\n        :param network: BRIAN2 network\n\n        :param network_dict: Dictionary of items shared among all components\n\n        :param component_list: List of :class:`~pypet.brian2.network.NetworkComponent` objects\n\n        :param analyser_list: List of :class:`~pypet.brian2.network.NetworkAnalyser` objects\n\n        \"\"\"\n        self._execute_network_run(traj, network, network_dict, component_list, analyser_list,\n                                  pre_run=True)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexecuting a network in an experimental run.", "response": "def execute_network_run(self, traj, network, network_dict, component_list, analyser_list):\n        \"\"\"Runs a network in an experimental run.\n\n        Called by a :class:`~pypet.brian2.network.NetworkManager`.\n\n        A network run is divided into several subruns which are defined as\n        :class:`~pypet.brian2.parameter.Brian2Parameter` instances.\n\n        These subruns are extracted from the trajectory. All\n        :class:`~pypet.brian2.parameter.Brian2Parameter` instances found under\n        `traj.parameters.simulation.durations` (default, you can change the\n        name of the group where to search for durations at runner initialisation).\n        The order is determined from\n        the `v_annotations.order` attributes. An error is thrown if no orders attribute\n        can be found or if two parameters have the same order.\n\n        There must be at least one subrun in the trajectory,\n        otherwise an AttributeError is thrown. If two subruns equal in their order\n        property a RuntimeError is thrown.\n\n        For every subrun the following steps are executed:\n\n        1.  Calling :func:`~pypet.brian2.network.NetworkComponent.add_to_network` for every\n            every :class:`~pypet.brian2.network.NetworkComponent` in the order as\n            they were passed to the :class:`~pypet.brian2.network.NetworkManager`.\n\n        2.  Calling :func:`~pypet.brian2.network.NetworkComponent.add_to_network` for every\n            every :class:`~pypet.brian2.network.NetworkAnalyser` in the order as\n            they were passed to the :class:`~pypet.brian2.network.NetworkManager`.\n\n        3.  Calling :func:`~pypet.brian2.network.NetworkComponent.add_to_network` of the\n            NetworkRunner itself (usually the network runner should not add or remove\n            anything from the network, but this step is executed for completeness).\n\n        4.  Running the BRIAN2 network for the duration of the current subrun by calling\n            the network's `run` function.\n\n        5.  Calling :func:`~pypet.brian2.network.NetworkAnalyser.analyse` for every\n            every :class:`~pypet.brian2.network.NetworkAnalyser` in the order as\n            they were passed to the :class:`~pypet.brian2.network.NetworkManager`.\n\n        6.  Calling :func:`~pypet.brian2.network.NetworkComponent.remove_from_network` of the\n            NetworkRunner itself (usually the network runner should not add or remove\n            anything from the network, but this step is executed for completeness).\n\n        7.  Calling :func:`~pypet.brian2.network.NetworkComponent.remove_from_network` for every\n            every :class:`~pypet.brian2.network.NetworkAnalyser` in the order as\n            they were passed to the :class:`~pypet.brian2.network.NetworkManager`\n\n        8.  Calling :func:`~pypet.brian2.network.NetworkComponent.remove_from_network` for every\n            every :class:`~pypet.brian2.network.NetworkComponent` in the order as\n            they were passed to the :class:`~pypet.brian2.network.NetworkManager`.\n\n\n        These 8 steps are repeated for every subrun in the `subrun_list`.\n        The `subrun_list` passed to all `add_to_network`, `analyse` and\n        `remove_from_network` methods can be modified\n        within these functions to potentially alter the order of execution or\n        even erase or add upcoming subruns if necessary.\n\n        For example, a NetworkAnalyser checks\n        for epileptic pathological activity and cancels all coming subruns in case\n        of undesired network dynamics.\n\n        :param traj: Trajectory container\n\n        :param network: BRIAN2 network\n\n        :param network_dict: Dictionary of items shared among all components\n\n        :param component_list: List of :class:`~pypet.brian2.network.NetworkComponent` objects\n\n        :param analyser_list: List of :class:`~pypet.brian2.network.NetworkAnalyser` objects\n\n        \"\"\"\n        self._execute_network_run(traj, network, network_dict, component_list, analyser_list,\n                                  pre_run=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _extract_subruns(self, traj, pre_run=False):\n        if pre_run:\n            durations_list = traj.f_get_all(self._pre_durations_group_name)\n        else:\n            durations_list = traj.f_get_all(self._durations_group_name)\n\n\n        subruns = {}\n        orders = []\n\n\n        for durations in durations_list:\n            for duration_param in durations.f_iter_leaves(with_links=False):\n\n                if 'order' in duration_param.v_annotations:\n                    order = duration_param.v_annotations.order\n                else:\n                    raise RuntimeError('Your duration parameter %s has no order. Please add '\n                                       'an order in `v_annotations.order`.' %\n                                       duration_param.v_full_name)\n\n                if order in subruns:\n                    raise RuntimeError('Your durations must differ in their order, there are two '\n                                       'with order %d.' % order)\n                else:\n                    subruns[order] = duration_param\n                    orders.append(order)\n\n        return [subruns[order] for order in sorted(orders)]", "response": "Extracts subruns from the trajectory."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexecutes all subruns of the current network.", "response": "def _execute_network_run(self, traj, network, network_dict, component_list,\n                             analyser_list, pre_run=False):\n        \"\"\"Generic `execute_network_run` function, handles experimental runs as well as pre-runs.\n\n        See also :func:`~pypet.brian2.network.NetworkRunner.execute_network_run` and\n         :func:`~pypet.brian2.network.NetworkRunner.execute_network_pre_run`.\n\n        \"\"\"\n\n        # Initially extract the `subrun_list`\n        subrun_list = self._extract_subruns(traj, pre_run=pre_run)\n\n        # counter for subruns\n        subrun_number = 0\n\n        # Execute all subruns in order\n        while len(subrun_list) > 0:\n\n            # Get the next subrun\n            current_subrun = subrun_list.pop(0)\n\n            # 1. Call `add` of all normal components\n            for component in component_list:\n                component.add_to_network(traj, network, current_subrun, subrun_list,\n                                         network_dict)\n\n            # 2. Call `add` of all analyser components\n            for analyser in analyser_list:\n                analyser.add_to_network(traj, network, current_subrun, subrun_list,\n                                        network_dict)\n\n            # 3. Call `add` of the network runner itself\n            self.add_to_network(traj, network, current_subrun, subrun_list,\n                                network_dict)\n\n            # 4. Run the network\n            self._logger.info('STARTING subrun `%s` (#%d) lasting %s.' %\n                             (current_subrun.v_name, subrun_number, str(current_subrun.f_get())))\n            network.run(duration=current_subrun.f_get(), report=self._report,\n                              report_period=self._report_period)\n\n            # 5. Call `analyse` of all analyser components\n            for analyser in analyser_list:\n                analyser.analyse(traj, network, current_subrun, subrun_list,\n                                 network_dict)\n\n            # 6. Call `remove` of the network runner itself\n            self.remove_from_network(traj, network, current_subrun, subrun_list,\n                                     network_dict)\n\n            # 7. Call `remove` for all analyser components\n            for analyser in analyser_list:\n                analyser.remove_from_network(traj, network, current_subrun, subrun_list,\n                                             network_dict)\n\n            # 8. Call `remove` for all normal components\n            for component in component_list:\n                component.remove_from_network(traj, network, current_subrun, subrun_list,\n                                              network_dict)\n\n\n            subrun_number += 1"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_parameters(self, traj):\n        self._logger.info('Adding Parameters of Components')\n\n        for component in self.components:\n            component.add_parameters(traj)\n\n        if self.analysers:\n            self._logger.info('Adding Parameters of Analysers')\n\n            for analyser in self.analysers:\n                analyser.add_parameters(traj)\n\n        self._logger.info('Adding Parameters of Runner')\n\n        self.network_runner.add_parameters(traj)", "response": "Adds parameters for a simulation."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pre_build(self, traj):\n        self._logger.info('Pre-Building Components')\n\n        for component in self.components:\n            component.pre_build(traj, self._brian_list, self._network_dict)\n\n        if self.analysers:\n\n            self._logger.info('Pre-Building Analysers')\n\n            for analyser in self.analysers:\n                analyser.pre_build(traj, self._brian_list, self._network_dict)\n\n        self._logger.info('Pre-Building NetworkRunner')\n        self.network_runner.pre_build(traj, self._brian_list, self._network_dict)\n\n        self._pre_built = True", "response": "Pre - builds the BRIAN2 network."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build(self, traj):\n        self._logger.info('Building Components')\n\n        for component in self.components:\n            component.build(traj, self._brian_list, self._network_dict)\n\n        if self.analysers:\n\n            self._logger.info('Building Analysers')\n\n            for analyser in self.analysers:\n                analyser.build(traj, self._brian_list, self._network_dict)\n\n        self._logger.info('Building NetworkRunner')\n        self.network_runner.build(traj, self._brian_list, self._network_dict)", "response": "Builds the network components and network runner."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pre_run_network(self, traj):\n        self.pre_build(traj)\n\n        self._logger.info('\\n------------------------\\n'\n                          'Pre-Running the Network\\n'\n                          '------------------------')\n\n\n        self._network = self._network_constructor(*self._brian_list)\n        self.network_runner.execute_network_pre_run(traj, self._network, self._network_dict,\n                                                    self.components, self.analysers)\n\n        self._logger.info('\\n-----------------------------\\n'\n                          'Network Simulation successful\\n'\n                          '-----------------------------')\n\n        self._pre_run = True\n        if hasattr(self._network, 'store'):\n            self._network.store('pre_run')", "response": "Starts a network run before the individual run."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntopping - level simulation function pass this to the environment with this NetworkManager build and run the network.", "response": "def run_network(self, traj):\n        \"\"\"Top-level simulation function, pass this to the environment\n\n        Performs an individual network run during parameter exploration.\n\n        `run_network` does not need to be called by the user. If this\n        method (not this one of the NetworkManager)\n        is passed to an :class:`~pypet.environment.Environment` with this NetworkManager,\n        `run_network` and :func:`~pypet.brian2.network.NetworkManager.build`\n        are automatically called for each individual experimental run.\n\n        This function will create a new BRIAN2 network in case one was not pre-run.\n        The execution of the network run is carried out by\n        the :class:`~pypet.brian2.network.NetworkRunner` and it's\n        :func:`~pypet.brian2.network.NetworkRunner.execute_network_run` (also take\n        a look at this function's documentation to see the structure of a network run).\n\n        :param traj: Trajectory container\n\n        \"\"\"\n\n        # Check if the network was pre-built\n        if self._pre_built:\n            if self._pre_run and hasattr(self._network, 'restore'):\n                self._network.restore('pre_run')\n                # Temprorary fix for https://github.com/brian-team/brian2/issues/681\n                self._network.store('pre_run')\n            self._run_network(traj)\n        else:\n            self._run_network(traj)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstart a single run carried out by a NetworkRunner.", "response": "def _run_network(self, traj):\n        \"\"\"Starts a single run carried out by a NetworkRunner.\n\n        Called from the public function :func:`~pypet.brian2.network.NetworkManger.run_network`.\n\n        :param traj: Trajectory container\n\n        \"\"\"\n        self.build(traj)\n\n        self._pretty_print_explored_parameters(traj)\n\n        # We need to construct a network object in case one was not pre-run\n        if not self._pre_run:\n\n            self._network = self._network_constructor(*self._brian_list)\n\n        # Start the experimental run\n        self.network_runner.execute_network_run(traj, self._network, self._network_dict,\n                                                self.components, self.analysers)\n\n        self._logger.info('\\n-----------------------------\\n'\n                          'Network Simulation successful\\n'\n                          '-----------------------------')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_filename(traj):\n    explored_parameters = traj.f_get_explored_parameters()\n    filename = ''\n    for param in explored_parameters.values():\n        short_name = param.v_name\n        val = param.f_get()\n        filename += '%s_%s__' % (short_name, str(val))\n\n    return filename[:-2] + '.png'", "response": "Function to create generic filenames based on what has been explored"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrap function for compatibility with pypet*.", "response": "def wrap_automaton(traj):\n    \"\"\" Simple wrapper function for compatibility with *pypet*.\n\n    We will call the original simulation functions with data extracted from ``traj``.\n\n    The resulting automaton patterns wil also be stored into the trajectory.\n\n    :param traj: Trajectory container for data\n\n    \"\"\"\n    # Make initial state\n    initial_state = make_initial_state(traj.initial_name, traj.ncells, traj.seed)\n    # Run simulation\n    pattern = cellular_automaton_1D(initial_state, traj.rule_number, traj.steps)\n    # Store the computed pattern\n    traj.f_add_result('pattern', pattern, comment='Development of CA over time')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef next(self):\n        while True:\n            # We need this loop because some iterators may already be empty.\n            # We keep on popping from the left until next succeeds and as long\n            # as there are iterators available\n            try:\n                return next(self._current)\n            except StopIteration:\n                try:\n                    self._current = iter(self._chain.popleft())\n                except IndexError:\n                    # If we run out of iterators we are sure that\n                    # there can be no more element\n                    raise StopIteration('Reached end of iterator chain')", "response": "Returns the next element of the chain."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef merge_all_in_folder(folder, ext='.hdf5',\n                        dynamic_imports=None,\n                        storage_service=None,\n                        force=False,\n                        ignore_data=(),\n                        move_data=False,\n                        delete_other_files=False,\n                        keep_info=True,\n                        keep_other_trajectory_info=True,\n                        merge_config=True,\n                        backup=True):\n    \"\"\"Merges all files in a given folder.\n\n    IMPORTANT: Does not check if there are more than 1 trajectory in a file. Always\n    uses the last trajectory in file and ignores the other ones.\n\n    Trajectories are merged according to the alphabetical order of the files,\n    i.e. the resulting merged trajectory is found in the first file\n    (according to lexicographic ordering).\n\n    :param folder: folder (not recursive) where to look for files\n    :param ext: only files with the given extension are used\n    :param dynamic_imports: Dynamic imports for loading\n    :param storage_service: storage service to use, leave `None` to use the default one\n    :param force: If loading should be forced.\n    :param delete_other_files: Deletes files of merged trajectories\n\n    All other parameters as in `f_merge_many` of the trajectory.\n\n    :return: The merged traj\n\n    \"\"\"\n    in_dir = os.listdir(folder)\n    all_files = []\n    # Find all files with matching extension\n    for file in in_dir:\n        full_file = os.path.join(folder, file)\n        if os.path.isfile(full_file):\n            _, extension = os.path.splitext(full_file)\n            if extension == ext:\n                all_files.append(full_file)\n    all_files = sorted(all_files)\n\n    # Open all trajectories\n    trajs = []\n    for full_file in all_files:\n        traj = load_trajectory(index=-1,\n                               storage_service=storage_service,\n                               filename=full_file,\n                               load_data=0,\n                               force=force,\n                               dynamic_imports=dynamic_imports)\n        trajs.append(traj)\n\n    # Merge all trajectories\n    first_traj = trajs.pop(0)\n    first_traj.f_merge_many(trajs,\n                ignore_data=ignore_data,\n                move_data=move_data,\n                delete_other_trajectory=False,\n                keep_info=keep_info,\n                keep_other_trajectory_info=keep_other_trajectory_info,\n                merge_config=merge_config,\n                backup=backup)\n\n    if delete_other_files:\n        # Delete all but the first file\n        for file in all_files[1:]:\n            os.remove(file)\n\n    return first_traj", "response": "Merges all files in a folder."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _handle_sigint(self, signum, frame):\n        if self.hit:\n            prompt = 'Exiting immediately!'\n            raise KeyboardInterrupt(prompt)\n        else:\n            self.hit = True\n            prompt = ('\\nYou killed the process(es) via `SIGINT` (`CTRL+C`). '\n                      'I am trying to exit '\n                      'gracefully. Using `SIGINT` (`CTRL+C`) '\n                      'again will cause an immediate exit.\\n')\n            sys.stderr.write(prompt)", "response": "Handler of SIGINT - > interrupt."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef refresh_tokens(self):\n        ''' Method to refresh API tokens from ecobee '''\n        url = 'https://api.ecobee.com/token'\n        params = {'grant_type': 'refresh_token',\n                  'refresh_token': self.refresh_token,\n                  'client_id': self.api_key}\n        request = requests.post(url, params=params)\n        if request.status_code == requests.codes.ok:\n            self.access_token = request.json()['access_token']\n            self.refresh_token = request.json()['refresh_token']\n            self.write_tokens_to_file()\n            return True\n        else:\n            self.request_pin()", "response": "Method to refresh API tokens from ecobee"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a list of thermostats from the ECobee API.", "response": "def get_thermostats(self):\n        ''' Set self.thermostats to a json list of thermostats from ecobee '''\n        url = 'https://api.ecobee.com/1/thermostat'\n        header = {'Content-Type': 'application/json;charset=UTF-8',\n                  'Authorization': 'Bearer ' + self.access_token}\n        params = {'json': ('{\"selection\":{\"selectionType\":\"registered\",'\n                            '\"includeRuntime\":\"true\",'\n                            '\"includeSensors\":\"true\",'\n                            '\"includeProgram\":\"true\",'\n                            '\"includeEquipmentStatus\":\"true\",'\n                            '\"includeEvents\":\"true\",'\n                            '\"includeWeather\":\"true\",'\n                            '\"includeSettings\":\"true\"}}')}\n        try:\n            request = requests.get(url, headers=header, params=params)\n        except RequestException:\n            logger.warn(\"Error connecting to Ecobee.  Possible connectivity outage.\")\n            return None\n        if request.status_code == requests.codes.ok:\n            self.authenticated = True\n            self.thermostats = request.json()['thermostatList']\n            return self.thermostats\n        else:\n            self.authenticated = False\n            logger.info(\"Error connecting to Ecobee while attempting to get \"\n                  \"thermostat data.  Refreshing tokens and trying again.\")\n            if self.refresh_tokens():\n                return self.get_thermostats()\n            else:\n                return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write_tokens_to_file(self):\n        ''' Write api tokens to a file '''\n        config = dict()\n        config['API_KEY'] = self.api_key\n        config['ACCESS_TOKEN'] = self.access_token\n        config['REFRESH_TOKEN'] = self.refresh_token\n        config['AUTHORIZATION_CODE'] = self.authorization_code\n        if self.file_based_config:\n            config_from_file(self.config_filename, config)\n        else:\n            self.config = config", "response": "Write api tokens to a file"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_hvac_mode(self, index, hvac_mode):\n        ''' possible hvac modes are auto, auxHeatOnly, cool, heat, off '''\n        body = {\"selection\": {\"selectionType\": \"thermostats\",\n                              \"selectionMatch\": self.thermostats[index]['identifier']},\n                              \"thermostat\": {\n                                  \"settings\": {\n                                      \"hvacMode\": hvac_mode\n                                  }\n                              }}\n        log_msg_action = \"set HVAC mode\"\n        return self.make_request(body, log_msg_action)", "response": "possible hvac modes are auto, auxHeatOnly, cool, heat, off"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset fan minimum on time.", "response": "def set_fan_min_on_time(self, index, fan_min_on_time):\n        ''' The minimum time, in minutes, to run the fan each hour. Value from 1 to 60 '''\n        body = {\"selection\": {\"selectionType\": \"thermostats\",\n                        \"selectionMatch\": self.thermostats[index]['identifier']},\n                        \"thermostat\": {\n                            \"settings\": {\n                                \"fanMinOnTime\": fan_min_on_time\n                            }\n                        }}\n        log_msg_action = \"set fan minimum on time.\"\n        return self.make_request(body, log_msg_action)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_fan_mode(self, index, fan_mode, cool_temp, heat_temp, hold_type=\"nextTransition\"):\n        ''' Set fan mode. Values: auto, minontime, on '''\n        body = {\"selection\": {\n                    \"selectionType\": \"thermostats\",\n                    \"selectionMatch\": self.thermostats[index]['identifier']},\n                \"functions\": [{\"type\": \"setHold\", \"params\": {\n                    \"holdType\": hold_type,\n                    \"coolHoldTemp\": int(cool_temp * 10),\n                    \"heatHoldTemp\": int(heat_temp * 10),\n                    \"fan\": fan_mode\n                }}]}\n        log_msg_action = \"set fan mode\"\n        return self.make_request(body, log_msg_action)", "response": "Set fan mode. Values auto minontime on"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_hold_temp(self, index, cool_temp, heat_temp,\n                      hold_type=\"nextTransition\"):\n        ''' Set a hold '''\n        body = {\"selection\": {\n                    \"selectionType\": \"thermostats\",\n                    \"selectionMatch\": self.thermostats[index]['identifier']},\n                \"functions\": [{\"type\": \"setHold\", \"params\": {\n                    \"holdType\": hold_type,\n                    \"coolHoldTemp\": int(cool_temp * 10),\n                    \"heatHoldTemp\": int(heat_temp * 10)\n                }}]}\n        log_msg_action = \"set hold temp\"\n        return self.make_request(body, log_msg_action)", "response": "Set a hold temp"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset a climate hold - ie away home sleep", "response": "def set_climate_hold(self, index, climate, hold_type=\"nextTransition\"):\n        ''' Set a climate hold - ie away, home, sleep '''\n        body = {\"selection\": {\n                    \"selectionType\": \"thermostats\",\n                    \"selectionMatch\": self.thermostats[index]['identifier']},\n                \"functions\": [{\"type\": \"setHold\", \"params\": {\n                    \"holdType\": hold_type,\n                    \"holdClimateRef\": climate\n                }}]}\n        log_msg_action = \"set climate hold\"\n        return self.make_request(body, log_msg_action)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete_vacation(self, index, vacation):\n        ''' Delete the vacation with name vacation '''\n        body = {\"selection\": {\n                    \"selectionType\": \"thermostats\",\n                    \"selectionMatch\": self.thermostats[index]['identifier']},\n                \"functions\": [{\"type\": \"deleteVacation\", \"params\": {\n                    \"name\": vacation\n                }}]}\n\n        log_msg_action = \"delete a vacation\"\n        return self.make_request(body, log_msg_action)", "response": "Delete the vacation with name vacation"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef resume_program(self, index, resume_all=False):\n        ''' Resume currently scheduled program '''\n        body = {\"selection\": {\n                    \"selectionType\": \"thermostats\",\n                    \"selectionMatch\": self.thermostats[index]['identifier']},\n                \"functions\": [{\"type\": \"resumeProgram\", \"params\": {\n                    \"resumeAll\": resume_all\n                }}]}\n\n        log_msg_action = \"resume program\"\n        return self.make_request(body, log_msg_action)", "response": "Resume currently scheduled program"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending a message to the thermostat", "response": "def send_message(self, index, message=\"Hello from python-ecobee!\"):\n        ''' Send a message to the thermostat '''\n        body = {\"selection\": {\n                    \"selectionType\": \"thermostats\",\n                    \"selectionMatch\": self.thermostats[index]['identifier']},\n                \"functions\": [{\"type\": \"sendMessage\", \"params\": {\n                    \"text\": message[0:500]\n                }}]}\n\n        log_msg_action = \"send message\"\n        return self.make_request(body, log_msg_action)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the humidity level of the thermostat at the specified index.", "response": "def set_humidity(self, index, humidity):\n        ''' Set humidity level'''\n        body = {\"selection\": {\"selectionType\": \"thermostats\",\n                              \"selectionMatch\": self.thermostats[index]['identifier']},\n                              \"thermostat\": {\n                                  \"settings\": {\n                                      \"humidity\": int(humidity)\n                                  }\n                              }}\n\n        log_msg_action = \"set humidity level\"\n        return self.make_request(body, log_msg_action)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nenabling or disable Alexa mic mode for the specified resource.", "response": "def set_mic_mode(self, index, mic_enabled):\n        '''Enable/disable Alexa mic (only for Ecobee 4)\n        Values: True, False\n        '''\n        \n        body = {\n            'selection': {\n                'selectionType': 'thermostats',\n                'selectionMatch': self.thermostats[index]['identifier']},\n            'thermostat': {\n                'audio': {\n                    'microphoneEnabled': mic_enabled}}}\n\n        log_msg_action = 'set mic mode'\n        return self.make_request(body, log_msg_action)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nenables or disable Smart Home Away and Follow Me modes Values : True False", "response": "def set_occupancy_modes(self, index, auto_away=None, follow_me=None):\n        '''Enable/disable Smart Home/Away and Follow Me modes\n        Values: True, False\n        '''\n        \n        body = {\n            'selection': {\n                'selectionType': 'thermostats',\n                'selectionMatch': self.thermostats[index]['identifier']},\n            'thermostat': {\n                'settings': {\n                    'autoAway': auto_away,\n                    'followMeComfort': follow_me}}}\n\n        log_msg_action = 'set occupancy modes'\n        return self.make_request(body, log_msg_action)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nenabling or disable daylight savings Values : True False", "response": "def set_dst_mode(self, index, dst):\n        '''Enable/disable daylight savings\n        Values: True, False\n        '''\n        \n        body = {\n            'selection': {\n                'selectionType': 'thermostats',\n                'selectionMatch': self.thermostats[index]['identifier']},\n            'thermostat': {\n                'location': {\n                    'isDaylightSaving': dst}}}\n\n        log_msg_action = 'set dst mode'\n        return self.make_request(body, log_msg_action)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef future_dt_str(dt, td):\n    if isinstance(td, str):\n        td = float(td)\n    td = timedelta(seconds=td)\n    future_dt = dt + td\n    return future_dt.strftime(DT_PRINT_FORMAT)", "response": "Return a string representation of a datetime."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate the delay in seconds in which the DISCOVER will be sent.", "response": "def gen_delay_selecting():\n    \"\"\"Generate the delay in seconds in which the DISCOVER will be sent.\n\n    [:rfc:`2131#section-4.4.1`]::\n\n        The client SHOULD wait a random time between one and ten seconds to\n        desynchronize the use of DHCP at startup.\n\n    \"\"\"\n    delay = float(random.randint(0, MAX_DELAY_SELECTING))\n    logger.debug('Delay to enter in SELECTING %s.', delay)\n    logger.debug('SELECTING will happen on %s',\n                 future_dt_str(nowutc(), delay))\n    return delay"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef gen_timeout_resend(attempts):\n    timeout = 2 ** (attempts + 1) + random.uniform(-1, +1)\n    logger.debug('next timeout resending will happen on %s',\n                 future_dt_str(nowutc(), timeout))\n    return timeout", "response": "Generate the time in seconds in which DHCPDISCOVER wil be retransmitted."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate time in seconds to retransmit a request in RENEWING or REBINDING states.", "response": "def gen_timeout_request_renew(lease):\n    \"\"\"Generate time in seconds to retransmit DHCPREQUEST.\n\n    [:rfc:`2131#section-4..4.5`]::\n\n        In both RENEWING and REBINDING states,\n        if the client receives no response to its DHCPREQUEST\n        message, the client SHOULD wait one-half of the remaining\n        time until T2 (in RENEWING state) and one-half of the\n        remaining lease time (in REBINDING state), down to a\n        minimum of 60 seconds, before retransmitting the\n        DHCPREQUEST message.\n\n    \"\"\"\n    time_left = (lease.rebinding_time - lease.renewing_time) * RENEW_PERC\n    if time_left < 60:\n        time_left = 60\n    logger.debug('Next request in renew will happen on %s',\n                 future_dt_str(nowutc(), time_left))\n    return time_left"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef gen_timeout_request_rebind(lease):\n    time_left = (lease.lease_time - lease.rebinding_time) * RENEW_PERC\n    if time_left < 60:\n        time_left = 60\n    logger.debug('Next request on rebinding will happen on %s',\n                 future_dt_str(nowutc(), time_left))\n    return time_left", "response": "Generate a new timeout request for rebinding."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates RENEWING time. [:rfc:`2131#section-4.4.5`]:: T1 defaults to (0.5 * duration_of_lease). T2 defaults to (0.875 * duration_of_lease). Times T1 and T2 SHOULD be chosen with some random \"fuzz\" around a fixed value, to avoid synchronization of client reacquisition.", "response": "def gen_renewing_time(lease_time, elapsed=0):\n    \"\"\"Generate RENEWING time.\n\n    [:rfc:`2131#section-4.4.5`]::\n\n        T1\n        defaults to (0.5 * duration_of_lease).  T2 defaults to (0.875 *\n        duration_of_lease).  Times T1 and T2 SHOULD be chosen with some\n        random \"fuzz\" around a fixed value, to avoid synchronization of\n        client reacquisition.\n\n    \"\"\"\n    renewing_time = int(lease_time) * RENEW_PERC - elapsed\n    # FIXME:80 [:rfc:`2131#section-4.4.5`]: the chosen \"fuzz\" could fingerprint\n    # the implementation\n    # NOTE: here using same \"fuzz\" as systemd?\n    range_fuzz = int(lease_time) * REBIND_PERC - renewing_time\n    logger.debug('rebinding fuzz range %s', range_fuzz)\n    fuzz = random.uniform(-(range_fuzz),\n                          +(range_fuzz))\n    renewing_time += fuzz\n    logger.debug('Renewing time %s.', renewing_time)\n    return renewing_time"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a rebinding time for a given lease time.", "response": "def gen_rebinding_time(lease_time, elapsed=0):\n    \"\"\".\"\"\"\n    rebinding_time = int(lease_time) * REBIND_PERC - elapsed\n    # FIXME:90 [:rfc:`2131#section-4.4.5`]: the chosen \"fuzz\" could fingerprint\n    # the implementation\n    # NOTE: here using same \"fuzz\" as systemd?\n    range_fuzz = int(lease_time) - rebinding_time\n    logger.debug('rebinding fuzz range %s', range_fuzz)\n    fuzz = random.uniform(-(range_fuzz),\n                          +(range_fuzz))\n    rebinding_time += fuzz\n    logger.debug('Rebinding time %s.', rebinding_time)\n    return rebinding_time"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dict_self(self):\n        return {k: v for k, v in self.__dict__.items() if k in FSM_ATTRS}", "response": "Return the self object attributes not inherited as dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nresets the object attributes.", "response": "def reset(self, iface=None, client_mac=None, xid=None, scriptfile=None):\n        \"\"\"Reset object attributes when state is INIT.\"\"\"\n        logger.debug('Reseting attributes.')\n        if iface is None:\n            iface = conf.iface\n        if client_mac is None:\n            # scapy for python 3 returns byte, not tuple\n            tempmac = get_if_raw_hwaddr(iface)\n            if isinstance(tempmac, tuple) and len(tempmac) == 2:\n                mac = tempmac[1]\n            else:\n                mac = tempmac\n            client_mac = str2mac(mac)\n        self.client = DHCPCAP(iface=iface, client_mac=client_mac, xid=xid)\n        if scriptfile is not None:\n            self.script = ClientScript(scriptfile)\n        else:\n            self.script = None\n        self.time_sent_request = None\n        self.discover_attempts = 0\n        self.request_attempts = 0\n        self.current_state = STATE_PREINIT\n        self.offers = list()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_timeout(self, state, function, newtimeout):\n        state = STATES2NAMES[state]\n        for timeout_fn_t in self.timeout[state]:\n            # access the function name\n            if timeout_fn_t[1] is not None and \\\n               timeout_fn_t[1].atmt_condname == function.atmt_condname:\n                # convert list to tuple to make it mutable\n                timeout_l = list(timeout_fn_t)\n                # modify the timeout\n                timeout_l[0] = newtimeout\n                # set the new timeoute to self.timeout\n                i = self.timeout[state].index(timeout_fn_t)\n                self.timeout[state][i] = tuple(timeout_l)\n                logger.debug('Set state %s, function %s, to timeout %s',\n                             state, function.atmt_condname, newtimeout)", "response": "Set the timeout of a specific state."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef send_discover(self):\n        assert self.client\n        assert self.current_state == STATE_INIT or \\\n            self.current_state == STATE_SELECTING\n        pkt = self.client.gen_discover()\n        sendp(pkt)\n        # FIXME:20 check that this is correct,: all or only discover?\n        if self.discover_attempts < MAX_ATTEMPTS_DISCOVER:\n            self.discover_attempts += 1\n        timeout = gen_timeout_resend(self.discover_attempts)\n        self.set_timeout(self.current_state,\n                         self.timeout_selecting,\n                         timeout)", "response": "Send a discover packet to the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef select_offer(self):\n        logger.debug('Selecting offer.')\n        pkt = self.offers[0]\n        self.client.handle_offer(pkt)", "response": "Select an offer from the offers received."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send_request(self):\n        assert self.client\n        if self.current_state == STATE_BOUND:\n            pkt = self.client.gen_request_unicast()\n        else:\n            pkt = self.client.gen_request()\n        sendp(pkt)\n        logger.debug('Modifying FSM obj, setting time_sent_request.')\n        self.time_sent_request = nowutc()\n        logger.info('DHCPREQUEST of %s on %s to %s port %s',\n                    self.client.iface, self.client.client_ip,\n                    self.client.server_ip, self.client.server_port)\n\n        # NOTE: see previous TODO, maybe the MAX_ATTEMPTS_REQUEST needs to be\n        # calculated per state.\n        if self.request_attempts < MAX_ATTEMPTS_REQUEST:\n            self.request_attempts *= 2\n            logger.debug('Increased request attempts to %s',\n                         self.request_attempts)\n        if self.current_state == STATE_RENEWING:\n            timeout_renewing = gen_timeout_request_renew(self.client.lease)\n            self.set_timeout(self.current_state,\n                             self.timeout_request_renewing,\n                             timeout_renewing)\n        elif self.current_state == STATE_REBINDING:\n            timeout_rebinding = gen_timeout_request_rebind(self.client.lease)\n            self.set_timeout(self.current_state,\n                             self.timeout_request_rebinding,\n                             timeout_rebinding)\n        else:\n            timeout_requesting = \\\n                gen_timeout_resend(self.request_attempts)\n            self.set_timeout(self.current_state,\n                             self.timeout_requesting,\n                             timeout_requesting)", "response": "Send a request to the server."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset renewal and rebinding times.", "response": "def set_timers(self):\n        \"\"\"Set renewal, rebinding times.\"\"\"\n        logger.debug('setting timeouts')\n        self.set_timeout(self.current_state,\n                         self.renewing_time_expires,\n                         self.client.lease.renewal_time)\n        self.set_timeout(self.current_state,\n                         self.rebinding_time_expires,\n                         self.client.lease.rebinding_time)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef process_received_ack(self, pkt):\n        if isack(pkt):\n            try:\n                self.event = self.client.handle_ack(pkt,\n                                                    self.time_sent_request)\n            except AddrFormatError as err:\n                logger.error(err)\n                # NOTE: see previous TODO, maybe should go back to other state.\n                raise self.SELECTING()\n            # NOTE: see previous TODO, not checking address with ARP.\n            logger.info('DHCPACK of %s from %s' %\n                        (self.client.client_ip, self.client.server_ip))\n            return True\n        return False", "response": "Process a received ACK packet."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprocesses a received NAK packet.", "response": "def process_received_nak(self, pkt):\n        \"\"\"Process a received NAK packet.\"\"\"\n        if isnak(pkt):\n            logger.info('DHCPNAK of %s from %s',\n                        self.client.client_ip, self.client.server_ip)\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninitialize the state of the object.", "response": "def INIT(self):\n        \"\"\"INIT state.\n\n        [:rfc:`2131#section-4.4.1`]::\n\n            The client SHOULD wait a random time between one and ten\n            seconds to desynchronize the use of DHCP at startup\n\n        .. todo::\n           - The initial delay is implemented, but probably is not in other\n             implementations. Check what other implementations do.\n        \"\"\"\n        # NOTE: in case INIT is reached from other state, initialize attributes\n        # reset all variables.\n        logger.debug('In state: INIT')\n        if self.current_state is not STATE_PREINIT:\n            self.reset()\n        self.current_state = STATE_INIT\n        # NOTE: see previous TODO, maybe this is not needed.\n        if self.delay_selecting:\n            if self.delay_before_selecting is None:\n                delay_before_selecting = gen_delay_selecting()\n            else:\n                delay_before_selecting = self.delay_before_selecting\n        else:\n            delay_before_selecting = 0\n        self.set_timeout(self.current_state,\n                         self.timeout_delay_before_selecting,\n                         delay_before_selecting)\n        if self.timeout_select is not None:\n            self.set_timeout(STATE_SELECTING,\n                             self.timeout_selecting,\n                             self.timeout_select)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef BOUND(self):\n        logger.debug('In state: BOUND')\n        logger.info('(%s) state changed %s -> bound', self.client.iface,\n                    STATES2NAMES[self.current_state])\n        self.current_state = STATE_BOUND\n        self.client.lease.info_lease()\n        if self.script is not None:\n            self.script.script_init(self.client.lease, self.current_state)\n            self.script.script_go()\n        else:\n            try:\n                set_net(self.client.lease)\n            except Exception as e:\n                logger.error('Can not set IP', exc_info=True)", "response": "Set the current state to BOUND."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef RENEWING(self):\n        logger.debug('In state: RENEWING')\n        self.current_state = STATE_RENEWING\n        if self.script is not None:\n            self.script.script_init(self.client.lease, self.current_state)\n            self.script.script_go()\n        else:\n            set_net(self.client.lease)", "response": "state is RENEWING state."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstating is ERROR state.", "response": "def ERROR(self):\n        \"\"\"ERROR state.\"\"\"\n        logger.debug('In state: ERROR')\n        self.current_state = STATE_ERROR\n        if self.script is not None:\n            self.script.script_init(self.client.lease, self.current_state)\n            self.script.script_go()\n        set_net(self.client.lease)\n        raise self.INIT()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef timeout_selecting(self):\n        logger.debug('C2.1: T In %s, timeout receiving response to select.',\n                     self.current_state)\n\n        if len(self.offers) >= MAX_OFFERS_COLLECTED:\n            logger.debug('C2.2: T Maximum number of offers reached, '\n                         'raise REQUESTING.')\n            raise self.REQUESTING()\n\n        if self.discover_attempts >= MAX_ATTEMPTS_DISCOVER:\n            logger.debug('C2.3: T Maximum number of discover retries is %s'\n                         ' and already sent %s.',\n                         MAX_ATTEMPTS_DISCOVER, self.discover_attempts)\n            if len(self.offers) <= 0:\n                logger.debug('C2.4: T. But no OFFERS where received, '\n                             'raise ERROR.')\n                raise self.ERROR()\n            logger.debug('C2.4: F. But there is some OFFERS, '\n                         'raise REQUESTING.')\n            raise self.REQUESTING()\n\n        logger.debug('C2.2: F. Still not received all OFFERS, but not '\n                     'max # attemps reached, raise SELECTING.')\n        raise self.SELECTING()", "response": "Timeout of selecting on SELECTING state."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef timeout_request_renewing(self):\n        logger.debug(\"C5.2:T In %s, timeout receiving response to request.\",\n                     self.current_state)\n        if self.request_attempts >= MAX_ATTEMPTS_REQUEST:\n            logger.debug('C2.3: T Maximum number %s of REQUESTs '\n                         'reached, already sent %s, wait to rebinding time.',\n                         MAX_ATTEMPTS_REQUEST, self.disover_requests)\n            # raise self.ERROR()\n        logger.debug(\"C2.3: F. Maximum number of REQUESTs retries not reached,\"\n                     \"raise RENEWING.\")\n        raise self.RENEWING()", "response": "Timeout request renewing on RENEWING state."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef timeout_request_rebinding(self):\n        logger.debug(\"C6.2:T In %s, timeout receiving response to request.\",\n                     self.current_state)\n        if self.request_attempts >= MAX_ATTEMPTS_REQUEST:\n            logger.debug('C.2.3: T. Maximum number %s of REQUESTs '\n                         'reached, already sent %s, wait lease time expires.',\n                         MAX_ATTEMPTS_REQUEST, self.disover_requests)\n            # raise self.ERROR()\n        logger.debug(\"C2.3: F. Maximum number of REQUESTs retries not reached,\"\n                     \"raise REBINDING.\")\n        raise self.REBINDING()", "response": "Timeout request rebinding on REBINDING state."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef receive_offer(self, pkt):\n        logger.debug(\"C2. Received OFFER?, in SELECTING state.\")\n        if isoffer(pkt):\n            logger.debug(\"C2: T, OFFER received\")\n            self.offers.append(pkt)\n            if len(self.offers) >= MAX_OFFERS_COLLECTED:\n                logger.debug(\"C2.5: T, raise REQUESTING.\")\n                self.select_offer()\n                raise self.REQUESTING()\n            logger.debug(\"C2.5: F, raise SELECTING.\")\n            raise self.SELECTING()", "response": "Receive an offer from the server."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprocesses an ACK packet and raise an exception if it is not in REQUESTING state.", "response": "def receive_ack_requesting(self, pkt):\n        \"\"\"Receive ACK in REQUESTING state.\"\"\"\n        logger.debug(\"C3. Received ACK?, in REQUESTING state.\")\n        if self.process_received_ack(pkt):\n            logger.debug(\"C3: T. Received ACK, in REQUESTING state, \"\n                         \"raise BOUND.\")\n            raise self.BOUND()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreceives NAK from the server.", "response": "def receive_nak_requesting(self, pkt):\n        \"\"\"Receive NAK in REQUESTING state.\"\"\"\n        logger.debug(\"C3.1. Received NAK?, in REQUESTING state.\")\n        if self.process_received_nak(pkt):\n            logger.debug(\"C3.1: T. Received NAK, in REQUESTING state, \"\n                         \"raise INIT.\")\n            raise self.INIT()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef receive_ack_renewing(self, pkt):\n        logger.debug(\"C3. Received ACK?, in RENEWING state.\")\n        if self.process_received_ack(pkt):\n            logger.debug(\"C3: T. Received ACK, in RENEWING state, \"\n                         \"raise BOUND.\")\n            raise self.BOUND()", "response": "Receive ACK in RENEWING state."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef receive_nak_renewing(self, pkt):\n        logger.debug(\"C3.1. Received NAK?, in RENEWING state.\")\n        if self.process_received_nak(pkt):\n            logger.debug(\"C3.1: T. Received NAK, in RENEWING state, \"\n                         \" raise INIT.\")\n            raise self.INIT()", "response": "Process a NAK packet and check if it is in RENEWING state."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprocess an ACK packet and raise an exception if it is not in REBINDING state.", "response": "def receive_ack_rebinding(self, pkt):\n        \"\"\"Receive ACK in REBINDING state.\"\"\"\n        logger.debug(\"C3. Received ACK?, in REBINDING state.\")\n        if self.process_received_ack(pkt):\n            logger.debug(\"C3: T. Received ACK, in REBINDING state, \"\n                         \"raise BOUND.\")\n            raise self.BOUND()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreceive NAK in REBINDING state.", "response": "def receive_nak_rebinding(self, pkt):\n        \"\"\"Receive NAK in REBINDING state.\"\"\"\n        logger.debug(\"C3.1. Received NAK?, in RENEWING state.\")\n        if self.process_received_nak(pkt):\n            logger.debug(\"C3.1: T. Received NAK, in RENEWING state, \"\n                         \"raise INIT.\")\n            raise self.INIT()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\naction on renewing on RENEWING state.", "response": "def on_renewing(self):\n        \"\"\"Action on renewing on RENEWING state.\n\n        Not recording lease, but restarting timers.\n\n        \"\"\"\n        self.client.lease.sanitize_net_values()\n        self.client.lease.set_times(self.time_sent_request)\n        self.set_timers()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef isoffer(packet):\n    if DHCP in packet and (DHCPTypes.get(packet[DHCP].options[0][1]) ==\n                           'offer' or packet[DHCP].options[0][1] == \"offer\"):\n        logger.debug('Packet is Offer.')\n        return True\n    return False", "response": "Returns True if packet is Offer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nassign a value to a key in the cache.", "response": "def set(self, name, value):\n        \"\"\" Assign a value, remove if it's None \"\"\"\n        clone = self._clone()\n        if django.VERSION[0] <= 1 and django.VERSION[1] <= 4:\n            value = value or None\n        clone._qsl = [(q, v) for (q, v) in self._qsl if q != name]\n        if value is not None:\n            clone._qsl.append((name, value))\n        return clone"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add(self, name, value):\n        clone = self._clone()\n        clone._qsl = [p for p in self._qsl\n                      if not(p[0] == name and p[1] == value)]\n        clone._qsl.append((name, value,))\n        return clone", "response": "Append a value to multiple value parameter."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remove(self, name, value):\n        clone = self._clone()\n        clone._qsl = [qb for qb in self._qsl if qb != (name, str(value))]\n        return clone", "response": "Remove a value from multiple value parameter."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef inc(self, name, value=1):\n        clone = self._clone()\n        clone._qsl = [(q, v) if q != name else (q, int(v) + value)\n                      for (q, v) in self._qsl]\n        if name not in dict(clone._qsl).keys():\n            clone._qsl.append((name, value))\n        return clone", "response": "Increment value for a key."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing command line options.", "response": "def parse_options():\n    \"\"\"\n    Commandline options arguments parsing.\n\n    :return: parsed commandline arguments.\n    :rtype: optparse.Values.\n    \"\"\"\n\n    version = \"%%prog {version}\".format(version=__version__)\n    parser = OptionParser(version=version)\n    parser.add_option(\n        \"-s\", \"--server\", action=\"store\", dest=\"server\",\n        type=\"string\", default=\"\", metavar=\"SERVER\",\n        help=\"server name, IP address or unix socket path\"\n    )\n    parser.add_option(\n        \"-p\", \"--port\", action=\"store\", type=\"int\", dest=\"port\",\n        default=9001, metavar=\"PORT\", help=\"port number\"\n    )\n    parser.add_option(\n        \"-P\", \"--programs\", action=\"store\", dest=\"programs\", type=\"string\", default=\"\",\n        metavar=\"PROGRAMS\", help=\"comma separated programs list, or empty for all programs in supervisord response\"\n    )\n    parser.add_option(\n        \"-u\", \"--username\", action=\"store\", dest=\"username\", type=\"string\", default=\"\",\n        metavar=\"USERNAME\", help=\"supervisord user\"\n    )\n    parser.add_option(\n        \"-S\", \"--password\", action=\"store\", dest=\"password\", type=\"string\", default=\"\",\n        metavar=\"PASSWORD\", help=\"supervisord user password\"\n    )\n    parser.add_option(\n        \"-q\", \"--quiet\", metavar=\"QUIET\", action=\"store_true\", default=False, dest=\"quiet\", help=\"be quiet\"\n    )\n    parser.add_option(\n        \"--stopped-state\", action=\"store\", dest=\"stopped_state\", type=\"choice\", choices=EXIT_CODES.keys(), default=EXIT_CODE_OK,\n        metavar=\"STOPPED_STATE\", help=\"stopped state\"\n    )\n    parser.add_option(\n        \"--network-errors-exit-code\", action=\"store\", dest=\"network_errors_exit_code\", type=\"choice\", choices=EXIT_CODES.keys(), default=EXIT_CODE_UNKNOWN,\n        metavar=\"NETWORK_ERRORS_EXIT_CODE\", help=\"network errors exit code\"\n    )\n\n    options = parser.parse_args(sys.argv)[0]\n    STATE2TEMPLATE[STATE_STOPPED] = options.stopped_state  # update stopped state value from command line argument\n\n    # check mandatory command line options supplied\n    if not options.server:\n        parser.error(\"Required server address option missing\")\n    if options.username and not options.password:\n        parser.error(\"Required supervisord user password\")\n\n    return options"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_status(options):\n\n    payload = {  # server connection URI formatted string payload\n        \"username\": options.username,\n        \"password\": options.password,\n        \"server\": options.server,\n        \"port\": options.port,\n    }\n\n    try:\n        if options.server.startswith(\"/\") and stat.S_ISSOCK(os.stat(options.server).st_mode):  # communicate with server via unix socket (simple check is server address is path and path is unix socket)\n            try:\n                import supervisor.xmlrpc\n            except ImportError as error:\n                sys.stderr.write(\"ERROR: Couldn't load module. {error}\\n\".format(error=error))\n                sys.stderr.write(\"ERROR: Unix socket support not available! Please install nagios-check-supervisord with unix socket support: 'nagios-check-supervisord[unix-socket-support]' or install 'supervisor' separately.\\n\")\n                sys.exit(-1)\n\n            if all([options.username, options.password, ]):  # with auth\n                connection = xmlrpclib.ServerProxy(\"https://\", transport=supervisor.xmlrpc.SupervisorTransport(options.username, options.password, serverurl=URI[URI_TPL_SOCKET].format(**payload)))\n            else:\n                connection = xmlrpclib.ServerProxy(\"https://\", transport=supervisor.xmlrpc.SupervisorTransport(None, None, serverurl=URI[URI_TPL_SOCKET].format(**payload)))\n\n        else:  # communicate with server via http\n            if all([options.username, options.password, ]):  # with auth\n                connection = xmlrpclib.Server(URI[URI_TPL_HTTP_AUTH].format(**payload))\n            else:\n                connection = xmlrpclib.Server(URI[URI_TPL_HTTP].format(**payload))\n\n        return connection.supervisor.getAllProcessInfo()\n\n    except Exception as error:\n        if not options.quiet:\n            sys.stdout.write(\"ERROR: Server communication problem. {error}\\n\".format(error=error))\n        sys.exit(EXIT_CODES.get(options.network_errors_exit_code, EXIT_CODE_UNKNOWN))", "response": "Get programs statuses.\n\n    :param options: parsed commandline arguments.\n    :type options: optparse.Values.\n    :return: supervisord XML-RPC call result.\n    :rtype: dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_output(data, options):\n\n    output = {}\n    programs = map(strip, options.programs.strip().split(\",\")) if options.programs else map(lambda x: x[\"name\"], data)\n\n    for program in programs:\n        try:\n            program_data = filter(lambda x: x[\"name\"] == program, data)[0]\n            output.update({\n                program: {\n                    \"name\": program,\n                    \"template\": STATE2TEMPLATE[program_data[\"statename\"]],\n                    \"status\": program_data[\"spawnerr\"] if program_data[\"spawnerr\"] else program_data[\"statename\"],\n                }\n            })\n        except IndexError:\n            output.update({\n                program: {\n                    \"name\": program,\n                    \"template\": \"unknown\",\n                    \"status\": \"\",\n                }\n            })\n\n    # getting main status for check (for multiple check need to get main status by priority)\n    statuses = [status[0] for status in sorted([(status, OUTPUT_TEMPLATES[status][\"priority\"]) for status in list(set([output[d][\"template\"] for d in output.keys()]))], key=lambda x: x[1])]\n    # if no programs found or configured by supervisord set status ok and custom message\n    status = statuses[0] if statuses else EXIT_CODE_OK\n    text = \", \".join([OUTPUT_TEMPLATES[output[program][\"template\"]][\"text\"].format(**output[program]) for program in sorted(output.keys(), key=lambda x: OUTPUT_TEMPLATES[output[x][\"template\"]][\"priority\"])]) if statuses else \"No program configured/found\"\n\n    # create exit code (unknown if something happened wrong)\n    code = EXIT_CODES.get(status, EXIT_CODE_UNKNOWN)\n\n    # return full status string with main status for multiple programs and all programs states\n    return \"{status}: {output}\\n\".format(**{\n        \"status\": status.upper(),\n        \"output\": text,\n    }), code", "response": "Create Nagios and human readable supervisord statuses and exit code."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate a decoded SNS message.", "response": "def validate(\n    message,\n    get_certificate=lambda url: urlopen(url).read(),\n    certificate_url_regex=DEFAULT_CERTIFICATE_URL_REGEX,\n    max_age=DEFAULT_MAX_AGE\n):\n    \"\"\"\n    Validate a decoded SNS message.\n\n    Parameters:\n        message:\n            Decoded SNS message.\n\n        get_certificate:\n            Function that receives a URL, and returns the certificate from that\n            URL as a string. The default doesn't implement caching.\n\n        certificate_url_regex:\n            Regex that validates the signing certificate URL. Default value\n            checks it's hosted on an AWS-controlled domain, in the format\n            \"https://sns.<data-center>.amazonaws.com/\"\n\n        max_age:\n            Maximum age of an SNS message before it fails validation, expressed\n            as a `datetime.timedelta`. Defaults to one hour, the max. lifetime\n            of an SNS message.\n    \"\"\"\n\n    # Check the signing certicate URL.\n    SigningCertURLValidator(certificate_url_regex).validate(message)\n\n    # Check the message age.\n    if not isinstance(max_age, datetime.timedelta):\n        raise ValueError(\"max_age must be None or a timedelta object\")\n    MessageAgeValidator(max_age).validate(message)\n\n    # Passed the basic checks, let's download the cert.\n    # We've validated the URL, so aren't worried about a malicious server.\n    certificate = get_certificate(message[\"SigningCertURL\"])\n\n    # Check the cryptographic signature.\n    SignatureValidator(certificate).validate(message)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting numpy data to an FCS file.", "response": "def write_fcs(filename, chn_names, data,\n              endianness=\"big\",\n              compat_chn_names=True,\n              compat_copy=True,\n              compat_negative=True,\n              compat_percent=True,\n              compat_max_int16=10000):\n    \"\"\"Write numpy data to an .fcs file (FCS3.0 file format)\n\n\n    Parameters\n    ----------\n    filename: str or pathlib.Path\n        Path to the output .fcs file\n    ch_names: list of str, length C\n        Names of the output channels\n    data: 2d ndarray of shape (N,C)\n        The numpy array data to store as .fcs file format.\n    endianness: str\n        Set to \"little\" or \"big\" to define the byte order used.\n    compat_chn_names: bool\n        Compatibility mode for 3rd party flow analysis software:\n        The characters \" \", \"?\", and \"_\" are removed in the output\n        channel names.\n    compat_copy: bool\n        Do not override the input array `data` when modified in\n        compatibility mode.\n    compat_negative: bool\n        Compatibliity mode for 3rd party flow analysis software:\n        Flip the sign of `data` if its mean is smaller than zero.\n    compat_percent: bool\n        Compatibliity mode for 3rd party flow analysis software:\n        If a column in `data` contains values only between 0 and 1,\n        they are multiplied by 100.\n    compat_max_int16: int\n        Compatibliity mode for 3rd party flow analysis software:\n        If a column in `data` has a maximum above this value,\n        then the display-maximum is set to 2**15.\n\n    Notes\n    -----\n\n    - These commonly used unicode characters are replaced: \"\u00b5\", \"\u00b2\"\n    - If the input data contain NaN values, the corresponding rows\n      are excluded due to incompatibility with the FCS file format.\n\n    \"\"\"\n    filename = pathlib.Path(filename)\n    if not isinstance(data, np.ndarray):\n        data = np.array(data, dtype=float)\n    # remove rows with nan values\n    nanrows = np.isnan(data).any(axis=1)\n    if np.sum(nanrows):\n        msg = \"Rows containing NaNs are not written to {}!\".format(filename)\n        warnings.warn(msg)\n        data = data[~nanrows]\n    if endianness not in [\"little\", \"big\"]:\n        raise ValueError(\"`endianness` must be 'little' or 'big'!\")\n\n    msg = \"length of `chn_names` must match length of 2nd axis of `data`\"\n    assert len(chn_names) == data.shape[1], msg\n\n    rpl = [[\"\u00b5\", \"u\"],\n           [\"\u00b2\", \"2\"],\n           ]\n\n    if compat_chn_names:\n        # Compatibility mode: Clean up headers.\n        rpl += [[\" \", \"\"],\n                [\"?\", \"\"],\n                [\"_\", \"\"],\n                ]\n\n    for ii in range(len(chn_names)):\n        for (a, b) in rpl:\n            chn_names[ii] = chn_names[ii].replace(a, b)\n\n    # Data with values between 0 and 1\n    pcnt_cands = []\n    for ch in range(data.shape[1]):\n        if data[:, ch].min() >= 0 and data[:, ch].max() <= 1:\n            pcnt_cands.append(ch)\n    if compat_percent and pcnt_cands:\n        # Compatibility mode: Scale values b/w 0 and 1 to percent\n        if compat_copy:\n            # copy if requested\n            data = data.copy()\n        for ch in pcnt_cands:\n            data[:, ch] *= 100\n\n    if compat_negative:\n        toflip = []\n        for ch in range(data.shape[1]):\n            if np.mean(data[:, ch]) < 0:\n                toflip.append(ch)\n        if len(toflip):\n            if compat_copy:\n                # copy if requested\n                data = data.copy()\n            for ch in toflip:\n                data[:, ch] *= -1\n\n    # DATA segment\n    data1 = data.flatten().tolist()\n    DATA = struct.pack('>%sf' % len(data1), *data1)\n\n    # TEXT segment\n    header_size = 256\n\n    if endianness == \"little\":\n        # use little endian\n        byteord = '1,2,3,4'\n    else:\n        # use big endian\n        byteord = '4,3,2,1'\n    TEXT = '/$BEGINANALYSIS/0/$ENDANALYSIS/0'\n    TEXT += '/$BEGINSTEXT/0/$ENDSTEXT/0'\n    # Add placeholders for $BEGINDATA and $ENDDATA, because we don't\n    # know yet how long TEXT is.\n    TEXT += '/$BEGINDATA/{data_start_byte}/$ENDDATA/{data_end_byte}'\n    TEXT += '/$BYTEORD/{0}/$DATATYPE/F'.format(byteord)\n    TEXT += '/$MODE/L/$NEXTDATA/0/$TOT/{0}'.format(data.shape[0])\n    TEXT += '/$PAR/{0}'.format(data.shape[1])\n\n    # Check for content of data columns and set range\n    for jj in range(data.shape[1]):\n        # Set data maximum to that of int16\n        if (compat_max_int16 and\n            np.max(data[:, jj]) > compat_max_int16 and\n                np.max(data[:, jj]) < 2**15):\n            pnrange = int(2**15)\n        # Set range for data with values between 0 and 1\n        elif jj in pcnt_cands:\n            if compat_percent:  # scaled to 100%\n                pnrange = 100\n            else:  # not scaled\n                pnrange = 1\n        # default: set range to maxium value found in column\n        else:\n            pnrange = int(abs(np.max(data[:, jj])))\n        # TODO:\n        # - Set log/lin\n        fmt_str = '/$P{0}B/32/$P{0}E/0,0/$P{0}N/{1}/$P{0}R/{2}/$P{0}D/Linear'\n        TEXT += fmt_str.format(jj+1, chn_names[jj], pnrange)\n    TEXT += '/'\n\n    # SET $BEGINDATA and $ENDDATA using the current size of TEXT plus padding.\n    text_padding = 47  # for visual separation and safety\n    data_start_byte = header_size + len(TEXT) + text_padding\n    data_end_byte = data_start_byte + len(DATA) - 1\n    TEXT = TEXT.format(data_start_byte=data_start_byte,\n                       data_end_byte=data_end_byte)\n    lentxt = len(TEXT)\n    # Pad TEXT segment with spaces until data_start_byte\n    TEXT = TEXT.ljust(data_start_byte - header_size, \" \")\n\n    # HEADER segment\n    ver = 'FCS3.0'\n\n    textfirst = '{0: >8}'.format(header_size)\n    textlast = '{0: >8}'.format(lentxt + header_size - 1)\n\n    # Starting with FCS 3.0, data segment can end beyond byte 99,999,999,\n    # in which case a zero is written in each of the two header fields (the\n    # values are given in the text segment keywords $BEGINDATA and $ENDDATA)\n    if data_end_byte <= 99999999:\n        datafirst = '{0: >8}'.format(data_start_byte)\n        datalast = '{0: >8}'.format(data_end_byte)\n    else:\n        datafirst = '{0: >8}'.format(0)\n        datalast = '{0: >8}'.format(0)\n\n    anafirst = '{0: >8}'.format(0)\n    analast = '{0: >8}'.format(0)\n\n    HEADER = '{0: <256}'.format(ver + '    '\n                                + textfirst\n                                + textlast\n                                + datafirst\n                                + datalast\n                                + anafirst\n                                + analast)\n\n    # Write data\n    with filename.open(\"wb\") as fd:\n        fd.write(HEADER.encode(\"ascii\", \"replace\"))\n        fd.write(TEXT.encode(\"ascii\", \"replace\"))\n        fd.write(DATA)\n        fd.write(b'00000000')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread tdms file and return channel names and data", "response": "def read_tdms(tdms_file):\n    \"\"\"Read tdms file and return channel names and data\"\"\"\n    tdms_file = nptdms.TdmsFile(tdms_file)\n    ch_names = []\n    ch_data = []\n\n    for o in tdms_file.objects.values():\n        if o.data is not None and len(o.data):\n            chn = o.path.split('/')[-1].strip(\"'\")\n            if \"unit_string\" in o.properties:\n                unit = o.properties[\"unit_string\"]\n                ch_names.append(\"{} [{}]\".format(chn, unit))\n            else:\n                ch_names.append(chn)\n\n            ch_data.append(o.data)\n\n    return ch_names, ch_data"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a deformation to the list of chn_names.", "response": "def add_deformation(chn_names, data):\n    \"\"\"From circularity, compute the deformation\n\n    This method is useful for RT-DC data sets that contain\n    the circularity but not the deformation.\n    \"\"\"\n    if \"deformation\" not in chn_names:\n        for ii, ch in enumerate(chn_names):\n            if ch == \"circularity\":\n                chn_names.append(\"deformation\")\n                data.append(1-data[ii])\n\n    return chn_names, data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating an fcs file for a given tdms file", "response": "def tdms2fcs(tdms_file):\n    \"\"\"Creates an fcs file for a given tdms file\"\"\"\n    fcs_file = tdms_file[:-4]+\"fcs\"\n    chn_names, data = read_tdms(tdms_file)\n    chn_names, data = add_deformation(chn_names, data)\n    fcswrite.write_fcs(filename=fcs_file,\n                       chn_names=chn_names,\n                       data=np.array(data).transpose())"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_header(self, patch_dir=None):\n        lines = []\n\n        if patch_dir:\n            file = patch_dir + File(self.get_name())\n            name = file.get_name()\n        else:\n            name = self.get_name()\n        with open(name, \"rb\") as f:\n            for line in f:\n                if line.startswith(b\"---\") or line.startswith(b\"Index:\"):\n                    break\n                lines.append(line)\n\n        return b\"\".join(lines)", "response": "Returns the header of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning True if left and right are equal False otherwise.", "response": "def equal(self, cwd):\n        \"\"\" Returns True if left and right are equal\n        \"\"\"\n        cmd = [\"diff\"]\n        cmd.append(\"-q\")\n        cmd.append(self.left.get_name())\n        cmd.append(self.right.get_name())\n\n        try:\n            Process(cmd).run(cwd=cwd, suppress_output=True)\n        except SubprocessError as e:\n            if e.get_returncode() == 1:\n                return False\n            else:\n                raise e\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a new patch with patchname to the queue", "response": "def create(self, patchname):\n        \"\"\" Adds a new patch with patchname to the queue\n\n        The new patch will be added as the topmost applied patch.\n        \"\"\"\n        patch = Patch(patchname)\n        if self.series.is_patch(patch):\n            raise PatchAlreadyExists(self.series, patchname)\n\n        patch_dir = self.quilt_patches\n        patch_dir.create()\n        patchfile = patch_dir + File(patchname)\n        patchfile.touch()\n\n        pc_dir = self.quilt_pc + patchname\n        if pc_dir.exists():\n            # be sure that the directory is clear\n            pc_dir.delete()\n\n        # create empty .pc/<patchname> directory as quilt does too\n        pc_dir.create()\n\n        top = self.db.top_patch()\n        # add new patch after the current topmost applied patch\n        self.series.add_patches([patch], top)\n        # \"apply\" patch\n        self.db.add_patch(patch)\n\n        # create patches/series files\n        self.series.save()\n        # create .pc/.version and .pc/applied-patches files\n        self.db.save()\n\n        self.patch_created(patch)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_next(self, remove=False, backup=False):\n        patch = self.db.top_patch()\n        if patch:\n            after = self.series.patch_after(patch)\n        else:\n            after = self.series.first_patch()\n        if not after:\n            raise QuiltError(\"No next patch\")\n\n        self._delete_patch(after, remove=remove, backup=backup)", "response": "Delete next unapplied patch."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete_patch(self, patch_name=None, remove=False, backup=False):\n        if patch_name:\n            patch = Patch(patch_name)\n        else:\n            patch = self.db.top_patch()\n            if not patch:\n                raise NoAppliedPatch(self.db)\n\n        self._delete_patch(patch, remove=remove, backup=backup)", "response": "Delete specified patch from the series\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if a backup file of the filename is already in the patch", "response": "def _file_in_patch(self, filename, patch, ignore):\n        \"\"\" Checks if a backup file of the filename in the current patch\n        exists \"\"\"\n        file = self.quilt_pc + File(os.path.join(patch.get_name(), filename))\n        if file.exists():\n            if ignore:\n                return True\n            else:\n                raise QuiltError(\"File %s is already in patch %s\" % (filename,\n                                 patch.get_name()))\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _backup_file(self, file, patch):\n        dest_dir = self.quilt_pc + patch.get_name()\n        file_dir = file.get_directory()\n        if file_dir:\n            #TODO get relative path\n            dest_dir = dest_dir + file_dir\n        backup = Backup()\n        backup.backup_file(file, dest_dir, copy_empty=True)", "response": "Creates a backup of the file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_file(self, filename, patch_name=None, ignore=False):\n        file = File(filename)\n\n        if patch_name:\n            patch = Patch(patch_name)\n        else:\n            patch = self.db.top_patch()\n            if not patch:\n                raise NoAppliedPatch(self.db)\n\n        exists = self._file_in_patch(filename, patch, ignore)\n        if exists:\n            return\n\n        self._file_in_next_patches(filename, patch)\n\n        if file.is_link():\n            raise QuiltError(\"Cannot add symbolic link %s\" % filename)\n\n        self._backup_file(file, patch)\n\n        if file.exists():\n            # be sure user can write original file\n            os.chmod(filename, file.get_mode() | stat.S_IWUSR | stat.S_IRUSR)\n\n        self.file_added(file, patch)", "response": "Add a file to the current version of the current version of the current version of the current version of the current version of the current version of the current version."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run(self, suppress_output=False, inputdata=None, **kw):\n        if inputdata is not None:\n            kw[\"stdin\"] = subprocess.PIPE\n        if suppress_output:\n            kw[\"stdout\"] = open(os.devnull, \"w\")\n            kw[\"stderr\"] = kw[\"stdout\"]\n        try:\n            try:\n                process = subprocess.Popen(self.cmd, **kw)\n            finally:\n                if suppress_output:\n                    kw[\"stdout\"].close()\n        except OSError as e:\n            msg = \"Failed starting command {!r}: {}\".format(self.cmd, e)\n            raise QuiltError(msg)\n\n        if inputdata is not None:\n            process.stdin.write(inputdata)\n            process.stdin.close()\n        ret = process.wait()\n        if ret != 0:\n            raise SubprocessError(self.cmd, ret)", "response": "Run command as a subprocess and wait until it is finished."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create(self):\n        if self.dirname and not os.path.exists(self.dirname):\n            os.makedirs(self.dirname)", "response": "Creates the directory and all its parent directories if it does not exist yet."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncopies the contents of this object to dest recursively.", "response": "def copy(self, dest, symlinks=False):\n        \"\"\" Copy to destination directory recursively.\n        If symlinks is true, symbolic links in the source tree are represented\n        as symbolic links in the new tree, but the metadata of the original\n        links is NOT copied; if false or omitted, the contents and metadata of\n        the linked files are copied to the new tree.\n        \"\"\"\n        if isinstance(dest, Directory):\n            dest = dest.get_name()\n\n        shutil.copytree(self.dirname, dest)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef link(self, link):\n        if isinstance(link, File):\n            link = link.filename\n        os.link(self.filename, link)", "response": "Create hard link to this file"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef copy(self, dest):\n        if isinstance(dest, File):\n            dest_dir = dest.get_directory()\n            dest_dir.create()\n            dest = dest.filename\n        elif isinstance(dest, Directory):\n            dest = dest.dirname\n\n        shutil.copy2(self.filename, dest)", "response": "Copy file to destination"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the directory where the file is placed in or None if the file does not contain a directory", "response": "def get_directory(self):\n        \"\"\" Returns the directory where the file is placed in or None if the\n        path to the file doesn't contain a directory\n        \"\"\"\n        dirname = os.path.dirname(self.filename)\n        if dirname:\n            return Directory(dirname)\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef backup_file(self, file, dest_dir, copy_empty=False):\n        if file.exists():\n            if not copy_empty and file.is_empty():\n                return None\n            dest_dir.create()\n            file.copy(dest_dir)\n            return dest_dir + file.get_basefile()\n        elif copy_empty:\n            # create new file in dest_dir\n            dest_dir = dest_dir + file.get_directory()\n            dest_dir.create()\n            dest_file = dest_dir + file.get_basefile()\n            dest_file.touch()\n            return dest_file\n        else:\n            return None", "response": "Backup a file in dest_dir."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrefresh the cache with the given name or applied top patch.", "response": "def refresh(self, patch_name=None, edit=False):\n        \"\"\" Refresh patch with patch_name or applied top patch if patch_name is\n        None\n        \"\"\"\n        if patch_name:\n            patch = Patch(patch_name)\n        else:\n            patch = self.db.top_patch()\n\n            if not patch:\n                raise QuiltError(\"No patch applied. Nothing to refresh.\")\n\n        pc_dir = self.quilt_pc + patch.get_name()\n        patch_file = self.quilt_patches + File(patch.get_name())\n        files = pc_dir.content()[1]\n\n        with TmpFile(prefix=\"pquilt-\") as tmpfile:\n            f = tmpfile.open()\n\n            if patch_file.exists():\n                header = patch.get_header(self.quilt_patches)\n                tmpfile.write(header)\n\n            for file_name in files:\n                if file_name == \".timestamp\":\n                    continue\n                orig_file = pc_dir + File(file_name)\n                new_file = File(file_name)\n                left_label, right_label, index = self._get_labels(file_name,\n                                                                  orig_file,\n                                                                  new_file)\n                self._write_index(tmpfile, index)\n\n                diff = Diff(orig_file, new_file)\n                diff.run(self.cwd, fd=f, left_label=left_label,\n                         right_label=right_label)\n\n            if tmpfile.is_empty():\n                raise QuiltError(\"Nothing to refresh.\")\n\n            if edit:\n                self.edit_patch(tmpfile)\n                tpatch = Patch(tmpfile.get_name())\n                tpatch.run(pc_dir.get_name(), dry_run=True, quiet=True)\n\n            if patch_file.exists():\n                diff = Diff(patch_file, tmpfile)\n                if diff.equal(self.cwd):\n                    raise QuiltError(\"Nothing to refresh.\")\n\n            tmpfile.copy(patch_file)\n\n        timestamp = pc_dir + File(\".timestamp\")\n        timestamp.touch()\n\n        refresh = self.quilt_pc + File(patch.get_name() + \"~refresh\")\n        refresh.delete_if_exists()\n\n        self.refreshed(patch)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef apply_patch(self, patch_name, force=False, quiet=False):\n        self._check()\n        patch = Patch(patch_name)\n        patches = self.series.patches_until(patch)[:]\n\n        applied = self.db.applied_patches()\n        for patch in applied:\n            if patch in patches:\n                patches.remove(patch)\n\n        if not patches:\n            raise AllPatchesApplied(self.series, self.db.top_patch())\n\n        self.applying(patch)\n\n        try:\n            for cur_patch in patches:\n                self._apply_patch(cur_patch, force, quiet)\n        finally:\n            self.db.save()\n\n        self.applied(self.db.top_patch())", "response": "Apply all patches up to patch_name."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef apply_next_patch(self, force=False, quiet=False):\n        self._check()\n        top = self.db.top_patch()\n        if not top:\n            patch = self.series.first_patch()\n        else:\n            patch = self.series.patch_after(top)\n\n        if not patch:\n            raise AllPatchesApplied(self.series, top)\n\n        self.applying(patch)\n\n        self._apply_patch(patch, force, quiet)\n\n        self.db.save()\n\n        self.applied(self.db.top_patch())", "response": "Apply next patch in series file"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef apply_all(self, force=False, quiet=False):\n        self._check()\n        top = self.db.top_patch()\n        if top:\n            patches = self.series.patches_after(top)\n        else:\n            patches = self.series.patches()\n\n        if not patches:\n            raise AllPatchesApplied(self.series, top)\n\n        try:\n            for patch in patches:\n                self.applying(patch)\n                self._apply_patch(patch, force, quiet)\n        finally:\n            self.db.save()\n\n        self.applied(self.db.top_patch())", "response": "Apply all patches in series file"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read(self):\n        self.patchlines = []\n        self.patch2line = dict()\n        if self.exists():\n            with open(self.series_file, \"r\") as f:\n                for line in f:\n                    self.add_patch(line)", "response": "Reads all patches from the series file"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save(self):\n        with open(self.series_file, \"wb\") as f:\n            for patchline in self.patchlines:\n                f.write(_encode_str(str(patchline)))\n                f.write(b\"\\n\")", "response": "Saves the current patches list in the series file"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_patch(self, patch):\n        patchline = PatchLine(patch)\n        patch = patchline.get_patch()\n        if patch:\n            self.patch2line[patch] = patchline\n        self.patchlines.append(patchline)", "response": "Adds a patch to the list of patches"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninsert list of patches at the front of the curent patches list", "response": "def insert_patches(self, patches):\n        \"\"\" Insert list of patches at the front of the curent patches list \"\"\"\n        patchlines = []\n        for patch_name in patches:\n            patchline = PatchLine(patch_name)\n            patch = patchline.get_patch()\n            if patch:\n                self.patch2line[patch] = patchline\n            patchlines.append(patchline)\n        patchlines.extend(self.patchlines)\n        self.patchlines = patchlines"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a list of patches to the list of patches.", "response": "def add_patches(self, patches, after=None):\n        \"\"\" Add a list of patches to the patches list \"\"\"\n        if after is None:\n            self.insert_patches(patches)\n        else:\n            self._check_patch(after)\n            patchlines = self._patchlines_before(after)\n            patchlines.append(self.patch2line[after])\n            for patch in patches:\n                patchline = PatchLine(patch)\n                patchlines.append(patchline)\n                self.patch2line[patchline.get_patch()] = patchline\n            patchlines.extend(self._patchlines_after(after))\n            self.patchlines = patchlines"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove a patch from the patches list.", "response": "def remove_patch(self, patch):\n        \"\"\" Remove a patch from the patches list \"\"\"\n        self._check_patch(patch)\n        patchline = self.patch2line[patch]\n        del self.patch2line[patch]\n        self.patchlines.remove(patchline)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef patches_after(self, patch):\n        return [line.get_patch() for line in self._patchlines_after(patch) if\n                line.get_patch()]", "response": "Returns a list of patches after patch from the patches list"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of patches before patch from the patches list", "response": "def patches_before(self, patch):\n        \"\"\" Returns a list of patches before patch from the patches list \"\"\"\n        return [line.get_patch() for line in self._patchlines_before(patch)\n                if line.get_patch()]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of patches before the given patch", "response": "def patches_until(self, patch):\n        \"\"\" Returns a list of patches before patch from the patches list\n        including the provided patch\n        \"\"\"\n        return [line.get_patch() for line in self._patchlines_until(patch) if\n                line.get_patch()]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreplaces old_patch with new_patch.", "response": "def replace(self, old_patch, new_patch):\n        \"\"\" Replace old_patch with new_patch\n        The method only replaces the patch and doesn't change any comments.\n        \"\"\"\n        self._check_patch(old_patch)\n        old_patchline = self.patch2line[old_patch]\n        index = self.patchlines.index(old_patchline)\n        self.patchlines.pop(index)\n        new_patchline = PatchLine(new_patch)\n        new_patchline.set_comment(old_patchline.get_comment())\n        self.patchlines.insert(index, new_patchline)\n        del self.patch2line[old_patch]\n        self.patch2line[new_patch] = new_patchline"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create(self):\n        if not os.path.exists(self.dirname):\n            os.makedirs(self.dirname)\n        self._create_version(self.version_file)", "response": "Creates the dirname and inserts a. version file"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if the. version file in dirname has the correct supported version number.", "response": "def check_version(self, version_file):\n        \"\"\" Checks if the .version file in dirname has the correct supported\n            version number \"\"\"\n        # The file contains a version number as a decimal integer, optionally\n        # followed by a newline\n        with open(version_file, \"r\") as f:\n            version = f.read(10)\n\n        version = version.rstrip(\"\\r\\n\")\n\n        if len(version) >= 10 or version != str(DB_VERSION):\n            raise DBError(\"The quilt meta-data version of %s is not supported \"\n                          \"by python-quilt. python-quilt only supports \"\n                          \"version %s.\" % (version, DB_VERSION))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding the group and its arguments to a argparse. ArgumentParser instance.", "response": "def add_to_parser(self, parser):\n        \"\"\"\n        Adds the group and its arguments to a argparse.ArgumentParser instance\n\n        @param parser A argparse.ArgumentParser instance\n        \"\"\"\n        self.group = parser.add_argument_group(self.title, self.description)\n        for arg in self.arguments:\n            arg.add_to_parser(self.group)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_to_parser(self, parser):\n        kwargs = self._get_kwargs()\n        args = self._get_args()\n        parser.add_argument(*args, **kwargs)", "response": "Adds the argument to an argparse. ArgumentParser instance"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_to_parser(self, subparsers):\n        parser = subparsers.add_parser(*self._get_args(), **self._get_kwargs())\n        parser.set_defaults(**self.get_defaults())\n        for name, group in self.base_argument_groups:\n            group.add_to_parser(parser)\n        for name, arg in self.base_arguments:\n            arg.add_to_parser(parser)\n        self.add_subparsers(parser)", "response": "Adds this SubParser to the subparsers created by\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_subparsers(self, parser):\n        sgroup = getattr(self, \"subparser_group\", None)\n        if sgroup:\n            sgroup.add_to_parser(self)\n\n        if not self.subparsers:\n            return\n\n        args = self.subparsers_args or self.get_default_subparsers_args()\n        kwargs = self.subparsers_kwargs or self.get_default_subparsers_kwargs()\n        subs = parser.add_subparsers(*args, **kwargs)\n\n        for subparser in self.subparsers:\n            subparser.add_to_parser(subs)", "response": "Adds the subparsers to an argparse. ArgumentParser instance."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _file_in_patch(self, filename, patch):\n        pc_dir = self.quilt_pc + patch.get_name()\n        file = pc_dir + File(filename)\n        if not file.exists():\n            raise QuiltError(\"File %s is not in patch %s\" % (filename,\n                             patch.get_name()))", "response": "Checks if a backup file of the filename is in the current patch"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if a backup file of the filename in the applied patches after patch exists", "response": "def _file_in_next_patches(self, filename, patch):\n        \"\"\" Checks if a backup file of the filename in the applied patches after\n        patch exists \"\"\"\n\n        if not self.db.is_patch(patch):\n            # no patches applied\n            return\n\n        patches = self.db.patches_after(patch)\n        for patch in patches:\n            file = self.quilt_pc + File(os.path.join(patch.get_name(),\n                                                     filename))\n            if file.exists():\n                raise QuiltError(\"File %s is modified by patch %s\" %\n                                 (filename, patch.get_name()))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef revert_file(self, filename, patch_name=None):\n        file = File(filename)\n\n        if patch_name:\n            patch = Patch(patch_name)\n        else:\n            patch = self.db.top_patch()\n\n            if not patch:\n                raise QuiltError(\"No patch available. Nothing to revert.\")\n\n        self._file_in_patch(filename, patch)\n        self._file_in_next_patches(filename, patch)\n        pc_dir = self.quilt_pc + patch.get_name()\n        pc_file = pc_dir + file\n\n        if not file.exists() and pc_file.is_empty():\n            # new and empty file will be reverted\n            pc_file.delete()\n            self.file_reverted(file, patch)\n            return\n\n        with TmpDirectory(prefix=\"pquilt-\") as tmpdir:\n            # apply current patch in temporary directory to revert changes of\n            # file that aren't committed in the patch\n            tmp_file = self._apply_patch_temporary(tmpdir, pc_file, patch)\n            if tmp_file and tmp_file.exists() and not tmp_file.is_empty():\n\n                diff = Diff(file, tmp_file)\n                if diff.equal(self.cwd):\n                    self.file_unchanged(file, patch)\n                    return\n\n                dir = file.get_directory()\n                if not dir:\n                    dir = Directory(os.getcwd())\n                else:\n                    dir.create()\n                tmp_file.copy(dir)\n                self.file_reverted(file, patch)\n            else:\n                self.file_unchanged(file, patch)", "response": "Revert not added changes of filename."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nimport a patch into the queue", "response": "def import_patch(self, patch_name, new_name=None):\n        \"\"\" Import patch into the patch queue\n        The patch is inserted as the next unapplied patch.\n        \"\"\"\n        if new_name:\n            dir_name = os.path.dirname(new_name)\n            name = os.path.basename(new_name)\n            dest_dir = self.quilt_patches + Directory(dir_name)\n            dest_dir.create()\n        else:\n            name = os.path.basename(patch_name)\n            dest_dir = self.quilt_patches\n\n        patch_file = File(patch_name)\n        dest_file = dest_dir + File(name)\n        patch_file.copy(dest_file)\n        self._import_patches([name])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef import_patches(self, patches):\n\n        dest_dir = self.quilt_patches\n        patch_names = []\n\n        for patch in patches:\n            patch_name = os.path.basename(patch)\n            patch_file = File(patch)\n            dest_file = dest_dir + File(patch_name)\n            patch_file.copy(dest_file)\n            patch_names.append(patch_name)\n\n        self._import_patches(patch_names)", "response": "Import several patches into the patch queue"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef missing_node_ids(self):\n        present_node_ids = self.nodes.keys()\n        for nid in self.node_ids:\n            if nid not in present_node_ids:\n                yield nid", "response": "Get a list of nodes not found in OSM data."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_route(relation):\n    if relation.tags.get('type') != 'route':\n        # Build route only for relations of type `route`\n        return\n\n    short_name = create_route_short_name(relation)\n    color = relation.tags.get('color')\n    return\\\n        Route(relation.id,\n              short_name,\n              create_route_long_name(relation, short_name),\n              map_osm_route_type_to_gtfs(relation.tags.get('route')),\n              'https://www.openstreetmap.org/relation/{}'.format(relation.id),\n              color.strip('#') if color else '',\n              get_agency_id(relation))", "response": "Extract information of one route."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_route_long_name(relation, short_name):\n    if relation.tags.get('from') and relation.tags.get('to'):\n        return \"{0}-to-{1}\".format(relation.tags.get('from'),\n                                   relation.tags.get('to'))\n    name = relation.tags.get('name') or\\\n        relation.tags.get('alt_name') or\\\n        \"OSM Route No. {}\".format(relation.id)\n\n    # Drop route_short_name from this one if it contains it\n    if short_name and name.startswith(short_name):\n        # Drop it\n        return name[len(short_name):]\n    return name", "response": "Create a meaningful route name."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconstructs an id for agency using its tags.", "response": "def get_agency_id(relation):\n    \"\"\"Construct an id for agency using its tags.\"\"\"\n    op = relation.tags.get('operator')\n    if op:\n        return int(hashlib.sha256(op.encode('utf-8')).hexdigest(), 16) % 10**8\n    return -1"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process(self):\n\n        # Extract relations\n        self.rh = RelationHandler()\n        self.rh.apply_file(self.filename)\n\n        logging.debug('Found %d public transport relations.', len(self.rh.relations))\n\n        # Collect ids of interest\n        node_ids, stop_node_ids, way_ids, reverse_map = self.__collect_ids()\n\n        # Extract nodes\n        self.nh = NodeHandler(node_ids)\n        self.nh.apply_file(self.filename, locations=True)\n\n        count = 0\n        for idx, missing_node_id in enumerate(self.nh.missing_node_ids):\n            count += 1\n            logging.warning(\n                '[no data] missing stop node. rel: https://osm.org/relation/%s node: https://osm.org/node/%s.',\n                reverse_map[missing_node_id], missing_node_id)\n\n        if count:\n            logging.warning(\n                '%d nodes that appear in relations are missing.',\n                count)\n        else:\n            logging.debug('Lucky you! All relation member nodes were found.')\n\n        # Extract ways\n        self.wh = WayHandler(way_ids)\n        self.wh.apply_file(self.filename, locations=True)", "response": "Process the files and collect necessary data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_dummy_data(routes, stops):\n    # Build stops per route auxiliary map\n    stops_per_route = defaultdict(lambda: [])\n    stops_map = {}\n    for s in stops:\n        if not s.route_id:\n            continue\n        stops_per_route[s.route_id].append(s)\n        stops_map[s.stop_id] = s\n\n    calendar = _create_dummy_calendar()\n\n    trips = \\\n        _create_dummy_trips(\n            routes,\n            stops_per_route,\n            calendar)\n\n    stop_times = _create_dummy_stoptimes(trips, stops_per_route)\n    frequencies = _create_dummy_frequencies(trips)\n\n    return DummyData(calendar, stop_times, trips, frequencies)", "response": "Create dummy data for a single item."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npatch the agencies to include the ones that are needed for passing transitfeed checks.", "response": "def patch_agencies(agencies):\n    \"\"\"Fill the fields that are necessary for passing transitfeed checks.\"\"\"\n    # First return the unknown agency entry\n    yield Agency(-1, 'http://hiposfer.com', 'Unknown agency', 'Europe/Berlin')\n\n    # Then return the rest.\n    for agency_id, agency_url, agency_name, agency_timezone in agencies:\n        if not agency_url:\n            agency_url = 'http://hiposfer.com'\n        if not agency_timezone:\n            # Set everything to one time zone to get rid of transitfeeds error.\n            agency_timezone = 'Europe/Berlin'\n        yield Agency(agency_id, agency_url, agency_name, agency_timezone)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates dummy trip stop times for each stop.", "response": "def _create_dummy_trip_stoptimes(trip_id, stops, first_service_time):\n    \"\"\"Create station stop times for each trip.\"\"\"\n    waiting = datetime.timedelta(seconds=30)\n    arrival = first_service_time\n    last_departure = first_service_time\n    last_departure_hour = (arrival + waiting).hour\n    last_stop = None\n\n    departure_hour = None\n    arrival_hour = None\n    for stop_sequence, stop in enumerate(stops):\n\n        # Avoid time travels\n        arrival = last_departure + get_time_from_last_stop(last_stop, stop)\n        departure = arrival + waiting\n\n        # Cover the case when the arrival time falls into the next day\n        if arrival.hour < last_departure_hour:\n            diff = last_departure_hour\n            arrival_hour = arrival.hour + diff\n            departure_hour = departure.hour + diff\n            last_departure_hour = departure.hour + diff\n        else:\n            arrival_hour = arrival.hour\n            departure_hour = departure.hour\n            last_departure_hour = departure.hour\n\n        # Cover the case when adding waiting time to the arrival time\n        # falls into the next day\n        if departure.hour < arrival.hour:\n            diff = last_departure_hour\n            departure_hour = departure.hour + diff\n            last_departure_hour = departure.hour + diff\n\n        yield {'trip_id': trip_id,\n               'arrival_time': '{:02}:{}'.format(\n                   arrival_hour,\n                   arrival.strftime('%M:%S')),\n               'departure_time': '{:02}:{}'.format(\n                   departure_hour,\n                   departure.strftime('%M:%S')),\n               'stop_id': stop.stop_id,\n               'stop_sequence': stop_sequence}\n\n        last_stop = stop\n        last_departure = departure"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_zipped(self, filepath):\n        with zipfile.ZipFile(filepath, mode='w', compression=zipfile.ZIP_DEFLATED) as zfile:\n            for name, buffer in self._buffers.items():\n                encoded_values = io.BytesIO(buffer.getvalue().encode('utf-8'))\n                zfile.writestr('{}.txt'.format(name),\n                               encoded_values.getbuffer())\n            for name, path in self._files.items():\n                zfile.write(path, arcname=name)", "response": "Write the GTFS feed in the given file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites GTFS text files in the given path.", "response": "def write_unzipped(self, destination):\n        \"\"\"Write GTFS text files in the given path.\"\"\"\n        for name, buffer in self._buffers.items():\n            with open(os.path.join(destination,\n                                   '{}.txt'.format(name)),\n                      'w', encoding='utf-8') as file:\n                file.write(buffer.getvalue())\n        for name, path in self._files.items():\n            shutil.copy(path, os.path.join(destination, name))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef extract_stops(relation, nodes, visited_stop_ids, stop_to_station_map):\n    # member_role: stop, halt, platform, terminal, etc.\n    for member_type, member_id, member_role in relation.member_info:\n\n        if member_id not in visited_stop_ids and \\\n            member_id in nodes and\\\n                member_role in ('stop', 'halt'):\n\n            location_type = ''\n\n            visited_stop_ids.add(member_id)\n            yield Stop(\n                member_id,\n                nodes[member_id].tags.get('name') or\n                \"Unnamed {} stop.\".format(relation.tags.get('route')),\n                nodes[member_id].lon if member_id in nodes else '',\n                nodes[member_id].lat if member_id in nodes else '',\n                relation.id,\n                _map_wheelchair(nodes[member_id].tags.get('wheelchair')),\n                location_type,\n                stop_to_station_map.get(member_id, ''))", "response": "Yields a generator of Stop objects for each stop in a relation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextracting shape of one route.", "response": "def build_shape(relation, nodes, ways):\n    \"\"\"Extract shape of one route.\"\"\"\n    sequence_index = 0\n\n    for member_type, member_id, member_role in relation.member_info:\n\n        if member_id in nodes:\n            yield Shape(\n                relation.id,\n                nodes[member_id].lat,\n                nodes[member_id].lon,\n                sequence_index)\n\n            sequence_index += 1\n\n        # Do we need to consider ways too? It dramatically increases the number of shapes.\n        elif member_id in ways:\n            continue\n        #     for point in ways[member_id].points:\n        #         shape = Shape(\n        #             relation.id,\n        #             point.lat,\n        #             point.lon,\n        #             sequence_index)\n        #         sequence_index += 1\n\n        else:\n            # Ignore excessive logging for now.\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_supported_versions(self):\n        if not hasattr(self, '_versions'):\n            try:\n                self._versions = [self.send_apdu(INS_GET_VERSION).decode()]\n            except exc.APDUError as e:\n                # v0 didn't support the instruction.\n                self._versions = ['v0'] if e.code == 0x6d00 else []\n\n        return self._versions", "response": "Gets a list of supported U2F versions from the device."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef send_apdu(self, ins, p1=0, p2=0, data=b''):\n        if data is None:\n            data = b''\n        elif isinstance(data, int):\n            data = int2byte(data)\n\n        size = len(data)\n        l0 = size >> 16 & 0xff\n        l1 = size >> 8 & 0xff\n        l2 = size & 0xff\n        apdu_data = struct.pack('B B B B B B B %is B B' % size,\n                                0, ins, p1, p2, l0, l1, l2, data, 0x00, 0x00)\n        try:\n            resp = self._do_send_apdu(apdu_data)\n        except Exception as e:\n            # TODO Use six.reraise if/when Six becomes an agreed dependency.\n            raise exc.DeviceError(e)\n        status = struct.unpack('>H', resp[-2:])[0]\n        data = resp[:-2]\n        if status != APDU_OK:\n            raise exc.APDUError(status)\n        return data", "response": "Send an APDU to the device and waits for a response."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nregister a U2F device", "response": "def register(device, data, facet):\n    \"\"\"\n    Register a U2F device\n\n    data = {\n        \"version\": \"U2F_V2\",\n        \"challenge\": string, //b64 encoded challenge\n        \"appId\": string, //app_id\n    }\n\n    \"\"\"\n\n    if isinstance(data, string_types):\n        data = json.loads(data)\n\n    if data['version'] != VERSION:\n        raise ValueError('Unsupported U2F version: %s' % data['version'])\n\n    app_id = data.get('appId', facet)\n    verify_facet(app_id, facet)\n    app_param = sha256(app_id.encode('utf8')).digest()\n\n    client_data = {\n        'typ': 'navigator.id.finishEnrollment',\n        'challenge': data['challenge'],\n        'origin': facet\n    }\n    client_data = json.dumps(client_data)\n    client_param = sha256(client_data.encode('utf8')).digest()\n\n    request = client_param + app_param\n\n    p1 = 0x03\n    p2 = 0\n    response = device.send_apdu(INS_ENROLL, p1, p2, request)\n\n    return {\n        'registrationData': websafe_encode(response),\n        'clientData': websafe_encode(client_data)\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nauthenticates the user with the specified facet.", "response": "def authenticate(device, data, facet, check_only=False):\n    \"\"\"\n    Signs an authentication challenge\n\n    data = {\n        'version': \"U2F_V2\",\n        'challenge': websafe_encode(self.challenge),\n        'appId': self.binding.app_id,\n        'keyHandle': websafe_encode(self.binding.key_handle)\n    }\n\n    \"\"\"\n\n    if isinstance(data, string_types):\n        data = json.loads(data)\n\n    if data['version'] != VERSION:\n        raise ValueError('Unsupported U2F version: %s' % data['version'])\n\n    app_id = data.get('appId', facet)\n    verify_facet(app_id, facet)\n    app_param = sha256(app_id.encode('utf8')).digest()\n\n    key_handle = websafe_decode(data['keyHandle'])\n\n    # Client data\n    client_data = {\n        'typ': 'navigator.id.getAssertion',\n        'challenge': data['challenge'],\n        'origin': facet\n    }\n    client_data = json.dumps(client_data)\n    client_param = sha256(client_data.encode('utf8')).digest()\n\n    request = client_param + app_param + int2byte(\n        len(key_handle)) + key_handle\n\n    p1 = 0x07 if check_only else 0x03\n    p2 = 0\n    response = device.send_apdu(INS_SIGN, p1, p2, request)\n\n    return {\n        'clientData': websafe_encode(client_data),\n        'signatureData': websafe_encode(response),\n        'keyHandle': data['keyHandle']\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef register(devices, params, facet):\n    for device in devices[:]:\n        try:\n            device.open()\n        except:\n            devices.remove(device)\n\n    sys.stderr.write('\\nTouch the U2F device you wish to register...\\n')\n    try:\n        while devices:\n            removed = []\n            for device in devices:\n                try:\n                    return u2f.register(device, params, facet)\n                except exc.APDUError as e:\n                    if e.code == APDU_USE_NOT_SATISFIED:\n                        pass\n                    else:\n                        removed.append(device)\n                except exc.DeviceError:\n                    removed.append(device)\n            devices = [d for d in devices if d not in removed]\n            for d in removed:\n                d.close()\n            time.sleep(0.25)\n    finally:\n        for device in devices:\n            device.close()\n    sys.stderr.write('\\nUnable to register with any U2F device.\\n')\n    sys.exit(1)", "response": "Interactively registers a single U2F device."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwraps a function with reporting to errors backend", "response": "def wrap_function(func=None, error_threshold=None, reraise_exception=True, save_current_stack_trace=True):\n    ''' Wraps a function with reporting to errors backend '''\n    # This if/else allows wrap_function to behave like a normal decorator when\n    # used like:\n    #         @wrap_function\n    #         def some_func():\n    #\n    # However, it also allows wrap_function to also be passed keyword arguments\n    # like the following:\n    #         @wrap_function(error_threshold=3, reraise_exception=False)\n    #         def some_func():\n    if func:\n        return flawless.client.client._wrap_function_with_error_decorator(\n            func=func,\n            error_threshold=error_threshold,\n            reraise_exception=reraise_exception,\n            save_current_stack_trace=save_current_stack_trace)\n    else:\n        return functools.partial(flawless.client.client._wrap_function_with_error_decorator,\n                                 error_threshold=error_threshold,\n                                 reraise_exception=reraise_exception,\n                                 save_current_stack_trace=save_current_stack_trace)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwraps a class with reporting to errors backend by decorating each function of the class.", "response": "def wrap_class(cls, error_threshold=None):\n    ''' Wraps a class with reporting to errors backend by decorating each function of the class.\n            Decorators are injected under the classmethod decorator if they exist.\n    '''\n    methods = inspect.getmembers(cls, inspect.ismethod) + inspect.getmembers(cls, inspect.isfunction)\n    for method_name, method in methods:\n        wrapped_method = flawless.client.client._wrap_function_with_error_decorator(\n            method if not im_self(method) else im_func(method),\n            save_current_stack_trace=False,\n            error_threshold=error_threshold,\n        )\n        if im_self(method):\n            wrapped_method = classmethod(wrapped_method)\n        setattr(cls, method_name, wrapped_method)\n    return cls"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _blame_line(self, traceback):\n        '''Figures out which line in traceback is to blame for the error.\n        Returns a 3-tuple of (ErrorKey, StackTraceEntry, [email recipients])'''\n        key = None\n        blamed_entry = None\n        email_recipients = []\n        for stack_line in traceback:\n            line_type = self._get_line_type(stack_line)\n            if line_type == api_ttypes.LineType.THIRDPARTY_WHITELIST:\n                return None, None, None, True\n            elif line_type in [api_ttypes.LineType.DEFAULT, api_ttypes.LineType.KNOWN_ERROR]:\n                filepath = self._get_basepath(stack_line.filename)\n                entry = api_ttypes.CodeIdentifier(filepath, stack_line.function_name, stack_line.text)\n                blamed_entry = entry\n                key = api_ttypes.ErrorKey(filepath, stack_line.line_number, stack_line.function_name, stack_line.text)\n                if filepath in self.watch_all_errors:\n                    email_recipients.extend(self.watch_all_errors[filepath])\n        return (key, blamed_entry, email_recipients, False)", "response": "Figures out which line in traceback is to blame for the error. Returns a 3 - tuple of ( ErrorKey StackTraceEntry email_recipients False )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _matches_filepath_pattern(self, filepath):\n        '''Given a filepath, and a list of regex patterns, this function returns true\n        if filepath matches any one of those patterns'''\n        if not self.only_blame_patterns:\n            return True\n\n        for pattern in self.only_blame_patterns:\n            if pattern.match(filepath):\n                return True\n        return False", "response": "Given a filepath and a list of regex patterns this function returns true\n        if filepath matches any of those patterns"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives an email address check the email_remapping table to see if the email address should be sent to a different address.", "response": "def _get_email(self, email):\n        '''Given an email address, check the email_remapping table to see if the email\n        should be sent to a different address. This function also handles overriding\n        the email domain if ignore_vcs_email_domain is set or the domain was missing'''\n        if not email or \"@\" not in email:\n            return None\n\n        if email in self.email_remapping.remap:\n            return self.email_remapping.remap[email]\n        prefix, domain = email.split(\"@\", 2)\n        if prefix in self.email_remapping.remap:\n            return self.email_remapping.remap[prefix]\n        if \".\" not in domain or config.ignore_vcs_email_domain:\n            return \"%s@%s\" % (prefix, config.email_domain_name)\n        return email"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_entry(self, entry, entry_tree):\n        '''Helper function for retrieving a particular entry from the prefix trees'''\n        for e in entry_tree[entry.filename]:\n            if entry == e:\n                return e", "response": "Helper function for retrieving a particular entry from the prefix trees"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef serve(conf_path, storage_factory=None):\n\n    flawless.lib.config.init_config(conf_path)\n    # Try and create datadir if it doesn't exist. For instance it might be in /tmp\n    if not os.path.exists(config.data_dir_path):\n        os.makedirs(config.data_dir_path)\n\n    storage_factory = storage_factory or (lambda partition: DiskStorage(partition=partition))\n\n    # Setup root logger\n    root_logger = logging.getLogger()\n    root_handler = logging.handlers.TimedRotatingFileHandler(\n        filename=config.log_file, when='d', interval=1, backupCount=config.log_days_to_keep)\n    root_logger.setLevel(getattr(logging, config.log_level))\n    root_logger.addHandler(root_handler)\n\n    child_pid = os.fork()\n    if child_pid == 0:\n        # Setup HTTP server\n        handler = FlawlessWebServiceHandler(storage_factory=storage_factory)\n        server = SimpleThreadedHTTPServer(('', config.http_port), SimpleRequestHTTPHandler)\n        server.attach_service(handler)\n        server.request_queue_size = 50\n\n        try:\n            server.serve_forever()\n        except (KeyboardInterrupt, SystemExit):\n            server.server_close()\n    else:\n        # Setup Thrift server\n        handler = FlawlessThriftServiceHandler(storage_factory=storage_factory)\n        processor = Flawless.Processor(handler)\n        transport = TSocket.TServerSocket(port=config.port)\n        tfactory = TTransport.TFramedTransportFactory()\n        pfactory = TBinaryProtocol.TBinaryProtocolFactory()\n        server = TServer.TThreadedServer(processor, transport, tfactory, pfactory)\n        try:\n            server.serve()\n        except (KeyboardInterrupt, SystemExit):\n            handler.errors_seen.sync()\n            transport.close()\n            os.kill(child_pid, signal.SIGINT)", "response": "This method starts the Flawless server."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef record_error(hostname, exc_info, preceding_stack=None, error_threshold=None, additional_info=None):\n    ''' Helper function to record errors to the flawless backend '''\n    stack = []\n    exc_type, exc_value, sys_traceback = exc_info\n\n    while sys_traceback is not None:\n        stack.append(sys_traceback)\n        sys_traceback = sys_traceback.tb_next\n\n    stack_lines = []\n    for row in preceding_stack or []:\n        stack_lines.append(\n            api_ttypes.StackLine(filename=os.path.abspath(row[0]), line_number=row[1],\n                                 function_name=row[2], text=row[3])\n        )\n\n    for index, tb in enumerate(stack):\n        filename = tb.tb_frame.f_code.co_filename\n        func_name = tb.tb_frame.f_code.co_name\n        lineno = tb.tb_lineno\n        line = linecache.getline(filename, lineno, tb.tb_frame.f_globals)\n        frame_locals = None\n        if index >= (len(stack) - NUM_FRAMES_TO_SAVE):\n            # Include some limits on max string length & number of variables to keep things from getting\n            # out of hand\n            frame_locals = dict((k, _myrepr(k, v)) for k, v in\n                                list(tb.tb_frame.f_locals.items())[:MAX_LOCALS] if k != \"self\")\n            if \"self\" in tb.tb_frame.f_locals and hasattr(tb.tb_frame.f_locals[\"self\"], \"__dict__\"):\n                frame_locals.update(dict((\"self.\" + k, _myrepr(k, v)) for k, v in\n                                         list(tb.tb_frame.f_locals[\"self\"].__dict__.items())[:MAX_LOCALS]\n                                         if k != \"self\"))\n\n        stack_lines.append(\n            api_ttypes.StackLine(filename=os.path.abspath(filename), line_number=lineno,\n                                 function_name=func_name, text=line, frame_locals=frame_locals)\n        )\n\n    # Check LRU cache & potentially do not send error report if this client has already reported this error\n    # several times.\n    key = CachedErrorInfo.get_hash_key(stack_lines)\n    info = ERROR_CACHE.get(key) or CachedErrorInfo()\n    info.increment()\n    ERROR_CACHE[key] = info\n    if info.should_report():\n        error_count = info.mark_reported()\n        _send_request(\n            api_ttypes.RecordErrorRequest(\n                traceback=stack_lines,\n                exception_message=repr(exc_value),\n                exception_type=exc_type.__module__ + \".\" + exc_type.__name__,\n                hostname=hostname,\n                error_threshold=error_threshold,\n                additional_info=additional_info,\n                error_count=error_count,\n            )\n        )", "response": "Helper function to record errors in the flawless backend."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef migrate_thrift_obj(self, obj):\n        if not hasattr(obj, \"thrift_spec\"):\n            return\n\n        obj_key_set = set(obj.__dict__.keys())\n        thrift_field_map = {t[2]: t[4] for t in obj.thrift_spec if t}\n        obj.__dict__.update({f: copy.copy(thrift_field_map[f]) for f in set(thrift_field_map.keys()) - obj_key_set})\n        for value in obj.__dict__.values():\n            self.migrate_thrift_obj(value)", "response": "Helper function that can be called when serializing or deserializing thrift objects whose definitions\n        have changed we need to update the attributes of the object to their default value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfetching an image from url and convert it into a Pillow Image object", "response": "def url_to_image(url):\n    \"\"\"\n    Fetch an image from url and convert it into a Pillow Image object\n    \"\"\"\n    r = requests.get(url)\n    image = StringIO(r.content)\n    return image"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a string datas into a Pillow Image object", "response": "def string_to_image(image_string):\n    \"\"\"\n    Convert string datas into a Pillow Image object\n    \"\"\"\n    image_filelike = StringIO(image_string)\n    image = Image.open(image_filelike)\n    return image"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef validate(validator):\n\n    def decorator(func):\n        \"\"\"Bound decorator to a particular validator function\"\"\"\n\n        @wraps(func)\n        def wrapper(image, size, validate=True):\n            if validate:\n                validator(image, size)\n            return func(image, size)\n        return wrapper\n\n    return decorator", "response": "Returns a decorator that validates arguments with a given validator function."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _is_big_enough(image, size):\n    if (size[0] > image.size[0]) and (size[1] > image.size[1]):\n        raise ImageSizeError(image.size, size)", "response": "Check that the image s size is big enough to the given size."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _width_is_big_enough(image, width):\n    if width > image.size[0]:\n        raise ImageSizeError(image.size[0], width)", "response": "Check that the image width is superior to width"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _height_is_big_enough(image, height):\n    if height > image.size[1]:\n        raise ImageSizeError(image.size[1], height)", "response": "Check that the height is superior to the image height."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncrop the image with a centered rectangle of the specified size image: a Pillow image instance size: a list of two integers [width, height]", "response": "def resize_crop(image, size):\n    \"\"\"\n    Crop the image with a centered rectangle of the specified size\n    image:      a Pillow image instance\n    size:       a list of two integers [width, height]\n    \"\"\"\n    img_format = image.format\n    image = image.copy()\n    old_size = image.size\n    left = (old_size[0] - size[0]) / 2\n    top = (old_size[1] - size[1]) / 2\n    right = old_size[0] - left\n    bottom = old_size[1] - top\n    rect = [int(math.ceil(x)) for x in (left, top, right, bottom)]\n    left, top, right, bottom = rect\n    crop = image.crop((left, top, right, bottom))\n    crop.format = img_format\n    return crop"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef resize_height(image, size, resample=Image.LANCZOS):\n    try:\n        height = size[1]\n    except:\n        height = size\n    img_format = image.format\n    img = image.copy()\n    img_size = img.size\n    # If the origial image has already the good height, return it\n    # fix issue #16\n    if img_size[1] == height:\n        return image\n    new_width = int(math.ceil((height / img_size[1]) * img_size[0]))\n    img.thumbnail((new_width, height), resample)\n    img.format = img_format\n    return img", "response": "Resize an image to a new height."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef resize_thumbnail(image, size, resample=Image.LANCZOS):\n\n    img_format = image.format\n    img = image.copy()\n    img.thumbnail((size[0], size[1]), resample)\n    img.format = img_format\n    return img", "response": "Resize image according to size."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting the text category to a tasks. Category instance.", "response": "def parse_category(self, item, field_name, source_name):\n        \"\"\"\n        Converts the text category to a tasks.Category instance.\n        \"\"\"\n        # Get and checks for the corresponding slug\n        slug = category_map.get(self.get_value(item, source_name), None)\n        if not slug:\n            return None\n        # Load the category instance\n        try:\n            return Category.objects.get(slug=slug)\n        except Category.DoesNotExist:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_date(self, item, field_name, source_name):\n        # Get the current date\n        now = datetime.now().date()\n        # Get the date from the source\n        val = self.get_value(item, source_name)\n        week_day, day = val.split()\n        day = int(day)\n        # If the current date is minor than the item date\n        # go back one month\n        if now.day < day:\n            if now.month == 1:\n                now = now.replace(month=12, year=now.year-1)\n            else:\n                now = now.replace(month=now.month-1)\n        # Finally, replace the source day in the current date\n        # and return\n        now = now.replace(day=day)\n        return now", "response": "Converts the date in the format Thu 03."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\niterating over the items in the XML source.", "response": "def get_items(self):\n        \"\"\"\n        Iterator of the list of items in the XML source.\n        \"\"\"\n        # Use `iterparse`, it's more efficient, specially for big files\n        for event, item in ElementTree.iterparse(self.source):\n            if item.tag == self.item_tag_name:\n                yield item\n                # Releases the item from memory\n                item.clear()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef save_error(self, data, exception_info):\n        # TODO: what to do with errors? Let it flow? Write to a log file?\n        self.errors.append({'data': data,\n                            'exception': ''.join(format_exception(*exception_info)),\n                            })", "response": "Save an error in the error list."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse all data from the source into a dict containing the fields from the source and saves the model instances.", "response": "def parse(self):\n        \"\"\"\n        Parses all data from the source, saving model instances.\n        \"\"\"\n        # Checks if the source is loaded\n        if not self.loaded:\n            self.load(self.source)\n        \n        for item in self.get_items():\n            # Parse the fields from the source into a dict\n            data = self.parse_item(item)\n            # Get the instance from the DB, or a new one\n            instance = self.get_instance(data)\n            # Feed instance with data\n            self.feed_instance(data, instance)\n            # Try to save the instance or keep the error\n            try:\n                self.save_item(item, data, instance)\n            except Exception as e:\n                self.save_error(data, sys.exc_info())\n\n        # Unload the source\n        self.unload()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_item(self, item):\n        # Create a dictionary from values for each field\n        parsed_data = {}\n        \n        for field_name in self.fields:\n            # A field-name may be mapped to another identifier on the source,\n            # it could be a XML path or a CSV column name / position.\n            # Defaults to the field-name itself.\n            source_name = self.field_map.get(field_name, field_name)\n            \n            # Uses a custom method \"parse_%(field_name)\"\n            # or get the value from the item\n            parse = getattr(self, 'parse_%s' % field_name, None)\n            if parse:\n                value = parse(item, field_name, source_name)\n            else:\n                value = self.get_value(item, source_name)\n                \n            # Add the value to the parsed data\n            parsed_data[field_name] = value\n        return parsed_data", "response": "Receives an item and returns a dictionary of field values."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget an item from the database or an empty one.", "response": "def get_instance(self, data):\n        \"\"\"\n        Get an item from the database or an empty one if not found.\n        \"\"\"\n        # Get unique fields\n        unique_fields = self.unique_fields\n        \n        # If there are no unique fields option, all items are new\n        if not unique_fields:\n            return self.model()\n        \n        # Build the filter\n        filter = dict([(f, data[f]) for f in unique_fields])\n        \n        # Get the instance from the DB or use a new instance\n        try:\n            instance = self.model._default_manager.get(**filter)\n        except self.model.DoesNotExist:\n            return self.model()\n        \n        return instance"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfeed a model instance using parsed data.", "response": "def feed_instance(self, data, instance):\n        \"\"\"\n        Feeds a model instance using parsed data (usually from `parse_item`).\n        \"\"\"\n        for prop, val in data.items():\n            setattr(instance, prop, val)\n        return instance"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save_item(self, item, data, instance, commit=True):\n        if commit:\n            instance.save()\n        return instance", "response": "Save a model instance to the database."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef download_file(url, dest):\n    \n    # Create the HTTP request\n    request = urllib2.Request(url)\n    \n    # Add the header to accept gzip encoding\n    request.add_header('Accept-encoding', 'gzip')\n    \n    # Open the request\n    opener = urllib2.build_opener()\n    response = opener.open(request)\n    \n    # Retrieve data\n    data = response.read()\n    \n    # If the data is compressed, put the data in a stream and decompress\n    if response.headers.get('content-encoding', '') == 'gzip':\n        stream = StringIO.StringIO(data)\n        gzipper = gzip.GzipFile(fileobj=stream)\n        data = gzipper.read()\n    \n    # Write to a file\n    f = open(dest, 'wb')\n    f.write(data)\n    f.close()", "response": "Downloads a file from url and saves it to dest."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load(self, source):\n        self.source = open(self.source, 'rb')    \n        self.loaded = True", "response": "Load the source file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\niterates over the rows of the CSV file and yield a dictionary mapping column names to the values of the rows.", "response": "def get_items(self):\n        \"\"\"\n        Iterator to read the rows of the CSV file.\n        \"\"\"\n        # Get the csv reader\n        reader = csv.reader(self.source)\n        # Get the headers from the first line\n        headers = reader.next()\n        # Read each line yielding a dictionary mapping\n        # the column headers to the row values\n        for row in reader:\n            # Skip empty rows\n            if not row:\n                continue\n            yield dict(zip(headers, row))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns value of variable set in the package where said variable is named in the Python meta format __<meta_name__.", "response": "def get_package_meta(meta_name):\n    \"\"\"Return value of variable set in the package where said variable is\n    named in the Python meta format `__<meta_name>__`.\n    \"\"\"\n    regex = \"__{0}__ = ['\\\"]([^'\\\"]+)['\\\"]\".format(meta_name)\n    return re.search(regex, package_file).group(1)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the allow_network_access flag.", "response": "def allow_network_access(self, value: bool):\n        \"\"\"\n        Raises ValueError if this sandbox instance is currently running.\n        \"\"\"\n        if self._is_running:\n            raise ValueError(\n                \"Cannot change network access settings on a running sandbox\")\n\n        self._allow_network_access = value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning a command inside the sandbox and returns the results.", "response": "def run_command(self,\n                    args: List[str],\n                    max_num_processes: int=None,\n                    max_stack_size: int=None,\n                    max_virtual_memory: int=None,\n                    as_root: bool=False,\n                    stdin: FileIO=None,\n                    timeout: int=None,\n                    check: bool=False,\n                    truncate_stdout: int=None,\n                    truncate_stderr: int=None) -> 'CompletedCommand':\n        \"\"\"\n        Runs a command inside the sandbox and returns the results.\n\n        :param args: A list of strings that specify which command should\n            be run inside the sandbox.\n\n        :param max_num_processes: The maximum number of processes the\n            command is allowed to spawn.\n\n        :param max_stack_size: The maximum stack size, in bytes, allowed\n            for the command.\n\n        :param max_virtual_memory: The maximum amount of memory, in\n            bytes, allowed for the command.\n\n        :param as_root: Whether to run the command as a root user.\n\n        :param stdin: A file object to be redirected as input to the\n            command's stdin. If this is None, /dev/null is sent to the\n            command's stdin.\n\n        :param timeout: The time limit for the command.\n\n        :param check: Causes CalledProcessError to be raised if the\n            command exits nonzero or times out.\n\n        :param truncate_stdout: When not None, stdout from the command\n            will be truncated after this many bytes.\n\n        :param truncate_stderr: When not None, stderr from the command\n            will be truncated after this many bytes.\n        \"\"\"\n        cmd = ['docker', 'exec', '-i', self.name, 'cmd_runner.py']\n\n        if stdin is None:\n            cmd.append('--stdin_devnull')\n\n        if max_num_processes is not None:\n            cmd += ['--max_num_processes', str(max_num_processes)]\n\n        if max_stack_size is not None:\n            cmd += ['--max_stack_size', str(max_stack_size)]\n\n        if max_virtual_memory is not None:\n            cmd += ['--max_virtual_memory', str(max_virtual_memory)]\n\n        if timeout is not None:\n            cmd += ['--timeout', str(timeout)]\n\n        if truncate_stdout is not None:\n            cmd += ['--truncate_stdout', str(truncate_stdout)]\n\n        if truncate_stderr is not None:\n            cmd += ['--truncate_stderr', str(truncate_stderr)]\n\n        if not as_root:\n            cmd += ['--linux_user_id', str(self._linux_uid)]\n\n        cmd += args\n\n        if self.debug:\n            print('running: {}'.format(cmd), flush=True)\n\n        with tempfile.TemporaryFile() as f:\n            try:\n                subprocess.run(cmd, stdin=stdin, stdout=f, stderr=subprocess.PIPE, check=True)\n                f.seek(0)\n                json_len = int(f.readline().decode().rstrip())\n                results_json = json.loads(f.read(json_len).decode())\n\n                stdout_len = int(f.readline().decode().rstrip())\n                stdout = tempfile.NamedTemporaryFile()\n                stdout.write(f.read(stdout_len))\n                stdout.seek(0)\n\n                stderr_len = int(f.readline().decode().rstrip())\n                stderr = tempfile.NamedTemporaryFile()\n                stderr.write(f.read(stderr_len))\n                stderr.seek(0)\n\n                result = CompletedCommand(return_code=results_json['return_code'],\n                                          timed_out=results_json['timed_out'],\n                                          stdout=stdout,\n                                          stderr=stderr,\n                                          stdout_truncated=results_json['stdout_truncated'],\n                                          stderr_truncated=results_json['stderr_truncated'])\n\n                if (result.return_code != 0 or results_json['timed_out']) and check:\n                    raise subprocess.CalledProcessError(\n                        result.return_code, cmd,\n                        output=result.stdout, stderr=result.stderr)\n\n                return result\n            except subprocess.CalledProcessError as e:\n                f.seek(0)\n                print(f.read())\n                print(e.stderr)\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_files(self, *filenames: str, owner: str=SANDBOX_USERNAME, read_only: bool=False):\n        if owner != SANDBOX_USERNAME and owner != 'root':\n            raise ValueError('Invalid value for parameter \"owner\": {}'.format(owner))\n\n        with tempfile.TemporaryFile() as f, \\\n                tarfile.TarFile(fileobj=f, mode='w') as tar_file:\n            for filename in filenames:\n                tar_file.add(filename, arcname=os.path.basename(filename))\n\n            f.seek(0)\n            subprocess.check_call(\n                ['docker', 'cp', '-',\n                 self.name + ':' + SANDBOX_WORKING_DIR_NAME],\n                stdin=f)\n\n            file_basenames = [os.path.basename(filename) for filename in filenames]\n            if owner == SANDBOX_USERNAME:\n                self._chown_files(file_basenames)\n\n            if read_only:\n                chmod_cmd = ['chmod', '444'] + file_basenames\n                self.run_command(chmod_cmd, as_root=True)", "response": "Adds the specified files to the current working directory of the current instance of the class."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_and_rename_file(self, filename: str, new_filename: str) -> None:\n        dest = os.path.join(\n            self.name + ':' + SANDBOX_WORKING_DIR_NAME,\n            new_filename)\n        subprocess.check_call(['docker', 'cp', filename, dest])\n        self._chown_files([new_filename])", "response": "Adds the specified file to the sandbox and renames it to new_filename."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of all enrollments for the passed course_id.", "response": "def get_enrollments_for_course(self, course_id, params={}):\n        \"\"\"\n        Return a list of all enrollments for the passed course_id.\n\n        https://canvas.instructure.com/doc/api/enrollments.html#method.enrollments_api.index\n        \"\"\"\n        url = COURSES_API.format(course_id) + \"/enrollments\"\n\n        enrollments = []\n        for datum in self._get_paged_resource(url, params=params):\n            enrollments.append(CanvasEnrollment(data=datum))\n\n        return enrollments"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_enrollments_for_course_by_sis_id(self, sis_course_id, params={}):\n        return self.get_enrollments_for_course(\n            self._sis_id(sis_course_id, sis_field=\"course\"), params)", "response": "Get all enrollments for the passed course sis id."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_enrollments_for_section(self, section_id, params={}):\n        url = SECTIONS_API.format(section_id) + \"/enrollments\"\n\n        enrollments = []\n        for datum in self._get_paged_resource(url, params=params):\n            enrollments.append(CanvasEnrollment(data=datum))\n\n        return enrollments", "response": "Returns a list of all enrollments for the passed section_id."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets all enrollments for a given sis id.", "response": "def get_enrollments_for_section_by_sis_id(self, sis_section_id, params={}):\n        \"\"\"\n        Return a list of all enrollments for the passed section sis id.\n        \"\"\"\n        return self.get_enrollments_for_section(\n            self._sis_id(sis_section_id, sis_field=\"section\"), params)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_enrollments_for_regid(self, regid, params={},\n                                  include_courses=True):\n        \"\"\"\n        Return a list of enrollments for the passed user regid.\n\n        https://canvas.instructure.com/doc/api/enrollments.html#method.enrollments_api.index\n        \"\"\"\n        sis_user_id = self._sis_id(regid, sis_field=\"user\")\n        url = USERS_API.format(sis_user_id) + \"/enrollments\"\n\n        courses = Courses() if include_courses else None\n\n        enrollments = []\n        for datum in self._get_paged_resource(url, params=params):\n            enrollment = CanvasEnrollment(data=datum)\n            if include_courses:\n                course_id = datum[\"course_id\"]\n                course = courses.get_course(course_id)\n\n                if course.sis_course_id is not None:\n                    enrollment.course = course\n                    # the following 3 lines are not removed\n                    # to be backward compatible.\n                    enrollment.course_url = course.course_url\n                    enrollment.course_name = course.name\n                    enrollment.sis_course_id = course.sis_course_id\n            else:\n                enrollment.course_url = re.sub(\n                    r'/users/\\d+$', '', enrollment.html_url)\n\n            enrollments.append(enrollment)\n        return enrollments", "response": "Get a list of CanvasEnrollment objects for the passed user regid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef enroll_user(self, course_id, user_id, enrollment_type, params=None):\n        url = COURSES_API.format(course_id) + \"/enrollments\"\n\n        if not params:\n            params = {}\n\n        params[\"user_id\"] = user_id\n        params[\"type\"] = enrollment_type\n\n        data = self._post_resource(url, {\"enrollment\": params})\n        return CanvasEnrollment(data=data)", "response": "Enroll a user into a course."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef capacity_vesics_1975(sl, fd, h_l=0, h_b=0, vertical_load=1, slope=0, base_tilt=0, verbose=0, gwl=1e6, **kwargs):\n\n    if not kwargs.get(\"disable_requires\", False):\n        models.check_required(sl, [\"phi_r\", \"cohesion\", \"unit_dry_weight\"])\n        models.check_required(fd, [\"length\", \"width\", \"depth\"])\n\n    area_foundation = fd.length * fd.width\n    c_a = 0.6 - 1.0 * sl.cohesion\n\n    horizontal_load = np.sqrt(h_l ** 2 + h_b ** 2)\n\n    fd.nq_factor = ((np.tan(np.pi / 4 + sl.phi_r / 2)) ** 2 * np.exp(np.pi * np.tan(sl.phi_r)))\n    if sl.phi_r == 0:\n        fd.nc_factor = 5.14\n    else:\n        fd.nc_factor = (fd.nq_factor - 1) / np.tan(sl.phi_r)\n    fd.ng_factor = 2.0 * (fd.nq_factor + 1) * np.tan(sl.phi_r)\n\n    # shape factors:\n    s_c = 1.0 + fd.nq_factor / fd.nc_factor * fd.width / fd.length\n    s_q = 1 + fd.width / fd.length * np.tan(sl.phi_r)\n    s_g = max(1.0 - 0.4 * fd.width / fd.length, 0.6)  # add limit of 0.6 based on Vesic\n    # depth factors:\n    if fd.depth / fd.width > 1:\n        k = np.arctan(fd.depth / fd.width)\n    else:\n        k = fd.depth / fd.width\n    d_c = 1 + 0.4 * k\n    d_q = 1 + 2 * np.tan(sl.phi_r) * (1 - np.sin(sl.phi_r)) ** 2 * k\n    d_g = 1.0\n\n    # load inclination factors\n    m__b = (2.0 + fd.width / fd.length) / (1 + fd.width / fd.length)\n    m_l = (2.0 + fd.length / fd.width) / (1 + fd.length / fd.width)\n    m = np.sqrt(m__b ** 2 + m_l ** 2)\n\n    if sl.phi_r == 0:\n        i_q = 1.0\n        i_g = 1.0\n    else:\n        i_q = (1.0 - horizontal_load / (vertical_load + area_foundation *\n                                        c_a / np.tan(sl.phi_r))) ** m\n        i_g = (1.0 - horizontal_load / (vertical_load + area_foundation *\n                                        c_a / np.tan(sl.phi_r))) ** (m + 1)\n    i_c = i_q - (1 - i_q) / (fd.nq_factor - 1)\n    check_i_c = 1 - m * horizontal_load / (area_foundation * c_a * fd.nc_factor)\n    if abs(check_i_c - i_c) / i_c > 0.001:\n        raise DesignError\n\n    # ground slope factors:\n    if sl.phi_r == 0:\n        # g_c = slope / 5.14\n        g_c = i_q\n    else:\n        g_c = i_q - (1 - i_q) / (5.14 * np.tan(sl.phi_r))\n    g_q = (1.0 - np.tan(slope)) ** 2\n    g_g = g_q\n\n    # tilted base factors\n    if sl.phi_r == 0:\n        b_c = g_c\n    else:\n        b_c = 1 - 2 * base_tilt / (5.14 * np.tan(sl.phi_r))\n    b_q = (1.0 - base_tilt * np.tan(sl.phi_r)) ** 2\n    b_g = b_q\n\n    # stress at footing base:\n\n    if gwl == 0:\n        q_d = sl.unit_eff_weight * fd.depth\n        unit_weight = sl.unit_bouy_weight\n\n    elif gwl > 0 and gwl < fd.depth:\n        q_d = (sl.unit_dry_weight * gwl) + (sl.unit_bouy_weight * (fd.depth - gwl))\n        unit_weight = sl.unit_bouy_weight\n\n    elif gwl >= fd.depth and gwl <= fd.depth + fd.width:\n        sl.average_unit_bouy_weight = sl.unit_bouy_weight + (\n            ((gwl - fd.depth) / fd.width) * (sl.unit_dry_weight - sl.unit_bouy_weight))\n        q_d = sl.unit_dry_weight * fd.depth\n        unit_weight = sl.average_unit_bouy_weight\n\n    elif gwl > fd.depth + fd.width:\n        q_d = sl.unit_dry_weight * fd.depth\n        unit_weight = sl.unit_dry_weight\n\n    if verbose:\n        log(\"Nc: \", fd.nc_factor)\n        log(\"N_qV: \", fd.nq_factor)\n        log(\"Ng: \", fd.ng_factor)\n        log(\"s_c: \", s_c)\n        log(\"s_q: \", s_q)\n        log(\"s_g: \", s_g)\n        log(\"d_c: \", d_c)\n        log(\"d_q: \", d_q)\n        log(\"d_g: \", d_g)\n        log(\"i_c: \", i_c)\n        log(\"i_q: \", i_q)\n        log(\"i_g: \", i_g)\n        log(\"g_c: \", g_c)\n        log(\"g_q: \", g_q)\n        log(\"g_g: \", g_g)\n        log(\"b_c: \", b_c)\n        log(\"b_q: \", b_q)\n        log(\"b_g: \", b_g)\n        log(\"q_d: \", q_d)\n\n    # Capacity\n    fd.q_ult = (sl.cohesion * fd.nc_factor * s_c * d_c * i_c * g_c * b_c +\n                q_d * fd.nq_factor * s_q * d_q * i_q * g_q * b_q +\n                0.5 * fd.width * unit_weight *\n                fd.ng_factor * s_g * d_g * i_g * g_g * b_g)\n\n    if verbose:\n        log(\"qult: \", fd.q_ult)\n    return fd.q_ult", "response": "Calculates the ultimate bearing stress of a foundation based on Vesics 1975."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the ultimate bearing stress of a foundation in the Terzaghi 1943 model.", "response": "def capacity_terzaghi_1943(sl, fd, round_footing=False, verbose=0, **kwargs):\n    \"\"\"\n    Calculates the foundation capacity according Terzaghi (1943)\n    Ref: http://geo.cv.nctu.edu.tw/foundation/\n    download/BearingCapacityOfFoundations.pdf\n\n    :param sl: Soil object\n    :param fd: Foundation object\n    :param round_footing: if True, then foundation is round\n    :param verbose: verbosity\n    :return: ultimate bearing stress\n    Note: the shape factor of 1.3 is used for aspect ratio > 6\n    \"\"\"\n    if not kwargs.get(\"disable_requires\", False):\n        models.check_required(sl, [\"phi_r\", \"cohesion\", \"unit_dry_weight\"])\n        models.check_required(fd, [\"length\", \"width\", \"depth\"])\n\n    a02 = ((np.exp(np.pi * (0.75 - sl.phi / 360) * np.tan(sl.phi_r))) ** 2)\n    a0_check = (np.exp((270 - sl.phi) / 180 * np.pi * np.tan(sl.phi_r)))\n    if (a02 - a0_check) / a02 > 0.001:\n        raise DesignError\n\n    fd.nq_factor = (a02 / (2 * (np.cos((45 + sl.phi / 2) * np.pi / 180)) ** 2))\n    fd.ng_factor = (2 * (fd.nq_factor + 1) * np.tan(sl.phi_r) / (1 + 0.4 * np.sin(4 * sl.phi_r)))\n    if sl.phi_r == 0:\n        fd.nc_factor = 5.7\n    else:\n        fd.nc_factor = (fd.nq_factor - 1) / np.tan(sl.phi_r)\n\n    # shape factors:\n    if round_footing:\n        s_c = 1.3\n        s_g = 0.6\n    elif fd.length / fd.width < 5:\n        s_c = 1.3\n        s_g = 0.8\n    else:\n        s_c = 1.0\n        s_g = 1.0\n    s_q = 1.0\n\n    # stress at footing base:\n    q_d = sl.unit_dry_weight * fd.depth\n\n    # Capacity\n    fd.q_ult = (sl.cohesion * fd.nc_factor * s_c + q_d * fd.nq_factor * s_q + 0.5 * fd.width *\n                sl.unit_dry_weight * fd.ng_factor * s_g)\n\n    if verbose:\n        log(\"Nc: \", fd.nc_factor)\n        log(\"Nq: \", fd.nq_factor)\n        log(\"Ng: \", fd.ng_factor)\n        log(\"s_c: \", s_c)\n        log(\"s_q: \", s_q)\n        log(\"s_g: \", s_g)\n        log(\"qult: \", fd.q_ult)\n    return fd.q_ult"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the ultimate bearing stress of a foundation in the 1970 system.", "response": "def capacity_hansen_1970(sl, fd, h_l=0, h_b=0, vertical_load=1, slope=0, base_tilt=0, verbose=0, **kwargs):\n    \"\"\"\n    Calculates the foundation capacity according Hansen (1970)\n    Ref: http://bestengineeringprojects.com/civil-projects/\n    hansens-bearing-capacity-theory/\n\n    :param sl: Soil object\n    :param fd: Foundation object\n    :param h_l: Horizontal load parallel to length\n    :param h_b: Horizontal load parallel to width\n    :param vertical_load: Vertical load\n    :param slope: ground slope\n    :param base_tilt: The slope of the underside of the foundation\n    :param verbose: verbosity\n    :return: ultimate bearing stress\n    \"\"\"\n    if not kwargs.get(\"disable_requires\", False):\n        models.check_required(sl, [\"phi_r\", \"cohesion\", \"unit_dry_weight\"])\n        models.check_required(fd, [\"length\", \"width\", \"depth\"])\n\n    area_foundation = fd.length * fd.width\n    horizontal_load = np.sqrt(h_l ** 2 + h_b ** 2)\n    c_a = 0.6 - 1.0 * sl.cohesion\n\n    fd.nq_factor = ((np.tan(np.pi / 4 + sl.phi_r / 2)) ** 2 * np.exp(np.pi * np.tan(sl.phi_r)))\n    if sl.phi_r == 0:\n        fd.nc_factor = 5.14\n    else:\n        fd.nc_factor = (fd.nq_factor - 1) / np.tan(sl.phi_r)\n    fd.ng_factor = 1.5 * (fd.nq_factor - 1) * np.tan(sl.phi_r)\n\n    # shape factors\n    if sl.phi_r == 0:\n        s_c = 0.2 * fd.width / fd.length\n    else:\n        s_c = 1.0 + fd.nq_factor / fd.nc_factor * fd.width / fd.length\n\n    s_q = 1.0 + fd.width / fd.length * np.sin(sl.phi_r)\n    s_g = 1.0 - 0.4 * fd.width / fd.length\n\n    # depth factors:\n    if fd.depth / fd.width > 1:\n        k = np.arctan(fd.depth / fd.width)\n    else:\n        k = fd.depth / fd.width\n    d_c = 1 + 0.4 * k\n    if sl.phi == 0:\n        d_c = 0.4 * k\n    d_q = 1 + 2 * np.tan(sl.phi_r) * (1 - np.sin(sl.phi_r)) ** 2 * k\n    d_g = 1.0\n\n    # incline load factors:\n    if sl.phi_r == 0:\n        i_q = 1.0\n        i_c = 0.5 - 0.5 * np.sqrt(1 - horizontal_load / area_foundation * c_a)\n        i_g = 1.0\n    else:\n        i_q = ((1.0 - 0.5 * horizontal_load /\n                (vertical_load + area_foundation * c_a / np.tan(sl.phi_r))) ** 5)\n        i_c = i_q - (1 - i_q) / (fd.nq_factor - 1)\n        i_g = ((1 - (0.7 * horizontal_load) /\n                (vertical_load + area_foundation * c_a / np.tan(sl.phi_r))) ** 5)\n\n    # slope factors:\n    if sl.phi_r == 0:\n        g_c = (slope / np.pi * 180) / 147\n    else:\n        g_c = 1.0 - (slope / np.pi * 180) / 147\n    g_q = 1 - 0.5 * np.tan(slope) ** 5\n    g_g = g_q\n\n    # base tilt factors:\n    if sl.phi_r == 0:\n        b_c = (base_tilt / np.pi * 180) / 147\n    else:\n        b_c = 1.0 - (base_tilt / np.pi * 180) / 147\n    b_q = (np.exp(-0.0349 * (base_tilt / np.pi * 180) * np.tan(sl.phi_r)))\n    b_g = (np.exp(-0.0471 * (base_tilt / np.pi * 180) * np.tan(sl.phi_r)))\n\n    if verbose:\n        log(\"Nc: \", fd.nc_factor)\n        log(\"Nq: \", fd.nq_factor)\n        log(\"Ng: \", fd.ng_factor)\n        log(\"s_c: \", s_c)\n        log(\"s_q: \", s_q)\n        log(\"s_g: \", s_g)\n        log(\"d_c: \", d_c)\n        log(\"d_q: \", d_q)\n        log(\"d_g: \", d_g)\n        log(\"i_c: \", i_c)\n        log(\"i_q: \", i_q)\n        log(\"i_g: \", i_g)\n        log(\"g_c: \", g_c)\n        log(\"g_q: \", g_q)\n        log(\"g_g: \", g_g)\n        log(\"b_c: \", b_c)\n        log(\"b_q: \", b_q)\n        log(\"b_g: \", b_g)\n\n    # stress at footing base:\n    q_d = sl.unit_dry_weight * fd.depth\n\n    # Capacity\n    if sl.phi_r == 0:\n        fd.q_ult = (sl.cohesion * fd.nc_factor *\n                    (1 + s_c + d_c - i_c - g_c - b_c) + q_d)\n    else:\n        fd.q_ult = (sl.cohesion * fd.nc_factor *\n                    s_c * d_c * i_c * g_c * b_c +\n                    q_d * fd.nq_factor * s_q * d_q * i_q * g_q * b_q +\n                    0.5 * fd.width * sl.unit_dry_weight *\n                    fd.ng_factor * s_g * d_g * i_g * g_g * b_g)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the foundation capacity according to Meyerhoff 1963", "response": "def capacity_meyerhof_1963(sl, fd, gwl=1e6, h_l=0, h_b=0, vertical_load=1, verbose=0, **kwargs):\n    \"\"\"\n    Calculates the foundation capacity according Meyerhoff (1963)\n    http://www.engs-comp.com/meyerhof/index.shtml\n\n    :param sl: Soil object\n    :param fd: Foundation object\n    :param h_l: Horizontal load parallel to length\n    :param h_b: Horizontal load parallel to width\n    :param vertical_load: Vertical load\n    :param verbose: verbosity\n    :return: ultimate bearing stress\n    \"\"\"\n    if not kwargs.get(\"disable_requires\", False):\n        models.check_required(sl, [\"phi_r\", \"cohesion\", \"unit_dry_weight\"])\n        models.check_required(fd, [\"length\", \"width\", \"depth\"])\n\n    horizontal_load = np.sqrt(h_l ** 2 + h_b ** 2)\n\n    fd.nq_factor = ((np.tan(np.pi / 4 + sl.phi_r / 2)) ** 2 *\n                    np.exp(np.pi * np.tan(sl.phi_r)))\n    if sl.phi_r == 0:\n        fd.nc_factor = 5.14\n    else:\n        fd.nc_factor = (fd.nq_factor - 1) / np.tan(sl.phi_r)\n    fd.ng_factor = (fd.nq_factor - 1) * np.tan(1.4 * sl.phi_r)\n\n    if verbose:\n        log(\"Nc: \", fd.nc_factor)\n        log(\"Nq: \", fd.nq_factor)\n        log(\"Ng: \", fd.ng_factor)\n\n    kp = (np.tan(np.pi / 4 + sl.phi_r / 2)) ** 2\n    # shape factors\n    s_c = 1 + 0.2 * kp * fd.width / fd.length\n    if sl.phi > 10:\n        s_q = 1.0 + 0.1 * kp * fd.width / fd.length\n    else:\n        s_q = 1.0\n    s_g = s_q\n\n    # depth factors\n    d_c = 1 + 0.2 * np.sqrt(kp) * fd.depth / fd.width\n    if sl.phi > 10:\n        d_q = 1 + 0.1 * np.sqrt(kp) * fd.depth / fd.width\n    else:\n        d_q = 1.0\n    d_g = d_q\n\n    # inclination factors:\n    theta_load = np.arctan(horizontal_load / vertical_load)\n    i_c = (1 - theta_load / (np.pi * 0.5)) ** 2\n    i_q = i_c\n    if sl.phi > 0:\n        i_g = (1 - theta_load / sl.phi_r) ** 2\n    else:\n        i_g = 0\n\n    # stress at footing base:\n\n    if gwl == 0:\n        q_d = sl.unit_bouy_weight * fd.depth\n        unit_weight = sl.unit_bouy_weight\n\n    elif gwl > 0 and gwl < fd.depth:\n        q_d = (sl.unit_dry_weight * gwl) + (sl.unit_bouy_weight * (fd.depth - gwl))\n        unit_weight = sl.unit_bouy_weight\n\n    elif gwl >= fd.depth and gwl <= fd.depth + fd.width:\n        sl.average_unit_bouy_weight = sl.unit_bouy_weight + (\n        ((gwl - fd.depth) / fd.width) * (sl.unit_dry_weight - sl.unit_bouy_weight))\n        q_d = sl.unit_dry_weight * fd.depth\n        unit_weight = sl.average_unit_bouy_weight\n\n    elif gwl > fd.depth + fd.width:\n        q_d = sl.unit_dry_weight * fd.depth\n        unit_weight = sl.unit_dry_weight\n\n    if verbose:\n        log(\"Nc: \", fd.nc_factor)\n        log(\"Nq: \", fd.nq_factor)\n        log(\"Ng: \", fd.ng_factor)\n        log(\"s_c: \", s_c)\n        log(\"s_q: \", s_q)\n        log(\"s_g: \", s_g)\n        log(\"d_c: \", d_c)\n        log(\"d_q: \", d_q)\n        log(\"d_g: \", d_g)\n        log(\"i_c: \", i_c)\n        log(\"i_q: \", i_q)\n        log(\"i_g: \", i_g)\n        log(\"q_d: \", q_d)\n\n    # Capacity\n    fd.q_ult = (sl.cohesion * fd.nc_factor * s_c * d_c * i_c +\n                q_d * fd.nq_factor * s_q * d_q * i_q +\n                0.5 * fd.width * unit_weight *\n                fd.ng_factor * s_g * d_g * i_g)\n    return fd.q_ult"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the capacity according to Appendix B verification method 4 of the NZ building code.", "response": "def capacity_nzs_vm4_2011(sl, fd, h_l=0, h_b=0, vertical_load=1, slope=0, verbose=0, **kwargs):\n    \"\"\"\n    calculates the capacity according to\n     Appendix B verification method 4 of the NZ building code\n\n    :param sl: Soil object\n    :param fd: Foundation object\n    :param h_l: Horizontal load parallel to length\n    :param h_b: Horizontal load parallel to width\n    :param vertical_load: Vertical load\n    :param slope: ground slope\n    :param verbose: verbosity\n    :return: ultimate bearing stress\n    \"\"\"\n    # Need to make adjustments if sand  has DR<40% or\n    # clay has liquidity indices greater than 0.7\n\n    if not kwargs.get(\"disable_requires\", False):\n        models.check_required(sl, [\"phi_r\", \"cohesion\", \"unit_dry_weight\"])\n        models.check_required(fd, [\"length\", \"width\", \"depth\"])\n\n    horizontal_load = np.sqrt(h_l ** 2 + h_b ** 2)\n\n    h_eff_b = kwargs.get(\"h_eff_b\", 0)\n    h_eff_l = kwargs.get(\"h_eff_l\", 0)\n    loc_v_l = kwargs.get(\"loc_v_l\", fd.length / 2)\n    loc_v_b = kwargs.get(\"loc_v_b\", fd.width / 2)\n\n    ecc_b = h_b * h_eff_b / vertical_load\n    ecc_l = h_l * h_eff_l / vertical_load\n\n    width_eff = min(fd.width, 2 *\n                    (loc_v_b + ecc_b), 2 * (fd.width - loc_v_b - ecc_b))\n    length_eff = min(fd.length, 2 *\n                     (loc_v_l + ecc_l), 2 * (fd.length - loc_v_l - ecc_l))\n    area_foundation = length_eff * width_eff\n\n    # check para 3.4.1\n    if width_eff / 2 < fd.width / 6:\n        raise DesignError(\"failed on eccentricity\")\n\n    # LOAD FACTORS:\n    fd.nq_factor = ((np.tan(np.pi / 4 + sl.phi_r / 2)) ** 2 * np.exp(np.pi * np.tan(sl.phi_r)))\n    if sl.phi_r == 0:\n        fd.nc_factor = 5.14\n    else:\n        fd.nc_factor = (fd.nq_factor - 1) / np.tan(sl.phi_r)\n    fd.ng_factor = 2.0 * (fd.nq_factor - 1) * np.tan(sl.phi_r)\n\n    # shape factors:\n    s_c = 1.0 + fd.nq_factor / fd.nc_factor * width_eff / length_eff\n    s_q = 1 + width_eff / length_eff * np.tan(sl.phi_r)\n    s_g = max(1.0 - 0.4 * width_eff / length_eff, 0.6)  # add limit of 0.6 based on Vesics\n\n    # depth factors:\n    if fd.depth / width_eff > 1:\n        k = np.arctan(fd.depth / width_eff)\n    else:\n        k = fd.depth / width_eff\n    if sl.phi_r == 0:\n        d_c = 1 + 0.4 * k\n        d_q = 1.0\n    else:\n        d_q = (1 + 2 * np.tan(sl.phi_r) *\n               (1 - np.sin(sl.phi_r)) ** 2 * k)\n        d_c = d_q - (1 - d_q) / (fd.nq_factor * np.tan(sl.phi_r))\n    d_g = 1.0\n\n    # load inclination factors:\n    if sl.phi_r == 0:\n        i_c = 0.5 * (1 + np.sqrt(1 - horizontal_load / (area_foundation * sl.cohesion)))\n        i_q = 1.0\n        i_g = 1.0\n    else:\n        if h_b == 0:\n            i_q = 1 - horizontal_load / (vertical_load + area_foundation * sl.cohesion /\n                                         np.tan(sl.phi_r))\n            i_g = i_q\n        elif h_b > 0 and h_l == 0:\n            i_q = ((1 - 0.7 * horizontal_load / (vertical_load + area_foundation * sl.cohesion /\n                                                 np.tan(sl.phi_r))) ** 3)\n            i_g = ((1 - horizontal_load / (vertical_load + area_foundation * sl.cohesion /\n                                           np.tan(sl.phi_r))) ** 3)\n        else:\n            raise DesignError(\"not setup for bi-directional loading\")\n        i_c = (i_q * fd.nq_factor - 1) / (fd.nq_factor - 1)\n\n    # ground slope factors:\n    g_c = 1 - slope * (1.0 - fd.depth / (2 * width_eff)) / 150\n    g_q = (1 - np.tan(slope * (1 - fd.depth / (2 * width_eff)))) ** 2\n    g_g = g_q\n\n    # stress at footing base:\n    q_d = sl.unit_dry_weight * fd.depth\n\n    if verbose:\n        log(\"Nc: \", fd.nc_factor)\n        log(\"Nq: \", fd.nq_factor)\n        log(\"Ng: \", fd.ng_factor)\n        log(\"H: \", horizontal_load)\n        log(\"s_c: \", s_c)\n        log(\"s_q: \", s_q)\n        log(\"s_g: \", s_g)\n        log(\"d_c: \", d_c)\n        log(\"d_q: \", d_q)\n        log(\"d_g: \", d_g)\n        log(\"i_c: \", i_c)\n        log(\"i_q: \", i_q)\n        log(\"i_g: \", i_g)\n        log(\"g_c: \", g_c)\n        log(\"g_q: \", g_q)\n        log(\"g_g: \", g_g)\n\n    # Capacity\n    fd.q_ult = (sl.cohesion * fd.nc_factor * s_c * d_c * i_c * g_c +\n                q_d * fd.nq_factor * s_q * d_q * i_q * g_q +\n                0.5 * width_eff * sl.unit_dry_weight *\n                fd.ng_factor * s_g * d_g * i_g * g_g)\n    if verbose:\n        log(\"q_ult: \", fd.q_ult)\n    return fd.q_ult"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the capacity according to Salgado 2008", "response": "def capacity_salgado_2008(sl, fd, h_l=0, h_b=0, vertical_load=1, verbose=0, **kwargs):\n    \"\"\"\n    calculates the capacity according to\n     THe Engineering of Foundations textbook by Salgado\n\n     ISBN: 0072500581\n\n    :param sl: Soil object\n    :param fd: Foundation object\n    :param h_l: Horizontal load parallel to length\n    :param h_b: Horizontal load parallel to width\n    :param vertical_load: Vertical load\n    :param verbose: verbosity\n    :return: ultimate bearing stress\n    \"\"\"\n    # Need to make adjustments if sand  has DR<40% or\n    # clay has liquidity indices greater than 0.7\n    if not kwargs.get(\"disable_requires\", False):\n        models.check_required(sl, [\"phi_r\", \"cohesion\", \"unit_dry_weight\"])\n        models.check_required(fd, [\"length\", \"width\", \"depth\"])\n\n    h_eff_b = kwargs.get(\"h_eff_b\", 0)\n    h_eff_l = kwargs.get(\"h_eff_l\", 0)\n    loc_v_l = kwargs.get(\"loc_v_l\", fd.length / 2)\n    loc_v_b = kwargs.get(\"loc_v_b\", fd.width / 2)\n\n    ecc_b = h_b * h_eff_b / vertical_load\n    ecc_l = h_l * h_eff_l / vertical_load\n\n    width_eff = min(fd.width, 2 * (loc_v_b + ecc_b), 2 * (fd.width - loc_v_b - ecc_b))\n    length_eff = min(fd.length, 2 * (loc_v_l + ecc_l), 2 * (fd.length - loc_v_l - ecc_l))\n\n    # check para 3.4.1\n    if width_eff / 2 < fd.width / 6:\n        DesignError(\"failed on eccentricity\")\n\n    # LOAD FACTORS:\n    fd.nq_factor = np.exp(np.pi * np.tan(sl.phi_r)) * (1 + np.sin(sl.phi_r)) / (1 - np.sin(sl.phi_r))\n    fd.ng_factor = 1.5 * (fd.nq_factor - 1) * np.tan(sl.phi_r)\n    # fd.ng_factor = (fd.nq_factor - 1) * np.tan(1.32 * sl.phi_r)\n    if sl.phi_r == 0:\n        fd.nc_factor = 5.14\n    else:\n        fd.nc_factor = (fd.nq_factor - 1) / np.tan(sl.phi_r)\n\n    # shape factors:\n    s_q = 1 + (width_eff / length_eff) * np.tan(sl.phi_r)\n    s_g = max(1 - 0.4 * width_eff / length_eff, 0.6)\n    s_c = 1.0\n\n    # depth factors:\n    d_q = 1 + 2 * np.tan(sl.phi_r) * (1 - np.sin(sl.phi_r)) ** 2 * fd.depth / width_eff\n    d_g = 1.0\n    d_c = 1.0\n\n    # stress at footing base:\n    q_d = sl.unit_dry_weight * fd.depth\n\n    if verbose:\n        log(\"width_eff: \", width_eff)\n        log(\"length_eff: \", length_eff)\n        log(\"Nc: \", fd.nc_factor)\n        log(\"Nq: \", fd.nq_factor)\n        log(\"Ng: \", fd.ng_factor)\n        log(\"s_c: \", s_c)\n        log(\"s_q: \", s_q)\n        log(\"s_g: \", s_g)\n        log(\"d_c: \", d_c)\n        log(\"d_q: \", d_q)\n        log(\"d_g: \", d_g)\n        log(\"q_d: \", q_d)\n\n    # Capacity\n    fd.q_ult = (sl.cohesion * fd.nc_factor * s_c * d_c +\n                q_d * fd.nq_factor * s_q * d_q +\n                0.5 * width_eff * sl.unit_dry_weight *\n                fd.ng_factor * s_g * d_g)\n\n    if verbose:\n        log(\"qult: \", fd.q_ult)\n    return fd.q_ult"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef size_footing_for_capacity(sl, vertical_load, fos=1.0, length_to_width=1.0, verbose=0, **kwargs):\n    method = kwargs.get(\"method\", 'vesics')\n    depth_to_width = kwargs.get(\"depth_to_width\", 0)\n    depth = kwargs.get(\"depth\", 0)\n    use_depth_to_width = 0\n    if not depth:\n        use_depth_to_width = 1\n\n    # Find approximate size\n    fd = models.FoundationRaft()\n    fd.width = .5  # start with B=1.0m\n    for i in range(50):\n        fd.length = length_to_width * fd.width\n        if use_depth_to_width:\n            fd.depth = depth_to_width * fd.width\n        capacity_method_selector(sl, fd, method)\n        q = fd.q_ult\n\n        bearing_capacity = q * fd.length * fd.width\n        fs_actual = bearing_capacity / vertical_load\n\n        if fs_actual < fos:\n            # Need to increase foundation sizes\n            fd.width += 0.5\n        else:\n            if verbose:\n                log(\"fs_actual: \", fs_actual)\n                log(\"fd.width: \", fd.width)\n            break\n\n    # at this stage the current size should be too big\n    width_array = []\n    fs_array = []\n    for j in range(11):\n        width_array.append(fd.width)\n        fd.length = length_to_width * fd.width\n        if use_depth_to_width:\n            fd.depth = depth_to_width * fd.width\n        capacity_method_selector(sl, fd, method)\n        q = fd.q_ult\n\n        capacity = q * fd.length * fd.width\n\n        fs_array.append(capacity / vertical_load)\n\n        fd.width = fd.width - 0.5 / 10\n\n    # search the array until FS satisfied:\n    if verbose:\n        log(\"reqFS: \", fos)\n        log(\"width array: \\n\", width_array)\n        log(\"FS array: \\n\", fs_array)\n\n    for fs in range(len(fs_array)):\n        if fs_array[fs] < fos:\n            fd.width = width_array[fs - 1]\n            fd.length = length_to_width * fd.width\n            if use_depth_to_width:\n                fd.depth = depth_to_width * fd.width\n            capacity_method_selector(sl, fd, method)\n            break\n        if fs == len(fs_array) - 1:\n            DesignError(\"No suitable foundation sizes could be determined!\")\n\n    return fd", "response": "Determines the size of a Foundation object based on the given vertical load and the given aspect ratio."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the bearing capacity of a foundation on soil using the specified method.", "response": "def capacity_method_selector(sl, fd, method, **kwargs):\n    \"\"\"\n    Calculates the bearing capacity of a foundation on soil using the specified method.\n    :param sl: Soil Object\n    :param fd: Foundation Object\n    :param method: Method\n    :param kwargs:\n    :return:\n    \"\"\"\n\n    if method == 'vesics':\n        capacity_vesics_1975(sl, fd, **kwargs)\n    elif method == 'nzs':\n        capacity_nzs_vm4_2011(sl, fd, **kwargs)\n    elif method == 'terzaghi':\n        capacity_terzaghi_1943(sl, fd, **kwargs)\n    elif method == 'hansen':\n        capacity_hansen_1970(sl, fd, **kwargs)\n    elif method == 'meyerhoff':\n        capacity_meyerhof_1963(sl, fd, **kwargs)\n    elif method == 'salgado':\n        capacity_salgado_2008(sl, fd, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndeprecate capacity calculation for two - layered foundation foundations according to Meyerhof and Hanna 1978.", "response": "def deprecated_capacity_meyerhof_and_hanna_1978(sl_0, sl_1, h0, fd, verbose=0):\n    \"\"\"\n    Calculates the two-layered foundation capacity according Meyerhof and Hanna (1978)\n\n    :param sl_0: Top Soil object\n    :param sl_1: Base Soil object\n    :param h0: Height of top soil layer\n    :param fd: Foundation object\n    :param h_l: Horizontal load parallel to length\n    :param h_b: Horizontal load parallel to width\n    :param vertical_load: Vertical load\n    :param verbose: verbosity\n    :return: ultimate bearing stress\n    \"\"\"\n\n    # UNFINISHED, this code is copied from the Meyerhoff method\n    # horizontal_load = np.sqrt(h_l ** 2 + h_b ** 2)\n\n    sl_0.nq_factor_0 = (\n        (np.tan(np.pi / 4 + np.deg2rad(sl_0.phi / 2))) ** 2 * np.exp(np.pi * np.tan(np.deg2rad(sl_0.phi))))\n    if sl_0.phi == 0:\n        sl_0.nc_factor_0 = 5.14\n    else:\n        sl_0.nc_factor_0 = (sl_0.nq_factor_0 - 1) / np.tan(np.deg2rad(sl_0.phi))\n    sl_0.ng_factor_0 = (sl_0.nq_factor_0 - 1) * np.tan(1.4 * np.deg2rad(sl_0.phi))\n\n    sl_1.nq_factor_1 = (\n        (np.tan(np.pi / 4 + np.deg2rad(sl_1.phi / 2))) ** 2 * np.exp(np.pi * np.tan(np.deg2rad(sl_1.phi))))\n    if sl_1.phi == 0:\n        sl_1.nc_factor_1 = 5.14\n    else:\n        sl_1.nc_factor_1 = (sl_1.nq_factor_1 - 1) / np.tan(np.deg2rad(sl_1.phi))\n    sl_1.ng_factor_1 = (sl_1.nq_factor_1 - 1) * np.tan(1.4 * np.deg2rad(sl_1.phi))\n\n    if verbose:\n        log(\"Nc: \", sl_1.nc_factor_1)\n        log(\"Nq: \", sl_1.nq_factor_1)\n        log(\"Ng: \", sl_1.ng_factor_1)\n\n    sl_0.kp_0 = (np.tan(np.pi / 4 + np.deg2rad(sl_0.phi / 2))) ** 2\n    sl_1.kp_1 = (np.tan(np.pi / 4 + np.deg2rad(sl_1.phi / 2))) ** 2\n    # shape factors\n\n    # s_c = 1 + 0.2 * kp * fd.width / fd.length\n    if sl_0.phi >= 10:\n        sl_0.s_c_0 = 1 + 0.2 * sl_0.kp_0 * (fd.width / fd.length)\n        sl_0.s_q_0 = 1.0 + 0.1 * sl_0.kp_0 * (fd.width / fd.length)\n    else:\n        sl_0.s_c_0 = 1 + 0.2 * (fd.width / fd.length)\n        sl_0.s_q_0 = 1.0\n    sl_0.s_g_0 = sl_0.s_q_0\n\n    if sl_1.phi >= 10:\n        sl_1.s_c_1 = 1 + 0.2 * sl_1.kp_1 * (fd.width / fd.length)\n        sl_1.s_q_1 = 1.0 + 0.1 * sl_1.kp_1 * (fd.width / fd.length)\n    else:\n        sl_1.s_c_1 = 1 + 0.2 * (fd.width / fd.length)\n        sl_1.s_q_1 = 1.0\n    sl_1.s_g_1 = sl_1.s_q_1\n\n    \"\"\"\n    # depth factors\n    d_c = 1 + 0.2 * np.sqrt(kp) * fd.depth / fd.width\n    if sl_0.phi > 10:\n        d_q = 1 + 0.1 * np.sqrt(kp) * fd.depth / fd.width\n    else:\n        d_q = 1.0\n    d_g = d_q\n\n    # inclination factors:\n    theta_load = np.arctan(horizontal_load / vertical_load)\n    i_c = (1 - theta_load / (np.pi * 0.5)) ** 2\n    i_q = i_c\n    if sl_0.phi > 0:\n        i_g = (1 - theta_load / sl_0.phi_r) ** 2\n    else:\n        i_g = 0\n    \"\"\"\n\n    # stress at footing base:\n    # q_d = sl_0.unit_dry_weight_0 * fd.depth\n\n    # ks\n    sl_0.q_0 = (sl_0.cohesion * sl_0.nc_factor_0) + (0.5 * sl_0.unit_dry_weight * fd.width * sl_0.ng_factor_0)\n    sl_1.q_1 = (sl_1.cohesion * sl_1.nc_factor_1) + (0.5 * sl_1.unit_dry_weight * fd.width * sl_1.ng_factor_1)\n    q1_q0 = sl_1.q_1 / sl_0.q_0\n\n    x_0 = np.array([0, 20.08, 22.42, 25.08, 27.58, 30.08, 32.58, 34.92, 37.83, 40.00, 42.67, 45.00, 47.00, 49.75])\n    y_0 = np.array([0.93, 0.93, 0.93, 0.93, 1.01, 1.17, 1.32, 1.56, 1.87, 2.26, 2.72, 3.35, 3.81, 4.82])\n    x_2 = np.array([0, 20.08, 22.50, 25.08, 27.58, 30.08, 32.50, 35.00, 37.67, 40.17, 42.67, 45.00, 47.50, 50.00])\n    y_2 = np.array([1.55, 1.55, 1.71, 1.86, 2.10, 2.33, 2.72, 3.11, 3.81, 4.43, 5.28, 6.14, 7.46, 9.24])\n    x_4 = np.array([0, 20.00, 22.51, 25.10, 27.69, 30.11, 32.45, 35.04, 37.88, 40.14, 42.65, 45.07, 47.33, 50.08])\n    y_4 = np.array([2.49, 2.49, 2.64, 2.87, 3.34, 3.81, 4.43, 5.20, 6.29, 7.38, 9.01, 11.11, 14.29, 19.34])\n    x_10 = np.array([0, 20.00, 22.50, 25.08, 28.00, 30.00, 32.50, 34.92, 37.50, 40.17, 42.42, 45.00, 47.17, 50.08])\n    y_10 = np.array([3.27, 3.27, 3.74, 4.44, 5.37, 6.07, 7.16, 8.33, 10.04, 12.30, 15.95, 21.17, 27.47, 40.00])\n    x_int = sl_0.phi\n\n    if sl_0.phi < 1:\n        fd.ks = 0\n    else:\n\n        if q1_q0 == 0:\n            fd.ks = np.interp(x_int, x_0, y_0)\n\n        elif q1_q0 == 0.2:\n            fd.ks = np.interp(x_int, x_2, y_2)\n\n        elif q1_q0 == 0.4:\n            fd.ks = np.interp(x_int, x_4, y_4)\n\n        elif q1_q0 == 1.0:\n            fd.ks = np.interp(x_int, x_10, y_10)\n\n        elif 0 < q1_q0 < 0.2:\n            ks_1 = np.interp(x_int, x_0, y_0)\n            ks_2 = np.interp(x_int, x_2, y_2)\n            fd.ks = (((ks_2 - ks_1) * q1_q0) / 0.2) + ks_1\n\n        elif 0.2 < q1_q0 < 0.4:\n            ks_1 = np.interp(x_int, x_2, y_2)\n            ks_2 = np.interp(x_int, x_4, y_4)\n            fd.ks = (((ks_2 - ks_1) * (q1_q0 - 0.2)) / 0.2) + ks_1\n\n        elif 0.4 < q1_q0 < 1.0:\n            ks_1 = np.interp(x_int, x_4, y_4)\n            ks_2 = np.interp(x_int, x_10, y_10)\n            fd.ks = (((ks_2 - ks_1) * (q1_q0 - 0.4)) / 0.6) + ks_1\n        else:\n            raise DesignError(\"Cannot compute 'ks', bearing ratio out-of-range (q1_q0 = %.3f) required: 0-1.\" % q1_q0)\n\n    # ca\n    if sl_0.cohesion == 0:\n        c1_c0 = 0\n    else:\n        c1_c0 = sl_1.cohesion / sl_0.cohesion\n    x = np.array([0.000, 0.082, 0.206, 0.298, 0.404, 0.509, 0.598, 0.685, 0.772])\n    y = np.array([0.627, 0.700, 0.794, 0.855, 0.912, 0.948, 0.968, 0.983, 0.997])\n    ca_c0 = np.interp(c1_c0, x, y)\n\n    fd.ca = ca_c0 * sl_0.cohesion\n\n    # Capacity\n    a = 1  # ????\n    s = 1  # ????\n\n    r = 1 + (fd.width / fd.length)\n    q_b1 = (sl_1.cohesion * sl_1.nc_factor_1 * sl_1.s_c_1)\n    q_b2 = (sl_0.unit_dry_weight * h0 * sl_1.nq_factor_1 * sl_1.s_q_1)\n    q_b3 = (sl_1.unit_dry_weight * fd.width * sl_1.ng_factor_1 * sl_1.s_g_1 / 2)\n    fd.q_b = q_b1 + q_b2 + q_b3\n    fd.q_ult4 = (r * (2 * fd.ca * (h0 - fd.depth) / fd.width) * a)\n    fd.q_ult5 = r * (sl_0.unit_dry_weight * ((h0 - fd.depth) ** 2)) * (1 + (2 * fd.depth / (h0 - fd.depth))) * (\n        fd.ks * np.tan(np.deg2rad(sl_0.phi)) / fd.width) * s\n    fd.q_ult6 = (sl_0.unit_dry_weight * (h0 - fd.depth))\n    fd.q_ult = fd.q_b + fd.q_ult4 + fd.q_ult5 - fd.q_ult6\n\n    # maximum value (qu <= qt)\n    q_t1 = (sl_0.cohesion * sl_0.nc_factor_0 * sl_0.s_c_0)\n    q_t2 = (sl_0.unit_dry_weight * fd.depth * sl_0.nq_factor_0 * sl_0.s_q_0)\n    q_t3 = (sl_0.unit_dry_weight * fd.width * sl_0.ng_factor_0 * sl_0.s_g_0 / 2)\n    fd.q_t = q_t1 + q_t2 + q_t3\n\n    if fd.q_ult > fd.q_t:\n        fd.q_ult = fd.q_t\n\n    return fd.q_ult"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef capacity_meyerhof_and_hanna_1978(sl_0, sl_1, h0, fd, gwl=1e6, verbose=0):\n    sp = sm.SoilProfile()\n    sp.add_layer(0, sl_0)\n    sp.add_layer(h0, sl_1)\n    sp.gwl = gwl\n    return capacity_sp_meyerhof_and_hanna_1978(sp, fd)", "response": "Calculates the two - layered foundation capacity according to Meyerhof and Hanna 1978."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef capacity_sp_meyerhof_and_hanna_1978(sp, fd, verbose=0):\n    assert isinstance(sp, sm.SoilProfile)\n\n    sl_0 = sp.layer(1)\n    sl_1 = sp.layer(2)\n    h0 = sp.layer_depth(2)\n    gwl = sp.gwl\n\n    sl_0.nq_factor_0 = (\n    (np.tan(np.pi / 4 + np.deg2rad(sl_0.phi / 2))) ** 2 * np.exp(np.pi * np.tan(np.deg2rad(sl_0.phi))))\n    if sl_0.phi == 0:\n        sl_0.nc_factor_0 = 5.14\n    else:\n        sl_0.nc_factor_0 = (sl_0.nq_factor_0 - 1) / np.tan(np.deg2rad(sl_0.phi))\n    sl_0.ng_factor_0 = (sl_0.nq_factor_0 - 1) * np.tan(1.4 * np.deg2rad(sl_0.phi))\n\n    sl_1.nq_factor_1 = (\n        (np.tan(np.pi / 4 + np.deg2rad(sl_1.phi / 2))) ** 2 * np.exp(np.pi * np.tan(np.deg2rad(sl_1.phi))))\n    if sl_1.phi == 0:\n        sl_1.nc_factor_1 = 5.14\n    else:\n        sl_1.nc_factor_1 = (sl_1.nq_factor_1 - 1) / np.tan(np.deg2rad(sl_1.phi))\n    sl_1.ng_factor_1 = (sl_1.nq_factor_1 - 1) * np.tan(1.4 * np.deg2rad(sl_1.phi))\n\n    if verbose:\n        log(\"Nc: \", sl_1.nc_factor_1)\n        log(\"Nq: \", sl_1.nq_factor_1)\n        log(\"Ng: \", sl_1.ng_factor_1)\n\n    sl_0.kp_0 = (np.tan(np.pi / 4 + np.deg2rad(sl_0.phi / 2))) ** 2\n    sl_1.kp_1 = (np.tan(np.pi / 4 + np.deg2rad(sl_1.phi / 2))) ** 2\n\n    # shape factors\n    if sl_0.phi >= 10:\n        sl_0.s_c_0 = 1 + 0.2 * sl_0.kp_0 * (fd.width / fd.length)\n        sl_0.s_q_0 = 1.0 + 0.1 * sl_0.kp_0 * (fd.width / fd.length)\n    else:\n        sl_0.s_c_0 = 1 + 0.2 * (fd.width / fd.length)\n        sl_0.s_q_0 = 1.0\n    sl_0.s_g_0 = sl_0.s_q_0\n\n    if sl_1.phi >= 10:\n        sl_1.s_c_1 = 1 + 0.2 * sl_1.kp_1 * (fd.width / fd.length)\n        sl_1.s_q_1 = 1.0 + 0.1 * sl_1.kp_1 * (fd.width / fd.length)\n    else:\n        sl_1.s_c_1 = 1 + 0.2 * (fd.width / fd.length)\n        sl_1.s_q_1 = 1.0\n    sl_1.s_g_1 = sl_1.s_q_1\n\n    # Note: this method explicitly accounts for the foundation depth, so there are no depth factors\n    # TODO: inclination factors, see doi.org/10.1139/t78-060\n\n    # Capacity\n    a = 1  # assumed to  be one but can range between 1.1 and 1.27 for square footings according to Das (1999) Ch 4\n    s = 1\n\n    r = 1 + (fd.width / fd.length)\n\n    # put the same things before that condition\n    # effective weight not in the soil object\n\n    if gwl == 0:  # case 1: GWL at surface\n        q_at_interface = sl_0.unit_bouy_weight * h0\n        unit_eff_weight_0_at_fd_depth = sl_0.unit_bouy_weight\n        unit_eff_weight_0_at_interface = sl_0.unit_bouy_weight\n        unit_eff_weight_1_below_foundation = sl_1.unit_bouy_weight\n\n    elif 0 < gwl <= fd.depth:  # Case 2: GWL at between foundation depth and surface\n        q_at_interface = (sl_0.unit_dry_weight * gwl) + (sl_0.unit_bouy_weight * (h0 - gwl))\n        q_d = (sl_0.unit_dry_weight * gwl) + (sl_0.unit_bouy_weight * (fd.depth - gwl))\n        unit_eff_weight_0_at_fd_depth = q_d / fd.depth\n        unit_eff_weight_0_at_interface = sl_0.unit_bouy_weight\n        unit_eff_weight_1_below_foundation = sl_1.unit_bouy_weight\n\n    elif fd.depth < gwl <= fd.width + fd.depth:\n        if gwl < h0:  # Case 3: GWL at between foundation depth and foundation depth plus width, and GWL < layer 1 depth\n\n            average_unit_bouy_weight = sl_0.unit_bouy_weight + (\n            ((gwl - fd.depth) / fd.width) * (sl_0.unit_dry_weight - sl_0.unit_bouy_weight))\n\n            q_at_interface = (sl_0.unit_dry_weight * gwl) + (sl_0.unit_bouy_weight * (h0 - gwl))\n            unit_eff_weight_0_at_fd_depth = sl_0.unit_dry_weight\n            unit_eff_weight_0_at_interface = average_unit_bouy_weight\n            unit_eff_weight_1_below_foundation = sl_1.unit_bouy_weight\n\n        else:  # Case 4: GWL at between foundation depth and foundation depth plus width, and GWL > layer 1 depth\n            average_unit_bouy_weight = sl_1.unit_bouy_weight + (\n            ((gwl - h0) / fd.width) * (sl_1.unit_dry_weight - sl_1.unit_bouy_weight))\n\n            q_at_interface = sl_0.unit_dry_weight * h0\n            unit_eff_weight_0_at_fd_depth = sl_0.unit_dry_weight\n            unit_eff_weight_0_at_interface = sl_0.unit_dry_weight\n            unit_eff_weight_1_below_foundation = average_unit_bouy_weight\n\n    elif gwl > fd.depth + fd.width:  # Case 5: GWL beyond foundation depth plus width\n        q_at_interface = sl_0.unit_dry_weight * h0\n        unit_eff_weight_0_at_fd_depth = sl_0.unit_dry_weight\n        unit_eff_weight_0_at_interface = sl_0.unit_dry_weight\n        unit_eff_weight_1_below_foundation = sl_1.unit_dry_weight\n    else:\n        raise ValueError(\"Could not interpret inputs\")  # never reached\n\n    # maximum value (qu <= qt)\n    q_ult6 = q_at_interface - unit_eff_weight_0_at_fd_depth * fd.depth\n    q_0 = (sl_0.cohesion * sl_0.nc_factor_0) + (0.5 * unit_eff_weight_0_at_interface * fd.width * sl_0.ng_factor_0)\n    q_b2 = (q_at_interface * sl_1.nq_factor_1 * sl_1.s_q_1)\n    q_1 = (sl_1.cohesion * sl_1.nc_factor_1) + (0.5 * unit_eff_weight_1_below_foundation * fd.width * sl_1.ng_factor_1)\n    q_b3 = (unit_eff_weight_1_below_foundation * fd.width * sl_1.ng_factor_1 * sl_1.s_g_1 / 2)\n    q_ult5 = r * (unit_eff_weight_0_at_interface * ((h0 - fd.depth) ** 2)) * (1 + (2 * fd.depth / (h0 - fd.depth))) * (\n        np.tan(np.deg2rad(sl_0.phi)) / fd.width) * s\n    q_t2 = (unit_eff_weight_0_at_fd_depth * fd.depth * sl_0.nq_factor_0 * sl_0.s_q_0)\n    q_t3 = (unit_eff_weight_0_at_interface * fd.width * sl_0.ng_factor_0 * sl_0.s_g_0 / 2)\n\n    # qb\n    q_b1 = (sl_1.cohesion * sl_1.nc_factor_1 * sl_1.s_c_1)\n    q_b = q_b1 + q_b2 + q_b3\n\n    q1_q0 = q_1 / q_0\n\n    # calculate the ca factor\n    # if sl_0.cohesion == 0:\n    #     c1_c0 = 0\n    # else:\n    #     c1_c0 = sl_1.cohesion / sl_0.cohesion\n    x = np.array([0.000, 0.082, 0.206, 0.298, 0.404, 0.509, 0.598, 0.685, 0.772])\n    y = np.array([0.627, 0.700, 0.794, 0.855, 0.912, 0.948, 0.968, 0.983, 0.997])\n\n    # raise Warning(\"ca should be interpolated using q1/q2 not cohesion, see Figure 4 in MH1978\")\n    ca_c0 = np.interp(q1_q0, x, y)\n    ca = ca_c0 * sl_0.cohesion\n\n    # ks\n    x_0 = np.array([0, 20.08, 22.42, 25.08, 27.58, 30.08, 32.58, 34.92, 37.83, 40.00, 42.67, 45.00, 47.00, 49.75])\n    y_0 = np.array([0.93, 0.93, 0.93, 0.93, 1.01, 1.17, 1.32, 1.56, 1.87, 2.26, 2.72, 3.35, 3.81, 4.82])\n    x_2 = np.array([0, 20.08, 22.50, 25.08, 27.58, 30.08, 32.50, 35.00, 37.67, 40.17, 42.67, 45.00, 47.50, 50.00])\n    y_2 = np.array([1.55, 1.55, 1.71, 1.86, 2.10, 2.33, 2.72, 3.11, 3.81, 4.43, 5.28, 6.14, 7.46, 9.24])\n    x_4 = np.array([0, 20.00, 22.51, 25.10, 27.69, 30.11, 32.45, 35.04, 37.88, 40.14, 42.65, 45.07, 47.33, 50.08])\n    y_4 = np.array([2.49, 2.49, 2.64, 2.87, 3.34, 3.81, 4.43, 5.20, 6.29, 7.38, 9.01, 11.11, 14.29, 19.34])\n    x_10 = np.array([0, 20.00, 22.50, 25.08, 28.00, 30.00, 32.50, 34.92, 37.50, 40.17, 42.42, 45.00, 47.17, 50.08])\n    y_10 = np.array([3.27, 3.27, 3.74, 4.44, 5.37, 6.07, 7.16, 8.33, 10.04, 12.30, 15.95, 21.17, 27.47, 40.00])\n    x_int = sl_0.phi\n\n    if sl_0.phi < 1:\n        fd.ks = 0\n    else:\n\n        if q1_q0 == 0:\n            fd.ks = np.interp(x_int, x_0, y_0)\n\n        elif q1_q0 == 0.2:\n            fd.ks = np.interp(x_int, x_2, y_2)\n\n        elif q1_q0 == 0.4:\n            fd.ks = np.interp(x_int, x_4, y_4)\n\n        elif q1_q0 == 1.0:\n            fd.ks = np.interp(x_int, x_10, y_10)\n\n        elif 0 < q1_q0 < 0.2:\n            ks_1 = np.interp(x_int, x_0, y_0)\n            ks_2 = np.interp(x_int, x_2, y_2)\n            fd.ks = (((ks_2 - ks_1) * q1_q0) / 0.2) + ks_1\n\n        elif 0.2 < q1_q0 < 0.4:\n            ks_1 = np.interp(x_int, x_2, y_2)\n            ks_2 = np.interp(x_int, x_4, y_4)\n            fd.ks = (((ks_2 - ks_1) * (q1_q0 - 0.2)) / 0.2) + ks_1\n\n        elif 0.4 < q1_q0 < 1.0:\n            ks_1 = np.interp(x_int, x_4, y_4)\n            ks_2 = np.interp(x_int, x_10, y_10)\n            fd.ks = (((ks_2 - ks_1) * (q1_q0 - 0.4)) / 0.6) + ks_1\n        else:\n            raise DesignError(\n                \"Cannot compute 'ks', bearing ratio out-of-range (q1_q0 = %.3f) required: 0-1.\" % q1_q0)\n\n    # qu\n    q_ult4 = (r * (2 * ca * (h0 - fd.depth) / fd.width) * a)\n    q_ult5_ks = q_ult5 * fd.ks\n    q_ult = q_b + q_ult4 + q_ult5_ks - q_ult6\n\n    q_t1 = (sl_0.cohesion * sl_0.nc_factor_0 * sl_0.s_c_0)\n    q_t = q_t1 + q_t2 + q_t3\n\n    if q_ult > q_t:\n        if h0 > fd.width/2:\n            fd.q_ult = q_t\n\n        else:\n            vert_eff_stress_interface = sp.vertical_effective_stress(h0)\n            vert_eff_stress_lowest = sp.vertical_effective_stress(fd.width+fd.depth)\n            average_eff_stress = (vert_eff_stress_interface + vert_eff_stress_lowest) / 2\n\n            c_2_eff = sl_1.cohesion + average_eff_stress * np.tan(np.radians(sl_1.phi))\n\n            if sl_0.cohesion > c_2_eff:\n                fd.q_ult = q_t\n\n            else:\n                # vd = {}\n                # vd[1] =[1, 1, 1, 1, 1]\n                # vd[0.667] = [1, 1.033, 1.064, 1.088, 1.109]\n                # vd[0.5] = [1, 1.056, 1.107, 1.152, 1.193]\n                # vd[0.333] = [1, 1.088, 1.167, 1.241, 1.311]\n                # vd[0.25] = [1, 1.107, 1.208, 1.302, 1.389]\n                # vd[0.2] = [1, 1.121, 1.235, 1.342, 1.444]\n                # vd[0.1] = [1, 1.154, 1.302, 1.446, 1.584]\n\n                h_over_b = (h0 - fd.depth)/fd.width\n                c1_over_c2 =sl_0.cohesion/c_2_eff\n\n                c_1_over_c_2 = [0.1, 0.2, 0.25, 0.333, 0.5, 0.667, 1.]\n                m_1 = [1.584, 1.444, 1.389, 1.311, 1.193, 1.109, 1.]\n                m_125 = [1.446, 1.342, 1.302, 1.241, 1.152, 1.088, 1.]\n                m_167 = [1.302, 1.235, 1.208, 1.167, 1.107, 1.064, 1.]\n                m_25 = [1.154, 1.121, 1.107, 1.088, 1.056, 1.033, 1.]\n                m_5 = [1, 1, 1, 1, 1, 1, 1]\n\n                if h_over_b == 0.1:\n                    m = np.interp(c1_over_c2, c_1_over_c_2, m_1)\n                elif h_over_b == 0.125:\n                    m = np.interp(c1_over_c2, c_1_over_c_2, m_125)\n                elif h_over_b == 0.167:\n                    m = np.interp(c1_over_c2, c_1_over_c_2, m_167)\n                elif h_over_b == 0.250:\n                    m = np.interp(c1_over_c2, c_1_over_c_2, m_25)\n                elif h_over_b >= 0.5:\n                    m = np.interp(c1_over_c2, c_1_over_c_2, m_5)\n                elif 0.1 < h_over_b < 0.125:\n                    m_a = np.interp(c1_over_c2, c_1_over_c_2, m_1)\n                    m_b = np.interp(c1_over_c2, c_1_over_c_2, m_125)\n                    m = np.interp(h_over_b, [0.1,0.125], [m_a,m_b])\n                elif 0.125 < h_over_b < 0.167:\n                    m_a = np.interp(c1_over_c2, c_1_over_c_2, m_125)\n                    m_b = np.interp(c1_over_c2, c_1_over_c_2, m_167)\n                    m = np.interp(h_over_b, [0.125, 0.167], [m_a, m_b])\n                elif 0.167 < h_over_b < 0.25:\n                    m_a = np.interp(c1_over_c2, c_1_over_c_2, m_167)\n                    m_b = np.interp(c1_over_c2, c_1_over_c_2, m_25)\n                    m = np.interp(h_over_b, [0.167, 0.250], [m_a, m_b])\n                elif 0.25 < h_over_b < 0.5:\n                    m_a = np.interp(c1_over_c2, c_1_over_c_2, m_25)\n                    m_b = np.interp(c1_over_c2, c_1_over_c_2, m_5)\n                    m = np.interp(h_over_b, [0.250, 0.500], [m_a, m_b])\n\n                fd.q_ult = (sl_0.cohesion * m * sl_0.nc_factor_0) + (unit_eff_weight_0_at_fd_depth * fd.depth)\n\n    else:\n        fd.q_ult = q_ult\n\n    return fd.q_ult", "response": "Calculates the two - layered foundation capacity according to Meyerhof and Hanna 1978."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the roles for an account.", "response": "def get_roles_in_account(self, account_id, params={}):\n        \"\"\"\n        List the roles for an account, for the passed Canvas account ID.\n\n        https://canvas.instructure.com/doc/api/roles.html#method.role_overrides.api_index\n        \"\"\"\n        url = ACCOUNTS_API.format(account_id) + \"/roles\"\n\n        roles = []\n        for datum in self._get_resource(url, params=params):\n            roles.append(CanvasRole(data=datum))\n        return roles"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_roles_by_account_sis_id(self, account_sis_id, params={}):\n        return self.get_roles_in_account(self._sis_id(account_sis_id,\n                                                      sis_field=\"account\"),\n                                         params)", "response": "Get the roles for an account SIS ID."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of all course roles available to an account.", "response": "def get_effective_course_roles_in_account(self, account_id):\n        \"\"\"\n        List all course roles available to an account, for the passed Canvas\n        account ID, including course roles inherited from parent accounts.\n        \"\"\"\n        course_roles = []\n        params = {\"show_inherited\": \"1\"}\n        for role in self.get_roles_in_account(account_id, params):\n            if role.base_role_type != \"AccountMembership\":\n                course_roles.append(role)\n        return course_roles"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_role(self, account_id, role_id):\n        url = ACCOUNTS_API.format(account_id) + \"/roles/{}\".format(role_id)\n        return CanvasRole(data=self._get_resource(url))", "response": "Get information about a single role for the passed Canvas account ID and role ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets information about a single role for the passed account SIS ID.", "response": "def get_role_by_account_sis_id(self, account_sis_id, role_id):\n        \"\"\"\n        Get information about a single role, for the passed account SIS ID.\n        \"\"\"\n        return self.get_role(self._sis_id(account_sis_id, sis_field=\"account\"),\n                             role_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_course(self, course_id, params={}):\n        include = params.get(\"include\", [])\n        if \"term\" not in include:\n            include.append(\"term\")\n        params[\"include\"] = include\n\n        url = COURSES_API.format(course_id)\n        return CanvasCourse(data=self._get_resource(url, params=params))", "response": "Return course resource for given canvas course id."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_course_by_sis_id(self, sis_course_id, params={}):\n        return self.get_course(self._sis_id(sis_course_id, sis_field=\"course\"),\n                               params)", "response": "Get course resource by sis id."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of courses in the passed account.", "response": "def get_courses_in_account(self, account_id, params={}):\n        \"\"\"\n        Returns a list of courses for the passed account ID.\n\n        https://canvas.instructure.com/doc/api/accounts.html#method.accounts.courses_api\n        \"\"\"\n        if \"published\" in params:\n            params[\"published\"] = \"true\" if params[\"published\"] else \"\"\n\n        url = ACCOUNTS_API.format(account_id) + \"/courses\"\n\n        courses = []\n        for data in self._get_paged_resource(url, params=params):\n            courses.append(CanvasCourse(data=data))\n        return courses"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a list of courses in the account with the passed SIS ID.", "response": "def get_courses_in_account_by_sis_id(self, sis_account_id, params={}):\n        \"\"\"\n        Return a list of courses for the passed account SIS ID.\n        \"\"\"\n        return self.get_courses_in_account(\n            self._sis_id(sis_account_id, sis_field=\"account\"), params)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_published_courses_in_account(self, account_id, params={}):\n        params[\"published\"] = True\n        return self.get_courses_in_account(account_id, params)", "response": "Get a list of all courses published in the passed account."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_published_courses_in_account_by_sis_id(self, sis_account_id,\n                                                   params={}):\n        \"\"\"\n        Return a list of published courses for the passed account SIS ID.\n        \"\"\"\n\n        return self.get_published_courses_in_account(\n            self._sis_id(sis_account_id, sis_field=\"account\"), params)", "response": "Get a list of published courses in the account with the passed SIS ID."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a list of courses for the passed regid.", "response": "def get_courses_for_regid(self, regid, params={}):\n        \"\"\"\n        Return a list of courses for the passed regid.\n\n        https://canvas.instructure.com/doc/api/courses.html#method.courses.index\n        \"\"\"\n        self._as_user = regid\n        data = self._get_resource(\"/api/v1/courses\", params=params)\n        self._as_user = None\n\n        courses = []\n        for datum in data:\n            if \"sis_course_id\" in datum:\n                courses.append(CanvasCourse(data=datum))\n            else:\n                courses.append(self.get_course(datum[\"id\"], params))\n\n        return courses"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_course(self, account_id, course_name):\n        url = ACCOUNTS_API.format(account_id) + \"/courses\"\n        body = {\"course\": {\"name\": course_name}}\n        return CanvasCourse(data=self._post_resource(url, body))", "response": "Create a canvas course with the given subaccount id and course name."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_sis_id(self, course_id, sis_course_id):\n        url = COURSES_API.format(course_id)\n        body = {\"course\": {\"sis_course_id\": sis_course_id}}\n        return CanvasCourse(data=self._put_resource(url, body))", "response": "Updates the SIS ID for the passed course ID."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the participation data for the given account_id and term_id.", "response": "def get_activity_by_account(self, account_id, term_id):\n        \"\"\"\n        Returns participation data for the given account_id and term_id.\n\n        https://canvas.instructure.com/doc/api/analytics.html#method.analytics_api.department_participation\n        \"\"\"\n        url = (\"/api/v1/accounts/sis_account_id:%s/analytics/\"\n               \"terms/sis_term_id:%s/activity.json\") % (account_id, term_id)\n        return self._get_resource(url)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the grade data for the given account_id and term_id.", "response": "def get_grades_by_account(self, account_id, term_id):\n        \"\"\"\n        Returns grade data for the given account_id and term_id.\n\n        https://canvas.instructure.com/doc/api/analytics.html#method.analytics_api.department_grades\n        \"\"\"\n        url = (\"/api/v1/accounts/sis_account_id:%s/analytics/\"\n               \"terms/sis_term_id:%s/grades.json\") % (account_id, term_id)\n        return self._get_resource(url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_statistics_by_account(self, account_id, term_id):\n        url = (\"/api/v1/accounts/sis_account_id:%s/analytics/\"\n               \"terms/sis_term_id:%s/statistics.json\") % (account_id, term_id)\n        return self._get_resource(url)", "response": "Returns statistics for the given account_id and term_id."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the participation data for the given sis_course_id.", "response": "def get_activity_by_sis_course_id(self, sis_course_id):\n        \"\"\"\n        Returns participation data for the given sis_course_id.\n\n        https://canvas.instructure.com/doc/api/analytics.html#method.analytics_api.course_participation\n        \"\"\"\n        url = \"/api/v1/courses/%s/analytics/activity.json\" % (\n            self._sis_id(sis_course_id, sis_field=\"course\"))\n        return self._get_resource(url)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the assignment data for the given sis_course_id.", "response": "def get_assignments_by_sis_course_id(self, sis_course_id):\n        \"\"\"\n        Returns assignment data for the given course_id.\n\n        https://canvas.instructure.com/doc/api/analytics.html#method.analytics_api.course_assignments\n        \"\"\"\n        url = \"/api/v1/courses/%s/analytics/assignments.json\" % (\n            self._sis_id(sis_course_id, sis_field=\"course\"))\n        return self._get_resource(url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_student_summaries_by_sis_course_id(self, sis_course_id):\n        url = \"/api/v1/courses/%s/analytics/student_summaries.json\" % (\n            self._sis_id(sis_course_id, sis_field=\"course\"))\n        return self._get_resource(url)", "response": "Returns the student summary data for the given sis_course_id."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_student_activity_for_sis_course_id_and_sis_user_id(\n            self, sis_user_id, sis_course_id):\n        \"\"\"\n        Returns student activity data for the given user_id and course_id.\n\n        https://canvas.instructure.com/doc/api/analytics.html#method.analytics_api.student_in_course_participation\n        \"\"\"\n        url = (\"/api/v1/courses/%s/analytics/users/\"\n               \"sis_user_id:%s/activity.json\") % (\n            self._sis_id(sis_course_id, sis_field=\"course\"), sis_user_id)\n        return self._get_resource(url)", "response": "Returns student activity data for the given user_id and course_id."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_student_assignments_for_sis_course_id_and_sis_user_id(\n            self, sis_user_id, sis_course_id):\n        \"\"\"\n        Returns student assignment data for the given user_id and course_id.\n\n        https://canvas.instructure.com/doc/api/analytics.html#method.analytics_api.student_in_course_assignments\n        \"\"\"\n        url = (\"/api/v1/courses/%s/analytics/\"\n               \"users/sis_user_id:%s/assignments.json\") % (\n            self._sis_id(sis_course_id, sis_field=\"course\"), sis_user_id)\n        return self._get_resource(url)", "response": "Returns the student assignment data for the given user_id and course_id."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_student_assignments_for_sis_course_id_and_canvas_user_id(\n            self, sis_course_id, user_id):\n        \"\"\"\n        Returns student assignment data for the given user_id and course_id.\n\n        https://canvas.instructure.com/doc/api/analytics.html#method.analytics_api.student_in_course_assignments\n        \"\"\"\n\n        url = \"/api/v1/courses/%s/analytics/users/%s/assignments.json\" % (\n            self._sis_id(sis_course_id, sis_field=\"course\"), user_id)\n        return self._get_resource(url)", "response": "Returns student assignment data for the given user_id and course_id."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_student_messaging_for_sis_course_id_and_sis_user_id(\n            self, sis_user_id, sis_course_id):\n        \"\"\"\n        Returns student messaging data for the given user_id and course_id.\n\n        https://canvas.instructure.com/doc/api/analytics.html#method.analytics_api.student_in_course_messaging\n        \"\"\"\n        url = (\"/api/v1/courses/%s/analytics/\"\n               \"users/sis_user_id:%s/communication.json\") % (\n            self._sis_id(sis_course_id, sis_field=\"course\"), sis_user_id)\n        return self._get_resource(url)", "response": "Returns student messaging data for the given user_id and course_id."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_submissions_by_course_and_assignment(\n            self, course_id, assignment_id, params={}):\n        \"\"\"\n        https://canvas.instructure.com/doc/api/submissions.html#method.submissions_api.index\n        \"\"\"\n        url = COURSES_API.format(course_id)\n        url += \"/assignments/{}/submissions\".format(assignment_id)\n\n        submissions = []\n        for data in self._get_paged_resource(url, params=params):\n            submissions.append(Submission(data=data))\n        return submissions", "response": "Get all submissions for a given course and assignment."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a list of submissions for multiple assignments by sis id and optionally student getTaxID", "response": "def get_submissions_multiple_assignments_by_sis_id(\n            self, is_section, sis_id, students=None, assignments=None,\n            **params):\n        \"\"\"\n        List submissions for multiple assignments by course/section sis id and\n        optionally student\n\n        https://canvas.instructure.com/doc/api/submissions.html#method.submissions_api.for_students\n        \"\"\"\n        if is_section:\n            return self.get_submissions_multiple_assignments(\n                is_section, self._sis_id(sis_id, 'section'), students,\n                assignments, **params)\n        else:\n            return self.get_submissions_multiple_assignments(\n                is_section, self._sis_id(sis_id, 'course'), students,\n                assignments, **params)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets all submissions for multiple assignments by course id and optionally students and assignments.", "response": "def get_submissions_multiple_assignments(\n            self, is_section, course_id, students=None, assignments=None,\n            **params):\n        \"\"\"\n        List submissions for multiple assignments by course/section id and\n        optionally student\n\n        https://canvas.instructure.com/doc/api/submissions.html#method.submissions_api.for_students\n        \"\"\"\n        api = SECTIONS_API if is_section else COURSES_API\n        if students is not None:\n            params['student_ids'] = students\n        if assignments is not None:\n            params['assignment_ids'] = assignments\n\n        url = api.format(course_id) + \"/students/submissions\"\n        data = self._get_paged_resource(url, params=params)\n        submissions = []\n        for submission in data:\n            submissions.append(Submission(data=submission))\n        return submissions"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the rotational stiffness of a foundation.", "response": "def rotational_stiffness(sl, fd, axis=\"length\", a0=0.0, **kwargs):\n    \"\"\"\n    Rotation stiffness of foundation.\n    :param fd: Foundation object\n    :param sl: Soil Object.\n    :param axis: The axis which it should be computed around\n    :return:\n    \"\"\"\n    if not kwargs.get(\"disable_requires\", False):\n        gf.models.check_required(sl, [\"g_mod\", \"poissons_ratio\"])\n        gf.models.check_required(fd, [\"length\", \"width\", \"depth\"])\n\n    if fd.depth > 0.0:\n        pass\n    l = fd.length * 0.5\n    b = fd.width * 0.5\n    v = sl.poissons_ratio\n    if axis == \"length\":\n        i_bx = fd.i_ll\n        k_rx = 1 - 0.2 * a0\n        k_f_0 = (sl.g_mod / (1 - v) * i_bx ** 0.75 * (l / b) ** 0.25 * (2.4 + 0.5 * (b / l))) * k_rx\n    else:\n        i_by = fd.i_ww\n        k_ry = 1 - 0.3 * a0\n        k_f_0 = (sl.g_mod / (1 - v) * i_by ** 0.75 * (3 * (l / b) ** 0.15)) * k_ry\n    return k_f_0"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget external tools in the passed canvas account id.", "response": "def get_external_tools_in_account(self, account_id, params={}):\n        \"\"\"\n        Return external tools for the passed canvas account id.\n\n        https://canvas.instructure.com/doc/api/external_tools.html#method.external_tools.index\n        \"\"\"\n        url = ACCOUNTS_API.format(account_id) + \"/external_tools\"\n\n        external_tools = []\n        for data in self._get_paged_resource(url, params=params):\n            external_tools.append(data)\n        return external_tools"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets external tools in a canvas course.", "response": "def get_external_tools_in_course(self, course_id, params={}):\n        \"\"\"\n        Return external tools for the passed canvas course id.\n\n        https://canvas.instructure.com/doc/api/external_tools.html#method.external_tools.index\n        \"\"\"\n        url = COURSES_API.format(course_id) + \"/external_tools\"\n\n        external_tools = []\n        for data in self._get_paged_resource(url, params=params):\n            external_tools.append(data)\n        return external_tools"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating an external tool using the passed json_data.", "response": "def _create_external_tool(self, context, context_id, json_data):\n        \"\"\"\n        Create an external tool using the passed json_data.\n\n        context is either COURSES_API or ACCOUNTS_API.\n        context_id is the Canvas course_id or account_id, depending on context.\n\n        https://canvas.instructure.com/doc/api/external_tools.html#method.external_tools.create\n        \"\"\"\n        url = context.format(context_id) + \"/external_tools\"\n        return self._post_resource(url, body=json_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _update_external_tool(self, context, context_id, external_tool_id,\n                              json_data):\n        \"\"\"\n        Update the external tool identified by external_tool_id with the passed\n        json data.\n\n        context is either COURSES_API or ACCOUNTS_API.\n        context_id is the course_id or account_id, depending on context\n\n        https://canvas.instructure.com/doc/api/external_tools.html#method.external_tools.update\n        \"\"\"\n        url = context.format(context_id) + \"/external_tools/{}\".format(\n            external_tool_id)\n        return self._put_resource(url, body=json_data)", "response": "Update the external tool identified by external_tool_id with the passed json_data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete the external tool identified by external_tool_id.", "response": "def _delete_external_tool(self, context, context_id, external_tool_id):\n        \"\"\"\n        Delete the external tool identified by external_tool_id.\n\n        context is either COURSES_API or ACCOUNTS_API.\n        context_id is the course_id or account_id, depending on context\n\n        https://canvas.instructure.com/doc/api/external_tools.html#method.external_tools.destroy\n        \"\"\"\n        url = context.format(context_id) + \"/external_tools/{}\".format(\n            external_tool_id)\n        response = self._delete_resource(url)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a sessionless launch url for an external tool.", "response": "def _get_sessionless_launch_url(self, context, context_id, tool_id):\n        \"\"\"\n        Get a sessionless launch url for an external tool.\n\n        https://canvas.instructure.com/doc/api/external_tools.html#method.external_tools.generate_sessionless_launch\n        \"\"\"\n        url = context.format(context_id) + \"/external_tools/sessionless_launch\"\n        params = {\"id\": tool_id}\n        return self._get_resource(url, params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_sessionless_launch_url_from_account_sis_id(\n            self, tool_id, account_sis_id):\n        \"\"\"\n        Get a sessionless launch url for an external tool.\n\n        https://canvas.instructure.com/doc/api/external_tools.html#method.external_tools.generate_sessionless_launch\n        \"\"\"\n        return self.get_sessionless_launch_url_from_account(\n            tool_id, self._sis_id(account_sis_id, \"account\"))", "response": "Get a sessionless launch url for an external tool and account."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a sessionless launch url for an external tool and a course s SI ID.", "response": "def get_sessionless_launch_url_from_course_sis_id(\n            self, tool_id, course_sis_id):\n        \"\"\"\n        Get a sessionless launch url for an external tool.\n\n        https://canvas.instructure.com/doc/api/external_tools.html#method.external_tools.generate_sessionless_launch\n        \"\"\"\n        return self.get_sessionless_launch_url_from_course(\n            tool_id, self._sis_id(course_sis_id, \"course\"))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_foundation(length, width, depth=0.0, height=0.0):\n    a_foundation = FoundationRaft()\n    a_foundation.length = length\n    a_foundation.width = width\n    a_foundation.depth = depth\n    a_foundation.height = height\n    return a_foundation", "response": "Create a Foundation object from dimensions."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_soil(phi=0.0, cohesion=0.0, unit_dry_weight=0.0, pw=9800):\n    soil = Soil(pw=pw)\n    soil.phi = phi\n    soil.cohesion = cohesion\n    soil.unit_dry_weight = unit_dry_weight\n    return soil", "response": "Create a Soil object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if a parameter is set on an object.", "response": "def check_required(obj, required_parameters):\n    \"\"\"\n    Check if a parameter is available on an object\n\n    :param obj: Object\n    :param required_parameters: list of parameters\n    :return:\n    \"\"\"\n    for parameter in required_parameters:\n        if not hasattr(obj, parameter) or getattr(obj, parameter) is None:\n            raise DesignError(\"parameter '%s' must be set for '%s' object.\" % (parameter, obj.base_type))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning user profile data.", "response": "def get_user(self, user_id):\n        \"\"\"\n        Returns user profile data.\n\n        https://canvas.instructure.com/doc/api/users.html#method.profile.settings\n        \"\"\"\n        url = USERS_API.format(user_id) + \"/profile\"\n        return CanvasUser(data=self._get_resource(url))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_users_for_course(self, course_id, params={}):\n        url = COURSES_API.format(course_id) + \"/users\"\n        data = self._get_paged_resource(url, params=params)\n        users = []\n        for datum in data:\n            users.append(CanvasUser(data=datum))\n        return users", "response": "Returns a list of users for the given course id."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_users_for_sis_course_id(self, sis_course_id, params={}):\n        return self.get_users_for_course(\n            self._sis_id(sis_course_id, sis_field=\"course\"), params)", "response": "Returns a list of users for the given sis course id."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_user(self, user, account_id=None):\n        if account_id is None:\n            account_id = self._canvas_account_id\n            if account_id is None:\n                raise MissingAccountID()\n\n        url = ACCOUNTS_API.format(account_id) + \"/users\"\n\n        data = self._post_resource(url, user.post_data())\n        return CanvasUser(data=data)", "response": "Create and return a new user and pseudonym for an account."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_user_logins(self, user_id, params={}):\n        url = USERS_API.format(user_id) + \"/logins\"\n\n        data = self._get_paged_resource(url, params=params)\n\n        logins = []\n        for login_data in data:\n            logins.append(Login(data=login_data))\n\n        return logins", "response": "Get a user s logins for the given user_id."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates an existing login for a user in the given account.", "response": "def update_user_login(self, login, account_id=None):\n        \"\"\"\n        Update an existing login for a user in the given account.\n\n        https://canvas.instructure.com/doc/api/logins.html#method.pseudonyms.update\n        \"\"\"\n        if account_id is None:\n            account_id = self._canvas_account_id\n            if account_id is None:\n                raise MissingAccountID\n\n        login_id = login.login_id\n        url = ACCOUNTS_API.format(account_id) + \"/logins/{}\".format(login_id)\n\n        data = self._put_resource(url, login.put_data())\n        return Login(data=data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _next_page(self, response):\n        for link in response.getheader(\"link\", \"\").split(\",\"):\n            try:\n                (url, rel) = link.split(\";\")\n                if \"next\" in rel:\n                    return url.lstrip(\"<\").rstrip(\">\")\n            except Exception:\n                return", "response": "Return the next page of data from the response."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncanvases GET method on a full url. Return representation of the requested resource.", "response": "def _get_resource_url(self, url, auto_page, data_key):\n        \"\"\"\n        Canvas GET method on a full url. Return representation of the\n        requested resource, chasing pagination links to coalesce resources\n        if indicated.\n        \"\"\"\n        headers = {'Accept': 'application/json',\n                   'Connection': 'keep-alive'}\n        response = DAO.getURL(url, headers)\n\n        if response.status != 200:\n            raise DataFailureException(url, response.status, response.data)\n\n        data = json.loads(response.data)\n\n        self.next_page_url = self._next_page(response)\n        if auto_page and self.next_page_url:\n            if isinstance(data, list):\n                data.extend(self._get_resource_url(self.next_page_url, True,\n                                                   data_key))\n            elif isinstance(data, dict) and data_key is not None:\n                data[data_key].extend(self._get_resource_url(\n                    self.next_page_url, True, data_key)[data_key])\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_paged_resource(self, url, params=None, data_key=None):\n        if not params:\n            params = {}\n\n        self._set_as_user(params)\n\n        auto_page = not ('page' in params or 'per_page' in params)\n\n        if 'per_page' not in params and self._per_page != DEFAULT_PAGINATION:\n            params[\"per_page\"] = self._per_page\n\n        full_url = url + self._params(params)\n        return self._get_resource_url(full_url, auto_page, data_key)", "response": "Canvas GET method. Return representation of the requested resource either the requested page or the pagination links to the requested page."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_resource(self, url, params=None, data_key=None):\n        if not params:\n            params = {}\n\n        self._set_as_user(params)\n\n        full_url = url + self._params(params)\n\n        return self._get_resource_url(full_url, True, data_key)", "response": "Canvas GET method. Return representation of the requested resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_admins(self, account_id, params={}):\n        url = ADMINS_API.format(account_id)\n\n        admins = []\n        for data in self._get_paged_resource(url, params=params):\n            admins.append(CanvasAdmin(data=data))\n        return admins", "response": "Get a list of the admins in the account."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_admin(self, account_id, user_id, role):\n        url = ADMINS_API.format(account_id)\n        body = {\"user_id\": unquote(str(user_id)),\n                \"role\": role,\n                \"send_confirmation\": False}\n\n        return CanvasAdmin(data=self._post_resource(url, body))", "response": "Flag an existing user as an admin within the account."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nflags an existing user as an admin within the account sis id.", "response": "def create_admin_by_sis_id(self, sis_account_id, user_id, role):\n        \"\"\"\n        Flag an existing user as an admin within the account sis id.\n        \"\"\"\n        return self.create_admin(self._sis_id(sis_account_id), user_id, role)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves an account admin role from a user.", "response": "def delete_admin(self, account_id, user_id, role):\n        \"\"\"\n        Remove an account admin role from a user.\n\n        https://canvas.instructure.com/doc/api/admins.html#method.admins.destroy\n        \"\"\"\n        url = ADMINS_API.format(account_id) + \"/{}?role={}\".format(\n            user_id, quote(role))\n\n        response = self._delete_resource(url)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete_admin_by_sis_id(self, sis_account_id, user_id, role):\n        return self.delete_admin(self._sis_id(sis_account_id), user_id, role)", "response": "Delete an account admin role from a user"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_grading_standards_for_course(self, course_id):\n        url = COURSES_API.format(course_id) + \"/grading_standards\"\n\n        standards = []\n        for data in self._get_resource(url):\n            standards.append(GradingStandard(data=data))\n        return standards", "response": "Get the grading standards available to a course"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new grading standard for the passed course.", "response": "def create_grading_standard_for_course(self, course_id, name,\n                                           grading_scheme, creator):\n        \"\"\"\n        Create a new grading standard for the passed course.\n\n        https://canvas.instructure.com/doc/api/grading_standards.html#method.grading_standards_api.create\n        \"\"\"\n        url = COURSES_API.format(course_id) + \"/grading_standards\"\n        body = {\n            \"title\": name,\n            \"grading_scheme_entry\": grading_scheme,\n            \"as_user_id\": creator\n        }\n\n        return GradingStandard(data=self._post_resource(url, body))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a section resource for given canvas section id.", "response": "def get_section(self, section_id, params={}):\n        \"\"\"\n        Return section resource for given canvas section id.\n\n        https://canvas.instructure.com/doc/api/sections.html#method.sections.show\n        \"\"\"\n        url = SECTIONS_API.format(section_id)\n        return CanvasSection(data=self._get_resource(url, params=params))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_section_by_sis_id(self, sis_section_id, params={}):\n        return self.get_section(\n            self._sis_id(sis_section_id, sis_field=\"section\"), params)", "response": "Get a section resource by its sis id."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_sections_in_course(self, course_id, params={}):\n        url = COURSES_API.format(course_id) + \"/sections\"\n\n        sections = []\n        for data in self._get_paged_resource(url, params=params):\n            sections.append(CanvasSection(data=data))\n\n        return sections", "response": "Get list of sections in the passed course."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a list of sections in the course given the SIS ID.", "response": "def get_sections_in_course_by_sis_id(self, sis_course_id, params={}):\n        \"\"\"\n        Return list of sections for the passed course SIS ID.\n        \"\"\"\n        return self.get_sections_in_course(\n            self._sis_id(sis_course_id, sis_field=\"course\"), params)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_sections_with_students_in_course(self, course_id, params={}):\n        include = params.get(\"include\", [])\n        if \"students\" not in include:\n            include.append(\"students\")\n        params[\"include\"] = include\n\n        return self.get_sections_in_course(course_id, params)", "response": "Returns a list of all sections in the passed course."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of sections including students in the course.", "response": "def get_sections_with_students_in_course_by_sis_id(self, sis_course_id,\n                                                       params={}):\n        \"\"\"\n        Return list of sections including students for the passed sis ID.\n        \"\"\"\n        return self.get_sections_with_students_in_course(\n            self._sis_id(sis_course_id, sis_field=\"course\"), params)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_section(self, course_id, name, sis_section_id):\n        url = COURSES_API.format(course_id) + \"/sections\"\n        body = {\"course_section\": {\"name\": name,\n                                   \"sis_section_id\": sis_section_id}}\n\n        return CanvasSection(data=self._post_resource(url, body))", "response": "Create a canvas section in the given course id."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates a canvas section with the given section id.", "response": "def update_section(self, section_id, name, sis_section_id):\n        \"\"\"\n        Update a canvas section with the given section id.\n\n        https://canvas.instructure.com/doc/api/sections.html#method.sections.update\n        \"\"\"\n        url = SECTIONS_API.format(section_id)\n        body = {\"course_section\": {}}\n\n        if name:\n            body[\"course_section\"][\"name\"] = name\n\n        if sis_section_id:\n            body[\"course_section\"][\"sis_section_id\"] = sis_section_id\n\n        return CanvasSection(data=self._put_resource(url, body))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a list of quizzes for a given course.", "response": "def get_quizzes(self, course_id):\n        \"\"\"\n        List quizzes for a given course\n\n        https://canvas.instructure.com/doc/api/quizzes.html#method.quizzes_api.index\n        \"\"\"\n        url = QUIZZES_API.format(course_id)\n        data = self._get_resource(url)\n        quizzes = []\n        for datum in data:\n            quizzes.append(Quiz(data=datum))\n        return quizzes"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn account resource for given canvas account id.", "response": "def get_account(self, account_id):\n        \"\"\"\n        Return account resource for given canvas account id.\n\n        https://canvas.instructure.com/doc/api/accounts.html#method.accounts.show\n        \"\"\"\n        url = ACCOUNTS_API.format(account_id)\n        return CanvasAccount(data=self._get_resource(url))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting list of subaccounts within the account with the passed account_id canvas id.", "response": "def get_sub_accounts(self, account_id, params={}):\n        \"\"\"\n        Return list of subaccounts within the account with the passed\n        canvas id.\n\n        https://canvas.instructure.com/doc/api/accounts.html#method.accounts.sub_accounts\n        \"\"\"\n        url = ACCOUNTS_API.format(account_id) + \"/sub_accounts\"\n\n        accounts = []\n        for datum in self._get_paged_resource(url, params=params):\n            accounts.append(CanvasAccount(data=datum))\n\n        return accounts"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_account(self, account):\n        url = ACCOUNTS_API.format(account.account_id)\n        body = {\"account\": {\"name\": account.name}}\n\n        return CanvasAccount(data=self._put_resource(url, body))", "response": "Updates the passed account. Returns the updated account."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates the SIS ID for the account identified by the passed account ID.", "response": "def update_sis_id(self, account_id, sis_account_id):\n        \"\"\"\n        Updates the SIS ID for the account identified by the passed account ID.\n\n        https://canvas.instructure.com/doc/api/accounts.html#method.accounts.update\n        \"\"\"\n        if account_id == self._canvas_account_id:\n            raise Exception(\"SIS ID cannot be updated for the root account\")\n\n        url = ACCOUNTS_API.format(account_id)\n        body = {\"account\": {\"sis_account_id\": sis_account_id}}\n\n        return CanvasAccount(data=self._put_resource(url, body))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_auth_settings(self, account_id):\n        url = ACCOUNTS_API.format(account_id) + \"/sso_settings\"\n        return CanvasSSOSettings(data=self._get_resource(url))", "response": "Returns the authentication settings for the passed account_id."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the authentication settings for the passed account_id.", "response": "def update_auth_settings(self, account_id, auth_settings):\n        \"\"\"\n        Update the authentication settings for the passed account_id.\n\n        https://canvas.instructure.com/doc/api/authentication_providers.html#method.account_authorization_configs.update_sso_settings\n        \"\"\"\n        url = ACCOUNTS_API.format(account_id) + \"/sso_settings\"\n        data = self._put_resource(url, auth_settings.json_data())\n        return CanvasSSOSettings(data=data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the settlement of a shallow foundation.", "response": "def settlement_schmertmann(sp, fd, load, youngs_modulus_soil, **kwargs):\n    \"\"\"\n    Calculates the settlement of a shallow foundation (Schmertmann, 19XX).\n\n    :param sp: Soil Profile object\n    :param fd: Foundation object\n    :param load:\n    :param youngs_modulus_soil: The Young's modulus of the soil.\n    :param kwargs:\n    :return: float, the settlement.\n    \"\"\"\n    length = float(fd.length)\n    breadth = float(fd.width)\n    depth = float(fd.depth)\n    load = float(load)\n\n    sp.gwl = kwargs.get(\"gwl\", sp.gwl)\n    sp.unit_sat_weight = kwargs.get(\"unit_sat_weight\", sp.unit_sat_weight)\n    verbose = kwargs.get(\"verbose\", 0)\n    years = kwargs.get(\"years\", 0)\n\n    q = load / (length * breadth)\n    sigma_v0_eff = (sp.unit_dry_weight * min(depth, sp.gwl) +\n                    (sp.unit_sat_weight - 9.8) * max([0, depth - sp.gwl]))\n    delta_q = q - sigma_v0_eff\n\n    # EMBEDMENT FACTOR\n    c_1 = max(1 - 0.5 * (sigma_v0_eff / delta_q), 0.5)\n    # CREEP FACTOR\n    if years == 0:\n        c_2 = 1.0\n    else:\n        c_2 = 1.0 + 0.2 * np.log10(years / 0.1)\n    # SHAPE FACTOR\n    long = max(length, breadth)\n    short = min(length, breadth)\n    c_3 = max(1.03 - 0.03 * (long / short), 0.73)\n    # Peak settlement index\n    if long / short > 10:\n        zp = short + depth\n        z_top = 0.2\n        z_bottom = 4 * short + depth\n    else:\n        z_top = 0.1\n        zp = 0.5 * short + depth\n        z_bottom = 2 * short + depth\n\n    sigma_vp_eff = (sp.unit_dry_weight * min(zp, sp.gwl) +\n                    (sp.unit_sat_weight - 9.8) * max([0, zp - sp.gwl]))\n    i_zp = 0.5 + 0.1 * (delta_q / sigma_vp_eff) ** 0.5\n\n    i_z_top = (i_zp + z_top) / 2\n    i_z_bottom = i_zp / 2\n\n    settlement = (c_1 * c_2 * c_3 * delta_q *\n                  (i_z_top * (zp - depth) + i_z_bottom * (z_bottom - zp)) / youngs_modulus_soil)\n\n    if verbose:\n        log(\"delta_q:\", delta_q)\n        log(\"c_1:\", c_1)\n        log(\"c_2:\", c_2)\n        log(\"c_3:\", c_3)\n        log(\"zp:\", zp)\n        log(\"sigma_vp_eff:\", sigma_vp_eff)\n        log(\"i_zp:\", i_zp)\n        log(\"i_z_top:\", i_z_top)\n        log(\"i_z_bottom:\", i_z_bottom)\n        log(\"settlement:\", settlement)\n    return settlement"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_all_terms(self):\n        if not self._canvas_account_id:\n            raise MissingAccountID()\n\n        params = {\"workflow_state\": 'all', 'per_page': 500}\n        url = ACCOUNTS_API.format(self._canvas_account_id) + \"/terms\"\n        data_key = 'enrollment_terms'\n\n        terms = []\n        response = self._get_paged_resource(url, params, data_key)\n        for data in response[data_key]:\n            terms.append(CanvasTerm(data=data))\n        return terms", "response": "Get all of the terms in the account."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_term_by_sis_id(self, sis_term_id):\n        for term in self.get_all_terms():\n            if term.sis_term_id == sis_term_id:\n                return term", "response": "Return a term resource for the passed SIS ID."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating an existing enrollment term with the passed overrides.", "response": "def update_term_overrides(self, sis_term_id, overrides={}):\n        \"\"\"\n        Update an existing enrollment term for the passed SIS ID.\n        https://canvas.instructure.com/doc/api/enrollment_terms.html#method.terms.update\n        \"\"\"\n        if not self._canvas_account_id:\n            raise MissingAccountID()\n\n        url = ACCOUNTS_API.format(\n            self._canvas_account_id) + \"/terms/{}\".format(\n                self._sis_id(sis_term_id, sis_field='term'))\n\n        body = {'enrollment_term': {'overrides': overrides}}\n        return CanvasTerm(data=self._put_resource(url, body))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nproduces console output. :param out_str: Output string :param o2: Additional output string :param o3: Additional output string :param o4: Additional output string :return: None", "response": "def log(out_str, o2=\"\", o3=\"\", o4=\"\"):\n    \"\"\"\n    Produces console output.\n    :param out_str: Output string\n    :param o2: Additional output string\n    :param o3: Additional output string\n    :param o4: Additional output string\n    :return: None\n    \"\"\"\n    print(out_str, o2, o3, o4)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nimporting a CSV string.", "response": "def import_str(self, csv, params={}):\n        \"\"\"\n        Imports a CSV string.\n\n        https://canvas.instructure.com/doc/api/sis_imports.html#method.sis_imports_api.create\n        \"\"\"\n        if not self._canvas_account_id:\n            raise MissingAccountID()\n\n        params[\"import_type\"] = SISImportModel.CSV_IMPORT_TYPE\n        url = SIS_IMPORTS_API.format(\n            self._canvas_account_id) + \".json{}\".format(self._params(params))\n        headers = {\"Content-Type\": \"text/csv\"}\n\n        return SISImportModel(data=self._post_resource(url, headers, csv))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nimports a directory of CSV files.", "response": "def import_dir(self, dir_path, params={}):\n        \"\"\"\n        Imports a directory of CSV files.\n\n        https://canvas.instructure.com/doc/api/sis_imports.html#method.sis_imports_api.create\n        \"\"\"\n        if not self._canvas_account_id:\n            raise MissingAccountID()\n\n        body = self._build_archive(dir_path)\n        params[\"import_type\"] = SISImportModel.CSV_IMPORT_TYPE\n        url = SIS_IMPORTS_API.format(\n            self._canvas_account_id) + \".json{}\".format(self._params(params))\n        headers = {\"Content-Type\": \"application/zip\"}\n\n        return SISImportModel(data=self._post_resource(url, headers, body))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the status of an already created SIS import.", "response": "def get_import_status(self, sis_import):\n        \"\"\"\n        Get the status of an already created SIS import.\n\n        https://canvas.instructure.com/doc/api/sis_imports.html#method.sis_imports_api.show\n        \"\"\"\n        if not self._canvas_account_id:\n            raise MissingAccountID()\n\n        url = SIS_IMPORTS_API.format(\n            self._canvas_account_id) + \"/{}.json\".format(sis_import.import_id)\n\n        return SISImportModel(data=self._get_resource(url))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _build_archive(self, dir_path):\n        zip_path = os.path.join(dir_path, \"import.zip\")\n        archive = zipfile.ZipFile(zip_path, \"w\")\n\n        for filename in CSV_FILES:\n            filepath = os.path.join(dir_path, filename)\n\n            if os.path.exists(filepath):\n                archive.write(filepath, filename, zipfile.ZIP_DEFLATED)\n\n        archive.close()\n\n        with open(zip_path, \"rb\") as f:\n            body = f.read()\n\n        return body", "response": "Builds a zip archive from the files in path."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_assignments(self, course_id):\n        url = ASSIGNMENTS_API.format(course_id)\n        data = self._get_resource(url)\n        assignments = []\n        for datum in data:\n            assignments.append(Assignment(data=datum))\n        return assignments", "response": "Get a list of assignments for a given course."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmodifying an existing assignment.", "response": "def update_assignment(self, assignment):\n        \"\"\"\n        Modify an existing assignment.\n\n        https://canvas.instructure.com/doc/api/assignments.html#method.assignments_api.update\n        \"\"\"\n        url = ASSIGNMENTS_API.format(assignment.course_id) + \"/{}\".format(\n            assignment.assignment_id)\n\n        data = self._put_resource(url, assignment.json_data())\n        return Assignment(data=data)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the list of available reports for the canvas account id.", "response": "def get_available_reports(self, account_id):\n        \"\"\"\n        Returns the list of reports for the canvas account id.\n\n        https://canvas.instructure.com/doc/api/account_reports.html#method.account_reports.available_reports\n        \"\"\"\n        url = ACCOUNTS_API.format(account_id) + \"/reports\"\n\n        report_types = []\n        for datum in self._get_resource(url):\n            report_types.append(ReportType(data=datum, account_id=account_id))\n        return report_types"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_reports_by_type(self, account_id, report_type):\n        url = ACCOUNTS_API.format(account_id) + \"/reports/{}\".format(\n            report_type)\n\n        reports = []\n        for datum in self._get_resource(url):\n            datum[\"account_id\"] = account_id\n            reports.append(Report(data=datum))\n\n        return reports", "response": "Returns a list of reports of the passed type for the passed account id."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new report instance for the given account id and term id.", "response": "def create_report(self, report_type, account_id, term_id=None, params={}):\n        \"\"\"\n        Generates a report instance for the canvas account id.\n\n        https://canvas.instructure.com/doc/api/account_reports.html#method.account_reports.create\n        \"\"\"\n        if term_id is not None:\n            params[\"enrollment_term_id\"] = term_id\n\n        url = ACCOUNTS_API.format(account_id) + \"/reports/{}\".format(\n            report_type)\n        body = {\"parameters\": params}\n\n        data = self._post_resource(url, body)\n        data[\"account_id\"] = account_id\n        return Report(data=data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_course_provisioning_report(self, account_id, term_id=None,\n                                          params={}):\n        \"\"\"\n        Convenience method for create_report, for creating a course\n        provisioning report.\n        \"\"\"\n        params[\"courses\"] = True\n        return self.create_report(ReportType.PROVISIONING, account_id, term_id,\n                                  params)", "response": "Creates a course provisioning report."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_course_sis_export_report(self, account_id, term_id=None,\n                                        params={}):\n        \"\"\"\n        Convenience method for create_report, for creating a course sis export\n        report.\n        \"\"\"\n        params[\"courses\"] = True\n        return self.create_report(ReportType.SIS_EXPORT, account_id, term_id,\n                                  params)", "response": "Convenience method for creating a course sis export report."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates an unused courses report.", "response": "def create_unused_courses_report(self, account_id, term_id=None):\n        \"\"\"\n        Convenience method for create_report, for creating an unused courses\n        report.\n        \"\"\"\n        return self.create_report(ReportType.UNUSED_COURSES, account_id,\n                                  term_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_report_data(self, report):\n        if report.report_id is None or report.status is None:\n            raise ReportFailureException(report)\n\n        interval = getattr(settings, 'CANVAS_REPORT_POLLING_INTERVAL', 5)\n        while report.status != \"complete\":\n            if report.status == \"error\":\n                raise ReportFailureException(report)\n            sleep(interval)\n            report = self.get_report_status(report)\n\n        if report.attachment is None or report.attachment.url is None:\n            return\n\n        data = self._get_report_file(report.attachment.url)\n\n        return data.split(\"\\n\")", "response": "Returns a completed report as a list of csv strings."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the status of a report.", "response": "def get_report_status(self, report):\n        \"\"\"\n        Returns the status of a report.\n\n        https://canvas.instructure.com/doc/api/account_reports.html#method.account_reports.show\n        \"\"\"\n        if (report.account_id is None or report.type is None or\n                report.report_id is None):\n            raise ReportFailureException(report)\n\n        url = ACCOUNTS_API.format(report.account_id) + \"/reports/{}/{}\".format(\n            report.type, report.report_id)\n\n        data = self._get_resource(url)\n        data[\"account_id\"] = report.account_id\n        return Report(data=data)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete_report(self, report):\n        url = ACCOUNTS_API.format(report.account_id) + \"/reports/{}/{}\".format(\n            report.type, report.report_id)\n\n        response = self._delete_resource(url)\n        return True", "response": "Deletes a generated report instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncrops an image given the top left corner.", "response": "def crop_image(img, start_y, start_x, h, w):\n    \"\"\"\n    Crop an image given the top left corner.\n    :param img: The image\n    :param start_y: The top left corner y coord\n    :param start_x: The top left corner x coord\n    :param h: The result height\n    :param w: The result width\n    :return: The cropped image.\n    \"\"\"\n    return img[start_y:start_y + h, start_x:start_x + w, :].copy()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef move_detections(label, dy, dx):\n    for k in label.keys():\n        if k.startswith(\"detection\"):\n            detections = label[k]\n            for detection in detections:\n                detection.move_image(-dx, -dy)", "response": "Moves all detections in direction dx dy."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef augment_detections(hyper_params, feature, label):\n    # Do not augment these ways:\n    # 1) Rotation is not possible\n    # 3) Scaling is not possible, because it ruins depth perception\n    # However, random crops can improve performance. (Training speed and accuracy)\n    if hyper_params.problem.get(\"augmentation\", None) is None:\n        return feature, label\n\n    img_h, img_w, img_c = feature[\"image\"].shape\n    augmented_feature = {}\n    augmented_label = {}\n    augmented_feature[\"image\"] = feature[\"image\"].copy()\n    if \"depth\" in feature:\n        augmented_feature[\"depth\"] = feature[\"depth\"].copy()\n    if \"calibration\" in feature:\n        augmented_feature[\"calibration\"] = feature[\"calibration\"]\n    augmented_feature[\"hflipped\"] = np.array([0], dtype=np.uint8)\n    augmented_feature[\"crop_offset\"] = np.array([0, 0], dtype=np.int8)\n\n    for k in label.keys():\n        augmented_label[k] = [detection.copy() for detection in label[k]]\n\n    if hyper_params.problem.augmentation.get(\"enable_horizontal_flip\", False):\n        if random.random() < 0.5:\n            img_h, img_w, img_c = augmented_feature[\"image\"].shape\n            augmented_feature[\"image\"] = np.fliplr(augmented_feature[\"image\"])\n            if \"depth\" in feature:\n                augmented_feature[\"depth\"] = np.fliplr(augmented_feature[\"depth\"])\n            augmented_feature[\"hflipped\"][0] = 1\n            hflip_detections(augmented_label, img_w)\n\n    if hyper_params.problem.augmentation.get(\"enable_micro_translation\", False):\n        img_h, img_w, img_c = augmented_feature[\"image\"].shape\n        dx = int(random.random() * 3)\n        dy = int(random.random() * 3)\n\n        augmented_feature[\"image\"] = crop_image(augmented_feature[\"image\"], dy, dx, img_h - dy, img_w - dx)\n        if \"depth\" in feature:\n            augmented_feature[\"depth\"] = crop_image(augmented_feature[\"depth\"], dy, dx, img_h - dy, img_w - dx)\n        augmented_feature[\"crop_offset\"][0] += dy\n        augmented_feature[\"crop_offset\"][1] += dx\n\n        move_detections(augmented_label, -dy, -dx)\n\n    if hyper_params.problem.augmentation.get(\"random_crop\", None) is not None:\n        img_h, img_w, img_c = augmented_feature[\"image\"].shape\n        target_w = hyper_params.problem.augmentation.random_crop.shape.width\n        target_h = hyper_params.problem.augmentation.random_crop.shape.height\n\n        delta_x = max(int(math.ceil((target_w + 1 - img_w) / 2)), 0)\n        delta_y = max(int(math.ceil((target_h + 1 - img_h) / 2)), 0)\n        move_detections(augmented_label, delta_y, delta_x)\n        augmented_feature[\"image\"] = cv2.copyMakeBorder(augmented_feature[\"image\"],\n                                                        delta_y, delta_y, delta_x, delta_x,\n                                                        cv2.BORDER_CONSTANT)\n        img_h, img_w, img_c = augmented_feature[\"image\"].shape\n\n        start_x = 0\n        start_y = 0\n        if len(augmented_label[\"detections_2d\"]) != 0:\n            idx = random.randint(0, len(augmented_label[\"detections_2d\"]) - 1)\n            detection = augmented_label[\"detections_2d\"][idx]\n            start_x = int(detection.cx - random.random() * (target_w - 20) / 2.0 - 10)\n            start_y = int(detection.cy - random.random() * (target_h - 20) / 2.0 - 10)\n        else:\n            start_x = int(img_w * random.random())\n            start_y = int(img_h * random.random())\n\n        # Compute start point so that crop fit's into image and random crop contains detection\n        if start_x < 0:\n            start_x = 0\n        if start_y < 0:\n            start_y = 0\n        if start_x >= img_w - target_w:\n            start_x = img_w - target_w - 1\n        if start_y >= img_h - target_h:\n            start_y = img_h - target_h - 1\n\n        # Crop image\n        augmented_feature[\"image\"] = crop_image(augmented_feature[\"image\"], start_y, start_x, target_h, target_w)\n        if \"depth\" in feature:\n            augmented_feature[\"depth\"] = crop_image(augmented_feature[\"depth\"], start_y, start_x, target_h, target_w)\n        augmented_feature[\"crop_offset\"][0] += start_y\n        augmented_feature[\"crop_offset\"][1] += start_x\n\n        # Crop labels\n        move_detections(augmented_label, -start_y, -start_x)\n\n    if hyper_params.problem.augmentation.get(\"enable_texture_augmentation\", False):\n        if random.random() < 0.5:\n            augmented_feature[\"image\"] = full_texture_augmentation(augmented_feature[\"image\"])\n\n    return augmented_feature, augmented_label", "response": "Augment the detection dataset."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_dict_from_obj(obj):\n    '''\n    Edit to get the dict even when the object is a GenericRelatedObjectManager.\n    Added the try except.\n    '''\n    obj_dict = obj.__dict__\n    obj_dict_result = obj_dict.copy()\n    for key, value in obj_dict.items():\n        if key.endswith('_id'):\n            key2 = key.replace('_id', '')\n            try:\n                field, model, direct, m2m = obj._meta.get_field_by_name(key2)\n                if isinstance(field, ForeignKey):\n                    obj_dict_result[key2] = obj_dict_result[key]\n                    del obj_dict_result[key]\n            except FieldDoesNotExist:\n                pass\n    manytomany_list = obj._meta.many_to_many\n    for manytomany in manytomany_list:\n        pks = [obj_rel.pk for obj_rel in manytomany.value_from_object(obj).select_related()]\n        if pks:\n            obj_dict_result[manytomany.name] = pks\n    return obj_dict_result", "response": "Get the dict representation of the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the config from the settings. py and the values given to the template tag element and complete these with the ones from the settings. py.", "response": "def get_config(self, request, **kwargs):\n        \"\"\"\n        Get the arguments given to the template tag element and complete these\n        with the ones from the settings.py if necessary.\n        \"\"\"\n        config = kwargs\n\n        config_from_settings = deepcopy(inplace_settings.DEFAULT_INPLACE_EDIT_OPTIONS)\n        config_one_by_one = inplace_settings.DEFAULT_INPLACE_EDIT_OPTIONS_ONE_BY_ONE\n\n        if not config_one_by_one:\n            # Solution 1: Using default config only if none specified.\n            if not config and config_from_settings:\n                config = config_from_settings\n        else:\n            # Solution 2: Updating the configured config with the default one.\n            config = dict(config_from_settings, **config)\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef empty_value(self):\n        '''\n        Get the text to display when the field is empty.\n        '''\n        edit_empty_value = self.config.get('edit_empty_value', False)\n        if edit_empty_value:\n            return edit_empty_value\n        else:\n            return unicode(inplace_settings.INPLACEEDIT_EDIT_EMPTY_VALUE)", "response": "Get the text to display when the field is empty."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef do_eval(parser, token):\n    \"Usage: {% eval %}1 + 1{% endeval %}\"\n\n    nodelist = parser.parse(('endeval',))\n\n    class EvalNode(template.Node):\n        def render(self, context):\n            return template.Template(nodelist.render(context)).render(template.Context(context))\n    parser.delete_first_token()\n    return EvalNode()", "response": "Usage : 1 + 1 { % eval % } 1 + 1 { % endeval % } 1 + 1 { % eval % } 1 + 1 { % endeval % } 1 + 1 { % eval % } 1 + 1 {% endeval % } 1 + 1 {% eval % } 1 + 1 {% eval % } 1 + 1 {% endeval % } 1 + 1 {% eval % } 1 + 1 {% eval % } 1"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse uniformly args and kwargs from a templatetag", "response": "def parse_args_kwargs(parser, token):\n    \"\"\"\n    Parse uniformly args and kwargs from a templatetag\n\n    Usage::\n\n      For parsing a template like this:\n\n      {% footag my_contents,height=10,zoom=20 as myvar %}\n\n      You simply do this:\n\n      @register.tag\n      def footag(parser, token):\n          args, kwargs = parse_args_kwargs(parser, token)\n    \"\"\"\n    bits = token.contents.split(' ')\n\n    if len(bits) <= 1:\n        raise template.TemplateSyntaxError(\"'%s' takes at least one argument\" % bits[0])\n\n    if token.contents[13] == '\"':\n        end_quote = token.contents.index('\"', 14) + 1\n        args = [template.Variable(token.contents[13:end_quote])]\n        kwargs_start = end_quote\n    else:\n        try:\n            next_space = token.contents.index(' ', 14)\n            kwargs_start = next_space + 1\n        except ValueError:\n            next_space = None\n            kwargs_start = None\n        args = [template.Variable(token.contents[13:next_space])]\n\n    kwargs = {}\n    kwargs_list = token.contents[kwargs_start:].split(',')\n    for kwargs_item in kwargs_list:\n        if '=' in kwargs_item:\n            k, v = kwargs_item.split('=', 1)\n            k = k.strip()\n            kwargs[k] = template.Variable(v)\n    return args, kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating and register metrics from a list of MetricConfigs.", "response": "def create_metrics(\n            self, metric_configs: Iterable[MetricConfig]) -> Dict[str, Metric]:\n        \"\"\"Create and register metrics from a list of MetricConfigs.\"\"\"\n        return self.registry.create_metrics(metric_configs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _setup_logging(self, log_level: str):\n        level = getattr(logging, log_level)\n        names = (\n            'aiohttp.access', 'aiohttp.internal', 'aiohttp.server',\n            'aiohttp.web', self.name)\n        for name in names:\n            setup_logger(name=name, stream=sys.stderr, level=level)", "response": "Setup logging for the application and aiohttp."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a PrometheusExporter configured with args.", "response": "def _get_exporter(self, args: argparse.Namespace) -> PrometheusExporter:\n        \"\"\"Return a :class:`PrometheusExporter` configured with args.\"\"\"\n        exporter = PrometheusExporter(\n            self.name, self.description, args.host, args.port, self.registry)\n        exporter.app.on_startup.append(self.on_application_startup)\n        exporter.app.on_shutdown.append(self.on_application_shutdown)\n        return exporter"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_metrics(self,\n                       configs: Iterable[MetricConfig]) -> Dict[str, Metric]:\n        \"\"\"Create Prometheus metrics from a list of MetricConfigs.\"\"\"\n        metrics: Dict[str, Metric] = {\n            config.name: self._register_metric(config)\n            for config in configs\n        }\n        self._metrics.update(metrics)\n        return metrics", "response": "Create Prometheus metrics from a list of MetricConfigs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a metric optionally configured with labels.", "response": "def get_metric(\n            self, name: str,\n            labels: Union[Dict[str, str], None] = None) -> Metric:\n        \"\"\"Return a metric, optionally configured with labels.\"\"\"\n        metric = self._metrics[name]\n        if labels:\n            return metric.labels(**labels)\n\n        return metric"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run(self):\n        run_app(\n            self.app,\n            host=self.host,\n            port=self.port,\n            print=lambda *args, **kargs: None,\n            access_log_format='%a \"%r\" %s %b \"%{Referrer}i\" \"%{User-Agent}i\"')", "response": "Run the application for the exporter."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nhomes page request handler.", "response": "async def _handle_home(self, request: Request) -> Response:\n        \"\"\"Home page request handler.\"\"\"\n        if self.description:\n            title = f'{self.name} - {self.description}'\n        else:\n            title = self.name\n\n        text = dedent(\n            f'''<!DOCTYPE html>\n            <html>\n              <head>\n                <title>{title}</title>\n              </head>\n              <body>\n                <h1>{title}</h1>\n                <p>\n                  Metric are exported at the\n                  <a href=\"/metrics\">/metrics</a> endpoint.\n                </p>\n              </body>\n            </html>\n            ''')\n        return Response(content_type='text/html', text=text)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef wa(client, event, channel, nick, rest):\n\tclient = wolframalpha.Client(pmxbot.config['Wolfram|Alpha API key'])\n\tres = client.query(rest)\n\treturn next(res.results).text", "response": "Wolfram query resolver by Wolfram|Alpha API. Returns the first result."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fix_HTTPMessage():\n\tif six.PY3:\n\t\treturn\n\n\thttp_client.HTTPMessage.get_content_type = http_client.HTTPMessage.gettype\n\thttp_client.HTTPMessage.get_param = http_client.HTTPMessage.getparam", "response": "Fixes the HTTPMessage. get_content_type and get_param methods."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nqueries Wolfram|Alpha using the v2. 0 API.", "response": "def query(self, input, params=(), **kwargs):\n        \"\"\"\n        Query Wolfram|Alpha using the v2.0 API\n\n        Allows for arbitrary parameters to be passed in\n        the query. For example, to pass assumptions:\n\n            client.query(input='pi', assumption='*C.pi-_*NamedConstant-')\n\n        To pass multiple assumptions, pass multiple items\n        as params:\n\n            params = (\n                ('assumption', '*C.pi-_*NamedConstant-'),\n                ('assumption', 'DateOrder_**Day.Month.Year--'),\n            )\n            client.query(input='pi', params=params)\n\n        For more details on Assumptions, see\n        https://products.wolframalpha.com/api/documentation.html#6\n        \"\"\"\n        data = dict(\n            input=input,\n            appid=self.app_id,\n        )\n        data = itertools.chain(params, data.items(), kwargs.items())\n\n        query = urllib.parse.urlencode(tuple(data))\n        url = 'https://api.wolframalpha.com/v2/query?' + query\n        resp = urllib.request.urlopen(url)\n        assert resp.headers.get_content_type() == 'text/xml'\n        assert resp.headers.get_param('charset') == 'utf-8'\n        return Result(resp)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef info(self):\n        return itertools.chain(self.pods, self.assumptions, self.warnings)", "response": "Returns a list of all the pods assumptions and warnings of this result."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of all the result pods that hold the response to a simple discrete query.", "response": "def results(self):\n        \"\"\"\n        The pods that hold the response to a simple, discrete query.\n        \"\"\"\n        return (\n            pod\n            for pod in self.pods\n            if pod.primary\n            or pod.title == 'Result'\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef encode(request, data):\n        if data is None:\n            return request\n\n        request.add_header('Content-Type', 'application/json')\n        request.data = json.dumps(data)\n\n        return request", "response": "Add request content data to request body set Content - type header."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef call_api(\n            self,\n            method,\n            url,\n            headers=None,\n            params=None,\n            data=None,\n            files=None,\n            timeout=None,\n    ):\n        \"\"\" Call API.\n\n        This returns object containing data, with error details if applicable.\n\n        Args:\n            method (str): The HTTP method to use.\n            url (str): Resource location relative to the base URL.\n            headers (dict or None): Extra request headers to set.\n            params (dict or None): Query-string parameters.\n            data (dict or None): Request body contents for POST or PUT requests.\n            files (dict or None: Files to be passed to the request.\n            timeout (int): Maximum time before timing out.\n\n        Returns:\n            ResultParser or ErrorParser.\n        \"\"\"\n        method = method.upper()\n        headers = deepcopy(headers) or {}\n        headers['Accept'] = self.accept_type\n        params = deepcopy(params) or {}\n        data = data or {}\n        files = files or {}\n\n        if self.username and self.api_key:\n            params.update(self.get_credentials())\n\n        url = urljoin(self.base_url, url)\n\n        r = requests.request(\n            method,\n            url,\n            headers=headers,\n            params=params,\n            files=files,\n            data=data,\n            timeout=timeout,\n        )\n\n        return r, r.status_code", "response": "Call API.\n\n        This returns object containing data, with error details if applicable.\n\n        Args:\n            method (str): The HTTP method to use.\n            url (str): Resource location relative to the base URL.\n            headers (dict or None): Extra request headers to set.\n            params (dict or None): Query-string parameters.\n            data (dict or None): Request body contents for POST or PUT requests.\n            files (dict or None: Files to be passed to the request.\n            timeout (int): Maximum time before timing out.\n\n        Returns:\n            ResultParser or ErrorParser."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(self, url, params=None, **kwargs):\n        return self.call_api(\n            \"GET\",\n            url,\n            params=params,\n            **kwargs\n        )", "response": "Call the API with a GET request."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete(self, url, params=None, **kwargs):\n        return self.call_api(\n            \"DELETE\",\n            url,\n            params=params,\n            **kwargs\n        )", "response": "Call the API with a DELETE request."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalling the API with a PUT request.", "response": "def put(self, url, params=None, data=None, files=None, **kwargs):\n        \"\"\" Call the API with a PUT request.\n\n        Args:\n            url (str): Resource location relative to the base URL.\n            params (dict or None): Query-string parameters.\n            data (dict or None): Request body contents.\n            files (dict or None: Files to be passed to the request.\n\n        Returns:\n            An instance of ResultParser or ErrorParser.\n        \"\"\"\n        return self.call_api(\n            \"PUT\",\n            url,\n            params=params,\n            data=data,\n            files=files,\n            **kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef post(self, url, params=None, data=None, files=None, **kwargs):\n        return self.call_api(\n            \"POST\",\n            url,\n            params=params,\n            data=data,\n            files=files,\n            **kwargs\n        )", "response": "Call the API with a POST request."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprocess a query and return the response.", "response": "def _process_query(self, query, prepared=False):\n        \"\"\" Process query recursively, if the text is too long,\n        it is split and processed bit a bit.\n\n        Args:\n            query (sdict): Text to be processed.\n            prepared (bool): True when the query is ready to be submitted via\n            POST request.\n        Returns:\n            str: Body ready to be submitted to the API.\n        \"\"\"\n\n        # Exit condition and POST\n        if prepared is True:\n            files = {'query': str(query)}\n\n            logger.debug('About to submit the following query {}'.format(query))\n\n            res, status = self.post(\n                self.disambiguate_service,\n                files=files,\n                headers={'Accept': 'application/json'},\n            )\n\n            if status == 200:\n                return self.decode(res), status\n            else:\n                logger.debug('Disambiguation failed.')\n                return None, status\n\n        text = query['text']\n\n        sentence_coordinates = [\n            {\n                \"offsetStart\": 0,\n                \"offsetEnd\": len(text)\n            }\n        ]\n\n        total_nb_sentences = len(sentence_coordinates)  # Sentences from text.\n        sentences_groups = []\n\n        if len(text) > self.max_text_length:\n            res, status_code = self.segment(text)\n\n            if status_code == 200:\n                sentence_coordinates = res['sentences']\n                total_nb_sentences = len(sentence_coordinates)\n            else:\n                logger.error('Error during the segmentation of the text.')\n\n            logger.debug(\n                'Text too long, split in {} sentences; building groups of {} '\n                'sentences.'.format(\n                    total_nb_sentences, self.sentences_per_group\n                )\n            )\n            sentences_groups = self._group_sentences(\n                total_nb_sentences,\n                self.sentences_per_group\n            )\n        else:\n            query['sentence'] = \"true\"\n\n        if total_nb_sentences > 1:\n            query['sentences'] = sentence_coordinates\n\n        if len(sentences_groups) > 0:\n            for group in sentences_groups:\n                query['processSentence'] = group\n\n                res, status_code = self._process_query(query, prepared=True)\n\n                if status_code == 200:\n                    if 'entities' in res:\n                        query['entities'] = res[u'entities']\n                    query['language'] = res[u'language']\n                else:\n                    logger.error(\n                        \"Error when processing the query {}\".format(query)\n                    )\n                    return None, status_code\n\n        else:\n            res, status_code = self._process_query(query, prepared=True)\n\n            if status_code == 200:\n                query['language'] = res[u'language']\n                if 'entities' in res:\n                    query['entities'] = res[u'entities']\n            else:\n                logger.error(\"Error when processing the query {}\".format(query))\n                return None, status_code\n\n        return query, status_code"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _group_sentences(total_nb_sentences, group_length):\n        sentences_groups = []\n        current_sentence_group = []\n\n        for i in range(0, total_nb_sentences):\n            if i % group_length == 0:\n                if len(current_sentence_group) > 0:\n                    sentences_groups.append(current_sentence_group)\n                current_sentence_group = [i]\n            else:\n                current_sentence_group.append(i)\n\n        if len(current_sentence_group) > 0:\n            sentences_groups.append(current_sentence_group)\n\n        return sentences_groups", "response": "Splits sentences in groups given a specific length."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndisambiguates a PDF file.", "response": "def disambiguate_pdf(self, file, language=None, entities=None):\n        \"\"\" Call the disambiguation service in order to process a pdf file .\n\n        Args:\n            pdf (file): PDF file to be disambiguated.\n            language (str): language of text (if known)\n\n        Returns:\n            dict, int: API response and API status.\n        \"\"\"\n\n        body = {\n            \"customisation\": \"generic\"\n        }\n\n        if language:\n            body['language'] = {\"lang\": language}\n\n        if entities:\n            body['entities'] = entities\n\n        files = {\n            'query': str(body),\n            'file': (\n                file,\n                open(file, 'rb'),\n                'application/pdf',\n                {'Expires': '0'}\n            )\n        }\n\n        res, status = self.post(\n            self.disambiguate_service,\n            files=files,\n            headers={'Accept': 'application/json'},\n        )\n\n        if status != 200:\n            logger.debug('Disambiguation failed with error ' + str(status))\n\n        return self.decode(res), status"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef disambiguate_terms(self, terms, language=\"en\", entities=None):\n\n        body = {\n            \"termVector\": terms,\n            \"entities\": [],\n            \"onlyNER\": \"false\",\n            \"customisation\": \"generic\"\n        }\n\n        body['language'] = {\"lang\": language}\n\n        if entities:\n            body['entities'] = entities\n\n        files = {'query': str(body)}\n\n        logger.debug('About to submit the following query {}'.format(body))\n\n        res, status = self.post(\n            self.disambiguate_service,\n            files=files,\n            headers={'Accept': 'application/json'},\n        )\n\n        if status == 200:\n            return self.decode(res), status\n        else:\n            logger.debug('Disambiguation failed.')\n            return None, status", "response": "Disambiguate the terms in the list of objects of term weight and language."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef disambiguate_text(self, text, language=None, entities=None):\n\n        body = {\n            \"text\": text,\n            \"entities\": [],\n            \"onlyNER\": \"false\",\n            \"customisation\": \"generic\"\n        }\n\n        if language:\n            body['language'] = {\"lang\": language}\n\n        if entities:\n            body['entities'] = entities\n\n        result, status_code = self._process_query(body)\n\n        if status_code != 200:\n            logger.debug('Disambiguation failed.')\n\n        return result, status_code", "response": "Disambiguate a text in the specified language and entities."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef disambiguate_query(self, query, language=None, entities=None):\n\n        body = {\n            \"shortText\": query,\n            \"entities\": [],\n            \"onlyNER\": \"false\",\n            \"customisation\": \"generic\"\n        }\n\n        if language:\n            body['language'] = {\"lang\": language}\n\n        if entities:\n            body['entities'] = entities\n\n        files = {'query': str(body)}\n\n        logger.debug('About to submit the following query {}'.format(body))\n\n        res, status = self.post(\n            self.disambiguate_service,\n            files=files,\n            headers={'Accept': 'application/json'},\n        )\n\n        if status == 200:\n            return self.decode(res), status\n        else:\n            logger.debug('Disambiguation failed.')\n            return None, status", "response": "Disambiguate a search query."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef segment(self, text):\n\n        files = {'text': text}\n        res, status_code = self.post(self.segmentation_service, files=files)\n\n        if status_code != 200:\n            logger.debug('Segmentation failed.')\n\n        return self.decode(res), status_code", "response": "Segment the text in a set of sentences."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_language(self, text):\n        files = {'text': text}\n        res, status_code = self.post(self.language_service, files=files)\n\n        if status_code != 200:\n            logger.debug('Language recognition failed.')\n\n        return self.decode(res), status_code", "response": "Recognise the language of the text in input\n getSeasons and return the result."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_concept(self, conceptId, lang='en'):\n        url = urljoin(self.concept_service + '/', conceptId)\n\n        res, status_code = self.get(url, params={'lang': lang})\n\n        if status_code != 200:\n            logger.debug('Fetch concept failed.')\n\n        return self.decode(res), status_code", "response": "Fetch the concept from the Knowledge base and return the response code."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfits the MDR ensemble from the provided training data and returns the feature map.", "response": "def fit(self, features, classes):\n        \"\"\"Constructs the MDR ensemble from the provided training data\n\n        Parameters\n        ----------\n        features: array-like {n_samples, n_features}\n            Feature matrix\n        classes: array-like {n_samples}\n            List of class labels for prediction\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n        self.ensemble.fit(features, classes)\n\n        # Construct the feature map from the ensemble predictions\n        unique_rows = list(set([tuple(row) for row in features]))\n        for row in unique_rows:\n            self.feature_map[row] = self.ensemble.predict([row])[0]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nestimates the accuracy of the predictions from the MDR ensemble and returns the score of the predictions based on the new feature.", "response": "def score(self, features, classes, scoring_function=None, **scoring_function_kwargs):\n        \"\"\"Estimates the accuracy of the predictions from the MDR ensemble\n\n        Parameters\n        ----------\n        features: array-like {n_samples, n_features}\n            Feature matrix to predict from\n        classes: array-like {n_samples}\n            List of true class labels\n\n        Returns\n        -------\n        accuracy_score: float\n            The estimated accuracy based on the constructed feature\n\n        \"\"\"\n        new_feature = self.ensemble.predict(features)\n\n        if scoring_function is None:\n            return accuracy_score(classes, new_feature)\n        else:\n            return scoring_function(classes, new_feature, **scoring_function_kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fit(self, features, class_labels):\n        unique_labels = sorted(np.unique(class_labels))\n        if len(unique_labels) != 2:\n            raise ValueError('MDR only supports binary endpoints.')\n\n        # Count the distribution of classes that fall into each MDR grid cell\n        self.class_count_matrix = defaultdict(lambda: defaultdict(int))\n        for row_i in range(features.shape[0]):\n            feature_instance = tuple(features[row_i])\n            self.class_count_matrix[feature_instance][class_labels[row_i]] += 1\n        self.class_count_matrix = dict(self.class_count_matrix)\n\n        # Only applies to binary classification\n        overall_class_fraction = float(sum(class_labels == unique_labels[0])) / class_labels.size\n\n        # If one class is more abundant in a MDR grid cell than it is overall, then assign the cell to that class\n        self.feature_map = {}\n        for feature_instance in self.class_count_matrix:\n            counts = self.class_count_matrix[feature_instance]\n            fraction = float(counts[unique_labels[0]]) / np.sum(list(counts.values()))\n            if fraction > overall_class_fraction:\n                self.feature_map[feature_instance] = unique_labels[0]\n            elif fraction == overall_class_fraction:\n                self.feature_map[feature_instance] = self.tie_break\n            else:\n                self.feature_map[feature_instance] = unique_labels[1]\n\n        return self", "response": "Constructs the MDR feature map from the provided training data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fit_transform(self, features, class_labels):\n        self.fit(features, class_labels)\n        return self.transform(features)", "response": "Convenience function that fits the provided data then constructs a new feature from the provided features."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fit_predict(self, features, class_labels):\n        self.fit(features, class_labels)\n        return self.predict(features)", "response": "Convenience function that fits the provided data then constructs predictions from the provided features."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nestimating the accuracy of the predictions from the constructed feature.", "response": "def score(self, features, class_labels, scoring_function=None, **scoring_function_kwargs):\n        \"\"\"Estimates the accuracy of the predictions from the constructed feature.\n\n        Parameters\n        ----------\n        features: array-like {n_samples, n_features}\n            Feature matrix to predict from\n        class_labels: array-like {n_samples}\n            List of true class labels\n\n        Returns\n        -------\n        accuracy_score: float\n            The estimated accuracy based on the constructed feature\n\n        \"\"\"\n        if self.feature_map is None:\n            raise ValueError('The MDR model must be fit before score can be called.')\n\n        new_feature = self.predict(features)\n\n        if scoring_function is None:\n            return accuracy_score(class_labels, new_feature)\n        else:\n            return scoring_function(class_labels, new_feature, **scoring_function_kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconstruct the Continuous MDR feature map from the provided training data.", "response": "def fit(self, features, targets):\n        \"\"\"Constructs the Continuous MDR feature map from the provided training data.\n\n        Parameters\n        ----------\n        features: array-like {n_samples, n_features}\n            Feature matrix\n        targets: array-like {n_samples}\n            List of target values for prediction\n\n        Returns\n        -------\n        self: A copy of the fitted model\n\n        \"\"\"\n        self.feature_map = defaultdict(lambda: self.default_label)\n        self.overall_mean_trait_value = np.mean(targets)\n        self.mdr_matrix_values = defaultdict(list)\n\n        for row_i in range(features.shape[0]):\n            feature_instance = tuple(features[row_i])\n            self.mdr_matrix_values[feature_instance].append(targets[row_i])\n\n        for feature_instance in self.mdr_matrix_values:\n            grid_mean_trait_value = np.mean(self.mdr_matrix_values[feature_instance])\n            if grid_mean_trait_value > self.overall_mean_trait_value: \n                self.feature_map[feature_instance] = 1\n            elif grid_mean_trait_value == self.overall_mean_trait_value:\n                self.feature_map[feature_instance] = self.tie_break\n            else:\n                self.feature_map[feature_instance] = 0\n\n        # Convert defaultdict to dict so CMDR objects can be easily pickled\n        self.feature_map = dict(self.feature_map)\n        self.mdr_matrix_values = dict(self.mdr_matrix_values)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef transform(self, features):\n        new_feature = np.zeros(features.shape[0], dtype=np.int)\n\n        for row_i in range(features.shape[0]):\n            feature_instance = tuple(features[row_i])\n            if feature_instance in self.feature_map:\n                new_feature[row_i] = self.feature_map[feature_instance]\n            else:\n                new_feature[row_i] = self.default_label\n\n        return new_feature.reshape(features.shape[0], 1)", "response": "Uses the Continuous MDR feature map to construct a new feature from the provided features."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nestimate the quality of the ContinuousMDR model using a t - statistic.", "response": "def score(self, features, targets):\n        \"\"\"Estimates the quality of the ContinuousMDR model using a t-statistic.\n\n        Parameters\n        ----------\n        features: array-like {n_samples, n_features}\n            Feature matrix to predict from\n        targets: array-like {n_samples}\n            List of true target values\n\n        Returns\n        -------\n        quality_score: float\n            The estimated quality of the Continuous MDR model\n\n        \"\"\"\n        if self.feature_map is None:\n            raise ValueError('The Continuous MDR model must be fit before score() can be called.')\n\n        group_0_trait_values = []\n        group_1_trait_values = []\n\n        for feature_instance in self.feature_map:\n            if self.feature_map[feature_instance] == 0:\n                group_0_trait_values.extend(self.mdr_matrix_values[feature_instance])\n            else:\n                group_1_trait_values.extend(self.mdr_matrix_values[feature_instance])\n\n        return abs(ttest_ind(group_0_trait_values, group_1_trait_values).statistic)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the entropy H ( X ) in the given base", "response": "def entropy(X, base=2):\n    \"\"\"Calculates the entropy, H(X), in the given base\n\n    Parameters\n    ----------\n    X: array-like (# samples)\n        An array of values for which to compute the entropy\n    base: integer (default: 2)\n        The base in which to calculate entropy\n\n    Returns\n    ----------\n    entropy: float\n        The entropy calculated according to the equation H(X) = -sum(p_x * log p_x) for all states of X\n\n    \"\"\"\n    return scipy.stats.entropy(list(Counter(X).values()), base=base)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef joint_entropy(X, Y, base=2):\n    X_Y = ['{}{}'.format(x, y) for x, y in zip(X, Y)]\n    return entropy(X_Y, base=base)", "response": "Calculates the joint entropy H ( X Y ) in the given base\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef conditional_entropy(X, Y, base=2):\n    return joint_entropy(X, Y, base=base) - entropy(Y, base=base)", "response": "Calculates the conditional entropy H ( X | Y ) in the given base\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the mutual information between two variables I ( X ; Y ) in the given base", "response": "def mutual_information(X, Y, base=2):\n    \"\"\"Calculates the mutual information between two variables, I(X;Y), in the given base\n\n    Parameters\n    ----------\n    X: array-like (# samples)\n        An array of values for which to compute the mutual information\n    Y: array-like (# samples)\n        An array of values for which to compute the mutual information\n    base: integer (default: 2)\n        The base in which to calculate mutual information\n\n    Returns\n    ----------\n    mutual_information: float\n        The mutual information calculated according to the equation I(X;Y) = H(Y) - H(Y|X)\n\n    \"\"\"\n    return entropy(Y, base=base) - conditional_entropy(Y, X, base=base)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef two_way_information_gain(X, Y, Z, base=2):\n    X_Y = ['{}{}'.format(x, y) for x, y in zip(X, Y)]\n    return (mutual_information(X_Y, Z, base=base) -\n            mutual_information(X, Z, base=base) -\n            mutual_information(Y, Z, base=base))", "response": "Calculates the two - way information gain between two variables I X Y and Z."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the three - way information gain between two variables I ( W X Y Z ) and I ( W ; X Y Z ) in the given base", "response": "def three_way_information_gain(W, X, Y, Z, base=2):\n    \"\"\"Calculates the three-way information gain between three variables, I(W;X;Y;Z), in the given base\n\n    IG(W;X;Y;Z) indicates the information gained about variable Z by the joint variable W_X_Y, after removing\n    the information that W, X, and Y have about Z individually and jointly in pairs. Thus, 3-way information gain\n    measures the synergistic predictive value of variables W, X, and Y about variable Z.\n\n    Parameters\n    ----------\n    W: array-like (# samples)\n        An array of values for which to compute the 3-way information gain\n    X: array-like (# samples)\n        An array of values for which to compute the 3-way information gain\n    Y: array-like (# samples)\n        An array of values for which to compute the 3-way information gain\n    Z: array-like (# samples)\n        An array of outcome values for which to compute the 3-way information gain\n    base: integer (default: 2)\n        The base in which to calculate 3-way information\n\n    Returns\n    ----------\n    mutual_information: float\n        The information gain calculated according to the equation:\n            IG(W;X;Y;Z) = I(W,X,Y;Z) - IG(W;X;Z) - IG(W;Y;Z) - IG(X;Y;Z) - I(W;Z) - I(X;Z) - I(Y;Z)\n\n    \"\"\"\n    W_X_Y = ['{}{}{}'.format(w, x, y) for w, x, y in zip(W, X, Y)]\n    return (mutual_information(W_X_Y, Z, base=base) -\n            two_way_information_gain(W, X, Z, base=base) -\n            two_way_information_gain(W, Y, Z, base=base) -\n            two_way_information_gain(X, Y, Z, base=base) -\n            mutual_information(W, Z, base=base) -\n            mutual_information(X, Z, base=base) -\n            mutual_information(Y, Z, base=base))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _mdr_predict(X, Y, labels):\n    return MDR().fit_predict(np.column_stack((X, Y)), labels)", "response": "Fits a MDR model to variables X and Y with the given labels then returns the predictions of the MDR model to the variables X and Y with the given labels."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mdr_entropy(X, Y, labels, base=2):\n    return entropy(_mdr_predict(X, Y, labels), base=base)", "response": "Calculates the MDR entropy H ( XY ) in the given base base"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the MDR conditional entropy of the given variables X and Y and returns the MDR conditional entropy of the resulting model.", "response": "def mdr_conditional_entropy(X, Y, labels, base=2):\n    \"\"\"Calculates the MDR conditional entropy, H(XY|labels), in the given base\n\n    MDR conditional entropy is calculated by combining variables X and Y into a single MDR model then calculating\n    the entropy of the resulting model's predictions conditional on the provided labels.\n\n    Parameters\n    ----------\n    X: array-like (# samples)\n        An array of values corresponding to one feature in the MDR model\n    Y: array-like (# samples)\n        An array of values corresponding to one feature in the MDR model\n    labels: array-like (# samples)\n        The class labels corresponding to features X and Y\n    base: integer (default: 2)\n        The base in which to calculate MDR conditional entropy\n\n    Returns\n    ----------\n    mdr_conditional_entropy: float\n        The MDR conditional entropy calculated according to the equation H(XY|labels) = H(XY,labels) - H(labels)\n\n    \"\"\"\n    return conditional_entropy(_mdr_predict(X, Y, labels), labels, base=base)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mdr_mutual_information(X, Y, labels, base=2):\n    return mutual_information(_mdr_predict(X, Y, labels), labels, base=base)", "response": "Calculates the MDR mutual information between two variables X and Y."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef n_way_models(mdr_instance, X, y, n=[2], feature_names=None):\n    if feature_names is None:\n        feature_names = list(range(X.shape[1]))\n\n    for cur_n in n:\n        for features in itertools.combinations(range(X.shape[1]), cur_n):\n            mdr_model = copy.deepcopy(mdr_instance)\n            mdr_model.fit(X[:, features], y)\n            mdr_model_score = mdr_model.score(X[:, features], y)\n            model_features = [feature_names[feature] for feature in features]\n            yield mdr_model, mdr_model_score, model_features", "response": "Returns a generator that yields the n - way models for each feature in X."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_mdr_grid(mdr_instance):\n    var1_levels = list(set([variables[0] for variables in mdr_instance.feature_map]))\n    var2_levels = list(set([variables[1] for variables in mdr_instance.feature_map]))\n    max_count = np.array(list(mdr_instance.class_count_matrix.values())).flatten().max()\n\n    \"\"\"\n    TODO:\n        - Add common axis labels\n        - Make sure this scales for smaller and larger record sizes\n        - Extend to 3-way+ models, e.g., http://4.bp.blogspot.com/-vgKCjEkWFUc/UPwPuHo6XvI/AAAAAAAAAE0/fORHqDcoikE/s1600/model.jpg\n    \"\"\"\n\n    fig, splots = plt.subplots(ncols=len(var1_levels), nrows=len(var2_levels), sharey=True, sharex=True)\n    fig.set_figwidth(6)\n    fig.set_figheight(6)\n\n    for (var1, var2) in itertools.product(var1_levels, var2_levels):\n        class_counts = mdr_instance.class_count_matrix[(var1, var2)]\n        splot = splots[var2_levels.index(var2)][var1_levels.index(var1)]\n        splot.set_yticks([])\n        splot.set_xticks([])\n        splot.set_ylim(0, max_count * 1.5)\n        splot.set_xlim(-0.5, 1.5)\n\n        if var2_levels.index(var2) == 0:\n            splot.set_title('X1 = {}'.format(var1), fontsize=12)\n        if var1_levels.index(var1) == 0:\n            splot.set_ylabel('X2 = {}'.format(var2), fontsize=12)\n\n        bars = splot.bar(left=range(class_counts.shape[0]),\n                         height=class_counts, width=0.5,\n                         color='black', align='center')\n\n        bgcolor = 'lightgrey' if mdr_instance.feature_map[(var1, var2)] == 0 else 'darkgrey'\n        splot.set_axis_bgcolor(bgcolor)\n        for index, bar in enumerate(bars):\n            splot.text(index, class_counts[index] + (max_count * 0.1), class_counts[index], ha='center')\n\n    fig.tight_layout()\n    return fig", "response": "Visualizes the MDR grid of a given fitted MDR instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef migrate(migrations_root):\n    from flask_migrate import Migrate, upgrade as migrate_upgrade\n    from flask_sqlalchemy import SQLAlchemy\n    from sqlalchemy.engine.url import make_url\n    from sqlalchemy_utils import database_exists, create_database\n\n    db = SQLAlchemy()\n    dsn = make_url(app.config['SQLALCHEMY_DATABASE_URI'])\n    if not database_exists(dsn):\n        create_database(dsn)\n        pass\n\n    migrations_root = migrations_root or os.path.join(\n        os.environ.get('FANTASY_MIGRATION_PATH',\n                       os.getcwd()),\n        'migrations')\n    migrations_root = os.path.expanduser(migrations_root)\n\n    if os.path.exists(migrations_root):\n        mig = Migrate(app, db, directory=migrations_root)\n        mig.init_app(app, db)\n        migrate_upgrade(migrations_root)\n    else:\n        click.echo(\n            click.style('migration files not exist,skip migrate...', fg='red'))\n        sys.exit(-1)\n    pass", "response": "\u7b49\u4ef7\u4e8e django migrate \u64cd\u4f5c"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild a new resource tree for the given hive. txt file.", "response": "def requirements(work_dir, hive_root, with_requirements,\n                 with_dockerfile, active_module, active_module_file):\n    \"\"\"\u7f16\u8bd1\u5168\u65b0\u4f9d\u8d56\u6587\u4ef6\"\"\"\n    import sys\n    sys.path.insert(0, hive_root)\n\n    hive_root = os.path.abspath(os.path.expanduser(hive_root))\n\n    work_dir = work_dir or os.path.join(\n        os.environ.get('FANTASY_APP_PATH',\n                       os.getcwd()))\n\n    work_dir = os.path.expanduser(work_dir)\n\n    requirements_root = os.path.join(work_dir, 'requirements')\n    migrate_root = os.path.join(work_dir, 'migrations')\n\n    # active_modules \u4e25\u683c\u6309\u7167\u987a\u5e8f\n    active_module_paths = []\n    active_module_list = []\n\n    if active_module_file:\n        with open(active_module_file, 'r') as fp:\n            for l in fp:\n                pkg = l.split('#')[0].strip()\n                if pkg:\n                    active_module_list.append(l.strip(\"\\n\"))\n            pass\n\n    active_module_list += active_module\n\n    for m in active_module_list:\n        try:\n            mod = importlib.import_module(m)\n            active_module_paths.append(os.path.dirname(mod.__file__))\n        except ImportError:\n            click.echo('module \"%s\" not found.' % m, color=\"yellow\")\n            pass\n        pass\n\n    def build_requirements():\n        \"\"\"\u6784\u9020requirements\u6587\u4ef6\n\n        requirements\u6587\u4ef6\u5171\u5206\u4e3a\u4e24\u4efd\uff1a\n\n        - hive.txt  \u4ecehive\u9879\u76ee\u4e2d\u76f4\u63a5\u590d\u5236\n        - hive-modules.txt \u4ece\u6307\u5b9a\u7684\u6a21\u5757\u4e2d\u88c5\u8f7d\u4f9d\u8d56\u9879\n\n        .. note::\n            requirements\u8981\u6c42\u5fc5\u987b\u662f\u987a\u5e8f\u65e0\u5173\u7684\n            \u56e0\u4e3a\u6211\u4eec\u4f1a\u4f7f\u7528set\u6765\u53bb\u91cd\uff0c\u5e76\u6309\u7167value\u6392\u5e8f\n\n        \"\"\"\n        if not os.path.exists(requirements_root):\n            os.makedirs(requirements_root)\n            pass\n\n        click.echo(click.style(\"Generate hive requirements...\", fg=\"yellow\"))\n\n        shutil.copy(\n            os.path.join(hive_root, 'requirements.txt'),\n            os.path.join(requirements_root, 'hive.txt')\n        )\n\n        click.echo(click.style(\"Generate hive-module requirements...\",\n                               fg=\"yellow\"))\n\n        requirements_files = []\n\n        for m in active_module_paths:\n            t = os.path.join(m, 'requirements.txt')\n            if os.path.exists(t):\n                requirements_files.append(t)\n            pass\n\n        module_packages = set()\n        with fileinput.input(requirements_files) as fp:\n            for line in fp:\n                pkg = line.split('#')[0].strip()\n                if pkg:\n                    module_packages.add(pkg)\n            pass\n\n        with click.open_file(\n                os.path.join(requirements_root, 'hive-modules.txt'),\n                'w') as fp:\n            for p in module_packages:\n                fp.write(\"%s\\n\" % p)\n            pass\n        pass\n\n    def build_dockerfile():\n        \"\"\"\u6784\u9020Dockerfile\"\"\"\n\n        modules_in_hive = map(\n            lambda x: x.replace(hive_root, '').lstrip('/'),\n            filter(lambda x: x.startswith(hive_root),\n                   active_module_paths))\n\n        modules_path = ' '.join(modules_in_hive)\n\n        docker_file = os.path.join(\n            os.path.dirname(requirements_root),\n            'Dockerfile'\n        )\n\n        # update Dockerfile\n        if os.path.exists(docker_file):\n            click.echo(click.style(\"Found Dockerfile,try update...\",\n                                   fg=\"yellow\"))\n\n            with open(docker_file, 'r') as fp:\n                buffer = fp.read()\n                pass\n            import re\n            replaced = re.sub('ARG HIVE_PACKAGES=\".*\"',\n                              'ARG HIVE_PACKAGES=\"%s\"' % modules_path,\n                              buffer)\n            with open(docker_file, 'w') as fp:\n                fp.write(replaced)\n                pass\n\n            pass\n        pass\n\n    def build_migrations():\n        models_pairs = filter(\n            lambda pair: os.path.exists(pair[0]),\n            map(lambda x: (os.path.join(x[0], 'models.py'), x[1]),\n                [(v, active_module_list[i]) for i, v in\n                 enumerate(active_module_paths)]))\n\n        try:\n            _, models = zip(*models_pairs)\n        except ValueError:\n            click.echo(click.style(\"No models found,\"\n                                   \"is it include in \"\n                                   \"your PYTHONPATH?\\n\"\n                                   \"Modules: %s\" %\n                                   ','.join(active_module_list),\n                                   fg=\"yellow\"))\n            return\n\n        click.echo(click.style(\"Found models.txt,try update...\",\n                               fg=\"yellow\"))\n\n        with open(os.path.join(migrate_root, 'models.txt'), 'w') as fp:\n            for p in models:\n                fp.write(\"%s\\n\" % p)\n            pass\n        pass\n\n    def build_tasks():\n        tasks_pairs = filter(\n            lambda pair: os.path.exists(pair[0]),\n            map(lambda x: (os.path.join(x[0], 'tasks.py'), x[1]),\n                [(v, active_module_list[i]) for i, v in\n                 enumerate(active_module_paths)]))\n\n        try:\n            _, tasks = zip(*tasks_pairs)\n        except ValueError:\n            click.echo(click.style(\"No tasks found,\"\n                                   \"is it include in \"\n                                   \"your PYTHONPATH?\\n\"\n                                   \"Modules: %s\" %\n                                   ','.join(active_module_list),\n                                   fg=\"yellow\"))\n            return\n\n        click.echo(click.style(\"Found tasks.txt,try update...\",\n                               fg=\"yellow\"))\n        with open(os.path.join(migrate_root, 'tasks.txt'), 'w') as fp:\n            for p in tasks:\n                fp.write(\"%s\\n\" % p)\n        pass\n\n    if with_requirements:\n        build_requirements()\n\n    if with_dockerfile:\n        build_dockerfile()\n\n    if os.path.exists(migrate_root):\n        build_migrations()\n\n    if os.path.exists(migrate_root):\n        build_tasks()\n\n    click.echo(click.style(\"Generate done...\", fg=\"yellow\"))\n    pass"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a smart database if it doesn t exist", "response": "def smart_database(app):\n    \"\"\"\u5c1d\u8bd5\u5bf9\u6570\u636e\u5e93\u505a\u521d\u59cb\u5316\u64cd\u4f5c\"\"\"\n\n    from sqlalchemy.engine.url import make_url\n    from sqlalchemy_utils import database_exists, create_database\n\n    # \u5982\u679c\u6570\u636e\u5e93\u4e0d\u5b58\u5728\uff0c\u5219\u5c1d\u8bd5\u521b\u5efa\u6570\u636e\n    dsn = make_url(app.config['SQLALCHEMY_DATABASE_URI'])\n    if not database_exists(dsn):\n        create_database(dsn)\n        pass\n    pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef smart_migrate(app, migrations_root):\n\n    db = app.db\n    if os.path.exists(migrations_root) and \\\n            os.environ['FANTASY_PRIMARY_NODE'] != 'no':\n        from flask_migrate import (Migrate,\n                                   upgrade as migrate_upgrade)\n\n        migrate = Migrate(app, db, directory=migrations_root)\n        migrate.init_app(app, db)\n        migrate_upgrade(migrations_root)\n        pass\n    pass", "response": "smart_migrate - \u5982\u679c\u5b58\u5728 migration\u4e14\u6307\u5b9a\u4e3a primary_node\u5219\u6267\u884c migrate\u64cd\u4f5c"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_tasks(app, entry_file=None):\n    from celery import Task\n    tasks_txt = os.path.join(os.path.dirname(entry_file), 'migrations',\n                             'tasks.txt')\n\n    if not os.path.exists(tasks_txt):\n        import sys\n        print('Tasks file not found:%s' % tasks_txt)\n        sys.exit(-1)\n\n    class ContextTask(Task):\n        abstract = True\n\n        def __call__(self, *args, **kwargs):\n            with app.app_context():\n                return super().__call__(*args, **kwargs)\n\n    app.celery.config_from_object(app.config, namespace='CELERY')\n    app.celery.Task = ContextTask\n\n    with app.app_context():\n        with open(tasks_txt, 'r') as f:\n            for line in f:\n                mod = line.strip('\\n')\n                if mod:\n                    importlib.import_module(mod + '.tasks')\n                pass\n            pass\n        pass\n    pass", "response": "Load tasks from a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_app(app_name, config={}, db=None, celery=None):\n\n    track_mode = os.environ['FANTASY_TRACK_MODE'] == 'yes'\n    if track_mode:\n        print('(00/14)fantasy track mode active...')\n\n    active_db = os.environ['FANTASY_ACTIVE_DB'] == 'yes'\n\n    if track_mode:\n        print('(01/14)hacking webargs...')\n\n    from webargs.flaskparser import parser\n    from . import error_handler, hacker, cli\n\n    hacker.hack_webargs()\n\n    migrations_root = os.path.join(\n        os.environ.get('FANTASY_MIGRATION_PATH',\n                       os.getcwd()),\n        'migrations')\n\n    if track_mode:\n        print('(02/14)initial app...')\n\n    mod = importlib.import_module(app_name)\n    app = FantasyFlask(__name__, root_path=os.path.dirname(mod.__file__))\n\n    if track_mode:\n        print('(03/14)update app.config...')\n\n    if config:\n        app.config.update(config)\n\n    # \u7531\u5916\u90e8\u505a\u663e\u5f0f\u58f0\u660e\uff0c\u5426\u5219\u4e0d\u505a\u8c03\u7528\n    config_module = os.environ.get('FANTASY_SETTINGS_MODULE', None)\n    if track_mode:\n        print(\"       found config module %s,try load it...\" % config_module)\n\n    if config_module:\n        app.config.from_object(config_module)\n\n    if track_mode:\n        print('(04/14)confirm celery...')\n\n    if celery:\n        app.celery = celery\n        pass\n\n    if track_mode:\n        print('(05/14)bind app context...')\n\n    with app.app_context():\n\n        if track_mode:\n            print('(06/14)confirm db handle...')\n\n        if db is None:\n            global _db\n            app.db = _db\n        else:\n            app.db = db\n\n        if track_mode:\n            print('(07/14)confirm cache...')\n\n        if os.environ['FANTASY_ACTIVE_CACHE'] != 'no':\n            from flask_caching import Cache\n            app.cache = Cache(app, config=app.config)\n            pass\n\n        if track_mode:\n            print('(08/14)confirm sentry...')\n\n        if os.environ.get('FANTASY_ACTIVE_SENTRY') != 'no':\n            from raven.contrib.flask import Sentry\n            Sentry(app)\n            pass\n\n        if track_mode:\n            print('(09/14)active app...')\n\n        if hasattr(mod, 'run_app'):\n            run_app = getattr(mod, 'run_app')\n\n            try:\n                run_app(app)\n            except Exception as e:\n                if hasattr(app, 'sentry'):\n                    app.sentry.handle_exception(e)\n                    pass\n\n                import sys\n                import traceback\n                traceback.print_exc()\n                sys.exit(-1)\n\n            pass\n\n        if active_db and app.db:\n            if track_mode:\n                print('(10/14)trigger auto migrate...')\n\n            smart_database(app)\n            smart_migrate(app, migrations_root)\n            smart_account(app)\n            app.db.init_app(app)\n\n            @app.teardown_request\n            def session_clear(exception=None):\n                if exception and app.db.session.is_active:\n                    app.db.session.rollback()\n                    app.db.session.remove()\n\n            pass\n\n        if track_mode:\n            print('(11/14)bind error handle...')\n\n        # \u6dfb\u52a0\u9519\u8bef\u63a7\u5236\n        @parser.error_handler\n        def h_webargs(error):\n            return error_handler.webargs_error(error)\n\n        @app.errorhandler(422)\n        def h_422(error):\n            return error_handler.http422(error)\n\n        @app.errorhandler(500)\n        def h_500(error):\n            return error_handler.http500(error)\n\n        if hasattr(mod, 'error_handler'):\n            error_handle = getattr(mod, 'error_handle')\n            error_handle(app)\n            pass\n\n        if track_mode:\n            print('(12/14)bind admin handle...')\n\n        if hasattr(mod, 'run_admin'):\n            import flask_admin\n            admin = flask_admin.Admin(name=os.environ.get('FANTASY_ADMIN_NAME',\n                                                          'Admin'),\n                                      template_mode=os.environ.get(\n                                          'FANTASY_ADMIN_TEMPLATE_MODE',\n                                          'bootstrap3'))\n\n            run_admin = getattr(mod, 'run_admin')\n            run_admin(admin)\n            admin.init_app(app)\n            pass\n\n        pass\n\n    if track_mode:\n        print('(13/14)bind ff command...')\n\n    app.cli.add_command(cli.ff)\n\n    if track_mode:\n        print('(14/14)bind cli command...')\n\n    if hasattr(mod, 'run_cli'):\n        run_cli = getattr(mod, 'run_cli')\n        run_cli(app)\n        pass\n\n    return app", "response": "create an app with the given name"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_config(app, prefix='hive_'):\n    items = app.config.items()\n    prefix = prefix.upper()\n\n    def strip_prefix(tup):\n        return (tup[0].replace(prefix, ''), tup[1])\n\n    return dict([strip_prefix(i) for i in items if i[0].startswith(prefix)])", "response": "Conveniently get the security configuration for the specified application without the annoying SECURITY_ prefix."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a Flask - Security configuration value.", "response": "def config_value(key, app=None, default=None, prefix='hive_'):\n    \"\"\"Get a Flask-Security configuration value.\n\n    :param key: The configuration key without the prefix `SECURITY_`\n    :param app: An optional specific application to inspect. Defaults to\n                Flask's `current_app`\n    :param default: An optional default value if the value is not set\n    \"\"\"\n    app = app or current_app\n    return get_config(app, prefix=prefix).get(key.upper(), default)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new vector with the given members.", "response": "def vector(members: Iterable[T], meta: Optional[IPersistentMap] = None) -> Vector[T]:\n    \"\"\"Creates a new vector.\"\"\"\n    return Vector(pvector(members), meta=meta)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef v(*members: T, meta: Optional[IPersistentMap] = None) -> Vector[T]:\n    return Vector(pvector(members), meta=meta)", "response": "Creates a new vector from members."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef eval_file(filename: str, ctx: compiler.CompilerContext, module: types.ModuleType):\n    last = None\n    for form in reader.read_file(filename, resolver=runtime.resolve_alias):\n        last = compiler.compile_and_exec_form(form, ctx, module)\n    return last", "response": "Evaluate a file with the given name into a Python module AST node."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nevaluate the forms in stdin into a Python module AST node.", "response": "def eval_stream(stream, ctx: compiler.CompilerContext, module: types.ModuleType):\n    \"\"\"Evaluate the forms in stdin into a Python module AST node.\"\"\"\n    last = None\n    for form in reader.read(stream, resolver=runtime.resolve_alias):\n        last = compiler.compile_and_exec_form(form, ctx, module)\n    return last"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef eval_str(s: str, ctx: compiler.CompilerContext, module: types.ModuleType, eof: Any):\n    last = eof\n    for form in reader.read_str(s, resolver=runtime.resolve_alias, eof=eof):\n        last = compiler.compile_and_exec_form(form, ctx, module)\n    return last", "response": "Evaluate the forms in a string into a Python module AST node."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bootstrap_repl(which_ns: str) -> types.ModuleType:\n    repl_ns = runtime.Namespace.get_or_create(sym.symbol(\"basilisp.repl\"))\n    ns = runtime.Namespace.get_or_create(sym.symbol(which_ns))\n    repl_module = importlib.import_module(\"basilisp.repl\")\n    ns.add_alias(sym.symbol(\"basilisp.repl\"), repl_ns)\n    ns.refer_all(repl_ns)\n    return repl_module", "response": "Bootstrap the REPL with a few useful vars and returned the REPL\n    bootstrapped module so it s functions can be used by the REPL\n    command."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun a Basilisp script or a line of code.", "response": "def run(  # pylint: disable=too-many-arguments\n    file_or_code,\n    code,\n    in_ns,\n    use_var_indirection,\n    warn_on_shadowed_name,\n    warn_on_shadowed_var,\n    warn_on_var_indirection,\n):\n    \"\"\"Run a Basilisp script or a line of code, if it is provided.\"\"\"\n    basilisp.init()\n    ctx = compiler.CompilerContext(\n        filename=CLI_INPUT_FILE_PATH\n        if code\n        else (\n            STDIN_INPUT_FILE_PATH if file_or_code == STDIN_FILE_NAME else file_or_code\n        ),\n        opts={\n            compiler.WARN_ON_SHADOWED_NAME: warn_on_shadowed_name,\n            compiler.WARN_ON_SHADOWED_VAR: warn_on_shadowed_var,\n            compiler.USE_VAR_INDIRECTION: use_var_indirection,\n            compiler.WARN_ON_VAR_INDIRECTION: warn_on_var_indirection,\n        },\n    )\n    eof = object()\n\n    with runtime.ns_bindings(in_ns) as ns:\n        if code:\n            print(runtime.lrepr(eval_str(file_or_code, ctx, ns.module, eof)))\n        elif file_or_code == STDIN_FILE_NAME:\n            print(\n                runtime.lrepr(\n                    eval_stream(click.get_text_stream(\"stdin\"), ctx, ns.module)\n                )\n            )\n        else:\n            print(runtime.lrepr(eval_file(file_or_code, ctx, ns.module)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef multifn(dispatch: DispatchFunction, default=None) -> MultiFunction[T]:\n    name = sym.symbol(dispatch.__qualname__, ns=dispatch.__module__)\n    return MultiFunction(name, dispatch, default)", "response": "Decorator function which can be used to make Python multi functions."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __add_method(m: lmap.Map, key: T, method: Method) -> lmap.Map:\n        return m.assoc(key, method)", "response": "Swap the methods atom to include method with key."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a new method to this function which will respond for key returned from the dispatch function.", "response": "def add_method(self, key: T, method: Method) -> None:\n        \"\"\"Add a new method to this function which will respond for\n        key returned from the dispatch function.\"\"\"\n        self._methods.swap(MultiFunction.__add_method, key, method)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_method(self, key: T) -> Optional[Method]:\n        method_cache = self.methods\n        # The 'type: ignore' comment below silences a spurious MyPy error\n        # about having a return statement in a method which does not return.\n        return Maybe(method_cache.entry(key, None)).or_else(\n            lambda: method_cache.entry(self._default, None)  # type: ignore\n        )", "response": "Return the method which would handle this dispatch key or\n            None if no method defined for this dispatch key and no default."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nswap the methods atom to remove method with key.", "response": "def __remove_method(m: lmap.Map, key: T) -> lmap.Map:\n        \"\"\"Swap the methods atom to remove method with key.\"\"\"\n        return m.dissoc(key)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves the method defined for this key and return it.", "response": "def remove_method(self, key: T) -> Optional[Method]:\n        \"\"\"Remove the method defined for this key and return it.\"\"\"\n        method = self.methods.entry(key, None)\n        if method:\n            self._methods.swap(MultiFunction.__remove_method, key)\n        return method"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True if the meta contains async keyword.", "response": "def _is_async(o: IMeta) -> bool:\n    \"\"\"Return True if the meta contains :async keyword.\"\"\"\n    return (  # type: ignore\n        Maybe(o.meta)\n        .map(lambda m: m.entry(SYM_ASYNC_META_KEY, None))\n        .or_else_get(False)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _is_macro(v: Var) -> bool:\n    return (\n        Maybe(v.meta)\n        .map(lambda m: m.entry(SYM_MACRO_META_KEY, None))  # type: ignore\n        .or_else_get(False)\n    )", "response": "Return True if the Var holds a macro function."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _loc(form: Union[LispForm, ISeq]) -> Optional[Tuple[int, int]]:\n    try:\n        meta = form.meta  # type: ignore\n        line = meta.get(reader.READER_LINE_KW)  # type: ignore\n        col = meta.get(reader.READER_COL_KW)  # type: ignore\n    except AttributeError:\n        return None\n    else:\n        assert isinstance(line, int) and isinstance(col, int)\n        return line, col", "response": "Fetch the location of the form in the original filename from the the\n    input form if it has metadata."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _with_loc(f: ParseFunction):\n\n    @wraps(f)\n    def _parse_form(ctx: ParserContext, form: Union[LispForm, ISeq]) -> Node:\n        form_loc = _loc(form)\n        if form_loc is None:\n            return f(ctx, form)\n        else:\n            return f(ctx, form).fix_missing_locations(form_loc)\n\n    return _parse_form", "response": "Attach any available location information from the input form to\n    the node environment returned from the parsing function returned from the parsing function."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves reader metadata from the form s meta map.", "response": "def _clean_meta(meta: Optional[lmap.Map]) -> Optional[lmap.Map]:\n    \"\"\"Remove reader metadata from the form's meta map.\"\"\"\n    if meta is None:\n        return None\n    else:\n        new_meta = meta.dissoc(reader.READER_LINE_KW, reader.READER_COL_KW)\n        return None if len(new_meta) == 0 else new_meta"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _with_meta(gen_node):\n\n    @wraps(gen_node)\n    def with_meta(\n        ctx: ParserContext,\n        form: Union[llist.List, lmap.Map, ISeq, lset.Set, vec.Vector],\n    ) -> Node:\n        assert not ctx.is_quoted, \"with-meta nodes are not used in quoted expressions\"\n\n        descriptor = gen_node(ctx, form)\n\n        if isinstance(form, IMeta):\n            assert isinstance(form.meta, (lmap.Map, type(None)))\n            form_meta = _clean_meta(form.meta)\n            if form_meta is not None:\n                meta_ast = _parse_ast(ctx, form_meta)\n                assert isinstance(meta_ast, MapNode) or (\n                    isinstance(meta_ast, Const) and meta_ast.type == ConstType.MAP\n                )\n                return WithMeta(\n                    form=form, meta=meta_ast, expr=descriptor, env=ctx.get_node_env()\n                )\n\n        return descriptor\n\n    return with_meta", "response": "Wraps the node generated by gen_node in a WithMeta AST node if the original form has meta."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nroll up deftype* declared bases and method implementations.", "response": "def __deftype_impls(  # pylint: disable=too-many-branches\n    ctx: ParserContext, form: ISeq\n) -> Tuple[List[DefTypeBase], List[Method]]:\n    \"\"\"Roll up deftype* declared bases and method implementations.\"\"\"\n    current_interface_sym: Optional[sym.Symbol] = None\n    current_interface: Optional[DefTypeBase] = None\n    interfaces = []\n    methods: List[Method] = []\n    interface_methods: MutableMapping[sym.Symbol, List[Method]] = {}\n    for elem in form:\n        if isinstance(elem, sym.Symbol):\n            if current_interface is not None:\n                if current_interface_sym in interface_methods:\n                    raise ParserException(\n                        f\"deftype* forms may only implement an interface once\",\n                        form=elem,\n                    )\n                assert (\n                    current_interface_sym is not None\n                ), \"Symbol must be defined with interface\"\n                interface_methods[current_interface_sym] = methods\n\n            current_interface_sym = elem\n            current_interface = _parse_ast(ctx, elem)\n            methods = []\n\n            if not isinstance(current_interface, (MaybeClass, MaybeHostForm, VarRef)):\n                raise ParserException(\n                    f\"deftype* interface implementation must be an existing interface\",\n                    form=elem,\n                )\n            interfaces.append(current_interface)\n        elif isinstance(elem, ISeq):\n            if current_interface is None:\n                raise ParserException(\n                    f\"deftype* method cannot be declared without interface\", form=elem\n                )\n            methods.append(__deftype_method(ctx, elem, current_interface))\n        else:\n            raise ParserException(\n                f\"deftype* must consist of interface or protocol names and methods\",\n                form=elem,\n            )\n\n    if current_interface is not None:\n        if len(methods) > 0:\n            if current_interface_sym in interface_methods:\n                raise ParserException(\n                    f\"deftype* forms may only implement an interface once\",\n                    form=current_interface_sym,\n                )\n            assert (\n                current_interface_sym is not None\n            ), \"Symbol must be defined with interface\"\n            interface_methods[current_interface_sym] = methods\n        else:\n            raise ParserException(\n                f\"deftype* may not declare interface without at least one method\",\n                form=current_interface_sym,\n            )\n\n    return interfaces, list(chain.from_iterable(interface_methods.values()))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nasserts that the node is not a cur node.", "response": "def _assert_no_recur(node: Node) -> None:\n    \"\"\"Assert that `recur` forms do not appear in any position of this or\n    child AST nodes.\"\"\"\n    if node.op == NodeOp.RECUR:\n        raise ParserException(\n            \"recur must appear in tail position\", form=node.form, lisp_ast=node\n        )\n    elif node.op in {NodeOp.FN, NodeOp.LOOP}:\n        pass\n    else:\n        node.visit(_assert_no_recur)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _assert_recur_is_tail(node: Node) -> None:  # pylint: disable=too-many-branches\n    if node.op == NodeOp.DO:\n        assert isinstance(node, Do)\n        for child in node.statements:\n            _assert_no_recur(child)\n        _assert_recur_is_tail(node.ret)\n    elif node.op in {NodeOp.FN, NodeOp.FN_METHOD, NodeOp.METHOD}:\n        assert isinstance(node, (Fn, FnMethod, Method))\n        node.visit(_assert_recur_is_tail)\n    elif node.op == NodeOp.IF:\n        assert isinstance(node, If)\n        _assert_no_recur(node.test)\n        _assert_recur_is_tail(node.then)\n        _assert_recur_is_tail(node.else_)\n    elif node.op in {NodeOp.LET, NodeOp.LETFN}:\n        assert isinstance(node, (Let, LetFn))\n        for binding in node.bindings:\n            assert binding.init is not None\n            _assert_no_recur(binding.init)\n        _assert_recur_is_tail(node.body)\n    elif node.op == NodeOp.LOOP:\n        assert isinstance(node, Loop)\n        for binding in node.bindings:\n            assert binding.init is not None\n            _assert_no_recur(binding.init)\n    elif node.op == NodeOp.RECUR:\n        pass\n    elif node.op == NodeOp.TRY:\n        assert isinstance(node, Try)\n        _assert_recur_is_tail(node.body)\n        for catch in node.catches:\n            _assert_recur_is_tail(catch)\n        if node.finally_:\n            _assert_no_recur(node.finally_)\n    else:\n        node.visit(_assert_no_recur)", "response": "Assert that the current node is the tail of the current node."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __resolve_namespaced_symbol(  # pylint: disable=too-many-branches\n    ctx: ParserContext, form: sym.Symbol\n) -> Union[MaybeClass, MaybeHostForm, VarRef]:\n    \"\"\"Resolve a namespaced symbol into a Python name or Basilisp Var.\"\"\"\n    assert form.ns is not None\n\n    if form.ns == ctx.current_ns.name:\n        v = ctx.current_ns.find(sym.symbol(form.name))\n        if v is not None:\n            return VarRef(form=form, var=v, env=ctx.get_node_env())\n    elif form.ns == _BUILTINS_NS:\n        class_ = munge(form.name, allow_builtins=True)\n        target = getattr(builtins, class_, None)\n        if target is None:\n            raise ParserException(\n                f\"cannot resolve builtin function '{class_}'\", form=form\n            )\n        return MaybeClass(\n            form=form, class_=class_, target=target, env=ctx.get_node_env()\n        )\n\n    if \".\" in form.name:\n        raise ParserException(\n            \"symbol names may not contain the '.' operator\", form=form\n        )\n\n    ns_sym = sym.symbol(form.ns)\n    if ns_sym in ctx.current_ns.imports or ns_sym in ctx.current_ns.import_aliases:\n        # We still import Basilisp code, so we'll want to make sure\n        # that the symbol isn't referring to a Basilisp Var first\n        v = Var.find(form)\n        if v is not None:\n            return VarRef(form=form, var=v, env=ctx.get_node_env())\n\n        # Fetch the full namespace name for the aliased namespace/module.\n        # We don't need this for actually generating the link later, but\n        # we _do_ need it for fetching a reference to the module to check\n        # for membership.\n        if ns_sym in ctx.current_ns.import_aliases:\n            ns = ctx.current_ns.import_aliases[ns_sym]\n            assert ns is not None\n            ns_name = ns.name\n        else:\n            ns_name = ns_sym.name\n\n        safe_module_name = munge(ns_name)\n        assert (\n            safe_module_name in sys.modules\n        ), f\"Module '{safe_module_name}' is not imported\"\n        ns_module = sys.modules[safe_module_name]\n        safe_name = munge(form.name)\n\n        # Try without allowing builtins first\n        if safe_name in vars(ns_module):\n            return MaybeHostForm(\n                form=form,\n                class_=munge(ns_sym.name),\n                field=safe_name,\n                target=vars(ns_module)[safe_name],\n                env=ctx.get_node_env(),\n            )\n\n        # Then allow builtins\n        safe_name = munge(form.name, allow_builtins=True)\n        if safe_name not in vars(ns_module):\n            raise ParserException(\"can't identify aliased form\", form=form)\n\n        # Aliased imports generate code which uses the import alias, so we\n        # don't need to care if this is an import or an alias.\n        return MaybeHostForm(\n            form=form,\n            class_=munge(ns_sym.name),\n            field=safe_name,\n            target=vars(ns_module)[safe_name],\n            env=ctx.get_node_env(),\n        )\n    elif ns_sym in ctx.current_ns.aliases:\n        aliased_ns: runtime.Namespace = ctx.current_ns.aliases[ns_sym]\n        v = Var.find(sym.symbol(form.name, ns=aliased_ns.name))\n        if v is None:\n            raise ParserException(\n                f\"unable to resolve symbol '{sym.symbol(form.name, ns_sym.name)}' in this context\",\n                form=form,\n            )\n        return VarRef(form=form, var=v, env=ctx.get_node_env())\n    else:\n        raise ParserException(\n            f\"unable to resolve symbol '{form}' in this context\", form=form\n        )", "response": "Resolve a namespaced symbol into a Python name or Basilisp Var."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __resolve_bare_symbol(\n    ctx: ParserContext, form: sym.Symbol\n) -> Union[MaybeClass, VarRef]:\n    \"\"\"Resolve a non-namespaced symbol into a Python name or a local\n    Basilisp Var.\"\"\"\n    assert form.ns is None\n\n    # Look up the symbol in the namespace mapping of the current namespace.\n    v = ctx.current_ns.find(form)\n    if v is not None:\n        return VarRef(form=form, var=v, env=ctx.get_node_env())\n\n    if \".\" in form.name:\n        raise ParserException(\n            \"symbol names may not contain the '.' operator\", form=form\n        )\n\n    munged = munge(form.name, allow_builtins=True)\n    if munged in vars(builtins):\n        return MaybeClass(\n            form=form,\n            class_=munged,\n            target=vars(builtins)[munged],\n            env=ctx.get_node_env(),\n        )\n\n    assert munged not in vars(ctx.current_ns.module)\n    raise ParserException(\n        f\"unable to resolve symbol '{form}' in this context\", form=form\n    )", "response": "Resolve a non - namespaced symbol into a Python name or a local\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nresolve a Basilisp symbol as a Var or Python name.", "response": "def _resolve_sym(\n    ctx: ParserContext, form: sym.Symbol\n) -> Union[MaybeClass, MaybeHostForm, VarRef]:\n    \"\"\"Resolve a Basilisp symbol as a Var or Python name.\"\"\"\n    # Support special class-name syntax to instantiate new classes\n    #   (Classname. *args)\n    #   (aliased.Classname. *args)\n    #   (fully.qualified.Classname. *args)\n    if form.ns is None and form.name.endswith(\".\"):\n        try:\n            ns, name = form.name[:-1].rsplit(\".\", maxsplit=1)\n            form = sym.symbol(name, ns=ns)\n        except ValueError:\n            form = sym.symbol(form.name[:-1])\n\n    if form.ns is not None:\n        return __resolve_namespaced_symbol(ctx, form)\n    else:\n        return __resolve_bare_symbol(ctx, form)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntaking a Lisp form as an argument and produce a Basilisp syntax tree matching the clojure. tools. analyzer AST spec.", "response": "def parse_ast(ctx: ParserContext, form: ReaderForm) -> Node:\n    \"\"\"Take a Lisp form as an argument and produce a Basilisp syntax\n    tree matching the clojure.tools.analyzer AST spec.\"\"\"\n    return _parse_ast(ctx, form).assoc(top_level=True)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef warn_on_shadowed_var(self) -> bool:\n        return self.warn_on_shadowed_name or self._opts.entry(\n            WARN_ON_SHADOWED_VAR, False\n        )", "response": "If True warn when a defed Var name is shadowed in an inner scope."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a new symbol to the symbol table.", "response": "def put_new_symbol(  # pylint: disable=too-many-arguments\n        self,\n        s: sym.Symbol,\n        binding: Binding,\n        warn_on_shadowed_name: bool = True,\n        warn_on_shadowed_var: bool = True,\n        warn_if_unused: bool = True,\n    ):\n        \"\"\"Add a new symbol to the symbol table.\n\n        This function allows individual warnings to be disabled for one run\n        by supplying keyword arguments temporarily disabling those warnings.\n        In certain cases, we do not want to issue warnings again for a\n        previously checked case, so this is a simple way of disabling these\n        warnings for those cases.\n\n        If WARN_ON_SHADOWED_NAME compiler option is active and the\n        warn_on_shadowed_name keyword argument is True, then a warning will be\n        emitted if a local name is shadowed by another local name. Note that\n        WARN_ON_SHADOWED_NAME implies WARN_ON_SHADOWED_VAR.\n\n        If WARN_ON_SHADOWED_VAR compiler option is active and the\n        warn_on_shadowed_var keyword argument is True, then a warning will be\n        emitted if a named var is shadowed by a local name.\"\"\"\n        st = self.symbol_table\n        if warn_on_shadowed_name and self.warn_on_shadowed_name:\n            if st.find_symbol(s) is not None:\n                logger.warning(f\"name '{s}' shadows name from outer scope\")\n        if (\n            warn_on_shadowed_name or warn_on_shadowed_var\n        ) and self.warn_on_shadowed_var:\n            if self.current_ns.find(s) is not None:\n                logger.warning(f\"name '{s}' shadows def'ed Var from outer scope\")\n        if s.meta is not None and s.meta.entry(SYM_NO_WARN_WHEN_UNUSED_META_KEY, None):\n            warn_if_unused = False\n        st.new_symbol(s, binding, warn_if_unused=warn_if_unused)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nproduces a Lisp representation of an associative collection with the start and end strings supplied.", "response": "def map_lrepr(\n    entries: Callable[[], Iterable[Tuple[Any, Any]]],\n    start: str,\n    end: str,\n    meta=None,\n    **kwargs,\n) -> str:\n    \"\"\"Produce a Lisp representation of an associative collection, bookended\n    with the start and end string supplied. The entries argument must be a\n    callable which will produce tuples of key-value pairs.\n\n    The keyword arguments will be passed along to lrepr for the sequence\n    elements.\"\"\"\n    print_level = kwargs[\"print_level\"]\n    if isinstance(print_level, int) and print_level < 1:\n        return SURPASSED_PRINT_LEVEL\n\n    kwargs = _process_kwargs(**kwargs)\n\n    def entry_reprs():\n        for k, v in entries():\n            yield \"{k} {v}\".format(k=lrepr(k, **kwargs), v=lrepr(v, **kwargs))\n\n    trailer = []\n    print_dup = kwargs[\"print_dup\"]\n    print_length = kwargs[\"print_length\"]\n    if not print_dup and isinstance(print_length, int):\n        items = seq(entry_reprs()).take(print_length + 1).to_list()\n        if len(items) > print_length:\n            items.pop()\n            trailer.append(SURPASSED_PRINT_LENGTH)\n    else:\n        items = list(entry_reprs())\n\n    seq_lrepr = PRINT_SEPARATOR.join(items + trailer)\n\n    print_meta = kwargs[\"print_meta\"]\n    if print_meta and meta:\n        return f\"^{lrepr(meta, **kwargs)} {start}{seq_lrepr}{end}\"\n\n    return f\"{start}{seq_lrepr}{end}\""}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nproducing a Lisp representation of a sequential collection with the start and end string supplied. The keyword arguments will be passed along to lrepr for the sequence elements.", "response": "def seq_lrepr(\n    iterable: Iterable[Any], start: str, end: str, meta=None, **kwargs\n) -> str:\n    \"\"\"Produce a Lisp representation of a sequential collection, bookended\n    with the start and end string supplied. The keyword arguments will be\n    passed along to lrepr for the sequence elements.\"\"\"\n    print_level = kwargs[\"print_level\"]\n    if isinstance(print_level, int) and print_level < 1:\n        return SURPASSED_PRINT_LEVEL\n\n    kwargs = _process_kwargs(**kwargs)\n\n    trailer = []\n    print_dup = kwargs[\"print_dup\"]\n    print_length = kwargs[\"print_length\"]\n    if not print_dup and isinstance(print_length, int):\n        items = seq(iterable).take(print_length + 1).to_list()\n        if len(items) > print_length:\n            items.pop()\n            trailer.append(SURPASSED_PRINT_LENGTH)\n    else:\n        items = iterable\n\n    items = list(map(lambda o: lrepr(o, **kwargs), items))\n    seq_lrepr = PRINT_SEPARATOR.join(items + trailer)\n\n    print_meta = kwargs[\"print_meta\"]\n    if print_meta and meta:\n        return f\"^{lrepr(meta, **kwargs)} {start}{seq_lrepr}{end}\"\n\n    return f\"{start}{seq_lrepr}{end}\""}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a string representation of a Lisp object.", "response": "def lrepr(  # pylint: disable=too-many-arguments\n    o: Any,\n    human_readable: bool = False,\n    print_dup: bool = PRINT_DUP,\n    print_length: PrintCountSetting = PRINT_LENGTH,\n    print_level: PrintCountSetting = PRINT_LEVEL,\n    print_meta: bool = PRINT_META,\n    print_readably: bool = PRINT_READABLY,\n) -> str:\n    \"\"\"Return a string representation of a Lisp object.\n\n    Permissible keyword arguments are:\n    - human_readable: if logical True, print strings without quotations or\n                      escape sequences (default: false)\n    - print_dup: if logical true, print objects in a way that preserves their\n                 types (default: false)\n    - print_length: the number of items in a collection which will be printed,\n                    or no limit if bound to a logical falsey value (default: 50)\n    - print_level: the depth of the object graph to print, starting with 0, or\n                   no limit if bound to a logical falsey value (default: nil)\n    - print_meta: if logical true, print objects meta in a way that can be\n                  read back by the reader (default: false)\n    - print_readably: if logical false, print strings and characters with\n                      non-alphanumeric characters converted to escape sequences\n                      (default: true)\n\n    Note that this function is not capable of capturing the values bound at\n    runtime to the basilisp.core dynamic variables which correspond to each\n    of the keyword arguments to this function. To use a version of lrepr\n    which does capture those values, call basilisp.lang.runtime.lrepr directly.\"\"\"\n    if isinstance(o, LispObject):\n        return o._lrepr(\n            human_readable=human_readable,\n            print_dup=print_dup,\n            print_length=print_length,\n            print_level=print_level,\n            print_meta=print_meta,\n            print_readably=print_readably,\n        )\n    else:  # pragma: no cover\n        return _lrepr_fallback(\n            o,\n            human_readable=human_readable,\n            print_dup=print_dup,\n            print_length=print_length,\n            print_level=print_level,\n            print_meta=print_meta,\n            print_readably=print_readably,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nvisits all immediate children of this node calling f on each child.", "response": "def visit(self, f: Callable[..., None], *args, **kwargs):\n        \"\"\"Visit all immediate children of this node, calling\n        f(child, *args, **kwargs) on each child.\"\"\"\n        for child_kw in self.children:\n            child_attr = munge(child_kw.name)\n\n            if child_attr.endswith(\"s\"):\n                iter_child: Iterable[Node] = getattr(self, child_attr)\n                assert iter_child is not None, \"Listed child must not be none\"\n                for item in iter_child:\n                    f(item, *args, **kwargs)\n            else:\n                child: Node = getattr(self, child_attr)\n                assert child is not None, \"Listed child must not be none\"\n                f(child, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a new copy of this node with location in this node s environment updated to match the start_loc if given or using its existing location otherwise.", "response": "def fix_missing_locations(\n        self, start_loc: Optional[Tuple[int, int]] = None\n    ) -> \"Node\":\n        \"\"\"Return a transformed copy of this node with location in this node's\n        environment updated to match the `start_loc` if given, or using its\n        existing location otherwise. All child nodes will be recursively\n        transformed and replaced. Child nodes will use their parent node\n        location if they do not have one.\"\"\"\n        if self.env.line is None or self.env.col is None:\n            loc = start_loc\n        else:\n            loc = (self.env.line, self.env.col)\n\n        assert loc is not None and all(\n            [e is not None for e in loc]\n        ), \"Must specify location information\"\n\n        new_attrs: MutableMapping[str, Union[NodeEnv, Node, Iterable[Node]]] = {\n            \"env\": attr.evolve(self.env, line=loc[0], col=loc[1])\n        }\n        for child_kw in self.children:\n            child_attr = munge(child_kw.name)\n            assert child_attr != \"env\", \"Node environment already set\"\n\n            if child_attr.endswith(\"s\"):\n                iter_child: Iterable[Node] = getattr(self, child_attr)\n                assert iter_child is not None, \"Listed child must not be none\"\n                new_children = []\n                for item in iter_child:\n                    new_children.append(item.fix_missing_locations(start_loc))\n                new_attrs[child_attr] = vec.vector(new_children)\n            else:\n                child: Node = getattr(self, child_attr)\n                assert child is not None, \"Listed child must not be none\"\n                new_attrs[child_attr] = child.fix_missing_locations(start_loc)\n\n        return self.assoc(**new_attrs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _emit_ast_string(module: ast.AST) -> None:  # pragma: no cover\n    # TODO: eventually, this default should become \"false\" but during this\n    #       period of heavy development, having it set to \"true\" by default\n    #       is tremendously useful\n    if os.getenv(\"BASILISP_EMIT_GENERATED_PYTHON\", \"true\") != \"true\":\n        return\n\n    if runtime.print_generated_python():\n        print(to_py_str(module))\n    else:\n        runtime.add_generated_python(to_py_str(module))", "response": "Emit the generated Python AST string either to standard out or to the\n    generated - python dynamic Var for the current namespace."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef compile_and_exec_form(  # pylint: disable= too-many-arguments\n    form: ReaderForm,\n    ctx: CompilerContext,\n    module: types.ModuleType,\n    wrapped_fn_name: str = _DEFAULT_FN,\n    collect_bytecode: Optional[BytecodeCollector] = None,\n) -> Any:\n    \"\"\"Compile and execute the given form. This function will be most useful\n    for the REPL and testing purposes. Returns the result of the executed expression.\n\n    Callers may override the wrapped function name, which is used by the\n    REPL to evaluate the result of an expression and print it back out.\"\"\"\n    if form is None:\n        return None\n\n    if not module.__basilisp_bootstrapped__:  # type: ignore\n        _bootstrap_module(ctx.generator_context, ctx.py_ast_optimizer, module)\n\n    final_wrapped_name = genname(wrapped_fn_name)\n\n    lisp_ast = parse_ast(ctx.parser_context, form)\n    py_ast = gen_py_ast(ctx.generator_context, lisp_ast)\n    form_ast = list(\n        map(\n            _statementize,\n            itertools.chain(\n                py_ast.dependencies,\n                [_expressionize(GeneratedPyAST(node=py_ast.node), final_wrapped_name)],\n            ),\n        )\n    )\n\n    ast_module = ast.Module(body=form_ast)\n    ast_module = ctx.py_ast_optimizer.visit(ast_module)\n    ast.fix_missing_locations(ast_module)\n\n    _emit_ast_string(ast_module)\n\n    bytecode = compile(ast_module, ctx.filename, \"exec\")\n    if collect_bytecode:\n        collect_bytecode(bytecode)\n    exec(bytecode, module.__dict__)\n    return getattr(module, final_wrapped_name)()", "response": "Compile and execute the given form."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _incremental_compile_module(\n    optimizer: PythonASTOptimizer,\n    py_ast: GeneratedPyAST,\n    mod: types.ModuleType,\n    source_filename: str,\n    collect_bytecode: Optional[BytecodeCollector] = None,\n) -> None:\n    \"\"\"Incrementally compile a stream of AST nodes in module mod.\n\n    The source_filename will be passed to Python's native compile.\n\n    Incremental compilation is an integral part of generating a Python module\n    during the same process as macro-expansion.\"\"\"\n    module_body = list(\n        map(_statementize, itertools.chain(py_ast.dependencies, [py_ast.node]))\n    )\n\n    module = ast.Module(body=list(module_body))\n    module = optimizer.visit(module)\n    ast.fix_missing_locations(module)\n\n    _emit_ast_string(module)\n\n    bytecode = compile(module, source_filename, \"exec\")\n    if collect_bytecode:\n        collect_bytecode(bytecode)\n    exec(bytecode, mod.__dict__)", "response": "Incrementally compile a module mod."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbootstrap a new module with imports and other boilerplate.", "response": "def _bootstrap_module(\n    gctx: GeneratorContext,\n    optimizer: PythonASTOptimizer,\n    mod: types.ModuleType,\n    collect_bytecode: Optional[BytecodeCollector] = None,\n) -> None:\n    \"\"\"Bootstrap a new module with imports and other boilerplate.\"\"\"\n    _incremental_compile_module(\n        optimizer,\n        py_module_preamble(gctx),\n        mod,\n        source_filename=gctx.filename,\n        collect_bytecode=collect_bytecode,\n    )\n    mod.__basilisp_bootstrapped__ = True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compile_module(\n    forms: Iterable[ReaderForm],\n    ctx: CompilerContext,\n    module: types.ModuleType,\n    collect_bytecode: Optional[BytecodeCollector] = None,\n) -> None:\n    \"\"\"Compile an entire Basilisp module into Python bytecode which can be\n    executed as a Python module.\n\n    This function is designed to generate bytecode which can be used for the\n    Basilisp import machinery, to allow callers to import Basilisp modules from\n    Python code.\n    \"\"\"\n    _bootstrap_module(ctx.generator_context, ctx.py_ast_optimizer, module)\n\n    for form in forms:\n        nodes = gen_py_ast(ctx.generator_context, parse_ast(ctx.parser_context, form))\n        _incremental_compile_module(\n            ctx.py_ast_optimizer,\n            nodes,\n            module,\n            source_filename=ctx.filename,\n            collect_bytecode=collect_bytecode,\n        )", "response": "Compile an entire Basilisp module into Python bytecode which can be executed as a Python module."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompile the given bytecode into the given module.", "response": "def compile_bytecode(\n    code: List[types.CodeType],\n    gctx: GeneratorContext,\n    optimizer: PythonASTOptimizer,\n    module: types.ModuleType,\n) -> None:\n    \"\"\"Compile cached bytecode into the given module.\n\n    The Basilisp import hook attempts to cache bytecode while compiling Basilisp\n    namespaces. When the cached bytecode is reloaded from disk, it needs to be\n    compiled within a bootstrapped module. This function bootstraps the module\n    and then proceeds to compile a collection of bytecodes into the module.\"\"\"\n    _bootstrap_module(gctx, optimizer, module)\n    for bytecode in code:\n        exec(bytecode, module.__dict__)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a Sequence from an iterable s.", "response": "def sequence(s: Iterable) -> ISeq[Any]:\n    \"\"\"Create a Sequence from Iterable s.\"\"\"\n    try:\n        i = iter(s)\n        return _Sequence(i, next(i))\n    except StopIteration:\n        return EMPTY"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreplace characters which are not valid in Python symbols with valid replacement strings.", "response": "def munge(s: str, allow_builtins: bool = False) -> str:\n    \"\"\"Replace characters which are not valid in Python symbols\n    with valid replacement strings.\"\"\"\n    new_str = []\n    for c in s:\n        new_str.append(_MUNGE_REPLACEMENTS.get(c, c))\n\n    new_s = \"\".join(new_str)\n\n    if keyword.iskeyword(new_s):\n        return f\"{new_s}_\"\n\n    if not allow_builtins and new_s in builtins.__dict__:\n        return f\"{new_s}_\"\n\n    return new_s"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef demunge(s: str) -> str:\n\n    def demunge_replacer(match: Match) -> str:\n        full_match = match.group(0)\n        replacement = _DEMUNGE_REPLACEMENTS.get(full_match, None)\n        if replacement:\n            return replacement\n        return full_match\n\n    return re.sub(_DEMUNGE_PATTERN, demunge_replacer, s).replace(\"_\", \"-\")", "response": "Replace munged string components with their original\n    representation."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a Fraction from a numerator and denominator.", "response": "def fraction(numerator: int, denominator: int) -> Fraction:\n    \"\"\"Create a Fraction from a numerator and denominator.\"\"\"\n    return Fraction(numerator=numerator, denominator=denominator)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the default logging handler for Basilisp.", "response": "def get_handler(level: str, fmt: str) -> logging.Handler:\n    \"\"\"Get the default logging handler for Basilisp.\"\"\"\n    handler: logging.Handler = logging.NullHandler()\n    if os.getenv(\"BASILISP_USE_DEV_LOGGER\") == \"true\":\n        handler = logging.StreamHandler()\n\n    handler.setFormatter(logging.Formatter(fmt))\n    handler.setLevel(level)\n    return handler"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef map(kvs: Mapping[K, V], meta=None) -> Map[K, V]:  # pylint:disable=redefined-builtin\n    return Map(pmap(initial=kvs), meta=meta)", "response": "Creates a new map with the given values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef timed(f: Optional[Callable[[int], None]] = None):\n    start = time.perf_counter()\n    yield\n    end = time.perf_counter()\n    if f:\n        ns = int((end - start) * 1_000_000_000)\n        f(ns)", "response": "A with - block that returns a new object with the time in nanoseconds."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef partition(coll, n: int):\n    assert n > 0\n    start = 0\n    stop = n\n    while stop <= len(coll):\n        yield tuple(e for e in coll[start:stop])\n        start += n\n        stop += n\n    if start < len(coll) < stop:\n        stop = len(coll)\n        yield tuple(e for e in coll[start:stop])", "response": "Partition coll into groups of size n."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrap a reader function in a decorator to supply line and column countryCode information along with relevant forms.", "response": "def _with_loc(f: W) -> W:\n    \"\"\"Wrap a reader function in a decorator to supply line and column\n    information along with relevant forms.\"\"\"\n\n    @functools.wraps(f)\n    def with_lineno_and_col(ctx):\n        meta = lmap.map(\n            {READER_LINE_KW: ctx.reader.line, READER_COL_KW: ctx.reader.col}\n        )\n        v = f(ctx)\n        try:\n            return v.with_meta(meta)  # type: ignore\n        except AttributeError:\n            return v\n\n    return cast(W, with_lineno_and_col)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _read_namespaced(\n    ctx: ReaderContext, allowed_suffix: Optional[str] = None\n) -> Tuple[Optional[str], str]:\n    \"\"\"Read a namespaced token from the input stream.\"\"\"\n    ns: List[str] = []\n    name: List[str] = []\n    reader = ctx.reader\n    has_ns = False\n    while True:\n        token = reader.peek()\n        if token == \"/\":\n            reader.next_token()\n            if has_ns:\n                raise SyntaxError(\"Found '/'; expected word character\")\n            elif len(name) == 0:\n                name.append(\"/\")\n            else:\n                if \"/\" in name:\n                    raise SyntaxError(\"Found '/' after '/'\")\n                has_ns = True\n                ns = name\n                name = []\n        elif ns_name_chars.match(token):\n            reader.next_token()\n            name.append(token)\n        elif allowed_suffix is not None and token == allowed_suffix:\n            reader.next_token()\n            name.append(token)\n        else:\n            break\n\n    ns_str = None if not has_ns else \"\".join(ns)\n    name_str = \"\".join(name)\n\n    # A small exception for the symbol '/ used for division\n    if ns_str is None:\n        if \"/\" in name_str and name_str != \"/\":\n            raise SyntaxError(\"'/' character disallowed in names\")\n\n    assert ns_str is None or len(ns_str) > 0\n\n    return ns_str, name_str", "response": "Read a namespaced token from the input stream."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _read_coll(\n    ctx: ReaderContext,\n    f: Callable[[Collection[Any]], Union[llist.List, lset.Set, vector.Vector]],\n    end_token: str,\n    coll_name: str,\n):\n    \"\"\"Read a collection from the input stream and create the\n    collection using f.\"\"\"\n    coll: List = []\n    reader = ctx.reader\n    while True:\n        token = reader.peek()\n        if token == \"\":\n            raise SyntaxError(f\"Unexpected EOF in {coll_name}\")\n        if whitespace_chars.match(token):\n            reader.advance()\n            continue\n        if token == end_token:\n            reader.next_token()\n            return f(coll)\n        elem = _read_next(ctx)\n        if elem is COMMENT:\n            continue\n        coll.append(elem)", "response": "Read a collection from the input stream and create the collection using f."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread a list element from the input stream.", "response": "def _read_list(ctx: ReaderContext) -> llist.List:\n    \"\"\"Read a list element from the input stream.\"\"\"\n    start = ctx.reader.advance()\n    assert start == \"(\"\n    return _read_coll(ctx, llist.list, \")\", \"list\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads a vector element from the input stream.", "response": "def _read_vector(ctx: ReaderContext) -> vector.Vector:\n    \"\"\"Read a vector element from the input stream.\"\"\"\n    start = ctx.reader.advance()\n    assert start == \"[\"\n    return _read_coll(ctx, vector.vector, \"]\", \"vector\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a set from the input stream.", "response": "def _read_set(ctx: ReaderContext) -> lset.Set:\n    \"\"\"Return a set from the input stream.\"\"\"\n    start = ctx.reader.advance()\n    assert start == \"{\"\n\n    def set_if_valid(s: Collection) -> lset.Set:\n        if len(s) != len(set(s)):\n            raise SyntaxError(\"Duplicated values in set\")\n        return lset.set(s)\n\n    return _read_coll(ctx, set_if_valid, \"}\", \"set\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _read_map(ctx: ReaderContext) -> lmap.Map:\n    reader = ctx.reader\n    start = reader.advance()\n    assert start == \"{\"\n    d: MutableMapping[Any, Any] = {}\n    while True:\n        if reader.peek() == \"}\":\n            reader.next_token()\n            break\n        k = _read_next(ctx)\n        if k is COMMENT:\n            continue\n        while True:\n            if reader.peek() == \"}\":\n                raise SyntaxError(\"Unexpected token '}'; expected map value\")\n            v = _read_next(ctx)\n            if v is COMMENT:\n                continue\n            if k in d:\n                raise SyntaxError(f\"Duplicate key '{k}' in map literal\")\n            break\n        d[k] = v\n\n    return lmap.map(d)", "response": "Read a map from the input stream."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread a numeric value from the input stream.", "response": "def _read_num(  # noqa: C901  # pylint: disable=too-many-statements\n    ctx: ReaderContext\n) -> MaybeNumber:\n    \"\"\"Return a numeric (complex, Decimal, float, int, Fraction) from the input stream.\"\"\"\n    chars: List[str] = []\n    reader = ctx.reader\n\n    is_complex = False\n    is_decimal = False\n    is_float = False\n    is_integer = False\n    is_ratio = False\n    while True:\n        token = reader.peek()\n        if token == \"-\":\n            following_token = reader.next_token()\n            if not begin_num_chars.match(following_token):\n                reader.pushback()\n                try:\n                    for _ in chars:\n                        reader.pushback()\n                except IndexError:\n                    raise SyntaxError(\n                        \"Requested to pushback too many characters onto StreamReader\"\n                    )\n                return _read_sym(ctx)\n            chars.append(token)\n            continue\n        elif token == \".\":\n            if is_float:\n                raise SyntaxError(\"Found extra '.' in float; expected decimal portion\")\n            is_float = True\n        elif token == \"J\":\n            if is_complex:\n                raise SyntaxError(\"Found extra 'J' suffix in complex literal\")\n            is_complex = True\n        elif token == \"M\":\n            if is_decimal:\n                raise SyntaxError(\"Found extra 'M' suffix in decimal literal\")\n            is_decimal = True\n        elif token == \"N\":\n            if is_integer:\n                raise SyntaxError(\"Found extra 'N' suffix in integer literal\")\n            is_integer = True\n        elif token == \"/\":\n            if is_ratio:\n                raise SyntaxError(\"Found extra '/' in ratio literal\")\n            is_ratio = True\n        elif not num_chars.match(token):\n            break\n        reader.next_token()\n        chars.append(token)\n\n    assert len(chars) > 0, \"Must have at least one digit in integer or float\"\n\n    s = \"\".join(chars)\n    if (\n        sum(\n            [\n                is_complex and is_decimal,\n                is_complex and is_integer,\n                is_complex and is_ratio,\n                is_decimal or is_float,\n                is_integer,\n                is_ratio,\n            ]\n        )\n        > 1\n    ):\n        raise SyntaxError(f\"Invalid number format: {s}\")\n\n    if is_complex:\n        imaginary = float(s[:-1]) if is_float else int(s[:-1])\n        return complex(0, imaginary)\n    elif is_decimal:\n        try:\n            return decimal.Decimal(s[:-1])\n        except decimal.InvalidOperation:\n            raise SyntaxError(f\"Invalid number format: {s}\") from None\n    elif is_float:\n        return float(s)\n    elif is_ratio:\n        assert \"/\" in s, \"Ratio must contain one '/' character\"\n        num, denominator = s.split(\"/\")\n        return Fraction(numerator=int(num), denominator=int(denominator))\n    elif is_integer:\n        return int(s[:-1])\n    return int(s)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a string from the input stream.", "response": "def _read_str(ctx: ReaderContext, allow_arbitrary_escapes: bool = False) -> str:\n    \"\"\"Return a string from the input stream.\n\n    If allow_arbitrary_escapes is True, do not throw a SyntaxError if an\n    unknown escape sequence is encountered.\"\"\"\n    s: List[str] = []\n    reader = ctx.reader\n    while True:\n        token = reader.next_token()\n        if token == \"\":\n            raise SyntaxError(\"Unexpected EOF in string\")\n        if token == \"\\\\\":\n            token = reader.next_token()\n            escape_char = _STR_ESCAPE_CHARS.get(token, None)\n            if escape_char:\n                s.append(escape_char)\n                continue\n            if allow_arbitrary_escapes:\n                s.append(\"\\\\\")\n            else:\n                raise SyntaxError(\"Unknown escape sequence: \\\\{token}\")\n        if token == '\"':\n            reader.next_token()\n            return \"\".join(s)\n        s.append(token)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _read_sym(ctx: ReaderContext) -> MaybeSymbol:\n    ns, name = _read_namespaced(ctx, allowed_suffix=\"#\")\n    if not ctx.is_syntax_quoted and name.endswith(\"#\"):\n        raise SyntaxError(\"Gensym may not appear outside syntax quote\")\n    if ns is not None:\n        if any(map(lambda s: len(s) == 0, ns.split(\".\"))):\n            raise SyntaxError(\n                \"All '.' separated segments of a namespace \"\n                \"must contain at least one character.\"\n            )\n    if name.startswith(\".\") and ns is not None:\n        raise SyntaxError(\"Symbols starting with '.' may not have a namespace\")\n    if ns is None:\n        if name == \"nil\":\n            return None\n        elif name == \"true\":\n            return True\n        elif name == \"false\":\n            return False\n    if ctx.is_syntax_quoted and not name.endswith(\"#\"):\n        return ctx.resolve(symbol.symbol(name, ns))\n    return symbol.symbol(name, ns=ns)", "response": "Read a symbol from the input stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a keyword from the input stream.", "response": "def _read_kw(ctx: ReaderContext) -> keyword.Keyword:\n    \"\"\"Return a keyword from the input stream.\"\"\"\n    start = ctx.reader.advance()\n    assert start == \":\"\n    ns, name = _read_namespaced(ctx)\n    if \".\" in name:\n        raise SyntaxError(\"Found '.' in keyword name\")\n    return keyword.keyword(name, ns=ns)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread metadata and apply that to the next object in the the input stream.", "response": "def _read_meta(ctx: ReaderContext) -> IMeta:\n    \"\"\"Read metadata and apply that to the next object in the\n    input stream.\"\"\"\n    start = ctx.reader.advance()\n    assert start == \"^\"\n    meta = _read_next_consuming_comment(ctx)\n\n    meta_map: Optional[lmap.Map[LispForm, LispForm]] = None\n    if isinstance(meta, symbol.Symbol):\n        meta_map = lmap.map({keyword.keyword(\"tag\"): meta})\n    elif isinstance(meta, keyword.Keyword):\n        meta_map = lmap.map({meta: True})\n    elif isinstance(meta, lmap.Map):\n        meta_map = meta\n    else:\n        raise SyntaxError(\n            f\"Expected symbol, keyword, or map for metadata, not {type(meta)}\"\n        )\n\n    obj_with_meta = _read_next_consuming_comment(ctx)\n    try:\n        return obj_with_meta.with_meta(meta_map)  # type: ignore\n    except AttributeError:\n        raise SyntaxError(\n            f\"Can not attach metadata to object of type {type(obj_with_meta)}\"\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading a function reader macro from the input stream.", "response": "def _read_function(ctx: ReaderContext) -> llist.List:\n    \"\"\"Read a function reader macro from the input stream.\"\"\"\n    if ctx.is_in_anon_fn:\n        raise SyntaxError(f\"Nested #() definitions not allowed\")\n\n    with ctx.in_anon_fn():\n        form = _read_list(ctx)\n    arg_set = set()\n\n    def arg_suffix(arg_num):\n        if arg_num is None:\n            return \"1\"\n        elif arg_num == \"&\":\n            return \"rest\"\n        else:\n            return arg_num\n\n    def sym_replacement(arg_num):\n        suffix = arg_suffix(arg_num)\n        return symbol.symbol(f\"arg-{suffix}\")\n\n    def identify_and_replace(f):\n        if isinstance(f, symbol.Symbol):\n            if f.ns is None:\n                match = fn_macro_args.match(f.name)\n                if match is not None:\n                    arg_num = match.group(2)\n                    suffix = arg_suffix(arg_num)\n                    arg_set.add(suffix)\n                    return sym_replacement(arg_num)\n        return f\n\n    body = walk.postwalk(identify_and_replace, form) if len(form) > 0 else None\n\n    arg_list: List[symbol.Symbol] = []\n    numbered_args = sorted(map(int, filter(lambda k: k != \"rest\", arg_set)))\n    if len(numbered_args) > 0:\n        max_arg = max(numbered_args)\n        arg_list = [sym_replacement(str(i)) for i in range(1, max_arg + 1)]\n        if \"rest\" in arg_set:\n            arg_list.append(_AMPERSAND)\n            arg_list.append(sym_replacement(\"rest\"))\n\n    return llist.l(_FN, vector.vector(arg_list), body)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads a quoted form from the input stream.", "response": "def _read_quoted(ctx: ReaderContext) -> llist.List:\n    \"\"\"Read a quoted form from the input stream.\"\"\"\n    start = ctx.reader.advance()\n    assert start == \"'\"\n    next_form = _read_next_consuming_comment(ctx)\n    return llist.l(_QUOTE, next_form)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _expand_syntax_quote(\n    ctx: ReaderContext, form: IterableLispForm\n) -> Iterable[LispForm]:\n    \"\"\"Expand syntax quoted forms to handle unquoting and unquote-splicing.\n\n    The unquoted form (unquote x) becomes:\n        (list x)\n\n    The unquote-spliced form (unquote-splicing x) becomes\n        x\n\n    All other forms are recursively processed as by _process_syntax_quoted_form\n    and are returned as:\n        (list form)\"\"\"\n    expanded = []\n\n    for elem in form:\n        if _is_unquote(elem):\n            expanded.append(llist.l(_LIST, elem[1]))\n        elif _is_unquote_splicing(elem):\n            expanded.append(elem[1])\n        else:\n            expanded.append(llist.l(_LIST, _process_syntax_quoted_form(ctx, elem)))\n\n    return expanded", "response": "Expand syntax quoted forms to handle unquoting and unquote - splicing."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nposts - process syntax quoted forms to generate forms that can be assembled into the correct types at runtime.", "response": "def _process_syntax_quoted_form(ctx: ReaderContext, form: ReaderForm) -> ReaderForm:\n    \"\"\"Post-process syntax quoted forms to generate forms that can be assembled\n    into the correct types at runtime.\n\n    Lists are turned into:\n        (basilisp.core/seq\n         (basilisp.core/concat [& rest]))\n\n    Vectors are turned into:\n        (basilisp.core/apply\n         basilisp.core/vector\n         (basilisp.core/concat [& rest]))\n\n    Sets are turned into:\n        (basilisp.core/apply\n         basilisp.core/hash-set\n         (basilisp.core/concat [& rest]))\n\n    Maps are turned into:\n        (basilisp.core/apply\n         basilisp.core/hash-map\n         (basilisp.core/concat [& rest]))\n\n    The child forms (called rest above) are processed by _expand_syntax_quote.\n\n    All other forms are passed through without modification.\"\"\"\n    lconcat = lambda v: llist.list(v).cons(_CONCAT)\n    if _is_unquote(form):\n        return form[1]  # type: ignore\n    elif _is_unquote_splicing(form):\n        raise SyntaxError(\"Cannot splice outside collection\")\n    elif isinstance(form, llist.List):\n        return llist.l(_SEQ, lconcat(_expand_syntax_quote(ctx, form)))\n    elif isinstance(form, vector.Vector):\n        return llist.l(_APPLY, _VECTOR, lconcat(_expand_syntax_quote(ctx, form)))\n    elif isinstance(form, lset.Set):\n        return llist.l(_APPLY, _HASH_SET, lconcat(_expand_syntax_quote(ctx, form)))\n    elif isinstance(form, lmap.Map):\n        flat_kvs = seq(form.items()).flatten().to_list()\n        return llist.l(_APPLY, _HASH_MAP, lconcat(_expand_syntax_quote(ctx, flat_kvs)))\n    elif isinstance(form, symbol.Symbol):\n        if form.ns is None and form.name.endswith(\"#\"):\n            try:\n                return llist.l(_QUOTE, ctx.gensym_env[form.name])\n            except KeyError:\n                genned = symbol.symbol(langutil.genname(form.name[:-1])).with_meta(\n                    form.meta\n                )\n                ctx.gensym_env[form.name] = genned\n                return llist.l(_QUOTE, genned)\n        return llist.l(_QUOTE, form)\n    else:\n        return form"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading a syntax - quoted form and set the syntax - quoting state in the reader.", "response": "def _read_syntax_quoted(ctx: ReaderContext) -> ReaderForm:\n    \"\"\"Read a syntax-quote and set the syntax-quoting state in the reader.\"\"\"\n    start = ctx.reader.advance()\n    assert start == \"`\"\n\n    with ctx.syntax_quoted():\n        return _process_syntax_quoted_form(ctx, _read_next_consuming_comment(ctx))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _read_unquote(ctx: ReaderContext) -> LispForm:\n    start = ctx.reader.advance()\n    assert start == \"~\"\n\n    with ctx.unquoted():\n        next_char = ctx.reader.peek()\n        if next_char == \"@\":\n            ctx.reader.advance()\n            next_form = _read_next_consuming_comment(ctx)\n            return llist.l(_UNQUOTE_SPLICING, next_form)\n        else:\n            next_form = _read_next_consuming_comment(ctx)\n            return llist.l(_UNQUOTE, next_form)", "response": "Read an unquoted form and handle any special logic of unquoting."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _read_deref(ctx: ReaderContext) -> LispForm:\n    start = ctx.reader.advance()\n    assert start == \"@\"\n    next_form = _read_next_consuming_comment(ctx)\n    return llist.l(_DEREF, next_form)", "response": "Read a derefed form from the input stream."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread a single character from the input stream.", "response": "def _read_character(ctx: ReaderContext) -> str:\n    \"\"\"Read a character literal from the input stream.\n\n    Character literals may appear as:\n      - \\\\a \\\\b \\\\c etc will yield 'a', 'b', and 'c' respectively\n\n      - \\\\newline, \\\\space, \\\\tab, \\\\formfeed, \\\\backspace, \\\\return yield\n        the named characters\n\n      - \\\\uXXXX yield the unicode digit corresponding to the code\n        point named by the hex digits XXXX\"\"\"\n    start = ctx.reader.advance()\n    assert start == \"\\\\\"\n\n    s: List[str] = []\n    reader = ctx.reader\n    token = reader.peek()\n    while True:\n        if token == \"\" or whitespace_chars.match(token):\n            break\n        if not alphanumeric_chars.match(token):\n            break\n        s.append(token)\n        token = reader.next_token()\n\n    char = \"\".join(s)\n    special = _SPECIAL_CHARS.get(char, None)\n    if special is not None:\n        return special\n\n    match = unicode_char.match(char)\n    if match is not None:\n        try:\n            return chr(int(f\"0x{match.group(1)}\", 16))\n        except (ValueError, OverflowError):\n            raise SyntaxError(f\"Unsupported character \\\\u{char}\") from None\n\n    if len(char) > 1:\n        raise SyntaxError(f\"Unsupported character \\\\{char}\")\n\n    return char"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread a regex reader macro from the input stream.", "response": "def _read_regex(ctx: ReaderContext) -> Pattern:\n    \"\"\"Read a regex reader macro from the input stream.\"\"\"\n    s = _read_str(ctx, allow_arbitrary_escapes=True)\n    try:\n        return langutil.regex_from_str(s)\n    except re.error:\n        raise SyntaxError(f\"Unrecognized regex pattern syntax: {s}\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _read_reader_macro(ctx: ReaderContext) -> LispReaderForm:\n    start = ctx.reader.advance()\n    assert start == \"#\"\n    token = ctx.reader.peek()\n    if token == \"{\":\n        return _read_set(ctx)\n    elif token == \"(\":\n        return _read_function(ctx)\n    elif token == \"'\":\n        ctx.reader.advance()\n        s = _read_sym(ctx)\n        return llist.l(_VAR, s)\n    elif token == '\"':\n        return _read_regex(ctx)\n    elif token == \"_\":\n        ctx.reader.advance()\n        _read_next(ctx)  # Ignore the entire next form\n        return COMMENT\n    elif ns_name_chars.match(token):\n        s = _read_sym(ctx)\n        assert isinstance(s, symbol.Symbol)\n        v = _read_next_consuming_comment(ctx)\n        if s in ctx.data_readers:\n            f = ctx.data_readers[s]\n            return f(v)\n        else:\n            raise SyntaxError(f\"No data reader found for tag #{s}\")\n\n    raise SyntaxError(f\"Unexpected token '{token}' in reader macro\")", "response": "Return a data structure evaluated as a reader\n    macro from the input stream."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread a single - line comment from the input stream. Return the next form after the next line break.", "response": "def _read_comment(ctx: ReaderContext) -> LispReaderForm:\n    \"\"\"Read (and ignore) a single-line comment from the input stream.\n    Return the next form after the next line break.\"\"\"\n    reader = ctx.reader\n    start = reader.advance()\n    assert start == \";\"\n    while True:\n        token = reader.peek()\n        if newline_chars.match(token):\n            reader.advance()\n            return _read_next(ctx)\n        if token == \"\":\n            return ctx.eof\n        reader.advance()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _read_next_consuming_comment(ctx: ReaderContext) -> ReaderForm:\n    while True:\n        v = _read_next(ctx)\n        if v is ctx.eof:\n            return ctx.eof\n        if v is COMMENT or isinstance(v, Comment):\n            continue\n        return v", "response": "Read the next full form consuming any\n    reader comments completely."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _read_next(ctx: ReaderContext) -> LispReaderForm:  # noqa: C901\n    reader = ctx.reader\n    token = reader.peek()\n    if token == \"(\":\n        return _read_list(ctx)\n    elif token == \"[\":\n        return _read_vector(ctx)\n    elif token == \"{\":\n        return _read_map(ctx)\n    elif begin_num_chars.match(token):\n        return _read_num(ctx)\n    elif whitespace_chars.match(token):\n        reader.next_token()\n        return _read_next(ctx)\n    elif token == \":\":\n        return _read_kw(ctx)\n    elif token == '\"':\n        return _read_str(ctx)\n    elif token == \"'\":\n        return _read_quoted(ctx)\n    elif token == \"\\\\\":\n        return _read_character(ctx)\n    elif ns_name_chars.match(token):\n        return _read_sym(ctx)\n    elif token == \"#\":\n        return _read_reader_macro(ctx)\n    elif token == \"^\":\n        return _read_meta(ctx)  # type: ignore\n    elif token == \";\":\n        return _read_comment(ctx)\n    elif token == \"`\":\n        return _read_syntax_quoted(ctx)\n    elif token == \"~\":\n        return _read_unquote(ctx)\n    elif token == \"@\":\n        return _read_deref(ctx)\n    elif token == \"\":\n        return ctx.eof\n    else:\n        raise SyntaxError(\"Unexpected token '{token}'\".format(token=token))", "response": "Read the next full form from the input stream."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading the contents of a Lisp grammar from a stream.", "response": "def read(\n    stream,\n    resolver: Resolver = None,\n    data_readers: DataReaders = None,\n    eof: Any = EOF,\n    is_eof_error: bool = False,\n) -> Iterable[ReaderForm]:\n    \"\"\"Read the contents of a stream as a Lisp expression.\n\n    Callers may optionally specify a namespace resolver, which will be used\n    to adjudicate the fully-qualified name of symbols appearing inside of\n    a syntax quote.\n\n    Callers may optionally specify a map of custom data readers that will\n    be used to resolve values in reader macros. Data reader tags specified\n    by callers must be namespaced symbols; non-namespaced symbols are\n    reserved by the reader. Data reader functions must be functions taking\n    one argument and returning a value.\n\n    The caller is responsible for closing the input stream.\"\"\"\n    reader = StreamReader(stream)\n    ctx = ReaderContext(reader, resolver=resolver, data_readers=data_readers, eof=eof)\n    while True:\n        expr = _read_next(ctx)\n        if expr is ctx.eof:\n            if is_eof_error:\n                raise EOFError\n            return\n        if expr is COMMENT or isinstance(expr, Comment):\n            continue\n        yield expr"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading the contents of a string as a Lisp expression.", "response": "def read_str(\n    s: str,\n    resolver: Resolver = None,\n    data_readers: DataReaders = None,\n    eof: Any = None,\n    is_eof_error: bool = False,\n) -> Iterable[ReaderForm]:\n    \"\"\"Read the contents of a string as a Lisp expression.\n\n    Keyword arguments to this function have the same meanings as those of\n    basilisp.lang.reader.read.\"\"\"\n    with io.StringIO(s) as buf:\n        yield from read(\n            buf,\n            resolver=resolver,\n            data_readers=data_readers,\n            eof=eof,\n            is_eof_error=is_eof_error,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads the contents of a file as a Lisp expression.", "response": "def read_file(\n    filename: str,\n    resolver: Resolver = None,\n    data_readers: DataReaders = None,\n    eof: Any = None,\n    is_eof_error: bool = False,\n) -> Iterable[ReaderForm]:\n    \"\"\"Read the contents of a file as a Lisp expression.\n\n    Keyword arguments to this function have the same meanings as those of\n    basilisp.lang.reader.read.\"\"\"\n    with open(filename) as f:\n        yield from read(\n            f,\n            resolver=resolver,\n            data_readers=data_readers,\n            eof=eof,\n            is_eof_error=is_eof_error,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the internal line and column buffers after a new character is added.", "response": "def _update_loc(self, c):\n        \"\"\"Update the internal line and column buffers after a new character\n        is added.\n\n        The column number is set to 0, so the first character on the next line\n        is column number 1.\"\"\"\n        if newline_chars.match(c):\n            self._col.append(0)\n            self._line.append(self._line[-1] + 1)\n        else:\n            self._col.append(self._col[-1] + 1)\n            self._line.append(self._line[-1])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npushing one character back onto the stream allowing it to be read again.", "response": "def pushback(self) -> None:\n        \"\"\"Push one character back onto the stream, allowing it to be\n        read again.\"\"\"\n        if abs(self._idx - 1) > self._pushback_depth:\n            raise IndexError(\"Exceeded pushback depth\")\n        self._idx -= 1"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadvance the stream forward by one character and return the next token in the stream.", "response": "def next_token(self) -> str:\n        \"\"\"Advance the stream forward by one character and return the\n        next token in the stream.\"\"\"\n        if self._idx < StreamReader.DEFAULT_INDEX:\n            self._idx += 1\n        else:\n            c = self._stream.read(1)\n            self._update_loc(c)\n            self._buffer.append(c)\n        return self.peek()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _basilisp_bytecode(\n    mtime: int, source_size: int, code: List[types.CodeType]\n) -> bytes:\n    \"\"\"Return the bytes for a Basilisp bytecode cache file.\"\"\"\n    data = bytearray(MAGIC_NUMBER)\n    data.extend(_w_long(mtime))\n    data.extend(_w_long(source_size))\n    data.extend(marshal.dumps(code))  # type: ignore\n    return data", "response": "Return the bytes for a Basilisp bytecode cache file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _cache_from_source(path: str) -> str:\n    cache_path, cache_file = os.path.split(importlib.util.cache_from_source(path))\n    filename, _ = os.path.splitext(cache_file)\n    return os.path.join(cache_path, filename + \".lpyc\")", "response": "Return the path to the cached file for the given path."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef hook_imports():\n    if any([isinstance(o, BasilispImporter) for o in sys.meta_path]):\n        return\n    sys.meta_path.insert(\n        0, BasilispImporter()  # pylint:disable=abstract-class-instantiated\n    )", "response": "Hook into Python s import machinery with a custom Basilisp code\n    importer."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds the ModuleSpec for the specified Basilisp module.", "response": "def find_spec(\n        self,\n        fullname: str,\n        path,  # Optional[List[str]] # MyPy complains this is incompatible with supertype\n        target: types.ModuleType = None,\n    ) -> Optional[importlib.machinery.ModuleSpec]:\n        \"\"\"Find the ModuleSpec for the specified Basilisp module.\n\n        Returns None if the module is not a Basilisp module to allow import processing to continue.\"\"\"\n        package_components = fullname.split(\".\")\n        if path is None:\n            path = sys.path\n            module_name = package_components\n        else:\n            module_name = [package_components[-1]]\n\n        for entry in path:\n            filenames = [\n                f\"{os.path.join(entry, *module_name, '__init__')}.lpy\",\n                f\"{os.path.join(entry, *module_name)}.lpy\",\n            ]\n            for filename in filenames:\n                if os.path.exists(filename):\n                    state = {\n                        \"fullname\": fullname,\n                        \"filename\": filename,\n                        \"path\": entry,\n                        \"target\": target,\n                        \"cache_filename\": _cache_from_source(filename),\n                    }\n                    logger.debug(\n                        f\"Found potential Basilisp module '{fullname}' in file '{filename}'\"\n                    )\n                    return importlib.machinery.ModuleSpec(\n                        fullname, self, origin=filename, loader_state=state\n                    )\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload and execute a cached Basilisp module.", "response": "def _exec_cached_module(\n        self,\n        fullname: str,\n        loader_state: Mapping[str, str],\n        path_stats: Mapping[str, int],\n        module: types.ModuleType,\n    ):\n        \"\"\"Load and execute a cached Basilisp module.\"\"\"\n        filename = loader_state[\"filename\"]\n        cache_filename = loader_state[\"cache_filename\"]\n\n        with timed(\n            lambda duration: logger.debug(\n                f\"Loaded cached Basilisp module '{fullname}' in {duration / 1000000}ms\"\n            )\n        ):\n            logger.debug(f\"Checking for cached Basilisp module '{fullname}''\")\n            cache_data = self.get_data(cache_filename)\n            cached_code = _get_basilisp_bytecode(\n                fullname, path_stats[\"mtime\"], path_stats[\"size\"], cache_data\n            )\n            compiler.compile_bytecode(\n                cached_code,\n                compiler.GeneratorContext(filename=filename),\n                compiler.PythonASTOptimizer(),\n                module,\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload and execute a non - cached Basilisp module.", "response": "def _exec_module(\n        self,\n        fullname: str,\n        loader_state: Mapping[str, str],\n        path_stats: Mapping[str, int],\n        module: types.ModuleType,\n    ):\n        \"\"\"Load and execute a non-cached Basilisp module.\"\"\"\n        filename = loader_state[\"filename\"]\n        cache_filename = loader_state[\"cache_filename\"]\n\n        with timed(\n            lambda duration: logger.debug(\n                f\"Loaded Basilisp module '{fullname}' in {duration / 1000000}ms\"\n            )\n        ):\n            # During compilation, bytecode objects are added to the list via the closure\n            # add_bytecode below, which is passed to the compiler. The collected bytecodes\n            # will be used to generate an .lpyc file for caching the compiled file.\n            all_bytecode = []\n\n            def add_bytecode(bytecode: types.CodeType):\n                all_bytecode.append(bytecode)\n\n            logger.debug(f\"Reading and compiling Basilisp module '{fullname}'\")\n            forms = reader.read_file(filename, resolver=runtime.resolve_alias)\n            compiler.compile_module(  # pylint: disable=unexpected-keyword-arg\n                forms,\n                compiler.CompilerContext(filename=filename),\n                module,\n                collect_bytecode=add_bytecode,\n            )\n\n        # Cache the bytecode that was collected through the compilation run.\n        cache_file_bytes = _basilisp_bytecode(\n            path_stats[\"mtime\"], path_stats[\"size\"], all_bytecode\n        )\n        self._cache_bytecode(filename, cache_filename, cache_file_bytes)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompile the Basilisp module into Python code.", "response": "def exec_module(self, module):\n        \"\"\"Compile the Basilisp module into Python code.\n\n        Basilisp is fundamentally a form-at-a-time compilation, meaning that\n        each form in a module may require code compiled from an earlier form, so\n        we incrementally compile a Python module by evaluating a single top-level\n        form at a time and inserting the resulting AST nodes into the Pyton module.\"\"\"\n        fullname = module.__name__\n        cached = self._cache[fullname]\n        cached[\"module\"] = module\n        spec = cached[\"spec\"]\n        filename = spec.loader_state[\"filename\"]\n        path_stats = self.path_stats(filename)\n\n        # During the bootstrapping process, the 'basilisp.core namespace is created with\n        # a blank module. If we do not replace the module here with the module we are\n        # generating, then we will not be able to use advanced compilation features such\n        # as direct Python variable access to functions and other def'ed values.\n        ns_name = demunge(fullname)\n        ns: runtime.Namespace = runtime.set_current_ns(ns_name).value\n        ns.module = module\n\n        # Check if a valid, cached version of this Basilisp namespace exists and, if so,\n        # load it and bypass the expensive compilation process below.\n        if os.getenv(_NO_CACHE_ENVVAR, None) == \"true\":\n            self._exec_module(fullname, spec.loader_state, path_stats, module)\n        else:\n            try:\n                self._exec_cached_module(\n                    fullname, spec.loader_state, path_stats, module\n                )\n            except (EOFError, ImportError, IOError, OSError) as e:\n                logger.debug(f\"Failed to load cached Basilisp module: {e}\")\n                self._exec_module(fullname, spec.loader_state, path_stats, module)\n\n        # Because we want to (by default) add 'basilisp.core into every namespace by default,\n        # we want to make sure we don't try to add 'basilisp.core into itself, causing a\n        # circular import error.\n        #\n        # Later on, we can probably remove this and just use the 'ns macro to auto-refer\n        # all 'basilisp.core values into the current namespace.\n        runtime.Namespace.add_default_import(ns_name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef symbol(name: str, ns: Optional[str] = None, meta=None) -> Symbol:\n    return Symbol(name, ns=ns, meta=meta)", "response": "Create a new symbol."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an iterable of possible completions for the given text.", "response": "def complete(\n    text: str, kw_cache: atom.Atom[\"PMap[int, Keyword]\"] = __INTERN\n) -> Iterable[str]:\n    \"\"\"Return an iterable of possible completions for the given text.\"\"\"\n    assert text.startswith(\":\")\n    interns = kw_cache.deref()\n    text = text[1:]\n\n    if \"/\" in text:\n        prefix, suffix = text.split(\"/\", maxsplit=1)\n        results = filter(\n            lambda kw: (kw.ns is not None and kw.ns == prefix)\n            and kw.name.startswith(suffix),\n            interns.itervalues(),\n        )\n    else:\n        results = filter(\n            lambda kw: kw.name.startswith(text)\n            or (kw.ns is not None and kw.ns.startswith(text)),\n            interns.itervalues(),\n        )\n\n    return map(str, results)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __get_or_create(\n    kw_cache: \"PMap[int, Keyword]\", h: int, name: str, ns: Optional[str]\n) -> PMap:\n    \"\"\"Private swap function used to either get the interned keyword\n    instance from the input string.\"\"\"\n    if h in kw_cache:\n        return kw_cache\n    kw = Keyword(name, ns=ns)\n    return kw_cache.set(h, kw)", "response": "Private swap function used to get the interned keyword\n    instance from the input string."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef keyword(\n    name: str,\n    ns: Optional[str] = None,\n    kw_cache: atom.Atom[\"PMap[int, Keyword]\"] = __INTERN,\n) -> Keyword:\n    \"\"\"Create a new keyword.\"\"\"\n    h = hash((name, ns))\n    return kw_cache.swap(__get_or_create, h, name, ns)[h]", "response": "Create a new keyword."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _chain_py_ast(*genned: GeneratedPyAST,) -> Tuple[PyASTStream, PyASTStream]:\n    deps = chain.from_iterable(map(lambda n: n.dependencies, genned))\n    nodes = map(lambda n: n.node, genned)\n    return deps, nodes", "response": "Chain a sequence of generated Python ASTs into a tuple of dependency nodes"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating recursive Python Attribute AST nodes for resolving nested names.", "response": "def _load_attr(name: str, ctx: ast.AST = ast.Load()) -> ast.Attribute:\n    \"\"\"Generate recursive Python Attribute AST nodes for resolving nested\n    names.\"\"\"\n    attrs = name.split(\".\")\n\n    def attr_node(node, idx):\n        if idx >= len(attrs):\n            node.ctx = ctx\n            return node\n        return attr_node(\n            ast.Attribute(value=node, attr=attrs[idx], ctx=ast.Load()), idx + 1\n        )\n\n    return attr_node(ast.Name(id=attrs[0], ctx=ast.Load()), 1)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrap simpler AST generators to return a GeneratedPyAST.", "response": "def _simple_ast_generator(gen_ast):\n    \"\"\"Wrap simpler AST generators to return a GeneratedPyAST.\"\"\"\n\n    @wraps(gen_ast)\n    def wrapped_ast_generator(ctx: GeneratorContext, form: LispForm) -> GeneratedPyAST:\n        return GeneratedPyAST(node=gen_ast(ctx, form))\n\n    return wrapped_ast_generator"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nturns a collection of Lisp forms into Python AST nodes.", "response": "def _collection_ast(\n    ctx: GeneratorContext, form: Iterable[Node]\n) -> Tuple[PyASTStream, PyASTStream]:\n    \"\"\"Turn a collection of Lisp forms into Python AST nodes.\"\"\"\n    return _chain_py_ast(*map(partial(gen_py_ast, ctx), form))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving reader metadata from the form s meta map.", "response": "def _clean_meta(form: IMeta) -> LispForm:\n    \"\"\"Remove reader metadata from the form's meta map.\"\"\"\n    assert form.meta is not None, \"Form must have non-null 'meta' attribute\"\n    meta = form.meta.dissoc(reader.READER_LINE_KW, reader.READER_COL_KW)\n    if len(meta) == 0:\n        return None\n    return cast(lmap.Map, meta)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _ast_with_loc(\n    py_ast: GeneratedPyAST, env: NodeEnv, include_dependencies: bool = False\n) -> GeneratedPyAST:\n    \"\"\"Hydrate Generated Python AST nodes with line numbers and column offsets\n    if they exist in the node environment.\"\"\"\n    if env.line is not None:\n        py_ast.node.lineno = env.line\n\n        if include_dependencies:\n            for dep in py_ast.dependencies:\n                dep.lineno = env.line\n\n    if env.col is not None:\n        py_ast.node.col_offset = env.col\n\n        if include_dependencies:\n            for dep in py_ast.dependencies:\n                dep.col_offset = env.col\n\n    return py_ast", "response": "Hydrate Generated Python AST nodes with line numbers and column offsets\n    if they exist in the node environment."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _with_ast_loc(f):\n\n    @wraps(f)\n    def with_lineno_and_col(\n        ctx: GeneratorContext, node: Node, *args, **kwargs\n    ) -> GeneratedPyAST:\n        py_ast = f(ctx, node, *args, **kwargs)\n        return _ast_with_loc(py_ast, node.env)\n\n    return with_lineno_and_col", "response": "Wrap a generator function in a decorator to supply line and column\n    information to the returned Python AST node."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _with_ast_loc_deps(f):\n\n    @wraps(f)\n    def with_lineno_and_col(\n        ctx: GeneratorContext, node: Node, *args, **kwargs\n    ) -> GeneratedPyAST:\n        py_ast = f(ctx, node, *args, **kwargs)\n        return _ast_with_loc(py_ast, node.env, include_dependencies=True)\n\n    return with_lineno_and_col", "response": "Wrap a generator function in a decorator to supply line and column\n    information to the returned Python AST node and dependency nodes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning True if the Var holds a value which should be compiled to a dynamic Var access.", "response": "def _is_dynamic(v: Var) -> bool:\n    \"\"\"Return True if the Var holds a value which should be compiled to a dynamic\n    Var access.\"\"\"\n    return (\n        Maybe(v.meta)\n        .map(lambda m: m.get(SYM_DYNAMIC_META_KEY, None))  # type: ignore\n        .or_else_get(False)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _is_redefable(v: Var) -> bool:\n    return (\n        Maybe(v.meta)\n        .map(lambda m: m.get(SYM_REDEF_META_KEY, None))  # type: ignore\n        .or_else_get(False)\n    )", "response": "Return True if the Var can be redefined."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef statementize(e: ast.AST) -> ast.AST:\n    # noinspection PyPep8\n    if isinstance(\n        e,\n        (\n            ast.Assign,\n            ast.AnnAssign,\n            ast.AugAssign,\n            ast.Expr,\n            ast.Raise,\n            ast.Assert,\n            ast.Pass,\n            ast.Import,\n            ast.ImportFrom,\n            ast.If,\n            ast.For,\n            ast.While,\n            ast.Continue,\n            ast.Break,\n            ast.Try,\n            ast.ExceptHandler,\n            ast.With,\n            ast.FunctionDef,\n            ast.Return,\n            ast.Yield,\n            ast.YieldFrom,\n            ast.Global,\n            ast.ClassDef,\n            ast.AsyncFunctionDef,\n            ast.AsyncFor,\n            ast.AsyncWith,\n        ),\n    ):\n        return e\n    return ast.Expr(value=e)", "response": "Transform non - statements into ast. Expr nodes so they can\n    stand alone as statements."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef expressionize(\n    body: GeneratedPyAST,\n    fn_name: str,\n    args: Optional[Iterable[ast.arg]] = None,\n    vargs: Optional[ast.arg] = None,\n) -> ast.FunctionDef:\n    \"\"\"Given a series of expression AST nodes, create a function AST node\n    with the given name that can be called and will return the result of\n    the final expression in the input body nodes.\n\n    This helps to fix the impedance mismatch of Python, which includes\n    statements and expressions, and Lisps, which have only expressions.\n    \"\"\"\n    args = Maybe(args).or_else_get([])\n    body_nodes: List[ast.AST] = list(map(statementize, body.dependencies))\n    body_nodes.append(ast.Return(value=body.node))\n\n    return ast.FunctionDef(\n        name=fn_name,\n        args=ast.arguments(\n            args=args,\n            kwarg=None,\n            vararg=vargs,\n            kwonlyargs=[],\n            defaults=[],\n            kw_defaults=[],\n        ),\n        body=body_nodes,\n        decorator_list=[],\n        returns=None,\n    )", "response": "Given a series of expression AST nodes create a function AST node that can be called with the given name arguments and vargs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if the compiler should emit a warning about this name being redefined.", "response": "def __should_warn_on_redef(\n    ctx: GeneratorContext, defsym: sym.Symbol, safe_name: str, def_meta: lmap.Map\n) -> bool:\n    \"\"\"Return True if the compiler should emit a warning about this name being redefined.\"\"\"\n    no_warn_on_redef = def_meta.entry(SYM_NO_WARN_ON_REDEF_META_KEY, False)\n    if no_warn_on_redef:\n        return False\n    elif safe_name in ctx.current_ns.module.__dict__:\n        return True\n    elif defsym in ctx.current_ns.interns:\n        var = ctx.current_ns.find(defsym)\n        assert var is not None, f\"Var {defsym} cannot be none here\"\n\n        if var.meta is not None and var.meta.entry(SYM_REDEF_META_KEY):\n            return False\n        elif var.is_bound:\n            return True\n        else:\n            return False\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _def_to_py_ast(  # pylint: disable=too-many-branches\n    ctx: GeneratorContext, node: Def\n) -> GeneratedPyAST:\n    \"\"\"Return a Python AST Node for a `def` expression.\"\"\"\n    assert node.op == NodeOp.DEF\n\n    defsym = node.name\n    is_defn = False\n\n    if node.init is not None:\n        # Since Python function definitions always take the form `def name(...):`,\n        # it is redundant to assign them to the their final name after they have\n        # been defined under a private alias. This codepath generates `defn`\n        # declarations by directly generating the Python `def` with the correct\n        # function name and short-circuiting the default double-declaration.\n        if node.init.op == NodeOp.FN:\n            assert isinstance(node.init, Fn)\n            def_ast = _fn_to_py_ast(ctx, node.init, def_name=defsym.name)\n            is_defn = True\n        elif (\n            node.init.op == NodeOp.WITH_META\n            and isinstance(node.init, WithMeta)\n            and node.init.expr.op == NodeOp.FN\n        ):\n            assert isinstance(node.init, WithMeta)\n            def_ast = _with_meta_to_py_ast(ctx, node.init, def_name=defsym.name)\n            is_defn = True\n        else:\n            def_ast = gen_py_ast(ctx, node.init)\n    else:\n        def_ast = GeneratedPyAST(node=ast.NameConstant(None))\n\n    ns_name = ast.Call(func=_NEW_SYM_FN_NAME, args=[_NS_VAR_NAME], keywords=[])\n    def_name = ast.Call(func=_NEW_SYM_FN_NAME, args=[ast.Str(defsym.name)], keywords=[])\n    safe_name = munge(defsym.name)\n\n    assert node.meta is not None, \"Meta should always be attached to Def nodes\"\n    def_meta = node.meta.form\n    assert isinstance(def_meta, lmap.Map), \"Meta should always be a map\"\n\n    # If the Var is marked as dynamic, we need to generate a keyword argument\n    # for the generated Python code to set the Var as dynamic\n    is_dynamic = def_meta.entry(SYM_DYNAMIC_META_KEY, False)\n    dynamic_kwarg = (\n        [ast.keyword(arg=\"dynamic\", value=ast.NameConstant(is_dynamic))]\n        if is_dynamic\n        else []\n    )\n\n    # Warn if this symbol is potentially being redefined\n    if __should_warn_on_redef(ctx, defsym, safe_name, def_meta):\n        logger.warning(\n            f\"redefining local Python name '{safe_name}' in module '{ctx.current_ns.module.__name__}'\"\n        )\n\n    meta_ast = gen_py_ast(ctx, node.meta)\n\n    # For defn style def generation, we specifically need to generate the\n    # global declaration prior to emitting the Python `def` otherwise the\n    # Python compiler will throw an exception during compilation\n    # complaining that we assign the value prior to global declaration.\n    if is_defn:\n        def_dependencies = list(\n            chain(\n                [] if node.top_level else [ast.Global(names=[safe_name])],\n                def_ast.dependencies,\n                [] if meta_ast is None else meta_ast.dependencies,\n            )\n        )\n    else:\n        def_dependencies = list(\n            chain(\n                def_ast.dependencies,\n                [] if node.top_level else [ast.Global(names=[safe_name])],\n                [\n                    ast.Assign(\n                        targets=[ast.Name(id=safe_name, ctx=ast.Store())],\n                        value=def_ast.node,\n                    )\n                ],\n                [] if meta_ast is None else meta_ast.dependencies,\n            )\n        )\n\n    return GeneratedPyAST(\n        node=ast.Call(\n            func=_INTERN_VAR_FN_NAME,\n            args=[ns_name, def_name, ast.Name(id=safe_name, ctx=ast.Load())],\n            keywords=list(\n                chain(\n                    dynamic_kwarg,\n                    []\n                    if meta_ast is None\n                    else [ast.keyword(arg=\"meta\", value=meta_ast.node)],\n                )\n            ),\n        ),\n        dependencies=def_dependencies,\n    )", "response": "Return a Python AST Node for a def expression."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _deftype_to_py_ast(  # pylint: disable=too-many-branches\n    ctx: GeneratorContext, node: DefType\n) -> GeneratedPyAST:\n    \"\"\"Return a Python AST Node for a `deftype*` expression.\"\"\"\n    assert node.op == NodeOp.DEFTYPE\n    type_name = munge(node.name)\n    ctx.symbol_table.new_symbol(sym.symbol(node.name), type_name, LocalType.DEFTYPE)\n\n    bases = []\n    for base in node.interfaces:\n        base_node = gen_py_ast(ctx, base)\n        assert (\n            count(base_node.dependencies) == 0\n        ), \"Class and host form nodes do not have dependencies\"\n        bases.append(base_node.node)\n\n    decorator = ast.Call(\n        func=_ATTR_CLASS_DECORATOR_NAME,\n        args=[],\n        keywords=[\n            ast.keyword(arg=\"cmp\", value=ast.NameConstant(False)),\n            ast.keyword(arg=\"frozen\", value=ast.NameConstant(node.is_frozen)),\n            ast.keyword(arg=\"slots\", value=ast.NameConstant(True)),\n        ],\n    )\n\n    with ctx.new_symbol_table(node.name):\n        type_nodes = []\n        for field in node.fields:\n            safe_field = munge(field.name)\n            type_nodes.append(\n                ast.Assign(\n                    targets=[ast.Name(id=safe_field, ctx=ast.Store())],\n                    value=ast.Call(func=_ATTRIB_FIELD_FN_NAME, args=[], keywords=[]),\n                )\n            )\n            ctx.symbol_table.new_symbol(sym.symbol(field.name), safe_field, field.local)\n\n        type_deps: List[ast.AST] = []\n        for method in node.methods:\n            type_ast = __deftype_method_to_py_ast(ctx, method)\n            type_nodes.append(type_ast.node)\n            type_deps.extend(type_ast.dependencies)\n\n        return GeneratedPyAST(\n            node=ast.Name(id=type_name, ctx=ast.Load()),\n            dependencies=list(\n                chain(\n                    type_deps,\n                    [\n                        ast.ClassDef(\n                            name=type_name,\n                            bases=bases,\n                            keywords=[],\n                            body=type_nodes,\n                            decorator_list=[decorator],\n                        )\n                    ],\n                )\n            ),\n        )", "response": "Return a Python AST Node for a deftype* expression."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a Python AST Node for a do expression.", "response": "def _do_to_py_ast(ctx: GeneratorContext, node: Do) -> GeneratedPyAST:\n    \"\"\"Return a Python AST Node for a `do` expression.\"\"\"\n    assert node.op == NodeOp.DO\n    assert not node.is_body\n\n    body_ast = GeneratedPyAST.reduce(\n        *map(partial(gen_py_ast, ctx), chain(node.statements, [node.ret]))\n    )\n\n    fn_body_ast: List[ast.AST] = []\n    do_result_name = genname(_DO_PREFIX)\n    fn_body_ast.extend(map(statementize, body_ast.dependencies))\n    fn_body_ast.append(\n        ast.Assign(\n            targets=[ast.Name(id=do_result_name, ctx=ast.Store())], value=body_ast.node\n        )\n    )\n\n    return GeneratedPyAST(\n        node=ast.Name(id=do_result_name, ctx=ast.Load()), dependencies=fn_body_ast\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _synthetic_do_to_py_ast(ctx: GeneratorContext, node: Do) -> GeneratedPyAST:\n    assert node.op == NodeOp.DO\n    assert node.is_body\n\n    # TODO: investigate how to handle recur in node.ret\n\n    return GeneratedPyAST.reduce(\n        *map(partial(gen_py_ast, ctx), chain(node.statements, [node.ret]))\n    )", "response": "Return AST elements generated from reducing a synthetic Lisp Do node."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __fn_name(s: Optional[str]) -> str:\n    return genname(\"__\" + munge(Maybe(s).or_else_get(_FN_PREFIX)))", "response": "Generate a safe Python function name from a function name symbol."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate a list of Python AST nodes from function method parameters.", "response": "def __fn_args_to_py_ast(\n    ctx: GeneratorContext, params: Iterable[Binding], body: Do\n) -> Tuple[List[ast.arg], Optional[ast.arg], List[ast.AST]]:\n    \"\"\"Generate a list of Python AST nodes from function method parameters.\"\"\"\n    fn_args, varg = [], None\n    fn_body_ast: List[ast.AST] = []\n    for binding in params:\n        assert binding.init is None, \":fn nodes cannot have bindint :inits\"\n        assert varg is None, \"Must have at most one variadic arg\"\n        arg_name = genname(munge(binding.name))\n\n        if not binding.is_variadic:\n            fn_args.append(ast.arg(arg=arg_name, annotation=None))\n            ctx.symbol_table.new_symbol(\n                sym.symbol(binding.name), arg_name, LocalType.ARG\n            )\n        else:\n            varg = ast.arg(arg=arg_name, annotation=None)\n            safe_local = genname(munge(binding.name))\n            fn_body_ast.append(\n                ast.Assign(\n                    targets=[ast.Name(id=safe_local, ctx=ast.Store())],\n                    value=ast.Call(\n                        func=_COLLECT_ARGS_FN_NAME,\n                        args=[ast.Name(id=arg_name, ctx=ast.Load())],\n                        keywords=[],\n                    ),\n                )\n            )\n            ctx.symbol_table.new_symbol(\n                sym.symbol(binding.name), safe_local, LocalType.ARG\n            )\n\n    body_ast = _synthetic_do_to_py_ast(ctx, body)\n    fn_body_ast.extend(map(statementize, body_ast.dependencies))\n    fn_body_ast.append(ast.Return(value=body_ast.node))\n\n    return fn_args, varg, fn_body_ast"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __single_arity_fn_to_py_ast(\n    ctx: GeneratorContext,\n    node: Fn,\n    method: FnMethod,\n    def_name: Optional[str] = None,\n    meta_node: Optional[MetaNode] = None,\n) -> GeneratedPyAST:\n    \"\"\"Return a Python AST node for a function with a single arity.\"\"\"\n    assert node.op == NodeOp.FN\n    assert method.op == NodeOp.FN_METHOD\n\n    lisp_fn_name = node.local.name if node.local is not None else None\n    py_fn_name = __fn_name(lisp_fn_name) if def_name is None else munge(def_name)\n    py_fn_node = ast.AsyncFunctionDef if node.is_async else ast.FunctionDef\n    with ctx.new_symbol_table(py_fn_name), ctx.new_recur_point(\n        method.loop_id, RecurType.FN, is_variadic=node.is_variadic\n    ):\n        # Allow named anonymous functions to recursively call themselves\n        if lisp_fn_name is not None:\n            ctx.symbol_table.new_symbol(\n                sym.symbol(lisp_fn_name), py_fn_name, LocalType.FN\n            )\n\n        fn_args, varg, fn_body_ast = __fn_args_to_py_ast(\n            ctx, method.params, method.body\n        )\n        meta_deps, meta_decorators = __fn_meta(ctx, meta_node)\n        return GeneratedPyAST(\n            node=ast.Name(id=py_fn_name, ctx=ast.Load()),\n            dependencies=list(\n                chain(\n                    meta_deps,\n                    [\n                        py_fn_node(\n                            name=py_fn_name,\n                            args=ast.arguments(\n                                args=fn_args,\n                                kwarg=None,\n                                vararg=varg,\n                                kwonlyargs=[],\n                                defaults=[],\n                                kw_defaults=[],\n                            ),\n                            body=fn_body_ast,\n                            decorator_list=list(\n                                chain(\n                                    meta_decorators,\n                                    [_BASILISP_FN_FN_NAME],\n                                    [_TRAMPOLINE_FN_NAME]\n                                    if ctx.recur_point.has_recur\n                                    else [],\n                                )\n                            ),\n                            returns=None,\n                        )\n                    ],\n                )\n            ),\n        )", "response": "Return a Python AST node for a single arity function."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the Python AST nodes for a multi - arity dispatch function.", "response": "def __multi_arity_dispatch_fn(  # pylint: disable=too-many-arguments,too-many-locals\n    ctx: GeneratorContext,\n    name: str,\n    arity_map: Mapping[int, str],\n    default_name: Optional[str] = None,\n    max_fixed_arity: Optional[int] = None,\n    meta_node: Optional[MetaNode] = None,\n    is_async: bool = False,\n) -> GeneratedPyAST:\n    \"\"\"Return the Python AST nodes for a argument-length dispatch function\n    for multi-arity functions.\n\n    def fn(*args):\n        nargs = len(args)\n        method = __fn_dispatch_map.get(nargs)\n        if method:\n            return method(*args)\n        # Only if default\n        if nargs > max_fixed_arity:\n            return default(*args)\n        raise RuntimeError\n    \"\"\"\n    dispatch_map_name = f\"{name}_dispatch_map\"\n\n    dispatch_keys, dispatch_vals = [], []\n    for k, v in arity_map.items():\n        dispatch_keys.append(ast.Num(k))\n        dispatch_vals.append(ast.Name(id=v, ctx=ast.Load()))\n\n    # Async functions should return await, otherwise just return\n    handle_return = __handle_async_return if is_async else __handle_return\n\n    nargs_name = genname(\"nargs\")\n    method_name = genname(\"method\")\n    body = [\n        ast.Assign(\n            targets=[ast.Name(id=nargs_name, ctx=ast.Store())],\n            value=ast.Call(\n                func=ast.Name(id=\"len\", ctx=ast.Load()),\n                args=[ast.Name(id=_MULTI_ARITY_ARG_NAME, ctx=ast.Load())],\n                keywords=[],\n            ),\n        ),\n        ast.Assign(\n            targets=[ast.Name(id=method_name, ctx=ast.Store())],\n            value=ast.Call(\n                func=ast.Attribute(\n                    value=ast.Name(id=dispatch_map_name, ctx=ast.Load()),\n                    attr=\"get\",\n                    ctx=ast.Load(),\n                ),\n                args=[ast.Name(id=nargs_name, ctx=ast.Load())],\n                keywords=[],\n            ),\n        ),\n        ast.If(\n            test=ast.Compare(\n                left=ast.NameConstant(None),\n                ops=[ast.IsNot()],\n                comparators=[ast.Name(id=method_name, ctx=ast.Load())],\n            ),\n            body=[\n                handle_return(\n                    ast.Call(\n                        func=ast.Name(id=method_name, ctx=ast.Load()),\n                        args=[\n                            ast.Starred(\n                                value=ast.Name(\n                                    id=_MULTI_ARITY_ARG_NAME, ctx=ast.Load()\n                                ),\n                                ctx=ast.Load(),\n                            )\n                        ],\n                        keywords=[],\n                    )\n                )\n            ],\n            orelse=[]\n            if default_name is None\n            else [\n                ast.If(\n                    test=ast.Compare(\n                        left=ast.Name(id=nargs_name, ctx=ast.Load()),\n                        ops=[ast.GtE()],\n                        comparators=[ast.Num(max_fixed_arity)],\n                    ),\n                    body=[\n                        handle_return(\n                            ast.Call(\n                                func=ast.Name(id=default_name, ctx=ast.Load()),\n                                args=[\n                                    ast.Starred(\n                                        value=ast.Name(\n                                            id=_MULTI_ARITY_ARG_NAME, ctx=ast.Load()\n                                        ),\n                                        ctx=ast.Load(),\n                                    )\n                                ],\n                                keywords=[],\n                            )\n                        )\n                    ],\n                    orelse=[],\n                )\n            ],\n        ),\n        ast.Raise(\n            exc=ast.Call(\n                func=_load_attr(\"basilisp.lang.runtime.RuntimeException\"),\n                args=[\n                    ast.Str(f\"Wrong number of args passed to function: {name}\"),\n                    ast.Name(id=nargs_name, ctx=ast.Load()),\n                ],\n                keywords=[],\n            ),\n            cause=None,\n        ),\n    ]\n\n    py_fn_node = ast.AsyncFunctionDef if is_async else ast.FunctionDef\n    meta_deps, meta_decorators = __fn_meta(ctx, meta_node)\n    return GeneratedPyAST(\n        node=ast.Name(id=name, ctx=ast.Load()),\n        dependencies=chain(\n            [\n                ast.Assign(\n                    targets=[ast.Name(id=dispatch_map_name, ctx=ast.Store())],\n                    value=ast.Dict(keys=dispatch_keys, values=dispatch_vals),\n                )\n            ],\n            meta_deps,\n            [\n                py_fn_node(\n                    name=name,\n                    args=ast.arguments(\n                        args=[],\n                        kwarg=None,\n                        vararg=ast.arg(arg=_MULTI_ARITY_ARG_NAME, annotation=None),\n                        kwonlyargs=[],\n                        defaults=[],\n                        kw_defaults=[],\n                    ),\n                    body=body,\n                    decorator_list=list(chain(meta_decorators, [_BASILISP_FN_FN_NAME])),\n                    returns=None,\n                )\n            ],\n        ),\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __multi_arity_fn_to_py_ast(  # pylint: disable=too-many-locals\n    ctx: GeneratorContext,\n    node: Fn,\n    methods: Collection[FnMethod],\n    def_name: Optional[str] = None,\n    meta_node: Optional[MetaNode] = None,\n) -> GeneratedPyAST:\n    \"\"\"Return a Python AST node for a function with multiple arities.\"\"\"\n    assert node.op == NodeOp.FN\n    assert all([method.op == NodeOp.FN_METHOD for method in methods])\n\n    lisp_fn_name = node.local.name if node.local is not None else None\n    py_fn_name = __fn_name(lisp_fn_name) if def_name is None else munge(def_name)\n\n    py_fn_node = ast.AsyncFunctionDef if node.is_async else ast.FunctionDef\n\n    arity_to_name = {}\n    rest_arity_name: Optional[str] = None\n    fn_defs = []\n    for method in methods:\n        arity_name = f\"{py_fn_name}__arity{'_rest' if method.is_variadic else method.fixed_arity}\"\n        if method.is_variadic:\n            rest_arity_name = arity_name\n        else:\n            arity_to_name[method.fixed_arity] = arity_name\n\n        with ctx.new_symbol_table(arity_name), ctx.new_recur_point(\n            method.loop_id, RecurType.FN, is_variadic=node.is_variadic\n        ):\n            # Allow named anonymous functions to recursively call themselves\n            if lisp_fn_name is not None:\n                ctx.symbol_table.new_symbol(\n                    sym.symbol(lisp_fn_name), py_fn_name, LocalType.FN\n                )\n\n            fn_args, varg, fn_body_ast = __fn_args_to_py_ast(\n                ctx, method.params, method.body\n            )\n            fn_defs.append(\n                py_fn_node(\n                    name=arity_name,\n                    args=ast.arguments(\n                        args=fn_args,\n                        kwarg=None,\n                        vararg=varg,\n                        kwonlyargs=[],\n                        defaults=[],\n                        kw_defaults=[],\n                    ),\n                    body=fn_body_ast,\n                    decorator_list=[_TRAMPOLINE_FN_NAME]\n                    if ctx.recur_point.has_recur\n                    else [],\n                    returns=None,\n                )\n            )\n\n    dispatch_fn_ast = __multi_arity_dispatch_fn(\n        ctx,\n        py_fn_name,\n        arity_to_name,\n        default_name=rest_arity_name,\n        max_fixed_arity=node.max_fixed_arity,\n        meta_node=meta_node,\n        is_async=node.is_async,\n    )\n\n    return GeneratedPyAST(\n        node=dispatch_fn_ast.node,\n        dependencies=list(chain(fn_defs, dispatch_fn_ast.dependencies)),\n    )", "response": "Return a Python AST node for a function with multiple arities."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _fn_to_py_ast(\n    ctx: GeneratorContext,\n    node: Fn,\n    def_name: Optional[str] = None,\n    meta_node: Optional[MetaNode] = None,\n) -> GeneratedPyAST:\n    \"\"\"Return a Python AST Node for a `fn` expression.\"\"\"\n    assert node.op == NodeOp.FN\n    if len(node.methods) == 1:\n        return __single_arity_fn_to_py_ast(\n            ctx, node, next(iter(node.methods)), def_name=def_name, meta_node=meta_node\n        )\n    else:\n        return __multi_arity_fn_to_py_ast(\n            ctx, node, node.methods, def_name=def_name, meta_node=meta_node\n        )", "response": "Return a Python AST Node for a fn expression."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __if_body_to_py_ast(\n    ctx: GeneratorContext, node: Node, result_name: str\n) -> GeneratedPyAST:\n    \"\"\"Generate custom `if` nodes to handle `recur` bodies.\n\n    Recur nodes can appear in the then and else expressions of `if` forms.\n    Recur nodes generate Python `continue` statements, which we would otherwise\n    attempt to insert directly into an expression. Python will complain if\n    it finds a statement in an expression AST slot, so we special case the\n    recur handling here.\"\"\"\n    if node.op == NodeOp.RECUR and ctx.recur_point.type == RecurType.LOOP:\n        assert isinstance(node, Recur)\n        return _recur_to_py_ast(ctx, node)\n    elif node.op == NodeOp.DO:\n        assert isinstance(node, Do)\n        if_body = _synthetic_do_to_py_ast(ctx, node.assoc(is_body=True))\n        return GeneratedPyAST(\n            node=ast.Assign(\n                targets=[ast.Name(id=result_name, ctx=ast.Store())], value=if_body.node\n            ),\n            dependencies=list(map(statementize, if_body.dependencies)),\n        )\n    else:\n        py_ast = gen_py_ast(ctx, node)\n        return GeneratedPyAST(\n            node=ast.Assign(\n                targets=[ast.Name(id=result_name, ctx=ast.Store())], value=py_ast.node\n            ),\n            dependencies=py_ast.dependencies,\n        )", "response": "Generate the ast for the if statement."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _if_to_py_ast(ctx: GeneratorContext, node: If) -> GeneratedPyAST:\n    assert node.op == NodeOp.IF\n\n    test_ast = gen_py_ast(ctx, node.test)\n    result_name = genname(_IF_RESULT_PREFIX)\n\n    then_ast = __if_body_to_py_ast(ctx, node.then, result_name)\n    else_ast = __if_body_to_py_ast(ctx, node.else_, result_name)\n\n    test_name = genname(_IF_TEST_PREFIX)\n    test_assign = ast.Assign(\n        targets=[ast.Name(id=test_name, ctx=ast.Store())], value=test_ast.node\n    )\n\n    ifstmt = ast.If(\n        test=ast.BoolOp(\n            op=ast.Or(),\n            values=[\n                ast.Compare(\n                    left=ast.NameConstant(None),\n                    ops=[ast.Is()],\n                    comparators=[ast.Name(id=test_name, ctx=ast.Load())],\n                ),\n                ast.Compare(\n                    left=ast.NameConstant(False),\n                    ops=[ast.Is()],\n                    comparators=[ast.Name(id=test_name, ctx=ast.Load())],\n                ),\n            ],\n        ),\n        values=[],\n        body=list(map(statementize, chain(else_ast.dependencies, [else_ast.node]))),\n        orelse=list(map(statementize, chain(then_ast.dependencies, [then_ast.node]))),\n    )\n\n    return GeneratedPyAST(\n        node=ast.Name(id=result_name, ctx=ast.Load()),\n        dependencies=list(chain(test_ast.dependencies, [test_assign, ifstmt])),\n    )", "response": "Generate an intermediate if statement which assigns to a temporary\n    variable which is returned as the expression value at the end of the evaluation."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a Python AST node for a Basilisp import expression.", "response": "def _import_to_py_ast(ctx: GeneratorContext, node: Import) -> GeneratedPyAST:\n    \"\"\"Return a Python AST node for a Basilisp `import*` expression.\"\"\"\n    assert node.op == NodeOp.IMPORT\n\n    last = None\n    deps: List[ast.AST] = []\n    for alias in node.aliases:\n        safe_name = munge(alias.name)\n\n        try:\n            module = importlib.import_module(safe_name)\n            if alias.alias is not None:\n                ctx.add_import(sym.symbol(alias.name), module, sym.symbol(alias.alias))\n            else:\n                ctx.add_import(sym.symbol(alias.name), module)\n        except ModuleNotFoundError as e:\n            raise ImportError(\n                f\"Python module '{alias.name}' not found\", node.form, node\n            ) from e\n\n        py_import_alias = (\n            munge(alias.alias)\n            if alias.alias is not None\n            else safe_name.split(\".\", maxsplit=1)[0]\n        )\n        deps.append(\n            ast.Assign(\n                targets=[ast.Name(id=py_import_alias, ctx=ast.Store())],\n                value=ast.Call(\n                    func=_load_attr(\"builtins.__import__\"),\n                    args=[ast.Str(safe_name)],\n                    keywords=[],\n                ),\n            )\n        )\n        last = ast.Name(id=py_import_alias, ctx=ast.Load())\n\n        # Note that we add this import to the live running system in the above\n        # calls to `ctx.add_import`, however, since we compile and cache Python\n        # bytecode, we need to generate calls to `add_import` for the running\n        # namespace so when this code is reloaded from the cache, the runtime\n        # is correctly configured.\n        deps.append(\n            ast.Call(\n                func=_load_attr(f\"{_NS_VAR_VALUE}.add_import\"),\n                args=[\n                    ast.Call(\n                        func=_NEW_SYM_FN_NAME, args=[ast.Str(safe_name)], keywords=[]\n                    ),\n                    last,\n                ],\n                keywords=[],\n            )\n        )\n\n    assert last is not None, \"import* node must have at least one import\"\n    return GeneratedPyAST(node=last, dependencies=deps)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _invoke_to_py_ast(ctx: GeneratorContext, node: Invoke) -> GeneratedPyAST:\n    assert node.op == NodeOp.INVOKE\n\n    fn_ast = gen_py_ast(ctx, node.fn)\n    args_deps, args_nodes = _collection_ast(ctx, node.args)\n\n    return GeneratedPyAST(\n        node=ast.Call(func=fn_ast.node, args=list(args_nodes), keywords=[]),\n        dependencies=list(chain(fn_ast.dependencies, args_deps)),\n    )", "response": "Return a Python AST Node for a Basilisp function invocation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a Python AST Node for a let expression.", "response": "def _let_to_py_ast(ctx: GeneratorContext, node: Let) -> GeneratedPyAST:\n    \"\"\"Return a Python AST Node for a `let*` expression.\"\"\"\n    assert node.op == NodeOp.LET\n\n    with ctx.new_symbol_table(\"let\"):\n        let_body_ast: List[ast.AST] = []\n        for binding in node.bindings:\n            init_node = binding.init\n            assert init_node is not None\n            init_ast = gen_py_ast(ctx, init_node)\n            binding_name = genname(munge(binding.name))\n            let_body_ast.extend(init_ast.dependencies)\n            let_body_ast.append(\n                ast.Assign(\n                    targets=[ast.Name(id=binding_name, ctx=ast.Store())],\n                    value=init_ast.node,\n                )\n            )\n            ctx.symbol_table.new_symbol(\n                sym.symbol(binding.name), binding_name, LocalType.LET\n            )\n\n        let_result_name = genname(\"let_result\")\n        body_ast = _synthetic_do_to_py_ast(ctx, node.body)\n        let_body_ast.extend(map(statementize, body_ast.dependencies))\n        let_body_ast.append(\n            ast.Assign(\n                targets=[ast.Name(id=let_result_name, ctx=ast.Store())],\n                value=body_ast.node,\n            )\n        )\n\n        return GeneratedPyAST(\n            node=ast.Name(id=let_result_name, ctx=ast.Load()), dependencies=let_body_ast\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a Python AST Node for a loop expression.", "response": "def _loop_to_py_ast(ctx: GeneratorContext, node: Loop) -> GeneratedPyAST:\n    \"\"\"Return a Python AST Node for a `loop*` expression.\"\"\"\n    assert node.op == NodeOp.LOOP\n\n    with ctx.new_symbol_table(\"loop\"):\n        binding_names = []\n        init_bindings: List[ast.AST] = []\n        for binding in node.bindings:\n            init_node = binding.init\n            assert init_node is not None\n            init_ast = gen_py_ast(ctx, init_node)\n            init_bindings.extend(init_ast.dependencies)\n            binding_name = genname(munge(binding.name))\n            binding_names.append(binding_name)\n            init_bindings.append(\n                ast.Assign(\n                    targets=[ast.Name(id=binding_name, ctx=ast.Store())],\n                    value=init_ast.node,\n                )\n            )\n            ctx.symbol_table.new_symbol(\n                sym.symbol(binding.name), binding_name, LocalType.LOOP\n            )\n\n        loop_result_name = genname(\"loop\")\n        with ctx.new_recur_point(\n            node.loop_id, RecurType.LOOP, binding_names=binding_names\n        ):\n            loop_body_ast: List[ast.AST] = []\n            body_ast = _synthetic_do_to_py_ast(ctx, node.body)\n            loop_body_ast.extend(body_ast.dependencies)\n            loop_body_ast.append(\n                ast.Assign(\n                    targets=[ast.Name(id=loop_result_name, ctx=ast.Store())],\n                    value=body_ast.node,\n                )\n            )\n            loop_body_ast.append(ast.Break())\n\n            return GeneratedPyAST(\n                node=_load_attr(loop_result_name),\n                dependencies=list(\n                    chain(\n                        [\n                            ast.Assign(\n                                targets=[\n                                    ast.Name(id=loop_result_name, ctx=ast.Store())\n                                ],\n                                value=ast.NameConstant(None),\n                            )\n                        ],\n                        init_bindings,\n                        [\n                            ast.While(\n                                test=ast.NameConstant(True),\n                                body=loop_body_ast,\n                                orelse=[],\n                            )\n                        ],\n                    )\n                ),\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _quote_to_py_ast(ctx: GeneratorContext, node: Quote) -> GeneratedPyAST:\n    assert node.op == NodeOp.QUOTE\n    return _const_node_to_py_ast(ctx, node.expr)", "response": "Return a Python AST Node for a quote expression."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __fn_recur_to_py_ast(ctx: GeneratorContext, node: Recur) -> GeneratedPyAST:\n    assert node.op == NodeOp.RECUR\n    assert ctx.recur_point.is_variadic is not None\n    recur_nodes: List[ast.AST] = []\n    recur_deps: List[ast.AST] = []\n    for expr in node.exprs:\n        expr_ast = gen_py_ast(ctx, expr)\n        recur_nodes.append(expr_ast.node)\n        recur_deps.extend(expr_ast.dependencies)\n\n    return GeneratedPyAST(\n        node=ast.Call(\n            func=_TRAMPOLINE_ARGS_FN_NAME,\n            args=list(\n                chain([ast.NameConstant(ctx.recur_point.is_variadic)], recur_nodes)\n            ),\n            keywords=[],\n        ),\n        dependencies=recur_deps,\n    )", "response": "Return a Python AST node for a given recur."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a Python AST node for recur occurring inside a deftype * method.", "response": "def __deftype_method_recur_to_py_ast(\n    ctx: GeneratorContext, node: Recur\n) -> GeneratedPyAST:\n    \"\"\"Return a Python AST node for `recur` occurring inside a `deftype*` method.\"\"\"\n    assert node.op == NodeOp.RECUR\n    recur_nodes: List[ast.AST] = []\n    recur_deps: List[ast.AST] = []\n    for expr in node.exprs:\n        expr_ast = gen_py_ast(ctx, expr)\n        recur_nodes.append(expr_ast.node)\n        recur_deps.extend(expr_ast.dependencies)\n\n    this_entry = ctx.symbol_table.find_symbol(ctx.current_this)\n    assert this_entry is not None, \"Field type local must have this\"\n\n    return GeneratedPyAST(\n        node=ast.Call(\n            func=_TRAMPOLINE_ARGS_FN_NAME,\n            args=list(\n                chain(\n                    [\n                        ast.NameConstant(ctx.recur_point.is_variadic),\n                        ast.Name(id=this_entry.munged, ctx=ast.Load()),\n                    ],\n                    recur_nodes,\n                )\n            ),\n            keywords=[],\n        ),\n        dependencies=recur_deps,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a Python AST node for a loop occurring inside a loop.", "response": "def __loop_recur_to_py_ast(ctx: GeneratorContext, node: Recur) -> GeneratedPyAST:\n    \"\"\"Return a Python AST node for `recur` occurring inside a `loop`.\"\"\"\n    assert node.op == NodeOp.RECUR\n\n    recur_deps: List[ast.AST] = []\n    recur_targets: List[ast.Name] = []\n    recur_exprs: List[ast.AST] = []\n    for name, expr in zip(ctx.recur_point.binding_names, node.exprs):\n        expr_ast = gen_py_ast(ctx, expr)\n        recur_deps.extend(expr_ast.dependencies)\n        recur_targets.append(ast.Name(id=name, ctx=ast.Store()))\n        recur_exprs.append(expr_ast.node)\n\n    if len(recur_targets) == 1:\n        assert len(recur_exprs) == 1\n        recur_deps.append(ast.Assign(targets=recur_targets, value=recur_exprs[0]))\n    else:\n        recur_deps.append(\n            ast.Assign(\n                targets=[ast.Tuple(elts=recur_targets, ctx=ast.Store())],\n                value=ast.Tuple(elts=recur_exprs, ctx=ast.Load()),\n            )\n        )\n    recur_deps.append(ast.Continue())\n\n    return GeneratedPyAST(node=ast.NameConstant(None), dependencies=recur_deps)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a Python AST Node for a recur expression.", "response": "def _recur_to_py_ast(ctx: GeneratorContext, node: Recur) -> GeneratedPyAST:\n    \"\"\"Return a Python AST Node for a `recur` expression.\n\n    Note that `recur` nodes can only legally appear in two AST locations:\n      (1) in :then or :else expressions in :if nodes, and\n      (2) in :ret expressions in :do nodes\n\n    As such, both of these handlers special case the recur construct, as it\n    is the only case in which the code generator emits a statement rather than\n    an expression.\"\"\"\n    assert node.op == NodeOp.RECUR\n    assert ctx.recur_point is not None, \"Must have set a recur point to recur\"\n    handle_recur = _RECUR_TYPE_HANDLER.get(ctx.recur_point.type)\n    assert (\n        handle_recur is not None\n    ), f\"No recur point handler defined for {ctx.recur_point.type}\"\n    ctx.recur_point.has_recur = True\n    return handle_recur(ctx, node)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _set_bang_to_py_ast(ctx: GeneratorContext, node: SetBang) -> GeneratedPyAST:\n    assert node.op == NodeOp.SET_BANG\n\n    val_temp_name = genname(\"set_bang_val\")\n    val_ast = gen_py_ast(ctx, node.val)\n\n    target = node.target\n    assert isinstance(\n        target, (HostField, Local, VarRef)\n    ), f\"invalid set! target type {type(target)}\"\n\n    if isinstance(target, HostField):\n        target_ast = _interop_prop_to_py_ast(ctx, target, is_assigning=True)\n    elif isinstance(target, VarRef):\n        target_ast = _var_sym_to_py_ast(ctx, target, is_assigning=True)\n    elif isinstance(target, Local):\n        target_ast = _local_sym_to_py_ast(ctx, target, is_assigning=True)\n    else:  # pragma: no cover\n        raise GeneratorException(\n            f\"invalid set! target type {type(target)}\", lisp_ast=target\n        )\n\n    return GeneratedPyAST(\n        node=ast.Name(id=val_temp_name, ctx=ast.Load()),\n        dependencies=list(\n            chain(\n                val_ast.dependencies,\n                [\n                    ast.Assign(\n                        targets=[ast.Name(id=val_temp_name, ctx=ast.Store())],\n                        value=val_ast.node,\n                    )\n                ],\n                target_ast.dependencies,\n                [ast.Assign(targets=[target_ast.node], value=val_ast.node)],\n            )\n        ),\n    )", "response": "Return a Python AST Node for a set! expression."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _throw_to_py_ast(ctx: GeneratorContext, node: Throw) -> GeneratedPyAST:\n    assert node.op == NodeOp.THROW\n\n    throw_fn = genname(_THROW_PREFIX)\n    exc_ast = gen_py_ast(ctx, node.exception)\n    raise_body = ast.Raise(exc=exc_ast.node, cause=None)\n\n    return GeneratedPyAST(\n        node=ast.Call(func=ast.Name(id=throw_fn, ctx=ast.Load()), args=[], keywords=[]),\n        dependencies=[\n            ast.FunctionDef(\n                name=throw_fn,\n                args=ast.arguments(\n                    args=[],\n                    kwarg=None,\n                    vararg=None,\n                    kwonlyargs=[],\n                    defaults=[],\n                    kw_defaults=[],\n                ),\n                body=list(chain(exc_ast.dependencies, [raise_body])),\n                decorator_list=[],\n                returns=None,\n            )\n        ],\n    )", "response": "Return a Python AST Node for a Throw expression."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a Python AST Node for a try expression.", "response": "def _try_to_py_ast(ctx: GeneratorContext, node: Try) -> GeneratedPyAST:\n    \"\"\"Return a Python AST Node for a `try` expression.\"\"\"\n    assert node.op == NodeOp.TRY\n\n    try_expr_name = genname(\"try_expr\")\n\n    body_ast = _synthetic_do_to_py_ast(ctx, node.body)\n    catch_handlers = list(\n        map(partial(__catch_to_py_ast, ctx, try_expr_name=try_expr_name), node.catches)\n    )\n\n    finallys: List[ast.AST] = []\n    if node.finally_ is not None:\n        finally_ast = _synthetic_do_to_py_ast(ctx, node.finally_)\n        finallys.extend(map(statementize, finally_ast.dependencies))\n        finallys.append(statementize(finally_ast.node))\n\n    return GeneratedPyAST(\n        node=ast.Name(id=try_expr_name, ctx=ast.Load()),\n        dependencies=[\n            ast.Try(\n                body=list(\n                    chain(\n                        body_ast.dependencies,\n                        [\n                            ast.Assign(\n                                targets=[ast.Name(id=try_expr_name, ctx=ast.Store())],\n                                value=body_ast.node,\n                            )\n                        ],\n                    )\n                ),\n                handlers=catch_handlers,\n                orelse=[],\n                finalbody=finallys,\n            )\n        ],\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates a Python AST node for accessing a locally defined Python variable.", "response": "def _local_sym_to_py_ast(\n    ctx: GeneratorContext, node: Local, is_assigning: bool = False\n) -> GeneratedPyAST:\n    \"\"\"Generate a Python AST node for accessing a locally defined Python variable.\"\"\"\n    assert node.op == NodeOp.LOCAL\n\n    sym_entry = ctx.symbol_table.find_symbol(sym.symbol(node.name))\n    assert sym_entry is not None\n\n    if node.local == LocalType.FIELD:\n        this_entry = ctx.symbol_table.find_symbol(ctx.current_this)\n        assert this_entry is not None, \"Field type local must have this\"\n\n        return GeneratedPyAST(\n            node=_load_attr(\n                f\"{this_entry.munged}.{sym_entry.munged}\",\n                ctx=ast.Store() if is_assigning else ast.Load(),\n            )\n        )\n    else:\n        return GeneratedPyAST(\n            node=ast.Name(\n                id=sym_entry.munged, ctx=ast.Store() if is_assigning else ast.Load()\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates Var. find calls for the named symbol.", "response": "def __var_find_to_py_ast(\n    var_name: str, ns_name: str, py_var_ctx: ast.AST\n) -> GeneratedPyAST:\n    \"\"\"Generate Var.find calls for the named symbol.\"\"\"\n    return GeneratedPyAST(\n        node=ast.Attribute(\n            value=ast.Call(\n                func=_FIND_VAR_FN_NAME,\n                args=[\n                    ast.Call(\n                        func=_NEW_SYM_FN_NAME,\n                        args=[ast.Str(var_name)],\n                        keywords=[ast.keyword(arg=\"ns\", value=ast.Str(ns_name))],\n                    )\n                ],\n                keywords=[],\n            ),\n            attr=\"value\",\n            ctx=py_var_ctx,\n        )\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating a Python AST node for accessing a Var.", "response": "def _var_sym_to_py_ast(\n    ctx: GeneratorContext, node: VarRef, is_assigning: bool = False\n) -> GeneratedPyAST:\n    \"\"\"Generate a Python AST node for accessing a Var.\n\n    If the Var is marked as :dynamic or :redef or the compiler option\n    USE_VAR_INDIRECTION is active, do not compile to a direct access.\n    If the corresponding function name is not defined in a Python module,\n    no direct variable access is possible and Var.find indirection must be\n    used.\"\"\"\n    assert node.op == NodeOp.VAR\n\n    var = node.var\n    ns = var.ns\n    ns_name = ns.name\n    ns_module = ns.module\n    safe_ns = munge(ns_name)\n    var_name = var.name.name\n    py_var_ctx = ast.Store() if is_assigning else ast.Load()\n\n    # Return the actual var, rather than its value if requested\n    if node.return_var:\n        return GeneratedPyAST(\n            node=ast.Call(\n                func=_FIND_VAR_FN_NAME,\n                args=[\n                    ast.Call(\n                        func=_NEW_SYM_FN_NAME,\n                        args=[ast.Str(var_name)],\n                        keywords=[ast.keyword(arg=\"ns\", value=ast.Str(ns_name))],\n                    )\n                ],\n                keywords=[],\n            )\n        )\n\n    # Check if we should use Var indirection\n    if ctx.use_var_indirection or _is_dynamic(var) or _is_redefable(var):\n        return __var_find_to_py_ast(var_name, ns_name, py_var_ctx)\n\n    # Otherwise, try to direct-link it like a Python variable\n    # Try without allowing builtins first\n    safe_name = munge(var_name)\n    if safe_name not in ns_module.__dict__:\n        # Try allowing builtins\n        safe_name = munge(var_name, allow_builtins=True)\n\n    if safe_name in ns_module.__dict__:\n        if ns is ctx.current_ns:\n            return GeneratedPyAST(node=ast.Name(id=safe_name, ctx=py_var_ctx))\n        return GeneratedPyAST(node=_load_attr(f\"{safe_ns}.{safe_name}\", ctx=py_var_ctx))\n\n    if ctx.warn_on_var_indirection:\n        logger.warning(f\"could not resolve a direct link to Var '{var_name}'\")\n\n    return __var_find_to_py_ast(var_name, ns_name, py_var_ctx)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a Python AST node for Python interop method calls.", "response": "def _interop_call_to_py_ast(ctx: GeneratorContext, node: HostCall) -> GeneratedPyAST:\n    \"\"\"Generate a Python AST node for Python interop method calls.\"\"\"\n    assert node.op == NodeOp.HOST_CALL\n\n    target_ast = gen_py_ast(ctx, node.target)\n    args_deps, args_nodes = _collection_ast(ctx, node.args)\n\n    return GeneratedPyAST(\n        node=ast.Call(\n            func=ast.Attribute(\n                value=target_ast.node,\n                attr=munge(node.method, allow_builtins=True),\n                ctx=ast.Load(),\n            ),\n            args=list(args_nodes),\n            keywords=[],\n        ),\n        dependencies=list(chain(target_ast.dependencies, args_deps)),\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a Python AST node for Python interop property access.", "response": "def _interop_prop_to_py_ast(\n    ctx: GeneratorContext, node: HostField, is_assigning: bool = False\n) -> GeneratedPyAST:\n    \"\"\"Generate a Python AST node for Python interop property access.\"\"\"\n    assert node.op == NodeOp.HOST_FIELD\n\n    target_ast = gen_py_ast(ctx, node.target)\n\n    return GeneratedPyAST(\n        node=ast.Attribute(\n            value=target_ast.node,\n            attr=munge(node.field),\n            ctx=ast.Store() if is_assigning else ast.Load(),\n        ),\n        dependencies=target_ast.dependencies,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _maybe_class_to_py_ast(_: GeneratorContext, node: MaybeClass) -> GeneratedPyAST:\n    assert node.op == NodeOp.MAYBE_CLASS\n    return GeneratedPyAST(\n        node=ast.Name(\n            id=Maybe(_MODULE_ALIASES.get(node.class_)).or_else_get(node.class_),\n            ctx=ast.Load(),\n        )\n    )", "response": "Generate a Python AST node for accessing a potential Python module\n    variable name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a Python AST node for accessing a potential Python module variable name with a namespace.", "response": "def _maybe_host_form_to_py_ast(\n    _: GeneratorContext, node: MaybeHostForm\n) -> GeneratedPyAST:\n    \"\"\"Generate a Python AST node for accessing a potential Python module\n    variable name with a namespace.\"\"\"\n    assert node.op == NodeOp.MAYBE_HOST_FORM\n    return GeneratedPyAST(\n        node=_load_attr(\n            f\"{Maybe(_MODULE_ALIASES.get(node.class_)).or_else_get(node.class_)}.{node.field}\"\n        )\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _with_meta_to_py_ast(\n    ctx: GeneratorContext, node: WithMeta, **kwargs\n) -> GeneratedPyAST:\n    \"\"\"Generate a Python AST node for Python interop method calls.\"\"\"\n    assert node.op == NodeOp.WITH_META\n\n    handle_expr = _WITH_META_EXPR_HANDLER.get(node.expr.op)\n    assert (\n        handle_expr is not None\n    ), \"No expression handler for with-meta child node type\"\n    return handle_expr(ctx, node.expr, meta_node=node.meta, **kwargs)", "response": "Generate a Python AST node for Python interop method calls."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _const_val_to_py_ast(ctx: GeneratorContext, form: LispForm) -> GeneratedPyAST:\n    handle_value = _CONST_VALUE_HANDLERS.get(type(form))\n    if handle_value is None and isinstance(form, ISeq):\n        handle_value = _const_seq_to_py_ast  # type: ignore\n    assert handle_value is not None, \"A type handler must be defined for constants\"\n    return handle_value(ctx, form)", "response": "Generate Python AST nodes for a constant Lisp form."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nturns a quoted collection literal of Lisp forms into Python AST nodes.", "response": "def _collection_literal_to_py_ast(\n    ctx: GeneratorContext, form: Iterable[LispForm]\n) -> Iterable[GeneratedPyAST]:\n    \"\"\"Turn a quoted collection literal of Lisp forms into Python AST nodes.\n\n    This function can only handle constant values. It does not call back into\n    the generic AST generators, so only constant values will be generated down\n    this path.\"\"\"\n    yield from map(partial(_const_val_to_py_ast, ctx), form)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate Python AST nodes for a const Lisp AST node.", "response": "def _const_node_to_py_ast(ctx: GeneratorContext, lisp_ast: Const) -> GeneratedPyAST:\n    \"\"\"Generate Python AST nodes for a :const Lisp AST node.\n\n    Nested values in collections for :const nodes are not parsed. Consequently,\n    this function cannot be called recursively for those nested values. Instead,\n    call `_const_val_to_py_ast` on nested values.\"\"\"\n    assert lisp_ast.op == NodeOp.CONST\n    node_type = lisp_ast.type\n    handle_const_node = _CONSTANT_HANDLER.get(node_type)\n    assert handle_const_node is not None, f\"No :const AST type handler for {node_type}\"\n    node_val = lisp_ast.val\n    return handle_const_node(ctx, node_val)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntake a Lisp AST node as an argument and produce zero or more Python AST nodes.", "response": "def gen_py_ast(ctx: GeneratorContext, lisp_ast: Node) -> GeneratedPyAST:\n    \"\"\"Take a Lisp AST node as an argument and produce zero or more Python\n    AST nodes.\n\n    This is the primary entrypoint for generating AST nodes from Lisp\n    syntax. It may be called recursively to compile child forms.\"\"\"\n    op: NodeOp = lisp_ast.op\n    assert op is not None, \"Lisp AST nodes must have an :op key\"\n    handle_node = _NODE_HANDLERS.get(op)\n    assert (\n        handle_node is not None\n    ), f\"Lisp AST nodes :op has no handler defined for op {op}\"\n    return handle_node(ctx, lisp_ast)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _module_imports(ctx: GeneratorContext) -> Iterable[ast.Import]:\n    # Yield `import basilisp` so code attempting to call fully qualified\n    # `basilisp.lang...` modules don't result in compiler errors\n    yield ast.Import(names=[ast.alias(name=\"basilisp\", asname=None)])\n    for imp in ctx.imports:\n        name = imp.key.name\n        alias = _MODULE_ALIASES.get(name, None)\n        yield ast.Import(names=[ast.alias(name=name, asname=alias)])", "response": "Generate the Python Import AST node for importing all required\n    language support modules."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates the Python From... Import AST node for importing language support modules.", "response": "def _from_module_import() -> ast.ImportFrom:\n    \"\"\"Generate the Python From ... Import AST node for importing\n    language support modules.\"\"\"\n    return ast.ImportFrom(\n        module=\"basilisp.lang.runtime\",\n        names=[ast.alias(name=\"Var\", asname=_VAR_ALIAS)],\n        level=0,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nassign a Python variable named ns_var to the value of the current namespace.", "response": "def _ns_var(\n    py_ns_var: str = _NS_VAR, lisp_ns_var: str = LISP_NS_VAR, lisp_ns_ns: str = CORE_NS\n) -> ast.Assign:\n    \"\"\"Assign a Python variable named `ns_var` to the value of the current\n    namespace.\"\"\"\n    return ast.Assign(\n        targets=[ast.Name(id=py_ns_var, ctx=ast.Store())],\n        value=ast.Call(\n            func=_FIND_VAR_FN_NAME,\n            args=[\n                ast.Call(\n                    func=_NEW_SYM_FN_NAME,\n                    args=[ast.Str(lisp_ns_var)],\n                    keywords=[ast.keyword(arg=\"ns\", value=ast.Str(lisp_ns_ns))],\n                )\n            ],\n            keywords=[],\n        ),\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef py_module_preamble(ctx: GeneratorContext,) -> GeneratedPyAST:\n    preamble: List[ast.AST] = []\n    preamble.extend(_module_imports(ctx))\n    preamble.append(_from_module_import())\n    preamble.append(_ns_var())\n    return GeneratedPyAST(node=ast.NameConstant(None), dependencies=preamble)", "response": "Bootstrap a new module with imports and other boilerplate."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if a Var reference cannot be direct linked.", "response": "def warn_on_var_indirection(self) -> bool:\n        \"\"\"If True, warn when a Var reference cannot be direct linked (iff\n        use_var_indirection is False)..\"\"\"\n        return not self.use_var_indirection and self._opts.entry(\n            WARN_ON_VAR_INDIRECTION, True\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set(members: Iterable[T], meta=None) -> Set[T]:  # pylint:disable=redefined-builtin\n    return Set(pset(members), meta=meta)", "response": "Creates a new set with the given members."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef s(*members: T, meta=None) -> Set[T]:\n    return Set(pset(members), meta=meta)", "response": "Creates a new set from members."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _filter_dead_code(nodes: Iterable[ast.AST]) -> List[ast.AST]:\n    new_nodes: List[ast.AST] = []\n    for node in nodes:\n        if isinstance(node, (ast.Break, ast.Continue, ast.Return)):\n            new_nodes.append(node)\n            break\n        new_nodes.append(node)\n    return new_nodes", "response": "Return a list of body nodes that are not unreachable code."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef visit_ExceptHandler(self, node: ast.ExceptHandler) -> Optional[ast.AST]:\n        new_node = self.generic_visit(node)\n        assert isinstance(new_node, ast.ExceptHandler)\n        return ast.copy_location(\n            ast.ExceptHandler(\n                type=new_node.type,\n                name=new_node.name,\n                body=_filter_dead_code(new_node.body),\n            ),\n            new_node,\n        )", "response": "Eliminate dead code from except handler bodies."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef visit_Expr(self, node: ast.Expr) -> Optional[ast.Expr]:\n        if isinstance(\n            node.value,\n            (\n                ast.Constant,  # type: ignore\n                ast.Name,\n                ast.NameConstant,\n                ast.Num,\n                ast.Str,\n            ),\n        ):\n            return None\n        return node", "response": "Eliminate no - op constant expressions which are in the tree\n        as standalone statements."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef visit_FunctionDef(self, node: ast.FunctionDef) -> Optional[ast.AST]:\n        new_node = self.generic_visit(node)\n        assert isinstance(new_node, ast.FunctionDef)\n        return ast.copy_location(\n            ast.FunctionDef(\n                name=new_node.name,\n                args=new_node.args,\n                body=_filter_dead_code(new_node.body),\n                decorator_list=new_node.decorator_list,\n                returns=new_node.returns,\n            ),\n            new_node,\n        )", "response": "Eliminate dead code from function bodies."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef visit_If(self, node: ast.If) -> Optional[ast.AST]:\n        new_node = self.generic_visit(node)\n        assert isinstance(new_node, ast.If)\n        return ast.copy_location(\n            ast.If(\n                test=new_node.test,\n                body=_filter_dead_code(new_node.body),\n                orelse=_filter_dead_code(new_node.orelse),\n            ),\n            new_node,\n        )", "response": "Eliminate dead code from if body."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\neliminate dead code from while bodies.", "response": "def visit_While(self, node: ast.While) -> Optional[ast.AST]:\n        \"\"\"Eliminate dead code from while bodies.\"\"\"\n        new_node = self.generic_visit(node)\n        assert isinstance(new_node, ast.While)\n        return ast.copy_location(\n            ast.While(\n                test=new_node.test,\n                body=_filter_dead_code(new_node.body),\n                orelse=_filter_dead_code(new_node.orelse),\n            ),\n            new_node,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\neliminating dead code from except body.", "response": "def visit_Try(self, node: ast.Try) -> Optional[ast.AST]:\n        \"\"\"Eliminate dead code from except try bodies.\"\"\"\n        new_node = self.generic_visit(node)\n        assert isinstance(new_node, ast.Try)\n        return ast.copy_location(\n            ast.Try(\n                body=_filter_dead_code(new_node.body),\n                handlers=new_node.handlers,\n                orelse=_filter_dead_code(new_node.orelse),\n                finalbody=_filter_dead_code(new_node.finalbody),\n            ),\n            new_node,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _new_module(name: str, doc=None) -> types.ModuleType:\n    mod = types.ModuleType(name, doc=doc)\n    mod.__loader__ = None\n    mod.__package__ = None\n    mod.__spec__ = None\n    mod.__basilisp_bootstrapped__ = False  # type: ignore\n    return mod", "response": "Create a new empty Basilisp Python module."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the first element of the sequence or None if o is None.", "response": "def first(o):\n    \"\"\"If o is a ISeq, return the first element from o. If o is None, return\n    None. Otherwise, coerces o to a Seq and returns the first.\"\"\"\n    if o is None:\n        return None\n    if isinstance(o, ISeq):\n        return o.first\n    s = to_seq(o)\n    if s is None:\n        return None\n    return s.first"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the elements after the first in o.", "response": "def rest(o) -> Optional[ISeq]:\n    \"\"\"If o is a ISeq, return the elements after the first in o. If o is None,\n    returns an empty seq. Otherwise, coerces o to a seq and returns the rest.\"\"\"\n    if o is None:\n        return None\n    if isinstance(o, ISeq):\n        s = o.rest\n        if s is None:\n            return lseq.EMPTY\n        return s\n    n = to_seq(o)\n    if n is None:\n        return lseq.EMPTY\n    return n.rest"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the nth rest sequence of coll.", "response": "def nthrest(coll, i: int):\n    \"\"\"Returns the nth rest sequence of coll, or coll if i is 0.\"\"\"\n    while True:\n        if coll is None:\n            return None\n        if i == 0:\n            return coll\n        i -= 1\n        coll = rest(coll)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the nth next sequence of coll.", "response": "def nthnext(coll, i: int) -> Optional[ISeq]:\n    \"\"\"Returns the nth next sequence of coll.\"\"\"\n    while True:\n        if coll is None:\n            return None\n        if i == 0:\n            return to_seq(coll)\n        i -= 1\n        coll = next_(coll)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cons(o, seq) -> ISeq:\n    if seq is None:\n        return llist.l(o)\n    if isinstance(seq, ISeq):\n        return seq.cons(o)\n    return Maybe(to_seq(seq)).map(lambda s: s.cons(o)).or_else(lambda: llist.l(o))", "response": "Creates a new sequence where o is the first element and seq is the rest."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_seq(o) -> Optional[ISeq]:\n    if o is None:\n        return None\n    if isinstance(o, ISeq):\n        return _seq_or_nil(o)\n    if isinstance(o, ISeqable):\n        return _seq_or_nil(o.seq())\n    return _seq_or_nil(lseq.sequence(o))", "response": "Coerce the argument o to a ISeq."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef concat(*seqs) -> ISeq:\n    allseqs = lseq.sequence(itertools.chain(*filter(None, map(to_seq, seqs))))\n    if allseqs is None:\n        return lseq.EMPTY\n    return allseqs", "response": "Concatenate the sequences given by seqs into a single ISeq."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef apply(f, args):\n    final = list(args[:-1])\n\n    try:\n        last = args[-1]\n    except TypeError as e:\n        logger.debug(\"Ignored %s: %s\", type(e).__name__, e)\n\n    s = to_seq(last)\n    if s is not None:\n        final.extend(s)\n\n    return f(*final)", "response": "Apply function f to the arguments provided."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef apply_kw(f, args):\n    final = list(args[:-1])\n\n    try:\n        last = args[-1]\n    except TypeError as e:\n        logger.debug(\"Ignored %s: %s\", type(e).__name__, e)\n\n    kwargs = to_py(last, lambda kw: munge(kw.name, allow_builtins=True))\n    return f(*final, **kwargs)", "response": "Apply function f to the arguments provided."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef nth(coll, i, notfound=__nth_sentinel):\n    if coll is None:\n        return None\n\n    try:\n        return coll[i]\n    except IndexError as ex:\n        if notfound is not __nth_sentinel:\n            return notfound\n        raise ex\n    except TypeError as ex:\n        # Log these at TRACE so they don't gum up the DEBUG logs since most\n        # cases where this exception occurs are not bugs.\n        logger.log(TRACE, \"Ignored %s: %s\", type(ex).__name__, ex)\n\n    try:\n        for j, e in enumerate(coll):\n            if i == j:\n                return e\n        if notfound is not __nth_sentinel:\n            return notfound\n        raise IndexError(f\"Index {i} out of bounds\")\n    except TypeError:\n        pass\n\n    raise TypeError(f\"nth not supported on object of type {type(coll)}\")", "response": "Returns the ith element of coll."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef assoc(m, *kvs):\n    if m is None:\n        return lmap.Map.empty().assoc(*kvs)\n    if isinstance(m, IAssociative):\n        return m.assoc(*kvs)\n    raise TypeError(\n        f\"Object of type {type(m)} does not implement Associative interface\"\n    )", "response": "Associate keys to values in associative data structure m. Returns a new Map with key - values kvs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating the value for key k in associative data structure m with the return value from calling f ( old_v new_v", "response": "def update(m, k, f, *args):\n    \"\"\"Updates the value for key k in associative data structure m with the return value from\n    calling f(old_v, *args). If m is None, use an empty map. If k is not in m, old_v will be\n    None.\"\"\"\n    if m is None:\n        return lmap.Map.empty().assoc(k, f(None, *args))\n    if isinstance(m, IAssociative):\n        old_v = m.entry(k)\n        new_v = f(old_v, *args)\n        return m.assoc(k, new_v)\n    raise TypeError(\n        f\"Object of type {type(m)} does not implement Associative interface\"\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconjoins xs to collection.", "response": "def conj(coll, *xs):\n    \"\"\"Conjoin xs to collection. New elements may be added in different positions\n    depending on the type of coll. conj returns the same type as coll. If coll\n    is None, return a list with xs conjoined.\"\"\"\n    if coll is None:\n        l = llist.List.empty()\n        return l.cons(*xs)\n    if isinstance(coll, IPersistentCollection):\n        return coll.cons(*xs)\n    raise TypeError(\n        f\"Object of type {type(coll)} does not implement Collection interface\"\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef partial(f, *args):\n\n    @functools.wraps(f)\n    def partial_f(*inner_args):\n        return f(*itertools.chain(args, inner_args))\n\n    return partial_f", "response": "Return a function which is the partial application of f with args."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef deref(o, timeout_s=None, timeout_val=None):\n    if isinstance(o, IDeref):\n        return o.deref()\n    elif isinstance(o, IBlockingDeref):\n        return o.deref(timeout_s, timeout_val)\n    raise TypeError(f\"Object of type {type(o)} cannot be dereferenced\")", "response": "Dereference a Deref object and return its contents."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomparing two objects by value.", "response": "def equals(v1, v2) -> bool:\n    \"\"\"Compare two objects by value. Unlike the standard Python equality operator,\n    this function does not consider 1 == True or 0 == False. All other equality\n    operations are the same and performed using Python's equality operator.\"\"\"\n    if isinstance(v1, (bool, type(None))) or isinstance(v2, (bool, type(None))):\n        return v1 is v2\n    return v1 == v2"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a sorted sequence of the elements in coll using a comparator function f.", "response": "def sort(coll, f=None) -> Optional[ISeq]:\n    \"\"\"Return a sorted sequence of the elements in coll. If a comparator\n    function f is provided, compare elements in coll using f.\"\"\"\n    return to_seq(sorted(coll, key=Maybe(f).map(functools.cmp_to_key).value))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef contains(coll, k):\n    if isinstance(coll, IAssociative):\n        return coll.contains(k)\n    return k in coll", "response": "Return true if o contains the key k."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the value of k in m. Return default if k not found in m.", "response": "def get(m, k, default=None):\n    \"\"\"Return the value of k in m. Return default if k not found in m.\"\"\"\n    if isinstance(m, IAssociative):\n        return m.entry(k, default=default)\n\n    try:\n        return m[k]\n    except (KeyError, IndexError, TypeError) as e:\n        logger.debug(\"Ignored %s: %s\", type(e).__name__, e)\n        return default"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_py(o, keyword_fn: Callable[[kw.Keyword], Any] = _kw_name):\n    if isinstance(o, ISeq):\n        return _to_py_list(o, keyword_fn=keyword_fn)\n    elif not isinstance(\n        o, (IPersistentList, IPersistentMap, IPersistentSet, IPersistentVector)\n    ):\n        return o\n    else:  # pragma: no cover\n        return _to_py_backup(o, keyword_fn=keyword_fn)", "response": "Recursively convert Lisp collections into Python collections."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nproducing a string representation of an object.", "response": "def lrepr(o, human_readable: bool = False) -> str:\n    \"\"\"Produce a string representation of an object. If human_readable is False,\n    the string representation of Lisp objects is something that can be read back\n    in by the reader as the same object.\"\"\"\n    core_ns = Namespace.get(sym.symbol(CORE_NS))\n    assert core_ns is not None\n    return lobj.lrepr(\n        o,\n        human_readable=human_readable,\n        print_dup=core_ns.find(sym.symbol(_PRINT_DUP_VAR_NAME)).value,  # type: ignore\n        print_length=core_ns.find(  # type: ignore\n            sym.symbol(_PRINT_LENGTH_VAR_NAME)\n        ).value,\n        print_level=core_ns.find(  # type: ignore\n            sym.symbol(_PRINT_LEVEL_VAR_NAME)\n        ).value,\n        print_meta=core_ns.find(sym.symbol(_PRINT_META_VAR_NAME)).value,  # type: ignore\n        print_readably=core_ns.find(  # type: ignore\n            sym.symbol(_PRINT_READABLY_VAR_NAME)\n        ).value,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncollect Python starred arguments into a Basilisp list.", "response": "def _collect_args(args) -> ISeq:\n    \"\"\"Collect Python starred arguments into a Basilisp list.\"\"\"\n    if isinstance(args, tuple):\n        return llist.list(args)\n    raise TypeError(\"Python variadic arguments should always be a tuple\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _trampoline(f):\n\n    @functools.wraps(f)\n    def trampoline(*args, **kwargs):\n        while True:\n            ret = f(*args, **kwargs)\n            if isinstance(ret, _TrampolineArgs):\n                args = ret.args\n                kwargs = ret.kwargs\n                continue\n            return ret\n\n    return trampoline", "response": "Trampoline a function repeatedly until it is finished recurring to help\n    avoid stack growth."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _fn_with_meta(f, meta: Optional[lmap.Map]):\n\n    if not isinstance(meta, lmap.Map):\n        raise TypeError(\"meta must be a map\")\n\n    if inspect.iscoroutinefunction(f):\n\n        @functools.wraps(f)\n        async def wrapped_f(*args, **kwargs):\n            return await f(*args, **kwargs)\n\n    else:\n\n        @functools.wraps(f)  # type: ignore\n        def wrapped_f(*args, **kwargs):\n            return f(*args, **kwargs)\n\n    wrapped_f.meta = (  # type: ignore\n        f.meta.update(meta)\n        if hasattr(f, \"meta\") and isinstance(f.meta, lmap.Map)\n        else meta\n    )\n    wrapped_f.with_meta = partial(_fn_with_meta, wrapped_f)  # type: ignore\n    return wrapped_f", "response": "Return a new function with the given meta."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _basilisp_fn(f):\n    assert not hasattr(f, \"meta\")\n    f._basilisp_fn = True\n    f.meta = None\n    f.with_meta = partial(_fn_with_meta, f)\n    return f", "response": "Create a Basilisp function setting meta and supplying a with_meta\n    method implementation."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef init_ns_var(which_ns: str = CORE_NS, ns_var_name: str = NS_VAR_NAME) -> Var:\n    core_sym = sym.Symbol(which_ns)\n    core_ns = Namespace.get_or_create(core_sym)\n    ns_var = Var.intern(core_sym, sym.Symbol(ns_var_name), core_ns, dynamic=True)\n    logger.debug(f\"Created namespace variable {sym.symbol(ns_var_name, ns=which_ns)}\")\n    return ns_var", "response": "Initialize the dynamic * ns * variable in the Namespace which_ns."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_current_ns(\n    ns_name: str,\n    module: types.ModuleType = None,\n    ns_var_name: str = NS_VAR_NAME,\n    ns_var_ns: str = NS_VAR_NS,\n) -> Var:\n    \"\"\"Set the value of the dynamic variable `*ns*` in the current thread.\"\"\"\n    symbol = sym.Symbol(ns_name)\n    ns = Namespace.get_or_create(symbol, module=module)\n    ns_var_sym = sym.Symbol(ns_var_name, ns=ns_var_ns)\n    ns_var = Maybe(Var.find(ns_var_sym)).or_else_raise(\n        lambda: RuntimeException(\n            f\"Dynamic Var {sym.Symbol(ns_var_name, ns=ns_var_ns)} not bound!\"\n        )\n    )\n    ns_var.push_bindings(ns)\n    logger.debug(f\"Setting {ns_var_sym} to {ns}\")\n    return ns_var", "response": "Set the value of the dynamic variable ns_var_name in the current thread."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_current_ns(\n    ns_var_name: str = NS_VAR_NAME, ns_var_ns: str = NS_VAR_NS\n) -> Namespace:\n    \"\"\"Get the value of the dynamic variable `*ns*` in the current thread.\"\"\"\n    ns_sym = sym.Symbol(ns_var_name, ns=ns_var_ns)\n    ns: Namespace = Maybe(Var.find(ns_sym)).map(lambda v: v.value).or_else_raise(\n        lambda: RuntimeException(f\"Dynamic Var {ns_sym} not bound!\")\n    )\n    return ns", "response": "Get the value of the dynamic variable ns in the current thread."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nresolves the aliased symbol in the current namespace.", "response": "def resolve_alias(s: sym.Symbol, ns: Optional[Namespace] = None) -> sym.Symbol:\n    \"\"\"Resolve the aliased symbol in the current namespace.\"\"\"\n    if s in _SPECIAL_FORMS:\n        return s\n\n    ns = Maybe(ns).or_else(get_current_ns)\n    if s.ns is not None:\n        aliased_ns = ns.get_alias(sym.symbol(s.ns))\n        if aliased_ns is not None:\n            return sym.symbol(s.name, aliased_ns.name)\n        else:\n            return s\n    else:\n        which_var = ns.find(sym.symbol(s.name))\n        if which_var is not None:\n            return sym.symbol(which_var.name.name, which_var.ns.name)\n        else:\n            return sym.symbol(s.name, ns=ns.name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nresolve the aliased symbol to a Var from the specified namespace or the current namespace.", "response": "def resolve_var(s: sym.Symbol, ns: Optional[Namespace] = None) -> Optional[Var]:\n    \"\"\"Resolve the aliased symbol to a Var from the specified\n    namespace, or the current namespace if none is specified.\"\"\"\n    return Var.find(resolve_alias(s, ns))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd generated Python code to a dynamic variable in which_ns.", "response": "def add_generated_python(\n    generated_python: str,\n    var_name: str = _GENERATED_PYTHON_VAR_NAME,\n    which_ns: Optional[str] = None,\n) -> None:\n    \"\"\"Add generated Python code to a dynamic variable in which_ns.\"\"\"\n    if which_ns is None:\n        which_ns = get_current_ns().name\n    ns_sym = sym.Symbol(var_name, ns=which_ns)\n    v = Maybe(Var.find(ns_sym)).or_else(\n        lambda: Var.intern(\n            sym.symbol(which_ns),  # type: ignore\n            sym.symbol(var_name),\n            \"\",\n            dynamic=True,\n            meta=lmap.map({_PRIVATE_META_KEY: True}),\n        )\n    )\n    v.value = v.value + generated_python"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef print_generated_python(\n    var_name: str = _PRINT_GENERATED_PY_VAR_NAME, core_ns_name: str = CORE_NS\n) -> bool:\n    \"\"\"Return the value of the `*print-generated-python*` dynamic variable.\"\"\"\n    ns_sym = sym.Symbol(var_name, ns=core_ns_name)\n    return (\n        Maybe(Var.find(ns_sym))\n        .map(lambda v: v.value)\n        .or_else_raise(lambda: RuntimeException(f\"Dynamic Var {ns_sym} not bound!\"))\n    )", "response": "Return the value of the * print - generated - python * dynamic variable."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bootstrap(ns_var_name: str = NS_VAR_NAME, core_ns_name: str = CORE_NS) -> None:\n    core_ns_sym = sym.symbol(core_ns_name)\n    ns_var_sym = sym.symbol(ns_var_name, ns=core_ns_name)\n    __NS = Maybe(Var.find(ns_var_sym)).or_else_raise(\n        lambda: RuntimeException(f\"Dynamic Var {ns_var_sym} not bound!\")\n    )\n\n    def in_ns(s: sym.Symbol):\n        ns = Namespace.get_or_create(s)\n        __NS.value = ns\n        return ns\n\n    Var.intern_unbound(core_ns_sym, sym.symbol(\"unquote\"))\n    Var.intern_unbound(core_ns_sym, sym.symbol(\"unquote-splicing\"))\n    Var.intern(\n        core_ns_sym, sym.symbol(\"in-ns\"), in_ns, meta=lmap.map({_REDEF_META_KEY: True})\n    )\n    Var.intern(\n        core_ns_sym,\n        sym.symbol(_PRINT_GENERATED_PY_VAR_NAME),\n        False,\n        dynamic=True,\n        meta=lmap.map({_PRIVATE_META_KEY: True}),\n    )\n    Var.intern(\n        core_ns_sym,\n        sym.symbol(_GENERATED_PYTHON_VAR_NAME),\n        \"\",\n        dynamic=True,\n        meta=lmap.map({_PRIVATE_META_KEY: True}),\n    )\n\n    # Dynamic Vars for controlling printing\n    Var.intern(\n        core_ns_sym, sym.symbol(_PRINT_DUP_VAR_NAME), lobj.PRINT_DUP, dynamic=True\n    )\n    Var.intern(\n        core_ns_sym, sym.symbol(_PRINT_LENGTH_VAR_NAME), lobj.PRINT_LENGTH, dynamic=True\n    )\n    Var.intern(\n        core_ns_sym, sym.symbol(_PRINT_LEVEL_VAR_NAME), lobj.PRINT_LEVEL, dynamic=True\n    )\n    Var.intern(\n        core_ns_sym, sym.symbol(_PRINT_META_VAR_NAME), lobj.PRINT_META, dynamic=True\n    )\n    Var.intern(\n        core_ns_sym,\n        sym.symbol(_PRINT_READABLY_VAR_NAME),\n        lobj.PRINT_READABLY,\n        dynamic=True,\n    )", "response": "Bootstrap the environment with functions that are difficult to maintain the minimal lisp environment."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninterns the value bound to the symbol name in namespace ns.", "response": "def intern(\n        ns: sym.Symbol, name: sym.Symbol, val, dynamic: bool = False, meta=None\n    ) -> \"Var\":\n        \"\"\"Intern the value bound to the symbol `name` in namespace `ns`.\"\"\"\n        var_ns = Namespace.get_or_create(ns)\n        var = var_ns.intern(name, Var(var_ns, name, dynamic=dynamic, meta=meta))\n        var.root = val\n        return var"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef intern_unbound(\n        ns: sym.Symbol, name: sym.Symbol, dynamic: bool = False, meta=None\n    ) -> \"Var\":\n        \"\"\"Create a new unbound `Var` instance to the symbol `name` in namespace `ns`.\"\"\"\n        var_ns = Namespace.get_or_create(ns)\n        return var_ns.intern(name, Var(var_ns, name, dynamic=dynamic, meta=meta))", "response": "Create a new unbound Var instance to the symbol name in namespace ns."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_in_ns(ns_sym: sym.Symbol, name_sym: sym.Symbol) -> \"Optional[Var]\":\n        ns = Namespace.get(ns_sym)\n        if ns:\n            return ns.find(name_sym)\n        return None", "response": "Return the value current bound to the name name_sym in the namespace\n        specified by ns_sym."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the value currently bound to the name in the namespace specified by ns_qualified_sym.", "response": "def find(ns_qualified_sym: sym.Symbol) -> \"Optional[Var]\":\n        \"\"\"Return the value currently bound to the name in the namespace specified\n        by `ns_qualified_sym`.\"\"\"\n        ns = Maybe(ns_qualified_sym.ns).or_else_raise(\n            lambda: ValueError(\n                f\"Namespace must be specified in Symbol {ns_qualified_sym}\"\n            )\n        )\n        ns_sym = sym.symbol(ns)\n        name_sym = sym.symbol(ns_qualified_sym.name)\n        return Var.find_in_ns(ns_sym, name_sym)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind the Var currently bound to the specified symbol.", "response": "def find_safe(ns_qualified_sym: sym.Symbol) -> \"Var\":\n        \"\"\"Return the Var currently bound to the name in the namespace specified\n        by `ns_qualified_sym`. If no Var is bound to that name, raise an exception.\n\n        This is a utility method to return useful debugging information when code\n        refers to an invalid symbol at runtime.\"\"\"\n        v = Var.find(ns_qualified_sym)\n        if v is None:\n            raise RuntimeException(\n                f\"Unable to resolve symbol {ns_qualified_sym} in this context\"\n            )\n        return v"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_default_import(cls, module: str):\n        if module in cls.GATED_IMPORTS:\n            cls.DEFAULT_IMPORTS.swap(lambda s: s.cons(sym.symbol(module)))", "response": "Add a gated default import to the default imports."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_alias(self, alias: sym.Symbol, namespace: \"Namespace\") -> None:\n        self._aliases.swap(lambda m: m.assoc(alias, namespace))", "response": "Add an alias for the given Namespace."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef intern(self, sym: sym.Symbol, var: Var, force: bool = False) -> Var:\n        m: lmap.Map = self._interns.swap(Namespace._intern, sym, var, force=force)\n        return m.entry(sym)", "response": "Intern the Var given in this namespace."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nswapping function used by intern to atomically intern a new variable in the symbol mapping for this Namespace.", "response": "def _intern(\n        m: lmap.Map, sym: sym.Symbol, new_var: Var, force: bool = False\n    ) -> lmap.Map:\n        \"\"\"Swap function used by intern to atomically intern a new variable in\n        the symbol mapping for this Namespace.\"\"\"\n        var = m.entry(sym, None)\n        if var is None or force:\n            return m.assoc(sym, new_var)\n        return m"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding the Vars mapped by the given Symbol input or None if no Vars are mapped by that Symbol.", "response": "def find(self, sym: sym.Symbol) -> Optional[Var]:\n        \"\"\"Find Vars mapped by the given Symbol input or None if no Vars are\n        mapped by that Symbol.\"\"\"\n        v = self.interns.entry(sym, None)\n        if v is None:\n            return self.refers.entry(sym, None)\n        return v"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds the Symbol as an imported Symbol in this Namespace.", "response": "def add_import(\n        self, sym: sym.Symbol, module: types.ModuleType, *aliases: sym.Symbol\n    ) -> None:\n        \"\"\"Add the Symbol as an imported Symbol in this Namespace. If aliases are given,\n        the aliases will be applied to the \"\"\"\n        self._imports.swap(lambda m: m.assoc(sym, module))\n        if aliases:\n            self._import_aliases.swap(\n                lambda m: m.assoc(\n                    *itertools.chain.from_iterable([(alias, sym) for alias in aliases])\n                )\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the module that has been imported into this Namespace or None if no module has been imported into this Namespace.", "response": "def get_import(self, sym: sym.Symbol) -> Optional[types.ModuleType]:\n        \"\"\"Return the module if a moduled named by sym has been imported into\n        this Namespace, None otherwise.\n\n        First try to resolve a module directly with the given name. If no module\n        can be resolved, attempt to resolve the module using import aliases.\"\"\"\n        mod = self.imports.entry(sym, None)\n        if mod is None:\n            alias = self.import_aliases.get(sym, None)\n            if alias is None:\n                return None\n            return self.imports.entry(alias, None)\n        return mod"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_refer(self, sym: sym.Symbol, var: Var) -> None:\n        if not var.is_private:\n            self._refers.swap(lambda s: s.assoc(sym, var))", "response": "Add a reference to the given variable."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the Var referred by a Symbol or None if it does not exist.", "response": "def get_refer(self, sym: sym.Symbol) -> Optional[Var]:\n        \"\"\"Get the Var referred by Symbol or None if it does not exist.\"\"\"\n        return self.refers.entry(sym, None)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrefer all _public_ interns from another namespace.", "response": "def __refer_all(cls, refers: lmap.Map, other_ns_interns: lmap.Map) -> lmap.Map:\n        \"\"\"Refer all _public_ interns from another namespace.\"\"\"\n        final_refers = refers\n        for entry in other_ns_interns:\n            s: sym.Symbol = entry.key\n            var: Var = entry.value\n            if not var.is_private:\n                final_refers = final_refers.assoc(s, var)\n        return final_refers"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrefers all the Vars in the other namespace.", "response": "def refer_all(self, other_ns: \"Namespace\"):\n        \"\"\"Refer all the Vars in the other namespace.\"\"\"\n        self._refers.swap(Namespace.__refer_all, other_ns.interns)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __get_or_create(\n        ns_cache: NamespaceMap,\n        name: sym.Symbol,\n        module: types.ModuleType = None,\n        core_ns_name=CORE_NS,\n    ) -> lmap.Map:\n        \"\"\"Private swap function used by `get_or_create` to atomically swap\n        the new namespace map into the global cache.\"\"\"\n        ns = ns_cache.entry(name, None)\n        if ns is not None:\n            return ns_cache\n        new_ns = Namespace(name, module=module)\n        if name.name != core_ns_name:\n            core_ns = ns_cache.entry(sym.symbol(core_ns_name), None)\n            assert core_ns is not None, \"Core namespace not loaded yet!\"\n            new_ns.refer_all(core_ns)\n        return ns_cache.assoc(name, new_ns)", "response": "Private function used by get_or_create to atomically swap\n        the new namespace map into the global cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_or_create(\n        cls, name: sym.Symbol, module: types.ModuleType = None\n    ) -> \"Namespace\":\n        \"\"\"Get the namespace bound to the symbol `name` in the global namespace\n        cache, creating it if it does not exist.\n        Return the namespace.\"\"\"\n        return cls._NAMESPACES.swap(Namespace.__get_or_create, name, module=module)[\n            name\n        ]", "response": "Get the namespace bound to the symbol name in the global namespace\n        cache creating it if it does not exist."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the namespace bound to the symbol name.", "response": "def get(cls, name: sym.Symbol) -> \"Optional[Namespace]\":\n        \"\"\"Get the namespace bound to the symbol `name` in the global namespace\n        cache. Return the namespace if it exists or None otherwise..\"\"\"\n        return cls._NAMESPACES.deref().entry(name, None)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove(cls, name: sym.Symbol) -> Optional[\"Namespace\"]:\n        while True:\n            oldval: lmap.Map = cls._NAMESPACES.deref()\n            ns: Optional[Namespace] = oldval.entry(name, None)\n            newval = oldval\n            if ns is not None:\n                newval = oldval.dissoc(name)\n            if cls._NAMESPACES.compare_and_set(oldval, newval):\n                return ns", "response": "Remove the namespace bound to the symbol name in the global cache and return that namespace. Return None if the symbol does not exist in the global cache."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __completion_matcher(text: str) -> CompletionMatcher:\n\n        def is_match(entry: Tuple[sym.Symbol, Any]) -> bool:\n            return entry[0].name.startswith(text)\n\n        return is_match", "response": "Return a function which matches any symbol keys from map entries\n        against the given text."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns an iterable of possible completions matching the given prefix from the list of aliased namespaces.", "response": "def __complete_alias(\n        self, prefix: str, name_in_ns: Optional[str] = None\n    ) -> Iterable[str]:\n        \"\"\"Return an iterable of possible completions matching the given\n        prefix from the list of aliased namespaces. If name_in_ns is given,\n        further attempt to refine the list to matching names in that namespace.\"\"\"\n        candidates = filter(\n            Namespace.__completion_matcher(prefix), [(s, n) for s, n in self.aliases]\n        )\n        if name_in_ns is not None:\n            for _, candidate_ns in candidates:\n                for match in candidate_ns.__complete_interns(\n                    name_in_ns, include_private_vars=False\n                ):\n                    yield f\"{prefix}/{match}\"\n        else:\n            for alias, _ in candidates:\n                yield f\"{alias}/\""}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an iterable of possible completions matching the given prefix from the list of imports and aliased imports.", "response": "def __complete_imports_and_aliases(\n        self, prefix: str, name_in_module: Optional[str] = None\n    ) -> Iterable[str]:\n        \"\"\"Return an iterable of possible completions matching the given\n        prefix from the list of imports and aliased imports. If name_in_module\n        is given, further attempt to refine the list to matching names in that\n        namespace.\"\"\"\n        imports = self.imports\n        aliases = lmap.map(\n            {\n                alias: imports.entry(import_name)\n                for alias, import_name in self.import_aliases\n            }\n        )\n\n        candidates = filter(\n            Namespace.__completion_matcher(prefix), itertools.chain(aliases, imports)\n        )\n        if name_in_module is not None:\n            for _, module in candidates:\n                for name in module.__dict__:\n                    if name.startswith(name_in_module):\n                        yield f\"{prefix}/{name}\"\n        else:\n            for candidate_name, _ in candidates:\n                yield f\"{candidate_name}/\""}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __complete_interns(\n        self, value: str, include_private_vars: bool = True\n    ) -> Iterable[str]:\n        \"\"\"Return an iterable of possible completions matching the given\n        prefix from the list of interned Vars.\"\"\"\n        if include_private_vars:\n            is_match = Namespace.__completion_matcher(value)\n        else:\n            _is_match = Namespace.__completion_matcher(value)\n\n            def is_match(entry: Tuple[sym.Symbol, Var]) -> bool:\n                return _is_match(entry) and not entry[1].is_private\n\n        return map(\n            lambda entry: f\"{entry[0].name}\",\n            filter(is_match, [(s, v) for s, v in self.interns]),\n        )", "response": "Return an iterable of possible completions matching the given\n        prefix from the list of interned Vars."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __complete_refers(self, value: str) -> Iterable[str]:\n        return map(\n            lambda entry: f\"{entry[0].name}\",\n            filter(\n                Namespace.__completion_matcher(value), [(s, v) for s, v in self.refers]\n            ),\n        )", "response": "Return an iterable of possible completions matching the given\n        prefix from the list of referred Vars."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn an iterable of possible completions for the given text in this namespace.", "response": "def complete(self, text: str) -> Iterable[str]:\n        \"\"\"Return an iterable of possible completions for the given text in\n        this namespace.\"\"\"\n        assert not text.startswith(\":\")\n\n        if \"/\" in text:\n            prefix, suffix = text.split(\"/\", maxsplit=1)\n            results = itertools.chain(\n                self.__complete_alias(prefix, name_in_ns=suffix),\n                self.__complete_imports_and_aliases(prefix, name_in_module=suffix),\n            )\n        else:\n            results = itertools.chain(\n                self.__complete_alias(text),\n                self.__complete_imports_and_aliases(text),\n                self.__complete_interns(text),\n                self.__complete_refers(text),\n            )\n\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the arguments for a trampolined function.", "response": "def args(self) -> Tuple:\n        \"\"\"Return the arguments for a trampolined function. If the function\n        that is being trampolined has varargs, unroll the final argument if\n        it is a sequence.\"\"\"\n        if not self._has_varargs:\n            return self._args\n\n        try:\n            final = self._args[-1]\n            if isinstance(final, ISeq):\n                inits = self._args[:-1]\n                return tuple(itertools.chain(inits, final))\n            return self._args\n        except IndexError:\n            return ()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new list with the given members.", "response": "def list(members, meta=None) -> List:  # pylint:disable=redefined-builtin\n    \"\"\"Creates a new list.\"\"\"\n    return List(  # pylint: disable=abstract-class-instantiated\n        plist(iterable=members), meta=meta\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new list from members.", "response": "def l(*members, meta=None) -> List:\n    \"\"\"Creates a new list from members.\"\"\"\n    return List(  # pylint: disable=abstract-class-instantiated\n        plist(iterable=members), meta=meta\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a function that changes the style of the key value", "response": "def change_style(style, representer):\n    \"\"\"\n    This function is used to format the key value as a multi-line string maintaining the line breaks\n    \"\"\"\n\n    def new_representer(dumper, data):\n        scalar = representer(dumper, data)\n        scalar.style = style\n        return scalar\n\n    return new_representer"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_public_key(platform, service, purpose, key_use, version, public_key, keys_folder):\n    '''\n    Loads a public key from the file system and adds it to a dict of keys\n    :param keys: A dict of keys\n    :param platform the platform the key is for\n    :param service the service the key is for\n    :param key_use what the key is used for\n    :param version the version of the key\n    :param purpose: The purpose of the public key\n    :param public_key: The name of the public key to add\n    :param keys_folder: The location on disk where the key exists\n    :param kid_override: This allows the caller to override the generated KID value\n    :return: None\n    '''\n    public_key_data = get_file_contents(keys_folder, public_key)\n\n    pub_key = load_pem_public_key(public_key_data.encode(), backend=backend)\n    pub_bytes = pub_key.public_bytes(Encoding.PEM, PublicFormat.SubjectPublicKeyInfo)\n\n    kid = _generate_kid_from_key(pub_bytes.decode())\n\n    key = _create_key(platform=platform, service=service, key_use=key_use,\n                      key_type=\"public\", purpose=purpose, version=version,\n                      public_key=public_key_data)\n    return kid, key", "response": "Loads a public key from the file system and adds it to a dict of keys\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_private_key(platform, service, purpose, key_use, version, private_key, keys_folder):\n    '''\n    Loads a private key from the file system and adds it to a dict of keys\n    :param keys: A dict of keys\n    :param platform the platform the key is for\n    :param service the service the key is for\n    :param key_use what the key is used for\n    :param version the version of the key\n    :param purpose: The purpose of the private key\n    :param private_key: The name of the private key to add\n    :param keys_folder: The location on disk where the key exists\n    :param kid_override: This allows the caller to override the generated KID value\n    :return: None\n    '''\n    private_key_data = get_file_contents(keys_folder, private_key)\n\n    private_key = load_pem_private_key(private_key_data.encode(), None, backend=backend)\n\n    pub_key = private_key.public_key()\n\n    pub_bytes = pub_key.public_bytes(Encoding.PEM, PublicFormat.SubjectPublicKeyInfo)\n\n    kid = _generate_kid_from_key(pub_bytes.decode())\n\n    key = _create_key(platform=platform, service=service, key_use=key_use,\n                      key_type=\"private\", purpose=purpose, version=version,\n                      public_key=pub_bytes.decode(), private_key=private_key_data)\n    return kid, key", "response": "Loads a private key from the file system and adds it to a dict of keys\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_key_for_purpose_and_type(self, purpose, key_type):\n        key = [key for key in self.keys.values() if key.purpose == purpose and key.key_type == key_type]\n        try:\n            return key[0]\n        except IndexError:\n            return None", "response": "Gets a key object that matches the purpose and key_type"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_default_args(func):\n    args, _, _, defaults, *rest = inspect.getfullargspec(func)\n    return dict(zip(reversed(args), reversed(defaults)))", "response": "returns a dictionary of arg_name = > default_values for the input function\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmapping the kwargs to objects in the order they were requested.", "response": "def map_arguments_to_objects(kwargs, objects, object_key, object_tuple_key, argument_key, result_value, default_result):\n    \"\"\"\n    :param kwargs: kwargs used to call the multiget function\n    :param objects: objects returned from the inner function\n    :param object_key: field or set of fields that map to the kwargs provided\n    :param object_tuple_key: A temporary shortcut until we allow dot.path traversal for object_key.\n    Will call getattr(getattr(result, join_table_name), object_key)\n    :param argument_key: field or set of fields that map to the objects provided\n    :param result_value: Limit the fields returned to this field or set of fields (none = whole object)\n    :param default_result: If the inner function returned none for a set of parameters, default to this\n    :return:\n    \"\"\"\n\n    # Map each object to the set of desired result data using a key\n    # that corresponds to the parameters for ordering purposes\n    map_ = map_objects_to_result(objects, object_key, object_tuple_key, result_value, default_result)\n    element_count = get_request_count(kwargs)\n    # Using the map we just made, return the objects in the same order\n    # they were primed using the object and argument keys to match against\n    return [map_[get_argument_key(kwargs, argument_key, index)] for index in range(0, element_count)]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete(self, *args):\n        cache = get_cache()\n        key = self.get_cache_key(*args)\n        if key in cache:\n            del cache[key]", "response": "Remove the key from the request cache and from memcache."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the current version and module in - dot - notation which is used by target parameters.", "response": "def get_dot_target_name(version=None, module=None):\n  \"\"\"Returns the current version/module in -dot- notation which is used by `target:` parameters.\"\"\"\n  version = version or get_current_version_name()\n  module = module or get_current_module_name()\n  return '-dot-'.join((version, module))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_dot_target_name_safe(version=None, module=None):\n  version = version or get_current_version_name_safe()\n  module = module or get_current_module_name_safe()\n  if version and module:\n    return '-dot-'.join((version, module))\n  return None", "response": "Returns the current version and module in - dot - notation which is used by target parameters."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a dictionary of key - value pairs from os. environ.", "response": "def _get_os_environ_dict(keys):\n  \"\"\"Return a dictionary of key/values from os.environ.\"\"\"\n  return {k: os.environ.get(k, _UNDEFINED) for k in keys}"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconnects to LASAF through a CAM - socket.", "response": "async def connect(self):\n        \"\"\"Connect to LASAF through a CAM-socket.\"\"\"\n        self.reader, self.writer = await asyncio.open_connection(\n            self.host, self.port, loop=self.loop)\n        self.welcome_msg = await self.reader.read(self.buffer_size)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def send(self, commands):\n        msg = self._prepare_send(commands)\n        self.writer.write(msg)\n        await self.writer.drain()", "response": "Send commands to LASAF through CAM - socket."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def receive(self):\n        try:\n            incomming = await self.reader.read(self.buffer_size)\n        except OSError:\n            return []\n\n        return _parse_receive(incomming)", "response": "Receive a message from the socket interface as list of OrderedDict."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwaits until command is received.", "response": "async def wait_for(self, cmd, value=None, timeout=60):\n        \"\"\"Hang until command is received.\n\n        If value is supplied, it will hang until ``cmd:value`` is received.\n\n        Parameters\n        ----------\n        cmd : string\n            Command to wait for in bytestring from microscope CAM interface. If\n            ``value`` is falsey, value of received command does not matter.\n        value : string\n            Wait until ``cmd:value`` is received.\n        timeout : int\n            Minutes to wait for command. If timeout is reached, an empty\n            OrderedDict will be returned.\n\n        Returns\n        -------\n        collections.OrderedDict\n            Last received messsage or empty message if timeout is reached.\n\n        \"\"\"\n        try:\n            async with async_timeout(timeout * 60):\n                while True:\n                    msgs = await self.receive()\n                    msg = check_messages(msgs, cmd, value=value)\n                    if msg:\n                        return msg\n        except asyncio.TimeoutError:\n            return OrderedDict()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef close(self):\n        if self.writer.can_write_eof():\n            self.writer.write_eof()\n        self.writer.close()", "response": "Close the underlying file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lazyload(reference: str, *args, **kw):\n\t\n\tassert check_argument_types()\n\t\n\tdef lazily_load_reference(self):\n\t\tref = reference\n\t\t\n\t\tif ref.startswith('.'):\n\t\t\tref = traverse(self, ref[1:])\n\t\t\n\t\treturn load(ref, *args, **kw)\n\t\n\treturn lazy(lazily_load_reference)", "response": "Lazy load and cache an object reference upon dereferencing."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\niterating through the FIQL string and yield a tuple containing the next set of entries.", "response": "def iter_parse(fiql_str):\n    \"\"\"Iterate through the FIQL string. Yield a tuple containing the\n    following FIQL components for each iteration:\n\n      - preamble: Any operator or opening/closing paranthesis preceding a\n        constraint or at the very end of the FIQL string.\n      - selector: The selector portion of a FIQL constraint or ``None`` if\n        yielding the last portion of the string.\n      - comparison: The comparison portion of a FIQL constraint or ``None``\n        if yielding the last portion of the string.\n      - argument: The argument portion of a FIQL constraint or ``None`` if\n        yielding the last portion of the string.\n\n    For usage see :func:`parse_str_to_expression`.\n\n    Args:\n        fiql_str (string): The FIQL formatted string we want to parse.\n\n    Yields:\n        tuple: Preamble, selector, comparison, argument.\n    \"\"\"\n    while len(fiql_str):\n        constraint_match = CONSTRAINT_COMP.split(fiql_str, 1)\n        if len(constraint_match) < 2:\n            yield (constraint_match[0], None, None, None)\n            break\n        yield (\n            constraint_match[0],\n            unquote_plus(constraint_match[1]),\n            constraint_match[4],\n            unquote_plus(constraint_match[6]) \\\n                    if constraint_match[6] else None\n        )\n        fiql_str = constraint_match[8]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_str_to_expression(fiql_str):\n    #pylint: disable=too-many-branches\n    nesting_lvl = 0\n    last_element = None\n    expression = Expression()\n    for (preamble, selector, comparison, argument) in iter_parse(fiql_str):\n        if preamble:\n            for char in preamble:\n                if char == '(':\n                    if isinstance(last_element, BaseExpression):\n                        raise FiqlFormatException(\n                            \"%s can not be followed by %s\" % (\n                                last_element.__class__, Expression))\n                    expression = expression.create_nested_expression()\n                    nesting_lvl += 1\n                elif char == ')':\n                    expression = expression.get_parent()\n                    last_element = expression\n                    nesting_lvl -= 1\n                else:\n                    if not expression.has_constraint():\n                        raise FiqlFormatException(\n                            \"%s proceeding initial %s\" % (\n                                Operator, Constraint))\n                    if isinstance(last_element, Operator):\n                        raise FiqlFormatException(\n                            \"%s can not be followed by %s\" % (\n                                Operator, Operator))\n                    last_element = Operator(char)\n                    expression = expression.add_operator(last_element)\n        if selector:\n            if isinstance(last_element, BaseExpression):\n                raise FiqlFormatException(\"%s can not be followed by %s\" % (\n                    last_element.__class__, Constraint))\n            last_element = Constraint(selector, comparison, argument)\n            expression.add_element(last_element)\n    if nesting_lvl != 0:\n        raise FiqlFormatException(\n            \"At least one nested expression was not correctly closed\")\n    if not expression.has_constraint():\n        raise FiqlFormatException(\n            \"Parsed string '%s' contained no constraint\" % fiql_str)\n    return expression", "response": "Parses a FIQL formatted string into an Expression object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nencode objects like ndb. Model which have a. to_dict method.", "response": "def encode_model(obj):\n  \"\"\"Encode objects like ndb.Model which have a `.to_dict()` method.\"\"\"\n  obj_dict = obj.to_dict()\n  for key, val in obj_dict.iteritems():\n    if isinstance(val, types.StringType):\n      try:\n        unicode(val)\n      except UnicodeDecodeError:\n        # Encode binary strings (blobs) to base64.\n        obj_dict[key] = base64.b64encode(val)\n  return obj_dict"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dump(ndb_model, fp, **kwargs):\n  for chunk in NdbEncoder(**kwargs).iterencode(ndb_model):\n    fp.write(chunk)", "response": "Custom json dump using the custom encoder above."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef object_hook_handler(self, val):\n    return {k: self.decode_date(v) for k, v in val.iteritems()}", "response": "Handles decoding of nested date strings."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntry to decode a string that looks like dates into datetime objects.", "response": "def decode_date(self, val):\n    \"\"\"Tries to decode strings that look like dates into datetime objects.\"\"\"\n    if isinstance(val, basestring) and val.count('-') == 2 and len(val) > 9:\n      try:\n        dt = dateutil.parser.parse(val)\n        # Check for UTC.\n        if val.endswith(('+00:00', '-00:00', 'Z')):\n          # Then remove tzinfo for gae, which is offset-naive.\n          dt = dt.replace(tzinfo=None)\n        return dt\n      except (TypeError, ValueError):\n        pass\n    return val"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef decode(self, val):\n    # First try the date decoder.\n    new_val = self.decode_date(val)\n    if val != new_val:\n      return new_val\n    # Fall back to the default decoder.\n    return json.JSONDecoder.decode(self, val)", "response": "Override of the default decode method that also uses decode_date."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\noverrides the default JSONEncoder. default for NDB support.", "response": "def default(self, obj):\n    \"\"\"Overriding the default JSONEncoder.default for NDB support.\"\"\"\n    obj_type = type(obj)\n    # NDB Models return a repr to calls from type().\n    if obj_type not in self._ndb_type_encoding:\n      if hasattr(obj, '__metaclass__'):\n        obj_type = obj.__metaclass__\n      else:\n        # Try to encode subclasses of types\n        for ndb_type in NDB_TYPES:\n          if isinstance(obj, ndb_type):\n            obj_type = ndb_type\n            break\n\n    fn = self._ndb_type_encoding.get(obj_type)\n\n    if fn:\n      return fn(obj)\n\n    return json.JSONEncoder.default(self, obj)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntraverse down an object using getattr or getitem.", "response": "def traverse(obj, target:str, default=nodefault, executable:bool=False, separator:str='.', protect:bool=True):\n\t\"\"\"Traverse down an object, using getattr or getitem.\n\t\n\tIf ``executable`` is ``True`` any executable function encountered will be, with no arguments. Traversal will\n\tcontinue on the result of that call.  You can change the separator as desired, i.e. to a '/'.\n\t\n\tBy default attributes (but not array elements) prefixed with an underscore are taboo.  They will not resolve,\n\traising a LookupError.\n\t\n\tCertain allowances are made: if a 'path segment' is numerical, it's treated as an array index. If attribute\n\tlookup fails, it will re-try on that object using array notation and continue from there.  This makes lookup\n\tvery flexible.\n\t\"\"\"\n\t\n\t# TODO: Support numerical slicing, i.e. ``1:4``, or even just ``:-1`` and things.\n\tassert check_argument_types()\n\t\n\tvalue = obj\n\tremainder = target\n\t\n\tif not target:\n\t\treturn obj\n\t\n\twhile separator:\n\t\tname, separator, remainder = remainder.partition(separator)\n\t\tnumeric = name.lstrip('-').isdigit()\n\t\t\n\t\ttry:\n\t\t\tif numeric or (protect and name.startswith('_')):\n\t\t\t\traise AttributeError()\n\t\t\t\n\t\t\tvalue = getattr(value, name)\n\t\t\t\n\t\t\tif executable and callable(value):\n\t\t\t\tvalue = value()\n\t\t\n\t\texcept AttributeError:\n\t\t\ttry:\n\t\t\t\tvalue = value[int(name) if numeric else name]\n\t\t\t\n\t\t\texcept (KeyError, TypeError):\n\t\t\t\tif default is nodefault:\n\t\t\t\t\traise LookupError(\"Could not resolve '\" + target + \"' on: \" + repr(obj))\n\t\t\t\t\n\t\t\t\treturn default\n\t\t\n\treturn value"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load(target:str, namespace:str=None, default=nodefault, executable:bool=False, separators:Sequence[str]=('.', ':'),\n\t\tprotect:bool=True):\n\t\"\"\"This helper function loads an object identified by a dotted-notation string.\n\t\n\tFor example::\n\t\n\t\t# Load class Foo from example.objects\n\t\tload('example.objects:Foo')\n\t\t\n\t\t# Load the result of the class method ``new`` of the Foo object\n\t\tload('example.objects:Foo.new', executable=True)\n\t\n\tIf a plugin namespace is provided simple name references are allowed.  For example::\n\t\n\t\t# Load the plugin named 'routing' from the 'web.dispatch' namespace\n\t\tload('routing', 'web.dispatch')\n\t\n\tThe ``executable``, ``protect``, and first tuple element of ``separators`` are passed to the traverse function.\n\tProviding a namespace does not prevent full object lookup (dot-colon notation) from working.\n\t\"\"\"\n\t\n\tassert check_argument_types()\n\t\n\tif namespace and ':' not in target:\n\t\tallowable = dict((i.name,  i) for i in iter_entry_points(namespace))\n\t\t\n\t\tif target not in allowable:\n\t\t\traise LookupError('Unknown plugin \"' + target + '\"; found: ' + ', '.join(allowable))\n\t\t\n\t\treturn allowable[target].load()\n\t\n\tparts, _, target = target.partition(separators[1])\n\t\n\ttry:\n\t\tobj = __import__(parts)\n\texcept ImportError:\n\t\tif default is not nodefault:\n\t\t\treturn default\n\t\t\n\t\traise\n\t\n\treturn traverse(\n\t\t\tobj,\n\t\t\tseparators[0].join(parts.split(separators[0])[1:] + target.split(separators[0])),\n\t\t\tdefault = default,\n\t\t\texecutable = executable,\n\t\t\tprotect = protect\n\t\t) if target else obj", "response": "This helper function loads an object identified by a dotted - notation string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef validate_version():\n    import leicacam\n    version_string = leicacam.__version__\n    versions = version_string.split('.', 3)\n    try:\n        for ver in versions:\n            int(ver)\n    except ValueError:\n        print(\n            'Only integers are allowed in release version, '\n            'please adjust current version {}'.format(version_string))\n        return None\n    return version_string", "response": "Validate version before release."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate a new empty version of the current project.", "response": "def generate():\n    \"\"\"Generate changelog.\"\"\"\n    old_dir = os.getcwd()\n    proj_dir = os.path.join(os.path.dirname(__file__), os.pardir)\n    os.chdir(proj_dir)\n    version = validate_version()\n    if not version:\n        os.chdir(old_dir)\n        return\n    print('Generating changelog for version {}'.format(version))\n    options = [\n        '--user', 'arve0', '--project', 'leicacam', '-v', '--with-unreleased',\n        '--future-release', version]\n    generator = ChangelogGenerator(options)\n    generator.run()\n    os.chdir(old_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds strongly connected components in a graph using Tarjan s algorithm.", "response": "def strongly_connected_components(graph: Graph) -> List:\n\t\"\"\"Find the strongly connected components in a graph using Tarjan's algorithm.\n\t\n\tThe `graph` argument should be a dictionary mapping node names to sequences of successor nodes.\n\t\"\"\"\n\t\n\tassert check_argument_types()\n\t\n\tresult = []\n\tstack = []\n\tlow = {}\n\t\n\tdef visit(node: str):\n\t\tif node in low: return\n\t\t\n\t\tnum = len(low)\n\t\tlow[node] = num\n\t\tstack_pos = len(stack)\n\t\tstack.append(node)\n\t\t\n\t\tfor successor in graph[node]:\n\t\t\tvisit(successor)\n\t\t\tlow[node] = min(low[node], low[successor])\n\t\t\n\t\tif num == low[node]:\n\t\t\tcomponent = tuple(stack[stack_pos:])\n\t\t\tdel stack[stack_pos:]\n\t\t\t\n\t\t\tresult.append(component)\n\t\t\t\n\t\t\tfor item in component:\n\t\t\t\tlow[item] = len(graph)\n\t\n\tfor node in graph:\n\t\tvisit(node)\n\t\n\treturn result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef robust_topological_sort(graph: Graph) -> list:\n\t\n\tassert check_argument_types()\n\t\n\tcomponents = strongly_connected_components(graph)\n\t\n\tnode_component = {}\n\tfor component in components:\n\t\tfor node in component:\n\t\t\tnode_component[node] = component\n\t\n\tcomponent_graph = {}\n\tfor component in components:\n\t\tcomponent_graph[component] = []\n\t\n\tfor node in graph:\n\t\tnode_c = node_component[node]\n\t\tfor successor in graph[node]:\n\t\t\tsuccessor_c = node_component[successor]\n\t\t\tif node_c != successor_c:\n\t\t\t\tcomponent_graph[node_c].append(successor_c) \n\t\n\treturn topological_sort(component_graph)", "response": "Perform a robust topological sort of the components in a graph."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the parent expression for this object.", "response": "def set_parent(self, parent):\n        \"\"\"Set parent ``Expression`` for this object.\n\n        Args:\n            parent (Expression): The ``Expression`` which contains this object.\n\n        Raises:\n            FiqlObjectException: Parent must be of type ``Expression``.\n        \"\"\"\n        if not isinstance(parent, Expression):\n            raise FiqlObjectException(\"Parent must be of %s not %s\" % (\n                Expression, type(parent)))\n        self.parent = parent"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the parent Expression for this object.", "response": "def get_parent(self):\n        \"\"\"Get the parent ``Expression`` for this object.\n\n        Returns:\n            Expression: The ``Expression`` which contains this object.\n\n        Raises:\n            FiqlObjectException: Parent is ``None``.\n        \"\"\"\n        if not isinstance(self.parent, Expression):\n            raise FiqlObjectException(\"Parent must be of %s not %s\" % (\n                Expression, type(self.parent)))\n        return self.parent"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd an operator to the internal list of the current FiqlObject.", "response": "def add_operator(self, operator):\n        \"\"\"Add an ``Operator`` to the ``Expression``.\n\n        The ``Operator`` may result in a new ``Expression`` if an ``Operator``\n        already exists and is of a different precedence.\n\n        There are three possibilities when adding an ``Operator`` to an\n        ``Expression`` depending on whether or not an ``Operator`` already\n        exists:\n\n          - No ``Operator`` on the working ``Expression``; Simply set the\n            ``Operator`` and return ``self``.\n          - ``Operator`` already exists and is higher in precedence; The\n            ``Operator`` and last ``Constraint`` belong in a sub-expression of\n            the working ``Expression``.\n          - ``Operator`` already exists and is lower in precedence; The\n            ``Operator`` belongs to the parent of the working ``Expression``\n            whether one currently exists or not. To remain in the context of\n            the top ``Expression``, this method will return the parent here\n            rather than ``self``.\n\n        Args:\n            operator (Operator): What we are adding.\n\n        Returns:\n            Expression: ``self`` or related ``Expression``.\n\n        Raises:\n            FiqlObjectExpression: Operator is not a valid ``Operator``.\n        \"\"\"\n        if not isinstance(operator, Operator):\n            raise FiqlObjectException(\"%s is not a valid element type\" % (\n                operator.__class__))\n\n        if not self._working_fragment.operator:\n            self._working_fragment.operator = operator\n        elif operator > self._working_fragment.operator:\n            last_constraint = self._working_fragment.elements.pop()\n            self._working_fragment = self._working_fragment \\\n                    .create_nested_expression()\n            self._working_fragment.add_element(last_constraint)\n            self._working_fragment.add_operator(operator)\n        elif operator < self._working_fragment.operator:\n            if self._working_fragment.parent:\n                return self._working_fragment.parent.add_operator(operator)\n            else:\n                return Expression().add_element(self._working_fragment) \\\n                        .add_operator(operator)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding an element of type Operator Constraint or Expression to the Expression.", "response": "def add_element(self, element):\n        \"\"\"Add an element of type ``Operator``, ``Constraint``, or\n        ``Expression`` to the ``Expression``.\n\n        Args:\n            element: ``Constraint``, ``Expression``, or ``Operator``.\n\n        Returns:\n            Expression: ``self``\n\n        Raises:\n            FiqlObjectException: Element is not a valid type.\n        \"\"\"\n        if isinstance(element, BaseExpression):\n            element.set_parent(self._working_fragment)\n            self._working_fragment.elements.append(element)\n            return self\n        else:\n            return self.add_operator(element)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating the Expression by joining the specified additional Locales with an AND operator.", "response": "def op_and(self, *elements):\n        \"\"\"Update the ``Expression`` by joining the specified additional\n        ``elements`` using an \"AND\" ``Operator``\n\n        Args:\n            *elements (BaseExpression): The ``Expression`` and/or\n                ``Constraint`` elements which the \"AND\" ``Operator`` applies\n                to.\n\n        Returns:\n            Expression: ``self`` or related ``Expression``.\n        \"\"\"\n        expression = self.add_operator(Operator(';'))\n        for element in elements:\n            expression.add_element(element)\n        return expression"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef op_or(self, *elements):\n        expression = self.add_operator(Operator(','))\n        for element in elements:\n            expression.add_element(element)\n        return expression", "response": "Update the expression by joining the specified additional\nLocales with an OR operator."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndecorate passed in function and log message to module logger.", "response": "def logger(function):\n    \"\"\"Decorate passed in function and log message to module logger.\"\"\"\n    @functools.wraps(function)\n    def wrapper(*args, **kwargs):\n        \"\"\"Wrap function.\"\"\"\n        sep = kwargs.get('sep', ' ')\n        end = kwargs.get('end', '')  # do not add newline by default\n        out = sep.join([repr(x) for x in args])\n        out = out + end\n        _LOGGER.debug(out)\n        return function(*args, **kwargs)\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the received response from the socket server into a list of OrderedDicts.", "response": "def _parse_receive(incomming):\n    \"\"\"Parse received response.\n\n    Parameters\n    ----------\n    incomming : bytes string\n        Incomming bytes from socket server.\n\n    Returns\n    -------\n    list of OrderedDict\n        Received message as a list of OrderedDict.\n\n    \"\"\"\n    debug(b'< ' + incomming)\n    # remove terminating null byte\n    incomming = incomming.rstrip(b'\\x00')\n    # split received messages\n    # return as list of several messages received\n    msgs = incomming.splitlines()\n    return [bytes_as_dict(msg) for msg in msgs]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nformatting list of tuples to bytes", "response": "def tuples_as_bytes(cmds):\n    \"\"\"Format list of tuples to CAM message with format /key:val.\n\n    Parameters\n    ----------\n    cmds : list of tuples\n        List of commands as tuples.\n\n    Returns\n    -------\n    bytes\n        Sequence of /key:val.\n\n    Example\n    -------\n    ::\n\n        >>> tuples_as_bytes([('cmd', 'val'), ('cmd2', 'val2')])\n        b'/cmd:val /cmd2:val2'\n\n    \"\"\"\n    cmds = OrderedDict(cmds)  # override equal keys\n    tmp = []\n    for key, val in cmds.items():\n        key = str(key)\n        val = str(val)\n        tmp.append('/' + key + ':' + val)\n    return ' '.join(tmp).encode()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntranslates a list of tuples to OrderedDict with key and val as strings.", "response": "def tuples_as_dict(_list):\n    \"\"\"Translate a list of tuples to OrderedDict with key and val as strings.\n\n    Parameters\n    ----------\n    _list : list of tuples\n\n    Returns\n    -------\n    collections.OrderedDict\n\n    Example\n    -------\n    ::\n\n        >>> tuples_as_dict([('cmd', 'val'), ('cmd2', 'val2')])\n        OrderedDict([('cmd', 'val'), ('cmd2', 'val2')])\n\n    \"\"\"\n    _dict = OrderedDict()\n    for key, val in _list:\n        key = str(key)\n        val = str(val)\n        _dict[key] = val\n    return _dict"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing bytes into OrderedDict based on format / key : val.", "response": "def bytes_as_dict(msg):\n    \"\"\"Parse CAM message to OrderedDict based on format /key:val.\n\n    Parameters\n    ----------\n    msg : bytes\n        Sequence of /key:val.\n\n    Returns\n    -------\n    collections.OrderedDict\n        With /key:val => dict[key] = val.\n\n    \"\"\"\n    # decode bytes, assume '/' in start\n    cmd_strings = msg.decode()[1:].split(r' /')\n    cmds = OrderedDict()\n    for cmd in cmd_strings:\n        unpacked = cmd.split(':')\n        # handle string not well formated (ex filenames with c:\\)\n        if len(unpacked) > 2:\n            key = unpacked[0]\n            val = ':'.join(unpacked[1:])\n        elif len(unpacked) < 2:\n            continue\n        else:\n            key, val = unpacked\n        cmds[key] = val\n    return cmds"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_messages(msgs, cmd, value=None):\n    for msg in msgs:\n        if value and msg.get(cmd) == value:\n            return msg\n        if not value and msg.get(cmd):\n            return msg\n    return None", "response": "Checks if a specific message is present in the microscope CAM interface."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprepare message to be sent.", "response": "def _prepare_send(self, commands):\n        \"\"\"Prepare message to be sent.\n\n        Parameters\n        ----------\n        commands : list of tuples or bytes string\n            Commands as a list of tuples or a bytes string. cam.prefix is\n            allways prepended before sending.\n\n        Returns\n        -------\n        string\n            Message to be sent.\n\n        \"\"\"\n        if isinstance(commands, bytes):\n            msg = self.prefix_bytes + commands\n        else:\n            msg = tuples_as_bytes(self.prefix + commands)\n        debug(b'> ' + msg)\n        return msg"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef connect(self):\n        self.socket = socket.socket()\n        self.socket.connect((self.host, self.port))\n        self.socket.settimeout(False)  # non-blocking\n        sleep(self.delay)  # wait for response\n        self.welcome_msg = self.socket.recv(\n            self.buffer_size)", "response": "Connect to LASAF through a CAM - socket."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef flush(self):\n        debug('flushing incomming socket messages')\n        try:\n            while True:\n                msg = self.socket.recv(self.buffer_size)\n                debug(b'< ' + msg)\n        except socket.error:\n            pass", "response": "Flush incomming socket messages."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send(self, commands):\n        self.flush()  # discard any waiting messages\n        msg = self._prepare_send(commands)\n        return self.socket.send(msg)", "response": "Send commands to LASAF through CAM - socket."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreceive a message from the socket interface as list of OrderedDict.", "response": "def receive(self):\n        \"\"\"Receive message from socket interface as list of OrderedDict.\"\"\"\n        try:\n            incomming = self.socket.recv(self.buffer_size)\n        except socket.error:\n            return []\n\n        return _parse_receive(incomming)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef wait_for(self, cmd, value=None, timeout=60):\n        wait = time() + timeout * 60\n        while True:\n            if time() > wait:\n                return OrderedDict()\n            msgs = self.receive()\n            msg = check_messages(msgs, cmd, value=value)\n            if msg:\n                return msg\n            sleep(self.delay)", "response": "Wait until command is received."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef enable(self, slide=0, wellx=1, welly=1, fieldx=1, fieldy=1):\n        # pylint: disable=too-many-arguments\n        cmd = [\n            ('cmd', 'enable'),\n            ('slide', str(slide)),\n            ('wellx', str(wellx)),\n            ('welly', str(welly)),\n            ('fieldx', str(fieldx)),\n            ('fieldy', str(fieldy)),\n            ('value', 'true')\n        ]\n        self.send(cmd)\n        return self.wait_for(*cmd[0])", "response": "Enable a given scan field."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save_template(self, filename=\"{ScanningTemplate}leicacam.xml\"):\n        cmd = [\n            ('sys', '0'),\n            ('cmd', 'save'),\n            ('fil', str(filename))\n        ]\n        self.send(cmd)\n        return self.wait_for(*cmd[0])", "response": "Save scanning template to file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_template(self, filename=\"{ScanningTemplate}leicacam.xml\"):\n        basename = os.path.basename(filename)\n        if basename[-4:] == '.xml':\n            basename = basename[:-4]\n        if basename[:18] != '{ScanningTemplate}':\n            basename = '{ScanningTemplate}' + basename\n        cmd = [\n            ('sys', '0'),\n            ('cmd', 'load'),\n            ('fil', str(basename))\n        ]\n        self.send(cmd)\n        return self.wait_for(*cmd[1])", "response": "Load scanning template from file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget information about given keyword. Defaults to stage.", "response": "def get_information(self, about='stage'):\n        \"\"\"Get information about given keyword. Defaults to stage.\"\"\"\n        cmd = [\n            ('cmd', 'getinfo'),\n            ('dev', str(about))\n        ]\n        self.send(cmd)\n        return self.wait_for(*cmd[1])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef incfile(fname, fpointer, lrange=\"1,6-\", sdir=None):\n    # Read file\n    file_dir = (\n        sdir\n        if sdir\n        else os.environ.get(\"TRACER_DIR\", os.path.abspath(os.path.dirname(__file__)))\n    )\n    fname = os.path.join(file_dir, fname)\n    with open(fname) as fobj:\n        lines = fobj.readlines()\n    # Parse line specification\n    tokens = [item.strip() for item in lrange.split(\",\")]\n    inc_lines = []\n    for token in tokens:\n        if \"-\" in token:\n            subtokens = token.split(\"-\")\n            lmin, lmax = (\n                int(subtokens[0]),\n                int(subtokens[1]) if subtokens[1] else len(lines),\n            )\n            for num in range(lmin, lmax + 1):\n                inc_lines.append(num)\n        else:\n            inc_lines.append(int(token))\n    # Produce output\n    fpointer(\".. code-block:: python\\n\")\n    fpointer(\"\\n\")\n    for num, line in enumerate(lines):\n        if num + 1 in inc_lines:\n            fpointer(\"    \" + line.replace(\"\\t\", \"    \") if line.strip() else \"\\n\")\n    fpointer(\"\\n\")", "response": "r Include a Python source file in a docstring formatted in reStructuredText."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind and return the location of package. json.", "response": "def locate_package_json():\n    \"\"\"\n    Find and return the location of package.json.\n    \"\"\"\n    directory = settings.SYSTEMJS_PACKAGE_JSON_DIR\n    if not directory:\n        raise ImproperlyConfigured(\n            \"Could not locate 'package.json'. Set SYSTEMJS_PACKAGE_JSON_DIR \"\n            \"to the directory that holds 'package.json'.\"\n        )\n    path = os.path.join(directory, 'package.json')\n    if not os.path.isfile(path):\n        raise ImproperlyConfigured(\"'package.json' does not exist, tried looking in %s\" % path)\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nextracts the JSPM configuration from package. json.", "response": "def parse_package_json():\n    \"\"\"\n    Extract the JSPM configuration from package.json.\n    \"\"\"\n    with open(locate_package_json()) as pjson:\n        data = json.loads(pjson.read())\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind where system. js will be put by JSPM.", "response": "def find_systemjs_location():\n    \"\"\"\n    Figure out where `jspm_packages/system.js` will be put by JSPM.\n    \"\"\"\n    location = os.path.abspath(os.path.dirname(locate_package_json()))\n    conf = parse_package_json()\n\n    if 'jspm' in conf:\n        conf = conf['jspm']\n\n    try:\n        conf = conf['directories']\n    except TypeError:\n        raise ImproperlyConfigured(\"`package.json` doesn't appear to be a valid json object. \"\n                                   \"Location: %s\" % location)\n    except KeyError:\n        raise ImproperlyConfigured(\"The `directories` configuarion was not found in package.json. \"\n                                   \"Please check your jspm install and/or configuarion. `package.json` \"\n                                   \"location: %s\" % location)\n\n    # check for explicit location, else fall back to the default as jspm does\n    jspm_packages = conf['packages'] if 'packages' in conf else 'jspm_packages'\n    base = conf['baseURL'] if 'baseURL' in conf else '.'\n    return os.path.join(location, base, jspm_packages, 'system.js')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nhandle YOURLS API errors. requests' raise_for_status doesn't show the user the YOURLS json response, so we parse that here and raise nicer exceptions.", "response": "def _handle_api_error_with_json(http_exc, jsondata, response):\n    \"\"\"Handle YOURLS API errors.\n\n    requests' raise_for_status doesn't show the user the YOURLS json response,\n    so we parse that here and raise nicer exceptions.\n    \"\"\"\n    if 'code' in jsondata and 'message' in jsondata:\n        code = jsondata['code']\n        message = jsondata['message']\n\n        if code == 'error:noloop':\n            raise YOURLSNoLoopError(message, response=response)\n        elif code == 'error:nourl':\n            raise YOURLSNoURLError(message, response=response)\n\n    elif 'message' in jsondata:\n        message = jsondata['message']\n        raise YOURLSHTTPError(message, response=response)\n\n    http_error_message = http_exc.args[0]\n    raise YOURLSHTTPError(http_error_message, response=response)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate YOURLS server response.", "response": "def _validate_yourls_response(response, data):\n    \"\"\"Validate response from YOURLS server.\"\"\"\n    try:\n        response.raise_for_status()\n    except HTTPError as http_exc:\n        # Collect full HTTPError information so we can reraise later if required.\n        http_error_info = sys.exc_info()\n\n        # We will reraise outside of try..except block to prevent exception\n        # chaining showing wrong traceback when we try and parse JSON response.\n        reraise = False\n\n        try:\n            jsondata = response.json()\n        except ValueError:\n            reraise = True\n        else:\n            logger.debug('Received error {response} with JSON {json}',\n                         response=response, json=jsondata)\n            _handle_api_error_with_json(http_exc, jsondata, response)\n\n        if reraise:\n            six.reraise(*http_error_info)\n    else:\n        # We have a valid HTTP response, but we need to check what the API says\n        # about the request.\n        jsondata = response.json()\n\n        logger.debug('Received {response} with JSON {json}', response=response,\n                     json=jsondata)\n\n        if {'status', 'code', 'message'} <= set(jsondata.keys()):\n            status = jsondata['status']\n            code = jsondata['code']\n            message = jsondata['message']\n\n            if status == 'fail':\n                if code == 'error:keyword':\n                    raise YOURLSKeywordExistsError(message, keyword=data['keyword'])\n                elif code == 'error:url':\n                    url = _json_to_shortened_url(jsondata['url'], jsondata['shorturl'])\n                    raise YOURLSURLExistsError(message, url=url)\n                else:\n                    raise YOURLSAPIError(message)\n            else:\n                return jsondata\n        else:\n            # Without status, nothing special needs to be handled.\n            return jsondata"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _homogenize_waves(wave_a, wave_b):\n    indep_vector = _get_indep_vector(wave_a, wave_b)\n    dep_vector_a = _interp_dep_vector(wave_a, indep_vector)\n    dep_vector_b = _interp_dep_vector(wave_b, indep_vector)\n    return (indep_vector, dep_vector_a, dep_vector_b)", "response": "Generate combined independent variable vector."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninterpolating the dependent variable vector.", "response": "def _interp_dep_vector(wave, indep_vector):\n    \"\"\"Create new dependent variable vector.\"\"\"\n    dep_vector_is_int = wave.dep_vector.dtype.name.startswith(\"int\")\n    dep_vector_is_complex = wave.dep_vector.dtype.name.startswith(\"complex\")\n    if (wave.interp, wave.indep_scale) == (\"CONTINUOUS\", \"LOG\"):\n        wave_interp_func = scipy.interpolate.interp1d(\n            np.log10(wave.indep_vector), wave.dep_vector\n        )\n        ret = wave_interp_func(np.log10(indep_vector))\n    elif (wave.interp, wave.indep_scale) == (\"CONTINUOUS\", \"LINEAR\"):\n        dep_vector = (\n            wave.dep_vector.astype(np.float64)\n            if not dep_vector_is_complex\n            else wave.dep_vector\n        )\n        wave_interp_func = scipy.interpolate.interp1d(wave.indep_vector, dep_vector)\n        ret = wave_interp_func(indep_vector)\n    else:  # wave.interp == 'STAIRCASE'\n        wave_interp_func = scipy.interpolate.interp1d(\n            wave.indep_vector, wave.dep_vector, kind=\"zero\"\n        )\n        # Interpolator does not return the right value for the last\n        # data point, it gives the previous \"stair\" value\n        ret = wave_interp_func(indep_vector)\n        eq_comp = np.all(\n            np.isclose(wave.indep_vector[-1], indep_vector[-1], FP_RTOL, FP_ATOL)\n        )\n        if eq_comp:\n            ret[-1] = wave.dep_vector[-1]\n    round_ret = np.round(ret, 0)\n    return (\n        round_ret.astype(\"int\")\n        if (dep_vector_is_int and np.all(np.isclose(round_ret, ret, FP_RTOL, FP_ATOL)))\n        else ret\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating new independent variable vector.", "response": "def _get_indep_vector(wave_a, wave_b):\n    \"\"\"Create new independent variable vector.\"\"\"\n    exobj = pexdoc.exh.addex(RuntimeError, \"Independent variable ranges do not overlap\")\n    min_bound = max(np.min(wave_a.indep_vector), np.min(wave_b.indep_vector))\n    max_bound = min(np.max(wave_a.indep_vector), np.max(wave_b.indep_vector))\n    exobj(bool(min_bound > max_bound))\n    raw_range = np.unique(np.concatenate((wave_a.indep_vector, wave_b.indep_vector)))\n    return raw_range[np.logical_and(min_bound <= raw_range, raw_range <= max_bound)]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nverifying that two waveforms can be combined with various mathematical functions.", "response": "def _verify_compatibility(wave_a, wave_b, check_dep_units=True):\n    \"\"\"Verify that two waveforms can be combined with various mathematical functions.\"\"\"\n    exobj = pexdoc.exh.addex(RuntimeError, \"Waveforms are not compatible\")\n    ctuple = (\n        bool(wave_a.indep_scale != wave_b.indep_scale),\n        bool(wave_a.dep_scale != wave_b.dep_scale),\n        bool(wave_a.indep_units != wave_b.indep_units),\n        (bool(wave_a.dep_units != wave_b.dep_units) if check_dep_units else False),\n        bool(wave_a.interp != wave_b.interp),\n    )\n    exobj(any(ctuple))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading the existing systemjs manifest and remove any entries that no longer exist on the storage.", "response": "def load_systemjs_manifest(self):\n        \"\"\"\n        Load the existing systemjs manifest and remove any entries that no longer\n        exist on the storage.\n        \"\"\"\n        # backup the original name\n        _manifest_name = self.manifest_name\n\n        # load the custom bundle manifest\n        self.manifest_name = self.systemjs_manifest_name\n        bundle_files = self.load_manifest()\n        # reset the manifest name\n        self.manifest_name = _manifest_name\n\n        # check that the files actually exist, if not, remove them from the manifest\n        for file, hashed_file in bundle_files.copy().items():\n            if not self.exists(file) or not self.exists(hashed_file):\n                del bundle_files[file]\n        return bundle_files"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns tracing on a module.", "response": "def run_trace(\n    mname,\n    fname,\n    module_prefix,\n    callable_names,\n    no_print,\n    module_exclude=None,\n    callable_exclude=None,\n    debug=False,\n):\n    \"\"\"Run module tracing.\"\"\"\n    # pylint: disable=R0913\n    module_exclude = [] if module_exclude is None else module_exclude\n    callable_exclude = [] if callable_exclude is None else callable_exclude\n    par = trace_pars(mname)\n    start_time = datetime.datetime.now()\n    with pexdoc.exdoc.ExDocCxt(\n        exclude=par.exclude + module_exclude,\n        pickle_fname=par.pickle_fname,\n        in_callables_fname=par.in_callables_fname,\n        out_callables_fname=par.out_callables_fname,\n        _no_print=no_print,\n    ) as exdoc_obj:\n        fname = os.path.realpath(\n            os.path.join(\n                os.path.dirname(__file__),\n                \"..\",\n                \"..\",\n                \"tests\",\n                \"test_{0}.py\".format(fname),\n            )\n        )\n        test_cmd = (\n            [\"--color=yes\"]\n            + ([\"-s\", \"-vv\"] if debug else [\"-q\", \"-q\", \"-q\"])\n            + [\"--disable-warnings\"]\n            + [\"-x\"]\n            + ([par.noption] if par.noption else [])\n            + [\"-m \" + mname]\n            + [fname]\n        )\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", category=PytestWarning)\n            if pytest.main(test_cmd):\n                raise RuntimeError(\"Tracing did not complete successfully\")\n    stop_time = datetime.datetime.now()\n    if not no_print:\n        print(\n            \"Auto-generation of exceptions documentation time: {0}\".format(\n                pmisc.elapsed_time_string(start_time, stop_time)\n            )\n        )\n        for callable_name in callable_names:\n            callable_name = module_prefix + callable_name\n            print(\"\\nCallable: {0}\".format(callable_name))\n            print(exdoc_obj.get_sphinx_doc(callable_name, exclude=callable_exclude))\n            print(\"\\n\")\n    return copy.copy(exdoc_obj)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nshorten URL with optional keyword and title.", "response": "def shorten(self, url, keyword=None, title=None):\n        \"\"\"Shorten URL with optional keyword and title.\n\n        Parameters:\n            url: URL to shorten.\n            keyword: Optionally choose keyword for short URL, otherwise automatic.\n            title: Optionally choose title, otherwise taken from web page.\n\n        Returns:\n            ShortenedURL: Shortened URL and associated data.\n\n        Raises:\n            ~yourls.exceptions.YOURLSKeywordExistsError: The passed keyword\n                already exists.\n\n                .. note::\n\n                    This exception has a ``keyword`` attribute.\n\n            ~yourls.exceptions.YOURLSURLExistsError: The URL has already been\n                shortened.\n\n                .. note::\n\n                    This exception has a ``url`` attribute, which is an instance\n                    of :py:class:`ShortenedURL` for the existing short URL.\n\n            ~yourls.exceptions.YOURLSNoURLError: URL missing.\n\n            ~yourls.exceptions.YOURLSNoLoopError: Cannot shorten a shortened URL.\n\n            ~yourls.exceptions.YOURLSAPIError: Unhandled API error.\n\n            ~yourls.exceptions.YOURLSHTTPError: HTTP error with response from\n                YOURLS API.\n\n            requests.exceptions.HTTPError: Generic HTTP error.\n        \"\"\"\n        data = dict(action='shorturl', url=url, keyword=keyword, title=title)\n        jsondata = self._api_request(params=data)\n\n        url = _json_to_shortened_url(jsondata['url'], jsondata['shorturl'])\n\n        return url"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexpand short URL or keyword to long URL.", "response": "def expand(self, short):\n        \"\"\"Expand short URL or keyword to long URL.\n\n        Parameters:\n            short: Short URL (``http://example.com/abc``) or keyword (abc).\n\n        :return: Expanded/long URL, e.g.\n                 ``https://www.youtube.com/watch?v=dQw4w9WgXcQ``\n\n        Raises:\n            ~yourls.exceptions.YOURLSHTTPError: HTTP error with response from\n                YOURLS API.\n            requests.exceptions.HTTPError: Generic HTTP error.\n        \"\"\"\n        data = dict(action='expand', shorturl=short)\n        jsondata = self._api_request(params=data)\n\n        return jsondata['longurl']"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting stats for short URL or keyword.", "response": "def url_stats(self, short):\n        \"\"\"Get stats for short URL or keyword.\n\n        Parameters:\n            short: Short URL (http://example.com/abc) or keyword (abc).\n\n        Returns:\n            ShortenedURL: Shortened URL and associated data.\n\n        Raises:\n            ~yourls.exceptions.YOURLSHTTPError: HTTP error with response from\n                YOURLS API.\n            requests.exceptions.HTTPError: Generic HTTP error.\n        \"\"\"\n        data = dict(action='url-stats', shorturl=short)\n        jsondata = self._api_request(params=data)\n\n        return _json_to_shortened_url(jsondata['link'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef stats(self, filter, limit, start=None):\n        # Normalise random to rand, even though it's accepted by API.\n        if filter == 'random':\n            filter = 'rand'\n\n        valid_filters = ('top', 'bottom', 'rand', 'last')\n        if filter not in valid_filters:\n            msg = 'filter must be one of {}'.format(', '.join(valid_filters))\n            raise ValueError(msg)\n\n        data = dict(action='stats', filter=filter, limit=limit, start=start)\n        jsondata = self._api_request(params=data)\n\n        stats = DBStats(total_clicks=int(jsondata['stats']['total_clicks']),\n                        total_links=int(jsondata['stats']['total_links']))\n\n        links = []\n\n        if 'links' in jsondata:\n            for i in range(1, limit + 1):\n                key = 'link_{}'.format(i)\n                links.append(_json_to_shortened_url(jsondata['links'][key]))\n\n        return links, stats", "response": "Get stats about links."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting database statistics. Returns: DBStats: Total clicks and links statistics. Raises: requests.exceptions.HTTPError: Generic HTTP Error", "response": "def db_stats(self):\n        \"\"\"Get database statistics.\n\n        Returns:\n            DBStats: Total clicks and links statistics.\n\n        Raises:\n            requests.exceptions.HTTPError: Generic HTTP Error\n        \"\"\"\n        data = dict(action='db-stats')\n        jsondata = self._api_request(params=data)\n\n        stats = DBStats(total_clicks=int(jsondata['db-stats']['total_clicks']),\n                        total_links=int(jsondata['db-stats']['total_links']))\n\n        return stats"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprinting a command to the terminal and return the output.", "response": "def term_echo(command, nindent=0, env=None, fpointer=None, cols=60):\n    \"\"\"\n    Print STDOUT resulting from a Bash shell command formatted in reStructuredText.\n\n    :param command: Bash shell command\n    :type  command: string\n\n    :param nindent: Indentation level\n    :type  nindent: integer\n\n    :param env: Environment variable replacement dictionary. The Bash\n                command is pre-processed and any environment variable\n                represented in the full notation (:bash:`${...}`) is replaced.\n                The dictionary key is the environment variable name and the\n                dictionary value is the replacement value. For example, if\n                **command** is :code:`'${PYTHON_CMD} -m \"x=5\"'` and **env**\n                is :code:`{'PYTHON_CMD':'python3'}` the actual command issued\n                is :code:`'python3 -m \"x=5\"'`\n    :type  env: dictionary\n\n    :param fpointer: Output function pointer. Normally is :code:`cog.out` but\n                     :code:`print` or other functions can be used for\n                     debugging\n    :type  fpointer: function object\n\n    :param cols: Number of columns of output\n    :type  cols: integer\n    \"\"\"\n    # pylint: disable=R0204\n    # Set argparse width so that output does not need horizontal scroll\n    # bar in narrow windows or displays\n    os.environ[\"COLUMNS\"] = str(cols)\n    command_int = command\n    if env:\n        for var, repl in env.items():\n            command_int = command_int.replace(\"${\" + var + \"}\", repl)\n    tokens = command_int.split(\" \")\n    # Add Python interpreter executable for Python scripts on Windows since\n    # the shebang does not work\n    if (platform.system().lower() == \"windows\") and (tokens[0].endswith(\".py\")):\n        tokens = [sys.executable] + tokens\n    proc = subprocess.Popen(tokens, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n    stdout = proc.communicate()[0]\n    if sys.hexversion >= 0x03000000:\n        stdout = stdout.decode(\"utf-8\")\n    stdout = stdout.split(\"\\n\")\n    indent = nindent * \" \"\n    fpointer(\"\\n\", dedent=False)\n    fpointer(\"{0}.. code-block:: bash\\n\".format(indent), dedent=False)\n    fpointer(\"\\n\", dedent=False)\n    fpointer(\"{0}    $ {1}\\n\".format(indent, command), dedent=False)\n    for line in stdout:\n        if line.strip():\n            fpointer(indent + \"    \" + line.replace(\"\\t\", \"    \") + \"\\n\", dedent=False)\n        else:\n            fpointer(\"\\n\", dedent=False)\n    fpointer(\"\\n\", dedent=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlogs a message to stdout.", "response": "def log(self, msg, level=2):\n        \"\"\"\n        Small log helper\n        \"\"\"\n        if self.verbosity >= level:\n            self.stdout.write(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cached(method) -> property:\n    name = \"_\" + method.__name__\n\n    @property\n    def wrapper(self):\n        try:\n            return getattr(self, name)\n        except AttributeError:\n            val = method(self)\n            setattr(self, name, val)\n            return val\n\n    return wrapper", "response": "alternative to reify and property decorators. caches the value when it s\n    generated. It cashes it as instance. _name_of_the_property."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbreaks an iterable into chunks and yield those chunks as lists until there s nothing left to yeild.", "response": "def chunkiter(iterable, chunksize):\n    \"\"\"break an iterable into chunks and yield those chunks as lists\n    until there's nothing left to yeild.\n    \"\"\"\n    iterator = iter(iterable)\n    for chunk in iter(lambda: list(itertools.islice(iterator, chunksize)), []):\n        yield chunk"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntaking a function that taks an iterable as the first argument. return a wrapper that will break an iterable into chunks using chunkiter and run each chunk in function call as an iterator.", "response": "def chunkprocess(func):\n    \"\"\"take a function that taks an iterable as the first argument.\n    return a wrapper that will break an iterable into chunks using\n    chunkiter and run each chunk in function, yielding the value of each\n    function call as an iterator.\n    \"\"\"\n\n    @functools.wraps(func)\n    def wrapper(iterable, chunksize, *args, **kwargs):\n        for chunk in chunkiter(iterable, chunksize):\n            yield func(chunk, *args, **kwargs)\n\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef deepupdate(\n    mapping: abc.MutableMapping, other: abc.Mapping, listextend=False\n):\n    \"\"\"update one dictionary from another recursively. Only individual\n    values will be overwritten--not entire branches of nested\n    dictionaries.\n    \"\"\"\n\n    def inner(other, previouskeys):\n        \"\"\"previouskeys is a tuple that stores all the names of keys\n        we've recursed into so far so it can they can be looked up\n        recursively on the pimary mapping when a value needs updateing.\n        \"\"\"\n        for key, value in other.items():\n            if isinstance(value, abc.Mapping):\n                inner(value, (*previouskeys, key))\n\n            else:\n                node = mapping\n                for previouskey in previouskeys:\n                    node = node.setdefault(previouskey, {})\n                target = node.get(key)\n                if (\n                    listextend\n                    and isinstance(target, abc.MutableSequence)\n                    and isinstance(value, abc.Sequence)\n                ):\n                    target.extend(value)\n                else:\n                    node[key] = value\n\n    inner(other, ())", "response": "update one dictionary from another recursively."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef quietinterrupt(msg=None):\n\n    def handler():\n        if msg:\n            print(msg, file=sys.stderr)\n        sys.exit(1)\n\n    signal.signal(signal.SIGINT, handler)", "response": "add a handler for SIGINT that optionally prints a given message."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mkdummy(name, **attrs):\n    return type(\n        name, (), dict(__repr__=(lambda self: \"<%s>\" % name), **attrs)\n    )()", "response": "Make a dummy object that uses its own name for its repr"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npipes value with functions", "response": "def pipe(value, *functions, funcs=None):\n    \"\"\"pipe(value, f, g, h) == h(g(f(value)))\"\"\"\n    if funcs:\n        functions = funcs\n    for function in functions:\n        value = function(value)\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pipeline(*functions, funcs=None):\n    if funcs:\n        functions = funcs\n    head, *tail = functions\n    return lambda *args, **kwargs: pipe(head(*args, **kwargs), funcs=tail)", "response": "like pipeline but curried"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef human_readable(self, decimal=False):\n        divisor = 1000 if decimal else 1024\n        number = int(self)\n        unit = \"\"\n        for unit in self.units:\n            if number < divisor:\n                break\n            number /= divisor\n        return number, unit.upper()", "response": "returns the size of size as a tuple of number and single - letter - unit"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nattempting to parse a size in bytes from a human - readable string.", "response": "def from_str(cls, human_readable_str, decimal=False, bits=False):\n        \"\"\"attempt to parse a size in bytes from a human-readable string.\"\"\"\n        divisor = 1000 if decimal else 1024\n        num = []\n        c = \"\"\n        for c in human_readable_str:\n            if c not in cls.digits:\n                break\n            num.append(c)\n        num = \"\".join(num)\n        try:\n            num = int(num)\n        except ValueError:\n            num = float(num)\n        if bits:\n            num /= 8\n        return cls(round(num * divisor ** cls.key[c.lower()]))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncommanding line interface for YOURLS.", "response": "def cli(ctx, apiurl, signature, username, password):\n    \"\"\"Command line interface for YOURLS.\n\n    Configuration parameters can be passed as switches or stored in .yourls or\n    ~/.yourls.\n\n    If your YOURLS server requires authentication, please provide one of the\n    following:\n\n    \\b\n    \u2022 apiurl and signature\n    \u2022 apiurl, username, and password\n\n    Configuration file format:\n\n    \\b\n    [yourls]\n    apiurl = http://example.com/yourls-api.php\n    signature = abcdefghij\n    \"\"\"\n    if apiurl is None:\n        raise click.UsageError(\"apiurl missing. See 'yourls --help'\")\n\n    auth_params = dict(signature=signature, username=username, password=password)\n\n    try:\n        ctx.obj = YOURLSClient(apiurl=apiurl, **auth_params)\n    except TypeError:\n        raise click.UsageError(\"authentication paremeters overspecified. \"\n                               \"See 'yourls --help'\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef trace_module(no_print=True):\n    mname = \"wave_core\"\n    fname = \"peng\"\n    module_prefix = \"peng.{0}.Waveform.\".format(mname)\n    callable_names = (\"__init__\",)\n    return docs.support.trace_support.run_trace(\n        mname, fname, module_prefix, callable_names, no_print\n    )", "response": "Trace eng wave module exceptions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndefining Sphinx requirements links.", "response": "def def_links(mobj):\n    \"\"\"Define Sphinx requirements links.\"\"\"\n    fdict = json_load(os.path.join(\"data\", \"requirements.json\"))\n    sdeps = sorted(fdict.keys())\n    olines = []\n    for item in sdeps:\n        olines.append(\n            \".. _{name}: {url}\\n\".format(\n                name=fdict[item][\"name\"], url=fdict[item][\"url\"]\n            )\n        )\n    ret = []\n    for line in olines:\n        wobj = textwrap.wrap(line, width=LINE_WIDTH, subsequent_indent=\"   \")\n        ret.append(\"\\n\".join([item for item in wobj]))\n    mobj.out(\"\\n\".join(ret))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_common_entry(plist, pyver, suffix, req_ver):\n    prefix = \"Python {pyver}.x{suffix}\".format(pyver=pyver, suffix=suffix)\n    plist.append(\"{prefix}{ver}\".format(prefix=prefix, ver=ops_to_words(req_ver)))", "response": "Generate common entry for 2. x or 3. x series."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate Python interpreter version entries.", "response": "def make_multi_entry(plist, pkg_pyvers, ver_dict):\n    \"\"\"Generate Python interpreter version entries.\"\"\"\n    for pyver in pkg_pyvers:\n        pver = pyver[2] + \".\" + pyver[3:]\n        plist.append(\"Python {0}: {1}\".format(pver, ops_to_words(ver_dict[pyver])))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef op_to_words(item):\n    sdicts = [\n        {\"==\": \"\"},\n        {\">=\": \" or newer\"},\n        {\">\": \"newer than \"},\n        {\"<=\": \" or older\"},\n        {\"<\": \"older than \"},\n        {\"!=\": \"except \"},\n    ]\n    for sdict in sdicts:\n        prefix = list(sdict.keys())[0]\n        suffix = sdict[prefix]\n        if item.startswith(prefix):\n            if prefix == \"==\":\n                return item[2:]\n            if prefix == \"!=\":\n                return suffix + item[2:]\n            if prefix in [\">\", \"<\"]:\n                return suffix + item[1:]\n            return item[2:] + suffix\n    raise RuntimeError(\"Inequality not supported\")", "response": "Translate > = < = to words."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntranslate requirement specification to words.", "response": "def ops_to_words(item):\n    \"\"\"Translate requirement specification to words.\"\"\"\n    unsupp_ops = [\"~=\", \"===\"]\n    # Ordered for  \"pleasant\" word specification\n    supp_ops = [\">=\", \">\", \"==\", \"<=\", \"<\", \"!=\"]\n    tokens = sorted(item.split(\",\"), reverse=True)\n    actual_tokens = []\n    for req in tokens:\n        for op in unsupp_ops:\n            if req.startswith(op):\n                raise RuntimeError(\"Unsupported version specification: {0}\".format(op))\n        for op in supp_ops:\n            if req.startswith(op):\n                actual_tokens.append(op)\n                break\n        else:\n            raise RuntimeError(\"Illegal comparison operator: {0}\".format(op))\n    if len(list(set(actual_tokens))) != len(actual_tokens):\n        raise RuntimeError(\"Multiple comparison operators of the same type\")\n    if \"!=\" in actual_tokens:\n        return (\n            \" and \".join([op_to_words(token) for token in tokens[:-1]])\n            + \" \"\n            + op_to_words(tokens[-1])\n        )\n    return \" and \".join([op_to_words(token) for token in tokens])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprocess the requirements file and return a list of all the available Python interpreter and Python version.", "response": "def proc_requirements(mobj):\n    \"\"\"Get requirements in reStructuredText format.\"\"\"\n    pyvers = [\"py{0}\".format(item.replace(\".\", \"\")) for item in get_supported_interps()]\n    py2vers = sorted([item for item in pyvers if item.startswith(\"py2\")])\n    py3vers = sorted([item for item in pyvers if item.startswith(\"py3\")])\n    fdict = json_load(os.path.join(\"data\", \"requirements.json\"))\n    olines = [\"\"]\n    sdict = dict([(item[\"name\"], item) for item in fdict.values()])\n    for real_name in sorted(sdict.keys()):\n        pkg_dict = sdict[real_name]\n        if pkg_dict[\"cat\"] == [\"rtd\"]:\n            continue\n        plist = [] if not pkg_dict[\"optional\"] else [\"optional\"]\n        # Convert instances that have a single version for all Python\n        # interpreters into a full dictionary of Python interpreter and\n        # package versions # so as to apply the same algorithm in all cases\n        if isinstance(pkg_dict[\"ver\"], str):\n            pkg_dict[\"ver\"] = dict([(pyver, pkg_dict[\"ver\"]) for pyver in pyvers])\n        pkg_pyvers = sorted(pkg_dict[\"ver\"].keys())\n        pkg_py2vers = sorted(\n            [item for item in pkg_dict[\"ver\"].keys() if item.startswith(\"py2\")]\n        )\n        req_vers = list(set(pkg_dict[\"ver\"].values()))\n        req_py2vers = list(\n            set([pkg_dict[\"ver\"][item] for item in py2vers if item in pkg_dict[\"ver\"]])\n        )\n        req_py3vers = list(\n            set([pkg_dict[\"ver\"][item] for item in py3vers if item in pkg_dict[\"ver\"]])\n        )\n        if (len(req_vers) == 1) and (pkg_pyvers == pyvers):\n            plist.append(ops_to_words(req_vers[0]))\n        elif (\n            (pkg_pyvers == pyvers)\n            and (len(req_py2vers) == 1)\n            and (len(req_py3vers) == 1)\n        ):\n            make_common_entry(plist, \"2\", \": \", req_py2vers[0])\n            make_common_entry(plist, \"3\", \": \", req_py3vers[0])\n        elif (\n            (pkg_pyvers == pyvers)\n            and (len(req_py2vers) == len(py2vers))\n            and (len(req_py3vers) == 1)\n            and (pkg_dict[\"ver\"][pkg_py2vers[-1]] == req_py3vers[0])\n        ):\n            py2dict = dict(\n                [\n                    (key, value)\n                    for key, value in pkg_dict[\"ver\"].items()\n                    if key.startswith(\"py2\") and (key != pkg_py2vers[-1])\n                ]\n            )\n            make_multi_entry(plist, py2vers[:-1], py2dict)\n            pver = pkg_py2vers[-1][2] + \".\" + pkg_py2vers[-1][3:]\n            plist.append(\n                \"Python {pyver} or newer: {ver}\".format(\n                    pyver=pver, ver=ops_to_words(req_py3vers[0])\n                )\n            )\n        elif (\n            (pkg_pyvers == pyvers)\n            and (len(req_py2vers) == len(py2vers))\n            and (len(req_py3vers) == 1)\n        ):\n            py2dict = dict(\n                [\n                    (key, value)\n                    for key, value in pkg_dict[\"ver\"].items()\n                    if key.startswith(\"py2\")\n                ]\n            )\n            make_multi_entry(plist, py2vers, py2dict)\n            make_common_entry(plist, \"3\", \": \", req_py3vers[0])\n        elif (\n            (pkg_pyvers == pyvers)\n            and (len(req_py3vers) == len(py3vers))\n            and (len(req_py2vers) == 1)\n        ):\n            py3dict = dict(\n                [\n                    (key, value)\n                    for key, value in pkg_dict[\"ver\"].items()\n                    if key.startswith(\"py3\")\n                ]\n            )\n            make_common_entry(plist, \"2\", \": \", req_py2vers[0])\n            make_multi_entry(plist, py3vers, py3dict)\n        elif (len(req_vers) == 1) and (pkg_pyvers == py2vers):\n            make_common_entry(plist, \"2\", \" only, \", req_vers[0])\n        elif (len(req_vers) == 1) and (pkg_pyvers == py3vers):\n            make_common_entry(plist, \"3\", \" only, \", req_vers[0])\n        else:\n            make_multi_entry(plist, pkg_pyvers, pkg_dict[\"ver\"])\n        olines.append(\n            \"    * `{name}`_ ({par})\".format(\n                name=pkg_dict[\"name\"], par=\", \".join(plist)\n            )\n        )\n    ret = []\n    for line in olines:\n        wobj = textwrap.wrap(line, width=LINE_WIDTH, subsequent_indent=\"      \")\n        ret.append(\"\\n\".join([item for item in wobj]))\n\n    mobj.out(\"\\n\\n\".join(ret) + \"\\n\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _make_version(major, minor, micro, level, serial):\n    level_dict = {\"alpha\": \"a\", \"beta\": \"b\", \"candidate\": \"rc\", \"final\": \"\"}\n    if level not in level_dict:\n        raise RuntimeError(\"Invalid release level\")\n    version = \"{0:d}.{1:d}\".format(major, minor)\n    if micro:\n        version += \".{0:d}\".format(micro)\n    if level != \"final\":\n        version += \"{0}{1:d}\".format(level_dict[level], serial)\n    return version", "response": "Generate version string from tuple."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nyield noise data into valid Touchstone file rows.", "response": "def _chunk_noise(noise):\n    \"\"\"Chunk input noise data into valid Touchstone file rows.\"\"\"\n    data = zip(\n        noise[\"freq\"],\n        noise[\"nf\"],\n        np.abs(noise[\"rc\"]),\n        np.angle(noise[\"rc\"]),\n        noise[\"res\"],\n    )\n    for freq, nf, rcmag, rcangle, res in data:\n        yield freq, nf, rcmag, rcangle, res"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _chunk_pars(freq_vector, data_matrix, pformat):\n    pformat = pformat.upper()\n    length = 4\n    for freq, data in zip(freq_vector, data_matrix):\n        data = data.flatten()\n        for index in range(0, data.size, length):\n            fpoint = [freq] if not index else [None]\n            cdata = data[index : index + length]\n            if pformat == \"MA\":\n                vector1 = np.abs(cdata)\n                vector2 = np.rad2deg(np.angle(cdata))\n            elif pformat == \"RI\":\n                vector1 = np.real(cdata)\n                vector2 = np.imag(cdata)\n            else:  # elif pformat == 'DB':\n                vector1 = 20.0 * np.log10(np.abs(cdata))\n                vector2 = np.rad2deg(np.angle(cdata))\n            sep_data = np.array([])\n            for item1, item2 in zip(vector1, vector2):\n                sep_data = np.concatenate((sep_data, np.array([item1, item2])))\n            ret = np.concatenate((np.array(fpoint), sep_data))\n            yield ret", "response": "Yields a list of valid Touchstone file rows."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_touchstone(fname):\n    # pylint: disable=R0912,R0915,W0702\n    # Exceptions definitions\n    exnports = pexdoc.exh.addex(\n        RuntimeError, \"File *[fname]* does not have a valid extension\"\n    )\n    exnoopt = pexdoc.exh.addex(\n        RuntimeError, \"First non-comment line is not the option line\"\n    )\n    exopt = pexdoc.exh.addex(RuntimeError, \"Illegal option line\")\n    exline = pexdoc.exh.addex(RuntimeError, \"Illegal data in line *[lineno]*\")\n    exnodata = pexdoc.exh.addex(RuntimeError, \"File *[fname]* has no data\")\n    exdata = pexdoc.exh.addex(RuntimeError, \"Malformed data\")\n    exndata = pexdoc.exh.addex(RuntimeError, \"Malformed noise data\")\n    exfreq = pexdoc.exh.addex(RuntimeError, \"Frequency must increase\")\n    exnfreq = pexdoc.exh.addex(RuntimeError, \"Noise frequency must increase\")\n    # Verify that file has correct extension format\n    _, ext = os.path.splitext(fname)\n    ext = ext.lower()\n    nports_regexp = re.compile(r\"\\.s(\\d+)p\")\n    match = nports_regexp.match(ext)\n    exnports(not match, edata={\"field\": \"fname\", \"value\": fname})\n    nports = int(match.groups()[0])\n    opt_line = False\n    units_dict = {\"GHZ\": \"GHz\", \"MHZ\": \"MHz\", \"KHZ\": \"KHz\", \"HZ\": \"Hz\"}\n    scale_dict = {\"GHZ\": 1e9, \"MHZ\": 1e6, \"KHZ\": 1e3, \"HZ\": 1.0}\n    units_opts = [\"GHZ\", \"MHZ\", \"KHZ\", \"HZ\"]\n    type_opts = [\"S\", \"Y\", \"Z\", \"H\", \"G\"]\n    format_opts = [\"DB\", \"MA\", \"RI\"]\n    opts = dict(units=None, ptype=None, pformat=None, z0=None)\n    data = []\n    with open(fname, \"r\") as fobj:\n        for num, line in enumerate(fobj):\n            line = line.strip().upper()\n            # Comment line\n            if line.startswith(\"!\"):\n                continue\n            # Options line\n            if (not opt_line) and (not line.startswith(\"#\")):\n                exnoopt(True)\n            if not opt_line:\n                # Each Touchstone data file must contain an option line\n                # (additional option lines after the first one will be ignored)\n                opt_line = True\n                tokens = line[1:].split()  # Remove initial hash\n                if \"R\" in tokens:\n                    idx = tokens.index(\"R\")\n                    add = 1\n                    if len(tokens) > idx + 1:\n                        try:\n                            opts[\"z0\"] = float(tokens[idx + 1])\n                            add = 2\n                        except:\n                            pass\n                    tokens = tokens[:idx] + tokens[idx + add :]\n                matches = 0\n                for token in tokens:\n                    if (token in format_opts) and (not opts[\"pformat\"]):\n                        matches += 1\n                        opts[\"pformat\"] = token\n                    elif (token in units_opts) and (not opts[\"units\"]):\n                        matches += 1\n                        opts[\"units\"] = units_dict[token]\n                    elif (token in type_opts) and (not opts[\"ptype\"]):\n                        matches += 1\n                        opts[\"ptype\"] = token\n                exopt(matches != len(tokens))\n            if opt_line and line.startswith(\"#\"):\n                continue\n            # Data lines\n            try:\n                if \"!\" in line:\n                    idx = line.index(\"!\")\n                    line = line[:idx]\n                tokens = [float(item) for item in line.split()]\n                data.append(tokens)\n            except:\n                exline(True, edata={\"field\": \"lineno\", \"value\": num + 1})\n    data = np.concatenate(data)\n    exnodata(not data.size, edata={\"field\": \"fname\", \"value\": fname})\n    # Set option defaults\n    opts[\"units\"] = opts[\"units\"] or \"GHz\"\n    opts[\"ptype\"] = opts[\"ptype\"] or \"S\"\n    opts[\"pformat\"] = opts[\"pformat\"] or \"MA\"\n    opts[\"z0\"] = opts[\"z0\"] or 50\n    # Format data\n    data_dict = {}\n    nums_per_freq = 1 + (2 * (nports ** 2))\n    fslice = slice(0, data.size, nums_per_freq)\n    freq = data[fslice]\n    ndiff = np.diff(freq)\n    ndict = {}\n    if (nports == 2) and ndiff.size and (min(ndiff) <= 0):\n        # Extract noise data\n        npoints = np.where(ndiff <= 0)[0][0] + 1\n        freq = freq[:npoints]\n        ndata = data[9 * npoints :]\n        nfpoints = int(ndata.size / 5.0)\n        exndata(ndata.size % 5 != 0)\n        data = data[: 9 * npoints]\n        ndiff = 1\n        nfslice = slice(0, ndata.size, 5)\n        nfreq = ndata[nfslice]\n        ndiff = np.diff(nfreq)\n        exnfreq(bool(ndiff.size and (min(ndiff) <= 0)))\n        nfig_slice = slice(1, ndata.size, 5)\n        rlmag_slice = slice(2, ndata.size, 5)\n        rlphase_slice = slice(3, ndata.size, 5)\n        res_slice = slice(4, ndata.size, 5)\n        ndict[\"freq\"] = scale_dict[opts[\"units\"].upper()] * nfreq\n        ndict[\"nf\"] = ndata[nfig_slice]\n        ndict[\"rc\"] = ndata[rlmag_slice] * np.exp(1j * ndata[rlphase_slice])\n        ndict[\"res\"] = ndata[res_slice]\n        ndict[\"points\"] = nfpoints\n    exdata(data.size % nums_per_freq != 0)\n    npoints = int(data.size / nums_per_freq)\n    exfreq(bool(ndiff.size and (min(ndiff) <= 0)))\n    data_dict[\"freq\"] = scale_dict[opts[\"units\"].upper()] * freq\n    d1slice = slice(0, data.size, 2)\n    d2slice = slice(1, data.size, 2)\n    data = np.delete(data, fslice)\n    # For format that has angle information, the angle is given in degrees\n    if opts[\"pformat\"] == \"MA\":\n        data = data[d1slice] * np.exp(1j * np.deg2rad(data[d2slice]))\n    elif opts[\"pformat\"] == \"RI\":\n        data = data[d1slice] + (1j * data[d2slice])\n    else:  # if opts['pformat'] == 'DB':\n        data = (10 ** (data[d1slice] / 20.0)) * np.exp(1j * np.deg2rad(data[d2slice]))\n    if nports > 1:\n        data_dict[\"pars\"] = np.resize(data, (npoints, nports, nports))\n    else:\n        data_dict[\"pars\"] = copy.copy(data)\n    del data\n    data_dict[\"points\"] = npoints\n    if nports == 2:\n        # The order of data for a two-port file is N11, N21, N12, N22 but for\n        # m ports where m > 2, the order is N11, N12, N13, ..., N1m\n        data_dict[\"pars\"] = np.transpose(data_dict[\"pars\"], (0, 2, 1))\n    return dict(nports=nports, opts=opts, data=data_dict, noise=ndict)", "response": "r Reads a Touchstone file and returns a dictionary of the data points and noise data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_touchstone(fname, options, data, noise=None, frac_length=10, exp_length=2):\n    # pylint: disable=R0913\n    # Exceptions definitions\n    exnports = pexdoc.exh.addex(\n        RuntimeError, \"File *[fname]* does not have a valid extension\"\n    )\n    exnoise = pexdoc.exh.addex(\n        RuntimeError, \"Noise data only supported in two-port files\"\n    )\n    expoints = pexdoc.exh.addex(RuntimeError, \"Malformed data\")\n    # Data validation\n    _, ext = os.path.splitext(fname)\n    ext = ext.lower()\n    nports_regexp = re.compile(r\"\\.s(\\d+)p\")\n    match = nports_regexp.match(ext)\n    exnports(not match, edata={\"field\": \"fname\", \"value\": fname})\n    nports = int(match.groups()[0])\n    exnoise(bool((nports != 2) and noise))\n    nums_per_freq = nports ** 2\n    expoints(data[\"points\"] * nums_per_freq != data[\"pars\"].size)\n    #\n    npoints = data[\"points\"]\n    par_data = np.resize(np.copy(data[\"pars\"]), (npoints, nports, nports))\n    if nports == 2:\n        par_data = np.transpose(par_data, (0, 2, 1))\n    units_dict = {\"ghz\": \"GHz\", \"mhz\": \"MHz\", \"khz\": \"KHz\", \"hz\": \"Hz\"}\n    options[\"units\"] = units_dict[options[\"units\"].lower()]\n    fspace = 2 + frac_length + (exp_length + 2)\n    # Format data\n    with open(fname, \"w\") as fobj:\n        fobj.write(\n            \"# {units} {ptype} {pformat} R {z0}\\n\".format(\n                units=options[\"units\"],\n                ptype=options[\"ptype\"],\n                pformat=options[\"pformat\"],\n                z0=options[\"z0\"],\n            )\n        )\n        for row in _chunk_pars(data[\"freq\"], par_data, options[\"pformat\"]):\n            row_data = [\n                to_scientific_string(item, frac_length, exp_length, bool(num != 0))\n                if item is not None\n                else fspace * \" \"\n                for num, item in enumerate(row)\n            ]\n            fobj.write(\" \".join(row_data) + \"\\n\")\n        if (nports == 2) and noise:\n            fobj.write(\"! Noise data\\n\")\n            for row in _chunk_noise(noise):\n                row_data = [\n                    to_scientific_string(item, frac_length, exp_length, bool(num != 0))\n                    for num, item in enumerate(row)\n                ]\n                fobj.write(\" \".join(row_data) + \"\\n\")", "response": "r Write a Touchstone file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _bound_waveform(wave, indep_min, indep_max):\n    indep_min, indep_max = _validate_min_max(wave, indep_min, indep_max)\n    indep_vector = copy.copy(wave._indep_vector)\n    if (\n        isinstance(indep_min, float) or isinstance(indep_max, float)\n    ) and indep_vector.dtype.name.startswith(\"int\"):\n        indep_vector = indep_vector.astype(float)\n    min_pos = np.searchsorted(indep_vector, indep_min)\n    if not np.isclose(indep_min, indep_vector[min_pos], FP_RTOL, FP_ATOL):\n        indep_vector = np.insert(indep_vector, min_pos, indep_min)\n    max_pos = np.searchsorted(indep_vector, indep_max)\n    if not np.isclose(indep_max, indep_vector[max_pos], FP_RTOL, FP_ATOL):\n        indep_vector = np.insert(indep_vector, max_pos, indep_max)\n    dep_vector = _interp_dep_vector(wave, indep_vector)\n    wave._indep_vector = indep_vector[min_pos : max_pos + 1]\n    wave._dep_vector = dep_vector[min_pos : max_pos + 1]", "response": "Add independent variable vector bounds if they are not in vector."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _build_units(indep_units, dep_units, op):\n    if (not dep_units) and (not indep_units):\n        return \"\"\n    if dep_units and (not indep_units):\n        return dep_units\n    if (not dep_units) and indep_units:\n        return (\n            remove_extra_delims(\"1{0}({1})\".format(op, indep_units))\n            if op == \"/\"\n            else remove_extra_delims(\"({0})\".format(indep_units))\n        )\n    return remove_extra_delims(\"({0}){1}({2})\".format(dep_units, op, indep_units))", "response": "Build unit math operations."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _operation(wave, desc, units, fpointer):\n    ret = copy.copy(wave)\n    ret.dep_units = units\n    ret.dep_name = \"{0}({1})\".format(desc, ret.dep_name)\n    ret._dep_vector = fpointer(ret._dep_vector)\n    return ret", "response": "Perform generic operation on a waveform object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _running_area(indep_vector, dep_vector):\n    rect_height = np.minimum(dep_vector[:-1], dep_vector[1:])\n    rect_base = np.diff(indep_vector)\n    rect_area = np.multiply(rect_height, rect_base)\n    triang_height = np.abs(np.diff(dep_vector))\n    triang_area = 0.5 * np.multiply(triang_height, rect_base)\n    return np.cumsum(np.concatenate((np.array([0.0]), triang_area + rect_area)))", "response": "Calculate running area under curve."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _validate_min_max(wave, indep_min, indep_max):\n    imin, imax = False, False\n    if indep_min is None:\n        indep_min = wave._indep_vector[0]\n        imin = True\n    if indep_max is None:\n        indep_max = wave._indep_vector[-1]\n        imax = True\n    if imin and imax:\n        return indep_min, indep_max\n    exminmax = pexdoc.exh.addex(\n        RuntimeError, \"Incongruent `indep_min` and `indep_max` arguments\"\n    )\n    exmin = pexdoc.exh.addai(\"indep_min\")\n    exmax = pexdoc.exh.addai(\"indep_max\")\n    exminmax(bool(indep_min >= indep_max))\n    exmin(\n        bool(\n            (indep_min < wave._indep_vector[0])\n            and (not np.isclose(indep_min, wave._indep_vector[0], FP_RTOL, FP_ATOL))\n        )\n    )\n    exmax(\n        bool(\n            (indep_max > wave._indep_vector[-1])\n            and (not np.isclose(indep_max, wave._indep_vector[-1], FP_RTOL, FP_ATOL))\n        )\n    )\n    return indep_min, indep_max", "response": "Validate that min and max bounds are within waveform s independent variable vector."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef acos(wave):\n    pexdoc.exh.addex(\n        ValueError,\n        \"Math domain error\",\n        bool((min(wave._dep_vector) < -1) or (max(wave._dep_vector) > 1)),\n    )\n    return _operation(wave, \"acos\", \"rad\", np.arccos)", "response": "r Returns the arc cosine of a waveform s dependent variable vector."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef acosh(wave):\n    pexdoc.exh.addex(ValueError, \"Math domain error\", bool(min(wave._dep_vector) < 1))\n    return _operation(wave, \"acosh\", \"\", np.arccosh)", "response": "r Returns the hyperbolic arc cosine of a waveform s dependent variable vector."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef db(wave):\n    pexdoc.exh.addex(\n        ValueError, \"Math domain error\", bool((np.min(np.abs(wave._dep_vector)) <= 0))\n    )\n    ret = copy.copy(wave)\n    ret.dep_units = \"dB\"\n    ret.dep_name = \"db({0})\".format(ret.dep_name)\n    ret._dep_vector = 20.0 * np.log10(np.abs(ret._dep_vector))\n    return ret", "response": "r Return a waveform s dependent variable vector expressed in decibels."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef derivative(wave, indep_min=None, indep_max=None):\n    ret = copy.copy(wave)\n    _bound_waveform(ret, indep_min, indep_max)\n    delta_indep = np.diff(ret._indep_vector)\n    delta_dep = np.diff(ret._dep_vector)\n    delta_indep = np.concatenate((np.array([delta_indep[0]]), delta_indep))\n    delta_dep = np.concatenate((np.array([delta_dep[0]]), delta_dep))\n    ret._dep_vector = np.divide(delta_dep, delta_indep)\n    ret.dep_name = \"derivative({0})\".format(ret._dep_name)\n    ret.dep_units = _build_units(ret.indep_units, ret.dep_units, \"/\")\n    return ret", "response": "r Return the numerical derivative of a waveform s dependent variable vector."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fft(wave, npoints=None, indep_min=None, indep_max=None):\n    ret = copy.copy(wave)\n    _bound_waveform(ret, indep_min, indep_max)\n    npoints = npoints or ret._indep_vector.size\n    fs = (npoints - 1) / float(ret._indep_vector[-1])\n    spoints = min(ret._indep_vector.size, npoints)\n    sdiff = np.diff(ret._indep_vector[:spoints])\n    cond = not np.all(\n        np.isclose(sdiff, sdiff[0] * np.ones(spoints - 1), FP_RTOL, FP_ATOL)\n    )\n    pexdoc.addex(RuntimeError, \"Non-uniform sampling\", cond)\n    finc = fs / float(npoints - 1)\n    indep_vector = _barange(-fs / 2.0, +fs / 2.0, finc)\n    dep_vector = np.fft.fft(ret._dep_vector, npoints)\n    return Waveform(\n        indep_vector=indep_vector,\n        dep_vector=dep_vector,\n        dep_name=\"fft({0})\".format(ret.dep_name),\n        indep_scale=\"LINEAR\",\n        dep_scale=\"LINEAR\",\n        indep_units=\"Hz\",\n        dep_units=\"\",\n    )", "response": "r Returns the Fast Fourier Transform of a waveform."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fftm(wave, npoints=None, indep_min=None, indep_max=None):\n    return abs(fft(wave, npoints, indep_min, indep_max))", "response": "r Returns the magnitude of the Fast Fourier Transform of a waveform."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find(wave, dep_var, der=None, inst=1, indep_min=None, indep_max=None):\n    # pylint: disable=C0325,R0914,W0613\n    ret = copy.copy(wave)\n    _bound_waveform(ret, indep_min, indep_max)\n    close_min = np.isclose(min(ret._dep_vector), dep_var, FP_RTOL, FP_ATOL)\n    close_max = np.isclose(max(ret._dep_vector), dep_var, FP_RTOL, FP_ATOL)\n    if ((np.amin(ret._dep_vector) > dep_var) and (not close_min)) or (\n        (np.amax(ret._dep_vector) < dep_var) and (not close_max)\n    ):\n        return None\n    cross_wave = ret._dep_vector - dep_var\n    sign_wave = np.sign(cross_wave)\n    exact_idx = np.where(np.isclose(ret._dep_vector, dep_var, FP_RTOL, FP_ATOL))[0]\n    # Locations where dep_vector crosses dep_var or it is equal to it\n    left_idx = np.where(np.diff(sign_wave))[0]\n    # Remove elements to the left of exact matches\n    left_idx = np.setdiff1d(left_idx, exact_idx)\n    left_idx = np.setdiff1d(left_idx, exact_idx - 1)\n    right_idx = left_idx + 1 if left_idx.size else np.array([])\n    indep_var = ret._indep_vector[exact_idx] if exact_idx.size else np.array([])\n    dvector = np.zeros(exact_idx.size).astype(int) if exact_idx.size else np.array([])\n    if left_idx.size and (ret.interp == \"STAIRCASE\"):\n        idvector = (\n            2.0 * (ret._dep_vector[right_idx] > ret._dep_vector[left_idx]).astype(int)\n            - 1\n        )\n        if indep_var.size:\n            indep_var = np.concatenate((indep_var, ret._indep_vector[right_idx]))\n            dvector = np.concatenate((dvector, idvector))\n            sidx = np.argsort(indep_var)\n            indep_var = indep_var[sidx]\n            dvector = dvector[sidx]\n        else:\n            indep_var = ret._indep_vector[right_idx]\n            dvector = idvector\n    elif left_idx.size:\n        y_left = ret._dep_vector[left_idx]\n        y_right = ret._dep_vector[right_idx]\n        x_left = ret._indep_vector[left_idx]\n        x_right = ret._indep_vector[right_idx]\n        slope = ((y_left - y_right) / (x_left - x_right)).astype(float)\n        # y = y0+slope*(x-x0) => x0+(y-y0)/slope\n        if indep_var.size:\n            indep_var = np.concatenate(\n                (indep_var, x_left + ((dep_var - y_left) / slope))\n            )\n            dvector = np.concatenate((dvector, np.where(slope > 0, 1, -1)))\n            sidx = np.argsort(indep_var)\n            indep_var = indep_var[sidx]\n            dvector = dvector[sidx]\n        else:\n            indep_var = x_left + ((dep_var - y_left) / slope)\n            dvector = np.where(slope > 0, +1, -1)\n    if der is not None:\n        indep_var = np.extract(dvector == der, indep_var)\n    return indep_var[inst - 1] if inst <= indep_var.size else None", "response": "r Returns the independent variable point associated with a dependent variable point."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ifftdb(wave, npoints=None, indep_min=None, indep_max=None):\n    return db(ifft(wave, npoints, indep_min, indep_max))", "response": "r Returns the inverse Fast Fourier Transform of a waveform."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ifftm(wave, npoints=None, indep_min=None, indep_max=None):\n    return abs(ifft(wave, npoints, indep_min, indep_max))", "response": "r Returns the magnitude of the inverse Fast Fourier Transform of a waveform."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ifftp(wave, npoints=None, indep_min=None, indep_max=None, unwrap=True, rad=True):\n    return phase(ifft(wave, npoints, indep_min, indep_max), unwrap=unwrap, rad=rad)", "response": "r Returns the phase of the inverse Fast Fourier Transform of a waveform."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ifftr(wave, npoints=None, indep_min=None, indep_max=None):\n    return real(ifft(wave, npoints, indep_min, indep_max))", "response": "r Returns the real part of the inverse Fast Fourier Transform of a waveform."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef naverage(wave, indep_min=None, indep_max=None):\n    ret = copy.copy(wave)\n    _bound_waveform(ret, indep_min, indep_max)\n    delta_x = ret._indep_vector[-1] - ret._indep_vector[0]\n    return np.trapz(ret._dep_vector, x=ret._indep_vector) / delta_x", "response": "r Return the numerical average of a waveform s dependent variable vector."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef nmax(wave, indep_min=None, indep_max=None):\n    ret = copy.copy(wave)\n    _bound_waveform(ret, indep_min, indep_max)\n    return np.max(ret._dep_vector)", "response": "r Return the maximum of a waveform s dependent variable vector."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef nmin(wave, indep_min=None, indep_max=None):\n    ret = copy.copy(wave)\n    _bound_waveform(ret, indep_min, indep_max)\n    return np.min(ret._dep_vector)", "response": "r Return the minimum of a waveform s dependent variable vector."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef phase(wave, unwrap=True, rad=True):\n    ret = copy.copy(wave)\n    ret.dep_units = \"rad\" if rad else \"deg\"\n    ret.dep_name = \"phase({0})\".format(ret.dep_name)\n    ret._dep_vector = (\n        np.unwrap(np.angle(ret._dep_vector)) if unwrap else np.angle(ret._dep_vector)\n    )\n    if not rad:\n        ret._dep_vector = np.rad2deg(ret._dep_vector)\n    return ret", "response": "r Return the phase of a waveform."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sqrt(wave):\n    dep_units = \"{0}**0.5\".format(wave.dep_units)\n    return _operation(wave, \"sqrt\", dep_units, np.sqrt)", "response": "r Returns the square root of a waveform s dependent variable vector."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef subwave(wave, dep_name=None, indep_min=None, indep_max=None, indep_step=None):\n    ret = copy.copy(wave)\n    if dep_name is not None:\n        ret.dep_name = dep_name\n    _bound_waveform(ret, indep_min, indep_max)\n    pexdoc.addai(\"indep_step\", bool((indep_step is not None) and (indep_step <= 0)))\n    exmsg = \"Argument `indep_step` is greater than independent vector range\"\n    cond = bool(\n        (indep_step is not None)\n        and (indep_step > ret._indep_vector[-1] - ret._indep_vector[0])\n    )\n    pexdoc.addex(RuntimeError, exmsg, cond)\n    if indep_step:\n        indep_vector = _barange(indep_min, indep_max, indep_step)\n        dep_vector = _interp_dep_vector(ret, indep_vector)\n        ret._set_indep_vector(indep_vector, check=False)\n        ret._set_dep_vector(dep_vector, check=False)\n    return ret", "response": "r Returns a waveform that is a sub - set of a waveform."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef wcomplex(wave):\n    ret = copy.copy(wave)\n    ret._dep_vector = ret._dep_vector.astype(np.complex)\n    return ret", "response": "r Converts a waveform s dependent variable vector to complex."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef wfloat(wave):\n    pexdoc.exh.addex(\n        TypeError,\n        \"Cannot convert complex to float\",\n        wave._dep_vector.dtype.name.startswith(\"complex\"),\n    )\n    ret = copy.copy(wave)\n    ret._dep_vector = ret._dep_vector.astype(np.float)\n    return ret", "response": "r Convert a waveform s dependent variable vector to float."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef wvalue(wave, indep_var):\n    close_min = np.isclose(indep_var, wave._indep_vector[0], FP_RTOL, FP_ATOL)\n    close_max = np.isclose(indep_var, wave._indep_vector[-1], FP_RTOL, FP_ATOL)\n    pexdoc.exh.addex(\n        ValueError,\n        \"Argument `indep_var` is not in the independent variable vector range\",\n        bool(\n            ((indep_var < wave._indep_vector[0]) and (not close_min))\n            or ((indep_var > wave._indep_vector[-1]) and (not close_max))\n        ),\n    )\n    if close_min:\n        return wave._dep_vector[0]\n    if close_max:\n        return wave._dep_vector[-1]\n    idx = np.searchsorted(wave._indep_vector, indep_var)\n    xdelta = wave._indep_vector[idx] - wave._indep_vector[idx - 1]\n    ydelta = wave._dep_vector[idx] - wave._dep_vector[idx - 1]\n    slope = ydelta / float(xdelta)\n    return wave._dep_vector[idx - 1] + slope * (indep_var - wave._indep_vector[idx - 1])", "response": "r Return the dependent variable value at a given independent variable point."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind all the resources in the path.", "response": "def find(self, path, all=False):\n        \"\"\"\n        Only allow lookups for jspm_packages.\n\n        # TODO: figure out the 'jspm_packages' dir from packag.json.\n        \"\"\"\n        bits = path.split('/')\n        dirs_to_serve = ['jspm_packages', settings.SYSTEMJS_OUTPUT_DIR]\n        if not bits or bits[0] not in dirs_to_serve:\n            return []\n        return super(SystemFinder, self).find(path, all=all)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_short_desc(long_desc):\n    found = False\n    olines = []\n    for line in [item.rstrip() for item in long_desc.split(\"\\n\")]:\n        if found and (((not line) and (not olines)) or (line and olines)):\n            olines.append(line)\n        elif found and olines and (not line):\n            return (\" \".join(olines).split(\".\")[0]).strip()\n        found = line == \".. [[[end]]]\" if not found else found\n    return \"\"", "response": "Get first sentence of first paragraph of long description."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind the apps that are defined in the templates.", "response": "def find_apps(self, templates=None):\n        \"\"\"\n        Crawls the (specified) template files and extracts the apps.\n\n        If `templates` is specified, the template loader is used and the template\n        is tokenized to extract the SystemImportNode. An empty context is used\n        to resolve the node variables.\n        \"\"\"\n\n        all_apps = OrderedDict()\n\n        if not templates:\n            all_files = self.discover_templates()\n            for tpl_name, fp in all_files:\n                # this is the most performant - a testcase that used the loader with tpl_name\n                # was about 8x slower for a project with ~5 apps in different templates :(\n                with io.open(fp, 'r', encoding=settings.FILE_CHARSET) as template_file:\n                    src_data = template_file.read()\n\n                for t in Lexer(src_data).tokenize():\n                    if t.token_type == TOKEN_BLOCK:\n                        imatch = SYSTEMJS_TAG_RE.match(t.contents)\n                        if imatch:\n                            all_apps.setdefault(tpl_name, [])\n                            all_apps[tpl_name].append(imatch.group('app'))\n        else:\n            for tpl_name in templates:\n                try:\n                    template = loader.get_template(tpl_name)\n                except TemplateDoesNotExist:\n                    raise CommandError('Template \\'%s\\' does not exist' % tpl_name)\n                import_nodes = template.template.nodelist.get_nodes_by_type(SystemImportNode)\n                for node in import_nodes:\n                    app = node.path.resolve(RESOLVE_CONTEXT)\n                    if not app:\n                        self.stdout.write(self.style.WARNING(\n                            '{tpl}: Could not resolve path with context {ctx}, skipping.'.format(\n                                tpl=tpl_name, ctx=RESOLVE_CONTEXT)\n                        ))\n                        continue\n                    all_apps.setdefault(tpl_name, [])\n                    all_apps[tpl_name].append(app)\n\n        return all_apps"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrender the js file for the current locale.", "response": "def render(self, context):\n        \"\"\"\n        Build the filepath by appending the extension.\n        \"\"\"\n        module_path = self.path.resolve(context)\n        if not settings.SYSTEMJS_ENABLED:\n            if settings.SYSTEMJS_DEFAULT_JS_EXTENSIONS:\n                name, ext = posixpath.splitext(module_path)\n                if not ext:\n                    module_path = '{}.js'.format(module_path)\n\n            if settings.SYSTEMJS_SERVER_URL:\n                tpl = \"\"\"<script src=\"{url}{app}\" type=\"text/javascript\"></script>\"\"\"\n            else:\n                tpl = \"\"\"<script type=\"text/javascript\">System.import('{app}');</script>\"\"\"\n            return tpl.format(app=module_path, url=settings.SYSTEMJS_SERVER_URL)\n\n        # else: create a bundle\n        rel_path = System.get_bundle_path(module_path)\n        url = staticfiles_storage.url(rel_path)\n\n        tag_attrs = {'type': 'text/javascript'}\n        for key, value in self.tag_attrs.items():\n            if not isinstance(value, bool):\n                value = value.resolve(context)\n            tag_attrs[key] = value\n\n        return \"\"\"<script{attrs} src=\"{url}\"></script>\"\"\".format(\n            url=url, attrs=flatatt(tag_attrs)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _build_expr(tokens, higher_oplevel=-1, ldelim=\"(\", rdelim=\")\"):\n    # Numbers\n    if isinstance(tokens, str):\n        return tokens\n    # Unary operators\n    if len(tokens) == 2:\n        return \"\".join(tokens)\n    # Multi-term operators\n    oplevel = _get_op_level(tokens[1])\n    stoken = \"\"\n    for num, item in enumerate(tokens):\n        if num % 2 == 0:\n            stoken += _build_expr(item, oplevel, ldelim=ldelim, rdelim=rdelim)\n        else:\n            stoken += item\n    if (oplevel < higher_oplevel) or (\n        (oplevel == higher_oplevel) and (oplevel in _OP_PREC_PAR)\n    ):\n        stoken = ldelim + stoken + rdelim\n    return stoken", "response": "Build mathematical expression from hierarchical list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _next_rdelim(items, pos):\n    for num, item in enumerate(items):\n        if item > pos:\n            break\n    else:\n        raise RuntimeError(\"Mismatched delimiters\")\n    del items[num]\n    return item", "response": "Return position of next matching closing delimiter."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse mathematical expression using PyParsing.", "response": "def _parse_expr(text, ldelim=\"(\", rdelim=\")\"):\n    \"\"\"Parse mathematical expression using PyParsing.\"\"\"\n    var = pyparsing.Word(pyparsing.alphas + \"_\", pyparsing.alphanums + \"_\")\n    point = pyparsing.Literal(\".\")\n    exp = pyparsing.CaselessLiteral(\"E\")\n    number = pyparsing.Combine(\n        pyparsing.Word(\"+-\" + pyparsing.nums, pyparsing.nums)\n        + pyparsing.Optional(point + pyparsing.Optional(pyparsing.Word(pyparsing.nums)))\n        + pyparsing.Optional(\n            exp + pyparsing.Word(\"+-\" + pyparsing.nums, pyparsing.nums)\n        )\n    )\n    atom = var | number\n    oplist = [\n        (pyparsing.Literal(\"**\"), 2, pyparsing.opAssoc.RIGHT),\n        (pyparsing.oneOf(\"+ - ~\"), 1, pyparsing.opAssoc.RIGHT),\n        (pyparsing.oneOf(\"* / // %\"), 2, pyparsing.opAssoc.LEFT),\n        (pyparsing.oneOf(\"+ -\"), 2, pyparsing.opAssoc.LEFT),\n        (pyparsing.oneOf(\"<< >>\"), 2, pyparsing.opAssoc.LEFT),\n        (pyparsing.Literal(\"&\"), 2, pyparsing.opAssoc.LEFT),\n        (pyparsing.Literal(\"^\"), 2, pyparsing.opAssoc.LEFT),\n        (pyparsing.Literal(\"|\"), 2, pyparsing.opAssoc.LEFT),\n    ]\n    # Get functions\n    expr = pyparsing.infixNotation(\n        atom, oplist, lpar=pyparsing.Suppress(ldelim), rpar=pyparsing.Suppress(rdelim)\n    )\n    return expr.parseString(text)[0]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove consecutive delimiters from an expression.", "response": "def _remove_consecutive_delims(expr, ldelim=\"(\", rdelim=\")\"):\n    \"\"\"Remove consecutive delimiters.\"\"\"\n    tpars = _pair_delims(expr, ldelim=ldelim, rdelim=rdelim)\n    # Flag superfluous delimiters\n    ddelim = []\n    for ctuple, ntuple in zip(tpars, tpars[1:]):\n        if ctuple == (ntuple[0] - 1, ntuple[1] + 1):\n            ddelim.extend(ntuple)\n    ddelim.sort()\n    # Actually remove delimiters from expression\n    for num, item in enumerate(ddelim):\n        expr = expr[: item - num] + expr[item - num + 1 :]\n    # Get functions\n    return expr"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _remove_extra_delims(expr, ldelim=\"(\", rdelim=\")\", fcount=None):\n    if not expr.strip():\n        return \"\"\n    fcount = [0] if fcount is None else fcount\n    tfuncs = _get_functions(expr, ldelim=ldelim, rdelim=rdelim)\n    # Replace function call with tokens\n    for fdict in reversed(tfuncs):\n        fcount[0] += 1\n        fdict[\"token\"] = \"__\" + str(fcount[0])\n        expr = expr[: fdict[\"start\"]] + fdict[\"token\"] + expr[fdict[\"stop\"] + 1 :]\n        fdict[\"expr\"] = _remove_extra_delims(\n            fdict[\"expr\"], ldelim=ldelim, rdelim=rdelim, fcount=fcount\n        )\n    # Parse expression with function calls removed\n    expr = _build_expr(\n        _parse_expr(expr, ldelim=ldelim, rdelim=rdelim), ldelim=ldelim, rdelim=rdelim\n    )\n    # Insert cleaned-up function calls\n    for fdict in tfuncs:\n        expr = expr.replace(\n            fdict[\"token\"], fdict[\"fname\"] + ldelim + fdict[\"expr\"] + rdelim\n        )\n    return expr", "response": "Remove unnecessary delimiters from an expression."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsplits the string into count words.", "response": "def _split_every(text, sep, count, lstrip=False, rstrip=False):\n    \"\"\"\n    Return list of the words in the string, using count of a separator as delimiter.\n\n    :param text: String to split\n    :type  text: string\n\n    :param sep: Separator\n    :type  sep: string\n\n    :param count: Number of separators to use as delimiter\n    :type  count: integer\n\n    :param lstrip: Flag that indicates whether whitespace is removed\n                   from the beginning of each list item (True) or not\n                   (False)\n    :type  lstrip: boolean\n\n    :param rstrip: Flag that indicates whether whitespace is removed\n                   from the end of each list item (True) or not (False)\n    :type  rstrip: boolean\n\n    :rtype: tuple\n    \"\"\"\n    ltr = \"_rl \"[2 * lstrip + rstrip].strip()\n    func = lambda x: getattr(x, ltr + \"strip\")() if ltr != \"_\" else x\n    items = text.split(sep)\n    groups = zip_longest(*[iter(items)] * count, fillvalue=\"\")\n    joints = (sep.join(group).rstrip(sep) for group in groups)\n    return tuple(func(joint) for joint in joints)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a number to engineering notation.", "response": "def _to_eng_tuple(number):\n    \"\"\"\n    Return tuple with mantissa and exponent of number formatted in engineering notation.\n\n    :param number: Number\n    :type  number: integer or float\n\n    :rtype: tuple\n    \"\"\"\n    # pylint: disable=W0141\n    # Helper function: split integer and fractional part of mantissa\n    #  + ljust ensures that integer part in engineering notation has\n    #    at most 3 digits (say if number given is 1E4)\n    #  + rstrip ensures that there is no empty fractional part\n    split = lambda x, p: (x.ljust(3 + neg, \"0\")[:p], x[p:].rstrip(\"0\"))\n    # Convert number to scientific notation, a \"constant\" format\n    mant, exp = to_scientific_tuple(number)\n    mant, neg = mant.replace(\".\", \"\"), mant.startswith(\"-\")\n    # New values\n    new_mant = \".\".join(filter(None, split(mant, 1 + (exp % 3) + neg)))\n    new_exp = int(3 * math.floor(exp / 3))\n    return NumComp(new_mant, new_exp)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef peng(number, frac_length, rjust=True):\n    # The decimal module has a to_eng_string() function, but it does not seem\n    # to work well in all cases. For example:\n    # >>> decimal.Decimal('34.5712233E8').to_eng_string()\n    # '3.45712233E+9'\n    # >>> decimal.Decimal('34.57122334E8').to_eng_string()\n    # '3457122334'\n    # It seems that the conversion function does not work in all cases\n    #\n    # Return formatted zero if number is zero, easier to not deal with this\n    # special case through the rest of the algorithm\n    if number == 0:\n        number = \"0.{zrs}\".format(zrs=\"0\" * frac_length) if frac_length else \"0\"\n        # Engineering notation numbers can have a sign, a 3-digit integer part,\n        # a period, and a fractional part of length frac_length, so the\n        # length of the number to the left of, and including, the period is 5\n        return \"{0} \".format(number.rjust(5 + frac_length)) if rjust else number\n    # Low-bound number\n    sign = +1 if number >= 0 else -1\n    ssign = \"-\" if sign == -1 else \"\"\n    anumber = abs(number)\n    if anumber < 1e-24:\n        anumber = 1e-24\n        number = sign * 1e-24\n    # Round fractional part if requested frac_length is less than length\n    # of fractional part. Rounding method is to add a '5' at the decimal\n    # position just after the end of frac_length digits\n    exp = 3.0 * math.floor(math.floor(math.log10(anumber)) / 3.0)\n    mant = number / 10 ** exp\n    # Because exponent is a float, mantissa is a float and its string\n    # representation always includes a period\n    smant = str(mant)\n    ppos = smant.find(\".\")\n    if len(smant) - ppos - 1 > frac_length:\n        mant += sign * 5 * 10 ** (-frac_length - 1)\n        if abs(mant) >= 1000:\n            exp += 3\n            mant = mant / 1e3\n        smant = str(mant)\n        ppos = smant.find(\".\")\n    # Make fractional part have frac_length digits\n    bfrac_length = bool(frac_length)\n    flength = ppos - (not bfrac_length) + frac_length + 1\n    new_mant = smant[:flength].ljust(flength, \"0\")\n    # Upper-bound number\n    if exp > 24:\n        new_mant, exp = (\n            \"{sign}999.{frac}\".format(sign=ssign, frac=\"9\" * frac_length),\n            24,\n        )\n    # Right-justify number, engineering notation numbers can have a sign,\n    # a 3-digit integer part and a period, and a fractional part of length\n    # frac_length, so the length of the number to the left of the\n    # period is 4\n    new_mant = new_mant.rjust(rjust * (4 + bfrac_length + frac_length))\n    # Format number\n    num = \"{mant}{suffix}\".format(\n        mant=new_mant, suffix=_POWER_TO_SUFFIX_DICT[exp] if exp else \" \" * bool(rjust)\n    )\n    return num", "response": "r Convert a number to engineering notation."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef peng_frac(snum):\n    snum = snum.rstrip()\n    pindex = snum.find(\".\")\n    if pindex == -1:\n        return 0\n    return int(snum[pindex + 1 :] if snum[-1].isdigit() else snum[pindex + 1 : -1])", "response": "r Returns the fractional part of a number represented in engineering notation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrounding the mantissa of the base class to given number of digits.", "response": "def round_mantissa(arg, decimals=0):\n    \"\"\"\n    Round floating point number(s) mantissa to given number of digits.\n\n    Integers are not altered. The mantissa used is that of the floating point\n    number(s) when expressed in `normalized scientific notation\n    <https://en.wikipedia.org/wiki/Scientific_notation#Normalized_notation>`_\n\n    :param arg: Input data\n    :type  arg: integer, float, Numpy vector of integers or floats, or None\n\n    :param decimals: Number of digits to round the fractional part of the\n                     mantissa to.\n    :type  decimals: integer\n\n    :rtype: same as **arg**\n\n    For example::\n\n        >>> import peng\n        >>> peng.round_mantissa(012345678E-6, 3)\n        12.35\n        >>> peng.round_mantissa(5, 3)\n        5\n    \"\"\"\n    if arg is None:\n        return arg\n    if isinstance(arg, np.ndarray):\n        foi = [isinstance(item, int) for item in arg]\n        return np.array(\n            [\n                item if isint else float(to_scientific_string(item, decimals))\n                for isint, item in zip(foi, arg)\n            ]\n        )\n    if isinstance(arg, int):\n        return arg\n    return float(to_scientific_string(arg, decimals))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_scientific_string(number, frac_length=None, exp_length=None, sign_always=False):\n    # pylint: disable=W0702\n    try:\n        number = -1e20 if np.isneginf(number) else number\n    except:\n        pass\n    try:\n        number = +1e20 if np.isposinf(number) else number\n    except:\n        pass\n    exp_length = 0 if not exp_length else exp_length\n    mant, exp = to_scientific_tuple(number)\n    fmant = float(mant)\n    if (not frac_length) or (fmant == int(fmant)):\n        return \"{sign}{mant}{period}{zeros}E{exp_sign}{exp}\".format(\n            sign=\"+\" if sign_always and (fmant >= 0) else \"\",\n            mant=mant,\n            period=\".\" if frac_length else \"\",\n            zeros=\"0\" * frac_length if frac_length else \"\",\n            exp_sign=\"-\" if exp < 0 else \"+\",\n            exp=str(abs(exp)).rjust(exp_length, \"0\"),\n        )\n    rounded_mant = round(fmant, frac_length)\n    # Avoid infinite recursion when rounded mantissa is _exactly_ 10\n    if abs(rounded_mant) == 10:\n        rounded_mant = fmant = -1.0 if number < 0 else 1.0\n        frac_length = 1\n        exp = exp + 1\n    zeros = 2 + (1 if (fmant < 0) else 0) + frac_length - len(str(rounded_mant))\n    return \"{sign}{mant}{zeros}E{exp_sign}{exp}\".format(\n        sign=\"+\" if sign_always and (fmant >= 0) else \"\",\n        mant=rounded_mant,\n        zeros=\"0\" * zeros,\n        exp_sign=\"-\" if exp < 0 else \"+\",\n        exp=str(abs(exp)).rjust(exp_length, \"0\"),\n    )", "response": "Convert a number or number string to a scientific string in the scientific notation."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a number into a scientific notation.", "response": "def to_scientific_tuple(number):\n    \"\"\"\n    Return mantissa and exponent of a number in scientific notation.\n\n    Full precision is maintained if the number is represented as a string\n\n    :param number: Number\n    :type  number: integer, float or string\n\n    :rtype: named tuple in which the first item is the mantissa (*string*)\n            and the second item is the exponent (*integer*) of the number\n            when expressed in scientific notation\n\n    For example:\n\n        >>> import peng\n        >>> peng.to_scientific_tuple('135.56E-8')\n        NumComp(mant='1.3556', exp=-6)\n        >>> peng.to_scientific_tuple(0.0000013556)\n        NumComp(mant='1.3556', exp=-6)\n    \"\"\"\n    # pylint: disable=W0632\n    convert = not isinstance(number, str)\n    # Detect zero and return, simplifies subsequent algorithm\n    if (convert and (number == 0)) or (\n        (not convert) and (not number.strip(\"0\").strip(\".\"))\n    ):\n        return (\"0\", 0)\n    # Break down number into its components, use Decimal type to\n    # preserve resolution:\n    # sign  : 0 -> +, 1 -> -\n    # digits: tuple with digits of number\n    # exp   : exponent that gives null fractional part\n    sign, digits, exp = Decimal(str(number) if convert else number).as_tuple()\n    mant = (\n        \"{sign}{itg}{frac}\".format(\n            sign=\"-\" if sign else \"\",\n            itg=digits[0],\n            frac=(\n                \".{frac}\".format(frac=\"\".join([str(num) for num in digits[1:]]))\n                if len(digits) > 1\n                else \"\"\n            ),\n        )\n        .rstrip(\"0\")\n        .rstrip(\".\")\n    )\n    exp += len(digits) - 1\n    return NumComp(mant, exp)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_sourcemap_comment(filepath, block_size=100):\n\n    MAX_TRACKBACK = 2  # look back at most 2 lines, catching potential blank line at the end\n\n    block_number = -1\n    # blocks of size block_size, in reverse order starting from the end of the file\n    blocks = []\n    sourcemap = None\n\n    try:\n        # open file in binary read+write mode, so we can seek with negative offsets\n        of = io.open(filepath, 'br+')\n        # figure out what's the end byte\n        of.seek(0, os.SEEK_END)\n        block_end_byte = of.tell()\n\n        # track back for maximum MAX_TRACKBACK lines and while we can track back\n        while block_end_byte > 0 and MAX_TRACKBACK > 0:\n            if (block_end_byte - block_size > 0):\n                # read the last block we haven't yet read\n                of.seek(block_number*block_size, os.SEEK_END)\n                blocks.append(of.read(block_size))\n            else:\n                # file too small, start from begining\n                of.seek(0, os.SEEK_SET)\n                # only read what was not read\n                blocks = [of.read(block_end_byte)]\n\n            # update variables that control while loop\n            content = b''.join(reversed(blocks))\n            lines_found = content.count(b'\\n')\n            MAX_TRACKBACK -= lines_found\n            block_end_byte -= block_size\n            block_number -= 1\n\n            # early check and bail out if we found the sourcemap comment\n            if SOURCEMAPPING_URL_COMMENT in content:\n                offset = 0\n                # splitlines eats the last \\n if its followed by a blank line\n                lines = content.split(b'\\n')\n                for i, line in enumerate(lines):\n                    if line.startswith(SOURCEMAPPING_URL_COMMENT):\n                        offset = len(line)\n                        sourcemap = line\n                        break\n                while i+1 < len(lines):\n                    offset += 1  # for the newline char\n                    offset += len(lines[i+1])\n                    i += 1\n                # track back until the start of the comment, and truncate the comment\n                if sourcemap:\n                    offset += 1  # for the newline before the sourcemap comment\n                    of.seek(-offset, os.SEEK_END)\n                    of.truncate()\n                return force_text(sourcemap)\n    finally:\n        of.close()\n    return sourcemap", "response": "This function finds the sourcemap comment in a bundle file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_paths(self):\n        outfile = self.get_outfile()\n        rel_path = os.path.relpath(outfile, settings.STATIC_ROOT)\n        return outfile, rel_path", "response": "Return a tuple with the absolute path and relative path"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef needs_ext(self):\n        if settings.SYSTEMJS_DEFAULT_JS_EXTENSIONS:\n            name, ext = posixpath.splitext(self.app)\n            if not ext:\n                return True\n        return False", "response": "Check whether the application needs a. js extension."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbundling the app and return the static url to the bundle.", "response": "def bundle(self):\n        \"\"\"\n        Bundle the app and return the static url to the bundle.\n        \"\"\"\n        outfile, rel_path = self.get_paths()\n\n        options = self.opts\n        if self.system._has_jspm_log():\n            self.command += ' --log {log}'\n            options.setdefault('log', 'err')\n\n        if options.get('minify'):\n            self.command += ' --minify'\n\n        if options.get('skip_source_maps'):\n            self.command += ' --skip-source-maps'\n\n        try:\n            cmd = self.command.format(app=self.app, outfile=outfile, **options)\n            proc = subprocess.Popen(\n                cmd, shell=True, cwd=self.system.cwd, stdout=self.stdout,\n                stdin=self.stdin, stderr=self.stderr)\n\n            result, err = proc.communicate()  # block until it's done\n            if err and self.system._has_jspm_log():\n                fmt = 'Could not bundle \\'%s\\': \\n%s'\n                logger.warn(fmt, self.app, err)\n                raise BundleError(fmt % (self.app, err))\n            if result.strip():\n                logger.info(result)\n        except (IOError, OSError) as e:\n            if isinstance(e, BundleError):\n                raise\n            raise BundleError('Unable to apply %s (%r): %s' % (\n                              self.__class__.__name__, cmd, e))\n        else:\n            if not options.get('sfx'):\n                # add the import statement, which is missing for non-sfx bundles\n                sourcemap = find_sourcemap_comment(outfile)\n                with open(outfile, 'a') as of:\n                    of.write(\"\\nSystem.import('{app}{ext}');\\n{sourcemap}\".format(\n                        app=self.app,\n                        ext='.js' if self.needs_ext() else '',\n                        sourcemap=sourcemap if sourcemap else '',\n                    ))\n        return rel_path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntracing the dependencies for app.", "response": "def trace(self, app):\n        \"\"\"\n        Trace the dependencies for app.\n\n        A tracer-instance is shortlived, and re-tracing the same app should\n        yield the same results. Since tracing is an expensive process, cache\n        the result on the tracer instance.\n        \"\"\"\n        if app not in self._trace_cache:\n            process = subprocess.Popen(\n                \"trace-deps.js {}\".format(app), shell=True,\n                stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n                env=self.env, universal_newlines=True, cwd=self._package_json_dir\n            )\n            out, err = process.communicate()\n            if err:\n                raise TraceError(err)\n            self._trace_cache[app] = json.loads(out)\n        return self._trace_cache[app]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef hashes_match(self, dep_tree):\n        hashes = self.get_hashes()\n        for module, info in dep_tree.items():\n            md5 = self.get_hash(info['path'])\n            if md5 != hashes[info['path']]:\n                return False\n        return True", "response": "Returns True if the hashes of the app deptree file matches the hashes stored in the app deptree cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts the bytes object to a hexdump.", "response": "def format_hexdump(arg):\n    \"\"\"Convert the bytes object to a hexdump.\n\n    The output format will be:\n\n    <offset, 4-byte>  <16-bytes of output separated by 1 space>  <16 ascii characters>\n    \"\"\"\n\n    line = ''\n\n    for i in range(0, len(arg), 16):\n        if i > 0:\n            line += '\\n'\n        chunk = arg[i:i + 16]\n        hex_chunk = hexlify(chunk).decode('utf-8')\n        hex_line = ' '.join(hex_chunk[j:j + 2] for j in range(0, len(hex_chunk), 2))\n\n        if len(hex_line) < (3 * 16) - 1:\n            hex_line += ' ' * (((3 * 16) - 1) - len(hex_line))\n\n        ascii_line = ''.join(_convert_to_ascii(x) for x in chunk)\n        offset_line = '%08x' % i\n\n        line += \"%s  %s  %s\" % (offset_line, hex_line, ascii_line)\n\n    return line"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_docstring(doc):\n\n    doc = inspect.cleandoc(doc)\n    lines = doc.split('\\n')\n    section = None\n    section_indent = None\n\n    params = {}\n    returns = None\n\n    for line in lines:\n        line = line.rstrip()\n\n        if len(line) == 0:\n            continue\n        elif str(line) == 'Args:':\n            section = 'args'\n            section_indent = None\n            continue\n        elif str(line) == 'Returns:':\n            section = 'return'\n            section_indent = None\n            continue\n\n        if section is not None:\n            stripped = line.lstrip()\n            margin = len(line) - len(stripped)\n\n            if section_indent is None:\n                section_indent = margin\n\n            if margin != section_indent:\n                continue\n\n            # These are all the param lines in the docstring that are\n            # not continuations of the previous line\n            if section == 'args':\n                param_name, type_info = parse_param(stripped)\n                params[param_name] = type_info\n            elif section == 'return':\n                returns = parse_return(stripped)\n\n    return params, returns", "response": "Parse a docstring into ParameterInfo and ReturnInfo objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef valid_identifiers(self):\n\n        funcs = list(utils.find_all(self.contexts[-1])) + list(self.builtins)\n        return funcs", "response": "Get a list of all valid identifiers for the current context."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _split_line(self, line):\n\n        parts = shlex.split(line, posix=self.posix_lex)\n        if not self.posix_lex:\n            parts = [self._remove_quotes(x) for x in parts]\n\n        return parts", "response": "Split a line into arguments using shlex and a dequoting routine."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _check_initialize_context(self):\n\n        path = \".\".join([annotate.context_name(x) for x in self.contexts])\n\n        #Make sure we don't clutter up the output with return values from\n        #initialization functions\n        old_interactive = type_system.interactive\n        type_system.interactive = False\n\n        for key, cmds in self.init_commands.items():\n            if path.endswith(key):\n                for cmd in cmds:\n                    line = self._split_line(cmd)\n                    self.invoke(line)\n\n        type_system.interactive = old_interactive", "response": "Check if our context matches something that we have initialization commands\n        for."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _builtin_help(self, args):\n\n        if len(args) == 0:\n            return self.list_dir(self.contexts[-1])\n        if len(args) == 1:\n            func = self.find_function(self.contexts[-1], args[0])\n            return annotate.get_help(func)\n\n        help_text = \"Too many arguments: \" + str(args) + \"\\n\"\n        help_text += \"Usage: help [function]\"\n        return help_text", "response": "Return help information for a context or function."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds a function in the given context by name.", "response": "def find_function(self, context, funname):\n        \"\"\"Find a function in the given context by name.\n\n        This function will first search the list of builtins and if the\n        desired function is not a builtin, it will continue to search\n        the given context.\n\n        Args:\n            context (object): A dict or class that is a typedargs context\n            funname (str): The name of the function to find\n\n        Returns:\n            callable: The found function.\n        \"\"\"\n\n        if funname in self.builtins:\n            return self.builtins[funname]\n\n        func = None\n        if isinstance(context, dict):\n            if funname in context:\n                func = context[funname]\n\n                #Allowed lazy loading of functions\n                if isinstance(func, str):\n                    func = self._deferred_add(func)\n                    context[funname] = func\n        elif hasattr(context, funname):\n            func = getattr(context, funname)\n\n        if func is None:\n            raise NotFoundError(\"Function not found\", function=funname)\n\n        return func"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a listing of all of the functions in this context including builtins.", "response": "def list_dir(self, context):\n        \"\"\"Return a listing of all of the functions in this context including builtins.\n\n        Args:\n            context (object): The context to print a directory for.\n\n        Returns:\n            str\n        \"\"\"\n\n        doc = inspect.getdoc(context)\n\n        listing = \"\"\n        listing += \"\\n\"\n\n        listing += annotate.context_name(context) + \"\\n\"\n\n        if doc is not None:\n            doc = inspect.cleandoc(doc)\n            listing += doc + \"\\n\"\n\n        listing += \"\\nDefined Functions:\\n\"\n        is_dict = False\n\n        if isinstance(context, dict):\n            funs = context.keys()\n            is_dict = True\n        else:\n            funs = utils.find_all(context)\n\n        for fun in sorted(funs):\n            override_name = None\n            if is_dict:\n                override_name = fun\n\n            fun = self.find_function(context, fun)\n\n            if isinstance(fun, dict):\n                if is_dict:\n                    listing += \" - \" + override_name + '\\n'\n                else:\n                    listing += \" - \" + fun.metadata.name + '\\n'\n            else:\n                listing += \" - \" + fun.metadata.signature(name=override_name) + '\\n'\n\n            if annotate.short_description(fun) != \"\":\n                listing += \"   \" + annotate.short_description(fun) + '\\n'\n\n        listing += \"\\nBuiltin Functions\\n\"\n        for bif in sorted(self.builtins.keys()):\n            listing += ' - ' + bif + '\\n'\n\n        listing += '\\n'\n        return listing"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _is_flag(cls, arg):\n\n        if arg == '--':\n            return False\n\n        if not arg.startswith('-'):\n            return False\n\n        if arg.startswith('--'):\n            first_char = arg[2]\n        else:\n            first_char = arg[1]\n\n        if not first_char.isalpha():\n            return False\n\n        return True", "response": "Check if an argument is a flag."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprocessing arguments from the command line into positional and keyword arguments.", "response": "def process_arguments(self, func, args):\n        \"\"\"Process arguments from the command line into positional and kw args.\n\n        Arguments are consumed until the argument spec for the function is filled\n        or a -- is found or there are no more arguments.  Keyword arguments can be\n        specified using --field=value, -f value or --field value.  Positional\n        arguments are specified just on the command line itself.\n\n        If a keyword argument (`field`) is a boolean, it can be set to True by just passing\n        --field or -f without needing to explicitly pass True unless this would cause\n        ambiguity in parsing since the next expected positional argument is also a boolean\n        or a string.\n\n        Args:\n            func (callable): A function previously annotated with type information\n            args (list): A list of all of the potential arguments to this function.\n\n        Returns:\n            (args, kw_args, unused args): A tuple with a list of args, a dict of\n                keyword args and a list of any unused args that were not processed.\n        \"\"\"\n\n        pos_args = []\n        kw_args = {}\n\n        while len(args) > 0:\n            if func.metadata.spec_filled(pos_args, kw_args) and not self._is_flag(args[0]):\n                break\n\n            arg = args.pop(0)\n\n            if arg == '--':\n                break\n            elif self._is_flag(arg):\n                arg_value = None\n                arg_name = None\n\n                if len(arg) == 2:\n                    arg_name = func.metadata.match_shortname(arg[1:], filled_args=pos_args)\n                else:\n                    if not arg.startswith('--'):\n                        raise ArgumentError(\"Invalid method of specifying keyword argument that did not start with --\", argument=arg)\n\n                    # Skip the --\n                    arg = arg[2:]\n\n                    # Check if the value is embedded in the parameter\n                    # Make sure we allow the value after the equals sign to also\n                    # contain an equals sign.\n                    if '=' in arg:\n                        arg, arg_value = arg.split('=', 1)\n\n                    arg_name = func.metadata.match_shortname(arg, filled_args=pos_args)\n\n                arg_type = func.metadata.param_type(arg_name)\n                if arg_type is None:\n                    raise ArgumentError(\"Attempting to set a parameter from command line that does not have type information\", argument=arg_name)\n\n                # If we don't have a value yet, attempt to get one from the next parameter on the command line\n                if arg_value is None:\n                    arg_value = self._extract_arg_value(arg_name, arg_type, args)\n\n                kw_args[arg_name] = arg_value\n            else:\n                pos_args.append(arg)\n\n        # Always check if there is a trailing '--' and chomp so that we always\n        # start on a function name.  This can happen if there is a gratuitous\n        # -- for a 0 arg function or after an implicit boolean flag like -f --\n        if len(args) > 0 and args[0] == '--':\n            args.pop(0)\n\n        return pos_args, kw_args, args"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntries to find the value for a keyword argument.", "response": "def _extract_arg_value(cls, arg_name, arg_type, remaining):\n        \"\"\"Try to find the value for a keyword argument.\"\"\"\n\n        next_arg = None\n        should_consume = False\n        if len(remaining) > 0:\n            next_arg = remaining[0]\n            should_consume = True\n\n            if next_arg == '--':\n                next_arg = None\n\n        # Generally we just return the next argument, however if the type\n        # is bool we allow not specifying anything to mean true if there\n        # is no ambiguity\n        if arg_type == \"bool\":\n            if next_arg is None or next_arg.startswith('-'):\n                next_arg = True\n                should_consume = False\n        else:\n            if next_arg is None:\n                raise ArgumentError(\"Could not find value for keyword argument\", argument=arg_name)\n\n        if should_consume:\n            remaining.pop(0)\n\n        return next_arg"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef invoke_one(self, line):\n\n        funname = line.pop(0)\n\n        context = self.contexts[-1]\n        func = self.find_function(context, funname)\n\n        #If this is a context derived from a module or package, just jump to it\n        #since there is no initialization function\n        if isinstance(func, dict):\n            self.contexts.append(func)\n            self._check_initialize_context()\n            return None, line, False\n\n        # If the function wants arguments directly, do not parse them, otherwise turn them\n        # into positional and kw arguments\n        if func.takes_cmdline is True:\n            val = func(line)\n            line = []\n        else:\n            posargs, kwargs, line = self.process_arguments(func, line)\n\n            #We need to check for not enough args for classes before calling or the call won't make it all the way to __init__\n            if inspect.isclass(func) and not func.metadata.spec_filled(posargs, kwargs):\n                raise ValidationError(\"Not enough parameters specified to call function\", function=func.metadata.name, signature=func.metadata.signature())\n\n            val = func(*posargs, **kwargs)\n\n        # Update our current context if this function destroyed it or returned a new one.\n        finished = True\n\n        if func.finalizer is True:\n            self.contexts.pop()\n        elif val is not None:\n            if func.metadata.returns_data():\n                val = func.metadata.format_returnvalue(val)\n            else:\n                self.contexts.append(val)\n                self._check_initialize_context()\n                finished = False\n                val = None\n\n        return val, line, finished", "response": "Invoke a function given a list of arguments with the function listed first."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninvoke a one or more functions given a list of arguments.", "response": "def invoke(self, line):\n        \"\"\"Invoke a one or more function given a list of arguments.\n\n        The functions are searched for using the current context on the context stack\n        and its annotated type information is used to convert all of the string parameters\n        passed in line to appropriate python types.\n\n        Args:\n            line (list): The list of command line arguments.\n\n        Returns:\n            bool: A boolean specifying if the last function created a new context\n                (False if a new context was created) and a list with the remainder of the\n                command line if this function did not consume all arguments.)\n        \"\"\"\n\n        finished = True\n\n        while len(line) > 0:\n            val, line, finished = self.invoke_one(line)\n            if val is not None:\n                iprint(val)\n\n        return finished"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef invoke_string(self, line):\n\n        # Make sure line is a unicode string on all python versions\n        line = str(line)\n\n        # Ignore empty lines and comments\n        if len(line) == 0:\n            return True\n\n        if line[0] == u'#':\n            return True\n\n        args = self._split_line(line)\n        return self.invoke(args)", "response": "Parse and invoke a string line."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_param(param, include_desc=False):\n\n    param_def, _colon, desc = param.partition(':')\n    if not include_desc:\n        desc = None\n    else:\n        desc = desc.lstrip()\n\n    if _colon == \"\":\n        raise ValidationError(\"Invalid parameter declaration in docstring, missing colon\", declaration=param)\n\n    param_name, _space, param_type = param_def.partition(' ')\n    if len(param_type) < 2 or param_type[0] != '(' or param_type[-1] != ')':\n        raise ValidationError(\"Invalid parameter type string not enclosed in ( ) characters\", param_string=param_def, type_string=param_type)\n\n    param_type = param_type[1:-1]\n    return param_name, ParameterInfo(param_type, [], desc)", "response": "Parse a single typed parameter statement."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a single return statement declaration.", "response": "def parse_return(return_line, include_desc=False):\n    \"\"\"Parse a single return statement declaration.\n\n    The valid types of return declarion are a Returns: section heading\n    followed a line that looks like:\n    type [format-as formatter]: description\n\n    OR\n\n    type [show-as (string | context)]: description sentence\n    \"\"\"\n\n    ret_def, _colon, desc = return_line.partition(':')\n    if _colon == \"\":\n        raise ValidationError(\"Invalid return declaration in docstring, missing colon\", declaration=ret_def)\n\n    if not include_desc:\n        desc = None\n\n    if 'show-as' in ret_def:\n        ret_type, _showas, show_type = ret_def.partition('show-as')\n        ret_type = ret_type.strip()\n        show_type = show_type.strip()\n\n        if show_type not in ('string', 'context'):\n            raise ValidationError(\"Unkown show-as formatting specifier\", found=show_type, expected=['string', 'context'])\n\n        if show_type == 'string':\n            return ReturnInfo(None, str, True, desc)\n\n        return ReturnInfo(None, None, False, desc)\n\n    if 'format-as' in ret_def:\n        ret_type, _showas, formatter = ret_def.partition('format-as')\n        ret_type = ret_type.strip()\n        formatter = formatter.strip()\n\n        return ReturnInfo(ret_type, formatter, True, desc)\n\n    return ReturnInfo(ret_def, None, True, desc)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nattempt to find the canonical name of this section.", "response": "def _classify_section(cls, section):\n        \"\"\"Attempt to find the canonical name of this section.\"\"\"\n\n        name = section.lower()\n\n        if name in frozenset(['args', 'arguments', \"params\", \"parameters\"]):\n            return cls.ARGS_SECTION\n\n        if name in frozenset(['returns', 'return']):\n            return cls.RETURN_SECTION\n\n        if name in frozenset(['main']):\n            return cls.MAIN_SECTION\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _classify_line(cls, line):\n\n        line = line.rstrip()\n\n        if len(line) == 0:\n            return BlankLine('')\n\n        if ' ' not in line and line.endswith(':'):\n            name = line[:-1]\n            return SectionHeader(name)\n\n        if line.startswith('  '):\n            return ContinuationLine(line.lstrip())\n\n        if line.startswith(' - '):\n            return ListItem('-', line[3:].lstrip())\n\n        if line.startswith('- '):\n            return ListItem('-', line[2:].lstrip())\n\n        return Line(line)", "response": "Classify a line into a type of object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\njoining adjacent lines together into paragraphs using either a blank line or indent as separator.", "response": "def _join_paragraphs(cls, lines, use_indent=False, leading_blanks=False, trailing_blanks=False):\n        \"\"\"Join adjacent lines together into paragraphs using either a blank line or indent as separator.\"\"\"\n\n        curr_para = []\n        paragraphs = []\n\n        for line in lines:\n            if use_indent:\n                if line.startswith(' '):\n                    curr_para.append(line.lstrip())\n                    continue\n                elif line == '':\n                    continue\n                else:\n                    if len(curr_para) > 0:\n                        paragraphs.append(cls._join_paragraph(curr_para, leading_blanks, trailing_blanks))\n\n                    curr_para = [line.lstrip()]\n            else:\n                if len(line) != 0:\n                    curr_para.append(line)\n                else:\n                    paragraphs.append(cls._join_paragraph(curr_para, leading_blanks, trailing_blanks))\n                    curr_para = []\n\n        # Finish the last paragraph if there is one\n        if len(curr_para) > 0:\n            paragraphs.append(cls._join_paragraph(curr_para, leading_blanks, trailing_blanks))\n\n        return paragraphs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef wrap_and_format(self, width=None, include_params=False, include_return=False, excluded_params=None):\n\n        if excluded_params is None:\n            excluded_params = []\n\n        out = StringIO()\n        if width is None:\n            width, _height = get_terminal_size()\n\n        for line in self.maindoc:\n            if isinstance(line, Line):\n                out.write(fill(line.contents, width=width))\n                out.write('\\n')\n            elif isinstance(line, BlankLine):\n                out.write('\\n')\n            elif isinstance(line, ListItem):\n                out.write(fill(line.contents, initial_indent=\" %s \" % line.marker[0], subsequent_indent=\"   \", width=width))\n                out.write('\\n')\n\n        if include_params:\n            included_params = set(self.param_info) - set(excluded_params)\n            if len(included_params) > 0:\n                out.write(\"\\nParameters:\\n\")\n\n                for param in included_params:\n                    info = self.param_info[param]\n                    out.write(\" - %s (%s):\\n\" % (param, info.type_name))\n                    out.write(fill(info.desc, initial_indent=\"   \", subsequent_indent=\"   \", width=width))\n                    out.write('\\n')\n\n        if include_return:\n            print(\"Returns:\")\n            print(\"    \" + self.return_info.type_name)\n            #pylint:disable=fixme; Issue tracked in #32\n            # TODO: Also include description information here\n\n        return out.getvalue()", "response": "Wrap and format the docstring of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert value to type typename", "response": "def convert_to_type(self, value, typename, **kwargs):\n        \"\"\"\n        Convert value to type 'typename'\n\n        If the conversion routine takes various kwargs to\n        modify the conversion process, \\\\**kwargs is passed\n        through to the underlying conversion function\n        \"\"\"\n        try:\n            if isinstance(value, bytearray):\n                return self.convert_from_binary(value, typename, **kwargs)\n\n            typeobj = self.get_type(typename)\n\n            conv = typeobj.convert(value, **kwargs)\n            return conv\n        except (ValueError, TypeError) as exc:\n            raise ValidationError(\"Could not convert value\", type=typename, value=value, error_message=str(exc))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef convert_from_binary(self, binvalue, type, **kwargs):\n\n        size = self.get_type_size(type)\n        if size > 0 and len(binvalue) != size:\n            raise ArgumentError(\"Could not convert type from binary since the data was not the correct size\", required_size=size, actual_size=len(binvalue), type=type)\n\n        typeobj = self.get_type(type)\n\n        if not hasattr(typeobj, 'convert_binary'):\n            raise ArgumentError(\"Type does not support conversion from binary\", type=type)\n\n        return typeobj.convert_binary(binvalue, **kwargs)", "response": "Convert binary data to type type."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_type_size(self, type):\n\n        typeobj = self.get_type(type)\n\n        if hasattr(typeobj, 'size'):\n            return typeobj.size()\n\n        return 0", "response": "Get the size of a type for converting a hex string to the\n        type. Return 0 if the type is not known."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef format_value(self, value, type, format=None, **kwargs):\n\n        typed_val = self.convert_to_type(value, type, **kwargs)\n        typeobj = self.get_type(type)\n\n        #Allow types to specify default formatting functions as 'default_formatter'\n        #otherwise if not format is specified, just convert the value to a string\n        if format is None:\n            if hasattr(typeobj, 'default_formatter'):\n                format_func = getattr(typeobj, 'default_formatter')\n                return format_func(typed_val, **kwargs)\n\n            return str(typed_val)\n\n        formatter = \"format_%s\" % str(format)\n        if not hasattr(typeobj, formatter):\n            raise ArgumentError(\"Unknown format for type\", type=type, format=format, formatter_function=formatter)\n\n        format_func = getattr(typeobj, formatter)\n        return format_func(typed_val, **kwargs)", "response": "Convert value to type and format it as a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nvalidates that all required type methods are implemented.", "response": "def _validate_type(cls, typeobj):\n        \"\"\"\n        Validate that all required type methods are implemented.\n\n        At minimum a type must have:\n        - a convert() or convert_binary() function\n        - a default_formatter() function\n\n        Raises an ArgumentError if the type is not valid\n        \"\"\"\n\n        if not (hasattr(typeobj, \"convert\") or hasattr(typeobj, \"convert_binary\")):\n            raise ArgumentError(\"type is invalid, does not have convert or convert_binary function\", type=typeobj, methods=dir(typeobj))\n\n        if not hasattr(typeobj, \"default_formatter\"):\n            raise ArgumentError(\"type is invalid, does not have default_formatter function\", type=typeobj, methods=dir(typeobj))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_known_type(self, type_name):\n\n        type_name = str(type_name)\n        if type_name in self.known_types:\n            return True\n\n        return False", "response": "Check if the type is a known simple type"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef split_type(self, typename):\n\n        name = self._canonicalize_type(typename)\n        if '(' not in name:\n            return name, False, []\n\n        base, sub = name.split('(')\n        if len(sub) == 0 or sub[-1] != ')':\n            raise ArgumentError(\"syntax error in complex type, no matching ) found\", passed_type=typename, basetype=base, subtype_string=sub)\n\n        sub = sub[:-1]\n\n        subs = sub.split(',')\n        return base, True, subs", "response": "Given a potentially complex type split it into its base type and subtypes and return the base type and subtypes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef instantiate_type(self, typename, base, subtypes):\n\n        if base not in self.type_factories:\n            raise ArgumentError(\"unknown complex base type specified\", passed_type=typename, base_type=base)\n\n        base_type = self.type_factories[base]\n\n        #Make sure all of the subtypes are valid\n        for sub_type in subtypes:\n            try:\n                self.get_type(sub_type)\n            except KeyValueException as exc:\n                raise ArgumentError(\"could not instantiate subtype for complex type\", passed_type=typename, sub_type=sub_type, error=exc)\n\n        typeobj = base_type.Build(*subtypes, type_system=self)\n        self.inject_type(typename, typeobj)", "response": "Instantiate a complex type."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the type object corresponding to a type name.", "response": "def get_type(self, type_name):\n        \"\"\"Return the type object corresponding to a type name.\n\n        If type_name is not found, this triggers the loading of\n        external types until a matching type is found or until there\n        are no more external type sources.\n        \"\"\"\n\n        type_name = self._canonicalize_type(type_name)\n\n        # Add basic transformations on common abbreviations\n        if str(type_name) == 'int':\n            type_name = 'integer'\n        elif str(type_name) == 'str':\n            type_name = 'string'\n        elif str(type_name) == 'dict':\n            type_name = 'basic_dict'\n\n        if self.is_known_type(type_name):\n            return self.known_types[type_name]\n\n        base_type, is_complex, subtypes = self.split_type(type_name)\n        if is_complex and base_type in self.type_factories:\n            self.instantiate_type(type_name, base_type, subtypes)\n            return self.known_types[type_name]\n\n        # If we're here, this is a type that we don't know anything about, so go find it.\n        i = 0\n        for i, (source, name) in enumerate(self._lazy_type_sources):\n            if isinstance(source, str):\n                import pkg_resources\n\n                for entry in pkg_resources.iter_entry_points(source):\n                    try:\n                        mod = entry.load()\n                        type_system.load_type_module(mod)\n                    except:  #pylint:disable=W0702; We want to catch everything here since we don't want external plugins breaking us\n                        fail_info = (\"Entry point group: %s, name: %s\" % (source, entry.name), sys.exc_info)\n                        logging.exception(\"Error loading external type source from entry point, group: %s, name: %s\", source, entry.name)\n                        self.failed_sources.append(fail_info)\n            else:\n                try:\n                    source(self)\n                except:  #pylint:disable=W0702; We want to catch everything here since we don't want external plugins breaking us\n                    fail_info = (\"source: %s\" % name, sys.exc_info)\n                    logging.exception(\"Error loading external type source, source: %s\", source)\n                    self.failed_sources.append(fail_info)\n\n            # Only load as many external sources as we need to resolve this type_name\n            if self.is_known_type(type_name) or (is_complex and base_type in self.type_factories):\n                break\n\n        self._lazy_type_sources = self._lazy_type_sources[i:]\n\n        # If we've loaded everything and we still can't find it then there's a configuration error somewhere\n        if not (self.is_known_type(type_name) or (is_complex and base_type in self.type_factories)):\n            raise ArgumentError(\"get_type called on unknown type\", type=type_name, failed_external_sources=[x[0] for x in self.failed_sources])\n\n        return self.get_type(type_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_known_format(self, type, format):\n\n        typeobj = self.get_type(type)\n\n        formatter = \"format_%s\" % str(format)\n        if not hasattr(typeobj, formatter):\n            return False\n\n        return True", "response": "Check if format is known for given type."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives a module - like object that defines a type add it to our type system so that it can be used with other API functions.", "response": "def inject_type(self, name, typeobj):\n        \"\"\"\n        Given a module-like object that defines a type, add it to our type system so that\n        it can be used with the iotile tool and with other annotated API functions.\n        \"\"\"\n\n        name = self._canonicalize_type(name)\n        _, is_complex, _ = self.split_type(name)\n\n        if self.is_known_type(name):\n            raise ArgumentError(\"attempting to inject a type that is already defined\", type=name)\n\n        if (not is_complex) and self._is_factory(typeobj):\n            if name in self.type_factories:\n                raise ArgumentError(\"attempted to inject a complex type factory that is already defined\", type=name)\n            self.type_factories[name] = typeobj\n        else:\n            self._validate_type(typeobj)\n            self.known_types[name] = typeobj\n\n        if not hasattr(typeobj, \"default_formatter\"):\n            raise ArgumentError(\"type is invalid, does not have default_formatter function\", type=typeobj, methods=dir(typeobj))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_type_module(self, module):\n\n        for name in (x for x in dir(module) if not x.startswith('_')):\n            typeobj = getattr(module, name)\n\n            try:\n                self.inject_type(name, typeobj)\n            except ArgumentError:\n                pass", "response": "Given a module that contains some types find all symbols in that module that do not start with _ and attempt to import them as types."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading external types from a python module or package.", "response": "def load_external_types(self, path):\n        \"\"\"\n        Given a path to a python package or module, load that module, search for all defined variables\n        inside of it that do not start with _ or __ and inject them into the type system.  If any of the\n        types cannot be injected, silently ignore them unless verbose is True.  If path points to a module\n        it should not contain the trailing .py since this is added automatically by the python import system\n        \"\"\"\n\n        folder, filename = os.path.split(path)\n\n        try:\n            fileobj, pathname, description = imp.find_module(filename, [folder])\n            mod = imp.load_module(filename, fileobj, pathname, description)\n        except ImportError as exc:\n            raise ArgumentError(\"could not import module in order to load external types\", module_path=path, parent_directory=folder, module_name=filename, error=str(exc))\n\n        self.load_type_module(mod)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef spec_filled(self, pos_args, kw_args):\n\n        req_names = self.arg_names\n        if len(self.arg_defaults) > 0:\n            req_names = req_names[:-len(self.arg_defaults)]\n\n        req = [x for x in req_names if x not in kw_args]\n        return len(req) <= len(pos_args)", "response": "Check if we have enough arguments to call this function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_param(self, name, type_name, validators, desc=None):\n\n        if name in self.annotated_params:\n            raise TypeSystemError(\"Annotation specified multiple times for the same parameter\", param=name)\n\n        if name not in self.arg_names and name != self.varargs and name != self.kwargs:\n            raise TypeSystemError(\"Annotation specified for unknown parameter\", param=name)\n\n        info = ParameterInfo(type_name, validators, desc)\n        self.annotated_params[name] = info", "response": "Add type information for a parameter by name."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd type information to the return value of this function.", "response": "def typed_returnvalue(self, type_name, formatter=None):\n        \"\"\"Add type information to the return value of this function.\n\n        Args:\n            type_name (str): The name of the type of the return value.\n            formatter (str): An optional name of a formatting function specified\n                for the type given in type_name.\n        \"\"\"\n        self.return_info = ReturnInfo(type_name, formatter, True, None)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nusing a custom function to print the return value.", "response": "def custom_returnvalue(self, printer, desc=None):\n        \"\"\"Use a custom function to print the return value.\n\n        Args:\n            printer (callable): A function that should take in the return\n                value and convert it to a string.\n            desc (str): An optional description of the return value.\n        \"\"\"\n        self.return_info = ReturnInfo(None, printer, True, desc)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef match_shortname(self, name, filled_args=None):\n\n        filled_count = 0\n        if filled_args is not None:\n            filled_count = len(filled_args)\n\n        possible = [x for x in self.arg_names[filled_count:] if x.startswith(name)]\n        if len(possible) == 0:\n            raise ArgumentError(\"Could not convert short-name full parameter name, none could be found\", short_name=name, parameters=self.arg_names)\n        elif len(possible) > 1:\n            raise ArgumentError(\"Short-name is ambiguous, could match multiple keyword parameters\", short_name=name, possible_matches=possible)\n\n        return possible[0]", "response": "Try to convert a prefix into a full matching parameter name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the type information by name.", "response": "def param_type(self, name):\n        \"\"\"Get the parameter type information by name.\n\n        Args:\n            name (str): The full name of a parameter.\n\n        Returns:\n            str: The type name or None if no type information is given.\n        \"\"\"\n\n        self._ensure_loaded()\n\n        if name not in self.annotated_params:\n            return None\n\n        return self.annotated_params[name].type_name"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef signature(self, name=None):\n\n        self._ensure_loaded()\n\n        if name is None:\n            name = self.name\n\n        num_args = len(self.arg_names)\n\n        num_def = 0\n        if self.arg_defaults is not None:\n            num_def = len(self.arg_defaults)\n\n        num_no_def = num_args - num_def\n\n        args = []\n        for i in range(0, len(self.arg_names)):\n            typestr = \"\"\n\n            if self.arg_names[i] in self.annotated_params:\n                typestr = \"{} \".format(self.annotated_params[self.arg_names[i]].type_name)\n\n            if i >= num_no_def:\n                default = str(self.arg_defaults[i-num_no_def])\n                if len(default) == 0:\n                    default = \"''\"\n\n                args.append(\"{}{}={}\".format(typestr, str(self.arg_names[i]), default))\n            else:\n                args.append(typestr + str(self.arg_names[i]))\n\n        return \"{}({})\".format(name, \", \".join(args))", "response": "Return our function signature as a string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef format_returnvalue(self, value):\n\n        self._ensure_loaded()\n\n        if not self.return_info.is_data:\n            return None\n\n        # If the return value is typed, use the type_system to format it\n        if self.return_info.type_name is not None:\n            return typeinfo.type_system.format_value(value, self.return_info.type_name, self.return_info.formatter)\n\n        # Otherwise we expect a callable function to convert this value to a string\n        return self.return_info.formatter(value)", "response": "Format the return value of this function as a string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef convert_positional_argument(self, index, arg_value):\n\n        # For bound methods, skip self\n        if self._has_self:\n            if index == 0:\n                return arg_value\n\n            index -= 1\n\n        arg_name = self.arg_names[index]\n        return self.convert_argument(arg_name, arg_value)", "response": "Convert and validate a positional argument."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_spec(self, pos_args, kwargs=None):\n\n        if kwargs is None:\n            kwargs = {}\n\n        if self.varargs is not None or self.kwargs is not None:\n            raise InternalError(\"check_spec cannot be called on a function that takes *args or **kwargs\")\n\n        missing = object()\n\n        arg_vals = [missing]*len(self.arg_names)\n        kw_indices = {name: i for i, name in enumerate(self.arg_names)}\n\n        for i, arg in enumerate(pos_args):\n            if i >= len(arg_vals):\n                raise ArgumentError(\"Too many positional arguments, first excessive argument=%s\" % str(arg))\n\n            arg_vals[i] = arg\n\n        for arg, val in kwargs.items():\n            index = kw_indices.get(arg)\n            if index is None:\n                raise ArgumentError(\"Cannot find argument by name: %s\" % arg)\n\n            if arg_vals[index] is not missing:\n                raise ValidationError(\"Argument %s passed twice\" % arg)\n\n            arg_vals[index] = val\n\n        # Fill in any default variables if their args are missing\n        if len(self.arg_defaults) > 0:\n            for i in range(0, len(self.arg_defaults)):\n                neg_index = -len(self.arg_defaults) + i\n                if arg_vals[neg_index] is missing:\n                    arg_vals[neg_index] = self.arg_defaults[i]\n\n        # Now make sure there isn't a missing gap\n        if missing in arg_vals:\n            index = arg_vals.index(missing)\n            raise ArgumentError(\"Missing a required argument (position: %d, name: %s)\" % (index, self.arg_names[index]))\n\n        return {name: val for name, val in zip(self.arg_names, arg_vals)}", "response": "Check if there are any missing or duplicate arguments."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef convert_argument(self, arg_name, arg_value):\n\n        self._ensure_loaded()\n\n        type_name = self.param_type(arg_name)\n        if type_name is None:\n            return arg_value\n\n        val = typeinfo.type_system.convert_to_type(arg_value, type_name)\n\n        validators = self.annotated_params[arg_name].validators\n        if len(validators) == 0:\n            return val\n\n        type_obj = typeinfo.type_system.get_type(type_name)\n\n        # Run all of the validators that were defined for this argument.\n        # If the validation fails, they will raise an exception that we convert to\n        # an instance of ValidationError\n        try:\n            for validator_name, extra_args in validators:\n                if not hasattr(type_obj, validator_name):\n                    raise ValidationError(\"Could not find validator specified for argument\", argument=arg_name, validator_name=validator_name, type=str(type_obj), method=dir(type_obj))\n\n                validator = getattr(type_obj, validator_name)\n                validator(val, *extra_args)\n        except (ValueError, TypeError) as exc:\n            raise ValidationError(exc.args[0], argument=arg_name, arg_value=val)\n\n        return val", "response": "Given a parameter with type information convert and validate it."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nformat this exception as a multiline string including class name and key value parameters.", "response": "def format(self, exclude_class=False):\n        \"\"\"Format this exception as a string including class name.\n\n        Args:\n            exclude_class (bool): Whether to exclude the exception class\n                name when formatting this exception\n\n        Returns:\n            string: a multiline string with the message, class name and\n                key value parameters passed to create the exception.\n        \"\"\"\n\n        if exclude_class:\n            msg = self.msg\n        else:\n            msg = \"%s: %s\" % (self.__class__.__name__, self.msg)\n\n        if len(self.params) != 0:\n            paramstring = \"\\n\".join([str(key) + \": \" + str(val) for key, val in self.params.items()])\n            msg += \"\\nAdditional Information:\\n\" + paramstring\n\n        return msg"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_dict(self):\n\n        out = {}\n        out['reason'] = self.msg\n        out['type'] = self.__class__.__name__\n        out['params'] = self.params\n\n        return out", "response": "Convert this exception to a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _check_and_execute(func, *args, **kwargs):\n\n    convargs = []\n\n    #Convert and validate all arguments\n    for i, arg in enumerate(args):\n        val = func.metadata.convert_positional_argument(i, arg)\n        convargs.append(val)\n\n    convkw = {}\n    for key, val in kwargs:\n        convkw[key] = func.metadata.convert_argument(key, val)\n\n    if not func.metadata.spec_filled(convargs, convkw):\n        raise ValidationError(\"Not enough parameters specified to call function\", function=func.metadata.name, signature=func.metadata.signature())\n\n    retval = func(*convargs, **convkw)\n    return retval", "response": "Check the type of all parameters with type information and execute the function."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a list of validators or n - tuples checking for errors.", "response": "def _parse_validators(valids):\n    \"\"\"Parse a list of validator names or n-tuples, checking for errors.\n\n    Returns:\n        list((func_name, [args...])): A list of validator function names and a\n            potentially empty list of optional parameters for each function.\n    \"\"\"\n\n    outvals = []\n\n    for val in valids:\n        if isinstance(val, str):\n            args = []\n        elif len(val) > 1:\n            args = val[1:]\n            val = val[0]\n        else:\n            raise ValidationError(\"You must pass either an n-tuple or a string to define a validator\", validator=val)\n\n        name = \"validate_%s\" % str(val)\n        outvals.append((name, args))\n\n    return outvals"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_all(container):\n\n    if isinstance(container, dict):\n        names = container.keys()\n    else:\n        names = dir(container)\n\n    built_context = BasicContext()\n\n    for name in names:\n        # Ignore _ and __ names\n        if name.startswith('_'):\n            continue\n\n        if isinstance(container, dict):\n            obj = container[name]\n        else:\n            obj = getattr(container, name)\n\n        # Check if this is an annotated object that should be included.  Check the type of\n        # annotated to avoid issues with module imports where someone did from annotate import *\n        # into the module causing an annotated symbol to be defined as a decorator\n\n        # If we are in a dict context then strings point to lazily loaded modules so include them too.\n        if isinstance(container, dict) and isinstance(obj, str):\n            built_context[name] = obj\n        elif hasattr(obj, 'metadata') and isinstance(getattr(obj, 'metadata'), AnnotatedMetadata):\n            built_context[name] = obj\n\n    return built_context", "response": "Find all annotated functions inside of a container."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef context_from_module(module):\n\n    con = find_all(module)\n\n    if hasattr(module, \"__doc__\"):\n        setattr(con, \"__doc__\", module.__doc__)\n\n    name = module.__name__\n    if hasattr(module, \"_name_\"):\n        name = module._name_  # pylint: disable=W0212\n\n    con = annotated(con, name)\n    setattr(con, 'context', True)\n\n    return name, con", "response": "Given a module create a context from all of the top level annotated\n    symbols in that module."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a help text for a context or function.", "response": "def get_help(func):\n    \"\"\"Return usage information about a context or function.\n\n    For contexts, just return the context name and its docstring\n    For functions, return the function signature as well as its\n    argument types.\n\n    Args:\n        func (callable): An annotated callable function\n\n    Returns:\n        str: The formatted help text\n    \"\"\"\n\n    help_text = \"\"\n    if isinstance(func, dict):\n        name = context_name(func)\n\n        help_text = \"\\n\" + name + \"\\n\\n\"\n        doc = inspect.getdoc(func)\n        if doc is not None:\n            doc = inspect.cleandoc(doc)\n            help_text += doc + '\\n'\n\n        return help_text\n\n    sig = func.metadata.signature()\n    doc = inspect.getdoc(func)\n    if doc is not None:\n        doc = inspect.cleandoc(doc)\n\n    help_text += \"\\n\" + sig + \"\\n\\n\"\n    if doc is not None:\n        help_text += doc + '\\n'\n\n    if inspect.isclass(func):\n        func = func.__init__\n\n    # If we derived the parameter annotations from a docstring,\n    # don't insert a custom arguments section since it already\n    # exists.\n    if func.metadata.load_from_doc:\n        return help_text\n\n    help_text += \"\\nArguments:\\n\"\n    for key, info in func.metadata.annotated_params.items():\n        type_name = info.type_name\n        desc = \"\"\n        if info.desc is not None:\n            desc = info.desc\n\n        help_text += \"  - %s (%s): %s\\n\" % (key, type_name, desc)\n\n    return help_text"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndecorates a function to give type information about its parameters.", "response": "def param(name, type_name, *validators, **kwargs):\n    \"\"\"Decorate a function to give type information about its parameters.\n\n    This function stores a type name, optional description and optional list\n    of validation functions along with the decorated function it is called\n    on in order to allow run time type conversions and validation.\n\n    Args:\n        name (string): name of the parameter\n        type_name (string): The name of a type that will be known to the type\n            system by the time this function is called for the first time.  Types\n            are lazily loaded so it is not required that the type resolve correctly\n            at the point in the module where this function is defined.\n        validators (list(string or tuple)): A list of validators.  Each validator\n            can be defined either using a string name or as an n-tuple of the form\n            [name, \\\\*extra_args].  The name is used to look up a validator function\n            of the form validate_name, which is called on the parameters value to\n            determine if it is valid.  If extra_args are given, they are passed\n            as extra arguments to the validator function, which is called as:\n\n            validator(value, \\\\*extra_args)\n        desc (string): An optional descriptioon for this parameter that must be\n            passed as a keyword argument.\n\n    Returns:\n        callable: A decorated function with additional type metadata\n    \"\"\"\n\n    def _param(func):\n        func = annotated(func)\n\n        valids = _parse_validators(validators)\n        func.metadata.add_param(name, type_name, valids, **kwargs)\n\n        # Only decorate the function once even if we have multiple param decorators\n        if func.decorated:\n            return func\n\n        func.decorated = True\n        return decorate(func, _check_and_execute)\n\n    return _param"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nspecifying how the return value of this function should be handled. Args: desc (str): A deprecated description of the return value printer (callable): A callable function that can format this return value data (bool): A deprecated parameter for specifying that this function returns data.", "response": "def returns(desc=None, printer=None, data=True):\n    \"\"\"Specify how the return value of this function should be handled.\n\n    Args:\n        desc (str): A deprecated description of the return value\n        printer (callable): A callable function that can format this return value\n        data (bool): A deprecated parameter for specifying that this function\n            returns data.\n    \"\"\"\n\n    if data is False:\n        raise ArgumentError(\"Specifying non data return type in returns is no longer supported\")\n\n    def _returns(func):\n        annotated(func)\n        func.custom_returnvalue(printer, desc)\n        return func\n\n    return _returns"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nspecifying that this function returns a typed value.", "response": "def return_type(type_name, formatter=None):\n    \"\"\"Specify that this function returns a typed value.\n\n    Args:\n        type_name (str): A type name known to the global typedargs type system\n        formatter (str): An optional name of a formatting function specified\n            for the type given in type_name.\n    \"\"\"\n\n    def _returns(func):\n        annotated(func)\n        func.metadata.typed_returnvalue(type_name, formatter)\n        return func\n\n    return _returns"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeclaring that a class defines a context.", "response": "def context(name=None):\n    \"\"\"Declare that a class defines a context.\n\n    Contexts are for use with HierarchicalShell for discovering\n    and using functionality from the command line.\n\n    Args:\n        name (str): Optional name for this context if you don't want\n            to just use the class name.\n    \"\"\"\n\n    def _context(cls):\n        annotated(cls, name)\n        cls.context = True\n\n        return cls\n\n    return _context"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef docannotate(func):\n\n    func = annotated(func)\n    func.metadata.load_from_doc = True\n\n    if func.decorated:\n        return func\n\n    func.decorated = True\n    return decorate(func, _check_and_execute)", "response": "Annotate a function using information from its docstring."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef annotated(func, name=None):\n\n    if hasattr(func, 'metadata'):\n        if name is not None:\n            func.metadata = AnnotatedMetadata(func, name)\n        return func\n\n    func.metadata = AnnotatedMetadata(func, name)\n\n    func.finalizer = False\n    func.takes_cmdline = False\n    func.decorated = False\n    func.context = False\n\n    return func", "response": "Mark a function as callable from the command line."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef short_description(func):\n\n    doc = inspect.getdoc(func)\n    if doc is not None:\n        doc = inspect.cleandoc(doc)\n        lines = doc.splitlines()\n        return lines[0]\n\n    return \"\"", "response": "Returns the first line of the docstring of the object"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload the cron modules for the current project.", "response": "def load():\n    \"\"\"\n    Load ``cron`` modules for applications listed in ``INSTALLED_APPS``.\n    \"\"\"\n    autodiscover_modules('cron')\n\n    if PROJECT_MODULE:\n        if '.' in PROJECT_MODULE.__name__:\n            try:\n                import_module('%s.cron' % '.'.join(\n                    PROJECT_MODULE.__name__.split('.')[0:-1]))\n            except ImportError as e:\n                if 'No module named' not in str(e):\n                    print(e)\n\n    # load django tasks\n    for cmd, app in get_commands().items():\n        try:\n            load_command_class(app, cmd)\n        except django.core.exceptions.ImproperlyConfigured:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninstalls tasks with cron.", "response": "def install():\n    \"\"\"\n    Register tasks with cron.\n    \"\"\"\n    load()\n    tab = crontab.CronTab(user=True)\n    for task in registry:\n        tab.new(task.command, KRONOS_BREADCRUMB).setall(task.schedule)\n    tab.write()\n    return len(registry)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprinting the tasks that would be installed in the crontab for debugging purposes.", "response": "def printtasks():\n    \"\"\"\n    Print the tasks that would be installed in the\n    crontab, for debugging purposes.\n    \"\"\"\n    load()\n\n    tab = crontab.CronTab('')\n    for task in registry:\n        tab.new(task.command, KRONOS_BREADCRUMB).setall(task.schedule)\n    print(tab.render())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new instance of a project handler based on the uri and local path.", "response": "def create(self, uri, local_path):\n        \"\"\"Create a project handler\n\n        Args:\n            uri (str): schema://something formatted uri\n            local_path (str): the project configs directory\n\n        Return:\n            ProjectHandler derived class instance\n        \"\"\"\n        matches = self.schema_pattern.search(uri)\n\n        if not matches:\n            logger.error(\"Unknown uri schema: '%s'. Added schemas: %s\", uri, list(self.handlers.keys()))\n            return None\n\n        schema = matches.group(1)\n        url = matches.group(2)\n\n        return self.handlers[schema](url, local_path)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads the projects config data from local path", "response": "def load(self):\n        \"\"\"Load the projects config data from local path\n\n        Returns:\n            Dict: project_name -> project_data\n        \"\"\"\n        projects = {}\n\n        path = os.path.expanduser(self.path)\n\n        if not os.path.isdir(path):\n            return projects\n\n        logger.debug(\"Load project configs from %s\", path)\n\n        for filename in os.listdir(path):\n            filename_parts = os.path.splitext(filename)\n\n            if filename_parts[1][1:] != PROJECT_CONFIG_EXTENSION:\n                continue\n            name = filename_parts[0]\n\n            try:\n                project_file_path = os.path.join(path, filename)\n                with open(project_file_path) as f:\n                    data = yaml.load(f)\n                projects[name] = data\n            except ValueError:\n                continue\n\n            logger.debug(\"Project '{}' config readed from {}\".format(name, project_file_path))\n\n        return projects"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsaving the projects configs to local path", "response": "def save(self, projects):\n        \"\"\"Save the projects configs to local path\n\n        Args:\n            projects (dict): project_name -> project_data\n        \"\"\"\n\n        base_path = os.path.expanduser(self.path)\n\n        if not os.path.isdir(base_path):\n            return\n\n        logger.debug(\"Save projects config to %s\", base_path)\n\n        for name, data in list(projects.items()):\n            project_file_path = self.get_project_config_path(name)\n            with open(project_file_path, \"w\") as f:\n                yaml.dump(data, stream = f, default_flow_style = False)\n                logger.debug(\"Project '%s' config has been writed to '%s'\", name, project_file_path)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef define_singleton(carrier, name, cls, cls_args = {}):\n    instance_name = \"__{}\".format(name)\n    setattr(carrier, instance_name, None)\n\n    def getter(self):\n        instance = getattr(carrier, instance_name)\n\n        if instance is None:\n            instance = cls(**cls_args)\n            setattr(carrier, instance_name, instance)\n\n        return instance\n\n    setattr(type(carrier), name, property(getter))", "response": "Define a property with the given name but the cls will create only with the first call\n ArcGIS object"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_dependent_projects(self, recursive = True):\n        projects = {}\n        for name, ref in list(self.dependencies.items()):\n            try:\n                prj = self.vcp.projects[name]\n            except KeyError:\n                logger.error(\"Unknown project '%s' in project '%s' dependencies!\", name, self.name)\n                continue\n            projects[name] = prj\n            if recursive:\n                projects.update(prj.get_dependent_projects())\n        return projects", "response": "Get the dependencies of the Project\n           "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef post_process(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        res = func(*args, **kwargs)\n        project_command = args[0]\n        project_handler = project_command.vcp.project_handler\n        if not project_handler:\n            return res\n        kwargs['command_result'] = res\n        getattr(project_handler, func.__name__)(**kwargs)\n        return res\n\n    return wrapper", "response": "Decorator to call the project handler same named function\n ArcGIS"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntaking an object a key and a value and produces a new object containing the original but with the new value of key.", "response": "def setitem(self, key, value):\n    # type: (Any, Any, Any) -> Any\n    '''Takes an object, a key, and a value and produces a new object\n    that is a copy of the original but with ``value`` as the new value of\n    ``key``.\n\n    The following equality should hold for your definition:\n\n    .. code-block:: python\n\n        setitem(obj, key, obj[key]) == obj\n\n    This function is used by many lenses (particularly GetitemLens) to\n    set items on states even when those states do not ordinarily support\n    ``setitem``. This function is designed to have a similar signature\n    as python's built-in ``setitem`` except that it returns a new object\n    that has the item set rather than mutating the object in place.\n\n    It's what enables the ``lens[some_key]`` functionality.\n\n    The corresponding method call for this hook is\n    ``obj._lens_setitem(key, value)``.\n\n    The default implementation makes a copy of the object using\n    ``copy.copy`` and then mutates the new object by setting the item\n    on it in the conventional way.\n    '''\n    try:\n        self._lens_setitem\n    except AttributeError:\n        selfcopy = copy.copy(self)\n        selfcopy[key] = value\n        return selfcopy\n    else:\n        return self._lens_setitem(key, value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntaking an object a string and a value and produces a new object containing the original object with the attribute called name and value.", "response": "def setattr(self, name, value):\n    # type: (Any, Any, Any) -> Any\n    '''Takes an object, a string, and a value and produces a new object\n    that is a copy of the original but with the attribute called ``name``\n    set to ``value``.\n\n    The following equality should hold for your definition:\n\n    .. code-block:: python\n\n        setattr(obj, 'attr', obj.attr) == obj\n\n    This function is used by many lenses (particularly GetattrLens) to set\n    attributes on states even when those states do not ordinarily support\n    ``setattr``. This function is designed to have a similar signature\n    as python's built-in ``setattr`` except that it returns a new object\n    that has the attribute set rather than mutating the object in place.\n\n    It's what enables the ``lens.some_attribute`` functionality.\n\n    The corresponding method call for this hook is\n    ``obj._lens_setattr(name, value)``.\n\n    The default implementation makes a copy of the object using\n    ``copy.copy`` and then mutates the new object by calling python's\n    built in ``setattr`` on it.\n    '''\n    try:\n        self._lens_setattr\n    except AttributeError:\n        selfcopy = copy.copy(self)\n        builtin_setattr(selfcopy, name, value)\n        return selfcopy\n    else:\n        return self._lens_setattr(name, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntakes a collection and an item and returns a new collection of the same type that contains the item.", "response": "def contains_add(self, item):\n    # type (Any, Any) -> Any\n    '''Takes a collection and an item and returns a new collection\n    of the same type that contains the item. The notion of \"contains\"\n    is defined by the object itself; The following must be ``True``:\n\n    .. code-block:: python\n\n        item in contains_add(obj, item)\n\n    This function is used by some lenses (particularly ContainsLens)\n    to add new items to containers when necessary.\n\n    The corresponding method call for this hook is\n    ``obj._lens_contains_add(item)``.\n\n    There is no default implementation.\n    '''\n    try:\n        self._lens_contains_add\n    except AttributeError:\n        message = 'Don\\'t know how to add an item to {}'\n        raise NotImplementedError(message.format(type(self)))\n    else:\n        return self._lens_contains_add(item)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef contains_remove(self, item):\n    # type (Any, Any) -> Any\n    '''Takes a collection and an item and returns a new collection\n    of the same type with that item removed. The notion of \"contains\"\n    is defined by the object itself; the following must be ``True``:\n\n    .. code-block:: python\n\n        item not in contains_remove(obj, item)\n\n    This function is used by some lenses (particularly ContainsLens)\n    to remove items from containers when necessary.\n\n    The corresponding method call for this hook is\n    ``obj._lens_contains_remove(item)``.\n\n    There is no default implementation.\n    '''\n    try:\n        self._lens_contains_remove\n    except AttributeError:\n        message = 'Don\\'t know how to remove an item from {}'\n        raise NotImplementedError(message.format(type(self)))\n    else:\n        return self._lens_contains_remove(item)", "response": "Takes a collection and an item and returns a new collection\n    of the same type with that item removed."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntake an object and an iterable and produces a new object that is a copy of the original with data from the iterable reincorporated.", "response": "def from_iter(self, iterable):\n    # type: (Any, Any) -> Any\n    '''Takes an object and an iterable and produces a new object that is\n    a copy of the original with data from ``iterable`` reincorporated. It\n    is intended as the inverse of the ``to_iter`` function. Any state in\n    ``self`` that is not modelled by the iterable should remain unchanged.\n\n    The following equality should hold for your definition:\n\n    .. code-block:: python\n\n        from_iter(self, to_iter(self)) == self\n\n    This function is used by EachLens to synthesise states from iterables,\n    allowing it to focus every element of an iterable state.\n\n    The corresponding method call for this hook is\n    ``obj._lens_from_iter(iterable)``.\n\n    There is no default implementation.\n    '''\n    try:\n        self._lens_from_iter\n    except AttributeError:\n        message = 'Don\\'t know how to create instance of {} from iterable'\n        raise NotImplementedError(message.format(type(self)))\n    else:\n        return self._lens_from_iter(iterable)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a function that will set the focus to newvalue.", "response": "def set(self, newvalue):\n        # type: (B) -> Callable[[S], T]\n        '''Set the focus to `newvalue`.\n\n            >>> from lenses import lens\n            >>> set_item_one_to_four = lens[1].set(4)\n            >>> set_item_one_to_four([1, 2, 3])\n            [1, 4, 3]\n        '''\n\n        def setter(state):\n            return self._optic.set(state, newvalue)\n\n        return setter"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_many(self, new_values):\n        # type: (Iterable[B]) -> Callable[[S], T]\n        '''Set many foci to values taken by iterating over `new_values`.\n\n            >>> from lenses import lens\n            >>> lens.Each().set_many(range(4, 7))([0, 1, 2])\n            [4, 5, 6]\n        '''\n\n        def setter_many(state):\n            return self._optic.iterate(state, new_values)\n\n        return setter_many", "response": "Set many foci to values taken by iterating over new_values."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef modify(self, func):\n        # type: (Callable[[A], B]) -> Callable[[S], T]\n        '''Apply a function to the focus.\n\n            >>> from lenses import lens\n            >>> convert_item_one_to_string = lens[1].modify(str)\n            >>> convert_item_one_to_string([1, 2, 3])\n            [1, '2', 3]\n            >>> add_ten_to_item_one = lens[1].modify(lambda n: n + 10)\n            >>> add_ten_to_item_one([1, 2, 3])\n            [1, 12, 3]\n        '''\n\n        def modifier(state):\n            return self._optic.over(state, func)\n\n        return modifier", "response": "Apply a function to the focus."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\napply func to the data inside the args functors incrementally.", "response": "def multiap(func, *args):\n    '''Applies `func` to the data inside the `args` functors\n    incrementally. `func` must be a curried function that takes\n    `len(args)` arguments.\n\n        >>> func = lambda a: lambda b: a + b\n        >>> multiap(func, [1, 10], [100])\n        [101, 110]\n    '''\n    functor = typeclass.fmap(args[0], func)\n    for arg in args[1:]:\n        functor = typeclass.apply(arg, functor)\n    return functor"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef collect_args(n):\n    '''Returns a function that can be called `n` times with a single\n    argument before returning all the args that have been passed to it\n    in a tuple. Useful as a substitute for functions that can't easily be\n    curried.\n\n        >>> collect_args(3)(1)(2)(3)\n        (1, 2, 3)\n    '''\n    args = []\n\n    def arg_collector(arg):\n        args.append(arg)\n        if len(args) == n:\n            return tuple(args)\n        else:\n            return arg_collector\n\n    return arg_collector", "response": "Returns a function that can be called n times with a single\n    argument before returning all the args that have been passed to it\n    in a tuple. Useful as a substitute for functions that can t easily be curried."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nintending to be overridden by subclasses. Raises NotImplementedError.", "response": "def func(self, f, state):\n        '''Intended to be overridden by subclasses. Raises\n        NotImplementedError.'''\n        message = 'Tried to use unimplemented lens {}.'\n        raise NotImplementedError(message.format(type(self)))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef apply(self, f, pure, state):\n        '''Runs the lens over the `state` applying `f` to all the foci\n        collecting the results together using the applicative functor\n        functions defined in `lenses.typeclass`. `f` must return an\n        applicative functor. For the case when no focus exists you must\n        also provide a `pure` which should take a focus and return the\n        pure form of the functor returned by `f`.\n        '''\n        return self.func(Functorisor(pure, f), state)", "response": "Runs the lens over the state applying f to all the foci\n        s elements and returns the result."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef preview(self, state):\n        # type: (S) -> Just[B]\n        '''Previews a potentially non-existant focus within\n        `state`. Returns `Just(focus)` if it exists, Nothing otherwise.\n\n        Requires kind Fold.\n        '''\n        if not self._is_kind(Fold):\n            raise TypeError('Must be an instance of Fold to .preview()')\n\n        pure = lambda a: Const(Nothing())\n        func = lambda a: Const(Just(a))\n        return self.apply(func, pure, state).unwrap()", "response": "Previews a potentially non - existant focus within state. Returns Nothing if it exists."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the focus within state.", "response": "def view(self, state):\n        # type: (S) -> B\n        '''Returns the focus within `state`. If multiple items are\n        focused then it will attempt to join them together as a monoid.\n        See `lenses.typeclass.mappend`.\n\n        Requires kind Fold. This method will raise TypeError if the\n        optic has no way to get any foci.\n\n        For technical reasons, this method requires there to be at least\n        one foci at the end of the view. It will raise ValueError when\n        there is none.\n        '''\n        if not self._is_kind(Fold):\n            raise TypeError('Must be an instance of Fold to .view()')\n\n        guard = object()\n        result = self.preview(state).maybe(guard)\n        if result is guard:\n            raise ValueError('No focus to view')\n        return cast(B, result)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of all the foci within state.", "response": "def to_list_of(self, state):\n        # type: (S) -> List[B]\n        '''Returns a list of all the foci within `state`.\n\n        Requires kind Fold. This method will raise TypeError if the\n        optic has no way to get any foci.\n        '''\n        if not self._is_kind(Fold):\n            raise TypeError('Must be an instance of Fold to .to_list_of()')\n\n        pure = lambda a: Const([])\n        func = lambda a: Const([a])\n        return self.apply(func, pure, state).unwrap()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef over(self, state, fn):\n        # type: (S, Callable[[A], B]) -> T\n        '''Applies a function `fn` to all the foci within `state`.\n\n        Requires kind Setter. This method will raise TypeError when the\n        optic has no way to set foci.\n        '''\n        if not self._is_kind(Setter):\n            raise TypeError('Must be an instance of Setter to .over()')\n\n        pure = lambda a: Identity(a)\n        func = lambda a: Identity(fn(a))\n        return self.apply(func, pure, state).unwrap()", "response": "Applies a function fn to all the foci within state."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set(self, state, value):\n        # type: (S, B) -> T\n        '''Sets all the foci within `state` to `value`.\n\n        Requires kind Setter. This method will raise TypeError when the\n        optic has no way to set foci.\n        '''\n        if not self._is_kind(Setter):\n            raise TypeError('Must be an instance of Setter to .set()')\n\n        pure = lambda a: Identity(a)\n        func = lambda a: Identity(value)\n        return self.apply(func, pure, state).unwrap()", "response": "Sets all the foci within state to value."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef iterate(self, state, iterable):\n        # type: (S, Iterable[B]) -> T\n        '''Sets all the foci within `state` to values taken from `iterable`.\n\n        Requires kind Setter. This method will raise TypeError when the\n        optic has no way to set foci.\n        '''\n        if not self._is_kind(Setter):\n            raise TypeError('Must be an instance of Setter to .iterate()')\n\n        i = iter(iterable)\n        pure = lambda a: Identity(a)\n        func = lambda a: Identity(next(i))\n        return self.apply(func, pure, state).unwrap()", "response": "Iterate over all the foci within state to values taken from iterable."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a class representing the kind of optic.", "response": "def kind(self):\n        '''Returns a class representing the 'kind' of optic.'''\n        optics = [\n            Equality,\n            Isomorphism,\n            Prism,\n            Review,\n            Lens,\n            Traversal,\n            Getter,\n            Setter,\n            Fold,\n        ]\n        for optic in optics:\n            if self._is_kind(optic):\n                return optic"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the vector moved one step in the direction of the architectural unit of the theCOOKIE other potentially diagonally.", "response": "def step_towards(self, other):\n        '''returns the vector moved one step in the direction of the\n        other, potentially diagonally.'''\n\n        return self + Vector(\n            (\n                (self[0] < other[0]) - (self[0] > other[0]),\n                (self[1] < other[1]) - (self[1] > other[1]),\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef handle_input(self, input):\n        '''Takes a single character string as input and alters the game\n        state according to that input. Mostly, this means moving the\n        player around. Returns a new game state and boolean indicating\n        whether the input had an effect on the state.'''\n\n        dirs = {\n            'h': (-1, 0),\n            'j': (0, 1),\n            'k': (0, -1),\n            'l': (1, 0),\n            'y': (-1, -1),\n            'u': (1, -1),\n            'n': (1, 1),\n            'b': (-1, 1),\n        }\n\n        if input in dirs:\n            new_self = (lens.player + dirs[input])(self)\n            if not new_self.player.inside():\n                return self, False\n            return new_self, True\n        elif input == '.':\n            return self, True\n        elif input == 'q':\n            return self.end_game(), False\n        elif input == 't':\n            self = lens.player.set(Vector.random())(self)\n            return self, True\n        else:\n            return self, False", "response": "Takes a single character string as input and alters the game\n            state according to that input. Returns a new game state and boolean indicating\n        whether the input had an effect on the state."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef advance_robots(self):\n        '''Produces a new game state in which the robots have advanced\n        towards the player by one step. Handles the robots crashing into\n        one another too.'''\n\n        # move the robots towards the player\n        self = lens.robots.Each().call_step_towards(self.player)(self)\n        # robots in the same place are crashes\n        self = lens.crashes.call_union(duplicates(self.robots))(self)\n        # remove crashed robots\n        self = lens.robots.modify(lambda r: list(set(r) - self.crashes))(self)\n\n        return self", "response": "Produces a new game state in which the robots have advanced\n        towards the player by one step. Handles the robots crashing into\n        one another too."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck for the game s win or lose conditions and alters the the game state to reflect the condition found.", "response": "def check_game_end(self):\n        '''Checks for the game's win/lose conditions and 'alters' the\n        game state to reflect the condition found. If the game has not\n        been won or lost then it just returns the game state\n        unaltered.'''\n\n        if self.player in self.crashes.union(self.robots):\n            return self.end_game('You Died!')\n        elif not self.robots:\n            return self.end_game('You Win!')\n        else:\n            return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef end_game(self, message=''):\n        '''Returns a completed game state object, setting an optional\n        message to display after the game is over.'''\n\n        return lens.running.set(False)(lens.message.set(message)(self))", "response": "Returns a completed game state object setting an optional\n        message to display after the game is over."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nshow the board to the player on the console and asks them to make a move.", "response": "def player_move(board):\n    '''Shows the board to the player on the console and asks them to\n    make a move.'''\n    print(board, end='\\n\\n')\n    x, y = input('Enter move (e.g. 2b): ')\n    print()\n    return int(x) - 1, ord(y) - ord('a')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef play():\n    'Play a game of naughts and crosses against the computer.'\n    ai = {'X': player_move, 'O': random_move}\n    board = Board()\n    while not board.winner:\n        x, y = ai[board.player](board)\n        board = board.make_move(x, y)\n    print(board, end='\\n\\n')\n    print(board.winner)", "response": "Play a game of naughts and crosses against the computer."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_move(self, x, y):\n        '''Return a board with a cell filled in by the current player. If\n        the cell is already occupied then return the board unchanged.'''\n        if self.board[y][x] == ' ':\n            return lens.board[y][x].set(self.player)(self)\n        return self", "response": "Return a board with a cell filled in by the current player. If the cell is already occupied then return the board unchanged."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating all the combinations of board positions that need to be checked for a win.", "response": "def _potential_wins(self):\n        '''Generates all the combinations of board positions that need\n        to be checked for a win.'''\n        yield from self.board\n        yield from zip(*self.board)\n        yield self.board[0][0], self.board[1][1], self.board[2][2]\n        yield self.board[0][2], self.board[1][1], self.board[2][0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process_item(self, item, spider):\n        self.items.append(item)\n        if len(self.items) >= self.max_chunk_size:\n            self._upload_chunk(spider)\n\n        return item", "response": "Process single item. Add item to items and then upload to S3 if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef open_spider(self, spider):\n        # Store timestamp to replace {time} in S3PIPELINE_URL\n        self.ts = datetime.utcnow().replace(microsecond=0).isoformat().replace(':', '-')", "response": "Callback function when spider is opened."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _upload_chunk(self, spider):\n\n        if not self.items:\n            return  # Do nothing when items is empty.\n\n        f = self._make_fileobj()\n\n        # Build object key by replacing variables in object key template.\n        object_key = self.object_key_template.format(**self._get_uri_params(spider))\n\n        try:\n            self.s3.upload_fileobj(f, self.bucket_name, object_key)\n        except ClientError:\n            self.stats.inc_value('pipeline/s3/fail')\n            raise\n        else:\n            self.stats.inc_value('pipeline/s3/success')\n        finally:\n            # Prepare for the next chunk\n            self.chunk_number += len(self.items)\n            self.items = []", "response": "Upload the next chunk to S3."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _make_fileobj(self):\n\n        bio = BytesIO()\n        f = gzip.GzipFile(mode='wb', fileobj=bio) if self.use_gzip else bio\n\n        # Build file object using ItemExporter\n        exporter = JsonLinesItemExporter(f)\n        exporter.start_exporting()\n        for item in self.items:\n            exporter.export_item(item)\n        exporter.finish_exporting()\n\n        if f is not bio:\n            f.close()  # Close the file if GzipFile\n\n        # Seek to the top of file to be read later\n        bio.seek(0)\n\n        return bio", "response": "Build a file object from items."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates the given wrapper with the contents of the given object.", "response": "def update_wrapper(wrapper, wrapped):\n    \"\"\" To be used under python2.4 because functools.update_wrapper() is available only from python2.5+ \"\"\"\n    for attr_name in ('__module__', '__name__', '__doc__'):\n        attr_value = getattr(wrapped, attr_name, None)\n        if attr_value is not None:\n            setattr(wrapper, attr_name, attr_value)\n    wrapper.__dict__.update(getattr(wrapped, '__dict__', {}))\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the account state information associated with a specific address.", "response": "def get_account_state(self, address, **kwargs):\n        \"\"\" Returns the account state information associated with a specific address.\n\n        :param address: a 34-bit length address (eg. AJBENSwajTzQtwyJFkiJSv7MAaaMc7DsRz)\n        :type address: str\n        :return: dictionary containing the account state information\n        :rtype: dict\n\n        \"\"\"\n        return self._call(JSONRPCMethods.GET_ACCOUNT_STATE.value, params=[address, ], **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_asset_state(self, asset_id, **kwargs):\n        return self._call(JSONRPCMethods.GET_ASSET_STATE.value, params=[asset_id, ], **kwargs)", "response": "Returns the asset state information associated with a specific asset ID."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the block information associated with a specific hash value or block index. :param block_hash: a block hash value or a block index (block height) :param verbose: a boolean indicating whether the detailed block information should be returned in JSON format (otherwise the block information is returned as an hexadecimal string by the JSON-RPC endpoint) :type block_hash: str or int :type verbose: bool :return: dictionary containing the block information (or an hexadecimal string if verbose is set to False) :rtype: dict or str", "response": "def get_block(self, block_hash, verbose=True, **kwargs):\n        \"\"\" Returns the block information associated with a specific hash value or block index.\n\n        :param block_hash: a block hash value or a block index (block height)\n        :param verbose:\n            a boolean indicating whether the detailed block information should be returned in JSON\n            format (otherwise the block information is returned as an hexadecimal string by the\n            JSON-RPC endpoint)\n        :type block_hash: str or int\n        :type verbose: bool\n        :return:\n            dictionary containing the block information (or an hexadecimal string if verbose is set\n            to False)\n        :rtype: dict or str\n\n        \"\"\"\n        return self._call(\n            JSONRPCMethods.GET_BLOCK.value, params=[block_hash, int(verbose), ], **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the hash value associated with a specific block index.", "response": "def get_block_hash(self, block_index, **kwargs):\n        \"\"\" Returns the hash value associated with a specific block index.\n\n        :param block_index: a block index (block height)\n        :type block_index: int\n        :return: hash of the block associated with the considered index\n        :rtype: str\n\n        \"\"\"\n        return self._call(JSONRPCMethods.GET_BLOCK_HASH.value, [block_index, ], **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve the system fees associated with a specific block index.", "response": "def get_block_sys_fee(self, block_index, **kwargs):\n        \"\"\" Returns the system fees associated with a specific block index.\n\n        :param block_index: a block index (block height)\n        :type block_index: int\n        :return: system fees of the block, expressed in NeoGas units\n        :rtype: str\n\n        \"\"\"\n        return self._call(JSONRPCMethods.GET_BLOCK_SYS_FEE.value, [block_index, ], **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the contract state associated with a specific script hash.", "response": "def get_contract_state(self, script_hash, **kwargs):\n        \"\"\" Returns the contract information associated with a specific script hash.\n\n        :param script_hash: contract script hash\n        :type script_hash: str\n        :return: dictionary containing the contract information\n        :rtype: dict\n\n        \"\"\"\n        return self._call(JSONRPCMethods.GET_CONTRACT_STATE.value, [script_hash, ], **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_raw_transaction(self, tx_hash, verbose=True, **kwargs):\n        return self._call(\n            JSONRPCMethods.GET_RAW_TRANSACTION.value, params=[tx_hash, int(verbose), ], **kwargs)", "response": "Returns detailed information associated with a specific transaction hash."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the value stored in the storage of a given script hash and key.", "response": "def get_storage(self, script_hash, key, **kwargs):\n        \"\"\" Returns the value stored in the storage of a contract script hash for a given key.\n\n        :param script_hash: contract script hash\n        :param key: key to look up in the storage\n        :type script_hash: str\n        :type key: str\n        :return: value associated with the storage key\n        :rtype: bytearray\n\n        \"\"\"\n        hexkey = binascii.hexlify(key.encode('utf-8')).decode('utf-8')\n        hexresult = self._call(\n            JSONRPCMethods.GET_STORAGE.value, params=[script_hash, hexkey, ], **kwargs)\n        try:\n            assert hexresult\n            result = bytearray(binascii.unhexlify(hexresult.encode('utf-8')))\n        except AssertionError:\n            result = hexresult\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_tx_out(self, tx_hash, index, **kwargs):\n        return self._call(JSONRPCMethods.GET_TX_OUT.value, params=[tx_hash, index, ], **kwargs)", "response": "Returns the transaction output information corresponding to a hash and index."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef invoke(self, script_hash, params, **kwargs):\n        contract_params = encode_invocation_params(params)\n        raw_result = self._call(\n            JSONRPCMethods.INVOKE.value, [script_hash, contract_params, ], **kwargs)\n        return decode_invocation_result(raw_result)", "response": "Invoke a smart contract with given parameters and returns the result."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninvokes a contract s function with given parameters and returns the result.", "response": "def invoke_function(self, script_hash, operation, params, **kwargs):\n        \"\"\" Invokes a contract's function with given parameters and returns the result.\n\n        :param script_hash: contract script hash\n        :param operation: name of the operation to invoke\n        :param params: list of paramaters to be passed in to the smart contract\n        :type script_hash: str\n        :type operation: str\n        :type params: list\n        :return: result of the invocation\n        :rtype: dictionary\n\n        \"\"\"\n        contract_params = encode_invocation_params(params)\n        raw_result = self._call(\n            JSONRPCMethods.INVOKE_FUNCTION.value, [script_hash, operation, contract_params, ],\n            **kwargs)\n        return decode_invocation_result(raw_result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef invoke_script(self, script, **kwargs):\n        raw_result = self._call(JSONRPCMethods.INVOKE_SCRIPT.value, [script, ], **kwargs)\n        return decode_invocation_result(raw_result)", "response": "Invoke a script on the VM and returns the result."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsend a raw transaction over the NEO network and returns the result.", "response": "def send_raw_transaction(self, hextx, **kwargs):\n        \"\"\" Broadcasts a transaction over the NEO network and returns the result.\n\n        :param hextx: hexadecimal string that has been serialized\n        :type hextx: str\n        :return: result of the transaction\n        :rtype: bool\n\n        \"\"\"\n        return self._call(JSONRPCMethods.SEND_RAW_TRANSACTION.value, [hextx, ], **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvalidating if the given string is a valid NEO address.", "response": "def validate_address(self, addr, **kwargs):\n        \"\"\" Validates if the considered string is a valid NEO address.\n\n        :param hex: string containing a potential NEO address\n        :type hex: str\n        :return: dictionary containing the result of the verification\n        :rtype: dictionary\n\n        \"\"\"\n        return self._call(JSONRPCMethods.VALIDATE_ADDRESS.value, [addr, ], **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalling the JSON - RPC endpoint.", "response": "def _call(self, method, params=None, request_id=None):\n        \"\"\" Calls the JSON-RPC endpoint. \"\"\"\n        params = params or []\n\n        # Determines which 'id' value to use and increment the counter associated with the current\n        # client instance if applicable.\n        rid = request_id or self._id_counter\n        if request_id is None:\n            self._id_counter += 1\n\n        # Prepares the payload and the headers that will be used to forge the request.\n        payload = {'jsonrpc': '2.0', 'method': method, 'params': params, 'id': rid}\n        headers = {'Content-Type': 'application/json'}\n        scheme = 'https' if self.tls else 'http'\n        url = '{}://{}:{}'.format(scheme, self.host, self.port)\n\n        # Calls the JSON-RPC endpoint!\n        try:\n            response = self.session.post(url, headers=headers, data=json.dumps(payload))\n            response.raise_for_status()\n        except HTTPError:\n            raise TransportError(\n                'Got unsuccessful response from server (status code: {})'.format(\n                    response.status_code),\n                response=response)\n\n        # Ensures the response body can be deserialized to JSON.\n        try:\n            response_data = response.json()\n        except ValueError as e:\n            raise ProtocolError(\n                'Unable to deserialize response body: {}'.format(e), response=response)\n\n        # Properly handles potential errors.\n        if response_data.get('error'):\n            code = response_data['error'].get('code', '')\n            message = response_data['error'].get('message', '')\n            raise ProtocolError(\n                'Error[{}] {}'.format(code, message), response=response, data=response_data)\n        elif 'result' not in response_data:\n            raise ProtocolError(\n                'Response is empty (result field is missing)', response=response,\n                data=response_data)\n\n        return response_data['result']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_hash256(s):\n    if not s or not isinstance(s, str):\n        return False\n    return re.match('^[0-9A-F]{64}$', s.strip(), re.IGNORECASE)", "response": "Returns True if the given string is a valid SHA256 hash."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning True if the given string is a valid RIPEMD160 hash.", "response": "def is_hash160(s):\n    \"\"\" Returns True if the considered string is a valid RIPEMD160 hash. \"\"\"\n    if not s or not isinstance(s, str):\n        return False\n    if not len(s) == 40:\n        return False\n    for c in s:\n        if (c < '0' or c > '9') and (c < 'A' or c > 'F') and (c < 'a' or c > 'f'):\n            return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef encode_invocation_params(params):\n    final_params = []\n    for p in params:\n        if isinstance(p, bool):\n            final_params.append({'type': ContractParameterTypes.BOOLEAN.value, 'value': p})\n        elif isinstance(p, int):\n            final_params.append({'type': ContractParameterTypes.INTEGER.value, 'value': p})\n        elif is_hash256(p):\n            final_params.append({'type': ContractParameterTypes.HASH256.value, 'value': p})\n        elif is_hash160(p):\n            final_params.append({'type': ContractParameterTypes.HASH160.value, 'value': p})\n        elif isinstance(p, bytearray):\n            final_params.append({'type': ContractParameterTypes.BYTE_ARRAY.value, 'value': p})\n        elif isinstance(p, str):\n            final_params.append({'type': ContractParameterTypes.STRING.value, 'value': p})\n        elif isinstance(p, list):\n            innerp = encode_invocation_params(p)\n            final_params.append({'type': ContractParameterTypes.ARRAY.value, 'value': innerp})\n    return final_params", "response": "Encodes the given list of parameteraters meant to be passed to JSON - RPC endpoints."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndecode the values embedded in an invocation result dictionary.", "response": "def decode_invocation_result(result):\n    \"\"\" Tries to decode the values embedded in an invocation result dictionary. \"\"\"\n    if 'stack' not in result:\n        return result\n    result = copy.deepcopy(result)\n    result['stack'] = _decode_invocation_result_stack(result['stack'])\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nemulating keyword - only arguments under python2 and python3.", "response": "def first_kwonly_arg(name):\n    \"\"\" Emulates keyword-only arguments under python2. Works with both python2 and python3.\n    With this decorator you can convert all or some of the default arguments of your function\n    into kwonly arguments. Use ``KWONLY_REQUIRED`` as the default value of required kwonly args.\n\n    :param name: The name of the first default argument to be treated as a keyword-only argument. This default\n    argument along with all default arguments that follow this one will be treated as keyword only arguments.\n\n    You can also pass here the ``FIRST_DEFAULT_ARG`` constant in order to select the first default argument. This\n    way you turn all default arguments into keyword-only arguments. As a shortcut you can use the\n    ``@kwonly_defaults`` decorator (without any parameters) instead of ``@first_kwonly_arg(FIRST_DEFAULT_ARG)``.\n\n        >>> from kwonly_args import first_kwonly_arg, KWONLY_REQUIRED, FIRST_DEFAULT_ARG, kwonly_defaults\n        >>>\n        >>> # this decoration converts the ``d1`` and ``d2`` default args into kwonly args\n        >>> @first_kwonly_arg('d1')\n        >>> def func(a0, a1, d0='d0', d1='d1', d2='d2', *args, **kwargs):\n        >>>     print(a0, a1, d0, d1, d2, args, kwargs)\n        >>>\n        >>> func(0, 1, 2, 3, 4)\n        0 1 2 d1 d2 (3, 4) {}\n        >>>\n        >>> func(0, 1, 2, 3, 4, d2='my_param')\n        0 1 2 d1 my_param (3, 4) {}\n        >>>\n        >>> # d0 is an optional deyword argument, d1 is required\n        >>> def func(d0='d0', d1=KWONLY_REQUIRED):\n        >>>     print(d0, d1)\n        >>>\n        >>> # The ``FIRST_DEFAULT_ARG`` constant automatically selects the first default argument so it\n        >>> # turns all default arguments into keyword-only ones. Both d0 and d1 are keyword-only arguments.\n        >>> @first_kwonly_arg(FIRST_DEFAULT_ARG)\n        >>> def func(a0, a1, d0='d0', d1='d1'):\n        >>>     print(a0, a1, d0, d1)\n        >>>\n        >>> # ``@kwonly_defaults`` is a shortcut for the ``@first_kwonly_arg(FIRST_DEFAULT_ARG)``\n        >>> # in the previous example. This example has the same effect as the previous one.\n        >>> @kwonly_defaults\n        >>> def func(a0, a1, d0='d0', d1='d1'):\n        >>>     print(a0, a1, d0, d1)\n    \"\"\"\n    def decorate(wrapped):\n        if sys.version_info[0] == 2:\n            arg_names, varargs, _, defaults = inspect.getargspec(wrapped)\n        else:\n            arg_names, varargs, _, defaults = inspect.getfullargspec(wrapped)[:4]\n\n        if not defaults:\n            raise TypeError(\"You can't use @first_kwonly_arg on a function that doesn't have default arguments!\")\n        first_default_index = len(arg_names) - len(defaults)\n\n        if name is FIRST_DEFAULT_ARG:\n            first_kwonly_index = first_default_index\n        else:\n            try:\n                first_kwonly_index = arg_names.index(name)\n            except ValueError:\n                raise ValueError(\"%s() doesn't have an argument with the specified first_kwonly_arg=%r name\" % (\n                                 getattr(wrapped, '__name__', '?'), name))\n\n        if first_kwonly_index < first_default_index:\n            raise ValueError(\"The specified first_kwonly_arg=%r must have a default value!\" % (name,))\n\n        kwonly_defaults = defaults[-(len(arg_names)-first_kwonly_index):]\n        kwonly_args = tuple(zip(arg_names[first_kwonly_index:], kwonly_defaults))\n        required_kwonly_args = frozenset(arg for arg, default in kwonly_args if default is KWONLY_REQUIRED)\n\n        def wrapper(*args, **kwargs):\n            if required_kwonly_args:\n                missing_kwonly_args = required_kwonly_args.difference(kwargs.keys())\n                if missing_kwonly_args:\n                    raise TypeError(\"%s() missing %s keyword-only argument(s): %s\" % (\n                                    getattr(wrapped, '__name__', '?'), len(missing_kwonly_args),\n                                    ', '.join(sorted(missing_kwonly_args))))\n            if len(args) > first_kwonly_index:\n                if varargs is None:\n                    raise TypeError(\"%s() takes exactly %s arguments (%s given)\" % (\n                                    getattr(wrapped, '__name__', '?'), first_kwonly_index, len(args)))\n                kwonly_args_from_kwargs = tuple(kwargs.pop(arg, default) for arg, default in kwonly_args)\n                args = args[:first_kwonly_index] + kwonly_args_from_kwargs + args[first_kwonly_index:]\n\n            return wrapped(*args, **kwargs)\n\n        return update_wrapper(wrapper, wrapped)\n    return decorate"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef devpiserver_on_upload(stage, project, version, link):\n    if link.entry and link.entry.file_exists() and link.entry.basename.endswith('.whl'):\n        threadlog.info(\"Wheel detected: %s\", link.entry.basename)\n        new_version = parse_version(version)\n        latest_version = parse_version(stage.get_latest_version_perstage(project))\n        if latest_version > new_version:\n            threadlog.debug(\"A newer release has already been uploaded: %s - nothing to do\", latest_version)\n            return\n        metadata = extract_metadata_from_wheel_file(link.entry.file_os_path())\n        linkstore = stage.get_linkstore_perstage(link.project, link.version)\n        project_dir = '%s/%s/+f/%s' % (linkstore.filestore.storedir, stage.name, project)\n\n        if not os.path.exists(project_dir):\n            os.mkdir(project_dir)\n\n        json_path = '%s/%s-%s.json' % (project_dir, project, new_version)\n        with open(json_path, 'w') as fd:\n            fd.write(json.dumps(metadata))\n\n        threadlog.info(\"Stored %s to: %s\", metadata, json_path)\n\n        # We symlink the latest version\n        symlink_path = '%s.json' % project_dir\n        if os.path.exists(symlink_path):\n            os.unlink(symlink_path)\n        os.symlink(json_path, symlink_path)", "response": "Called when a file is uploaded to a private stage."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a datetime that is the datetime resulting from applying all transformations of a time to the input datetime.", "response": "def snap(dttm, instruction):\n    \"\"\"\n    Args:\n        instruction (string): a string that encodes 0 to n transformations of a time, i.e. \"-1h@h\", \"@mon+2d+4h\", ...\n        dttm (datetime):\n    Returns:\n        datetime: The datetime resulting from applying all transformations to the input datetime.\n\n    Example:\n        >>> snap(datetime(2016, 1, 1, 15, 30), \"-1h@h\")\n        datetime(2016, 1, 1, 14)\n    \"\"\"\n    transformations = parse(instruction)\n    return reduce(lambda dt, transformation: transformation.apply_to(dt), transformations, dttm)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef snap_tz(dttm, instruction, timezone):\n    transformations = parse(instruction)\n    return reduce(lambda dt, transformation: transformation.apply_to_with_tz(dt, timezone), transformations, dttm)", "response": "This function handles timezone aware datetimes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\napply the time switch to a datetime object and return the datetime object.", "response": "def apply_to_with_tz(self, dttm, timezone):\n        \"\"\"We make sure that after truncating we use the correct timezone,\n        even if we 'jump' over a daylight saving time switch.\n\n        I.e. if we apply \"@d\" to `Sun Oct 30 04:30:00 CET 2016` (1477798200)\n        we want to have `Sun Oct 30 00:00:00 CEST 2016` (1477778400)\n        but not `Sun Oct 30 00:00:00 CET 2016` (1477782000)\n        \"\"\"\n        result = self.apply_to(dttm)\n        if self.unit in [DAYS, WEEKS, MONTHS, YEARS]:\n            naive_dttm = datetime(result.year, result.month, result.day)\n            result = timezone.localize(naive_dttm)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrenders the barcode and saves it in filename.", "response": "def save(self, filename, options=None):\n        \"\"\"Renders the barcode and saves it in `filename`.\n\n        :parameters:\n            filename : String\n                Filename to save the barcode in (without filename\n                extension).\n            options : Dict\n                The same as in `self.render`.\n\n        :returns: The full filename with extension.\n        :rtype: String\n        \"\"\"\n        output = self.render(options)\n        _filename = self.writer.save(filename, output)\n        return _filename"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef render(self, writer_options=None):\n        options = Barcode.default_writer_options.copy()\n        options.update(writer_options or {})\n        if options['write_text']:\n            options['text'] = self.get_fullcode()\n        self.writer.set_options(options)\n        code = self.build()\n        raw = Barcode.raw = self.writer.render(code)\n        return raw", "response": "Renders the barcode using the writer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calculate_checksum(self):\n        def sum_(x, y):\n            return int(x) + int(y)\n\n        evensum = reduce(sum_, self.ean[::2])\n        oddsum = reduce(sum_, self.ean[1::2])\n        return (10 - ((evensum + oddsum * 3) % 10)) % 10", "response": "Calculates the checksum for the EAN13 - Code."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrendering the barcode to whatever the inheriting writer provides.", "response": "def render(self, code):\n        \"\"\"Renders the barcode to whatever the inheriting writer provides,\n        using the registered callbacks.\n\n        :parameters:\n            code : List\n                List of strings matching the writer spec\n                (only contain 0 or 1).\n        \"\"\"\n        if self._callbacks['initialize'] is not None:\n            self._callbacks['initialize'](code)\n        ypos = 1.0\n        for line in code:\n            # Left quiet zone is x startposition\n            xpos = self.quiet_zone\n            for mod in line:\n                if mod == '0':\n                    color = self.background\n                else:\n                    color = self.foreground\n                self._callbacks['paint_module'](xpos, ypos, self.module_width,\n                                                color)\n                xpos += self.module_width\n            # Add right quiet zone to every line\n            self._callbacks['paint_module'](xpos, ypos, self.quiet_zone,\n                                            self.background)\n            ypos += self.module_height\n        if self.text and self._callbacks['paint_text'] is not None:\n            ypos += self.text_distance\n            if self.center_text:\n                xpos = xpos / 2.0\n            else:\n                xpos = self.quiet_zone + 4.0\n            self._callbacks['paint_text'](xpos, ypos)\n        return self._callbacks['finish']()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_df(self, **kwargs):\n\n        return pd.read_sql(sql=self.statement, con=self.session.bind, **kwargs)", "response": "Returns a DataFrame from the SQL query"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef connect(cls, settings):\n        server = serializer('json').loads(settings['kvs.perlsess'])\n        server.setdefault('key_prefix', 'perlsess::')\n        server.setdefault('codec', 'storable')\n        cls.cookie_name = server.pop('cookie_name', 'session_id')\n        cls.client = KVS(**server)", "response": "Connect to the KVS server."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads the file defined by the S3CONF variable and output its contents to stdout.", "response": "def env(section, map_files, phusion, phusion_path, quiet, edit, create):\n    \"\"\"\n    Reads the file defined by the S3CONF variable and output its contents to stdout. Logs are printed to stderr.\n    See options for added functionality: editing file, mapping files, dumping in the phusion-baseimage format, etc.\n    \"\"\"\n    try:\n        logger.debug('Running env command')\n        settings = config.Settings(section=section)\n        storage = STORAGES['s3'](settings=settings)\n        conf = s3conf.S3Conf(storage=storage, settings=settings)\n\n        if edit:\n            conf.edit(create=create)\n        else:\n            env_vars = conf.get_envfile().as_dict()\n            if env_vars.get('S3CONF_MAP') and map_files:\n                conf.download_mapping(env_vars.get('S3CONF_MAP'))\n            if not quiet:\n                for var_name, var_value in sorted(env_vars.items(), key=lambda x: x[0]):\n                    click.echo('{}={}'.format(var_name, var_value))\n            if phusion:\n                s3conf.phusion_dump(env_vars, phusion_path)\n    except exceptions.EnvfilePathNotDefinedError:\n        raise exceptions.EnvfilePathNotDefinedUsageError()\n    except exceptions.FileDoesNotExist as e:\n        raise UsageError('The file {} does not exist. Try \"-c\" option if you want to create it.'.format(str(e)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexecutes a command in the same context.", "response": "def exec_command(ctx, section, command, map_files):\n    \"\"\"\n    Sets the process environemnt and executes the [COMMAND] in the same context. Does not modify the current shell\n    environment.\n\n    If the [COMMAND] has option-like arguments, use the standard POSIX pattern \"--\" to separate options\n    from arguments. Considering our configuration in the \"dev\" section, we could write:\n\n    s3conf -v info exec dev -- ping -v google.com\n    \"\"\"\n    try:\n        logger.debug('Running exec command')\n        existing_sections = config.ConfigFileResolver(config.LOCAL_CONFIG_FILE).sections()\n        command = ' '.join(command)\n        if section not in existing_sections:\n            command = '{} {}'.format(section, command) if command else section\n            section = None\n\n        if not command:\n            logger.warning('No command detected.')\n            click.echo(exec_command.get_help(ctx))\n            return\n\n        settings = config.Settings(section=section)\n        storage = STORAGES['s3'](settings=settings)\n        conf = s3conf.S3Conf(storage=storage, settings=settings)\n\n        env_vars = conf.get_envfile().as_dict()\n        if env_vars.get('S3CONF_MAP') and map_files:\n            conf.download_mapping(env_vars.get('S3CONF_MAP'))\n\n        current_env = os.environ.copy()\n        current_env.update(env_vars)\n        logger.debug('Executing command \"%s\"', command)\n        subprocess.run(shlex.split(command), env=current_env, check=True)\n    except exceptions.EnvfilePathNotDefinedError:\n        raise exceptions.EnvfilePathNotDefinedUsageError()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndownloads a file or folder from the S3 - like service.", "response": "def download(remote_path, local_path):\n    \"\"\"\n    Download a file or folder from the S3-like service.\n\n    If REMOTE_PATH has a trailing slash it is considered to be a folder, e.g.: \"s3://my-bucket/my-folder/\". In this\n    case, LOCAL_PATH must be a folder as well. The files and subfolder structure in REMOTE_PATH are copied to\n    LOCAL_PATH.\n\n    If REMOTE_PATH does not have a trailing slash, it is considered to be a file, and LOCAL_PATH should be a file as\n    well.\n    \"\"\"\n    storage = STORAGES['s3']()\n    conf = s3conf.S3Conf(storage=storage)\n    conf.download(remote_path, local_path)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupload a file or folder to the S3 - like service.", "response": "def upload(remote_path, local_path):\n    \"\"\"\n    Upload a file or folder to the S3-like service.\n\n    If LOCAL_PATH is a folder, the files and subfolder structure in LOCAL_PATH are copied to REMOTE_PATH.\n\n    If LOCAL_PATH is a file, the REMOTE_PATH file is created with the same contents.\n    \"\"\"\n    storage = STORAGES['s3']()\n    conf = s3conf.S3Conf(storage=storage)\n    conf.upload(local_path, remote_path)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndownload the environemnt file for each section in the local config file and creates a folder inside the local config folder containing the map files.", "response": "def downsync(section, map_files):\n    \"\"\"\n    For each section defined in the local config file, creates a folder inside the local config folder\n    named after the section. Downloads the environemnt file defined by the S3CONF variable for this section\n    to this folder.\n    \"\"\"\n    try:\n        settings = config.Settings(section=section)\n        storage = STORAGES['s3'](settings=settings)\n        conf = s3conf.S3Conf(storage=storage, settings=settings)\n        local_root = os.path.join(config.LOCAL_CONFIG_FOLDER, section)\n        conf.downsync(local_root, map_files=map_files)\n    except exceptions.EnvfilePathNotDefinedError:\n        raise exceptions.EnvfilePathNotDefinedUsageError()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef diff(section):\n    try:\n        settings = config.Settings(section=section)\n        storage = STORAGES['s3'](settings=settings)\n        conf = s3conf.S3Conf(storage=storage, settings=settings)\n        local_root = os.path.join(config.LOCAL_CONFIG_FOLDER, section)\n        click.echo(''.join(conf.diff(local_root)))\n    except exceptions.EnvfilePathNotDefinedError:\n        raise exceptions.EnvfilePathNotDefinedUsageError()", "response": "Get the diff of the local config file and the remote config file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting a value for a variable in an environment file for the given section.", "response": "def set_variable(section, value, create):\n    \"\"\"\n    Set value of a variable in an environment file for the given section.\n    If the variable is already defined, its value is replaced, otherwise, it is added to the end of the file.\n    The value is given as \"ENV_VAR_NAME=env_var_value\", e.g.:\n\n    s3conf set test ENV_VAR_NAME=env_var_value\n    \"\"\"\n    if not value:\n        value = section\n        section = None\n    try:\n        logger.debug('Running env command')\n        settings = config.Settings(section=section)\n        conf = s3conf.S3Conf(settings=settings)\n\n        env_vars = conf.get_envfile()\n        env_vars.set(value, create=create)\n    except exceptions.EnvfilePathNotDefinedError:\n        raise exceptions.EnvfilePathNotDefinedUsageError()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unset_variable(section, value):\n    if not value:\n        value = section\n        section = None\n    try:\n        logger.debug('Running env command')\n        settings = config.Settings(section=section)\n        conf = s3conf.S3Conf(settings=settings)\n\n        env_vars = conf.get_envfile()\n        env_vars.unset(value)\n    except exceptions.EnvfilePathNotDefinedError:\n        raise exceptions.EnvfilePathNotDefinedUsageError()", "response": "Unset a variable in an environment file for the given section."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef init(section, remote_file):\n    if not remote_file.startswith('s3://'):\n        raise UsageError('REMOTE_FILE must be a S3-like path. E.g.:\\n\\n'\n                         's3conf init development s3://my-project/development.env')\n    logger.debug('Running init command')\n    config_file_path = os.path.join(os.getcwd(), '.s3conf', 'config')\n    config_file = config.ConfigFileResolver(config_file_path, section=section)\n    config_file.set('S3CONF', remote_file)\n    gitignore_file_path = os.path.join(os.getcwd(), '.s3conf', '.gitignore')\n    config_file.save()\n    open(gitignore_file_path, 'w').write('*\\n!config\\n')", "response": "Create the. s3conf config folder and. gitignore file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update(self, t_obj):\n\n        if isinstance(t_obj, Iterable):\n            self._session.add_all(t_obj)\n        else:\n            self._session.add(t_obj)", "response": "Updates the set of items in the current session."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef insert(self, table, insert_obj, ignore=True):\n\n        if isinstance(insert_obj, pd.DataFrame):\n            if insert_obj.empty:\n                raise ValueError('The input DataFrame is empty, please check!')\n            insert_obj = insert_obj.to_dict(orient='records')\n        elif not isinstance(insert_obj, list):\n            raise ValueError(\n                f\"The {reprlib.repr(insert_obj)} must be list of dicts type!\")\n\n        ignore_str = 'IGNORE' if ignore else ''\n        return self._session.execute(\n            table.__table__.insert().prefix_with(ignore_str), insert_obj)", "response": "Insert a new entry into the specified table."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsplit a env var text like ENV_VAR_NAME = env_var_value ENV_VAR_NAME = env_var_value ENV_VAR_VALUE = env_var_value", "response": "def parse_env_var(value):\n    \"\"\"\n    Split a env var text like\n\n    ENV_VAR_NAME=env_var_value\n\n    into a tuple ('ENV_VAR_NAME', 'env_var_value')\n    \"\"\"\n    k, _, v = value.partition('=')\n\n    # Remove any leading and trailing spaces in key, value\n    k, v = k.strip(), v.strip().encode('unicode-escape').decode('ascii')\n\n    if v and v[0] == v[-1] in ['\"', \"'\"]:\n        v = __escape_decoder(v[1:-1])[0]\n    return k, v"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding basic authentication to the requests of the clients.", "response": "def basic(username, password):\n    \"\"\"Add basic authentication to the requests of the clients.\"\"\"\n    none()\n    _config.username = username\n    _config.password = password"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef api_key(api_key):\n    none()\n    _config.api_key_prefix[\"Authorization\"] = \"api-key\"\n    _config.api_key[\"Authorization\"] = \"key=\" + b64encode(api_key.encode()).decode()", "response": "Authenticate via an api key."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_json_content_from_folder(folder):\n    for dirpath, dirnames, filenames in os.walk(folder):\n        for filename in filenames:\n            if filename.lower().endswith(\".json\"):\n                filepath = os.path.join(dirpath, filename)\n                with open(filepath, \"rb\") as file:\n                    yield json.loads(file.read().decode(\"UTF-8\"))", "response": "yields objects from json files in the folder and subfolders"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a dict of schema names mapping to a Schema.", "response": "def get_schemas():\n    \"\"\"Return a dict of schema names mapping to a Schema.\n\n    The schema is of type schul_cloud_resources_api_v1.schema.Schema\n    \"\"\"\n    schemas = {}\n    for name in os.listdir(JSON_PATH):\n        if name not in NO_SCHEMA:\n            schemas[name] = Schema(name)\n    return schemas"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the schema of the current user.", "response": "def get_schema(self):\n        \"\"\"Return the schema.\"\"\"\n        path = os.path.join(self._get_schema_folder(), self._name + \".json\")\n        with open(path, \"rb\") as file:\n            schema = json.loads(file.read().decode(\"UTF-8\"))\n        return schema"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_resolver(self):\n        store = {}\n        for schema in get_schemas().values():\n            store[schema.get_uri()] = schema.get_schema()\n        schema = self.get_schema()\n        return jsonschema.RefResolver.from_schema(schema, store=store)", "response": "Return a jsonschema. RefResolver for the schemas."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate(self, object):\n        resolver=self.get_resolver()\n        jsonschema.validate(object, self.get_schema(), resolver=resolver)", "response": "Validate an object against the schema."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_valid_examples(self):\n        path = os.path.join(self._get_schema_folder(), \"examples\", \"valid\")\n        return list(_get_json_content_from_folder(path))", "response": "Return a list of valid examples for the given schema."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of examples which violate the schema.", "response": "def get_invalid_examples(self):\n        \"\"\"Return a list of examples which violate the schema.\"\"\"\n        path = os.path.join(self._get_schema_folder(), \"examples\", \"invalid\")\n        return list(_get_json_content_from_folder(path))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking synchronous HTTP request.", "response": "def request( self, url, method='get', data=None, files=None,\n\t\t\t\traw=False, raw_all=False, headers=dict(), raise_for=dict(), session=None ):\n\t\t'''Make synchronous HTTP request.\n\t\t\tCan be overidden to use different http module (e.g. urllib2, twisted, etc).'''\n\t\ttry: import requests # import here to avoid dependency on the module\n\t\texcept ImportError as exc:\n\t\t\texc.args = ( 'Unable to find/import \"requests\" module.'\n\t\t\t\t' Please make sure that it is installed, e.g. by running \"pip install requests\" command.'\n\t\t\t\t'\\nFor more info, visit: http://docs.python-requests.org/en/latest/user/install/',)\n\t\t\traise exc\n\n\t\tif not self._requests_setup_done:\n\t\t\tpatched_session = self._requests_setup(\n\t\t\t\trequests, **(self.request_adapter_settings or dict()) )\n\t\t\tif patched_session is not None: self._requests_session = patched_session\n\n\t\tif session is None:\n\t\t\tsession = getattr(self, '_requests_session', None)\n\t\t\tif not session: session = self._requests_session = requests.session()\n\t\telif not session: session = requests\n\n\t\tmethod = method.lower()\n\t\tkwz = (self._requests_base_keywords or dict()).copy()\n\t\tkwz.update(self.request_extra_keywords or dict())\n\t\tkwz, func = dict(), ft.partial(session.request, method.upper(), **kwz)\n\t\tkwz_headers = (self.request_base_headers or dict()).copy()\n\t\tkwz_headers.update(headers)\n\t\tif data is not None:\n\t\t\tif method in ['post', 'put']:\n\t\t\t\tif all(hasattr(data, k) for k in ['seek', 'read']):\n\t\t\t\t\t# Force chunked encoding for files, as uploads hang otherwise\n\t\t\t\t\t# See https://github.com/mk-fg/python-onedrive/issues/30 for details\n\t\t\t\t\tdata.seek(0)\n\t\t\t\t\tkwz['data'] = iter(ft.partial(data.read, 200 * 2**10), b'')\n\t\t\t\telse: kwz['data'] = data\n\t\t\telse:\n\t\t\t\tkwz['data'] = json.dumps(data)\n\t\t\t\tkwz_headers.setdefault('Content-Type', 'application/json')\n\t\tif files is not None:\n\t\t\t# requests-2+ doesn't seem to add default content-type header\n\t\t\tfor k, file_tuple in files.iteritems():\n\t\t\t\tif len(file_tuple) == 2: files[k] = tuple(file_tuple) + ('application/octet-stream',)\n\t\t\t\t# Rewind is necessary because request can be repeated due to auth failure\n\t\t\t\tfile_tuple[1].seek(0)\n\t\t\tkwz['files'] = files\n\t\tif kwz_headers: kwz['headers'] = kwz_headers\n\n\t\tcode = res = None\n\t\ttry:\n\t\t\tres = func(url, **kwz)\n\t\t\t# log.debug('Response headers: %s', res.headers)\n\t\t\tcode = res.status_code\n\t\t\tif code == requests.codes.no_content: return\n\t\t\tif code != requests.codes.ok: res.raise_for_status()\n\t\texcept requests.RequestException as err:\n\t\t\tmessage = b'{0} [type: {1}, repr: {0!r}]'.format(err, type(err))\n\t\t\tif (res and getattr(res, 'text', None)) is not None: # \"res\" with non-200 code can be falsy\n\t\t\t\tmessage = res.text\n\t\t\t\ttry: message = json.loads(message)\n\t\t\t\texcept: message = '{}: {!r}'.format(str(err), message)[:300]\n\t\t\t\telse:\n\t\t\t\t\tmsg_err, msg_data = message.pop('error', None), message\n\t\t\t\t\tif msg_err:\n\t\t\t\t\t\tmessage = '{}: {}'.format(msg_err.get('code', err), msg_err.get('message', msg_err))\n\t\t\t\t\t\tif msg_data: message = '{} (data: {})'.format(message, msg_data)\n\t\t\traise raise_for.get(code, ProtocolError)(code, message)\n\t\tif raw: res = res.content\n\t\telif raw_all: res = code, dict(res.headers.items()), res.content\n\t\telse: res = json.loads(res.text)\n\t\treturn res"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef auth_user_get_url(self, scope=None):\n\t\t'Build authorization URL for User Agent.'\n\t\tif not self.client_id: raise AuthMissingError('No client_id specified')\n\t\treturn '{}?{}'.format(self.auth_url_user, urllib.urlencode(dict(\n\t\t\tclient_id=self.client_id, scope=' '.join(scope or self.auth_scope),\n\t\t\tresponse_type='code', redirect_uri=self.auth_redirect_uri )))", "response": "Build authorization URL for User Agent."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef auth_user_process_url(self, url):\n\t\t'Process tokens and errors from redirect_uri.'\n\t\turl = urlparse.urlparse(url)\n\t\turl_qs = dict(it.chain.from_iterable(\n\t\t\turlparse.parse_qsl(v) for v in [url.query, url.fragment] ))\n\t\tif url_qs.get('error'):\n\t\t\traise APIAuthError(\n\t\t\t\t'{} :: {}'.format(url_qs['error'], url_qs.get('error_description')) )\n\t\tself.auth_code = url_qs['code']\n\t\treturn self.auth_code", "response": "Process tokens and errors from redirect_uri."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrefreshes or acquire access_token.", "response": "def auth_get_token(self, check_scope=True):\n\t\t'Refresh or acquire access_token.'\n\t\tres = self.auth_access_data_raw = self._auth_token_request()\n\t\treturn self._auth_token_process(res, check_scope=check_scope)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_user_id(self):\n\t\t'Returns \"id\" of a OneDrive user.'\n\t\tif self._user_id is None:\n\t\t\tself._user_id = self.get_user_data()['id']\n\t\treturn self._user_id", "response": "Returns id of a OneDrive user."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget OneDrive object representing list of objects in a folder.", "response": "def listdir(self, folder_id='me/skydrive', limit=None, offset=None):\n\t\t'Get OneDrive object representing list of objects in a folder.'\n\t\treturn self(self._api_url_join(folder_id, 'files'), dict(limit=limit, offset=offset))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(self, obj_id, byte_range=None):\n\t\t'''Download and return a file object or a specified byte_range from it.\n\t\t\tSee HTTP Range header (rfc2616) for possible byte_range formats,\n\t\t\tExamples: \"0-499\" - byte offsets 0-499 (inclusive), \"-500\" - final 500 bytes.'''\n\t\tkwz = dict()\n\t\tif byte_range: kwz['headers'] = dict(Range='bytes={}'.format(byte_range))\n\t\treturn self(self._api_url_join(obj_id, 'content'), dict(download='true'), raw=True, **kwz)", "response": "Download and return a file object or a specified byte_range from it."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef put( self, path_or_tuple, folder_id='me/skydrive',\n\t\t\toverwrite=None, downsize=None, bits_api_fallback=True ):\n\t\t'''Upload a file (object), possibly overwriting (default behavior)\n\t\t\t\ta file with the same \"name\" attribute, if it exists.\n\n\t\t\tFirst argument can be either path to a local file or tuple\n\t\t\t\tof \"(name, file)\", where \"file\" can be either a file-like object\n\t\t\t\tor just a string of bytes.\n\n\t\t\toverwrite option can be set to False to allow two identically-named\n\t\t\t\tfiles or \"ChooseNewName\" to let OneDrive derive some similar\n\t\t\t\tunique name. Behavior of this option mimics underlying API.\n\n\t\t\tdownsize is a true/false API flag, similar to overwrite.\n\n\t\t\tbits_api_fallback can be either True/False or an integer (number of\n\t\t\t\tbytes), and determines whether method will fall back to using BITS API\n\t\t\t\t(as implemented by \"put_bits\" method) for large files. Default \"True\"\n\t\t\t\t(bool) value will use non-BITS file size limit (api_put_max_bytes, ~100 MiB)\n\t\t\t\tas a fallback threshold, passing False will force using single-request uploads.'''\n\t\tapi_overwrite = self._translate_api_flag(overwrite, 'overwrite', ['ChooseNewName'])\n\t\tapi_downsize = self._translate_api_flag(downsize, 'downsize')\n\t\tname, src = self._process_upload_source(path_or_tuple)\n\n\t\tif not isinstance(bits_api_fallback, (int, float, long)):\n\t\t\tbits_api_fallback = bool(bits_api_fallback)\n\t\tif bits_api_fallback is not False:\n\t\t\tif bits_api_fallback is True: bits_api_fallback = self.api_put_max_bytes\n\t\t\tsrc.seek(0, os.SEEK_END)\n\t\t\tif src.tell() >= bits_api_fallback:\n\t\t\t\tif bits_api_fallback > 0: # not really a \"fallback\" in this case\n\t\t\t\t\tlog.info(\n\t\t\t\t\t\t'Falling-back to using BITS API due to file size (%.1f MiB > %.1f MiB)',\n\t\t\t\t\t\t*((float(v) / 2**20) for v in [src.tell(), bits_api_fallback]) )\n\t\t\t\tif overwrite is not None and api_overwrite != 'true':\n\t\t\t\t\traise NoAPISupportError( 'Passed \"overwrite\" flag (value: {!r})'\n\t\t\t\t\t\t' is not supported by the BITS API (always \"true\" there)'.format(overwrite) )\n\t\t\t\tif downsize is not None:\n\t\t\t\t\tlog.info( 'Passed \"downsize\" flag (value: %r) will not'\n\t\t\t\t\t\t' be used with BITS API, as it is not supported there', downsize )\n\t\t\t\tfile_id = self.put_bits(path_or_tuple, folder_id=folder_id) # XXX: overwrite/downsize\n\t\t\t\treturn self.info(file_id)\n\n\t\t# PUT seem to have better support for unicode\n\t\t#  filenames and is recommended in the API docs, see #19.\n\t\t# return self( self._api_url_join(folder_id, 'files'),\n\t\t# \tdict(overwrite=api_overwrite, downsize_photo_uploads=api_downsize),\n\t\t# \tmethod='post', files=dict(file=(name, src)) )\n\t\treturn self( self._api_url_join(folder_id, 'files', name),\n\t\t\tdict(overwrite=api_overwrite, downsize_photo_uploads=api_downsize),\n\t\t\tdata=src, method='put', auth_header=True )", "response": "Upload a file to OneDrive."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef put_bits( self, path_or_tuple,\n\t\t\tfolder_id=None, folder_path=None, frag_bytes=None,\n\t\t\traw_id=False, chunk_callback=None ):\n\t\t'''Upload a file (object) using BITS API (via several http requests), possibly\n\t\t\t\toverwriting (default behavior) a file with the same \"name\" attribute, if it exists.\n\n\t\t\tUnlike \"put\" method, uploads to \"folder_path\" (instead of folder_id) are\n\t\t\t\tsupported here. Either folder path or id can be specified, but not both.\n\n\t\t\tPassed \"chunk_callback\" function (if any) will be called after each\n\t\t\t\tuploaded chunk with keyword parameters corresponding to\n\t\t\t\tupload state and BITS session info required to resume it, if necessary.\n\n\t\t\tReturns id of the uploaded file, as retured by the API\n\t\t\t\tif raw_id=True is passed, otherwise in a consistent (with other calls)\n\t\t\t\t\"file.{user_id}.{file_id}\" format (default).'''\n\t\t# XXX: overwrite/downsize are not documented/supported here (yet?)\n\t\tname, src = self._process_upload_source(path_or_tuple)\n\n\t\tif folder_id is not None and folder_path is not None:\n\t\t\traise ValueError('Either \"folder_id\" or \"folder_path\" can be specified, but not both.')\n\t\tif folder_id is None and folder_path is None: folder_id = 'me/skydrive'\n\t\tif folder_id and re.search(r'^me(/.*)$', folder_id): folder_id = self.info(folder_id)['id']\n\t\tif not frag_bytes: frag_bytes = self.api_bits_default_frag_bytes\n\n\t\tuser_id = self.get_user_id()\n\t\tif folder_id: # workaround for API-ids inconsistency between BITS and regular API\n\t\t\tmatch = re.search( r'^(?i)folder.[a-f0-9]+.'\n\t\t\t\t'(?P<user_id>[a-f0-9]+(?P<folder_n>!\\d+)?)$', folder_id )\n\t\t\tif match and not match.group('folder_n'):\n\t\t\t\t# root folder is a special case and can't seem to be accessed by id\n\t\t\t\tfolder_id, folder_path = None, ''\n\t\t\telse:\n\t\t\t\tif not match:\n\t\t\t\t\traise ValueError('Failed to process folder_id for BITS API: {!r}'.format(folder_id))\n\t\t\t\tfolder_id = match.group('user_id')\n\n\t\tif folder_id:\n\t\t\turl = self.api_bits_url_by_id.format(folder_id=folder_id, user_id=user_id, filename=name)\n\t\telse:\n\t\t\turl = self.api_bits_url_by_path.format(\n\t\t\t\tfolder_id=folder_id, user_id=user_id, file_path=ujoin(folder_path, name).lstrip('/') )\n\n\t\tcode, headers, body = self(\n\t\t\turl, method='post', auth_header=True, raw_all=True,\n\t\t\theaders={\n\t\t\t\t'X-Http-Method-Override': 'BITS_POST',\n\t\t\t\t'BITS-Packet-Type': 'Create-Session',\n\t\t\t\t'BITS-Supported-Protocols': self.api_bits_protocol_id })\n\n\t\th = lambda k,hs=dict((k.lower(), v) for k,v in headers.viewitems()): hs.get(k, '')\n\t\tchecks = [ code == 201,\n\t\t\th('bits-packet-type').lower() == 'ack',\n\t\t\th('bits-protocol').lower() == self.api_bits_protocol_id.lower(),\n\t\t\th('bits-session-id') ]\n\t\tif not all(checks):\n\t\t\traise ProtocolError(code, 'Invalid BITS Create-Session response', headers, body, checks)\n\t\tbits_sid = h('bits-session-id')\n\n\t\tsrc.seek(0, os.SEEK_END)\n\t\tc, src_len = 0, src.tell()\n\t\tcn = src_len / frag_bytes\n\t\tif frag_bytes * cn != src_len: cn += 1\n\t\tsrc.seek(0)\n\t\tfor n in xrange(1, cn+1):\n\t\t\tlog.debug( 'Uploading BITS fragment'\n\t\t\t\t' %s / %s (max-size: %.2f MiB)', n, cn, frag_bytes / float(2**20) )\n\t\t\tfrag = BITSFragment(src, frag_bytes)\n\t\t\tc1 = c + frag_bytes\n\t\t\tself(\n\t\t\t\turl, method='post', raw=True, data=frag,\n\t\t\t\theaders={\n\t\t\t\t\t'X-Http-Method-Override': 'BITS_POST',\n\t\t\t\t\t'BITS-Packet-Type': 'Fragment',\n\t\t\t\t\t'BITS-Session-Id': bits_sid,\n\t\t\t\t\t'Content-Range': 'bytes {}-{}/{}'.format(c, min(c1, src_len)-1, src_len) })\n\t\t\tc = c1\n\n\t\t\tif chunk_callback:\n\t\t\t\tchunk_callback(\n\t\t\t\t\tbytes_transferred=c, bytes_total=src_len,\n\t\t\t\t\tchunks_transferred=n, chunks_total=cn,\n\t\t\t\t\tbits_session_id=bits_sid )\n\n\t\tif self.api_bits_auth_refresh_before_commit_hack:\n\t\t\t# As per #39 and comments under the gist with the spec,\n\t\t\t#  apparently this trick fixes occasional http-5XX errors from the API\n\t\t\tself.auth_get_token()\n\n\t\tcode, headers, body = self(\n\t\t\turl, method='post', auth_header=True, raw_all=True,\n\t\t\theaders={\n\t\t\t\t'X-Http-Method-Override': 'BITS_POST',\n\t\t\t\t'BITS-Packet-Type': 'Close-Session',\n\t\t\t\t'BITS-Session-Id': bits_sid })\n\t\th = lambda k,hs=dict((k.lower(), v) for k,v in headers.viewitems()): hs.get(k, '')\n\t\tchecks = [code in [200, 201], h('bits-packet-type').lower() == 'ack' ]\n\t\t\t# int(h('bits-received-content-range') or 0) == src_len -- documented, but missing\n\t\t\t# h('bits-session-id') == bits_sid -- documented, but missing\n\t\tif not all(checks):\n\t\t\traise ProtocolError(code, 'Invalid BITS Close-Session response', headers, body, checks)\n\n\t\t# Workaround for API-ids inconsistency between BITS and regular API\n\t\tfile_id = h('x-resource-id')\n\t\tif not raw_id: file_id = 'file.{}.{}'.format(user_id, file_id)\n\t\treturn file_id", "response": "Upload a file to BITS."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a folder with a specified name.", "response": "def mkdir(self, name=None, folder_id='me/skydrive', metadata=dict()):\n\t\t'''Create a folder with a specified \"name\" attribute.\n\t\t\t\tfolder_id allows to specify a parent folder.\n\t\t\t\tmetadata mapping may contain additional folder properties to pass to an API.'''\n\t\tmetadata = metadata.copy()\n\t\tif name: metadata['name'] = name\n\t\treturn self(folder_id, data=metadata, method='post', auth_header=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating metadata with of a specified object.", "response": "def info_update(self, obj_id, data):\n\t\t'''Update metadata with of a specified object.\n\t\t\tSee http://msdn.microsoft.com/en-us/library/live/hh243648.aspx\n\t\t\t\tfor the list of RW keys for each object type.'''\n\t\treturn self(obj_id, method='put', data=data, auth_header=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a preauthenticated link to a specific object.", "response": "def link(self, obj_id, link_type='shared_read_link'):\n\t\t'''Return a preauthenticated (usable by anyone) link to a\n\t\t\t\tspecified object. Object will be considered \"shared\" by OneDrive,\n\t\t\t\teven if link is never actually used.\n\t\t\tlink_type can be either \"embed\" (returns html), \"shared_read_link\" or \"shared_edit_link\".'''\n\t\tassert link_type in ['embed', 'shared_read_link', 'shared_edit_link']\n\t\treturn self(self._api_url_join(obj_id, link_type), method='get')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef copy(self, obj_id, folder_id, move=False):\n\t\t'''Copy specified file (object) to a folder with a given ID.\n\t\t\t\tWell-known folder names (like \"me/skydrive\")\n\t\t\t\tdon't seem to work here.\n\t\t\tFolders cannot be copied; this is an API limitation.'''\n\t\treturn self( obj_id,\n\t\t\tmethod='copy' if not move else 'move',\n\t\t\tdata=dict(destination=folder_id), auth_header=True )", "response": "Copy specified file to a folder with a given ID."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef move(self, obj_id, folder_id):\n\t\t'''Move specified file (object) to a folder.\n\t\t\tNote that folders cannot be moved, this is an API limitation.'''\n\t\treturn self.copy(obj_id, folder_id, move=True)", "response": "Move specified file to a folder."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding comment message to a specified object.", "response": "def comment_add(self, obj_id, message):\n\t\t'Add comment message to a specified object.'\n\t\treturn self( self._api_url_join(obj_id, 'comments'),\n\t\t\tmethod='post', data=dict(message=message), auth_header=True )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef resolve_path(self, path, root_id='me/skydrive', objects=False, listdir_limit=500):\n\t\t'''Return id (or metadata) of an object, specified by chain\n\t\t\t\t(iterable or fs-style path string) of \"name\" attributes\n\t\t\t\tof its ancestors, or raises DoesNotExists error.\n\n\t\t\tRequires many calls to resolve each name in path, so use with care.\n\t\t\t\troot_id parameter allows to specify path relative to some folder_id\n\t\t\t\t(default: me/skydrive).'''\n\t\tif path:\n\t\t\tif isinstance(path, types.StringTypes):\n\t\t\t\tif not path.startswith('me/skydrive'):\n\t\t\t\t\t# Split path by both kinds of slashes\n\t\t\t\t\tpath = filter(None, it.chain.from_iterable(p.split('\\\\') for p in path.split('/')))\n\t\t\t\telse: root_id, path = path, None\n\t\t\tif path:\n\t\t\t\ttry:\n\t\t\t\t\tfor i, name in enumerate(path):\n\t\t\t\t\t\toffset = None\n\t\t\t\t\t\twhile True:\n\t\t\t\t\t\t\tobj_list = self.listdir(root_id, offset=offset, limit=listdir_limit)\n\t\t\t\t\t\t\ttry: root_id = dict(it.imap(op.itemgetter('name', 'id'), obj_list))[name]\n\t\t\t\t\t\t\texcept KeyError:\n\t\t\t\t\t\t\t\tif len(obj_list) < listdir_limit: raise # assuming that it's the last page\n\t\t\t\t\t\t\t\toffset = (offset or 0) + listdir_limit\n\t\t\t\t\t\t\telse: break\n\t\t\t\texcept (KeyError, ProtocolError) as err:\n\t\t\t\t\tif isinstance(err, ProtocolError) and err.code != 404: raise\n\t\t\t\t\traise DoesNotExists(root_id, path[i:])\n\t\treturn root_id if not objects else self.info(root_id)", "response": "Resolve a path to a root object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of objects in the specified folder_id.", "response": "def listdir(self, folder_id='me/skydrive', type_filter=None, limit=None, offset=None):\n\t\t'''Return a list of objects in the specified folder_id.\n\t\t\tlimit is passed to the API, so might be used as optimization.\n\t\t\ttype_filter can be set to type (str) or sequence\n\t\t\t\tof object types to return, post-api-call processing.'''\n\t\tlst = super(OneDriveAPI, self)\\\n\t\t\t.listdir(folder_id=folder_id, limit=limit, offset=offset)['data']\n\t\tif type_filter:\n\t\t\tif isinstance(type_filter, types.StringTypes): type_filter = {type_filter}\n\t\t\tlst = list(obj for obj in lst if obj['type'] in type_filter)\n\t\treturn lst"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef copy(self, obj_id, folder_id, move=False):\n\t\t'''Copy specified file (object) to a folder.\n\t\t\tNote that folders cannot be copied, this is an API limitation.'''\n\t\tif folder_id.startswith('me/skydrive'):\n\t\t\tlog.info( 'Special folder names (like \"me/skydrive\") dont'\n\t\t\t\t' seem to work with copy/move operations, resolving it to id' )\n\t\t\tfolder_id = self.info(folder_id)['id']\n\t\treturn super(OneDriveAPI, self).copy(obj_id, folder_id, move=move)", "response": "Copy specified file to a folder."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_conf(cls, path=None, **overrides):\n\t\t'''Initialize instance from YAML configuration file,\n\t\t\twriting updates (only to keys, specified by \"conf_update_keys\") back to it.'''\n\t\tfrom onedrive import portalocker\n\t\timport yaml\n\n\t\tif path is None:\n\t\t\tpath = cls.conf_path_default\n\t\t\tlog.debug('Using default state-file path: %r', path)\n\t\tpath = os.path.expanduser(path)\n\t\twith open(path, 'rb') as src:\n\t\t\tportalocker.lock(src, portalocker.LOCK_SH)\n\t\t\tyaml_str = src.read()\n\t\t\tportalocker.unlock(src)\n\t\tconf = yaml.safe_load(yaml_str)\n\t\tconf.setdefault('conf_save', path)\n\n\t\tconf_cls = dict()\n\t\tfor ns, keys in cls.conf_update_keys.viewitems():\n\t\t\tfor k in keys:\n\t\t\t\ttry:\n\t\t\t\t\tv = conf.get(ns, dict()).get(k)\n\t\t\t\texcept AttributeError:\n\t\t\t\t\tif not cls.conf_raise_structure_errors: raise\n\t\t\t\t\traise KeyError((\n\t\t\t\t\t\t'Unable to get value for configuration parameter'\n\t\t\t\t\t\t\t' \"{k}\" in section \"{ns}\", check configuration file (path: {path}) syntax'\n\t\t\t\t\t\t\t' near the aforementioned section/value.' ).format(ns=ns, k=k, path=path))\n\t\t\t\tif v is not None: conf_cls['{}_{}'.format(ns, k)] = conf[ns][k]\n\t\tconf_cls.update(overrides)\n\n\t\t# Hack to work around YAML parsing client_id of e.g. 000123 as an octal int\n\t\tif isinstance(conf.get('client', dict()).get('id'), (int, long)):\n\t\t\tlog.warn( 'Detected client_id being parsed as an integer (as per yaml), trying to un-mangle it.'\n\t\t\t\t' If requests will still fail afterwards, please replace it in the configuration file (path: %r),'\n\t\t\t\t\t' also putting single or double quotes (either one should work) around the value.', path )\n\t\t\tcid = conf['client']['id']\n\t\t\tif not re.search(r'\\b(0*)?{:d}\\b'.format(cid), yaml_str)\\\n\t\t\t\t\tand re.search(r'\\b(0*)?{:o}\\b'.format(cid), yaml_str):\n\t\t\t\tcid = int('{:0}'.format(cid))\n\t\t\tconf['client']['id'] = '{:016d}'.format(cid)\n\n\t\tself = cls(**conf_cls)\n\t\tself.conf_save = conf['conf_save']\n\t\treturn self", "response": "Initialize an object from a YAML configuration file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef decode_obj(obj, force=False):\n\t'Convert or dump object to unicode.'\n\tif isinstance(obj, unicode): return obj\n\telif isinstance(obj, bytes):\n\t\tif force_encoding is not None: return obj.decode(force_encoding)\n\t\tif chardet:\n\t\t\tenc_guess = chardet.detect(obj)\n\t\t\tif enc_guess['confidence'] > 0.7:\n\t\t\t\treturn obj.decode(enc_guess['encoding'])\n\t\treturn obj.decode('utf-8')\n\telse:\n\t\treturn obj if not force else repr(obj)", "response": "Convert or dump object to unicode."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_drop_target(obj, root, designer, inspector):\n    \"Recursively create and set the drop target for obj and childs\"\n    if obj._meta.container:\n        dt = ToolBoxDropTarget(obj, root, designer=designer, \n                                          inspector=inspector)\n        obj.drop_target = dt\n    for child in obj:\n        set_drop_target(child, root, designer, inspector)", "response": "Recursively create and set the drop target for obj and childs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef start_drag_opperation(self, evt):\n        \"Event handler for drag&drop functionality\"\n    \n        # get the control\n        ctrl = self.menu_ctrl_map[evt.GetToolId()]\n\n        # create our own data format and use it in a custom data object\n        ldata = wx.CustomDataObject(\"gui\")\n        ldata.SetData(ctrl._meta.name)      # only strings are allowed!\n\n        # Also create a Bitmap version of the drawing\n        bmp = ctrl._image.GetBitmap()\n\n        # Now make a data object for the bitmap and also a composite\n        # data object holding both of the others.\n        bdata = wx.BitmapDataObject(bmp)\n        data = wx.DataObjectComposite()\n        data.Add(ldata)\n        data.Add(bdata)\n\n        # And finally, create the drop source and begin the drag\n        # and drop opperation\n        dropSource = wx.DropSource(self)\n        dropSource.SetData(data)\n        if DEBUG: print(\"Begining DragDrop\\n\")\n        result = dropSource.DoDragDrop(wx.Drag_AllowMove)\n        if DEBUG: print(\"DragDrop completed: %d\\n\" % result)\n\n        if result == wx.DragMove:\n            if DEBUG: print \"dragmove!\"\n            self.Refresh()", "response": "Event handler for drag & drop functionality"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntrack default top level window for toolbox menu default action", "response": "def set_default_tlw(self, tlw, designer, inspector):\n        \"track default top level window for toolbox menu default action\"\n        self.designer = designer\n        self.inspector = inspector"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef copy(self):\n        \"Return a copy of the drop target (to avoid wx problems on rebuild)\"\n        return ToolBoxDropTarget(self.dv, self.root, \n                                 self.designer, self.inspector)", "response": "Return a copy of the drop target ( to avoid wx problems on rebuild"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef inspect(obj):\n    \"Open the inspector windows for a given object\"\n    from gui.tools.inspector import InspectorTool\n    inspector = InspectorTool()\n    inspector.show(obj)\n    return inspector", "response": "Open the inspector windows for a given object"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntake a pythoncard background resource and convert to a gui2py window", "response": "def migrate_window(bg):\n    \"Take a pythoncard background resource and convert to a gui2py window\"\n    ret = {}\n    for k, v in bg.items():\n        if k == 'type':\n            v = WIN_MAP[v]._meta.name\n        elif k == 'menubar':\n            menus = v['menus']\n            v = [migrate_control(menu) for menu in menus]\n        elif k == 'components':\n            v = [migrate_control(comp) for comp in v]\n        else:\n            k = SPEC_MAP['Widget'].get(k, k)\n        ret[k] = v\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef migrate_control(comp):\n    \"Take a pythoncard background resource and convert to a gui2py window\"\n    ret = {}\n    for k, v in comp.items():\n        if k == 'type':\n            v = CTRL_MAP[v]._meta.name\n        elif k == 'menubar':\n            pass\n        elif k == 'components':\n            v = [migrate_control(comp) for comp in v]\n        else:\n            k = SPEC_MAP['Widget'].get(k, k)\n            if comp['type'] in SPEC_MAP:\n                k = SPEC_MAP[comp['type']].get(k, k)\n            if k == 'font':\n                v = migrate_font(v)\n        ret[k] = v\n    return ret", "response": "Take a pythoncard background resource and convert to a gui2py window"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef migrate_font(font):\n    \"Convert PythonCard font description to gui2py style\"\n    if 'faceName' in font:\n        font['face'] = font.pop('faceName')\n    if 'family' in font and font['family'] == 'sansSerif':\n        font['family'] = 'sans serif'\n    return font", "response": "Convert PythonCard font description to gui2py style"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads HTML page from location and then displays it", "response": "def load_page(self, location):\n        \"Loads HTML page from location and then displays it\"\n        if not location:\n            self.wx_obj.SetPage(\"\")\n        else:\n            self.wx_obj.LoadPage(location)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef edit(self, name=\"\"):\n        \"Programatically select a (default) property to start editing it\"\n        # for more info see DoSelectAndEdit in propgrid.cpp\n        for name in (name, \"label\", \"value\", \"text\", \"title\", \"filename\", \n                           \"name\"):\n            prop = self.pg.GetPropertyByName(name)\n            if prop is not None:\n                break\n        self.Parent.SetFocus()\n        self.Parent.Raise()\n        self.pg.SetFocus()\n        # give time to the ui to show the prop grid and set focus:\n        wx.CallLater(250, self.select, prop.GetName())", "response": "Programatically select a ( default ) property to start editing it"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nselects a property ( and start the editor )", "response": "def select(self, name, flags=0):\n        \"Select a property (and start the editor)\"\n        # do not call this directly from another window, use edit() instead\n        # // wxPropertyGrid::DoSelectProperty flags (selFlags) -see propgrid.h-\n        wxPG_SEL_FOCUS=0x0001  # Focuses to created editor\n        wxPG_SEL_FORCE=0x0002  # Forces deletion and recreation of editor\n        flags |= wxPG_SEL_FOCUS # | wxPG_SEL_FORCE\n        prop = self.pg.GetPropertyByName(name)\n        self.pg.SelectProperty(prop, flags)\n        if DEBUG: print \"selected!\", prop"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef send(evt):\n    \"Process an outgoing communication\"\n    # get the text written by the user (input textbox control)\n    msg = ctrl_input.value\n    # send the message (replace with socket/queue/etc.)\n    gui.alert(msg, \"Message\")\n    # record the message (update the UI)\n    log(msg)\n    ctrl_input.value = \"\"\n    ctrl_input.set_focus()", "response": "Process an outgoing communication"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save(evt, designer):\n    \"Basic save functionality: just replaces the gui code\"\n\n    # ask the user if we should save the changes:\n    ok = gui.confirm(\"Save the changes?\", \"GUI2PY Designer\", \n                     cancel=True, default=True)\n    if ok:\n        wx_obj = evt.GetEventObject()\n        w = wx_obj.obj\n        try:\n            if DEBUG: print \"saving...\"\n            # make a backup:\n            fin = open(designer.filename, \"r\")\n            fout = open(designer.filename + \".bak\", \"w\")\n            fout.write(fin.read())\n            fout.close()\n            fin.close()\n            if designer.filename.endswith(\".rsrc.py\"):\n                # serialize and save the resource\n                gui.save(designer.filename, [gui.dump(w)])\n            else:\n                # reopen the files to proccess them\n                fin = open(designer.filename + \".bak\", \"r\")\n                fout = open(designer.filename, \"w\")\n                copy = True\n                newlines = fin.newlines or \"\\n\"\n\n                def dump(obj, indent=1):\n                    \"recursive convert object to string\"\n                    for ctl in obj[:]:\n                        write(ctl, indent)\n\n                def write(ctl, indent):\n                    if ctl[:]:\n                        fout.write(\" \" * indent * 4)\n                        fout.write(\"with %s:\" % ctl.__repr__(parent=None, indent=indent, context=True))\n                        fout.write(newlines)\n                        dump(ctl, indent + 1)                \n                    else:\n                        fout.write(\" \" * indent * 4)\n                        fout.write(ctl.__repr__(parent=None, indent=indent))\n                        fout.write(newlines)\n\n                dumped = False\n                for line in fin:\n                    if line.startswith(\"# --- gui2py designer generated code starts ---\"):\n                        fout.write(line)\n                        fout.write(newlines)\n                        write(w, indent=0)\n                        fout.write(newlines)\n                        dumped = True\n                        copy = False\n                    if line.startswith(\"# --- gui2py designer generated code ends ---\"):\n                        copy = True\n                    if copy:\n                        fout.write(line)\n                        #fout.write(\"\\n\\r\")\n                if not dumped:\n                    gui.alert(\"No valid # --- gui2py... delimiters! \\n\"\n                              \"Unable to write down design code!\", \n                              \"Design not updated:\")\n            fout.close()\n            fin.close()\n        except Exception, e:\n            import traceback\n            print(traceback.print_exc())\n            ok = gui.confirm(\"Close anyway?\\n%s\" % str(e), 'Unable to save:', \n                             ok=True, cancel=True)\n    if ok is not None:\n        wx.CallAfter(exit)    # terminate the designer program\n    return ok", "response": "Basic save functionality just replaces the gui code"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nshow a tip message", "response": "def wellcome_tip(wx_obj):\n    \"Show a tip message\"\n    \n    msg = (\"Close the main window to exit & save.\\n\"\n           \"Drag & Drop / Click the controls from the ToolBox to create new ones.\\n\"\n           \"Left click on the created controls to select them.\\n\"\n           \"Double click to edit the default property.\\n\"\n           \"Right click to pop-up the context menu.\\n\")\n    # create a super tool tip manager and set some styles\n    stt = STT.SuperToolTip(msg)\n    stt.SetHeader(\"Welcome to gui2py designer!\")\n    stt.SetDrawHeaderLine(True)\n    stt.ApplyStyle(\"Office 2007 Blue\")\n    stt.SetDropShadow(True)\n    stt.SetHeaderBitmap(images.designer.GetBitmap())\n    stt.SetEndDelay(15000)                      # hide in 15 s\n    # create a independent tip window, show/hide manually (avoid binding wx_obj)\n    tip = CustomToolTipWindow(wx_obj, stt)\n    tip.CalculateBestSize()\n    tip.CalculateBestPosition(wx_obj)\n    tip.DropShadow(stt.GetDropShadow())\n    if stt.GetUseFade():\n        show = lambda: tip.StartAlpha(True)\n    else:\n        show = lambda: tip.Show()\n    wx.CallLater(1000, show)           # show the tip in 1 s\n    wx.CallLater(30000, tip.Destroy)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mouse_down(self, evt): \n        \"Get the selected object and store start position\"\n        if DEBUG: print \"down!\"\n        if (not evt.ControlDown() and not evt.ShiftDown()) or evt.AltDown():\n            for obj in self.selection:\n                # clear marker\n                if obj.sel_marker:\n                    obj.sel_marker.show(False)\n                    obj.sel_marker.destroy()\n                    obj.sel_marker = None\n            self.selection = []  # clear previous selection\n\n        wx_obj = evt.GetEventObject()\n\n        if wx_obj.Parent is None or evt.AltDown():\n            if not evt.AltDown():\n                evt.Skip()\n            # start the rubberband effect (multiple selection using the mouse) \n            self.current = wx_obj\n            self.overlay = wx.Overlay()\n            self.pos = evt.GetPosition() \n            self.parent.wx_obj.CaptureMouse()\n            #if self.inspector and hasattr(wx_obj, \"obj\"):\n            #    self.inspector.inspect(wx_obj.obj)  # inspect top level window\n            #self.dclick = False\n        else:\n            # create the selection marker and assign it to the control\n            obj = wx_obj.obj\n            self.overlay = None\n            if DEBUG: print wx_obj\n            sx, sy = wx_obj.ScreenToClient(wx_obj.GetPositionTuple())\n            dx, dy = wx_obj.ScreenToClient(wx.GetMousePosition())\n            self.pos = wx_obj.ScreenToClient(wx.GetMousePosition())\n            self.start = (sx - dx, sy - dy)\n            self.current = wx_obj\n            if DEBUG: print \"capture...\"\n            # do not capture on TextCtrl, it will fail (blocking) at least in gtk\n            # do not capture on wx.Notebook to allow selecting the tabs\n            if not isinstance(wx_obj, wx.Notebook):\n                self.parent.wx_obj.CaptureMouse()\n            self.select(obj, keep_selection=True)", "response": "Get the selected object and store start position"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmoving the selected object", "response": "def mouse_move(self, evt):\n        \"Move the selected object\"\n        if DEBUG: print \"move!\"\n        if self.current and not self.overlay:\n            wx_obj = self.current\n            sx, sy = self.start\n            x, y = wx.GetMousePosition()\n            # calculate the new position (this will overwrite relative dimensions):\n            x, y = (x + sx, y + sy)\n            if evt.ShiftDown():     # snap to grid:\n                x = x / GRID_SIZE[0] * GRID_SIZE[0]\n                y = y / GRID_SIZE[1] * GRID_SIZE[1]\n            # calculate the diff to use in the rest of the selected objects:\n            ox, oy = wx_obj.obj.pos\n            dx, dy = (x - ox), (y - oy)\n            # move all selected objects:\n            for obj in self.selection:\n                x, y = obj.pos\n                x = x + dx\n                y = y + dy\n                obj.pos = (wx.Point(x, y))\n        elif self.overlay:\n            wx_obj = self.current\n            pos = evt.GetPosition()\n            # convert to relative client coordinates of the containter:\n            if evt.GetEventObject() != wx_obj:\n                pos = evt.GetEventObject().ClientToScreen(pos)  # frame\n                pos = wx_obj.ScreenToClient(pos)                # panel\n            rect = wx.RectPP(self.pos, pos) \n            # Draw the rubber-band rectangle using an overlay so it \n            # will manage keeping the rectangle and the former window \n            # contents separate. \n            dc = wx.ClientDC(wx_obj) \n            odc = wx.DCOverlay(self.overlay, dc) \n            odc.Clear() \n            dc.SetPen(wx.Pen(\"blue\", 2)) \n            if 'wxMac' in wx.PlatformInfo: \n                dc.SetBrush(wx.Brush(wx.Colour(0xC0, 0xC0, 0xC0, 0x80))) \n            else: \n                dc.SetBrush(wx.TRANSPARENT_BRUSH) \n            dc.DrawRectangleRect(rect)\n            del odc"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreleasing the selected object ( pass a wx_obj if the event was captured )", "response": "def mouse_up(self, evt, wx_obj=None):\n        \"Release the selected object (pass a wx_obj if the event was captured)\"\n        self.resizing = False\n        if self.current: \n            wx_obj = self.current\n            if self.parent.wx_obj.HasCapture():\n                self.parent.wx_obj.ReleaseMouse()\n            self.current = None\n            if self.overlay:\n                # When the mouse is released we reset the overlay and it \n                # restores the former content to the window. \n                dc = wx.ClientDC(wx_obj)\n                odc = wx.DCOverlay(self.overlay, dc)\n                odc.Clear()\n                del odc\n                self.overlay.Reset()                \n                self.overlay = None\n                pos = evt.GetPosition()\n                # convert to relative client coordinates of the container:\n                if evt.GetEventObject() != wx_obj:\n                    pos = evt.GetEventObject().ClientToScreen(pos)  # frame\n                    pos = wx_obj.ScreenToClient(pos)                # panel\n                # finish the multiple selection using the mouse:\n                rect = wx.RectPP(self.pos, pos)\n                for obj in wx_obj.obj:\n                    # only check child controls (not menubar/statusbar)\n                    if isinstance(obj, Control):\n                        obj_rect = obj.wx_obj.GetRect()\n                        if rect.ContainsRect(obj_rect):\n                            self.select(obj, keep_selection=True)\n                self.pos = None \n\n        if self.inspector and wx_obj:\n            self.inspector.inspect(wx_obj.obj)\n        if DEBUG: print \"SELECTION\", self.selection"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsupports cursor keys to move components one pixel at a time", "response": "def key_press(self, event):\n        \"support cursor keys to move components one pixel at a time\"\n        key = event.GetKeyCode()\n        if key in (wx.WXK_LEFT, wx.WXK_UP, wx.WXK_RIGHT, wx.WXK_DOWN):\n            for obj in self.selection:\n                x, y = obj.pos\n                if event.ShiftDown():     # snap to grid:t \n                    # for now I'm only going to align to grid\n                    # in the direction of the cursor movement \n                    if key == wx.WXK_LEFT:\n                        x = (x - GRID_SIZE[0]) / GRID_SIZE[0] * GRID_SIZE[0]\n                    elif key == wx.WXK_RIGHT:\n                        x = (x + GRID_SIZE[0]) / GRID_SIZE[0] * GRID_SIZE[0]\n                    elif key == wx.WXK_UP:\n                        y = (y - GRID_SIZE[1]) / GRID_SIZE[1] * GRID_SIZE[1]\n                    elif key == wx.WXK_DOWN:\n                        y = (y + GRID_SIZE[1]) / GRID_SIZE[1] * GRID_SIZE[1]\n                else:\n                    if key == wx.WXK_LEFT:\n                        x = x - 1\n                    elif key == wx.WXK_RIGHT:\n                        x = x + 1\n                    elif key == wx.WXK_UP:\n                        y = y - 1\n                    elif key == wx.WXK_DOWN:\n                        y = y + 1\n                obj.pos = (x, y)\n        elif key == wx.WXK_DELETE:\n            self.delete(event)\n        elif key == wx.WXK_INSERT:\n            self.duplicate(event)\n        else:\n            if DEBUG: print \"KEY:\", key"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete(self, event):\n        \"delete all of the selected objects\"\n        # get the selected objects (if any)\n        for obj in self.selection:\n            if obj:\n                if DEBUG: print \"deleting\", obj.name\n                obj.destroy()\n        self.selection = []                         # clean selection\n        self.inspector.load_object()", "response": "delete all of the selected objects"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a copy of each selected object", "response": "def duplicate(self, event):\n        \"create a copy of each selected object\"\n        # duplicate the selected objects (if any)\n        new_selection = []\n        for obj in self.selection:\n            if obj:\n                if DEBUG: print \"duplicating\", obj.name\n                obj.sel_marker.destroy()\n                obj.sel_marker = None\n                obj2 = obj.duplicate()\n                obj2.sel_marker = SelectionMarker(obj2)\n                obj2.sel_marker.show(True)\n                new_selection.append(obj2)\n        self.selection = new_selection              # update with new obj's\n        self.inspector.load_object()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadjust facade with the dimensions of the original object ( and repaint )", "response": "def update(self):\n        \"Adjust facade with the dimensions of the original object (and repaint)\"\n        x, y = self.obj.wx_obj.GetPosition()\n        w, h = self.obj.wx_obj.GetSize()\n        self.Hide()\n        self.Move((x, y))\n        self.SetSize((w, h))\n        # allow original control to repaint before taking the new snapshot image:\n        wx.CallLater(200, self.refresh)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef refresh(self):\n        \"Capture the new control superficial image after an update\"\n        self.bmp = self.obj.snapshot()\n        # change z-order to overlap controls (windows) and show the image:\n        self.Raise()\n        self.Show()\n        self.Refresh()", "response": "Capture the new control superficial image after an update"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the pyth item data associated with the item", "response": "def GetPyData(self, item):\r\n        \"Returns the pyth item data associated with the item\"\r\n        wx_data = self.GetItemData(item)\r\n        py_data = self._py_data_map.get(wx_data)\r\n        return py_data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef SetPyData(self, item, py_data):\r\n        \"Set the python item data associated wit the wx item\"\r\n        wx_data = wx.NewId()                           # create a suitable key\r\n        self.SetItemData(item, wx_data)   # store it in wx \r\n        self._py_data_map[wx_data] = py_data           # map it internally\r\n        self._wx_data_map[py_data] = wx_data           # reverse map\r\n        return wx_data", "response": "Set the python item data associated wit the wx item"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef FindPyData(self, start, py_data):\r\n        \"Do a reverse look up for an item containing the requested data\"\r\n        # first, look at our internal dict:\r\n        wx_data = self._wx_data_map[py_data]\r\n        # do the real search at the wx control:\r\n        if wx.VERSION < (3, 0, 0) or 'classic' in wx.version():\r\n            data = self.FindItemData(start, wx_data)\r\n        else:\r\n            data = self.FindItem(start, wx_data)\r\n        return data", "response": "Do a reverse look up for an item containing the requested data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove the item from the list and unset the related data", "response": "def DeleteItem(self, item):\r\n        \"Remove the item from the list and unset the related data\"\r\n        wx_data = self.GetItemData(item)\r\n        py_data = self._py_data_map[wx_data]\r\n        del self._py_data_map[wx_data]\r\n        del self._wx_data_map[py_data]\r\n        wx.ListCtrl.DeleteItem(self, item)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove all the item from the list and unset the related data", "response": "def DeleteAllItems(self):\r\n        \"Remove all the item from the list and unset the related data\"\r\n        self._py_data_map.clear()\r\n        self._wx_data_map.clear()\r\n        wx.ListCtrl.DeleteAllItems(self)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_count(self, value):\r\n        \"Set item (row) count -useful only in virtual mode-\"\r\n        if self.view == \"report\" and self.virtual and value is not None:\r\n            self.wx_obj.SetItemCount(value)", "response": "Set item ( row ) count - useful only in virtual mode -\r\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete the item at the zero - based index n from the control.", "response": "def delete(self, a_position):\r\n        \"Deletes the item at the zero-based index 'n' from the control.\"\r\n        key = self.wx_obj.GetPyData(a_position)\r\n        del self._items[key]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove all items and column headings", "response": "def clear_all(self):\r\n        \"Remove all items and column headings\"\r\n        self.clear()\r\n        for ch in reversed(self.columns):\r\n            del self[ch.name]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nassociates the header to the control ( it could be recreated )", "response": "def set_parent(self, new_parent, init=False):\r\n        \"Associate the header to the control (it could be recreated)\"\r\n        self._created = False\r\n        SubComponent.set_parent(self, new_parent, init)\r\n        # if index not given, append the column at the last position:\r\n        if self.index == -1 or self.index > self._parent.wx_obj.GetColumnCount():\r\n            self.index = self._parent.wx_obj.GetColumnCount()\r\n        # insert the column in the listview:\r\n        self._parent.wx_obj.InsertColumn(self.index, self.text, self._align, \r\n                                         self.width)\r\n        self._created = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clear(self):\r\n        \"Remove all items and reset internal structures\"\r\n        dict.clear(self)\r\n        self._key = 0\r\n        if hasattr(self._list_view, \"wx_obj\"):\r\n            self._list_view.wx_obj.DeleteAllItems()", "response": "Remove all items and reset internal structures"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the index of the selected item ( list for multiselect ) or None", "response": "def _get_selection(self):\r\n        \"Returns the index of the selected item (list for multiselect) or None\"\r\n        if self.multiselect:\r\n            return self.wx_obj.GetSelections()\r\n        else:\r\n            sel = self.wx_obj.GetSelection()\r\n            if sel == wx.NOT_FOUND:\r\n                return None\r\n            else:\r\n                return sel"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _set_selection(self, index, dummy=False):\r\n        \"Sets the item at index 'n' to be the selected item.\"\r\n        # only change selection if index is None and not dummy:\r\n        if index is None:\r\n            self.wx_obj.SetSelection(-1)\r\n            # clean up text if control supports it:\r\n            if hasattr(self.wx_obj, \"SetValue\"):\r\n                self.wx_obj.SetValue(\"\")\r\n        else:\r\n            self.wx_obj.SetSelection(index)\r\n        # send a programmatically event (not issued by wx)\r\n        wx_event = ItemContainerControlSelectEvent(self._commandtype,\r\n                                                   index, self.wx_obj)\r\n        if hasattr(self, \"onchange\") and self.onchange:\r\n            # TODO: fix (should work but it doesn't):\r\n            ## wx.PostEvent(self.wx_obj, wx_evt)\r\n            # WORKAROUND:\r\n            event = FormEvent(name=\"change\", wx_event=wx_event)\r\n            self.onchange(event)", "response": "Sets the item at index n to be the selected item."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the label of the selected item or an empty string if none", "response": "def _get_string_selection(self):\r\n        \"Returns the label of the selected item or an empty string if none\"\r\n        if self.multiselect:\r\n            return [self.wx_obj.GetString(i) for i in \r\n                    self.wx_obj.GetSelections()]\r\n        else:\r\n            return self.wx_obj.GetStringSelection()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _set_items(self, a_iter):\r\n        \"Clear and set the strings (and data if any) in the control from a list\"\r\n\r\n        self._items_dict = {}\r\n        if not a_iter:\r\n            string_list = []\r\n            data_list = []\r\n        elif not isinstance(a_iter, (tuple, list, dict)):\r\n            raise ValueError(\"items must be an iterable\")\r\n        elif isinstance(a_iter, dict):\r\n            # use keys as data, values as label strings\r\n            self._items_dict = a_iter\r\n            string_list = a_iter.values()\r\n            data_list = a_iter.keys()\r\n        elif isinstance(a_iter[0], (tuple, list)) and len(a_iter[0]) == 2:\r\n            # like the dict, but ordered\r\n            self._items_dict = dict(a_iter)\r\n            data_list, string_list = zip(*a_iter)\r\n        else:\r\n            # use the same strings as data\r\n            string_list = a_iter\r\n            data_list = a_iter\r\n\r\n        # set the strings\r\n        self.wx_obj.SetItems(string_list)\r\n        # set the associated data\r\n        for i, data in enumerate(data_list):\r\n            self.set_data(i, data)", "response": "Clear and set the strings and data if any in the control from a list"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_data(self, n, data):\r\n        \"Associate the given client data with the item at position n.\"\r\n        self.wx_obj.SetClientData(n, data)\r\n        # reverse association:\r\n        self._items_dict[data] = self.get_string(n)", "response": "Associate the given client data with the item at position n."}
